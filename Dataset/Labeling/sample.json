[
    {
        "Question_title":"AWS Sagemaker Workflow pIpeline use the code stored in artifact created from Codebuild",
        "Question_body":"<p>I have created a <code>sagemaker.workflow.pipeline.Pipeline<\/code> object, in which, there are couple of processing step where I am trying to reference to an s3 file path rather than a local file path, so that it won't upload files to s3 everytime the pipeline runs.<\/p>\n<p>My question is, can I modify the <code>step<\/code> or <code>scriptprocessor<\/code> or <code>pipeline<\/code> object so that I can reference a code from artifact created from AWS Codebuild?<\/p>\n<p>If not, can I use codebuild to first copy my local file to a specific S3 position (I am having permission issue so far) and then run the pipeline?<\/p>\n<p>As your reference<\/p>\n<pre><code>...\nstep_data_ingest = ProcessingStep(\n        name=&quot;DataIngestion&quot;,\n        processor=sklearn_data_ingest_processor,\n        inputs=[\n            ProcessingInput(\n                input_name=&quot;input_train_data&quot;,\n                source=input_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/train&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;input_test_data&quot;,\n                source=test_data, \n                destination=&quot;\/opt\/ml\/processing\/input\/data\/test&quot;\n            ),\n            ProcessingInput(\n                input_name=&quot;requirement_file&quot;,\n                source=os.path.join(code_dir, &quot;requirements.txt&quot;), \n                destination=&quot;\/opt\/ml\/processing\/input\/requirement&quot;\n            ),\n        ],\n        outputs=[\n            ProcessingOutput(\n                output_name=&quot;train&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/train&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/train&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;validation&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/validation&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/validation&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;test&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/test&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/test&quot;)\n            ),\n            ProcessingOutput(\n                output_name=&quot;sample&quot;, \n                source=&quot;\/opt\/ml\/processing\/output\/sample&quot;,\n                destination=get_projection_s3_dir(experiment_dir, &quot;datasets\/sample&quot;)\n            ),\n        ],\n        code=os.path.join(code_dir, &quot;data_ingestion.py&quot;),\n        # something like s3:\/\/some_code_dir\/data_ingestion.py\n        job_arguments = [&quot;-c&quot;, country, \n                         &quot;-v&quot;, train_val_split_percentage],\n    )\n...\n<\/code><\/pre>\n<p>What I expect to do is something like:<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;data_ingestion.py&quot;\n    code_location=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdskz.zip&quot;\n\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in processing step or processor\nProcessingStep(\n    ...\n    code=&quot;s3:\/\/some_artifact_bucket\/buildartifact\/fdsix\/data_ingestion.py&quot;\n    ...\n)\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code># in buildspec.yml for codebuild\naws s3 sync .\/code_dir\/ s3:\/\/some_code_dir\/\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1615324569760,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":156.0,
        "Poster_id":12481696.0,
        "Poster_created_time":1575485255683,
        "Poster_last_access_time":1634441626670,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":3.0,
        "Poster_views":3.0,
        "Answer_body":"<p>When using the <code>ProcessingStep<\/code>, you can use an <code>S3 URI<\/code> as the code location, take a look on <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/9fc57555bba4fc1d33064478dc209a84a6726c57\/src\/sagemaker\/workflow\/steps.py#L374\" rel=\"nofollow noreferrer\">this<\/a> for reference.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1628026967547,
        "Answer_score":0.0,
        "Answerer_id":16247336.0,
        "Answerer_created_time":1623879163643,
        "Answerer_last_access_time":1663856103556,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":224.0,
        "Answerer_views":40.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66554893",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1627578025127,
        "Question_original_content":"workflow pipelin us code store artifact creat codebuild creat workflow pipelin pipelin object coupl process step try refer file path local file path won upload file everytim pipelin run question modifi step scriptprocessor pipelin object refer code artifact creat aw codebuild us codebuild copi local file specif posit have permiss issu far run pipelin refer step data ingest processingstep dataingest processor sklearn data ingest processor input processinginput input input train data sourc input data destin opt process input data train processinginput input input test data sourc test data destin opt process input data test processinginput input requir file sourc path join code dir requir txt destin opt process input requir output processingoutput output train sourc opt process output train destin project dir experi dir dataset train processingoutput output valid sourc opt process output valid destin project dir experi dir dataset valid processingoutput output test sourc opt process output test destin project dir experi dir dataset test processingoutput output sampl sourc opt process output sampl destin project dir experi dir dataset sampl code path join code dir data ingest like code dir data ingest job argument countri train val split percentag expect like process step processor processingstep code data ingest code locat artifact bucket buildartifact fdskz zip process step processor processingstep code artifact bucket buildartifact fdsix data ingest buildspec yml codebuild aw sync code dir code dir",
        "Question_preprocessed_content":"workflow pipelin us code store artifact creat codebuild creat object coupl process step try refer file path local file path won upload file everytim pipelin run question modifi object refer code artifact creat aw codebuild us codebuild copi local file specif posit run pipelin refer expect like",
        "Question_gpt_summary_original":"The user is facing challenges in referencing an S3 file path instead of a local file path in a sagemaker.workflow.pipeline.Pipeline object. They are trying to modify the step, scriptprocessor, or pipeline object to reference code from an artifact created from AWS Codebuild. They are also considering using Codebuild to copy their local file to a specific S3 position before running the pipeline, but are having permission issues. The user provides code examples of what they expect to do, including referencing code from an S3 artifact bucket or syncing code from a local directory to an S3 bucket using buildspec.yml.",
        "Question_gpt_summary":"user face challeng referenc file path instead local file path workflow pipelin pipelin object try modifi step scriptprocessor pipelin object refer code artifact creat aw codebuild consid codebuild copi local file specif posit run pipelin have permiss issu user provid code exampl expect includ referenc code artifact bucket sync code local directori bucket buildspec yml",
        "Answer_original_content":"processingstep us uri code locat look refer",
        "Answer_preprocessed_content":"us code locat look refer",
        "Answer_gpt_summary_original":"Solution: The user can use an S3 URI as the code location when using the ProcessingStep in the sagemaker.workflow.pipeline.Pipeline object. They can refer to the provided link for reference.",
        "Answer_gpt_summary":"solut user us uri code locat processingstep workflow pipelin pipelin object refer provid link refer"
    },
    {
        "Question_title":"Azure Machine Learning (AutoML) export data to SharePoint",
        "Question_body":"I am using Azure Machine Learning Studio to design pipelines to analyze data.\nIs there any possibility to export data to sharepoint?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1631064917827,
        "Question_favorite_count":13.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi @MiaZhangWHQWistron-2092\nPer my research, there is no way to export data from Azure Machine Learning Studio to SharePoint directly.\n\nAs an alternative, you could export data to Azure SQL database first:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/export-to-azure-sql-database\n\nThen export data from Azure SQL database to SharePoint list:\nhttps:\/\/social.technet.microsoft.com\/wiki\/contents\/articles\/39170.azure-sql-db-with-sharepoint-online-as-external-list-using-business-connectivity-services.aspx\n\n\nIf an Answer is helpful, please click \"Accept Answer\" and upvote it.\n\nNote: Please follow the steps in our documentation to enable e-mail notifications if you want to receive the related email notification for this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/543361\/azure-machine-learning-automl-export-data-to-share.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1631084842440,
        "Question_original_content":"automl export data sharepoint studio design pipelin analyz data possibl export data sharepoint",
        "Question_preprocessed_content":"export data sharepoint studio design pipelin analyz data possibl export data sharepoint",
        "Question_gpt_summary_original":"The user is facing a challenge in exporting data from Azure Machine Learning Studio to SharePoint.",
        "Question_gpt_summary":"user face challeng export data studio sharepoint",
        "Answer_original_content":"miazhangwhqwistron research wai export data studio sharepoint directli altern export data azur sql databas http doc microsoft com azur machin learn studio modul refer export azur sql databas export data azur sql databas sharepoint list http social technet microsoft com wiki content articl azur sql sharepoint onlin extern list busi connect servic aspx answer help click accept answer upvot note follow step document enabl mail notif want receiv relat email notif thread",
        "Answer_preprocessed_content":"research wai export data studio sharepoint directli altern export data azur sql databas export data azur sql databas sharepoint list answer help click accept answer upvot note follow step document enabl notif want receiv relat email notif thread",
        "Answer_gpt_summary_original":"Solution: There is no direct way to export data from Azure Machine Learning Studio to SharePoint. However, an alternative solution is to export data to Azure SQL database first and then export data from Azure SQL database to SharePoint list. The links to the documentation for both steps are provided in the discussion.",
        "Answer_gpt_summary":"solut direct wai export data studio sharepoint altern solut export data azur sql databas export data azur sql databas sharepoint list link document step provid discuss"
    },
    {
        "Question_title":"How to prevent a NoCredentialsError when calling the fit method in SageMaker?",
        "Question_body":"<p>I am a newbie when it comes to Python SageMaker (my background is C#). Currently, I have a problem because the last method call (I mean the fit method) results in a \"NoCredentialsError\". I do not understand that. The AWS credentials have been set and I do use them to communicate with AWS, for example to communicate with S3. How can I prevent this error? <\/p>\n\n<pre><code>import io\nimport os\nimport gzip\nimport pickle\nimport urllib.request\nimport boto3\nimport sagemaker\nimport sagemaker.amazon.common as smac\n\nDOWNLOADED_FILENAME = 'C:\/Users\/Daan\/PycharmProjects\/downloads\/mnist.pkl.gz'\nif not os.path.exists(DOWNLOADED_FILENAME):\n    urllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", DOWNLOADED_FILENAME)\n\nwith gzip.open(DOWNLOADED_FILENAME, 'rb') as f:\n    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\nvectors = train_set[0].T\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, vectors)\nbuf.seek(0)\nkey = 'recordio-pb-data'\nbucket_name = 'SOMEKINDOFBUCKETNAME'\nprefix = 'sagemaker\/pca'\npath = os.path.join(prefix, 'train', key)\nprint(path)\n\nsession = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\nregion='eu-west-1'\nsagemakerSession= sagemaker.Session(sagemaker_client=client,boto_session=session)\ns3_resource=session.resource('s3')\nbucket = s3_resource.Bucket(bucket_name)\ncurrent_bucket = bucket.Object(path)\n\ntrain_data = 's3:\/\/{}\/{}\/train\/{}'.format(bucket_name, prefix, key)\nprint('uploading training data location: {}'.format(train_data))\ncurrent_bucket.upload_fileobj(buf)\n\noutput_location = 's3:\/\/{}\/{}\/output'.format('SOMEBUCKETNAME', prefix)\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nregion='eu-west-1'\n\ncontainers = {'us-west-2': 'SOMELOCATION',\n              'us-east-1': 'SOMELOCATION',\n              'us-east-2': 'SOMELOCATION',\n              'eu-west-1': 'SOMELOCATION'}\ncontainer = containers[region]\n\nrole='AmazonSageMaker-ExecutionRole-SOMEVALUE'\npca = sagemaker.estimator.Estimator(container,\n                                    role,\n                                    train_instance_count=1,\n                                    train_instance_type='ml.c4.xlarge',\n                                    output_path=output_location,\n                                    sagemaker_session=sagemakerSession)\n\n\npca.set_hyperparameters(feature_dim=50000,\n                        num_components=10,\n                        subtract_mean=True,\n                        algorithm_mode='randomized',\n                        mini_batch_size=200)\n\npca.fit(inputs=train_data)\n\nprint('END')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1526393414957,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":447.0,
        "Poster_id":1987258.0,
        "Poster_created_time":1358429250663,
        "Poster_last_access_time":1663950242467,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":2120.0,
        "Poster_views":279.0,
        "Answer_body":"<p>I am not sure if you have masked the actual access id and key or this is what you are running.<\/p>\n<pre><code>session = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\n<\/code><\/pre>\n<p>I am hoping you are providing the actual aws_access_key_id and aws_secret_access_key in the above lines of code.<\/p>\n<p>Another way of specifying the same and not hardcoding in the code is to create a credentials file in your profile directory i.e.<\/p>\n<p>in Mac    ~\/.aws\/<\/p>\n<p>and in Windows <code>&quot;%UserProfile%\\.aws&quot;<\/code><\/p>\n<p>the file is a plain text file with a name &quot;credentials&quot; (without the quotes).\nfile contains<\/p>\n<pre><code>[default]\naws_access_key_id=XXXXXXXXXXXXXX\naws_secret_access_key=YYYYYYYYYYYYYYYYYYYYYYYYYYY\n<\/code><\/pre>\n<p>AWS CLI would pick it up from the above location and use it. You can also use non-default profiles and pass on the profile with<\/p>\n<pre><code>os.environ[&quot;AWS_PROFILE&quot;] = &quot;profile-name&quot;\n<\/code><\/pre>\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1636970707960,
        "Answer_score":2.0,
        "Answerer_id":6530653.0,
        "Answerer_created_time":1467237684900,
        "Answerer_last_access_time":1661100770547,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":613.0,
        "Answerer_views":39.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50352412",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1526405500043,
        "Question_original_content":"prevent nocredentialserror call fit method newbi come python background current problem method mean fit method result nocredentialserror understand aw credenti set us commun aw exampl commun prevent error import import import gzip import pickl import urllib request import boto import import amazon common smac download filenam user daan pycharmproject download mnist pkl path exist download filenam urllib request urlretriev http deeplearn net data mnist mnist pkl download filenam gzip open download filenam train set valid set test set pickl load encod latin vector train set buf bytesio smac write numpi dens tensor buf vector buf seek kei recordio data bucket somekindofbucketnam prefix pca path path join prefix train kei print path session boto session session aw access kei secret aw secret access kei secret region west client boto client region west aw access kei secret aw secret access kei secret region west session session client client boto session session resourc session resourc bucket resourc bucket bucket current bucket bucket object path train data train format bucket prefix kei print upload train data locat format train data current bucket upload fileobj buf output locat output format somebucketnam prefix print train artifact upload format output locat region west contain west someloc east someloc east someloc west someloc contain contain region role amazon executionrol somevalu pca estim estim contain role train instanc count train instanc type xlarg output path output locat session session pca set hyperparamet featur dim num compon subtract mean true algorithm mode random mini batch size pca fit input train data print end",
        "Question_preprocessed_content":"prevent nocredentialserror call fit method newbi come python current problem method result nocredentialserror understand aw credenti set us commun aw exampl commun prevent error",
        "Question_gpt_summary_original":"The user is encountering a \"NoCredentialsError\" when calling the fit method in SageMaker, despite having set the AWS credentials and using them to communicate with AWS. The user is seeking advice on how to prevent this error.",
        "Question_gpt_summary":"user encount nocredentialserror call fit method despit have set aw credenti commun aw user seek advic prevent error",
        "Answer_original_content":"sure mask actual access kei run session boto session session aw access kei secret aw secret access kei secret region west client boto client region west aw access kei secret aw secret access kei secret hope provid actual aw access kei aw secret access kei line code wai specifi hardcod code creat credenti file profil directori mac aw window userprofil aw file plain text file credenti quot file contain default aw access kei aw secret access kei aw cli pick locat us us non default profil pass profil environ aw profil profil hope help",
        "Answer_preprocessed_content":"sure mask actual access kei run hope provid actual line code wai specifi hardcod code creat credenti file profil directori mac window file plain text file credenti file contain aw cli pick locat us us profil pass profil hope help",
        "Answer_gpt_summary_original":"Solution:\n- The user can create a credentials file in their profile directory and specify the AWS access key and secret access key in it. The AWS CLI would pick it up from the specified location and use it. The file is a plain text file with a name \"credentials\" (without the quotes) and contains the access key and secret access key. The user can also use non-default profiles and pass on the profile with \"os.environ[\"AWS_PROFILE\"] = \"profile-name\"\".",
        "Answer_gpt_summary":"solut user creat credenti file profil directori specifi aw access kei secret access kei aw cli pick specifi locat us file plain text file credenti quot contain access kei secret access kei user us non default profil pass profil environ aw profil profil"
    },
    {
        "Question_title":"Unable to install toc2 notebook extension for AWS Sagemaker Instance (Lifecycle Configurations)",
        "Question_body":"<p>There's probably something very obvious I'm missing or Sagemaker just doesn't support these kinds of extensions, but I've been trying to enable toc2 (Table of Contents) jupyter extension for my Sagemaker notebook via lifecycle configurations, but for whatever reason it still isn't showing up.<\/p>\n\n<p>I built my script out combining a sample AWS script and a quick article on the usual ways of enabling extensions:<\/p>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-nb-extension\/on-start.sh<\/a><\/p>\n\n<p><a href=\"https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231\" rel=\"nofollow noreferrer\">https:\/\/towardsdatascience.com\/jupyter-notebook-extensions-517fa69d2231<\/a><\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;EOF\n\n--Activate notebook environment\nsource activate JupyterSystemEnv\n\n--Install extensions\npip install jupyter_contrib_nbextensions &amp;&amp; jupyter contrib\nnbextension install\njupyter nbextension enable toc2 --py --sys-prefix\n\nsource deactivate\n\n\nEOF\n<\/code><\/pre>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_created_time":1586792396013,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":1027.0,
        "Poster_id":6011629.0,
        "Poster_created_time":1456986606312,
        "Poster_last_access_time":1663952857883,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":757.0,
        "Poster_views":80.0,
        "Answer_body":"<p>Answering my question, looks like I was just missing the line <code>jupyter contrib nbextension install --user<\/code> to copy the JS\/CSS files into Jupyter's search directory and some config updates (<a href=\"https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions\" rel=\"nofollow noreferrer\">https:\/\/github.com\/ipython-contrib\/jupyter_contrib_nbextensions<\/a>).<\/p>\n<p>Corrected statement<\/p>\n<pre><code>#!\/bin\/bash\n\nset -e\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\nsource \/home\/ec2-user\/anaconda3\/bin\/activate JupyterSystemEnv\n\npip install jupyter_contrib_nbextensions\njupyter contrib nbextension install --user\njupyter nbextension enable toc2\/main\n\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\n\n\nEOF\n\n##Below may be unnecessary, but other user needed to run to see success\ninitctl restart jupyter-server --no-wait\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1615352840848,
        "Answer_score":2.0,
        "Answerer_id":6011629.0,
        "Answerer_created_time":1456986606312,
        "Answerer_last_access_time":1663952857883,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":757.0,
        "Answerer_views":80.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61191412",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1610748471692,
        "Question_original_content":"unabl instal toc notebook extens instanc lifecycl configur probabl obviou miss support kind extens try enabl toc tabl content jupyt extens notebook lifecycl configur reason isn show built script combin sampl aw script quick articl usual wai enabl extens http github com aw sampl amazon notebook instanc lifecycl config sampl blob master script instal extens start http towardsdatasci com jupyt notebook extens fad bin bash set sudo user eof activ notebook environ sourc activ jupytersystemenv instal extens pip instal jupyt contrib nbextens jupyt contrib nbextens instal jupyt nbextens enabl toc sy prefix sourc deactiv eof thank",
        "Question_preprocessed_content":"unabl instal toc notebook extens instanc probabl obviou miss support kind extens try enabl toc jupyt extens notebook lifecycl configur reason isn show built script combin sampl aw script quick articl usual wai enabl extens thank",
        "Question_gpt_summary_original":"The user is facing challenges in enabling the toc2 (Table of Contents) Jupyter extension for their AWS Sagemaker notebook via lifecycle configurations. They have tried to enable the extension using a combination of a sample AWS script and a quick article on enabling extensions, but it still isn't showing up.",
        "Question_gpt_summary":"user face challeng enabl toc tabl content jupyt extens notebook lifecycl configur tri enabl extens combin sampl aw script quick articl enabl extens isn show",
        "Answer_original_content":"answer question look like miss line jupyt contrib nbextens instal user copi css file jupyt search directori config updat http github com ipython contrib jupyt contrib nbextens correct statement bin bash set sudo user eof sourc home user anaconda bin activ jupytersystemenv pip instal jupyt contrib nbextens jupyt contrib nbextens instal user jupyt nbextens enabl toc main sourc home user anaconda bin deactiv eof unnecessari user need run success initctl restart jupyt server wait",
        "Answer_preprocessed_content":"answer question look like miss line copi file jupyt search directori config updat correct statement",
        "Answer_gpt_summary_original":"Solution:\nThe user was missing the line \"jupyter contrib nbextension install --user\" to copy the JS\/CSS files into Jupyter's search directory and some config updates. By adding this line to the script and running it, the toc2 Jupyter extension was successfully enabled for their AWS Sagemaker notebook.",
        "Answer_gpt_summary":"solut user miss line jupyt contrib nbextens instal user copi css file jupyt search directori config updat ad line script run toc jupyt extens successfulli enabl notebook"
    },
    {
        "Question_title":"How do you see ALL predictors by influence not just the top predictors of AutoML training reports?",
        "Question_body":"The \"top predictors by influence\" in the training reports of AutoML regression models is very useful (see reference image), but I'm looking for a way to display all of the predictors, not just the top 10. Any way I can visualise this either in the training report or using the data tables themselves would be very useful.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1635159990897,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi, PowerBi is not currently supported here on Q&A. Please post your question on the PowerBI community forum for faster response. Thanks.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/602784\/how-do-you-see-all-predictors-by-influence-not-jus.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1635215186563,
        "Question_original_content":"predictor influenc predictor automl train report predictor influenc train report automl regress model us refer imag look wai displai predictor wai visualis train report data tabl us",
        "Question_preprocessed_content":"predictor influenc predictor automl train report predictor influenc train report automl regress model us look wai displai predictor wai visualis train report data tabl us",
        "Question_gpt_summary_original":"The user is facing a challenge with the AutoML regression model's training reports, which only display the top 10 predictors by influence. The user is looking for a way to visualize all predictors, and is seeking a solution either within the training report or using the data tables themselves.",
        "Question_gpt_summary":"user face challeng automl regress model train report displai predictor influenc user look wai visual predictor seek solut train report data tabl",
        "Answer_original_content":"powerbi current support post question powerbi commun forum faster respons thank kindli accept answer inform help thank",
        "Answer_preprocessed_content":"powerbi current support post question powerbi commun forum faster respons thank kindli accept answer inform help thank",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Azure OpenAI service capabilities",
        "Question_body":"How do I get access to the Azure OpenAI service to evaluate it's capabilities?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1663989341807,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@Srin-4824 Thanks for the question. It is a Limited Access service so you have to apply for it https:\/\/aka.ms\/oai\/access",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1021561\/azure-openai-service-capabilities.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1664001167767,
        "Question_original_content":"azur openai servic capabl access azur openai servic evalu capabl",
        "Question_preprocessed_content":"azur openai servic capabl access azur openai servic evalu capabl",
        "Question_gpt_summary_original":"The user is facing the challenge of accessing the Azure OpenAI service to evaluate its capabilities.",
        "Question_gpt_summary":"user face challeng access azur openai servic evalu capabl",
        "Answer_original_content":"srin thank question limit access servic appli http aka oai access",
        "Answer_preprocessed_content":"thank question limit access servic appli",
        "Answer_gpt_summary_original":"Solution: The user can apply for access to the Azure OpenAI service through the provided link.",
        "Answer_gpt_summary":"solut user appli access azur openai servic provid link"
    },
    {
        "Question_title":"Return confidence score with custom model for Vertex AI batch predictions",
        "Question_body":"<p>I uploaded a pretrained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples. It just returned a list of false predictions with no confidence score. I don't see anywhere in the SDK documentation or Google console for how to get batch predictions to include the confidence scores. Is that something Vertex AI can do?<\/p>\n<p>My intent is to automate a batch prediction pipeline using the following code.<\/p>\n<pre><code># Predict\n# &quot;csv&quot;, &quot;&quot;bigquery&quot;, &quot;tf-record&quot;, &quot;tf-record-gzip&quot;, or &quot;file-list&quot;\nbatch_prediction_job = model.batch_predict(\n    job_display_name = job_display_name,\n    gcs_source = input_path,\n    instances_format = &quot;&quot;, # jsonl, csv, bigquery, \n    gcs_destination_prefix = output_path,\n    starting_replica_count = 1,\n    max_replica_count = 10,\n    sync = True,\n)\n\nbatch_prediction_job.wait()\n\nreturn batch_prediction_job.resource_name\n<\/code><\/pre>\n<p>I tried it out in google console as a test to make sure my input data was properly formatted.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_created_time":1637604275697,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1637686316647,
        "Question_score":0,
        "Question_view_count":319.0,
        "Poster_id":4296426.0,
        "Poster_created_time":1417013182680,
        "Poster_last_access_time":1663940014888,
        "Poster_age":null,
        "Poster_location":"Boston, MA",
        "Poster_reputation":1256.0,
        "Poster_views":245.0,
        "Answer_body":"<p>I don't think so; the stock sklearn container provided by vertex doesn't provide such a score I guess. You might need to write a <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/use-custom-container\" rel=\"nofollow noreferrer\">custom container<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":165835.0,
        "Answerer_created_time":1251699930780,
        "Answerer_last_access_time":1663861305950,
        "Answerer_age":null,
        "Answerer_location":"Santa Clara, CA, United States",
        "Answerer_reputation":1538.0,
        "Answerer_views":198.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70070421",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1637647501512,
        "Question_original_content":"return confid score custom model batch predict upload pretrain scikit learn classif model ran batch predict sampl return list fals predict confid score sdk document googl consol batch predict includ confid score intent autom batch predict pipelin follow code predict csv bigqueri record record gzip file list batch predict job model batch predict job displai job displai gc sourc input path instanc format jsonl csv bigqueri gc destin prefix output path start replica count max replica count sync true batch predict job wait return batch predict job resourc tri googl consol test sure input data properli format",
        "Question_preprocessed_content":"return confid score custom model batch predict upload pretrain scikit learn classif model ran batch predict sampl return list fals predict confid score sdk document googl consol batch predict includ confid score intent autom batch predict pipelin follow code tri googl consol test sure input data properli format",
        "Question_gpt_summary_original":"The user uploaded a pre-trained scikit learn classification model to Vertex AI and ran a batch prediction on 5 samples, but the predictions returned were false with no confidence score. The user is unable to find any documentation or information on how to get batch predictions to include confidence scores. The user's intent is to automate a batch prediction pipeline using the provided code.",
        "Question_gpt_summary":"user upload pre train scikit learn classif model ran batch predict sampl predict return fals confid score user unabl document inform batch predict includ confid score user intent autom batch predict pipelin provid code",
        "Answer_original_content":"think stock sklearn contain provid vertex provid score guess need write custom contain",
        "Answer_preprocessed_content":"think stock sklearn contain provid vertex provid score guess need write custom contain",
        "Answer_gpt_summary_original":"Solution: The only solution mentioned in the discussion is to write a custom container to include confidence scores in batch predictions as the stock sklearn container provided by Vertex AI does not provide such a score.",
        "Answer_gpt_summary":"solut solut mention discuss write custom contain includ confid score batch predict stock sklearn contain provid provid score"
    },
    {
        "Question_title":"How to retrieve the model signature from the MLflow Model Registry",
        "Question_body":"<p>I have registered a scikit learn model on my MLflow Tracking server, and I am loading it with <code>sklearn.load_model(model_uri)<\/code>.<\/p>\n<p>Now, I would like to access the signature of the model so I can get a list of the model's required inputs\/features so I can retrieve them from my feature store by name. I can't seem to find any utility or method in the <code>mlflow<\/code> API or the <code>MLFlowClient<\/code> API that will let me access a signature or inputs\/outputs attribute, even though I can see a list of inputs and outputs under each version of the model in the UI.<\/p>\n<p>I know that I can find the input sample and the model configuration in the model's artifacts, but that would require me actually downloading the artifacts and loading them manually in my script. I don't need to avoid that, but I am surprised that I can't just return the signature as a dictionary the same way I can return a run's parameters or metrics.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1643655088920,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":4,
        "Question_view_count":904.0,
        "Poster_id":6480757.0,
        "Poster_created_time":1466188731112,
        "Poster_last_access_time":1663868994303,
        "Poster_age":null,
        "Poster_location":"Michigan",
        "Poster_reputation":414.0,
        "Poster_views":39.0,
        "Answer_body":"<p>The way to access the model's signature without downloading the MLModel file is under the loaded model. And then you'll access the model's attributes, such as its signature or even other Pyfunc-defined methods.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\n\nmodel = mlflow.pyfunc.load_model(&quot;runs:\/&lt;run_id&gt;\/model&quot;)\nprint(model._model_meta._signature)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":8830597.0,
        "Answerer_created_time":1508924024027,
        "Answerer_last_access_time":1663957908303,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":118.0,
        "Answerer_views":17.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70931309",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1645469817663,
        "Question_original_content":"retriev model signatur model registri regist scikit learn model track server load sklearn load model model uri like access signatur model list model requir input featur retriev featur store util method api client api let access signatur input output attribut list input output version model know input sampl model configur model artifact requir actual download artifact load manual script need avoid surpris return signatur dictionari wai return run paramet metric",
        "Question_preprocessed_content":"retriev model signatur model registri regist scikit learn model track server load like access signatur model list model requir retriev featur store util method api api let access signatur attribut list input output version model know input sampl model configur model artifact requir actual download artifact load manual script need avoid surpris return signatur dictionari wai return run paramet metric",
        "Question_gpt_summary_original":"The user is facing a challenge in retrieving the model signature from the MLflow Model Registry. They have registered a scikit learn model on their MLflow Tracking server and loaded it, but they cannot find any utility or method in the mlflow API or the MLFlowClient API that will let them access a signature or inputs\/outputs attribute. Although they can see a list of inputs and outputs under each version of the model in the UI, they need to access the signature to get a list of the model's required inputs\/features so they can retrieve them from their feature store by name. The user knows that they can find the input sample and the model configuration in the model's artifacts, but that would require them to download the artifacts and load them manually in their script.",
        "Question_gpt_summary":"user face challeng retriev model signatur model registri regist scikit learn model track server load util method api client api let access signatur input output attribut list input output version model need access signatur list model requir input featur retriev featur store user know input sampl model configur model artifact requir download artifact load manual script",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"How to assign two or more time series identifier columns in Vertex AI Tabular Forecasting",
        "Question_body":"<p>I was wondering if it is possible to have more than one time series identifier column in the model? Let's assume I'd like to create a forecast at a product and store level (which the documentation suggests should be possible).<\/p>\n<p>If I select product as the series identifier, the only options I have left for store is either a covariate or an attribute and neither is applicable in this scenario.<\/p>\n<p>Would concatenating product and store and using the individual product and store code values for that concatenated ID as attributes be a solution? It doesn't feel right, but I can't see any other option - am I missing something?<\/p>\n<p>Note: I understand that this feature of Vertex AI is currently in preview and that because of that the options may be limited.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1632228068913,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":269.0,
        "Poster_id":9411953.0,
        "Poster_created_time":1519630645620,
        "Poster_last_access_time":1663095146407,
        "Poster_age":null,
        "Poster_location":"Northampton, UK",
        "Poster_reputation":333.0,
        "Poster_views":46.0,
        "Answer_body":"<p>There isn't an alternate way to assign 2 or more <strong>Time Series Identifiers<\/strong> in the <strong>Forecasting Model<\/strong> on <strong>Vertex AI<\/strong>. The &quot;<strong>Forecasting model<\/strong>&quot; is in the &quot;<strong>Preview<\/strong>&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a>, as you are aware, with all consequences of that fact the options are limited. Please refer to this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">doc<\/a> for more information about the best practices for data preparation to train the forecasting model.<\/p>\n<p>As a workaround, the two columns can be concatenated and assigned a Time Series Identifier on that concatenated column, as you have mentioned in the question. This way, the concatenated column carries more contextual information into the training of the model.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1632482492492,
        "Answer_score":2.0,
        "Answerer_id":15747106.0,
        "Answerer_created_time":1619174589310,
        "Answerer_last_access_time":1664047335000,
        "Answerer_age":null,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation":1200.0,
        "Answerer_views":804.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69269073",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1632376096768,
        "Question_original_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level document suggest possibl select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_preprocessed_content":"assign time seri identifi column tabular forecast wonder possibl time seri identifi column model let assum like creat forecast product store level select product seri identifi option left store covari attribut applic scenario concaten product store individu product store code valu concaten attribut solut feel right option miss note understand featur current preview option limit",
        "Question_gpt_summary_original":"The user is facing a challenge in assigning two or more time series identifier columns in Vertex AI Tabular Forecasting. They want to create a forecast at a product and store level, but the only options available for store are either a covariate or an attribute, which are not applicable in this scenario. The user is considering concatenating product and store and using the individual product and store code values for that concatenated ID as attributes, but they are unsure if this is the right solution. The user acknowledges that this feature is currently in preview and the options may be limited.",
        "Question_gpt_summary":"user face challeng assign time seri identifi column tabular forecast want creat forecast product store level option avail store covari attribut applic scenario user consid concaten product store individu product store code valu concaten attribut unsur right solut user acknowledg featur current preview option limit",
        "Answer_original_content":"isn altern wai assign time seri identifi forecast model forecast model preview product launch stage awar consequ fact option limit refer doc inform best practic data prepar train forecast model workaround column concaten assign time seri identifi concaten column mention question wai concaten column carri contextu inform train model",
        "Answer_preprocessed_content":"isn altern wai assign time seri identifi forecast model forecast model preview product launch stage awar consequ fact option limit refer doc inform best practic data prepar train forecast model workaround column concaten assign time seri identifi concaten column mention question wai concaten column carri contextu inform train model",
        "Answer_gpt_summary_original":"Solution: The only solution mentioned in the discussion is to concatenate the two columns and assign a Time Series Identifier on that concatenated column. This workaround can carry more contextual information into the training of the model. However, it is acknowledged that this feature is currently in preview and the options may be limited.",
        "Answer_gpt_summary":"solut solut mention discuss concaten column assign time seri identifi concaten column workaround carri contextu inform train model acknowledg featur current preview option limit"
    },
    {
        "Question_title":"How to create Parallel Coordinates plot without sweeps",
        "Question_body":"<p>It would be great to plot hparams without doing sweeps, most of the time I\u2019m doing experiments and I would love the plot to be across runs and not as a sweep. It might be complex to make this feature automated, but I\u2019m fine if it\u2019s within one run, would be great to have something like <code>wandb.plots.ParallelCoordinates<\/code><\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1671394517195,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":146.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Hi Faris!<\/p>\n<p>You absolutely can use Parallel coordinates plots without sweeps. The web UI has an option to add additional plots on the top right of the graph section which contains the Parallel Coordinates Plot.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-create-parallel-coordinates-plot-without-sweeps\/3566",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1671425374288,
        "Question_original_content":"creat parallel coordin plot sweep great plot hparam sweep time experi love plot run sweep complex featur autom fine run great like plot parallelcoordin",
        "Question_preprocessed_content":"creat parallel coordin plot sweep great plot hparam sweep time experi love plot run sweep complex featur autom fine run great like",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a Parallel Coordinates plot without doing sweeps. They would like to plot hparams across runs and not as a sweep, but find it complex to make this feature automated. They suggest having something like \"wandb.plots.ParallelCoordinates\" within one run.",
        "Question_gpt_summary":"user face challeng creat parallel coordin plot sweep like plot hparam run sweep complex featur autom suggest have like plot parallelcoordin run",
        "Answer_original_content":"fari absolut us parallel coordin plot sweep web option add addit plot right graph section contain parallel coordin plot thank ramit",
        "Answer_preprocessed_content":"fari absolut us parallel coordin plot sweep web option add addit plot right graph section contain parallel coordin plot thank ramit",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to use the web UI option to add additional plots on the top right of the graph section which contains the Parallel Coordinates Plot. This allows the user to plot hparams across runs without doing sweeps.",
        "Answer_gpt_summary":"solut solut provid discuss us web option add addit plot right graph section contain parallel coordin plot allow user plot hparam run sweep"
    },
    {
        "Question_title":"How to return all labels and scores in SageMaker Inference?",
        "Question_body":"<p>I am using the <code>HuggingFacePredictor<\/code> from <code>sagemaker.huggingface<\/code> to inference some text and I would like to get all label scores.<\/p>\n<p>Is there any way of getting, as response from the endpoint:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;labels&quot;: [&quot;help&quot;, &quot;Greeting&quot;, &quot;Farewell&quot;] ,\n    &quot;score&quot;: [0.81, 0.1, 0.09],\n}\n<\/code><\/pre>\n<p>(or similar)<\/p>\n<p>Instead of:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;label&quot;: &quot;help&quot;,\n    &quot;score&quot;: 0.81,\n}\n<\/code><\/pre>\n<p>Here is some example code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nfrom sagemaker.huggingface import HuggingFacePredictor\nfrom sagemaker.session import Session\n\nsagemaker_session = Session(boto_session=boto3.session.Session())\n\npredictor = HuggingFacePredictor(\n    endpoint_name=project, sagemaker_session=sagemaker_session\n)\nprediciton = predictor.predict({&quot;inputs&quot;: text})[0]\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1643804083130,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1643805132912,
        "Question_score":1,
        "Question_view_count":193.0,
        "Poster_id":17041637.0,
        "Poster_created_time":1632991357332,
        "Poster_last_access_time":1663942581167,
        "Poster_age":null,
        "Poster_location":"Barcelona",
        "Poster_reputation":373.0,
        "Poster_views":17.0,
        "Answer_body":"<p>With your current code sample, it is not quite clear what specific task you are performing, but for the sake of this answer, I'll assume you're doing text classification.<\/p>\n<p>Most importantly, though, we can read the following in <a href=\"https:\/\/huggingface.co\/docs\/sagemaker\/reference#inference-toolkit-api\" rel=\"nofollow noreferrer\">Huggingface's Sagemaker reference document<\/a> (bold highlight by me):<\/p>\n<blockquote>\n<p>The Inference Toolkit accepts inputs in the inputs key, and <strong>supports additional <code>pipelines<\/code> parameters in the <code>parameters<\/code> key<\/strong>. You can provide any of the supported <code>kwargs<\/code> from <code>pipelines<\/code> as parameters.<\/p>\n<\/blockquote>\n<p>If we check out the <a href=\"https:\/\/huggingface.co\/docs\/transformers\/v4.16.2\/en\/main_classes\/pipelines#transformers.TextClassificationPipeline.__call__\" rel=\"nofollow noreferrer\">accepted arguments by the <code>TextClassificationPipeline<\/code><\/a>, we can see that there is indeed one that returns all samples:<\/p>\n<blockquote>\n<p><code>return_all_scores<\/code> (bool, optional, defaults to False) \u2014 Whether to return scores for all labels.<\/p>\n<\/blockquote>\n<p>While I unfortunately don't have access to Sagemaker inference, I can run a sample to illustrate the output with a local pipeline:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from transformers import pipeline\n# uses 2-way sentiment classification model per default\npipe = pipeline(&quot;text-classification&quot;) \n\npipe(&quot;I am really angry right now &gt;:(&quot;, return_all_scores=True)\n# Output: [[{'label': 'NEGATIVE', 'score': 0.9989138841629028},\n#           {'label': 'POSITIVE', 'score': 0.0010860705515369773}]]\n<\/code><\/pre>\n<p>Based on the slightly different input format expected by Sagemaker, coupled with the example given in <a href=\"https:\/\/github.com\/huggingface\/notebooks\/blob\/master\/sagemaker\/10_deploy_model_from_s3\/deploy_transformer_model_from_s3.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>, I would assume that a corrected input in your own example code should look like this:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;inputs&quot;: text,\n    &quot;parameters&quot;: {&quot;return_all_scores&quot;: True}\n}\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1643813773008,
        "Answer_score":1.0,
        "Answerer_id":3607203.0,
        "Answerer_created_time":1399363600132,
        "Answerer_last_access_time":1663586706736,
        "Answerer_age":null,
        "Answerer_location":"Heidelberg, Germany",
        "Answerer_reputation":8423.0,
        "Answerer_views":1313.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70955450",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1643809235227,
        "Question_original_content":"return label score infer huggingfacepredictor huggingfac infer text like label score wai get respons endpoint label help greet farewel score similar instead label help score exampl code import boto huggingfac import huggingfacepredictor session import session session session boto session boto session session predictor huggingfacepredictor endpoint project session session prediciton predictor predict input text",
        "Question_preprocessed_content":"return label score infer infer text like label score wai get respons endpoint similar instead exampl code",
        "Question_gpt_summary_original":"The user is using HuggingFacePredictor from sagemaker.huggingface to inference some text and wants to get all label scores instead of just one label and score. The user is seeking a way to get a response from the endpoint that includes all labels and scores, rather than just one label and score.",
        "Question_gpt_summary":"user huggingfacepredictor huggingfac infer text want label score instead label score user seek wai respons endpoint includ label score label score",
        "Answer_original_content":"current code sampl clear specif task perform sake answer assum text classif importantli read follow huggingfac refer document bold highlight infer toolkit accept input input kei support addit pipelin paramet paramet kei provid support kwarg pipelin paramet check accept argument textclassificationpipelin return sampl return score bool option default fals return score label unfortun access infer run sampl illustr output local pipelin transform import pipelin us wai sentiment classif model default pipe pipelin text classif pipe angri right return score true output label neg score label posit score base slightli differ input format expect coupl exampl given notebook assum correct input exampl code look like input text paramet return score true",
        "Answer_preprocessed_content":"current code sampl clear specif task perform sake answer assum text classif importantli read follow huggingfac refer document infer toolkit accept input input kei support addit paramet kei provid support paramet check accept argument return sampl return score label unfortun access infer run sampl illustr output local pipelin base slightli differ input format expect coupl exampl given notebook assum correct input exampl code look like",
        "Answer_gpt_summary_original":"Solution: The Huggingface's Sagemaker reference document supports additional pipelines parameters in the parameters key. The TextClassificationPipeline has an argument called return_all_scores that returns scores for all labels. The user can set return_all_scores to True in the parameters key to get all label scores instead of just one label and score.",
        "Answer_gpt_summary":"solut huggingfac refer document support addit pipelin paramet paramet kei textclassificationpipelin argument call return score return score label user set return score true paramet kei label score instead label score"
    },
    {
        "Question_title":"Can't find the run button",
        "Question_body":"Do I need to add the .ipynb extension manually to my notebook file ?\nI can't find the run button.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1617852662423,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@paulgureghian-9874 You can re-name your file with .ipynb extension which should help to display the cells and the available options like run button for cell. Usually while creating new files in your workspace the UI prompts to select the extension of the file. If you can create a new file with .ipynb extension and copy these cells individually that should also work. Thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/348777\/can39t-find-the-run-button.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1617872811416,
        "Question_original_content":"run button need add ipynb extens manual notebook file run button",
        "Question_preprocessed_content":"run button need add ipynb extens manual notebook file run button",
        "Question_gpt_summary_original":"The user is facing a challenge in finding the run button and is unsure if they need to manually add the .ipynb extension to their notebook file.",
        "Question_gpt_summary":"user face challeng find run button unsur need manual add ipynb extens notebook file",
        "Answer_original_content":"paulgureghian file ipynb extens help displai cell avail option like run button cell usual creat new file workspac prompt select extens file creat new file ipynb extens copi cell individu work thank",
        "Answer_preprocessed_content":"file ipynb extens help displai cell avail option like run button cell usual creat new file workspac prompt select extens file creat new file ipynb extens copi cell individu work thank",
        "Answer_gpt_summary_original":"Solution: The user can manually add the .ipynb extension to their notebook file by renaming it. This should help to display the cells and the available options like the run button for the cell. Alternatively, the user can create a new file with .ipynb extension and copy the cells individually.",
        "Answer_gpt_summary":"solut user manual add ipynb extens notebook file renam help displai cell avail option like run button cell altern user creat new file ipynb extens copi cell individu"
    },
    {
        "Question_title":"Terraform - Optional Nested Variable",
        "Question_body":"<p>I'm trying to create a module for Sagemaker endpoints. There's an optional object variable called <code>async_inference_config<\/code>. If you omit it, the endpoint being deployed is synchronous, but if you include it, the endpoint deployed is asynchronous. To satisfy both of these usecases, the <code>async_inference_config<\/code> needs to be an optional block.<\/p>\n<p>I am unsure of how to make this block optional though.<br \/>\nAny guidance would be greatly appreciated. See example below of structure of the optional parameter.<\/p>\n<p><strong>Example:<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_endpoint_configuration&quot; &quot;sagemaker_endpoint_configuration&quot; {\n  count = var.create ? 1 : 0\n\n  name = var.endpoint_configuration_name\n  production_variants {\n    instance_type          = var.instance_type\n    initial_instance_count = var.instance_count\n    model_name             = var.model_name\n    variant_name           = var.variant_name\n  }\n  async_inference_config {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n  lifecycle {\n    create_before_destroy = true\n    ignore_changes        = [&quot;name&quot;]\n  }\n\n  tags = var.tags\n\n  depends_on = [aws_sagemaker_model.sagemaker_model]\n}\n<\/code><\/pre>\n<p><strong>Update:<\/strong> What I tried based on the below suggestion, which seemed to work<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.async_inference_config == null ? [] : [true]\n    content {\n      output_config {\n        s3_output_path = lookup(var.async_inference_config, &quot;s3_output_path&quot;, null)\n      }\n      client_config {\n        max_concurrent_invocations_per_instance = lookup(var.async_inference_config, &quot;max_concurrent_invocations_per_instance&quot;, null)\n      }\n    }\n  }\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658386596203,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1658442017223,
        "Question_score":0,
        "Question_view_count":55.0,
        "Poster_id":5378132.0,
        "Poster_created_time":1443225809767,
        "Poster_last_access_time":1663898334270,
        "Poster_age":null,
        "Poster_location":"Vancouver, BC, Canada",
        "Poster_reputation":2332.0,
        "Poster_views":560.0,
        "Answer_body":"<p>You could use a <code>dynamic<\/code> block [1] in combination with <code>for_each<\/code> meta-argument [2]. It would look something like:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.s3_output_path != null &amp;&amp; var.max_concurrent_invocations_per_instance != null ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<p>Of course, you could come up with a different variable, say <code>enable_async_inference_config<\/code> (probalby of type <code>bool<\/code>) and base the <code>for_each<\/code> on that, e.g.:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.enable_async_inference_config ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks<\/a><\/p>\n<p>[2] <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/for_each\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/meta-arguments\/for_each<\/a><\/p>",
        "Answer_comment_count":13.0,
        "Answer_last_edit_time":1658923699476,
        "Answer_score":2.0,
        "Answerer_id":8343484.0,
        "Answerer_created_time":1500629225150,
        "Answerer_last_access_time":1664055649380,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":5939.0,
        "Answerer_views":886.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73061907",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1658387166296,
        "Question_original_content":"terraform option nest variabl try creat modul endpoint option object variabl call async infer config omit endpoint deploi synchron includ endpoint deploi asynchron satisfi usecas async infer config need option block unsur block option guidanc greatli appreci exampl structur option paramet exampl resourc aw endpoint configur endpoint configur count var creat var endpoint configur product variant instanc type var instanc type initi instanc count var instanc count model var model variant var variant async infer config output config output path var output path client config max concurr invoc instanc var max concurr invoc instanc lifecycl creat destroi true ignor chang tag var tag depend aw model model updat tri base suggest work dynam async infer config var async infer config null true content output config output path lookup var async infer config output path null client config max concurr invoc instanc lookup var async infer config max concurr invoc instanc null",
        "Question_preprocessed_content":"terraform option nest variabl try creat modul endpoint option object variabl call omit endpoint deploi synchron includ endpoint deploi asynchron satisfi usecas need option block unsur block option guidanc greatli appreci exampl structur option paramet exampl updat tri base suggest work",
        "Question_gpt_summary_original":"The user is trying to create a module for Sagemaker endpoints, and is facing a challenge with an optional object variable called \"async_inference_config\". If the variable is omitted, the endpoint being deployed is synchronous, but if it is included, the endpoint deployed is asynchronous. The user needs to make this block optional, but is unsure of how to do so. The user has provided an example of the structure of the optional parameter and has also shared an update on what they have tried based on a suggestion.",
        "Question_gpt_summary":"user try creat modul endpoint face challeng option object variabl call async infer config variabl omit endpoint deploi synchron includ endpoint deploi asynchron user need block option unsur user provid exampl structur option paramet share updat tri base suggest",
        "Answer_original_content":"us dynam block combin meta argument look like dynam async infer config var output path null var max concurr invoc instanc null content output config output path var output path client config max concurr invoc instanc var max concurr invoc instanc cours come differ variabl enabl async infer config probalbi type bool base dynam async infer config var enabl async infer config content output config output path var output path client config max concurr invoc instanc var max concurr invoc instanc http terraform languag express dynam block http terraform languag meta argument",
        "Answer_preprocessed_content":"us block combin look like cours come differ variabl base",
        "Answer_gpt_summary_original":"The solution suggested in the discussion is to use a dynamic block in combination with the for_each meta-argument. The user can create a dynamic block for the optional object variable and use the for_each meta-argument to make it optional. The user can also create a different variable and base the for_each on that.",
        "Answer_gpt_summary":"solut suggest discuss us dynam block combin meta argument user creat dynam block option object variabl us meta argument option user creat differ variabl base"
    },
    {
        "Question_title":"[problem at MMS predict] At MMS(sagemaker), error code(500), type(InternalServerException)",
        "Question_body":"I make pytorch model with sagemaker, MMS. This is my mms code.\n\n%%time\ninstance_type = 'c5.large'\n# accelerator_type = 'eia2.medium'\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=f\"ml.{instance_type}\"\n)\n\nmme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\")\nlist(mme.list_models())\n#> [ 'model.tar.gz']\n\nI try to predict with this code.\n\nstart_time = time.time()\npredicted_value = predictor.predict(requests, target_model=\"LV1\")\nduration = time.time() - start_time\nprint(\"${:,.2f}, took {:,d} ms\\n\".format(predicted_value[0], int(duration * 1000)))\n\nAnd, return error message.\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers\"\n}\n\n\nMMS with pytorch is 'little' difficult. X)\n\nhelp me, please.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1660209790025,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":80.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi , I think your target model on the prediction needs to have the name of the model you have deployed - for example , when you are adding the model with mme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\") the model_data_path contains the name of the model . From the sagemaker-examples: (https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb) **model_data_path is the relative path to the S3 prefix we specified above (i.e. model_data_prefix) where our endpoint will source models for inference requests.Since this is a relative path, we can simply pass the name of what we wish to call the model artifact at inference time (i.e. Chicago_IL.tar.gz). In your case \"model.tar.gz\". However, when predicting you call the model ,target_model=\"LV1\"?",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBCxtcfyrTymZ7isHG3X5Qg\/problem-at-mms-predict-at-mms-sagemaker-error-code-500-type-internal-server-exception",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1660312923532,
        "Question_original_content":"problem mm predict mm error code type internalserverexcept pytorch model mm mm code time instanc type larg acceler type eia medium predictor mme deploi initi instanc count instanc type instanc type mme add model model data sourc model path model data path model tar list mme list model model tar try predict code start time time time predict valu predictor predict request target model durat time time start time print took format predict valu int durat return error messag modelerror error occur modelerror call invokeendpoint oper receiv server error model messag code type internalserverexcept messag fail start worker mm pytorch littl difficult help",
        "Question_preprocessed_content":"mm error code type pytorch model mm mm code time predictor try predict code durat took int return error messag modelerror error occur call invokeendpoint oper receiv server error model messag code type internalserverexcept messag fail start worker mm pytorch littl difficult help",
        "Question_gpt_summary_original":"The user encountered an error code 500 with type InternalServerException while trying to predict with a pytorch model using MMS on Sagemaker. The error message stated that the workers failed to start, and the user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error code type internalserverexcept try predict pytorch model mm error messag state worker fail start user seek help resolv issu",
        "Answer_original_content":"think target model predict need model deploi exampl ad model mme add model model data sourc model path model data path model tar model data path contain model exampl http github com aw amazon exampl blob main advanc function multi model xgboost home valu xgboost multi model endpoint home valu ipynb model data path rel path prefix specifi model data prefix endpoint sourc model infer request rel path simpli pass wish model artifact infer time chicago tar case model tar predict model target model",
        "Answer_preprocessed_content":"think target model predict need model deploi exampl ad model contain model exampl rel path prefix specifi endpoint sourc model infer rel path simpli pass wish model artifact infer time case predict model",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to ensure that the target model on the prediction has the name of the model that has been deployed. The model_data_path should contain the name of the model, and when predicting, the model should be called with the correct target_model name.",
        "Answer_gpt_summary":"solut possibl solut mention discuss ensur target model predict model deploi model data path contain model predict model call correct target model"
    },
    {
        "Question_title":"How to generate Recordio from Java object",
        "Question_body":"<p>I am trying to serialize a list of java Objects (POJO) into RecordIO format. I have seen this BeanIO (<a href=\"http:\/\/beanio.org\/\" rel=\"nofollow noreferrer\">http:\/\/beanio.org\/<\/a>) but it seems to be outdated. Is there any other Java library that could be used or a different way to do this ?<\/p>\n\n<p>Once list of objects is serialized it will be used to train a model with SageMaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1526314257120,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":160.0,
        "Poster_id":4761371.0,
        "Poster_created_time":1428454496052,
        "Poster_last_access_time":1570063575876,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":35.0,
        "Poster_views":10.0,
        "Answer_body":"<p>Solving my own problem. I decided to use Apache Avro instead of BeanIO. Spark allow to serialize using Avro (c.f. Spark-Avro). This seems to work however it did not fit my use case has I was trying to serialize an array of numbers.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":4761371.0,
        "Answerer_created_time":1428454496052,
        "Answerer_last_access_time":1570063575876,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":35.0,
        "Answerer_views":10.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50334735",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1526652887110,
        "Question_original_content":"gener recordio java object try serial list java object pojo recordio format seen beanio http beanio org outdat java librari differ wai list object serial train model",
        "Question_preprocessed_content":"gener recordio java object try serial list java object recordio format seen beanio outdat java librari differ wai list object serial train model",
        "Question_gpt_summary_original":"The user is facing a challenge in serializing a list of Java objects into RecordIO format for training a model with SageMaker. They have explored using BeanIO but found it to be outdated and are seeking alternative Java libraries or methods to accomplish this task.",
        "Question_gpt_summary":"user face challeng serial list java object recordio format train model explor beanio outdat seek altern java librari method accomplish task",
        "Answer_original_content":"solv problem decid us apach avro instead beanio spark allow serial avro spark avro work fit us case try serial arrai number",
        "Answer_preprocessed_content":"solv problem decid us apach avro instead beanio spark allow serial avro work fit us case try serial arrai number",
        "Answer_gpt_summary_original":"Solution: The user found a solution to their challenge by using Apache Avro instead of BeanIO. They used Spark-Avro to serialize the Java objects into RecordIO format for training a model with SageMaker. However, they noted that this solution did not work for their specific use case of serializing an array of numbers.",
        "Answer_gpt_summary":"solut user solut challeng apach avro instead beanio spark avro serial java object recordio format train model note solut work specif us case serial arrai number"
    },
    {
        "Question_title":"Logging and Fetching Run Parameters in AzureML",
        "Question_body":"<p>I am able to log and fetch metrics to AzureML using Run.log, however, I need a way to also log run parameters, like Learning Rate, or Momentum. I can't seem to find anything in the AzureML Python SDK documentation to achieve this. However, if I use MLflow's mlflow.log_param, I am able to log parameters, and they even nicely show up on the AzureML Studio Dashboard (bottom right of the image):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/q9b7Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Again, I am able to fetch this using MLflow's get_params() function, but I can't find a way to do this using just AzureML's Python SDK. Is there a way to do this directly using <code>azureml<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1654521747343,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":73.0,
        "Poster_id":11319137.0,
        "Poster_created_time":1554497484963,
        "Poster_last_access_time":1664005308092,
        "Poster_age":null,
        "Poster_location":"Hyderabad, Telangana, India",
        "Poster_reputation":438.0,
        "Poster_views":120.0,
        "Answer_body":"<p>The retrieving of log run parameters like <strong>Learning Rate, or Momentum<\/strong> is not possible with <strong>AzureML<\/strong> alone. Because it was tied with <strong>MLFlow<\/strong> and <strong>azureml-core<\/strong>. without those two involvements, we cannot retrieve the log run parameters.<\/p>\n<pre><code>pip install azureml-core mlflow azureml-mlflow\n<\/code><\/pre>\n<p>Need to install these three for getting run parameters. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics\" rel=\"nofollow noreferrer\">Link<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":18428148.0,
        "Answerer_created_time":1646907459852,
        "Answerer_last_access_time":1664066617772,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":1624.0,
        "Answerer_views":1376.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72518344",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1656998954310,
        "Question_original_content":"log fetch run paramet abl log fetch metric run log need wai log run paramet like learn rate momentum python sdk document achiev us log param abl log paramet nice studio dashboard right imag abl fetch param function wai python sdk wai directli",
        "Question_preprocessed_content":"log fetch run paramet abl log fetch metric need wai log run paramet like learn rate momentum python sdk document achiev us abl log paramet nice studio dashboard abl fetch function wai python sdk wai directli",
        "Question_gpt_summary_original":"The user is able to log and fetch metrics to AzureML using Run.log, but is facing challenges in logging run parameters like Learning Rate or Momentum. The user has tried to find a solution in the AzureML Python SDK documentation but has not been successful. However, the user has found a workaround by using MLflow's mlflow.log_param to log parameters, which show up nicely on the AzureML Studio Dashboard. The user is seeking a way to log run parameters directly using AzureML's Python SDK.",
        "Question_gpt_summary":"user abl log fetch metric run log face challeng log run paramet like learn rate momentum user tri solut python sdk document success user workaround log param log paramet nice studio dashboard user seek wai log run paramet directli python sdk",
        "Answer_original_content":"retriev log run paramet like learn rate momentum possibl ti core involv retriev log run paramet pip instal core need instal get run paramet link",
        "Answer_preprocessed_content":"retriev log run paramet like learn rate momentum possibl ti core involv retriev log run paramet need instal get run paramet link",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that it is not possible to retrieve run parameters like Learning Rate or Momentum directly using AzureML's Python SDK. However, the user has found a workaround by using MLflow's mlflow.log_param to log parameters, which show up nicely on the AzureML Studio Dashboard. To log run parameters directly using AzureML's Python SDK, the user needs to install azureml-core, mlflow, and azureml-mlflow.",
        "Answer_gpt_summary":"solut discuss suggest possibl retriev run paramet like learn rate momentum directli python sdk user workaround log param log paramet nice studio dashboard log run paramet directli python sdk user need instal core"
    },
    {
        "Question_title":"How to make parameters available to SageMaker Tensorflow Endpoint",
        "Question_body":"<p>I'm looking to make some hyper parameters available to the serving endpoint in SageMaker. The training instances is given access to input parameters using hyperparameters in:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='autocat.py',\n                       role=role,\n                       output_path=params['output_path'],\n                       code_location=params['code_location'],\n                       train_instance_count=1,\n                       train_instance_type='ml.c4.xlarge',\n                       training_steps=10000,\n                       evaluation_steps=None,\n                       hyperparameters=params)\n<\/code><\/pre>\n\n<p>However, when the endpoint is deployed, there is no way to pass in parameters that are used to control the data processing in the <code>input_fn(serialized_input, content_type)<\/code> function.<\/p>\n\n<p>What would be the best way to pass parameters to the serving instance?? Is the <code>source_dir<\/code> parameter defined in the <code>sagemaker.tensorflow.TensorFlow<\/code> class copied to the serving instance? If so, I could use a config.yml or similar.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1521754623920,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1521754933880,
        "Question_score":1,
        "Question_view_count":1426.0,
        "Poster_id":5377184.0,
        "Poster_created_time":1443201378360,
        "Poster_last_access_time":1653587950968,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":749.0,
        "Poster_views":49.0,
        "Answer_body":"<p>Ah i have had a similar problem to you where I needed to download something off S3 to use in the input_fn for inference. In my case it was a dictionary.<\/p>\n\n<p>Three options:<\/p>\n\n<ol>\n<li>use your config.yml approach, and download and import the s3 file from within your entrypoint file before any function declarations. This would make it available to the input_fn <\/li>\n<li>Keep using the hyperparameter approach, download and import the vectorizer in <code>serving_input_fn<\/code> and make it available via a global variable so that <code>input_fn<\/code> has access to it.<\/li>\n<li>Download the file from s3 before training and include it in the source_dir directly.<\/li>\n<\/ol>\n\n<p>Option 3 would only work if you didnt need to make changes to the vectorizer seperately after initial training.<\/p>\n\n<p>Whatever you do, don't download the file directly in input_fn. I made that mistake and the performance is terrible as each invoking of the endpoint would result in the s3 file being downloaded.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":3709072.0,
        "Answerer_created_time":1401922736652,
        "Answerer_last_access_time":1663755541688,
        "Answerer_age":null,
        "Answerer_location":"Brisbane",
        "Answerer_reputation":748.0,
        "Answerer_views":32.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49438903",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1523591814356,
        "Question_original_content":"paramet avail tensorflow endpoint look hyper paramet avail serv endpoint train instanc given access input paramet hyperparamet estim tensorflow entri point autocat role role output path param output path code locat param code locat train instanc count train instanc type xlarg train step evalu step hyperparamet param endpoint deploi wai pass paramet control data process input serial input content type function best wai pass paramet serv instanc sourc dir paramet defin tensorflow tensorflow class copi serv instanc us config yml similar",
        "Question_preprocessed_content":"paramet avail tensorflow endpoint look hyper paramet avail serv endpoint train instanc given access input paramet hyperparamet endpoint deploi wai pass paramet control data process function best wai pass paramet serv instanc paramet defin class copi serv instanc us similar",
        "Question_gpt_summary_original":"The user is facing a challenge in making hyperparameters available to the serving endpoint in SageMaker Tensorflow. While the training instances have access to input parameters using hyperparameters, there is no way to pass in parameters to control data processing in the input function when the endpoint is deployed. The user is seeking advice on the best way to pass parameters to the serving instance.",
        "Question_gpt_summary":"user face challeng make hyperparamet avail serv endpoint tensorflow train instanc access input paramet hyperparamet wai pass paramet control data process input function endpoint deploi user seek advic best wai pass paramet serv instanc",
        "Answer_original_content":"similar problem need download us input infer case dictionari option us config yml approach download import file entrypoint file function declar avail input hyperparamet approach download import vector serv input avail global variabl input access download file train includ sourc dir directli option work didnt need chang vector seper initi train download file directli input mistak perform terribl invok endpoint result file download",
        "Answer_preprocessed_content":"similar problem need download us infer case dictionari option us approach download import file entrypoint file function declar avail hyperparamet approach download import vector avail global variabl access download file train includ directli option work didnt need chang vector seper initi train download file directli mistak perform terribl invok endpoint result file download",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use the config.yml approach, download and import the S3 file from within the entrypoint file before any function declarations. This would make it available to the input_fn.\n2. Keep using the hyperparameter approach, download and import the vectorizer in serving_input_fn and make it available via a global variable so that input_fn has access to it.\n3. Download the file from S3 before training and include it in the source_dir directly.\n\nIt is also mentioned that option 3 would only work if the vectorizer does not need to be changed separately after initial training. Additionally, it is advised not to download the file directly in input_fn as it would result in poor performance.",
        "Answer_gpt_summary":"possibl solut mention discuss us config yml approach download import file entrypoint file function declar avail input hyperparamet approach download import vector serv input avail global variabl input access download file train includ sourc dir directli mention option work vector need chang separ initi train addition advis download file directli input result poor perform"
    },
    {
        "Question_title":"debug and deploy featurizer (data processor for imodel inference) of sagemaker endpoint",
        "Question_body":"<p>I am looking at <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\/sklearn_abalone_featurizer.py\" rel=\"nofollow noreferrer\">this example<\/a> to implement the data processing of incoming raw data for a sagemaker endpoint prior to model inference\/scoring. This is all great but I have 2 questions:<\/p>\n<ul>\n<li>How can one debug this (e.g can I invoke endpoint without it being exposed as restful API and then use Sagemaker debugger)<\/li>\n<li>Sagemaker can be used &quot;remotely&quot; - e.g. via VSC. Can such a script be uploaded programatically?<\/li>\n<\/ul>\n<p>Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1651131440200,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":33.0,
        "Poster_id":283538.0,
        "Poster_created_time":1267440784443,
        "Poster_last_access_time":1664045779312,
        "Poster_age":null,
        "Poster_location":"Somewhere",
        "Poster_reputation":15705.0,
        "Poster_views":2150.0,
        "Answer_body":"<p>Sagemaker Debugger is only to monitor the training jobs.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-debugger.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-debugger.html<\/a><\/p>\n<p>I dont think you can use it on Endpoints.<\/p>\n<p>The script that you have provided is used both for training and inference. The container used by the estimator will take care of what functions to run. So it is not possible to debug the script directly. But what are you debugging in the code ? Training part or the inference part ?<\/p>\n<p>While creating the estimator we need to give either the entry_point or the source directory. If you are using the &quot;entry_point&quot; then the value should be relative path to the file, if you are using &quot;source_dir&quot; then you should be able to give an S3 path. So before running the estimator, you can programmatically tar the files and upload it to S3 and then use the S3 path in the estimator.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":7849350.0,
        "Answerer_created_time":1491898605956,
        "Answerer_last_access_time":1663894607407,
        "Answerer_age":null,
        "Answerer_location":"Sydney NSW, Australia",
        "Answerer_reputation":161.0,
        "Answerer_views":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72039744",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1651199956772,
        "Question_original_content":"debug deploi featur data processor imodel infer endpoint look exampl implement data process incom raw data endpoint prior model infer score great question debug invok endpoint expos rest api us debugg remot vsc script upload programat thank",
        "Question_preprocessed_content":"debug deploi featur endpoint look exampl implement data process incom raw data endpoint prior model great question debug remot vsc script upload programat thank",
        "Question_gpt_summary_original":"The user is facing two challenges related to implementing the data processing of incoming raw data for a sagemaker endpoint prior to model inference\/scoring. The first challenge is how to debug the implementation, and the second challenge is whether a script can be uploaded programmatically when using Sagemaker remotely.",
        "Question_gpt_summary":"user face challeng relat implement data process incom raw data endpoint prior model infer score challeng debug implement second challeng script upload programmat remot",
        "Answer_original_content":"debugg monitor train job http doc aw amazon com latest train debugg html dont think us endpoint script provid train infer contain estim care function run possibl debug script directli debug code train infer creat estim need entri point sourc directori entri point valu rel path file sourc dir abl path run estim programmat tar file upload us path estim",
        "Answer_preprocessed_content":"debugg monitor train job dont think us endpoint script provid train infer contain estim care function run possibl debug script directli debug code train infer creat estim need sourc directori valu rel path file abl path run estim programmat tar file upload us path estim",
        "Answer_gpt_summary_original":"Solutions provided:\n- Sagemaker Debugger cannot be used on Endpoints, so it cannot be used to debug the implementation.\n- The script provided is used for both training and inference, and the container used by the estimator will take care of what functions to run. Therefore, it is not possible to debug the script directly.\n- Before running the estimator, the files can be programmatically tarred and uploaded to S3, and then the S3 path can be used in the estimator.",
        "Answer_gpt_summary":"solut provid debugg endpoint debug implement script provid train infer contain estim care function run possibl debug script directli run estim file programmat tar upload path estim"
    },
    {
        "Question_title":"How to install Phyton package in Jupyter Notebook instance in SageMaker?",
        "Question_body":"Hi,\n\nI want to use awswrangler package in my Jupyter Notebook instance of SageMaker.\n\nI understand that we have to use Lifecycle configuration. I tried to do it using the following script:\n\n#!\/bin\/bash\n\npip install awswrangler==0.2.2\n\n\nBut when I import that package into my Notebook:\n\nimport boto3                                      # For executing native S3 APIs\nimport pandas as pd                               # For munging tabulara data\nimport numpy as np                                # For doing some calculation\nimport awswrangler as wr\nimport io\nfrom io import StringIO\n\n\nI still get the following error:\n\n---------------------------------------------------------------------------\nModuleNotFoundError                       Traceback (most recent call last)\n<ipython-input-1-f3d85c7dd0f6> in <module>()\n      2 import pandas as pd                               # For munging tabulara data\n      3 import numpy as np                                # For doing some calculation\n----> 4 import awswrangler as wr\n      5 import io\n      6 from io import StringIO\n\nModuleNotFoundError: No module named 'awswrangler'\n\n\nAny documentation or reference on how to install certain package for Jupyter Notebook in SageMaker?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1592823369000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":636.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi,\n\nexample how to use lifecycle config to install python package in one environment : https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-single-environment\/on-start.sh\n\nand to all conda env - https:\/\/github.com\/aws-samples\/amazon-sagemaker-notebook-instance-lifecycle-config-samples\/blob\/master\/scripts\/install-pip-package-all-environments\/on-start.sh",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4pvReJNZS6eDLxhd4pK-tQ\/how-to-install-phyton-package-in-jupyter-notebook-instance-in-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1592832724000,
        "Question_original_content":"instal phyton packag jupyt notebook instanc want us awswrangl packag jupyt notebook instanc understand us lifecycl configur tri follow script bin bash pip instal awswrangl import packag notebook import boto execut nativ api import panda mung tabulara data import numpi calcul import awswrangl import import stringio follow error modulenotfounderror traceback recent import panda mung tabulara data import numpi calcul import awswrangl import import stringio modulenotfounderror modul name awswrangl document refer instal certain packag jupyt notebook",
        "Question_preprocessed_content":"instal phyton packag jupyt notebook instanc want us awswrangl packag jupyt notebook instanc understand us lifecycl configur tri follow script pip instal import packag notebook import boto execut nativ api import panda mung tabulara data import numpi calcul import awswrangl import import stringio follow error modulenotfounderror traceback import panda mung tabulara data import numpi calcul import awswrangl import import stringio modulenotfounderror modul name awswrangl document refer instal certain packag jupyt notebook",
        "Question_gpt_summary_original":"The user is facing challenges in installing the awswrangler package in their Jupyter Notebook instance of SageMaker. They have attempted to use a script in the Lifecycle configuration, but are still receiving a \"ModuleNotFoundError\" when attempting to import the package into their Notebook. The user is seeking documentation or reference on how to properly install a package for Jupyter Notebook in SageMaker.",
        "Question_gpt_summary":"user face challeng instal awswrangl packag jupyt notebook instanc attempt us script lifecycl configur receiv modulenotfounderror attempt import packag notebook user seek document refer properli instal packag jupyt notebook",
        "Answer_original_content":"exampl us lifecycl config instal python packag environ http github com aw sampl amazon notebook instanc lifecycl config sampl blob master script instal pip packag singl environ start conda env http github com aw sampl amazon notebook instanc lifecycl config sampl blob master script instal pip packag environ start",
        "Answer_preprocessed_content":"exampl us lifecycl config instal python packag environ conda env",
        "Answer_gpt_summary_original":"Solution: The discussion provides two links to GitHub repositories that contain scripts for installing Python packages in SageMaker's Jupyter Notebook instance using the Lifecycle configuration. The first link provides an example of how to install a Python package in a single environment, while the second link provides a script for installing a package in all conda environments.",
        "Answer_gpt_summary":"solut discuss provid link github repositori contain script instal python packag jupyt notebook instanc lifecycl configur link provid exampl instal python packag singl environ second link provid script instal packag conda environ"
    },
    {
        "Question_title":"Getting the error while removing the duplicates in python AzureML classification problem",
        "Question_body":"<p>I'm getting this error while calling drop.duplicate function:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;train.py&quot;, line 159, in &lt;module&gt;\n    orders_dfx = preprocess_orders(orders_df)\n  File &quot;train.py&quot;, line 20, in preprocess_orders\n    ao = ao.drop_duplicates(subset=['order_id'], keep='last')\nAttributeError: 'TabularDataset' object has no attribute 'drop_duplicates'\n<\/code><\/pre>\n<p>Here is a part of <code>train.py<\/code> code<\/p>\n<pre><code>def preprocess_orders(ao):\n  ao = ao.drop_duplicates(subset=['order_id'], keep='last')\n  ao['order_id'] = ao['order_id'].astype('str')\n  ao['class'] = ao['class'].astype('int')\n  ao['age'] = ao['age'].astype('float').fillna(ao['age'].mean()).round(2)\n  return ao\n\norders_df = Dataset.get_by_name(ws, name='class_cancelled_orders')\norders_df.to_pandas_dataframe()\n# Doing processing\norders_dfx = preprocess_orders(orders_df)\n<\/code><\/pre>\n<p>I'm getting the data from the datasets in azureml studio. The job.py file is used for running experiment as:<\/p>\n<pre><code># submit job\nrun = Experiment(ws, experiment_name).submit(src)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1615028731157,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1615029902280,
        "Question_score":0,
        "Question_view_count":37.0,
        "Poster_id":14143684.0,
        "Poster_created_time":1598018883048,
        "Poster_last_access_time":1663862765087,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":77.0,
        "Poster_views":35.0,
        "Answer_body":"<p>The <code>to_pandas_dataframe()<\/code>method <em>returns<\/em> a pandas DataFrame, so you need to assign it back your variable:<\/p>\n<pre><code>orders_df = orders_df.to_pandas_dataframe()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":8222791.0,
        "Answerer_created_time":1498598028756,
        "Answerer_last_access_time":1663762787860,
        "Answerer_age":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation":2103.0,
        "Answerer_views":129.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66504979",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1615031651630,
        "Question_original_content":"get error remov duplic python classif problem get error call drop duplic function traceback recent file train line order dfx preprocess order order file train line preprocess order drop duplic subset order attributeerror tabulardataset object attribut drop duplic train code def preprocess order drop duplic subset order order order astyp str class class astyp int ag ag astyp float fillna ag mean round return order dataset class cancel order order panda datafram process order dfx preprocess order order get data dataset studio job file run experi submit job run experi experi submit src run wait complet output true",
        "Question_preprocessed_content":"get error remov duplic python classif problem get error call function code get data dataset studio file run experi",
        "Question_gpt_summary_original":"The user is encountering an error while trying to remove duplicates in a Python AzureML classification problem. The error message indicates that the 'TabularDataset' object has no attribute 'drop_duplicates'. The user has provided a part of the 'train.py' code where the 'preprocess_orders' function is defined, and the 'orders_df' dataset is obtained from AzureML studio. The 'job.py' file is used to run the experiment.",
        "Question_gpt_summary":"user encount error try remov duplic python classif problem error messag indic tabulardataset object attribut drop duplic user provid train code preprocess order function defin order dataset obtain studio job file run experi",
        "Answer_original_content":"panda datafram method return panda datafram need assign variabl order order panda datafram",
        "Answer_preprocessed_content":"method return panda datafram need assign variabl",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to assign the 'orders_df' variable to the 'to_pandas_dataframe()' method, which returns a pandas DataFrame. The updated code should be: \n\norders_df = orders_df.to_pandas_dataframe() \n\nThis should resolve the error message indicating that the 'TabularDataset' object has no attribute 'drop_duplicates'.",
        "Answer_gpt_summary":"solut solut provid discuss assign order variabl panda datafram method return panda datafram updat code order order panda datafram resolv error messag indic tabulardataset object attribut drop duplic"
    },
    {
        "Question_title":"Can I make Amazon SageMaker deliver a recommendation based on historic data instead of a probability score?",
        "Question_body":"<p>We have a huge set of data in CSV format, containing a few numeric elements, like this:<\/p>\n\n<pre><code>Year,BinaryDigit,NumberToPredict,JustANumber, ...other stuff\n1954,1,762,16, ...other stuff\n1965,0,142,16, ...other stuff\n1977,1,172,16, ...other stuff\n<\/code><\/pre>\n\n<p>The thing here is that there is a strong correlation between the third column and the columns before that. So I have pre-processed the data and it's now available in a format I think is perfect:<\/p>\n\n<pre><code>1954,1,762\n1965,0,142\n1977,1,172\n<\/code><\/pre>\n\n<p>What I want is a predicition on the value in the third column, using the first two as input. So in the case above, I want the input 1965,0 to return 142. In real life this file is thousands of rows, but since there's a pattern, I'd like to retrieve the most possible value.<\/p>\n\n<p>So far I've setup a train job on the CSV file using the L<em>inear Learner<\/em> algorithm, with the following settings:<\/p>\n\n<pre><code>label_size = 1\nfeature_dim = 2\npredictor_type = regression\n<\/code><\/pre>\n\n<p>I've also created a model from it, and setup an endpoint. When I invoke it, I get a score in return.<\/p>\n\n<pre><code>    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>My goal here is to get the third column prediction instead. How can I achieve that? I have read a lot of the documentation regarding this, but since I'm not very familiar with AWS, I might as well have used the wrong algorithms for what I am trying to do.<\/p>\n\n<p>(Please feel free to edit this question to better suit AWS terminology)<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1552553455710,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1552895653107,
        "Question_score":1,
        "Question_view_count":271.0,
        "Poster_id":4069901.0,
        "Poster_created_time":1411464641600,
        "Poster_last_access_time":1663665272227,
        "Poster_age":null,
        "Poster_location":"\u00d6rebro, Sverige",
        "Poster_reputation":205.0,
        "Poster_views":38.0,
        "Answer_body":"<p>For csv input, the label should be in the first column, as mentioned <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">here<\/a>:  So you should preprocess your data to put the label (the column you want to predict) on the left.<\/p>\n\n<p>Next, you need to decide whether this is a regression problem or a classification problem. <\/p>\n\n<p>If you want to predict a number that's as close as possible to the true number, that's regression. For example, the truth might be 4, and the model might predict 4.15. If you need an integer prediction, you could round the model's output.<\/p>\n\n<p>If you want the prediction to be one of a few categories, then you have a classification problem. For example, we might encode 'North America' = 0, 'Europe' = 1, 'Africa' = 2, and so on. In this case, a fractional prediction wouldn't make sense. <\/p>\n\n<p>For regression, use <code>'predictor_type' = 'regressor'<\/code> and for classification with more than 2 classes, use <code>'predictor_type' = 'multiclass_classifier'<\/code> as documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The output of regression will contain only a <code>'score'<\/code> field, which is the model's prediction. The output of multiclass classification will contain a <code>'predicted_label'<\/code> field, which is the model's prediction, as well as a <code>'score'<\/code> field, which is a vector of probabilities representing the model's confidence. The index with the highest probability will be the one that's predicted as the <code>'predicted_label'<\/code>. The output formats are documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/LL-in-formats.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":2564688.0,
        "Answerer_created_time":1373375969332,
        "Answerer_last_access_time":1554320237872,
        "Answerer_age":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation":126.0,
        "Answerer_views":16.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55158307",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1553880598532,
        "Question_original_content":"deliv recommend base histor data instead probabl score huge set data csv format contain numer element like year binarydigit numbertopredict justanumb stuff stuff stuff stuff thing strong correl column column pre process data avail format think perfect want predicit valu column input case want input return real life file thousand row pattern like retriev possibl valu far setup train job csv file linear learner algorithm follow set label size featur dim predictor type regress creat model setup endpoint invok score return respons runtim invok endpoint endpointnam endpoint contenttyp text csv bodi payload goal column predict instead achiev read lot document familiar aw wrong algorithm try feel free edit question better suit aw terminolog",
        "Question_preprocessed_content":"deliv recommend base histor data instead probabl score huge set data csv format contain numer element like thing strong correl column column data avail format think perfect want predicit valu column input case want input return real life file thousand row pattern like retriev possibl valu far setup train job csv file linear learner algorithm follow set creat model setup endpoint invok score return goal column predict instead achiev read lot document familiar aw wrong algorithm try feel free edit question better suit aw terminolog",
        "Question_gpt_summary_original":"The user has a large dataset in CSV format with a strong correlation between the third column and the columns before it. They have pre-processed the data and want to predict the value in the third column using the first two columns as input. The user has set up a train job using the Linear Learner algorithm with regression settings and created a model with an endpoint. However, when invoking the endpoint, they receive a score instead of the desired prediction for the third column. The user is seeking guidance on how to achieve their goal.",
        "Question_gpt_summary":"user larg dataset csv format strong correl column column pre process data want predict valu column column input user set train job linear learner algorithm regress set creat model endpoint invok endpoint receiv score instead desir predict column user seek guidanc achiev goal",
        "Answer_original_content":"csv input label column mention preprocess data label column want predict left need decid regress problem classif problem want predict number close possibl true number regress exampl truth model predict need integ predict round model output want predict categori classif problem exampl encod north america europ africa case fraction predict wouldn sens regress us predictor type regressor classif class us predictor type multiclass classifi document output regress contain score field model predict output multiclass classif contain predict label field model predict score field vector probabl repres model confid index highest probabl predict predict label output format document",
        "Answer_preprocessed_content":"csv input label column mention preprocess data label left need decid regress problem classif problem want predict number close possibl true number regress exampl truth model predict need integ predict round model output want predict categori classif problem exampl encod north america europ africa case fraction predict wouldn sens regress us classif class us document output regress contain field model predict output multiclass classif contain field model predict field vector probabl repres model confid index highest probabl predict output format document",
        "Answer_gpt_summary_original":"The user should preprocess their data to put the label (the column they want to predict) on the left. They need to decide whether this is a regression problem or a classification problem. For regression, use 'predictor_type' = 'regressor' and for classification with more than 2 classes, use 'predictor_type' = 'multiclass_classifier'. The output of regression will contain only a 'score' field, which is the model's prediction. The output of multiclass classification will contain a 'predicted_label' field, which is the model's prediction, as well as a 'score' field, which is a vector of probabilities representing the model's confidence.",
        "Answer_gpt_summary":"user preprocess data label column want predict left need decid regress problem classif problem regress us predictor type regressor classif class us predictor type multiclass classifi output regress contain score field model predict output multiclass classif contain predict label field model predict score field vector probabl repres model confid"
    },
    {
        "Question_title":"Best practice about the frequency of sentiment analysis in Azure Text Analytics",
        "Question_body":"<p>In my asp.net application, I want to do a sentiment analysis on each discussion forum item as they are posted by the users. I wonder if it is a good practice to make a request to Azure Text Analytics server to do a new Sentiment Analysis each time a text is posted by any user. Or, is it better to do this somehow once a day on all posts as a batch. I wonder what is the best practice about this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1562097728357,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":59.0,
        "Poster_id":1845408.0,
        "Poster_created_time":1353596362916,
        "Poster_last_access_time":1662822169860,
        "Poster_age":null,
        "Poster_location":"USA",
        "Poster_reputation":8007.0,
        "Poster_views":792.0,
        "Answer_body":"<p>We are working on a similar project as you. My suggestion for your question is, you should select proper interval based on your requirement and need. For example, if you want to react based on the sentiment result of every post in a timely manner, you need to do a analysis every time you pull a new post. Also you can adjust your time interval of \"pull and analyze\" based on your business\/ research need. IF you just want to train your model and predict something based on the data you get and you have no need for timely reaction, I think once a day is enough.<\/p>\n\n<p>For our project, we are in the situation 1. So we will do a quick analysis when we receive any new post so that we can have a quick reaction.<\/p>\n\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1562698026600,
        "Answer_score":1.0,
        "Answerer_id":9598801.0,
        "Answerer_created_time":1522870754323,
        "Answerer_last_access_time":1663925504390,
        "Answerer_age":null,
        "Answerer_location":"Redmond, WA, USA",
        "Answerer_reputation":366.0,
        "Answerer_views":173.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56859324",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1562697661556,
        "Question_original_content":"best practic frequenc sentiment analysi azur text analyt asp net applic want sentiment analysi discuss forum item post user wonder good practic request azur text analyt server new sentiment analysi time text post user better dai post batch wonder best practic",
        "Question_preprocessed_content":"best practic frequenc sentiment analysi azur text analyt applic want sentiment analysi discuss forum item post user wonder good practic request azur text analyt server new sentiment analysi time text post user better dai post batch wonder best practic",
        "Question_gpt_summary_original":"The user is seeking advice on the best practice for conducting sentiment analysis on discussion forum items in their asp.net application. They are unsure whether to make a request to Azure Text Analytics server for each post or to conduct the analysis once a day on all posts as a batch.",
        "Question_gpt_summary":"user seek advic best practic conduct sentiment analysi discuss forum item asp net applic unsur request azur text analyt server post conduct analysi dai post batch",
        "Answer_original_content":"work similar project suggest question select proper interv base requir need exampl want react base sentiment result post time manner need analysi time pull new post adjust time interv pull analyz base busi research need want train model predict base data need time reaction think dai project situat quick analysi receiv new post quick reaction hope help",
        "Answer_preprocessed_content":"work similar project suggest question select proper interv base requir need exampl want react base sentiment result post time manner need analysi time pull new post adjust time interv pull analyz base busi research need want train model predict base data need time reaction think dai project situat quick analysi receiv new post quick reaction hope help",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user should select the proper interval based on their requirement and need. If the user wants to react based on the sentiment result of every post in a timely manner, they need to conduct analysis every time they pull a new post. However, if the user just wants to train their model and predict something based on the data they get and they have no need for timely reaction, once a day is enough.",
        "Answer_gpt_summary":"solut discuss suggest user select proper interv base requir need user want react base sentiment result post time manner need conduct analysi time pull new post user want train model predict base data need time reaction dai"
    },
    {
        "Question_title":"How to manage datasets in ClearML Web UI?",
        "Question_body":"<p>Using a self-deployed ClearML server with the clearml-data CLI, I would like to manage (or view) my datasets in the WebUI as shown on the ClearML webpage (<a href=\"https:\/\/clear.ml\/mlops\/clearml-feature-store\/\" rel=\"nofollow noreferrer\">https:\/\/clear.ml\/mlops\/clearml-feature-store\/<\/a>):<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2T8IC.png\" alt=\"ClearML feature store\" \/><\/a><\/p>\n<p>However, this feature does not show up in my Web UI. According to the pricing page, the feature store is not a premium feature. Do I need to configure my server in a special way to use this feature?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1615821875663,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1615822213440,
        "Question_score":3,
        "Question_view_count":310.0,
        "Poster_id":11165362.0,
        "Poster_created_time":1551960780550,
        "Poster_last_access_time":1663614230163,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":354.0,
        "Poster_views":58.0,
        "Answer_body":"<p>Disclaimer: I'm part of the ClearML (formerly Trains) Team<\/p>\n<p>I think this screenshot is taken from the premium version...\nThe feature itself exists in the open-source version, but I &quot;think&quot; some of the dataset visualization capabilities are not available in the open-source self hosted version.<\/p>\n<p>Nonetheless, you have a fully featured feature-store, with the ability to add your own metrics \/ samples for every dataset\/feature version. The open-source version also includes the advanced versioning &amp; delta based storage for datasets\/features (i.e. only the change set from the parent version is stored)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":11682840.0,
        "Answerer_created_time":1561143508792,
        "Answerer_last_access_time":1659992319200,
        "Answerer_age":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation":489.0,
        "Answerer_views":60.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66640850",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1615831186403,
        "Question_original_content":"manag dataset web self deploi server data cli like manag view dataset webui shown webpag http clear mlop featur store featur web accord price page featur store premium featur need configur server special wai us featur",
        "Question_preprocessed_content":"manag dataset web server data cli like manag dataset webui shown webpag featur web accord price page featur store premium featur need configur server special wai us featur",
        "Question_gpt_summary_original":"The user is facing challenges in managing or viewing their datasets in the ClearML Web UI, despite using a self-deployed ClearML server with the clearml-data CLI. The feature to manage datasets in the Web UI is not showing up, and the user is unsure if any special configuration is required to use this feature.",
        "Question_gpt_summary":"user face challeng manag view dataset web despit self deploi server data cli featur manag dataset web show user unsur special configur requir us featur",
        "Answer_original_content":"disclaim train team think screenshot taken premium version featur exist open sourc version think dataset visual capabl avail open sourc self host version nonetheless fulli featur featur store abil add metric sampl dataset featur version open sourc version includ advanc version delta base storag dataset featur chang set parent version store",
        "Answer_preprocessed_content":"disclaim team think screenshot taken premium featur exist version think dataset visual capabl avail self host version nonetheless fulli featur abil add metric sampl version version includ advanc version delta base storag",
        "Answer_gpt_summary_original":"Solutions provided: \n- The feature to manage datasets in the ClearML Web UI may not be available in the open-source self-hosted version, but the user still has access to a fully-featured feature-store with the ability to add their own metrics\/samples for every dataset\/feature version. \n- The open-source version also includes advanced versioning and delta-based storage for datasets\/features, where only the change set from the parent version is stored.",
        "Answer_gpt_summary":"solut provid featur manag dataset web avail open sourc self host version user access fulli featur featur store abil add metric sampl dataset featur version open sourc version includ advanc version delta base storag dataset featur chang set parent version store"
    },
    {
        "Question_title":"I want to register the model learned by AutoML in Azure Machine learning in ONNX format and call it in Azure Synapse Analitics.",
        "Question_body":"I found that I can register the model using Mlflow, but I don't know how to register it in ONNX format.\nI found out that the model is registered using Mlflow.\nBut I don't know how to convert AutoML models to ONNX format and register them with Mlflow.\n\nfrom azure.ai.ml import MLClient\nfrom azure.identity import DefaultAzureCredential\nfrom azureml.train.automl import AutoMLConfig\nfrom azureml.core import Workspace, Dataset\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.model import Model\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nfrom azureml.automl.runtime.onnx_convert import OnnxConverter\nfrom random import random\nfrom mlflow.tracking import MlflowClient\nimport mlflow\nimport mlflow.onnx\nimport os\nimport azureml.mlflow\n\nauth = ServicePrincipalAuthentication(\ntenant_id=\"\",\nservice_principal_id=\"\",\nservice_principal_password=\"\")\n\nsubscription_id = ''\nresource_group = ''\nworkspace_name = ''\n\nml_client = MLClient(credential=auth,\nsubscription_id=subscription_id,\nresource_group_name=resource_group)\n\nazure_mlflow_uri = ml_client.workspaces.get(workspace_name).mlflow_tracking_uri\nmlflow.set_tracking_uri(azure_mlflow_uri)\n\nws = Workspace(subscription_id, resource_group, workspace_name, auth=auth)\n\ntrain_data = Dataset.get_by_name(ws, name='iris')\n\nlabel = \"class\"\n\nautoml_settings = {\n\"primary_metric\": 'AUC_weighted',\n\"n_cross_validations\": 2\n}\n\nautoml_classifier = AutoMLConfig(\ntask='classification',\nblocked_models=['XGBoostClassifier'],\nenable_onnx_compatible_models=True,\nexperiment_timeout_minutes=30,\ntraining_data=train_data,\nlabel_column_name=label,\n**automl_settings\n)\n\nexperiment_name = 'experimetn_with_mlflow'\nmlflow.set_experiment(experiment_name)\nexperiment = Experiment(ws, experiment_name)\n\nwith mlflow.start_run() as mlflow_run:\nmlflow.log_metric(\"iris_metric\", random())\n\n mlflow_run = experiment.submit(automl_classifier, show_output=True)\n description = 'iris_Description'\n model = mlflow_run.register_model(description=description,\n                                model_name='iris_Model')\n best_run, onnx_mdl = mlflow_run.get_output(return_onnx_model=True)\n onnx_fl_path = \".\/best_model.onnx\"\n OnnxConverter.save_onnx_model(onnx_mdl, onnx_fl_path)\n model = Model.register(workspace=ws,\n                     description=description,\n                     model_name='iris_onnx_model',\n                     model_path=onnx_fl_path)\n client = MlflowClient()\n finished_mlflow_run = MlflowClient().get_run(mlflow_run.run_id)\n metrics = finished_mlflow_run.data.metrics\n tags = finished_mlflow_run.data.tags\n params = finished_mlflow_run.data.params\n model_path  = \"best_model\"\n model_uri = 'runs:\/{}\/{}'.format(mlflow_run.run_id, model_path)\n mlflow.register_model(model_uri, 'iris_onnx_mlflow_model')",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1664411309103,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@10433767 Thanks for the question. Can you please share document\/sample that you are trying. In order to save trained model download (and score) as the ONNX model you have here a few code examples.\nMLflow model registry will enable Synapse to run ONNX models is in preview.\n\nHere is the ONNX prediction section in the sample notebook.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1027830\/i-want-to-register-the-model-learned-by-automl-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1664453127300,
        "Question_original_content":"want regist model learn automl onnx format azur synaps analit regist model know regist onnx format model regist know convert automl model onnx format regist azur import mlclient azur ident import defaultazurecredenti train automl import automlconfig core import workspac dataset core experi import experi core model import model core authent import serviceprincipalauthent automl runtim onnx convert import onnxconvert random import random track import client import import onnx import import auth serviceprincipalauthent tenant servic princip servic princip password subscript resourc group workspac client mlclient credenti auth subscript subscript resourc group resourc group azur uri client workspac workspac track uri set track uri azur uri workspac subscript resourc group workspac auth auth train data dataset iri label class automl set primari metric auc weight cross valid automl classifi automlconfig task classif block model xgboostclassifi enabl onnx compat model true experi timeout minut train data train data label column label automl set experi experimetn set experi experi experi experi experi start run run log metric iri metric random run experi submit automl classifi output true descript iri descript model run regist model descript descript model iri model best run onnx mdl run output return onnx model true onnx path best model onnx onnxconvert save onnx model onnx mdl onnx path model model regist workspac descript descript model iri onnx model model path onnx path client client finish run client run run run metric finish run data metric tag finish run data tag param finish run data param model path best model model uri run format run run model path regist model model uri iri onnx model",
        "Question_preprocessed_content":"want regist model learn automl onnx format azur synaps analit regist model know regist onnx format model regist know convert automl model onnx format regist import mlclient import defaultazurecredenti import automlconfig core import workspac dataset import experi import model import serviceprincipalauthent import onnxconvert random import random track import client import import onnx import import auth serviceprincipalauthent mlclient auth auth iri label class automlconfig experi experi random descript model model descript descript client client metric tag param",
        "Question_gpt_summary_original":"The user wants to register the model learned by AutoML in Azure Machine Learning in ONNX format and call it in Azure Synapse Analytics. The user has encountered a challenge in converting AutoML models to ONNX format and registering them with Mlflow.",
        "Question_gpt_summary":"user want regist model learn automl onnx format azur synaps analyt user encount challeng convert automl model onnx format regist",
        "Answer_original_content":"thank question share document sampl try order save train model download score onnx model code exampl model registri enabl synaps run onnx model preview onnx predict section sampl notebook",
        "Answer_preprocessed_content":"thank question share try order save train model download onnx model code exampl model registri enabl synaps run onnx model preview onnx predict section sampl notebook",
        "Answer_gpt_summary_original":"Solution: The MLflow model registry in Azure Machine Learning can be used to register the AutoML model in ONNX format. The ONNX model can be downloaded and scored using code examples provided in the discussion. Additionally, the ONNX prediction section in the sample notebook can be used as a reference.",
        "Answer_gpt_summary":"solut model registri regist automl model onnx format onnx model download score code exampl provid discuss addition onnx predict section sampl notebook refer"
    },
    {
        "Question_title":"Azure Machine Learning Batch request returning mal-formatted umlauts with Unixes",
        "Question_body":"<p>I have deployed my code in Azure Machine Learning and run the batch request in R with different operating systems, such as Unix and W10. For some reason, the host outputs are properly formatted only in R of W10 but I am unable to get properly formatted output in Unix systems. Only way I can get properly formatted outputs in all systems is through the Azure GUI and manually download the file. In W10, I have the luxury to get the properly formatted file directly with my Rscript\/Rstudio thing. In R, I have used <code>system(&quot;defaults write org.R-project.R force.LANG en_US.UTF-8&quot;)<\/code> as hinted <a href=\"https:\/\/stackoverflow.com\/questions\/41873359\/r-encoding-failing-with-umlauts-such-as-%C3%A4-and-%C3%B6\">here<\/a> to explicitly specify the encoding but this does not have any effect on the batch request R script that is executed in Azure servers run by Microsoft.<\/p>\n<p>What is happening is that <code>UTF-8 characters bytes are returned as Latin-1 characters bytes<\/code>, for example<\/p>\n<blockquote>\n<ol>\n<li><p><code>\u00f6<\/code> as <code>\u00c3 \u00b6<\/code><\/p>\n<\/li>\n<li><p><code>\u00e4<\/code> as <code>\u00c3 \u00a4<\/code><\/p>\n<\/li>\n<li><p><code>\u00c4<\/code> as <code>\u00c3 \u00a5<\/code><\/p>\n<\/li>\n<\/ol>\n<\/blockquote>\n<p>as can be demonstrated and tested with this tool <a href=\"http:\/\/www.ltg.ed.ac.uk\/%7Erichard\/utf-8.cgi?input=%C3%84&amp;mode=char\" rel=\"nofollow noreferrer\">here<\/a> about Latin-1 characters. So what are best ways to deal with this encoding issue, can it be addressed somehow inside Azure ML? Where can you do bug reports? Does there exist some tool to convert Latin-1 to UTF-8 in R?<\/p>\n<p><strong>How can you get properly formatted UTF-8 files with umlauts with R batch requests in Azure ML (not in Latin-1 characters)?<\/strong><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1485549679360,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1592644375060,
        "Question_score":0,
        "Question_view_count":62.0,
        "Poster_id":164148.0,
        "Poster_created_time":1251372839052,
        "Poster_last_access_time":1653648989307,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":48616.0,
        "Poster_views":3348.0,
        "Answer_body":"<p>The Batch request R command has a <code>saveBlobToFile<\/code> function. The problem is in the <code>saveBlobToFile<\/code> function that uses wrong encoding with <code>getUrl<\/code>.  <code>getUrl<\/code> function needs to specify the encodings explicitly. Do the following changes<\/p>\n\n<pre><code>blobContent = getURL(blobUrl, .encoding=\"UTF-8\")\n<\/code><\/pre>\n\n<p>where without <code>.encoding<\/code>, the output is <code>ISO8859-1('latin1')<\/code> or something inherited from your system.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1486367568768,
        "Answer_score":0.0,
        "Answerer_id":164148.0,
        "Answerer_created_time":1251372839052,
        "Answerer_last_access_time":1653648989307,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":48616.0,
        "Answerer_views":3348.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41902672",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1486366682512,
        "Question_original_content":"batch request return mal format umlaut unix deploi code run batch request differ oper system unix reason host output properli format unabl properli format output unix system wai properli format output system azur gui manual download file luxuri properli format file directli rscript rstudio thing default write org project forc lang utf hint explicitli specifi encod effect batch request script execut azur server run microsoft happen utf charact byte return latin charact byte exampl demonstr test tool latin charact best wai deal encod issu address insid bug report exist tool convert latin utf properli format utf file umlaut batch request latin charact",
        "Question_preprocessed_content":"batch request return umlaut unix deploi code run batch request differ oper system unix reason host output properli format unabl properli format output unix system wai properli format output system azur gui manual download file luxuri properli format file directli thing hint explicitli specifi encod effect batch request script execut azur server run microsoft happen exampl demonstr test tool charact best wai deal encod issu address insid bug report exist tool convert properli format file umlaut batch request",
        "Question_gpt_summary_original":"The user is facing challenges with Azure Machine Learning batch request returning mal-formatted umlauts with Unixes. The host outputs are properly formatted only in R of W10 but not in Unix systems. The UTF-8 characters bytes are returned as Latin-1 characters bytes, which is causing the issue. The user has tried to explicitly specify the encoding but it does not have any effect on the batch request R script that is executed in Azure servers run by Microsoft. The user is seeking solutions to address this encoding issue and looking for ways to get properly formatted UTF-8 files with umlauts with R batch requests in Azure ML.",
        "Question_gpt_summary":"user face challeng batch request return mal format umlaut unix host output properli format unix system utf charact byte return latin charact byte caus issu user tri explicitli specifi encod effect batch request script execut azur server run microsoft user seek solut address encod issu look wai properli format utf file umlaut batch request",
        "Answer_original_content":"batch request command saveblobtofil function problem saveblobtofil function us wrong encod geturl geturl function need specifi encod explicitli follow chang blobcont geturl bloburl encod utf encod output iso latin inherit",
        "Answer_preprocessed_content":"batch request command function problem function us wrong encod function need specifi encod explicitli follow chang output inherit",
        "Answer_gpt_summary_original":"Solution: The solution to the encoding issue is to specify the encoding explicitly in the <code>getUrl<\/code> function by adding <code>.encoding=\"UTF-8\"<\/code> to the <code>blobContent = getURL(blobUrl)<\/code> line of code.",
        "Answer_gpt_summary":"solut solut encod issu specifi encod explicitli geturl function ad encod utf blobcont geturl bloburl line code"
    },
    {
        "Question_title":"struggling to install python package via amazon sagemaker",
        "Question_body":"<p>I am using Amazon Sagemaker and trying to install gaapi4py package via anaconda python3 notebook.<\/p>\n<p>So far I've tried the following commands:<\/p>\n<pre><code>%conda install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>conda install gaapi4py\n\nGot same error:\n\nCollecting package metadata (current_repodata.json): failed\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json&gt;\nElapsed: -\n\nAn HTTP error occurred when trying to retrieve this URL.\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n'https:\/\/conda.anaconda.org\/conda-forge\/linux-64'\n\n\n\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>Alternatively I've tried the below but it failed as well:<\/p>\n<pre><code>pip install gaapi4py\n<\/code><\/pre>\n<p>Error text:<\/p>\n<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803c50&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c8035f8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803550&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803400&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803358&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nERROR: Could not find a version that satisfies the requirement gaapi4py (from versions: none)\nERROR: No matching distribution found for gaapi4py\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>What am I doing wrong? All previous packages worked well.<\/p>\n<p>UPD:<\/p>\n<p>Tried also as recommended in amazon book:<\/p>\n<pre><code>import sys\n!{sys.executable} -m pip install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>import sys\n!conda install -y --prefix {sys.prefix} gaapi4py\n<\/code><\/pre>\n<p>Both didn't work neither, getting same errors as above.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1606886205577,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1606887753310,
        "Question_score":1,
        "Question_view_count":1148.0,
        "Poster_id":6886838.0,
        "Poster_created_time":1474967309676,
        "Poster_last_access_time":1661467549403,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":59.0,
        "Poster_views":66.0,
        "Answer_body":"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":6886838.0,
        "Answerer_created_time":1474967309676,
        "Answerer_last_access_time":1661467549403,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":59.0,
        "Answerer_views":66.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65102618",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1644193786212,
        "Question_original_content":"struggl instal python packag try instal gaapipi packag anaconda python notebook far tri follow command conda instal gaapipi conda instal gaapipi got error collect packag metadata current repodata json fail condahttperror http connect fail url elaps http error occur try retriev url http error intermitt simpl retri wai http conda anaconda org conda forg linux note need restart kernel us updat packag altern tri fail pip instal gaapipi error text warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi error version satisfi requir gaapipi version error match distribut gaapipi warn pip version version avail consid upgrad home user anaconda env python bin python pip instal upgrad pip command note need restart kernel us updat packag wrong previou packag work upd tri recommend amazon book import sy sy execut pip instal gaapipi import sy conda instal prefix sy prefix gaapipi work get error",
        "Question_preprocessed_content":"struggl instal python packag try instal gaapi packag anaconda python notebook far tri follow command altern tri fail error text wrong previou packag work upd tri recommend amazon book work get error",
        "Question_gpt_summary_original":"The user is facing challenges while trying to install the gaapi4py package via anaconda python3 notebook on Amazon Sagemaker. The user has tried various commands, including 'conda install' and 'pip install', but has encountered errors such as HTTP 000 CONNECTION FAILED and Failed to establish a new connection. The user has also tried alternative methods recommended by Amazon, but none of them have worked.",
        "Question_gpt_summary":"user face challeng try instal gaapipi packag anaconda python notebook user tri command includ conda instal pip instal encount error http connect fail fail establish new connect user tri altern method recommend amazon work",
        "Answer_original_content":"talk forth depart figur custom librari instal block secur reason",
        "Answer_preprocessed_content":"talk depart figur custom librari instal block secur reason",
        "Answer_gpt_summary_original":"Solution: The user found out that the installation of custom libraries was blocked for security reasons. Therefore, it is not possible to install the gaapi4py package via anaconda python3 notebook on Amazon Sagemaker. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user instal custom librari block secur reason possibl instal gaapipi packag anaconda python notebook solut mention discuss"
    },
    {
        "Question_title":"Remote run model unable to be saved",
        "Question_body":"I've built models using the AutoML function and I'm trying to call the best model to deploy into production. The AutoML function ran correctly and produced the ~35 models. My goal is to pull the best model. Here is the code:\n\nbest_run, fitted_model = remote_run.get_output()\nfitted_model\n\nWhen runnning the code, I get the following error:\n\nAttributeError: 'DataTransformer' object has no attribute 'enable_dnn'\n\nAny help would be much appreciated.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1613083790957,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@BernardoJaccoud-0827 Did your run configure enable_dnn i.e bert settings of automated ML? I am curious to understand what the status of your run is directly on the portal ml.azure.com?",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/270011\/remote-run-model-unable-to-be-saved.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1613128985603,
        "Question_original_content":"remot run model unabl save built model automl function try best model deploi product automl function ran correctli produc model goal pull best model code best run fit model remot run output fit model ru code follow error attributeerror datatransform object attribut enabl dnn help appreci",
        "Question_preprocessed_content":"remot run model unabl save built model automl function try best model deploi product automl function ran correctli produc model goal pull best model code ru code follow error attributeerror datatransform object attribut help appreci",
        "Question_gpt_summary_original":"The user is facing a challenge in saving a remote run model built using the AutoML function. The AutoML function produced around 35 models, and the user is trying to call the best model to deploy into production. However, when running the code to pull the best model, the user encounters an error stating that the 'DataTransformer' object has no attribute 'enable_dnn'.",
        "Question_gpt_summary":"user face challeng save remot run model built automl function automl function produc model user try best model deploi product run code pull best model user encount error state datatransform object attribut enabl dnn",
        "Answer_original_content":"bernardojaccoud run configur enabl dnn bert set autom curiou understand statu run directli portal azur com",
        "Answer_preprocessed_content":"run configur bert set autom curiou understand statu run directli portal",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"ModuleNotFoundError: No module named 'ruamel' when excuting from azureml.core",
        "Question_body":"<p>I am trying to execute the Azure ml sdk from the local system using the Jupyter notebook. When I run the below code i am getting an error.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore, Dataset\n\nModuleNotFoundError: No module named 'ruamel' \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1622646368700,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":332.0,
        "Poster_id":14259505.0,
        "Poster_created_time":1599816833352,
        "Poster_last_access_time":1636176255430,
        "Poster_age":null,
        "Poster_location":"New Delhi, Delhi, India",
        "Poster_reputation":329.0,
        "Poster_views":58.0,
        "Answer_body":"<p>You have to add pip 20.1.1<\/p>\n<p>Conda ruamel needs higher version of pip<\/p>\n<pre><code>conda install pip=20.1.1\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":10330386.0,
        "Answerer_created_time":1536318818623,
        "Answerer_last_access_time":1663913110647,
        "Answerer_age":null,
        "Answerer_location":"\u0130zmit, Kocaeli, T\u00fcrkiye",
        "Answerer_reputation":1033.0,
        "Answerer_views":55.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67807756",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1626743937907,
        "Question_original_content":"modulenotfounderror modul name ruamel excut core try execut sdk local jupyt notebook run code get error core import workspac datastor dataset modulenotfounderror modul name ruamel",
        "Question_preprocessed_content":"modulenotfounderror modul name ruamel excut core try execut sdk local jupyt notebook run code get error",
        "Question_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" when trying to execute the Azure ml sdk from their local system using Jupyter notebook. The error message specifically states that there is no module named 'ruamel'.",
        "Question_gpt_summary":"user encount modulenotfounderror try execut sdk local jupyt notebook error messag specif state modul name ruamel",
        "Answer_original_content":"add pip conda ruamel need higher version pip conda instal pip",
        "Answer_preprocessed_content":"add pip conda ruamel need higher version pip",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to install pip version 20.1.1 using the command \"conda install pip=20.1.1\" as the Conda ruamel needs a higher version of pip.",
        "Answer_gpt_summary":"solut solut mention discuss instal pip version command conda instal pip conda ruamel need higher version pip"
    },
    {
        "Question_title":"New Azure Machine Learning & Excel",
        "Question_body":"Hi All\n\nI have been working with Azure Machine Learning Studio (Classic) and have always found its integration with Excel super mega useful.\n\nAll I had to do was to get the URI and the API_Key of my web service and paste them on the Azure Machine Learning Add-In, that I had downloaded. Easy and useful.\n\nHowever, with the new Azure Machine Learning studio that does not seem possible any more.\n\nUnder the new Azure Machine Learning studio when I deploy a model I get a REST endpoint and that's it? !? I cannot find anywhere the API_key for my web service. I cannot even find a web service section as such.\n\nHow do I get the API_Key for the web service I need?\n\nIf I get the API_Key could I use it on the Excel Azure Machine Learning add-in. It looks as if this is no longer an option and we need to start using Power BI instead.\n\nI have read this interesting post where someone mentions a work around that consist of creating an Excel macro. Is this the best option? https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/236781\/consume-scoreing-api-in-excel.html\n\n\n\n\nThank you",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1651222482640,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi, thanks for reaching out. The new AzureML integration with Excel isn't supported at this time. More details are provided on this thread. The alternative approach would be to use a Client or PowerBI to consume the model. For future reference, you can find your webservice endpoint and keys under Studio > Endpoints > Endpoint > Consume.\n\n\n\n\n\n--please don't forget to Accept Answer if the reply is helpful. Thanks.--",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/831512\/new-azure-machine-learning-amp-excel.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1651319301696,
        "Question_original_content":"new excel work studio classic integr excel super mega us uri api kei web servic past add download easi us new studio possibl new studio deploi model rest endpoint api kei web servic web servic section api kei web servic need api kei us excel add look longer option need start power instead read interest post mention work consist creat excel macro best option http doc microsoft com answer question consum score api excel html thank",
        "Question_preprocessed_content":"new excel work studio integr excel super mega us uri web servic past download easi us new studio possibl new studio deploi model rest endpoint web servic web servic section web servic need us excel look longer option need start power instead read interest post mention work consist creat excel macro best option thank",
        "Question_gpt_summary_original":"The user is facing challenges with the new Azure Machine Learning studio as they are unable to find the API_Key for their web service and cannot use the Azure Machine Learning add-in for Excel. They are considering using Power BI or creating an Excel macro as a workaround.",
        "Question_gpt_summary":"user face challeng new studio unabl api kei web servic us add excel consid power creat excel macro workaround",
        "Answer_original_content":"thank reach new integr excel isn support time detail provid thread altern approach us client powerbi consum model futur refer webservic endpoint kei studio endpoint endpoint consum forget accept answer repli help thank",
        "Answer_preprocessed_content":"thank reach new integr excel isn support time detail provid thread altern approach us client powerbi consum model futur refer webservic endpoint kei studio endpoint endpoint consum forget accept answer repli help",
        "Answer_gpt_summary_original":"Solutions provided:\n- The new AzureML integration with Excel is not supported at this time. \n- The alternative approach would be to use a Client or PowerBI to consume the model. \n- For future reference, the user can find their webservice endpoint and keys under Studio > Endpoints > Endpoint > Consume.",
        "Answer_gpt_summary":"solut provid new integr excel support time altern approach us client powerbi consum model futur refer user webservic endpoint kei studio endpoint endpoint consum"
    },
    {
        "Question_title":"Why does my ML model deployment in Azure Container Instance still fail with \"current service state: Transitioning\"?",
        "Question_body":"<p>I am using Azure Machine Learning Service to deploy a ML model as web service.<\/p>\n<p>I <a href=\"https:\/\/stackoverflow.com\/a\/55281703\/4240413\">registered a <code>model<\/code><\/a> and now would like to deploy it as an ACI web service as in <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#aci\" rel=\"nofollow noreferrer\">the guide<\/a>.<\/p>\n<p>To do so I define<\/p>\n<pre><code>from azureml.core.webservice import Webservice, AciWebservice\nfrom azureml.core.image import ContainerImage\n\naciconfig = AciWebservice.deploy_configuration(cpu_cores=4, \n                      memory_gb=32, \n                      tags={&quot;data&quot;: &quot;text&quot;,  &quot;method&quot; : &quot;NB&quot;}, \n                      description='Predict something')\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>image_config = ContainerImage.image_configuration(execution_script=&quot;score.py&quot;, \n                      docker_file=&quot;Dockerfile&quot;,\n                      runtime=&quot;python&quot;, \n                      conda_file=&quot;myenv.yml&quot;)\n<\/code><\/pre>\n<p>and create an image with<\/p>\n<pre><code>image = ContainerImage.create(name = &quot;scorer-image&quot;,\n                      models = [model],\n                      image_config = image_config,\n                      workspace = ws\n                      )\n<\/code><\/pre>\n<p>Image creation succeeds with<\/p>\n<blockquote>\n<p>Creating image Image creation operation finished for image\nscorer-image:5, operation &quot;Succeeded&quot;<\/p>\n<\/blockquote>\n<p>Also, troubleshooting the image by running it locally on an Azure VM with<\/p>\n<pre><code>sudo docker run -p 8002:5001 myscorer0588419434.azurecr.io\/scorer-image:5\n<\/code><\/pre>\n<p>allows me to run (locally) queries successfully against <code>http:\/\/localhost:8002\/score<\/code>.<\/p>\n<p>However, deployment with<\/p>\n<pre><code>service_name = 'scorer-svc'\nservice = Webservice.deploy_from_image(deployment_config = aciconfig,\n                                        image = image,\n                                        name = service_name,\n                                        workspace = ws)\n<\/code><\/pre>\n<p>fails with<\/p>\n<blockquote>\n<p>Creating service<br \/>\nRunning.<br \/>\nFailedACI service creation operation finished, operation &quot;Failed&quot;<br \/>\nService creation polling reached terminal state, current service state: Transitioning<br \/>\nService creation polling reached terminal state, unexpected response received. Transitioning<\/p>\n<\/blockquote>\n<p>I tried setting in the <code>aciconfig<\/code> more generous <code>memory_gb<\/code>, but to no avail: the deployment stays in a <em>transitioning<\/em> state (like in the image below if monitored on the Azure portal):\n<a href=\"https:\/\/i.stack.imgur.com\/gCjI3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gCjI3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Also, running <code>service.get_logs()<\/code> gives me<\/p>\n<blockquote>\n<p>WebserviceException: Received bad response from Model Management\nService: Response Code: 404<\/p>\n<\/blockquote>\n<p>What could possibly be the culprit?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1553557082573,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1597618502196,
        "Question_score":4,
        "Question_view_count":3489.0,
        "Poster_id":4240413.0,
        "Poster_created_time":1415722650716,
        "Poster_last_access_time":1664051478172,
        "Poster_age":null,
        "Poster_location":"Verona, VR, Italy",
        "Poster_reputation":4811.0,
        "Poster_views":713.0,
        "Answer_body":"<p>If ACI deployment fails, one solution is trying to allocate <em>less<\/em> resources, e.g.<\/p>\n\n<pre><code>aciconfig = AciWebservice.deploy_configuration(cpu_cores=1, \n                  memory_gb=8, \n                  tags={\"data\": \"text\",  \"method\" : \"NB\"}, \n                  description='Predict something')\n<\/code><\/pre>\n\n<p>While the error messages thrown are not particularly informative, this is actually clearly stated in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-instances\/container-instances-region-availability\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>When a region is under heavy load, you may experience a failure when\n  deploying instances. To mitigate such a deployment failure, try\n  deploying instances with lower resource settings [...]<\/p>\n<\/blockquote>\n\n<p>The documentation also states which are the maximum values of the CPU\/RAM resources available in the different regions (at the time of writing, requiring a deployment with <code>memory_gb=32<\/code> would likely fail in all regions because of insufficient resources).<\/p>\n\n<p>Upon requiring less resources, deployment should succeed with <\/p>\n\n<blockquote>\n  <p>Creating service<br>\n  Running......................................................<br>\n  SucceededACI service creation operation finished, operation<br>\n  \"Succeeded\" Healthy<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1554115865523,
        "Answer_score":3.0,
        "Answerer_id":4240413.0,
        "Answerer_created_time":1415722650716,
        "Answerer_last_access_time":1664051478172,
        "Answerer_age":null,
        "Answerer_location":"Verona, VR, Italy",
        "Answerer_reputation":4811.0,
        "Answerer_views":713.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55347910",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1553818018048,
        "Question_original_content":"model deploy azur contain instanc fail current servic state transit servic deploi model web servic regist model like deploi aci web servic guid defin core webservic import webservic aciwebservic core imag import containerimag aciconfig aciwebservic deploi configur cpu core memori tag data text method descript predict imag config containerimag imag configur execut script score docker file dockerfil runtim python conda file myenv yml creat imag imag containerimag creat scorer imag model model imag config imag config workspac imag creation succe creat imag imag creation oper finish imag scorer imag oper succeed troubleshoot imag run local azur sudo docker run myscor azurecr scorer imag allow run local queri successfulli http localhost score deploy servic scorer svc servic webservic deploi imag deploy config aciconfig imag imag servic workspac fail creat servic run failedaci servic creation oper finish oper fail servic creation poll reach termin state current servic state transit servic creation poll reach termin state unexpect respons receiv transit tri set aciconfig gener memori avail deploy stai transit state like imag monitor azur portal run servic log give webserviceexcept receiv bad respons model manag servic respons code possibl culprit",
        "Question_preprocessed_content":"model deploy azur contain instanc fail current servic state transit servic deploi model web servic regist like deploi aci web servic guid defin creat imag imag creation succe creat imag imag creation oper finish imag oper succeed troubleshoot imag run local azur allow run queri successfulli deploy fail creat servic run failedaci servic creation oper finish oper fail servic creation poll reach termin state current servic state transit servic creation poll reach termin state unexpect respons receiv transit tri set gener avail deploy stai transit state run give webserviceexcept receiv bad respons model manag servic respons code possibl culprit",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a machine learning model as a web service using Azure Machine Learning Service. The image creation is successful, and the image can be run locally, but the deployment fails with a \"transitioning\" state. The user has tried increasing the memory_gb in the aciconfig, but the deployment still fails. Additionally, running service.get_logs() gives a WebserviceException with a response code of 404.",
        "Question_gpt_summary":"user face challeng deploi machin learn model web servic servic imag creation success imag run local deploy fail transit state user tri increas memori aciconfig deploy fail addition run servic log give webserviceexcept respons code",
        "Answer_original_content":"aci deploy fail solut try alloc resourc aciconfig aciwebservic deploi configur cpu core memori tag data text method descript predict error messag thrown particularli inform actual clearli state document region heavi load experi failur deploi instanc mitig deploy failur try deploi instanc lower resourc set document state maximum valu cpu ram resourc avail differ region time write requir deploy memori like fail region insuffici resourc requir resourc deploy succe creat servic run succeededaci servic creation oper finish oper succeed healthi",
        "Answer_preprocessed_content":"aci deploy fail solut try alloc resourc error messag thrown particularli inform actual clearli state document region heavi load experi failur deploi instanc mitig deploy failur try deploi instanc lower resourc set document state maximum valu resourc avail differ region requir resourc deploy succe creat servic succeededaci servic creation oper finish oper succeed healthi",
        "Answer_gpt_summary_original":"One solution suggested in the discussion is to allocate less resources when deploying the machine learning model as a web service using Azure Machine Learning Service. The documentation states that when a region is under heavy load, deploying instances with lower resource settings can mitigate deployment failure. The user can try deploying instances with lower resource settings, such as reducing the memory_gb value in the aciconfig. This solution has been successful in resolving the deployment failure issue.",
        "Answer_gpt_summary":"solut suggest discuss alloc resourc deploi machin learn model web servic servic document state region heavi load deploi instanc lower resourc set mitig deploy failur user try deploi instanc lower resourc set reduc memori valu aciconfig solut success resolv deploy failur issu"
    },
    {
        "Question_title":"How to update the labelled tags in the azure machine learning ?",
        "Question_body":"![151940-test-img.png][1]\n[1]: \/answers\/storage\/attachments\/151940-test-img.png\n\nHow can I edit the existing tags and it will update to the tags in labelled images.\nFor example :\nEdit the tags['test1'] to new tag['Breeze'] and the tags['test1] in the image will replace with the new tag ['Breeze']\nHow can I do it in the azure UI or by using python",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1637728063797,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@ZiXiangYan-6877 You can edit the tags using the Details tab -> Label Classes screen of your project from the portal. If the project is in paused state you can add new labels and choose the required option to continue or start over by keeping existing labels or removing all labels and relabel.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/638862\/how-to-update-the-labelled-tags-in-the-azure-machi.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1637743807056,
        "Question_original_content":"updat label tag test img png answer storag attach test img png edit exist tag updat tag label imag exampl edit tag test new tag breez tag test imag replac new tag breez azur python",
        "Question_preprocessed_content":"updat label tag edit exist tag updat tag label imag exampl edit tag new tag tag imag replac new tag azur python",
        "Question_gpt_summary_original":"The user is facing challenges in updating labelled tags in Azure Machine Learning. They are seeking guidance on how to edit existing tags and update them in labelled images, either through the Azure UI or by using Python.",
        "Question_gpt_summary":"user face challeng updat label tag seek guidanc edit exist tag updat label imag azur python",
        "Answer_original_content":"zixiangyan edit tag detail tab label class screen project portal project paus state add new label choos requir option continu start keep exist label remov label relabel answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"edit tag detail tab label class screen project portal project paus state add new label choos requir option continu start keep exist label remov label relabel answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Solution: The user can edit the tags using the Details tab -> Label Classes screen of their project from the Azure portal. If the project is in paused state, they can add new labels and choose the required option to continue or start over by keeping existing labels or removing all labels and relabel. No Python solution is mentioned in the discussion.",
        "Answer_gpt_summary":"solut user edit tag detail tab label class screen project azur portal project paus state add new label choos requir option continu start keep exist label remov label relabel python solut mention discuss"
    },
    {
        "Question_title":"SageMaker Inference for a video input",
        "Question_body":"<p>I wonder if it's possible to run SageMaker Inference or Batch Transform job directly for a video input (.mp4 or another format)?<\/p>\n<p>If no could you please advice the best practice that might be used for pre-processing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1654268345127,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1654268469487,
        "Question_score":0,
        "Question_view_count":93.0,
        "Poster_id":5357103.0,
        "Poster_created_time":1442786553536,
        "Poster_last_access_time":1664039462827,
        "Poster_age":null,
        "Poster_location":"Kyiv",
        "Poster_reputation":71.0,
        "Poster_views":30.0,
        "Answer_body":"<p>Asynchronous inference could be a good option for this use case. There is a blog published by AWS that talks about how you can do this.<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-computer-vision-inference-on-large-videos-with-amazon-sagemaker-asynchronous-endpoints\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":16247336.0,
        "Answerer_created_time":1623879163643,
        "Answerer_last_access_time":1663856103556,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":224.0,
        "Answerer_views":40.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72491505",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1654270724136,
        "Question_original_content":"infer video input wonder possibl run infer batch transform job directli video input format advic best practic pre process",
        "Question_preprocessed_content":"infer video input wonder possibl run infer batch transform job directli video input advic best practic",
        "Question_gpt_summary_original":"The user is seeking advice on whether it is possible to run SageMaker Inference or Batch Transform job directly for a video input and if not, they are asking for the best practice for pre-processing.",
        "Question_gpt_summary":"user seek advic possibl run infer batch transform job directli video input ask best practic pre process",
        "Answer_original_content":"asynchron infer good option us case blog publish aw talk http aw amazon com blog machin learn run vision infer larg video amazon asynchron endpoint",
        "Answer_preprocessed_content":"asynchron infer good option us case blog publish aw talk",
        "Answer_gpt_summary_original":"Solution: Asynchronous inference could be a good option for running SageMaker Inference or Batch Transform job directly for a video input. AWS has published a blog that talks about how to do this.",
        "Answer_gpt_summary":"solut asynchron infer good option run infer batch transform job directli video input aw publish blog talk"
    },
    {
        "Question_title":"How to catalog datasets & models by S3 URI, but keep a local copy?",
        "Question_body":"<p>I'm trying to figure out how to store intermediate Kedro pipeline objects both locally AND on S3. In particular, say I have a dataset on S3:<\/p>\n<pre><code>my_big_dataset.hdf5:\n  type: kedro.extras.datasets.pandas.HDFDataSet\n  filepath: &quot;s3:\/\/my_bucket\/data\/04_feature\/my_big_dataset.hdf5&quot;\n<\/code><\/pre>\n<p>I want to refer to these objects in the catalog by their S3 URI so that my team can use them. HOWEVER, I want to avoid re-downloading the datasets, model weights, etc. every time I run a pipeline by keeping a local copy in addition to the S3 copy. How do I mirror files with Kedro?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1597008515007,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597058318400,
        "Question_score":2,
        "Question_view_count":595.0,
        "Poster_id":4212158.0,
        "Poster_created_time":1415053264667,
        "Poster_last_access_time":1663642907372,
        "Poster_age":null,
        "Poster_location":"USA",
        "Poster_reputation":11166.0,
        "Poster_views":653.0,
        "Answer_body":"<p>This is a good question, Kedro has <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> for caching datasets within the same run, which handles caching the dataset in memory when it's used\/loaded multiple times in the same run. There isn't really the same thing that persists across runs, in general Kedro doesn't do much persistent stuff.<\/p>\n<p>That said, off the top of my head, I can think of two options that (mostly) replicates or gives this functionality:<\/p>\n<ol>\n<li>Use the same <code>catalog<\/code> in the same config environment but with the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_kedro_project_setup\/02_configuration.html?#templating-configuration\" rel=\"nofollow noreferrer\"><code>TemplatedConfigLoader<\/code><\/a> where your catalog datasets have their filepaths looking something like:<\/li>\n<\/ol>\n<pre><code>my_dataset:\n  filepath: ${base_data}\/01_raw\/blah.csv\n<\/code><\/pre>\n<p>and you set <code>base_data<\/code> to <code>s3:\/\/bucket\/blah<\/code> when running in &quot;production&quot; mode and with <code>local_filepath\/data<\/code> locally. You can decide how exactly you do this in your overriden <code>context<\/code> method (whether it's using <code>local\/globals.yml<\/code> (see the linked documentation above) or environment variables or what not.<\/p>\n<ol start=\"2\">\n<li>Use separate environments, likely <code>local<\/code> (it's kind of what it was made for!) where you keep a separate copy of your catalog where the filepaths are replaced with local ones.<\/li>\n<\/ol>\n<p>Otherwise, your next best bet is to write a <code>PersistentCachedDataSet<\/code> similar to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> which intercepts the loading\/saving for the wrapped dataset and makes a local copy when loading for the first time in a deterministic location that you look up on subsequent loads.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Answerer_id":9999106.0,
        "Answerer_created_time":1530092504712,
        "Answerer_last_access_time":1652101318888,
        "Answerer_age":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation":915.0,
        "Answerer_views":288.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63331505",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1597010598252,
        "Question_original_content":"catalog dataset model uri local copi try figur store intermedi pipelin object local particular dataset big dataset hdf type extra dataset panda hdfdataset filepath bucket data featur big dataset hdf want refer object catalog uri team us want avoid download dataset model weight time run pipelin keep local copi addit copi mirror file",
        "Question_preprocessed_content":"catalog dataset model uri local copi try figur store intermedi pipelin object local particular dataset want refer object catalog uri team us want avoid dataset model weight time run pipelin keep local copi addit copi mirror file",
        "Question_gpt_summary_original":"The user is facing a challenge in storing intermediate Kedro pipeline objects both locally and on S3. They want to refer to the objects in the catalog by their S3 URI to enable their team to use them, but also want to avoid re-downloading the datasets and model weights every time they run a pipeline by keeping a local copy in addition to the S3 copy. The user is seeking guidance on how to mirror files with Kedro.",
        "Question_gpt_summary":"user face challeng store intermedi pipelin object local want refer object catalog uri enabl team us want avoid download dataset model weight time run pipelin keep local copi addit copi user seek guidanc mirror file",
        "Answer_original_content":"good question cacheddataset cach dataset run handl cach dataset memori load multipl time run isn thing persist run gener persist stuff said head think option replic give function us catalog config environ templatedconfigload catalog dataset filepath look like dataset filepath base data raw blah csv set base data bucket blah run product mode local filepath data local decid exactli overriden context method local global yml link document environ variabl us separ environ like local kind separ copi catalog filepath replac local on best bet write persistentcacheddataset similar cacheddataset intercept load save wrap dataset make local copi load time determinist locat look subsequ load",
        "Answer_preprocessed_content":"good question cach dataset run handl cach dataset memori multipl time run isn thing persist run gener persist stuff said head think option replic give function us config environ catalog dataset filepath look like set run product mode local decid exactli overriden method environ variabl us separ environ like separ copi catalog filepath replac local on best bet write similar intercept wrap dataset make local copi load time determinist locat look subsequ load",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use the same catalog in the same config environment but with the TemplatedConfigLoader where the catalog datasets have their filepaths looking something like: `my_dataset: filepath: ${base_data}\/01_raw\/blah.csv` and set `base_data` to `s3:\/\/bucket\/blah` when running in \"production\" mode and with `local_filepath\/data` locally.\n2. Use separate environments, likely `local`, where you keep a separate copy of your catalog where the filepaths are replaced with local ones.\n3. Write a `PersistentCachedDataSet` similar to `CachedDataSet` which intercepts the loading\/saving for the wrapped dataset and makes a local copy when loading for the first",
        "Answer_gpt_summary":"possibl solut mention discuss us catalog config environ templatedconfigload catalog dataset filepath look like dataset filepath base data raw blah csv set base data bucket blah run product mode local filepath data local us separ environ like local separ copi catalog filepath replac local on write persistentcacheddataset similar cacheddataset intercept load save wrap dataset make local copi load"
    },
    {
        "Question_title":"How to deploy a Pre-Trained model using AWS SageMaker Notebook Instance?",
        "Question_body":"<p>I have a pre-trained model which I am loading in AWS SageMaker Notebook Instance from S3 Bucket and upon providing a test image for prediction from S3 bucket it gives me the accurate results as required. I want to deploy it so that I can have an endpoint which I can further integrate with AWS Lambda Function and AWS API GateWay so that I can use the model with real time application.\nAny idea how can I deploy the model from AWS Sagemaker Notebook Instance and get its endpoint?\nCode inside the <code>.ipynb<\/code> file is given below for reference.<\/p>\n<pre><code>import boto3\nimport pandas as pd\nimport sagemaker\n#from sagemaker import get_execution_role\nfrom skimage.io import imread\nfrom skimage.transform import resize\nimport numpy as np\nfrom keras.models import load_model\nimport os\nimport time\nimport json\n#role = get_execution_role()\nrole = sagemaker.get_execution_role()\n\nbucketname = 'bucket' # bucket where the model is hosted\nfilename = 'test_model.h5' # name of the model\ns3 = boto3.resource('s3')\nimage= s3.Bucket(bucketname).download_file(filename, 'test_model_new.h5')\nmodel= 'test_model_new.h5'\n\nmodel = load_model(model)\n\nbucketname = 'bucket' # name of the bucket where the test image is hosted\nfilename = 'folder\/image.png' # prefix\ns3 = boto3.resource('s3')\nfile= s3.Bucket(bucketname).download_file(filename, 'image.png')\nfile_name='image.png'\n\ntest=np.array([resize(imread(file_name), (137, 310, 3))])\n\ntest_predict = model.predict(test)\n\nprint ((test_predict &gt; 0.5).astype(np.int))\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1608787488573,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3,
        "Question_view_count":1387.0,
        "Poster_id":9920934.0,
        "Poster_created_time":1528629350990,
        "Poster_last_access_time":1663824563776,
        "Poster_age":null,
        "Poster_location":"Pakistan",
        "Poster_reputation":439.0,
        "Poster_views":51.0,
        "Answer_body":"<p>Here is the solution that worked for me. Simply follow the following steps.<\/p>\n<p>1 - Load your model in the SageMaker's jupyter environment with the help of<\/p>\n<pre><code>from keras.models import load_model\n\nmodel = load_model (&lt;Your Model name goes here&gt;) #In my case it's model.h5\n<\/code><\/pre>\n<p>2 - Now that the model is loaded convert it into the <code>protobuf format<\/code> that is required by <code>AWS<\/code> with the help of<\/p>\n<pre><code>def convert_h5_to_aws(loaded_model):\n\nfrom tensorflow.python.saved_model import builder\nfrom tensorflow.python.saved_model.signature_def_utils import predict_signature_def\nfrom tensorflow.python.saved_model import tag_constants\n\nmodel_version = '1'\nexport_dir = 'export\/Servo\/' + model_version\n# Build the Protocol Buffer SavedModel at 'export_dir'\nbuilder = builder.SavedModelBuilder(export_dir)\n# Create prediction signature to be used by TensorFlow Serving Predict API\nsignature = predict_signature_def(\n    inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;: loaded_model.output})\nfrom keras import backend as K\n\nwith K.get_session() as sess:\n    # Save the meta graph and variables\n    builder.add_meta_graph_and_variables(\n        sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n    builder.save()\nimport tarfile\nwith tarfile.open('model.tar.gz', mode='w:gz') as archive:\n    archive.add('export', recursive=True)\nimport sagemaker\n\nsagemaker_session = sagemaker.Session()\ninputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\nconvert_h5_to_aws(model):\n<\/code><\/pre>\n<p>3 - And now you can deploy your model with the help of<\/p>\n<pre><code>!touch train.py\nfrom sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = '1.15.2',\n                                  entry_point = 'train.py')\n%%timelog\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>This will generate the endpoint which can be seen in the Inference section of the Amazon SageMaker and with the help of that endpoint you can now make predictions from the jupyter notebook as well as from web and mobile applications.\nThis <a href=\"https:\/\/www.youtube.com\/watch?v=RPnvfxR5DY8\" rel=\"nofollow noreferrer\">Youtube tutorial<\/a> by Liam and <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">AWS blog<\/a> by Priya helped me alot.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1629005164648,
        "Answer_score":3.0,
        "Answerer_id":9920934.0,
        "Answerer_created_time":1528629350990,
        "Answerer_last_access_time":1663824563776,
        "Answerer_age":null,
        "Answerer_location":"Pakistan",
        "Answerer_reputation":439.0,
        "Answerer_views":51.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65434323",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1616234632040,
        "Question_original_content":"deploi pre train model notebook instanc pre train model load notebook instanc bucket provid test imag predict bucket give accur result requir want deploi endpoint integr aw lambda function aw api gatewai us model real time applic idea deploi model notebook instanc endpoint code insid ipynb file given refer import boto import panda import import execut role skimag import imread skimag transform import resiz import numpi kera model import load model import import time import json role execut role role execut role bucketnam bucket bucket model host filenam test model model boto resourc imag bucket bucketnam download file filenam test model new model test model new model load model model bucketnam bucket bucket test imag host filenam folder imag png prefix boto resourc file bucket bucketnam download file filenam imag png file imag png test arrai resiz imread file test predict model predict test print test predict astyp int",
        "Question_preprocessed_content":"deploi model notebook instanc model load notebook instanc bucket provid test imag predict bucket give accur result requir want deploi endpoint integr aw lambda function aw api gatewai us model real time applic idea deploi model notebook instanc endpoint code insid file given refer",
        "Question_gpt_summary_original":"The user has a pre-trained model that they have loaded into an AWS SageMaker Notebook Instance from an S3 bucket. They are able to get accurate results when providing a test image for prediction from the S3 bucket. However, they are facing a challenge in deploying the model to get an endpoint that can be integrated with AWS Lambda Function and AWS API Gateway for real-time application use. The user has provided code for reference.",
        "Question_gpt_summary":"user pre train model load notebook instanc bucket abl accur result provid test imag predict bucket face challeng deploi model endpoint integr aw lambda function aw api gatewai real time applic us user provid code refer",
        "Answer_original_content":"solut work simpli follow follow step load model jupyt environ help kera model import load model model load model case model model load convert protobuf format requir aw help def convert aw load model tensorflow python save model import builder tensorflow python save model signatur def util import predict signatur def tensorflow python save model import tag constant model version export dir export servo model version build protocol buffer savedmodel export dir builder builder savedmodelbuild export dir creat predict signatur tensorflow serv predict api signatur predict signatur def input input load model input output score load model output kera import backend session sess save meta graph variabl builder add meta graph variabl sess sess tag tag constant serv signatur def map serv default signatur builder save import tarfil tarfil open model tar mode archiv archiv add export recurs true import session session input session upload data path model tar kei prefix model convert aw model deploi model help touch train tensorflow model import tensorflowmodel model tensorflowmodel model data session default bucket model model tar role role framework version entri point train timelog predictor model deploi initi instanc count instanc type xlarg gener endpoint seen infer section help endpoint predict jupyt notebook web mobil applic youtub tutori liam aw blog priya help alot",
        "Answer_preprocessed_content":"solut work simpli follow follow step load model jupyt environ help model load convert requir help deploi model help gener endpoint seen infer section help endpoint predict jupyt notebook web mobil applic youtub tutori liam aw blog priya help alot",
        "Answer_gpt_summary_original":"Solution:\n1. Load the pre-trained model in the SageMaker's jupyter environment using the Keras library.\n2. Convert the loaded model into the protobuf format required by AWS using the TensorFlow library.\n3. Deploy the model using the TensorFlowModel class from the sagemaker.tensorflow.model library and generate an endpoint that can be used for real-time application use.",
        "Answer_gpt_summary":"solut load pre train model jupyt environ kera librari convert load model protobuf format requir aw tensorflow librari deploi model tensorflowmodel class tensorflow model librari gener endpoint real time applic us"
    },
    {
        "Question_title":"Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo",
        "Question_body":"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.<\/p>\n<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull\/update&quot;) on my local project of repository A.<\/p>\n<p>The problem comes when I use colab to run my project. So what I did was the following:<\/p>\n<ol>\n<li>Created a new notebook on colab<\/li>\n<li>Successfully git-cloned my machine learning project (repository A)<\/li>\n<li>Ran &quot;!pip install dvc&quot;<\/li>\n<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)<\/li>\n<\/ol>\n<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)<\/p>\n<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/config.local' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/tmp' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/cache' to gitignore file.\n2022-03-08 08:53:31,916 DEBUG: Creating external repo https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75\n2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to a temporary dir\nEverything is up to date.\n2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 185, in clone\n    tmp_repo = clone_from()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1148, in clone_from\n    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1079, in _clone\n    finalize_process, decode_streams=False)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 176, in handle_process_output\n    return finalizer(process)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/util.py&quot;, line 386, in finalize_process\n    proc.wait(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 502, in wait\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v --no-single-branch --progress https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git \/tmp\/tmp2x7y7xgedvc-clone\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 104, in clone\n    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/__init__.py&quot;, line 121, in clone\n    backend.clone(url, to_path, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 190, in clone\n    raise CloneError(url, to_path) from exc\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/pull.py&quot;, line 38, in pull\n    run_cache=run_cache,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/fetch.py&quot;, line 50, in fetch\n    revs=revs,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 437, in used_objs\n    with_deps=with_deps,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/index.py&quot;, line 190, in used_objs\n    filter_info=filter_info,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/stage\/__init__.py&quot;, line 660, in get_used_objs\n    for odb, objs in out.get_used_objs(*args, **kwargs).items():\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 918, in get_used_objs\n    return self.get_used_external(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 973, in get_used_external\n    return dep.get_used_objs(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 94, in get_used_objs\n    used, _ = self._get_used_and_obj(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 108, in _get_used_and_obj\n    locked=locked, cache_dir=local_odb.cache_dir\n  File &quot;\/usr\/lib\/python3.7\/contextlib.py&quot;, line 112, in __enter__\n    return next(self.gen)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 35, in external_repo\n    path = _cached_clone(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 155, in _cached_clone\n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/flow.py&quot;, line 274, in wrap_with\n    return call()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 220, in _clone_default_branch\n    git = clone(url, clone_path)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 106, in clone\n    raise CloneError(str(exc))\ndvc.scm.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\n2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.\n2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n<\/code><\/pre>\n<p>And btw this is how I cloned my git repository (repo A)<\/p>\n<pre><code>!git config - global user.name &quot;Zharfan&quot;\n!git config - global user.email &quot;zharfan@myemail.com&quot;\n!git clone https:\/\/&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git\n<\/code><\/pre>\n<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":12,
        "Question_created_time":1646641948613,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1652856778060,
        "Question_score":1,
        "Question_view_count":707.0,
        "Poster_id":9728121.0,
        "Poster_created_time":1525227015312,
        "Poster_last_access_time":1663842020310,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":53.0,
        "Poster_views":11.0,
        "Answer_body":"<p>To summarize the discussion in the comments thread.<\/p>\n<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)<\/p>\n<p>The same way you would not be able to run:<\/p>\n<pre><code>!git clone https:\/\/gitlab.com\/org\/&lt;private-repo&gt;\n<\/code><\/pre>\n<p>It also returns a pretty obscure error:<\/p>\n<pre><code>Cloning into '&lt;private-repo&gt;'...\nfatal: could not read Username for 'https:\/\/gitlab.com': No such device or address\n<\/code><\/pre>\n<p>(I think it's something related to how tty is setup in Colab?)<\/p>\n<p>The best approach to solve this is to use SSH like described <a href=\"https:\/\/medium.com\/@sadiaafrinpurba\/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f\" rel=\"nofollow noreferrer\">here<\/a> for example.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":298182.0,
        "Answerer_created_time":1250347954880,
        "Answerer_last_access_time":1664073072270,
        "Answerer_age":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation":5575.0,
        "Answerer_views":358.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71378280",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_closed_time":1647022114532,
        "Question_original_content":"error googl colab scm cloneerror fail clone repo have problem try run pull googl colab repositori let repositori machin learn code repositori dataset successfulli push dataset repositori gdrive remot storag manag successfulli run import pull updat local project repositori problem come us colab run project follow creat new notebook colab successfulli git clone machin learn project repositori ran pip instal ran pull caus error step got error stack trace note chang repo url stack trace confidenti reason debug ad content config local gitignor file debug ad content tmp gitignor file debug ad content cach gitignor file debug creat extern repo http gitlab com git afefabffecdabdbd debug erepo git clone http gitlab com git temporari dir date error fail pull data cloud fail clone repo http gitlab com git tmp tmpxyxg clone traceback recent file usr local lib python dist packag scmrepo git backend gitpython line clone tmp repo clone file usr local lib python dist packag git repo base line clone return cl clone git url path gitcmdobjectdb progress multi option kwarg file usr local lib python dist packag git repo base line clone final process decod stream fals file usr local lib python dist packag git cmd line handl process output return final process file usr local lib python dist packag git util line final process proc wait kwarg file usr local lib python dist packag git cmd line wait rais gitcommanderror remov password present self arg statu errstr git exc gitcommanderror cmd git fail exit code cmdline git clone singl branch progress http gitlab com git tmp tmpxyxg clone except direct caus follow except traceback recent file usr local lib python dist packag scm line clone return git clone url path progress pbar updat git kwarg file usr local lib python dist packag scmrepo git init line clone backend clone url path kwarg file usr local lib python dist packag scmrepo git backend gitpython line clone rais cloneerror url path exc scmrepo except cloneerror fail clone repo http gitlab com git tmp tmpxyxg clone handl except except occur traceback recent file usr local lib python dist packag command data sync line run glob self arg glob file usr local lib python dist packag repo init line wrapper return repo arg kwarg file usr local lib python dist packag repo pull line pull run cach run cach file usr local lib python dist packag repo init line wrapper return repo arg kwarg file usr local lib python dist packag repo fetch line fetch rev rev file usr local lib python dist packag repo init line obj dep dep file usr local lib python dist packag repo index line obj filter info filter info file usr local lib python dist packag stage init line obj odb obj obj arg kwarg item file usr local lib python dist packag output line obj return self extern kwarg file usr local lib python dist packag output line extern return dep obj kwarg file usr local lib python dist packag depend repo line obj self obj kwarg file usr local lib python dist packag depend repo line obj lock lock cach dir local odb cach dir file usr lib python contextlib line enter return self gen file usr local lib python dist packag extern repo line extern repo path cach clone url rev write write file usr local lib python dist packag extern repo line cach clone clone path shallow clone default branch url rev write write file usr local lib python dist packag funci decor line wrapper return deco darg dkwarg file usr local lib python dist packag funci flow line wrap return file usr local lib python dist packag funci decor line return self func self arg self kwarg file usr local lib python dist packag extern repo line clone default branch git clone url clone path file usr local lib python dist packag scm line clone rais cloneerror str exc scm cloneerror fail clone repo http gitlab com git tmp tmpxyxg clone debug analyt enabl debug try spawn daemon analyt tmp tmpxjsdk debug spawn daemon analyt tmp tmpxjsdk btw clone git repositori repo git config global user zharfan git config global user email zharfan myemail com git clone http link repo git know help greatli appreci thank advanc",
        "Question_preprocessed_content":"error googl colab fail clone repo have problem try run pull googl colab repositori repositori machin learn code repositori dataset successfulli push dataset repositori manag successfulli run import local project repositori problem come us colab run project follow creat new notebook colab successfulli machin learn project ran pip instal ran pull step got error btw clone git repositori know help greatli appreci thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error while trying to run \"dvc pull\" on Google Colab. The user has two repositories, A and B, where A is for machine learning codes and B is for the dataset. The user has successfully pushed the dataset to repository B with DVC and also managed to run \"dvc import\" on the local project of repository A. However, when the user tries to run \"dvc pull\" on Colab, an error occurs. The error message indicates that the repository B failed to clone to '\/tmp\/tmp2x7y7xgedvc-clone'. The user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error try run pull googl colab user repositori machin learn code dataset user successfulli push dataset repositori manag run import local project repositori user tri run pull colab error occur error messag indic repositori fail clone tmp tmpxyxg clone user seek help resolv issu",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"SageMaker Batch Transform fails with ID Column",
        "Question_body":"<p>I am using SageMaker pipeline to do inference on test data. The Pipeline uses a SKLearn perprocessor and a XGBoost model. The pipeline works fine on data without an ID column. However, when I try to include an ID column to track the predictions, it fails. I have given the code snippets below.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker.predictor import json_serializer, csv_serializer, json_deserializer\n\ninput_data_path = 's3:\/\/batch-transform\/input-data\/validation_data.csv'\noutput_data_path = 's3:\/\/batch-transform\/predictions\/'\n\ntransform_job = sagemaker.transformer.Transformer(\n    model_name = model_name,\n    instance_count = 1,\n    instance_type = 'ml.m4.xlarge',\n    strategy = 'MultiRecord',\n    assemble_with = 'Line',\n    output_path = output_data_path,\n    base_transform_job_name='pipeline_with_id',\n    sagemaker_session=sagemaker.Session(),\n    accept = 'text\/csv')\n\ntransform_job.transform(data = input_data_path,\n                        content_type = 'text\/csv', \n                        split_type = 'Line',\n                        input_filter='$[1:]', \n                        join_source='Input')\n                        output_filter='$[0,-1]')\n<\/code><\/pre>\n<p>This results in the following error:<\/p>\n<pre><code>Fail to join data: mismatched line count between the input and the output\n<\/code><\/pre>\n<p>I am following the example given in this page:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/associating-prediction-results-with-input-data-using-amazon-sagemaker-batch-transform\/<\/a><\/p>\n<p>Can someone provide pointers to what is causing the error? Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1605472910920,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":1080.0,
        "Poster_id":1002903.0,
        "Poster_created_time":1319019150600,
        "Poster_last_access_time":1662714723143,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":3073.0,
        "Poster_views":341.0,
        "Answer_body":"<p>Came across the same issue.<\/p>\n<p>Check the number of rows returned after prediction in your serving code. In my case, my prediction output didn't have a column header.<\/p>\n<p>e.g. As a text\/csv response, using batch transform with join will post join the input &amp; output.<\/p>\n<p>A single input record would be [[&quot;feature_1&quot;, &quot;feature_2&quot;],[0, 1]], while my model predicted output returned [1].<\/p>\n<p>add column name to predicted output like this [&quot;result&quot;, 1] then returning the csv result will yield [[&quot;result&quot;],[1]] matching input.<\/p>\n<p>P.S. you may need to find a scalable way of doing this for multi-row  batch. Not sure.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":15011512.0,
        "Answerer_created_time":1610703423912,
        "Answerer_last_access_time":1643179669747,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":36.0,
        "Answerer_views":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64849557",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1610703448710,
        "Question_original_content":"batch transform fail column pipelin infer test data pipelin us sklearn perprocessor xgboost model pipelin work fine data column try includ column track predict fail given code snippet import predictor import json serial csv serial json deseri input data path batch transform input data valid data csv output data path batch transform predict transform job transform transform model model instanc count instanc type xlarg strategi multirecord assembl line output path output data path base transform job pipelin session session accept text csv transform job transform data input data path content type text csv split type line input filter join sourc input output filter result follow error fail join data mismatch line count input output follow exampl given page http aw amazon com blog machin learn associ predict result input data amazon batch transform provid pointer caus error thank",
        "Question_preprocessed_content":"batch transform fail column pipelin infer test data pipelin us sklearn perprocessor xgboost model pipelin work fine data column try includ column track predict fail given code snippet result follow error follow exampl given page provid pointer caus error thank",
        "Question_gpt_summary_original":"The user is encountering an error when using SageMaker Batch Transform to do inference on test data that includes an ID column to track predictions. The pipeline works fine on data without an ID column, but when an ID column is included, the error \"Fail to join data: mismatched line count between the input and the output\" occurs. The user is seeking help to identify the cause of the error.",
        "Question_gpt_summary":"user encount error batch transform infer test data includ column track predict pipelin work fine data column column includ error fail join data mismatch line count input output occur user seek help identifi caus error",
        "Answer_original_content":"came issu check number row return predict serv code case predict output column header text csv respons batch transform join post join input output singl input record featur featur model predict output return add column predict output like result return csv result yield result match input need scalabl wai multi row batch sure",
        "Answer_preprocessed_content":"came issu check number row return predict serv code case predict output column header respons batch transform join post join input output singl input record model predict output return add column predict output like return csv result yield match input need scalabl wai batch sure",
        "Answer_gpt_summary_original":"Solution: The cause of the error is the mismatched line count between the input and the output. One possible solution is to check the number of rows returned after prediction in the serving code and ensure that the predicted output has a column header. Adding a column name to the predicted output can help in returning the CSV result that matches the input. However, it may be necessary to find a scalable way of doing this for multi-row batch.",
        "Answer_gpt_summary":"solut caus error mismatch line count input output possibl solut check number row return predict serv code ensur predict output column header ad column predict output help return csv result match input necessari scalabl wai multi row batch"
    },
    {
        "Question_title":"Azure AutoML dependency failure",
        "Question_body":"<p><strong>Issue<\/strong> : Unable to get best model from AutoML run.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code>best_run, fitted_model = automl_run.get_output()\nprint(best_run.properties[&quot;run_algorithm&quot;])\n<\/code><\/pre>\n<p><strong>Error Message :<\/strong><\/p>\n<pre><code>ErrorResponse \n[stderr]{\n[stderr]    &quot;error&quot;: {\n[stderr]        &quot;code&quot;: &quot;UserError&quot;,\n[stderr]        &quot;message&quot;: &quot;The model you attempted to retrieve requires 'xgboost' to be installed at '==1.3.3'. You have 'xgboost==1.3.3', please reinstall 'xgboost==1.3.3' (e.g. `pip install xgboost==1.3.3`) and rerun the previous command.&quot;,\n[stderr]        &quot;target&quot;: &quot;get_output&quot;,\n[stderr]        &quot;inner_error&quot;: {\n[stderr]            &quot;code&quot;: &quot;NotSupported&quot;,\n[stderr]            &quot;inner_error&quot;: {\n[stderr]                &quot;code&quot;: &quot;IncompatibleOrMissingDependency&quot;\n[stderr]            }\n[stderr]        },\n[stderr]        &quot;reference_code&quot;: &quot;910310e6-2433-40cd-b597-9ec2950bc1d8&quot;\n[stderr]    }\n<\/code><\/pre>\n<p><strong>Conda Dependency<\/strong><\/p>\n<pre><code># Conda environment specification. The dependencies defined in this file will\n# be automatically provisioned for runs with userManagedDependencies=False.\n\n# Details about the Conda environment file format:\n# https:\/\/conda.io\/docs\/user-guide\/tasks\/manage-environments.html#create-env-file-manually\n\nname: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.12\n\n- pip:\n  - azureml-train-automl-runtime==1.38.0\n  - azureml-train-automl-client==1.38.0\n  - inference-schema\n  - azureml-interpret==1.38.0\n  - azureml-defaults==1.38.0\n- numpy&gt;=1.16.0,&lt;1.19.0\n- pandas==0.25.1\n- scikit-learn==0.22.1\n- py-xgboost&lt;=1.3.3\n- fbprophet==0.5\n- holidays==0.9.11\n- psutil&gt;=5.2.2,&lt;6.0.0\n- matplotlib=3.3.2\n- seaborn=0.9.0\n- joblib=0.13.2\n- joblib\nchannels:\n- anaconda\n- conda-forge\n<\/code><\/pre>\n<p><strong>Question:<\/strong><\/p>\n<ul>\n<li>What should be in my conda dependency that can fix this error<\/li>\n<li>I've tried making <code>py-xgboost==1.3.3<\/code> , but it didn't work.<\/li>\n<li>Any luck - how to fix this ?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1648153690787,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":142.0,
        "Poster_id":17977730.0,
        "Poster_created_time":1642621384680,
        "Poster_last_access_time":1662661734323,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":41.0,
        "Poster_views":11.0,
        "Answer_body":"<blockquote>\n<p>Error - &quot;The model you attempted to retrieve requires 'xgboost' to be\ninstalled at '==1.3.3'. You have 'xgboost==1.3.3', please reinstall\n'xgboost==1.3.3' (e.g. <code>pip install xgboost==1.3.3<\/code>) and rerun the\nprevious command.&quot;<\/p>\n<\/blockquote>\n<p>As given in above error message, it should be <code>pip install xgboost==1.3.3<\/code> not <code>py-xgboost&lt;=1.3.3<\/code><\/p>\n<p>If it does not work, try downgraded version of <code>xgboost<\/code><\/p>\n<pre><code>pip install xgboost==0.90\n<\/code><\/pre>\n<p>Refer this github <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1421\" rel=\"nofollow noreferrer\">link<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":11104805.0,
        "Answerer_created_time":1550902509267,
        "Answerer_last_access_time":1664081158016,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":2669.0,
        "Answerer_views":3292.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71609028",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1648186385707,
        "Question_original_content":"azur automl depend failur issu unabl best model automl run code best run fit model automl run output print best run properti run algorithm error messag errorrespons stderr stderr error stderr code usererror stderr messag model attempt retriev requir xgboost instal xgboost reinstal xgboost pip instal xgboost rerun previou command stderr target output stderr inner error stderr code notsupport stderr inner error stderr code incompatibleormissingdepend stderr stderr stderr refer code ecbcd stderr conda depend conda environ specif depend defin file automat provis run usermanageddepend fals detail conda environ file format http conda doc user guid task manag environ html creat env file manual project environ depend python interpret version current support later python pip train automl runtim train automl client infer schema interpret default numpi matplotlib seaborn joblib joblib channel anaconda conda forg question conda depend fix error tri make xgboost work luck fix",
        "Question_preprocessed_content":"azur automl depend failur issu unabl best model automl run code error messag conda depend question conda depend fix error tri make work luck fix",
        "Question_gpt_summary_original":"The user encountered an error while trying to retrieve the best model from an Azure AutoML run. The error message indicates that the model requires 'xgboost' to be installed at '==1.3.3', but the user has 'xgboost==1.3.3' installed. The user is seeking advice on how to fix this error and what should be included in the conda dependency to resolve the issue.",
        "Question_gpt_summary":"user encount error try retriev best model azur automl run error messag indic model requir xgboost instal user xgboost instal user seek advic fix error includ conda depend resolv issu",
        "Answer_original_content":"error model attempt retriev requir xgboost instal xgboost reinstal xgboost pip instal xgboost rerun previou command given error messag pip instal xgboost xgboost work try downgrad version xgboost pip instal xgboost refer github link",
        "Answer_preprocessed_content":"error model attempt retriev requir xgboost instal reinstal rerun previou given error messag work try downgrad version refer github link",
        "Answer_gpt_summary_original":"Solution:\n- Reinstall 'xgboost==1.3.3' using pip install xgboost==1.3.3 and rerun the previous command.\n- If the above solution does not work, try downgrading the version of 'xgboost' to 0.90.\n- Refer to the GitHub link provided for more information.",
        "Answer_gpt_summary":"solut reinstal xgboost pip instal xgboost rerun previou command solut work try downgrad version xgboost refer github link provid inform"
    },
    {
        "Question_title":"Web service input into SQL query into R in Azure ML",
        "Question_body":"<p>I have the following simple setup in Azure ML. <a href=\"https:\/\/i.stack.imgur.com\/kWi0S.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kWi0S.jpg\" alt=\"ML setup\"><\/a> \nBasically the Reader is a SQL query to a DB which returns a vector called Pdelta, which is then passed to the R script for further processing  and the results are then returned back to the web service. The DB query is simple (<code>SELECT Pdelta FROM ...<\/code>) and it works fine. I have set the DB query as a web service paramater as well. <\/p>\n\n<p>Everything seems to work fine, but at the end when i publish it as a web service and test it, it somehow asks for an additional input parameter. The additional parameter gets called <code>PDELTA<\/code>.\n<a href=\"https:\/\/i.stack.imgur.com\/mnzPZ.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mnzPZ.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I am wondering why is this happening, what is it that I am overlooking? I would like to make this web service ask for only one parameter - the SQL query (Delta Query) which would then deliver the Pdeltas. <\/p>\n\n<p>Any ideas or suggestions would be grealty appreciated! <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1443692439630,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1456850075240,
        "Question_score":3,
        "Question_view_count":347.0,
        "Poster_id":4950019.0,
        "Poster_created_time":1432829415467,
        "Poster_last_access_time":1542573864587,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":501.0,
        "Poster_views":76.0,
        "Answer_body":"<p>You can remove the web service input block and publish the web service without it. That way the Pdelta input will be passed in only from the Reader module.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":5414914.0,
        "Answerer_created_time":1444147981107,
        "Answerer_last_access_time":1493389129992,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":46.0,
        "Answerer_views":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32884296",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1444147981107,
        "Question_original_content":"web servic input sql queri follow simpl setup basic reader sql queri return vector call pdelta pass script process result return web servic queri simpl select pdelta work fine set queri web servic paramat work fine end publish web servic test ask addit input paramet addit paramet get call pdelta wonder happen overlook like web servic ask paramet sql queri delta queri deliv pdelta idea suggest grealti appreci",
        "Question_preprocessed_content":"web servic input sql queri follow simpl setup basic reader sql queri return vector call pdelta pass script process result return web servic queri simpl work fine set queri web servic paramat work fine end publish web servic test ask addit input paramet addit paramet get call wonder happen overlook like web servic ask paramet sql queri deliv pdelta idea suggest grealti appreci",
        "Question_gpt_summary_original":"The user has encountered a challenge while setting up a simple Azure ML system where the Reader is a SQL query to a database that returns a vector called Pdelta, which is then passed to the R script for further processing. The results are then returned back to the web service. The DB query is simple and works fine, but when the user publishes it as a web service and tests it, an additional input parameter called PDELTA is requested. The user is seeking suggestions to make the web service ask for only one parameter, the SQL query (Delta Query), which would then deliver the Pdeltas.",
        "Question_gpt_summary":"user encount challeng set simpl reader sql queri databas return vector call pdelta pass script process result return web servic queri simpl work fine user publish web servic test addit input paramet call pdelta request user seek suggest web servic ask paramet sql queri delta queri deliv pdelta",
        "Answer_original_content":"remov web servic input block publish web servic wai pdelta input pass reader modul",
        "Answer_preprocessed_content":"remov web servic input block publish web servic wai pdelta input pass reader modul",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to remove the web service input block and publish the web service without it. This way, the Pdelta input will be passed in only from the Reader module, and the web service will ask for only one parameter, the SQL query (Delta Query), which would then deliver the Pdeltas.",
        "Answer_gpt_summary":"solut possibl solut mention discuss remov web servic input block publish web servic wai pdelta input pass reader modul web servic ask paramet sql queri delta queri deliv pdelta"
    },
    {
        "Question_title":"What are valid Azure ML Workspace connection argument options?",
        "Question_body":"<p>I want to build an Azure ML environment with two python packages that I have in Azure Devops.\nFor this I need a workspace connection to Azure Devops. One package is published to an artifact feed and I can access it using the python SDK using a personal access token:<\/p>\n<pre><code>ws.set_connection(name=&quot;ConnectionName&quot;, \n                  category= &quot;PythonFeed&quot;, \n                  target = &quot;https:\/\/pkgs.dev.azure.com\/&quot;, \n                  authType = &quot;PAT&quot;, \n                  value = PAT_TOKEN)\n<\/code><\/pre>\n<p>However, for the other I need to get the package from the git repository in Azure Devops. The documentation of the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace?view=azure-ml-py#azureml-core-workspace-workspace-set-connection\" rel=\"nofollow noreferrer\">Python SDK<\/a> and the underlying <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspace-connections\/create\" rel=\"nofollow noreferrer\">REST API<\/a> don't give the options for the arguments, only that they need to be strings (see links).<\/p>\n<p>My question: what are the options for the following arguments:<\/p>\n<ul>\n<li>authType<\/li>\n<li>category<\/li>\n<li>valueFormat<\/li>\n<\/ul>\n<p>And what do I need to set for target argument, so that I can connect to the Azure DevOps repository with potentially different authentication?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1646219844250,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":93.0,
        "Poster_id":18353193.0,
        "Poster_created_time":1646219230528,
        "Poster_last_access_time":1663852436163,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1.0,
        "Poster_views":1.0,
        "Answer_body":"<p>To get the package from a Azure DevOps git repository you can change the target to the repository URL:<\/p>\n<pre><code>ws.set_connection(\n    name=&quot;ConnectionName&quot;, \n    category = &quot;PythonFeed&quot;,\n    target = &quot;https:\/\/dev.azure.com\/&lt;MY-ORG&gt;\/&lt;MY-PROJECT&gt;\/_git\/&lt;MY-REPO&gt;&quot;, \n    authType = &quot;PAT&quot;, \n    value = &lt;PAT-TOKEN&gt;)\n<\/code><\/pre>\n<p>Note here that there is no user specified in the URL (the standard &quot;clone&quot; URL in Azure DevOps also contains &quot;DevOps-Vx@&quot;).<\/p>\n<p>As for any other options for &quot;authType&quot;, &quot;category&quot; and &quot;valueFormat&quot;, I don't know.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":18353193.0,
        "Answerer_created_time":1646219230528,
        "Answerer_last_access_time":1663852436163,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1.0,
        "Answerer_views":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71321757",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659435013616,
        "Question_original_content":"valid workspac connect argument option want build environ python packag azur devop need workspac connect azur devop packag publish artifact feed access python sdk person access token set connect connectionnam categori pythonfe target http pkg dev azur com authtyp pat valu pat token need packag git repositori azur devop document python sdk underli rest api option argument need string link question option follow argument authtyp categori valueformat need set target argument connect azur devop repositori potenti differ authent",
        "Question_preprocessed_content":"valid workspac connect argument option want build environ python packag azur devop need workspac connect azur devop packag publish artifact feed access python sdk person access token need packag git repositori azur devop document python sdk underli rest api option argument need string question option follow argument authtyp categori valueformat need set target argument connect azur devop repositori potenti differ authent",
        "Question_gpt_summary_original":"The user is trying to build an Azure ML environment with two python packages from Azure Devops, but is facing challenges in setting up a workspace connection to access the packages. While they were able to access one package using a personal access token, they are unsure about the options for the authType, category, and valueFormat arguments for the other package. Additionally, they are unsure about what to set for the target argument to connect to the Azure DevOps repository with potentially different authentication.",
        "Question_gpt_summary":"user try build environ python packag azur devop face challeng set workspac connect access packag abl access packag person access token unsur option authtyp categori valueformat argument packag addition unsur set target argument connect azur devop repositori potenti differ authent",
        "Answer_original_content":"packag azur devop git repositori chang target repositori url set connect connectionnam categori pythonfe target http dev azur com git authtyp pat valu note user specifi url standard clone url azur devop contain devop option authtyp categori valueformat know",
        "Answer_preprocessed_content":"packag azur devop git repositori chang target repositori url note user specifi url option authtyp categori valueformat know",
        "Answer_gpt_summary_original":"Solution: The user can change the target to the repository URL to get the package from an Azure DevOps git repository. They can set the connection with the name, category, target, authType, and value arguments. However, there were no solutions provided for the authType, category, and valueFormat arguments for the other package.",
        "Answer_gpt_summary":"solut user chang target repositori url packag azur devop git repositori set connect categori target authtyp valu argument solut provid authtyp categori valueformat argument packag"
    },
    {
        "Question_title":"SageMaker experiments store",
        "Question_body":"<p>I just started using aws sagemaker for running and maintaining models, experiments. just wanted to know is there any persistent layer for the sagemaker from where i can get data of my experiments\/models instead of looking into the sagemaker studio. Does sagemaker saves the experiments or its data like s3 location in any table  something like modelsdb? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1584418712033,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1584419644403,
        "Question_score":1,
        "Question_view_count":229.0,
        "Poster_id":4704178.0,
        "Poster_created_time":1427130497092,
        "Poster_last_access_time":1650636970363,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":187.0,
        "Poster_views":151.0,
        "Answer_body":"<p>SageMaker Studio is using the SageMaker API to pull all of the data its displaying.  Essentially there's no secret API here getting invoked.<\/p>\n\n<p>Quite a bit of what's being displayed with respect to experiments is from the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_Search.html\" rel=\"nofollow noreferrer\">search results<\/a>, the rest coming from either List* or Describe* calls.  Studio is taking the results from the search request and displaying it in the table format that you're seeing.  Search results when searching over resource ExperimentTrialComponent that have a source (such as a training job) will be enhanced with the original sources data ([result]::SourceDetail::TrainingJob) if supported (work is ongoing to add additional source detail resource types).<\/p>\n\n<p>All of the metadata that is related to resources in SageMaker is available via the APIs; there is no other location (in the cloud) like s3 for that data.<\/p>\n\n<p>As of this time there is no effort to determine if it's possible to add support into <a href=\"https:\/\/github.com\/VertaAI\/modeldb\" rel=\"nofollow noreferrer\">modeldb<\/a> for SageMaker that I'm aware of.  Given that modeldb appears to make some assumptions about it's talking to a relational database it would appear unlikely to be something that would be doable. (I only read the overview very quickly so this might be inaccurate.)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":52136.0,
        "Answerer_created_time":1231266594036,
        "Answerer_last_access_time":1587577543676,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation":2800.0,
        "Answerer_views":262.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60716202",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1584546176820,
        "Question_original_content":"experi store start run maintain model experi want know persist layer data experi model instead look studio save experi data like locat tabl like modelsdb",
        "Question_preprocessed_content":"experi store start run maintain model experi want know persist layer data instead look studio save experi data like locat tabl like modelsdb",
        "Question_gpt_summary_original":"The user is facing a challenge in finding a persistent layer for SageMaker experiments and models. They are looking for a way to access the data of their experiments and models without having to look into SageMaker studio. They are wondering if SageMaker saves the experiments or its data in a table or location like modelsdb or S3.",
        "Question_gpt_summary":"user face challeng find persist layer experi model look wai access data experi model have look studio wonder save experi data tabl locat like modelsdb",
        "Answer_original_content":"studio api pull data displai essenti secret api get invok bit displai respect experi search result rest come list call studio take result search request displai tabl format see search result search resourc experimenttrialcompon sourc train job enhanc origin sourc data result sourcedetail trainingjob support work ongo add addit sourc resourc type metadata relat resourc avail api locat cloud like data time effort determin possibl add support modeldb awar given modeldb appear assumpt talk relat databas appear unlik doabl read overview quickli inaccur",
        "Answer_preprocessed_content":"studio api pull data displai essenti secret api get invok bit displai respect experi search result rest come list call studio take result search request displai tabl format see search result search resourc experimenttrialcompon sourc enhanc origin sourc data support metadata relat resourc avail api locat like data time effort determin possibl add support modeldb awar given modeldb appear assumpt talk relat databas appear unlik doabl",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion:\n- All metadata related to resources in SageMaker is available via the APIs, and there is no other location in the cloud like S3 for that data. Therefore, the user can access the data of their experiments and models through the SageMaker API.\n- There is no effort to determine if it's possible to add support into modeldb for SageMaker that the discussion participant is aware of.",
        "Answer_gpt_summary":"possibl solut mention discuss metadata relat resourc avail api locat cloud like data user access data experi model api effort determin possibl add support modeldb discuss particip awar"
    },
    {
        "Question_title":"How to use different remotes for different folders?",
        "Question_body":"<p>I want my data and models stored in separate Google Cloud buckets. The idea is that I want to be able to share the data with others without sharing the models.<\/p>\n\n<p>One idea I can think of is using separate git submodules for data and models. But that feels cumbersome and imposes some additional requirements from the end user (e.g. having to do <code>git submodule update<\/code>).<\/p>\n\n<p>So can I do this without using git submodules?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1574248229420,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":12,
        "Question_view_count":1984.0,
        "Poster_id":857731.0,
        "Poster_created_time":1311330349880,
        "Poster_last_access_time":1663773102327,
        "Poster_age":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation":3784.0,
        "Poster_views":342.0,
        "Answer_body":"<p>You can first add the different <a href=\"https:\/\/dvc.org\/doc\/command-reference\/remote\" rel=\"nofollow noreferrer\">DVC remotes<\/a> you want to establish (let's say you call them <code>data<\/code> and <code>models<\/code>, each one pointing to a different <a href=\"https:\/\/cloud.google.com\/storage\/docs\/json_api\/v1\/buckets\" rel=\"nofollow noreferrer\">GC bucket<\/a>). <strong>But don't set any remote as the project's default<\/strong>; This way, <a href=\"https:\/\/dvc.org\/doc\/command-reference\/push\" rel=\"nofollow noreferrer\"><code>dvc push<\/code><\/a> won't work without the <code>-r<\/code> (or <code>--remote<\/code>) option.<\/p>\n<p>You would then need to push each directory or file individually to the appropriate remote, like <code>dvc push data\/ -r data<\/code> and <code>dvc push model.dat -r models<\/code>.<\/p>\n<p>Note that a feature request to configure this exists on the DVC repo too. See <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/2095\" rel=\"nofollow noreferrer\">Specify file types that can be pushed to remote<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1642527991692,
        "Answer_score":13.0,
        "Answerer_id":761963.0,
        "Answerer_created_time":1305851487736,
        "Answerer_last_access_time":1663864920647,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":5993.0,
        "Answerer_views":457.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58952962",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_closed_time":1574267475363,
        "Question_original_content":"us differ remot differ folder want data model store separ googl cloud bucket idea want abl share data share model idea think separ git submodul data model feel cumbersom impos addit requir end user have git submodul updat git submodul",
        "Question_preprocessed_content":"us differ remot differ folder want data model store separ googl cloud bucket idea want abl share data share model idea think separ git submodul data model feel cumbersom impos addit requir end user git submodul",
        "Question_gpt_summary_original":"The user wants to store data and models in separate Google Cloud buckets to share data without sharing models. They are considering using separate git submodules for data and models but find it cumbersome and want to know if there is an alternative method.",
        "Question_gpt_summary":"user want store data model separ googl cloud bucket share data share model consid separ git submodul data model cumbersom want know altern method",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"Is there a way to install R libraries in SageMaker that receive a non-zero exit status?",
        "Question_body":"Hi all, I'm having an issue with an R kernel\/Jupyter notebook. I've come across two different libraries that result in the following error:\n\nWarning message in install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\"):\n\u201cinstallation of package \u2018XML\u2019 had non-zero exit status\u201dUpdating HTML index of packages in '.Library'\nMaking 'packages.html' ... done\n\n\nXML is the second package that I have run into this issue with. The other is rJava.\n\nI found a workaround that could work if I had root access, which is installing via the command line in a terminal. Which involves commands such as:\n\nyum install r-cran-rjava\n\n\nHowever, I don't have root access and cannot install as I get the message \"You need to be root to perform this command.\" So this workaround hasn't been possible.\n\nAfter checking the documentation for rJava and XML, I am running the requirements for JDK and other system requirements in SageMaker. This issue wasn't reproducible on a local RStudio environment. XML is a dependency for multiple R libraries (as is rJava). Is there a way that I can still install these packages?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1527798496000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":464.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"For Amazon SageMaker notebook Instances, you have the ability to assume root privileges, so instead of:\n\n$ yum install r-cran-rjava\n\nyou can try:\n\n$ sudo yum install r-cran-rjava\n\nwhich will allow you to impersonate the superuser (ie. root) for that command 1\n\nBut I don't believe that package exists in the available repos (ie. may be valid for another distro of Linux, but does not appear to be available in the yum repos -- running yum search 'r-cran-rjava' returned no results 2)\n\nInstead, from a prompt, install R + the necessary development files for later installation of R packages:\n\n$ sudo yum install -y R-java-devel.x86_64\n\nAnd finally, install the necessary XML libraries to support the XML package in R:\n\n$ sudo yum install -y libxml2-devel 3\n\nAfter which you can then open R (either as root user...)\n\n$ sudo R\n\nor personal\/local user\n\n$ R\n\nand execute the package installation:\n\n> install.packages(\"XML\", repos = \"https:\/\/cran.r-project.org\")\n\nEDITED TO FIX RJAVA PACKAGE INSTALLATION\n\nIt looks like the installation is requiring libgomp.spec\/libgomp.a files, so you can first find that file:\n\n$ sudo find \/ -iname libgomp.spec\n\nwhich should be located at \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec -- if so, you can manually create symlinks to fix this:\n\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.spec \/usr\/lib64\/\n$ sudo ln -s \/usr\/lib\/gcc\/x86_64-amazon-linux\/4.8.5\/libgomp.a \/usr\/lib64\/\n\n\nIf that ran correctly, you should now see both files in \/usr\/lib64 path:\n\n$ ls \/usr\/lib64\/libgomp*\n\nOnce confirmed, you can run the install.package('rJava') command.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE-0c9SwxRViGhd-lAYtsuw\/is-there-a-way-to-install-r-libraries-in-sage-maker-that-receive-a-non-zero-exit-status",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1527821535000,
        "Question_original_content":"wai instal librari receiv non zero exit statu have issu kernel jupyt notebook come differ librari result follow error warn messag instal packag xml repo http cran project org instal packag xml non zero exit statusupd html index packag librari make packag html xml second packag run issu rjava workaround work root access instal command line termin involv command yum instal cran rjava root access instal messag need root perform command workaround hasn possibl check document rjava xml run requir jdk requir issu wasn reproduc local rstudio environ xml depend multipl librari rjava wai instal packag",
        "Question_preprocessed_content":"wai instal librari receiv exit statu have issu notebook come differ librari result follow error warn messag repo instal packag xml exit statusupd html index packag make xml second packag run issu rjava workaround work root access instal command line termin involv command yum instal root access instal messag need root perform workaround hasn possibl check document rjava xml run requir jdk requir issu wasn reproduc local rstudio environ xml depend multipl librari wai instal packag",
        "Question_gpt_summary_original":"The user is facing challenges in installing R libraries, specifically XML and rJava, in SageMaker due to non-zero exit status errors. The user has tried a workaround that involves installing via the command line in a terminal, but cannot do so as they do not have root access. The issue was not reproducible on a local RStudio environment, and the user is running the requirements for JDK and other system requirements in SageMaker. The user is seeking a solution to install these packages without root access.",
        "Question_gpt_summary":"user face challeng instal librari specif xml rjava non zero exit statu error user tri workaround involv instal command line termin root access issu reproduc local rstudio environ user run requir jdk requir user seek solut instal packag root access",
        "Answer_original_content":"notebook instanc abil assum root privileg instead yum instal cran rjava try sudo yum instal cran rjava allow imperson superus root command believ packag exist avail repo valid distro linux appear avail yum repo run yum search cran rjava return result instead prompt instal necessari develop file later instal packag sudo yum instal java devel final instal necessari xml librari support xml packag sudo yum instal libxml devel open root user sudo person local user execut packag instal instal packag xml repo http cran project org edit fix rjava packag instal look like instal requir libgomp spec libgomp file file sudo inam libgomp spec locat usr lib gcc amazon linux libgomp spec manual creat symlink fix sudo usr lib gcc amazon linux libgomp spec usr lib sudo usr lib gcc amazon linux libgomp usr lib ran correctli file usr lib path usr lib libgomp confirm run instal packag rjava command",
        "Answer_preprocessed_content":"notebook instanc abil assum root privileg instead yum instal try sudo yum instal allow imperson superus command believ packag exist avail repo instead prompt instal necessari develop file later instal packag sudo yum instal final instal necessari xml librari support xml packag sudo yum instal open sudo user execut packag instal repo edit fix rjava packag instal look like instal requir file file sudo inam locat manual creat symlink fix sudo sudo ran correctli file path confirm run command",
        "Answer_gpt_summary_original":"The solution suggested in the discussion involves assuming root privileges by using the command \"sudo yum install\" instead of \"yum install\" to install the necessary packages. If the required package is not available in the available repositories, the user can install R and the necessary development files, followed by installing the necessary XML libraries to support the XML package in R. The user can then open R and execute the package installation. Additionally, the user can manually create symlinks to fix the issue of missing libgomp.spec\/libgomp.a files and then run the install.package('rJava') command.",
        "Answer_gpt_summary":"solut suggest discuss involv assum root privileg command sudo yum instal instead yum instal instal necessari packag requir packag avail avail repositori user instal necessari develop file follow instal necessari xml librari support xml packag user open execut packag instal addition user manual creat symlink fix issu miss libgomp spec libgomp file run instal packag rjava command"
    },
    {
        "Question_title":"Does a call to \"Deploy web service(via API key) \" re run trained Azure ML model again",
        "Question_body":"<p>I wanted to know how exactly the following works in backend<\/p>\n\n<p><strong>Scenario :<\/strong> <\/p>\n\n<blockquote>\n  <p>-> We get data from Edgex foundry in UTC format and we it store it in Azure Document DB in (CST\/CDT timezone) format<\/p>\n  \n  <p>-> We trained ML model on data(with Date in CST\/CDT timezone) and Deploy web service.<\/p>\n<\/blockquote>\n\n<p><strong>So I have few basic doubts below<\/strong><\/p>\n\n<blockquote>\n  <ol>\n  <li><p>When web job hits our predictive webservice , will the trained ML model be run again?<\/p><\/li>\n  <li><p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does\n  matter for our prediction?<\/p><\/li>\n  <li><p>What happens in backend when predictive webservice API is called?<\/p><\/li>\n  <\/ol>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1520339633757,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":92.0,
        "Poster_id":8579587.0,
        "Poster_created_time":1504867604870,
        "Poster_last_access_time":1562061256807,
        "Poster_age":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation":391.0,
        "Poster_views":125.0,
        "Answer_body":"<p>This is only based on my experience with Azure ML, but I think I can help with your questions.<\/p>\n\n<blockquote>\n  <p>When web job hits our predictive webservice, will the trained ML model be run again?<\/p>\n<\/blockquote>\n\n<p>Yes, in the sense that it will call the <code>predict<\/code> (or similar) method on the model on the new data. For instance, in <code>scikit-learn<\/code> you would train your model using the <code>fit<\/code> method. Once the model is in production, only the <code>predict<\/code> method would be called.<\/p>\n\n<p>It will also run the whole workflow you have set up to be deployed as the web service. As an example below is a workflow I've played around with before. Each time the web service is run with new data, this whole thing will be run. This is like creating a Pipeline in <code>scikit-learn<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YMFZb.png\" alt=\"Azure ML Workflow\"><\/a><\/p>\n\n<blockquote>\n  <p>Do we need to convert the UTC timezone for new incoming test data( which we want to predict) into CST\/CDT timezone, as TimeStamp does matter for our prediction?<\/p>\n<\/blockquote>\n\n<p>I would say yes, you would need to convert to the timezone that was used when training in the model. This can be done by adding a step in your workflow then when you call the web service it will do the necessary converting for you before making a prediction.<\/p>\n\n<blockquote>\n  <p>What happens in backend when predictive webservice API is called?<\/p>\n<\/blockquote>\n\n<p>I'm not sure if anyone knows for sure other than the folks at Microsoft, but for sure it will run the workflow you have set up.<\/p>\n\n<hr>\n\n<p>I know it's not much, but I hope this helps or at least gets you on the right track for what you need.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":186013.0,
        "Answerer_created_time":1254957460063,
        "Answerer_last_access_time":1659941040212,
        "Answerer_age":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation":2484.0,
        "Answerer_views":362.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49130977",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1521026355920,
        "Question_original_content":"deploi web servic api kei run train model want know exactli follow work backend scenario data edgex foundri utc format store azur document cst cdt timezon format train model data date cst cdt timezon deploi web servic basic doubt web job hit predict webservic train model run need convert utc timezon new incom test data want predict cst cdt timezon timestamp matter predict happen backend predict webservic api call",
        "Question_preprocessed_content":"deploi web servic run train model want know exactli follow work backend scenario data edgex foundri utc format store azur document format train model data deploi web servic basic doubt web job hit predict webservic train model run need convert utc timezon new incom test data timezon timestamp matter predict happen backend predict webservic api call",
        "Question_gpt_summary_original":"The user is seeking clarification on how a trained ML model works in the backend when a web service is deployed via an API key. They have specific doubts about whether the model is run again when the web job hits the predictive web service, whether incoming test data needs to be converted to the correct timezone, and what happens in the backend when the predictive web service API is called.",
        "Question_gpt_summary":"user seek clarif train model work backend web servic deploi api kei specif doubt model run web job hit predict web servic incom test data need convert correct timezon happen backend predict web servic api call",
        "Answer_original_content":"base experi think help question web job hit predict webservic train model run ye sens predict similar method model new data instanc scikit learn train model fit method model product predict method call run workflow set deploi web servic exampl workflow plai time web servic run new data thing run like creat pipelin scikit learn need convert utc timezon new incom test data want predict cst cdt timezon timestamp matter predict ye need convert timezon train model ad step workflow web servic necessari convert make predict happen backend predict webservic api call sure know sure folk microsoft sure run workflow set know hope help get right track need",
        "Answer_preprocessed_content":"base experi think help question web job hit predict webservic train model run ye sens method model new data instanc train model method model product method call run workflow set deploi web servic exampl workflow plai time web servic run new data thing run like creat pipelin need convert utc timezon new incom test data timezon timestamp matter predict ye need convert timezon train model ad step workflow web servic necessari convert make predict happen backend predict webservic api call sure know sure folk microsoft sure run workflow set know hope help get right track need",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- When the web job hits the predictive web service, the trained ML model will be run again by calling the predict method on the model with the new data.\n- The whole workflow that has been set up to be deployed as the web service will also be run each time the web service is called with new data.\n- It is recommended to convert incoming test data to the timezone that was used when training the model. This can be done by adding a step in the workflow.\n- It is unclear what exactly happens in the backend when the predictive web service API is called, but it is certain that the workflow set up for the web service will be executed.",
        "Answer_gpt_summary":"possibl solut mention discuss web job hit predict web servic train model run call predict method model new data workflow set deploi web servic run time web servic call new data recommend convert incom test data timezon train model ad step workflow unclear exactli happen backend predict web servic api call certain workflow set web servic execut"
    },
    {
        "Question_title":"Adding python modules to AzureML workspace",
        "Question_body":"<p>I've been working recently on deploying a machine learning model as a web service. I used Azure Machine Learning Studio for creating my own Workspace ID and Authorization Token. Then, I trained LogisticRegressionCV model from <strong>sklearn.linear_model<\/strong> locally on my machine (using python 2.7.13) and with the usage of below code snippet I wanted to publish my model as web service:<\/p>\n\n<pre><code>from azureml import services\n\n@services.publish('workspaceID','authorization_token')\n@services.types(var_1= float, var_2= float)\n@services.returns(int)\n\ndef predicting(var_1, var_2):\n    input = np.array([var_1, var_2].reshape(1,-1)\nreturn model.predict_proba(input)[0][1]\n<\/code><\/pre>\n\n<p>where <em>input<\/em> variable is a list with data to be scored and <em>model<\/em> variable contains trained classifier. Then after defining above function I want to make a prediction on sample input vector:<\/p>\n\n<pre><code>predicting.service(1.21, 1.34)\n<\/code><\/pre>\n\n<p>However following error occurs:<\/p>\n\n<pre><code>RuntimeError: Error 0085: The following error occurred during script \nevaluation, please view the output log for more information:\n<\/code><\/pre>\n\n<p>And the most important message in log is: <\/p>\n\n<pre><code>AttributeError: 'module' object has no attribute 'LogisticRegressionCV'\n<\/code><\/pre>\n\n<p>The error is strange to me because when I was using normal <em>sklearn.linear_model.LogisticRegression<\/em> everything was fine. I was able to make predictions sending POST requests to created endpoint, so I guess <strong>sklearn<\/strong> worked correctly. \nAfter changing to <em>LogisticRegressionCV<\/em> it does not. <\/p>\n\n<p>Therefore I wanted to update sklearn on my workspace.<\/p>\n\n<p>Do you have any ideas how to do it? Or even more general question: how to install any python module on azure machine learning studio in a way to use predict functions of any model I develpoed locally?<\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1506940629737,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":2578.0,
        "Poster_id":6105469.0,
        "Poster_created_time":1458750704640,
        "Poster_last_access_time":1662640939316,
        "Poster_age":null,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation":186.0,
        "Poster_views":55.0,
        "Answer_body":"<p>For installing python module on Azure ML Studio, there is a section <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/#Anchor_3\" rel=\"nofollow noreferrer\"><code>Technical Notes<\/code><\/a> of the offical document <code>Execute Python Script<\/code> which introduces it.<\/p>\n\n<p>The general steps as below.<\/p>\n\n<ol>\n<li>Create a Python project via <code>virtualenv<\/code> and active it.<\/li>\n<li>Install all packages you want via <code>pip<\/code> on the virtual Python environment, and then<\/li>\n<li>Package all files and directorys under the path <code>Lib\\site-packages<\/code> of your project as a zip file.<\/li>\n<li>Upload the zip package into your Azure ML WorkSpace as a dataSet.<\/li>\n<li>Follow the offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts#importing-existing-python-script-modules\" rel=\"nofollow noreferrer\">document<\/a> to import Python Module for your <code>Execute Python Script<\/code>.<\/li>\n<\/ol>\n\n<p>For more details, you can refer to the other similar SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/46222606\/updating-pandas-to-version-0-19-in-azure-ml-studio\/46232963#46232963\">Updating pandas to version 0.19 in Azure ML Studio<\/a>, it even introduced how to update the version of Python packages installed by Azure.<\/p>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":4989676.0,
        "Answerer_created_time":1433841188323,
        "Answerer_last_access_time":1662775294983,
        "Answerer_age":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation":22467.0,
        "Answerer_views":2692.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46523924",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1507014239332,
        "Question_original_content":"ad python modul workspac work recent deploi machin learn model web servic studio creat workspac author token train logisticregressioncv model sklearn linear model local machin python usag code snippet want publish model web servic import servic servic publish workspaceid author token servic type var float var float servic return int def predict var var input arrai var var reshap return model predict proba input input variabl list data score model variabl contain train classifi defin function want predict sampl input vector predict servic follow error occur runtimeerror error follow error occur script evalu view output log inform import messag log attributeerror modul object attribut logisticregressioncv error strang normal sklearn linear model logisticregress fine abl predict send post request creat endpoint guess sklearn work correctli chang logisticregressioncv want updat sklearn workspac idea gener question instal python modul studio wai us predict function model develpo local thank",
        "Question_preprocessed_content":"ad python modul workspac work recent deploi machin learn model web servic studio creat workspac author token train logisticregressioncv model local machin usag code snippet want publish model web servic input variabl list data score model variabl contain train classifi defin function want predict sampl input vector follow error occur import messag log error strang normal fine abl predict send post request creat endpoint guess sklearn work correctli chang logisticregressioncv want updat sklearn workspac idea gener question instal python modul studio wai us predict function model develpo local thank",
        "Question_gpt_summary_original":"The user encountered an error while trying to publish a machine learning model as a web service using Azure Machine Learning Studio. The error occurred when using the LogisticRegressionCV model from sklearn.linear_model, and the log showed an AttributeError indicating that the module did not have the required attribute. The user wants to update sklearn on their workspace and is seeking advice on how to install any python module on Azure Machine Learning Studio to use predict functions of any locally developed model.",
        "Question_gpt_summary":"user encount error try publish machin learn model web servic studio error occur logisticregressioncv model sklearn linear model log show attributeerror indic modul requir attribut user want updat sklearn workspac seek advic instal python modul studio us predict function local develop model",
        "Answer_original_content":"instal python modul studio section technic note offic document execut python script introduc gener step creat python project virtualenv activ instal packag want pip virtual python environ packag file directori path lib site packag project zip file upload zip packag workspac dataset follow offic document import python modul execut python script detail refer similar thread updat panda version studio introduc updat version python packag instal azur hope help",
        "Answer_preprocessed_content":"instal python modul studio section offic document introduc gener step creat python project activ instal packag want virtual python environ packag file directori path project zip file upload zip packag workspac dataset follow offic document import python modul detail refer similar thread updat panda version studio introduc updat version python packag instal azur hope help",
        "Answer_gpt_summary_original":"Solution: The user can install any python module on Azure Machine Learning Studio by creating a Python project via virtualenv, installing all required packages via pip on the virtual Python environment, packaging all files and directories under the path Lib\\site-packages of the project as a zip file, and uploading the zip package into the Azure ML WorkSpace as a dataSet. Then, the user can follow the official document to import Python Module for the Execute Python Script. The user can refer to the official document and a similar Stack Overflow thread for more details.",
        "Answer_gpt_summary":"solut user instal python modul studio creat python project virtualenv instal requir packag pip virtual python environ packag file directori path lib site packag project zip file upload zip packag workspac dataset user follow offici document import python modul execut python script user refer offici document similar stack overflow thread detail"
    },
    {
        "Question_title":"Moving resources across the regions",
        "Question_body":"Hi,\nWe have resource group(RG1) created in Germany region. Resource created are\n1) Vnet\n2) VPN gateway\n3) Disks - HDD\n4) VM (database installed)\n\nWe wanted to use Azure machine learning service to read data from database(present in resource group RG1 in German region). We later found that azure machine learning service is not available in Germany region and in order to create end points for Azure ML service both the networks(vnets of the DB and Azure ML ) should be in the same region.\n\nSo we tried migrating the resource and resource group (RG1) from German region to west Europe by using wizard on the portal. But we got prompted that disk , vnet and vpn gateway cannot be moved to different region ) .\n\nIs there anyway we could move them ? or any alternate solutions ? .Else, we would end up recreating every thing in westeurope which i would like to avoid.\n\nRegards,\nSuman",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1606983526427,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Maybe this is helpful:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/site-recovery\/azure-to-azure-tutorial-migrate\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/184565\/moving-resources-across-the-regions.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1606984258243,
        "Question_original_content":"move resourc region resourc group creat germani region resourc creat vnet vpn gatewai disk hdd databas instal want us servic read data databas present resourc group german region later servic avail germani region order creat end point servic network vnet region tri migrat resourc resourc group german region west europ wizard portal got prompt disk vnet vpn gatewai move differ region altern solut end recreat thing westeurop like avoid regard suman",
        "Question_preprocessed_content":"move resourc region resourc group creat germani region resourc creat vnet vpn gatewai disk hdd want us servic read data databas later servic avail germani region order creat end point servic network region tri migrat resourc resourc group german region west europ wizard portal got prompt disk vnet vpn gatewai move differ region altern solut end recreat thing westeurop like avoid regard suman",
        "Question_gpt_summary_original":"The user is facing challenges in moving their resource group from the German region to West Europe in order to use Azure machine learning service to read data from a database. They discovered that Azure machine learning service is not available in the German region and in order to create endpoints for Azure ML service, both networks should be in the same region. However, they were unable to move the disk, Vnet, and VPN gateway to a different region using the wizard on the portal. The user is seeking alternative solutions to avoid recreating everything in West Europe.",
        "Question_gpt_summary":"user face challeng move resourc group german region west europ order us servic read data databas discov servic avail german region order creat endpoint servic network region unabl disk vnet vpn gatewai differ region wizard portal user seek altern solut avoid recreat west europ",
        "Answer_original_content":"mayb help http doc microsoft com azur site recoveri azur azur tutori migrat repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_preprocessed_content":"mayb help repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to a Microsoft tutorial on Azure-to-Azure migration using Azure Site Recovery. It is suggested as a possible solution to the challenge of moving the resource group from the German region to West Europe without having to recreate everything.",
        "Answer_gpt_summary":"solut discuss provid link microsoft tutori azur azur migrat azur site recoveri suggest possibl solut challeng move resourc group german region west europ have recreat"
    },
    {
        "Question_title":"how to turn a matrix into a sparse matrix and protobuf it",
        "Question_body":"<p>I have a data set with 16 columns and 100,000 rows which I'm trying to prepare for a matrix-factorization training. I'm using the following code to split it and turn it into a sparse matrix.<\/p>\n\n<pre><code>X=data.drop([data.columns[0]],axis='columns')\ny=data[[1]]\nX=lil_matrix(100000,15).astype('float32')\ny=np.array(y).astype('float32')\nX\n<\/code><\/pre>\n\n<p>But when I run it, I get this error:<\/p>\n\n<blockquote>\n  <p>&lt;1x1 sparse matrix of type ''  with 1 stored\n  elements in LInked List format> .<\/p>\n<\/blockquote>\n\n<p>When I try to plug it into a training\/testing split it gives me further errors:<\/p>\n\n<blockquote>\n  <p>Found input variables with inconsistent numbers of samples: [1,\n  100000]<\/p>\n<\/blockquote>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_created_time":1562690689400,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1562705343183,
        "Question_score":0,
        "Question_view_count":498.0,
        "Poster_id":10824677.0,
        "Poster_created_time":1545524391676,
        "Poster_last_access_time":1663958935043,
        "Poster_age":null,
        "Poster_location":"Kansas City, MO, USA",
        "Poster_reputation":91.0,
        "Poster_views":24.0,
        "Answer_body":"<p>Your linked <code>notebook<\/code> is creating a 'blank' sparse matrix, and setting selected elements from data it reads from a <code>csv<\/code>.<\/p>\n\n<p>A simple example of this:<\/p>\n\n<pre><code>In [565]: from scipy import sparse                                                                           \nIn [566]: M = sparse.lil_matrix((10,5), dtype=float)                                                         \nIn [567]: M                                                                                                  \nOut[567]: \n&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 0 stored elements in LInked List format&gt;\n<\/code><\/pre>\n\n<p>Note that I use <code>(10,5)<\/code> to specify the matrix shape.  The () matter!  That's why I stressed reading the <code>docs<\/code>.  In the link the relevant line is:<\/p>\n\n<pre><code>X = lil_matrix((lines, columns)).astype('float32')\n<\/code><\/pre>\n\n<p>Now I can set a couple elements, just as I would an dense array:<\/p>\n\n<pre><code>In [568]: M[1,2] = 12.3                                                                                      \nIn [569]: M[3,1] = 1.1                                                                                       \nIn [570]: M                                                                                                  \nOut[570]: \n&lt;10x5 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2 stored elements in LInked List format&gt;\n<\/code><\/pre>\n\n<p>I can use <code>toarray<\/code> to display the matrix as a dense array (don't try this with large dimensions).<\/p>\n\n<pre><code>In [571]: M.toarray()                                                                                        \nOut[571]: \narray([[ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. , 12.3,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  1.1,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ],\n       [ 0. ,  0. ,  0. ,  0. ,  0. ]])\n<\/code><\/pre>\n\n<hr>\n\n<p>If I omit the (), it makes a (1,1) matrix with just one element, the first number.<\/p>\n\n<pre><code>In [572]: sparse.lil_matrix(10,5)                                                                            \nOut[572]: \n&lt;1x1 sparse matrix of type '&lt;class 'numpy.int64'&gt;'\n    with 1 stored elements in LInked List format&gt;\nIn [573]: _.A                                                                                                \nOut[573]: array([[10]], dtype=int64)\n<\/code><\/pre>\n\n<p>Look again at your code.  You set the <code>X<\/code> value twice, once it is a dataframe.  The second time is this bad <code>lil<\/code> initialization.  The second time does not make use of the first <code>X<\/code>.<\/p>\n\n<pre><code>X=data.drop([data.columns[0]],axis='columns')\n...\nX=lil_matrix(100000,15).astype('float32')\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1562801656230,
        "Answer_score":0.0,
        "Answerer_id":901925.0,
        "Answerer_created_time":1313736279736,
        "Answerer_last_access_time":1664084652287,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation":207794.0,
        "Answerer_views":16864.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56957206",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1562801273076,
        "Question_original_content":"turn matrix spars matrix protobuf data set column row try prepar matrix factor train follow code split turn spars matrix data drop data column axi column data lil matrix astyp float arrai astyp float run error try plug train test split give error input variabl inconsist number sampl",
        "Question_preprocessed_content":"turn matrix spars matrix protobuf data set column row try prepar train follow code split turn spars matrix run error spars matrix type store element link list format try plug split give error input variabl inconsist number sampl",
        "Question_gpt_summary_original":"The user is facing challenges in preparing a data set with 16 columns and 100,000 rows for matrix-factorization training. They are trying to split the data and turn it into a sparse matrix, but encountering errors related to inconsistent numbers of samples.",
        "Question_gpt_summary":"user face challeng prepar data set column row matrix factor train try split data turn spars matrix encount error relat inconsist number sampl",
        "Answer_original_content":"link notebook creat blank spars matrix set select element data read csv simpl exampl scipi import spars spars lil matrix dtype float store element link list format note us specifi matrix shape matter stress read doc link relev line lil matrix line column astyp float set coupl element dens arrai store element link list format us toarrai displai matrix dens arrai try larg dimens toarrai arrai omit make matrix element number spars lil matrix store element link list format arrai dtype int look code set valu twice datafram second time bad lil initi second time us data drop data column axi column lil matrix astyp float",
        "Answer_preprocessed_content":"link creat blank spars matrix set select element data read simpl exampl note us specifi matrix shape matter stress read link relev line set coupl element dens arrai us displai matrix dens arrai omit make matrix element number look code set valu twice datafram second time bad initi second time us",
        "Answer_gpt_summary_original":"Solution:\n- The user can create a sparse matrix using the `scipy.sparse` module and specifying the shape of the matrix using `(rows, columns)` in the `lil_matrix` function.\n- The user can set the elements of the sparse matrix using the same syntax as a dense array.\n- The user should make sure to use the correct shape when initializing the sparse matrix and avoid overwriting the `X` variable.",
        "Answer_gpt_summary":"solut user creat spars matrix scipi spars modul specifi shape matrix row column lil matrix function user set element spars matrix syntax dens arrai user sure us correct shape initi spars matrix avoid overwrit variabl"
    },
    {
        "Question_title":"Connect to redshift using sagemaker notebook instances",
        "Question_body":"<p>I would like to connect to redshift using sagemaker notebook instances. I want to run Unload commands to unload data from redshift to s3 using IAM role and schedule the sagemaker notebook.\nI want to know how I can import db credentials in sagemaker without hardcoding.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1618603786843,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":1504.0,
        "Poster_id":6595151.0,
        "Poster_created_time":1468599558956,
        "Poster_last_access_time":1658530343460,
        "Poster_age":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation":107.0,
        "Poster_views":26.0,
        "Answer_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/secretsmanager\/latest\/userguide\/intro.html\" rel=\"nofollow noreferrer\">AWS Secrets Manager<\/a> to store and access credentials. Your Sagemaker execution role should have permission to read from Secrets Manager (AFAIK AWS managed-role does have it). This is the same mechanism that's used by Sagemaker notebooks to get access to github repo, for example<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":5242656.0,
        "Answerer_created_time":1439975952067,
        "Answerer_last_access_time":1646728942728,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":16.0,
        "Answerer_views":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67131617",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1618908321396,
        "Question_original_content":"connect redshift notebook instanc like connect redshift notebook instanc want run unload command unload data redshift iam role schedul notebook want know import credenti hardcod thank",
        "Question_preprocessed_content":"connect redshift notebook instanc like connect redshift notebook instanc want run unload command unload data redshift iam role schedul notebook want know import credenti hardcod thank",
        "Question_gpt_summary_original":"The user is facing challenges in connecting to Redshift using Sagemaker notebook instances and wants to unload data from Redshift to S3 using IAM role and schedule the Sagemaker notebook. They are also looking for a way to import database credentials in Sagemaker without hardcoding.",
        "Question_gpt_summary":"user face challeng connect redshift notebook instanc want unload data redshift iam role schedul notebook look wai import databas credenti hardcod",
        "Answer_original_content":"us aw secret manag store access credenti execut role permiss read secret manag afaik aw manag role mechan notebook access github repo exampl",
        "Answer_preprocessed_content":"us aw secret manag store access credenti execut role permiss read secret manag mechan notebook access github repo exampl",
        "Answer_gpt_summary_original":"Solution: The user can use AWS Secrets Manager to store and access credentials. The Sagemaker execution role should have permission to read from Secrets Manager. This is the same mechanism used by Sagemaker notebooks to access Github repositories.",
        "Answer_gpt_summary":"solut user us aw secret manag store access credenti execut role permiss read secret manag mechan notebook access github repositori"
    },
    {
        "Question_title":"Part of speech tagging and entity recognition - python",
        "Question_body":"<p>I want to perform part of speech tagging and entity recognition in python similar to Maxent_POS_Tag_Annotator and Maxent_Entity_Annotator functions of openNLP in R.  I would prefer a code in python which takes input as textual sentence and gives output as different features- like number of \"CC\", number of \"CD\", number of \"DT\" etc.. CC, CD, DT are POS tags as used in Penn Treebank. So there should be 36 columns\/features for POS tagging corresponding to 36 POS tags as in <a href=\"http:\/\/www.ling.upenn.edu\/courses\/Fall_2003\/ling001\/penn_treebank_pos.html\" rel=\"nofollow\">Penn Treebank POS<\/a>. I want to implement this on Azure ML \"Execute Python Script\" module and Azure ML supports python 2.7.7. I heard nltk in python may does the job, but I am a beginner on python. Any help would be appreciated. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1441535794287,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1441776651843,
        "Question_score":0,
        "Question_view_count":1014.0,
        "Poster_id":4886093.0,
        "Poster_created_time":1431324152360,
        "Poster_last_access_time":1444093237568,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":19.0,
        "Poster_views":20.0,
        "Answer_body":"<p>Take a look at <a href=\"http:\/\/www.nltk.org\/book\/ch05.html\" rel=\"nofollow\">NTLK book<\/a>, Categorizing and Tagging Words section.<\/p>\n\n<p>Simple example, it uses the Penn Treebank tagset:<\/p>\n\n<pre><code>from nltk.tag import pos_tag\nfrom nltk.tokenize import word_tokenize\npos_tag(word_tokenize(\"John's big idea isn't all that bad.\")) \n\n[('John', 'NNP'),\n(\"'s\", 'POS'),\n ('big', 'JJ'),\n ('idea', 'NN'),\n ('is', 'VBZ'),\n (\"n't\", 'RB'),\n ('all', 'DT'),\n ('that', 'DT'),\n ('bad', 'JJ'),\n ('.', '.')]\n<\/code><\/pre>\n\n<p>Then you can use<\/p>\n\n<pre><code>from collections import defaultdict\ncounts = defaultdict(int)\nfor (word, tag) in pos_tag(word_tokenize(\"John's big idea isn't all that bad.\")):\n    counts[tag] += 1\n<\/code><\/pre>\n\n<p>to get frequencies:<\/p>\n\n<pre><code>defaultdict(&lt;type 'int'&gt;, {'JJ': 2, 'NN': 1, 'POS': 1, '.': 1, 'RB': 1, 'VBZ': 1, 'DT': 2, 'NNP': 1})\n<\/code><\/pre>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":4016674.0,
        "Answerer_created_time":1410097885876,
        "Answerer_last_access_time":1664082090056,
        "Answerer_age":null,
        "Answerer_location":"Moscow, Russia",
        "Answerer_reputation":5271.0,
        "Answerer_views":412.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32422626",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1441539548048,
        "Question_original_content":"speech tag entiti recognit python want perform speech tag entiti recognit python similar maxent po tag annot maxent entiti annot function opennlp prefer code python take input textual sentenc give output differ featur like number number number po tag penn treebank column featur po tag correspond po tag penn treebank po want implement execut python script modul support python heard nltk python job beginn python help appreci",
        "Question_preprocessed_content":"speech tag entiti recognit python want perform speech tag entiti recognit python similar function opennlp prefer code python take input textual sentenc give output differ featur like number number number po tag penn treebank po tag correspond po tag penn treebank po want implement execut python script modul support python heard nltk python job beginn python help appreci",
        "Question_gpt_summary_original":"The user is facing a challenge in performing part of speech tagging and entity recognition in Python, similar to the Maxent_POS_Tag_Annotator and Maxent_Entity_Annotator functions of openNLP in R. They require a code in Python that takes input as a textual sentence and gives output as different features, such as the number of \"CC\", \"CD\", \"DT\", etc. The user wants to implement this on Azure ML \"Execute Python Script\" module, which supports Python 2.7.7. Although they have heard that nltk in Python may do the job, they are a beginner in Python and would appreciate any help.",
        "Question_gpt_summary":"user face challeng perform speech tag entiti recognit python similar maxent po tag annot maxent entiti annot function opennlp requir code python take input textual sentenc give output differ featur number user want implement execut python script modul support python heard nltk python job beginn python appreci help",
        "Answer_original_content":"look ntlk book categor tag word section simpl exampl us penn treebank tagset nltk tag import po tag nltk token import word token po tag word token john big idea isn bad john nnp po big idea vbz bad us collect import defaultdict count defaultdict int word tag po tag word token john big idea isn bad count tag frequenc defaultdict po vbz nnp",
        "Answer_preprocessed_content":"look ntlk book categor tag word section simpl exampl us penn treebank tagset us frequenc",
        "Answer_gpt_summary_original":"Solution:\nThe discussion suggests using the NLTK library in Python to perform part of speech tagging and entity recognition. The code provided in the discussion takes input as a textual sentence and gives output as different features, such as the number of \"CC\", \"CD\", \"DT\", etc. The NLTK library can be used to achieve this. The code provided in the discussion can be used to get the frequencies of different parts of speech in a sentence.",
        "Answer_gpt_summary":"solut discuss suggest nltk librari python perform speech tag entiti recognit code provid discuss take input textual sentenc give output differ featur number nltk librari achiev code provid discuss frequenc differ part speech sentenc"
    },
    {
        "Question_title":"SageMaker deploying to EIA from TF Script Mode Python3",
        "Question_body":"<p>I've fitted a Tensorflow Estimator in SageMaker using Script Mode with <code>framework_version='1.12.0'<\/code> and <code>python_version='py3'<\/code>, using a GPU instance. <\/p>\n\n<p>Calling deploy directly on this estimator works if I select deployment instance type as GPU as well. However, if I select a CPU instance type and\/or try to add an accelerator, it fails with an error that docker cannot find a corresponding image to pull. <\/p>\n\n<p>Anybody know how to train a py3 model on a GPU with Script Mode and then deploy to a CPU+EIA instance? <\/p>\n\n<hr>\n\n<p>I've found a partial workaround by taking the intermediate step of creating a TensorFlowModel from the estimator's training artifacts and then deploying from the model, but this does not seem to support python 3 (again, doesn't find a corresponding container). If I switch to python_version='py2', it will find the container, but fail to pass health checks because all my code is for python 3.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1549048113447,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1549048982467,
        "Question_score":0,
        "Question_view_count":378.0,
        "Poster_id":2089899.0,
        "Poster_created_time":1361339272692,
        "Poster_last_access_time":1663965928400,
        "Poster_age":null,
        "Poster_location":"NYC",
        "Poster_reputation":6281.0,
        "Poster_views":958.0,
        "Answer_body":"<p>Unfortunately there are no TF + Python 3 + EI serving images at this time. If you would like to use TF + EI, you'll need to make sure your code is compatible with Python 2.<\/p>\n\n<p>Edit: after I originally wrote this, support for TF + Python 3 + EI has been released. At the time of this writing, I believe TF 1.12.0, 1.13.1, and 1.14.0 all have Python 3 + EI support. For the full list, see <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#tensorflow-sagemaker-estimators<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1585784312460,
        "Answer_score":2.0,
        "Answerer_id":9074534.0,
        "Answerer_created_time":1512770138847,
        "Answerer_last_access_time":1662131461223,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":493.0,
        "Answerer_views":47.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54485769",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1549915825300,
        "Question_original_content":"deploi eia script mode python fit tensorflow estim script mode framework version python version gpu instanc call deploi directli estim work select deploy instanc type gpu select cpu instanc type try add acceler fail error docker correspond imag pull anybodi know train model gpu script mode deploi cpu eia instanc partial workaround take intermedi step creat tensorflowmodel estim train artifact deploi model support python correspond contain switch python version contain fail pass health check code python",
        "Question_preprocessed_content":"deploi eia script mode python fit tensorflow estim script mode gpu instanc call deploi directli estim work select deploy instanc type gpu select cpu instanc type try add acceler fail error docker correspond imag pull anybodi know train model gpu script mode deploi cpu eia instanc partial workaround take intermedi step creat tensorflowmodel estim train artifact deploi model support python switch contain fail pass health check code python",
        "Question_gpt_summary_original":"The user encountered challenges when deploying a Tensorflow Estimator in SageMaker using Script Mode with framework_version='1.12.0' and python_version='py3'. The deployment works when selecting a GPU instance type, but fails when selecting a CPU instance type and\/or adding an accelerator due to docker not finding a corresponding image to pull. The user found a partial workaround by creating a TensorFlowModel from the estimator's training artifacts, but this does not support python 3.",
        "Question_gpt_summary":"user encount challeng deploi tensorflow estim script mode framework version python version deploy work select gpu instanc type fail select cpu instanc type ad acceler docker find correspond imag pull user partial workaround creat tensorflowmodel estim train artifact support python",
        "Answer_original_content":"unfortun python serv imag time like us need sure code compat python edit origin wrote support python releas time write believ python support list http github com aw python sdk tensorflow estim",
        "Answer_preprocessed_content":"unfortun python serv imag time like us need sure code compat python edit origin wrote support python releas time write believ python support list",
        "Answer_gpt_summary_original":"Solution: The user can use TensorFlow + EI serving images with Python 2. If the user wants to use TensorFlow + EI with Python 3, they can use TF 1.12.0, 1.13.1, or 1.14.0 which all have Python 3 + EI support. The user can find the full list of supported versions on the SageMaker Python SDK GitHub page.",
        "Answer_gpt_summary":"solut user us tensorflow serv imag python user want us tensorflow python us python support user list support version python sdk github page"
    },
    {
        "Question_title":"How to build a Convolution Neural Net in Azure Machine Learning?",
        "Question_body":"<p>Someone should add \"net#\" as a tag. I'm trying to improve my neural network in Azure Machine Learning Studio by turning it into a convolution neural net using this tutorial:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2\" rel=\"noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Neural-Network-Convolution-and-pooling-deep-net-2<\/a><\/p>\n\n<p>The differences between mine and the tutorial is I'm doing regression with 35 features and 1 label and they're doing classification with 28x28 features and 10 labels. <\/p>\n\n<p>I start with the basic and 2nd example and get them working with:<\/p>\n\n<pre><code>input Data [35];\n\nhidden H1 [100]\n    from Data all;\n\nhidden H2 [100]\n    from H1 all;\n\noutput Result [1] linear\n    from H2 all;\n<\/code><\/pre>\n\n<p>Now the transformation to convolution I misunderstand. In the tutorial and documentation here: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-netsharp-reference-guide<\/a> it doesn't mention how the node tuple values are calculated for the hidden layers. The tutorial says:<\/p>\n\n<pre><code>hidden C1 [5, 12, 12]\n  from Picture convolve {\n    InputShape  = [28, 28];\n    KernelShape = [ 5,  5];\n    Stride      = [ 2,  2];\n    MapCount = 5;\n  }\n\nhidden C2 [50, 4, 4]\n   from C1 convolve {\n     InputShape  = [ 5, 12, 12];\n     KernelShape = [ 1,  5,  5];\n     Stride      = [ 1,  2,  2];\n     Sharing     = [ F,  T,  T];\n     MapCount = 10;\n  }\n<\/code><\/pre>\n\n<p>Seems like the [5, 12, 12] and [50,4,4] pop out of no where along with the KernalShape, Stride, and MapCount. How do I know what values are valid for my example? I tried using the same values, but it didn't work and I have a feeling since he has a [28,28] input and I have a [35], I need tuples with 2 integers not 3. <\/p>\n\n<p>I just tried with random values that seem to correlate with the tutorial:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 23]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [200, 6]\n   from C1 convolve {\n     InputShape  = [ 7, 23];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>Right now it seems impossible to debug because the only error code Azure Machine Learning Studio ever gives is:<\/p>\n\n<pre><code>Exception\":{\"ErrorId\":\"LibraryException\",\"ErrorCode\":\"1000\",\"ExceptionType\":\"ModuleException\",\"Message\":\"Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\",\"Exception\":{\"Library\":\"TLC\",\"ExceptionType\":\"LibraryException\",\"Message\":\"Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown.\"}}}Error: Error 1000: TLC library exception: Exception of type 'Microsoft.Numerics.AFxLibraryException' was thrown. Process exited with error code -2\n<\/code><\/pre>\n\n<p>Lastly my setup is <a href=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PBN9L.png\" alt=\"Azure Machine Learning Setup\"><\/a> <\/p>\n\n<p>Thanks for the help!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1502257048253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1502686418056,
        "Question_score":9,
        "Question_view_count":1268.0,
        "Poster_id":4358137.0,
        "Poster_created_time":1418505926276,
        "Poster_last_access_time":1664040770892,
        "Poster_age":null,
        "Poster_location":"Missouri",
        "Poster_reputation":1454.0,
        "Poster_views":328.0,
        "Answer_body":"<p>The correct network definition for 35-column length input with given kernels and strides would be following:<\/p>\n\n<pre><code>const { T = true; F = false; }\n\ninput Data [35];\n\nhidden C1 [7, 15]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [14, 7, 5]\n   from C1 convolve {\n     InputShape  = [ 7, 15];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden H3 [100]\n  from C2 all;\n\noutput Result [1] linear\n  from H3 all;\n<\/code><\/pre>\n\n<p>First, the C1 = [7,15]. The first dimension is simply the MapCount. For the second dimension, the kernel shape defines the length of the \"window\" that's used to scan the input columns, and the stride defines how much it moves at each step. So the kernel windows would cover columns 1-7, 3-9, 5-11,...,29-35, yielding the second dimension of 15 when you tally the windows.<\/p>\n\n<p>Next, the C2 = [14,7,5]. The first dimension is again the MapCount. For the second and third dimension, the 1-by-7 kernel \"window\" has to cover the input size of 7-by-15, using steps of 1 and 2 along corresponding dimensions. <\/p>\n\n<p>Note that you could specify C2 hidden layer shape of [98,5] or even [490], if you wanted to flatten the outputs. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":5784983.0,
        "Answerer_created_time":1452696930640,
        "Answerer_last_access_time":1603992113372,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":746.0,
        "Answerer_views":112.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45582412",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1503342256368,
        "Question_original_content":"build convolut neural net add net tag try improv neural network studio turn convolut neural net tutori http galleri cortanaintellig com experi neural network convolut pool deep net differ tutori regress featur label classif featur label start basic exampl work input data hidden data hidden output result linear transform convolut misunderstand tutori document http doc microsoft com azur machin learn machin learn azur netsharp refer guid mention node tupl valu calcul hidden layer tutori sai hidden pictur convolv inputshap kernelshap stride mapcount hidden convolv inputshap kernelshap stride share mapcount like pop kernalshap stride mapcount know valu valid exampl tri valu work feel input need tupl integ tri random valu correl tutori const true fals input data hidden data convolv inputshap kernelshap stride mapcount hidden convolv inputshap kernelshap stride share mapcount hidden output result linear right imposs debug error code studio give except errorid libraryexcept errorcod exceptiontyp moduleexcept messag error tlc librari except except type microsoft numer afxlibraryexcept thrown except librari tlc exceptiontyp libraryexcept messag except type microsoft numer afxlibraryexcept thrown error error tlc librari except except type microsoft numer afxlibraryexcept thrown process exit error code lastli setup thank help",
        "Question_preprocessed_content":"build convolut neural net add net tag try improv neural network studio turn convolut neural net tutori differ tutori regress featur label classif featur label start basic exampl work transform convolut misunderstand tutori document mention node tupl valu calcul hidden layer tutori sai like pop kernalshap stride mapcount know valu valid exampl tri valu work feel input need tupl integ tri random valu correl tutori right imposs debug error code studio give lastli setup thank help",
        "Question_gpt_summary_original":"The user is facing challenges in building a convolution neural net in Azure Machine Learning Studio. They are trying to follow a tutorial but are having difficulty understanding how to calculate the node tuple values for the hidden layers. The tutorial provides values for a classification problem with 28x28 features and 10 labels, while the user is working on a regression problem with 35 features and 1 label. The user has tried using random values that seem to correlate with the tutorial but is unable to debug due to the error code provided by Azure Machine Learning Studio.",
        "Question_gpt_summary":"user face challeng build convolut neural net studio try follow tutori have difficulti understand calcul node tupl valu hidden layer tutori provid valu classif problem featur label user work regress problem featur label user tri random valu correl tutori unabl debug error code provid studio",
        "Answer_original_content":"correct network definit column length input given kernel stride follow const true fals input data hidden data convolv inputshap kernelshap stride mapcount hidden convolv inputshap kernelshap stride share mapcount hidden output result linear dimens simpli mapcount second dimens kernel shape defin length window scan input column stride defin move step kernel window cover column yield second dimens talli window dimens mapcount second dimens kernel window cover input size step correspond dimens note specifi hidden layer shape want flatten output",
        "Answer_preprocessed_content":"correct network definit length input given kernel stride follow dimens simpli mapcount second dimens kernel shape defin length window scan input column stride defin move step kernel window cover column yield second dimens talli window dimens mapcount second dimens kernel window cover input size step correspond dimens note specifi hidden layer shape want flatten output",
        "Answer_gpt_summary_original":"Solution:\nThe user can use the following network definition for 35-column length input with given kernels and strides:\n\ninput Data [35];\n\nhidden C1 [7, 15]\n  from Data convolve {\n    InputShape  = [35];\n    KernelShape = [7];\n    Stride      = [2];\n    MapCount = 7;\n  }\n\nhidden C2 [14, 7, 5]\n   from C1 convolve {\n     InputShape  = [ 7, 15];\n     KernelShape = [ 1,  7];\n     Stride      = [ 1,  2];\n     Sharing     = [ F,  T];\n     MapCount = 14;\n  }\n\nhidden",
        "Answer_gpt_summary":"solut user us follow network definit column length input given kernel stride input data hidden data convolv inputshap kernelshap stride mapcount hidden convolv inputshap kernelshap stride share mapcount hidden"
    },
    {
        "Question_title":"Invoke aws sagemaker endpoint",
        "Question_body":"<p>I have some data in S3 and I want to create a lambda function to predict the output with my deployed aws sagemaker endpoint then I put the outputs in S3 again. Is it necessary in this case to create an api gateway like decribed in this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"noreferrer\">link<\/a> ? and in the lambda function what I have to put. I expect to put (where to find the data, how to invoke the endpoint, where to put the data) <\/p>\n\n<pre><code>import boto3\nimport io\nimport json\nimport csv\nimport os\n\n\nclient = boto3.client('s3') #low-level functional API\n\nresource = boto3.resource('s3') #high-level object-oriented API\nmy_bucket = resource.Bucket('demo-scikit-byo-iris') #subsitute this for your s3 bucket name. \n\nobj = client.get_object(Bucket='demo-scikit-byo-iris', Key='foo.csv')\nlines= obj['Body'].read().decode('utf-8').splitlines()\nreader = csv.reader(lines)\n\nimport io\nfile = io.StringIO(lines)\n\n# grab environment variables\nruntime= boto3.client('runtime.sagemaker')\n\nresponse = runtime.invoke_endpoint(\n    EndpointName= 'nilm2',\n    Body = file.getvalue(),\n    ContentType='*\/*',\n    Accept = 'Accept')\n\noutput = response['Body'].read().decode('utf-8')\n<\/code><\/pre>\n\n<p>my data is a csv file of 2 columns of floats with no headers, the problem is that lines return a list of strings(each row is an element of this list:['11.55,65.23', '55.68,69.56'...]) the invoke work well but the response is also a string: output = '65.23\\n,65.23\\n,22.56\\n,...' <\/p>\n\n<p>So how to save this output to S3 as a csv file <\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1549885499183,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":5,
        "Question_view_count":3146.0,
        "Poster_id":8424070.0,
        "Poster_created_time":1502010899808,
        "Poster_last_access_time":1573819157507,
        "Poster_age":null,
        "Poster_location":"Tunis, Tunisia",
        "Poster_reputation":109.0,
        "Poster_views":14.0,
        "Answer_body":"<p>If your Lambda function is scheduled, then you won't need an API Gateway. But if the predict action will be triggered by a user, by an application, for example, you will need.<\/p>\n\n<p>When you call the invoke endpoint, actually you are calling a SageMaker endpoint, which is not the same as an API Gateway endpoint. <\/p>\n\n<p>A common architecture with SageMaker is:<\/p>\n\n<ol>\n<li>API Gateway with receives a request then calls an authorizer, then\ninvoke your Lambda; <\/li>\n<li>A Lambda with does some parsing in your input data, then calls your SageMaker prediction endpoint, then, handles the result and returns to your application.<\/li>\n<\/ol>\n\n<p>By the situation you describe, I can't say if your task is some academic stuff or a production one.<\/p>\n\n<p>So, how you can save the data as a CSV file from your Lambda? <\/p>\n\n<p>I believe you can just parse the output, then just upload the file to S3. Here you will parse manually or with a lib, with boto3 you can upload the file. The output of your model depends on your implementation on SageMaker image. So, if you need the response data in another format, maybe you will need to use a <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">custom image<\/a>. I normally use a custom image, which I can define how I want to handle my data on requests\/responses.<\/p>\n\n<p>In terms of a production task, I certainly recommend you check Batch transform jobs from SageMaker. You can provide an input file (the S3 path) and also a destination file (another S3 path). The SageMaker will run the batch predictions and will persist a file with the results. Also, you won't need to deploy your model to an endpoint, when this job run, will create an instance of your endpoint, download your data to predict, do the predictions, upload the output, and shut down the instance. You only need a trained model.<\/p>\n\n<p>Here some info about Batch transform jobs:<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html<\/a><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html<\/a><\/p>\n\n<p>I hope it helps, let me know if need more info.<\/p>\n\n<p>Regards.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Answerer_id":5741449.0,
        "Answerer_created_time":1451855177167,
        "Answerer_last_access_time":1663876713340,
        "Answerer_age":null,
        "Answerer_location":"Crici\u00fama - State of Santa Catarina, Brazil",
        "Answerer_reputation":68.0,
        "Answerer_views":17.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54629890",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1550022296023,
        "Question_original_content":"invok endpoint data want creat lambda function predict output deploi endpoint output necessari case creat api gatewai like decrib link lambda function expect data invok endpoint data import boto import import json import csv import client boto client low level function api resourc boto resourc high level object orient api bucket resourc bucket demo scikit byo iri subsitut bucket obj client object bucket demo scikit byo iri kei foo csv line obj bodi read decod utf splitlin reader csv reader line import file stringio line grab environ variabl runtim boto client runtim respons runtim invok endpoint endpointnam nilm bodi file getvalu contenttyp accept accept output respons bodi read decod utf data csv file column float header problem line return list string row element list invok work respons string output save output csv file thank",
        "Question_preprocessed_content":"invok endpoint data want creat lambda function predict output deploi endpoint output necessari case creat api gatewai like decrib link lambda function expect data csv file column float header problem line return list string invok work respons string output save output csv file thank",
        "Question_gpt_summary_original":"The user wants to create a lambda function to predict the output with their deployed AWS SageMaker endpoint and put the outputs in S3 again. They are unsure if it is necessary to create an API gateway and what to put in the lambda function. The user's data is a CSV file of 2 columns of floats with no headers, and they are having trouble saving the output to S3 as a CSV file.",
        "Question_gpt_summary":"user want creat lambda function predict output deploi endpoint output unsur necessari creat api gatewai lambda function user data csv file column float header have troubl save output csv file",
        "Answer_original_content":"lambda function schedul won need api gatewai predict action trigger user applic exampl need invok endpoint actual call endpoint api gatewai endpoint common architectur api gatewai receiv request call author invok lambda lambda pars input data call predict endpoint handl result return applic situat task academ stuff product save data csv file lambda believ pars output upload file pars manual lib boto upload file output model depend implement imag need respons data format mayb need us custom imag normal us custom imag defin want handl data request respons term product task certainli recommend check batch transform job provid input file path destin file path run batch predict persist file result won need deploi model endpoint job run creat instanc endpoint download data predict predict upload output shut instanc need train model info batch transform job http doc aw amazon com latest work batch html http doc aw amazon com latest batch transform html hope help let know need info regard",
        "Answer_preprocessed_content":"lambda function schedul won need api gatewai predict action trigger user applic exampl need invok endpoint actual call endpoint api gatewai endpoint common architectur api gatewai receiv request call author invok lambda lambda pars input data call predict endpoint handl result return applic situat task academ stuff product save data csv file lambda believ pars output upload file pars manual lib boto upload file output model depend implement imag need respons data format mayb need us custom imag normal us custom imag defin want handl data term product task certainli recommend check batch transform job provid input file destin file run batch predict persist file result won need deploi model endpoint job run creat instanc endpoint download data predict predict upload output shut instanc need train model info batch transform job hope help let know need info regard",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- If the Lambda function is scheduled, then an API Gateway is not needed. But if the predict action will be triggered by a user, an API Gateway is needed.\n- A common architecture with SageMaker is to use an API Gateway that receives a request, calls an authorizer, and then invokes the Lambda function. The Lambda function does some parsing in the input data, calls the SageMaker prediction endpoint, handles the result, and returns it to the application.\n- To save the data as a CSV file from the Lambda function, the output can be parsed and uploaded to S3 using boto3. If the response data is needed in another format, a custom image can be used.\n- For a production",
        "Answer_gpt_summary":"possibl solut mention discuss lambda function schedul api gatewai need predict action trigger user api gatewai need common architectur us api gatewai receiv request call author invok lambda function lambda function pars input data call predict endpoint handl result return applic save data csv file lambda function output pars upload boto respons data need format custom imag product"
    },
    {
        "Question_title":"import custom python module in azure ml deployment environment",
        "Question_body":"<p>I have an sklearn k-means model. I am training the model and saving it in a pickle file so I can deploy it later using azure ml library. The model that I am training uses a custom Feature Encoder called <strong>MultiColumnLabelEncoder<\/strong>.\nThe pipeline model is defined as follow :<\/p>\n\n<pre><code># Pipeline\nkmeans = KMeans(n_clusters=3, random_state=0)\npipe = Pipeline([\n(\"encoder\", MultiColumnLabelEncoder()),\n('k-means', kmeans),\n])\n#Training the pipeline\nmodel = pipe.fit(visitors_df)\nprediction = model.predict(visitors_df)\n#save the model in pickle\/joblib format\nfilename = 'k_means_model.pkl'\njoblib.dump(model, filename)\n<\/code><\/pre>\n\n<p>The model saving works fine. The Deployment steps are the same as the steps in this link : <\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb<\/a><\/p>\n\n<p>However the deployment always fails with this error :<\/p>\n\n<pre><code>  File \"\/var\/azureml-server\/create_app.py\", line 3, in &lt;module&gt;\n    from app import main\n  File \"\/var\/azureml-server\/app.py\", line 27, in &lt;module&gt;\n    import main as user_main\n  File \"\/var\/azureml-app\/main.py\", line 19, in &lt;module&gt;\n    driver_module_spec.loader.exec_module(driver_module)\n  File \"\/structure\/azureml-app\/score.py\", line 22, in &lt;module&gt;\n    importlib.import_module(\"multilabelencoder\")\n  File \"\/azureml-envs\/azureml_b707e8c15a41fd316cf6c660941cf3d5\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named 'multilabelencoder'\n<\/code><\/pre>\n\n<p>I understand that pickle\/joblib has some problems unpickling the custom function MultiLabelEncoder. That's why I defined this class in a separate python script (which I executed also). I called this custom function in the training python script, in the deployment script and in the scoring python file (score.py). The importing in the score.py file is not successful. \nSo my question is how can I import custom python module to azure ml deployment environment ?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>EDIT: \nThis is my .yml file<\/p>\n\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  - multilabelencoder==1.0.4\n  - scikit-learn\n  - azureml-defaults==1.0.74.*\n  - pandas\nchannels:\n- conda-forge\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1575463048677,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1575634698240,
        "Question_score":4,
        "Question_view_count":2611.0,
        "Poster_id":5159740.0,
        "Poster_created_time":1437986390372,
        "Poster_last_access_time":1663943741496,
        "Poster_age":null,
        "Poster_location":"M\u00fcnchen, Deutschland",
        "Poster_reputation":361.0,
        "Poster_views":149.0,
        "Answer_body":"<p>In fact, the solution was to import my customized class <strong>MultiColumnLabelEncoder<\/strong> as a pip package (You can find it through pip install multilllabelencoder==1.0.5).\nThen I passed the pip package to the .yml file or in the InferenceConfig of the azure ml environment.\nIn the score.py file, I imported the class as follows :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder\ndef init():\n    global model\n\n    # Call the custom encoder to be used dfor unpickling the model\n    encoder = multilabelencoder.MultiColumnLabelEncoder() \n    # Get the path where the deployed model can be found.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'k_means_model_45.pkl')\n    model = joblib.load(model_path)\n<\/code><\/pre>\n\n<p>Then the deployment was successful. \nOne more important thing is I had to use the same pip package (multilabelencoder) in the training pipeline as here :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder \npipe = Pipeline([\n    (\"encoder\", multilabelencoder.MultiColumnLabelEncoder(columns)),\n    ('k-means', kmeans),\n])\n#Training the pipeline\ntrainedModel = pipe.fit(df)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Answerer_id":5159740.0,
        "Answerer_created_time":1437986390372,
        "Answerer_last_access_time":1663943741496,
        "Answerer_age":null,
        "Answerer_location":"M\u00fcnchen, Deutschland",
        "Answerer_reputation":361.0,
        "Answerer_views":149.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59176241",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1575635052340,
        "Question_original_content":"import custom python modul deploy environ sklearn mean model train model save pickl file deploi later librari model train us custom featur encod call multicolumnlabelencod pipelin model defin follow pipelin kmean kmean cluster random state pipe pipelin encod multicolumnlabelencod mean kmean train pipelin model pipe fit visitor predict model predict visitor save model pickl joblib format filenam mean model pkl joblib dump model filenam model save work fine deploy step step link http notebook azur com project get start html us deploi cloud model regist deploi ipynb deploy fail error file var server creat app line app import main file var server app line import main user main file var app main line driver modul spec loader exec modul driver modul file structur app score line importlib import modul multilabelencod file env becafdcfccfd lib python importlib init line import modul return bootstrap gcd import level packag level modulenotfounderror modul name multilabelencod understand pickl joblib problem unpickl custom function multilabelencod defin class separ python script execut call custom function train python script deploy script score python file score import score file success question import custom python modul deploy environ thank advanc edit yml file project environ depend python interpret version current support later python pip multilabelencod scikit learn default panda channel conda forg",
        "Question_preprocessed_content":"import custom python modul deploy environ sklearn model train model save pickl file deploi later librari model train us custom featur encod call multicolumnlabelencod pipelin model defin follow model save work fine deploy step step link deploy fail error understand problem unpickl custom function multilabelencod defin class separ python script call custom function train python script deploy script score python file import file success question import custom python modul deploy environ thank advanc edit yml file",
        "Question_gpt_summary_original":"The user is facing challenges in importing a custom Python module called MultiColumnLabelEncoder in the Azure ML deployment environment. The user has trained an sklearn k-means model and saved it in a pickle file, which uses the custom Feature Encoder. However, during deployment, the user encounters an error due to the failure of importing the custom module. The user has defined the custom function in a separate Python script and called it in the training, deployment, and scoring Python files. The user is seeking guidance on how to import the custom Python module to the Azure ML deployment environment.",
        "Question_gpt_summary":"user face challeng import custom python modul call multicolumnlabelencod deploy environ user train sklearn mean model save pickl file us custom featur encod deploy user encount error failur import custom modul user defin custom function separ python script call train deploy score python file user seek guidanc import custom python modul deploy environ",
        "Answer_original_content":"fact solut import custom class multicolumnlabelencod pip packag pip instal multiabelencod pass pip packag yml file inferenceconfig environ score file import class follow multilabelencod import multilabelencod def init global model custom encod dfor unpickl model encod multilabelencod multicolumnlabelencod path deploi model model path path join getenv model dir mean model pkl model joblib load model path deploy success import thing us pip packag multilabelencod train pipelin multilabelencod import multilabelencod pipe pipelin encod multilabelencod multicolumnlabelencod column mean kmean train pipelin trainedmodel pipe fit",
        "Answer_preprocessed_content":"fact solut import custom class multicolumnlabelencod pip packag pass pip packag yml file inferenceconfig environ file import class follow deploy success import thing us pip packag train pipelin",
        "Answer_gpt_summary_original":"The solution to the challenge was to import the customized class MultiColumnLabelEncoder as a pip package and pass it to the .yml file or InferenceConfig of the Azure ML environment. In the score.py file, the class was imported using the pip package. It was also important to use the same pip package in the training pipeline.",
        "Answer_gpt_summary":"solut challeng import custom class multicolumnlabelencod pip packag pass yml file inferenceconfig environ score file class import pip packag import us pip packag train pipelin"
    },
    {
        "Question_title":"Serialise objects in azure ML pipeline runs",
        "Question_body":"<p>I have Python package for pre-processing the data for train and scoring\/inference purpose. I am using it as a python step in a pipeline. The entry script (which is in package) takes argument i.e task argument choices=(train,score) and does the pre-processing. Here is the step code:<\/p>\n<pre><code># Pipeline parameter: task, config_path\nparam_task = PipelineParameter(name='task', default_value='train')\nparam_config_path = PipelineParameter(name=&quot;config_path&quot;, default_value='Preprocess\/preprocess_config.json')\n\n\n# Define pipeline steps\nStepPreprocessing = PythonScriptStep(\n    name=&quot;Preprocessing&quot;,\n    script_name=e.preprocess_script_path,\n    arguments=[\n        &quot;--config_path&quot;, param_config_path, \n        &quot;--task&quot;, param_task,\n    ], \n    inputs=None,\n    compute_target=aml_compute,\n    runconfig=run_config,\n    source_directory=e.sources_directory,\n    allow_reuse=False\n)\n<\/code><\/pre>\n<p><strong>With argument task=='train'<\/strong> it loads data and does pre-processing according to steps mentioned in a config file. During this process it creates StandardScaler, SimpleImpute objects (sklearn objects) and stores the sklearn objects in a data\/output folder inside the package, and the processed data on azure storage.<\/p>\n<p>The problem is, when the pipeline is run again with <strong>task =='score'<\/strong> it is unable to find the sklearn objects with error.<\/p>\n<pre><code>User program failed with FileNotFoundError: [Errno 2] No such file or directory: 'data\/output\/StandardScaler.joblib'\n<\/code><\/pre>\n<p>What is the best way to save the sklearn objects so that these can be accessed by pipeline when pipeline in run again but with argument task=='score'.<\/p>\n<p>I don't want to register these objects in model registry and don't want to save them in datastores as well.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1644510872937,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":50.0,
        "Poster_id":5368122.0,
        "Poster_created_time":1443017464707,
        "Poster_last_access_time":1663923275743,
        "Poster_age":null,
        "Poster_location":"Sweden",
        "Poster_reputation":644.0,
        "Poster_views":126.0,
        "Answer_body":"<p>The way to do that is either:<\/p>\n<ol>\n<li><p>Register the artifacts in model registry and get them in scoring.<\/p>\n<\/li>\n<li><p>Configure output of pipeline step as PipelineData or OutputFileDatasetConfig, write artifacts to configured output. While scoring, get run of the train pipeline, get its outputs, retrieve the artifacts. This involves experiment name to get run of the pipeline.<\/p>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":5368122.0,
        "Answerer_created_time":1443017464707,
        "Answerer_last_access_time":1663923275743,
        "Answerer_age":null,
        "Answerer_location":"Sweden",
        "Answerer_reputation":644.0,
        "Answerer_views":126.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71068837",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1645522336520,
        "Question_original_content":"serialis object pipelin run python packag pre process data train score infer purpos python step pipelin entri script packag take argument task argument choic train score pre process step code pipelin paramet task config path param task pipelineparamet task default valu train param config path pipelineparamet config path default valu preprocess preprocess config json defin pipelin step steppreprocess pythonscriptstep preprocess script preprocess script path argument config path param config path task param task input comput target aml comput runconfig run config sourc directori sourc directori allow reus fals argument task train load data pre process accord step mention config file process creat standardscal simpleimput object sklearn object store sklearn object data output folder insid packag process data azur storag problem pipelin run task score unabl sklearn object error user program fail filenotfounderror errno file directori data output standardscal joblib best wai save sklearn object access pipelin pipelin run argument task score want regist object model registri want save datastor",
        "Question_preprocessed_content":"serialis object pipelin run python packag data train purpos python step pipelin entri script take argument task argument choic train score step code argument load data accord step mention config file process creat standardscal simpleimput object store sklearn object folder insid packag process data azur storag problem pipelin run task unabl sklearn object error best wai save sklearn object access pipelin pipelin run argument want regist object model registri want save datastor",
        "Question_gpt_summary_original":"The user is facing a challenge in serializing objects in an Azure ML pipeline run. The user has a Python package for pre-processing data for training and scoring purposes, which is used as a Python step in a pipeline. The entry script takes an argument, and during the pre-processing, it creates StandardScaler and SimpleImpute objects and stores them in a data\/output folder inside the package. When the pipeline is run again with a different argument, it is unable to find the sklearn objects, resulting in a FileNotFoundError. The user is looking for the best way to save the sklearn objects so that they can be accessed by the pipeline when it is run again with a different argument.",
        "Question_gpt_summary":"user face challeng serial object pipelin run user python packag pre process data train score purpos python step pipelin entri script take argument pre process creat standardscal simpleimput object store data output folder insid packag pipelin run differ argument unabl sklearn object result filenotfounderror user look best wai save sklearn object access pipelin run differ argument",
        "Answer_original_content":"wai regist artifact model registri score configur output pipelin step pipelinedata outputfiledatasetconfig write artifact configur output score run train pipelin output retriev artifact involv experi run pipelin",
        "Answer_preprocessed_content":"wai regist artifact model registri score configur output pipelin step pipelinedata outputfiledatasetconfig write artifact configur output score run train pipelin output retriev artifact involv experi run pipelin",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are: \n1. Register the artifacts in model registry and get them in scoring.\n2. Configure output of pipeline step as PipelineData or OutputFileDatasetConfig, write artifacts to configured output. While scoring, get run of the train pipeline, get its outputs, retrieve the artifacts. This involves experiment name to get run of the pipeline.",
        "Answer_gpt_summary":"possibl solut mention discuss regist artifact model registri score configur output pipelin step pipelinedata outputfiledatasetconfig write artifact configur output score run train pipelin output retriev artifact involv experi run pipelin"
    },
    {
        "Question_title":"In Kedro, How to specify layer to parameters.yml?",
        "Question_body":"<p>Currently, I'm using kedro and kedro-viz.<\/p>\n<p>I can specify a layer of dataset from catalog.yml.<\/p>\n<pre><code>hoge:\n  type: MemoryDataSet\n  layer: raw\n<\/code><\/pre>\n<p>but I don't know how to do it with parameters.yml<\/p>\n<pre><code>step_size: 1\nlearning_rate: 0.01\n<\/code><\/pre>\n<p>if it can be done not in parameters.yml but in run.py, I want to see example code.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1598383098480,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":225.0,
        "Poster_id":10810062.0,
        "Poster_created_time":1545209959320,
        "Poster_last_access_time":1607654795560,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":53.0,
        "Poster_views":7.0,
        "Answer_body":"<p>At the moment layers can only be specified for datasets, not for nodes or parameters.<\/p>\n<p>If you have a specific use case for adding layers to nodes\/parameters, please let us know by opening a feature request in the Kedro repo: <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/issues\" rel=\"nofollow noreferrer\">https:\/\/github.com\/quantumblacklabs\/kedro\/issues<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":14169842.0,
        "Answerer_created_time":1598441874396,
        "Answerer_last_access_time":1658935604608,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":26.0,
        "Answerer_views":15.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63585717",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1598442196703,
        "Question_original_content":"specifi layer paramet yml current viz specifi layer dataset catalog yml hoge type memorydataset layer raw know paramet yml step size learn rate paramet yml run want exampl code",
        "Question_preprocessed_content":"specifi layer current viz specifi layer dataset know want exampl code",
        "Question_gpt_summary_original":"The user is facing a challenge in specifying a layer to parameters.yml in Kedro. They are able to specify a layer of dataset from catalog.yml but do not know how to do it with parameters.yml. They are seeking an example code if it can be done in run.py instead of parameters.yml.",
        "Question_gpt_summary":"user face challeng specifi layer paramet yml abl specifi layer dataset catalog yml know paramet yml seek exampl code run instead paramet yml",
        "Answer_original_content":"moment layer specifi dataset node paramet specif us case ad layer node paramet let know open featur request repo http github com quantumblacklab issu",
        "Answer_preprocessed_content":"moment layer specifi dataset node paramet specif us case ad layer let know open featur request repo",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Commands in the Azure ML yml files",
        "Question_body":"<p>When reading the examples from Microsoft on azure ML CLI v2, they use the symbols:\n&quot;|&quot;, &quot;&gt;&quot;, etc., in their yml files.<\/p>\n<p>What do they mean, and where can I find explanations of possible syntax for the Azure CLI v2 engine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1649142429250,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":100.0,
        "Poster_id":15823622.0,
        "Poster_created_time":1620049475608,
        "Poster_last_access_time":1663926087860,
        "Poster_age":null,
        "Poster_location":"Denmark",
        "Poster_reputation":3.0,
        "Poster_views":8.0,
        "Answer_body":"<p>| - This pipe symbol in YAML document is used for <em><strong>&quot;Multiple line statements&quot;<\/strong><\/em><\/p>\n<pre><code>description: |\n  # Azure Machine Learning &quot;hello world&quot; job\n\n  This is a &quot;hello world&quot; job running in the cloud via Azure Machine Learning!\n\n  ## Description\n\n  Markdown is supported in the studio for job descriptions! You can edit the description there or via CLI.\n<\/code><\/pre>\n<p>in the above example, we need to write some multiple line description. So, we need to use &quot;|&quot; symbol<\/p>\n<p>&quot;&gt;&quot; - This symbol is used to save some content directly to a specific location document.<\/p>\n<pre><code>command: echo &quot;hello world&quot; &gt; .\/outputs\/helloworld.txt\n<\/code><\/pre>\n<p>In this above command, we need to post <strong>&quot;hello world&quot;<\/strong> to <em><strong>&quot;helloworld.txt&quot;<\/strong><\/em><\/p>\n<p>Check the below link for complete documentation regarding YAML files.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command<\/a><\/p>\n<p>All these symbols are the YAML job commands which are used to accomplish a specific task through CLI.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":18428148.0,
        "Answerer_created_time":1646907459852,
        "Answerer_last_access_time":1664066617772,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":1624.0,
        "Answerer_views":1376.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71747545",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1649231701843,
        "Question_original_content":"command yml file read exampl microsoft cli us symbol yml file mean explan possibl syntax azur cli engin",
        "Question_preprocessed_content":"command yml file read exampl microsoft cli us symbol yml file mean explan possibl syntax azur cli engin",
        "Question_gpt_summary_original":"The user is facing challenges in understanding the symbols used in yml files for Azure ML CLI v2 and is seeking explanations for the possible syntax used in the engine.",
        "Question_gpt_summary":"user face challeng understand symbol yml file cli seek explan possibl syntax engin",
        "Answer_original_content":"pipe symbol yaml document multipl line statement descript hello world job hello world job run cloud descript markdown support studio job descript edit descript cli exampl need write multipl line descript need us symbol symbol save content directli specif locat document command echo hello world output helloworld txt command need post hello world helloworld txt check link complet document yaml file http doc microsoft com azur machin learn refer yaml job command symbol yaml job command accomplish specif task cli",
        "Answer_preprocessed_content":"pipe symbol yaml document multipl line statement exampl need write multipl line descript need us symbol symbol save content directli specif locat document command need post hello world check link complet document yaml file symbol yaml job command accomplish specif task cli",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include using the pipe symbol \"|\" for multiple line statements in YAML documents and using the symbol \">\" to save content directly to a specific location in the document. The discussion also suggests referring to the complete documentation regarding YAML files for more information. No personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss includ pipe symbol multipl line statement yaml document symbol save content directli specif locat document discuss suggest refer complet document yaml file inform person opinion bias includ summari"
    },
    {
        "Question_title":"Kubernetes MLflow Service Pod Connection",
        "Question_body":"<p>I have deployed a build of mlflow to a pod in my kubernetes cluster. I'm able to port forward to the mlflow ui, and now I'm attempting to test it. To do this, I am running the following test on a jupyter notebook that is running on another pod in the same cluster.<\/p>\n<pre><code>import mlflow\n\nprint(&quot;Setting Tracking Server&quot;)\ntracking_uri = &quot;http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000&quot;\n\nmlflow.set_tracking_uri(tracking_uri)\n\nprint(&quot;Logging Artifact&quot;)\nmlflow.log_artifact('\/home\/test\/mlflow-example-artifact.png')\n\nprint(&quot;DONE&quot;)\n<\/code><\/pre>\n<p>When I run this though, I get<\/p>\n<pre><code>ConnectionError: HTTPConnectionPool(host='mlflow-tracking-server.default.svc.cluster.local', port=5000): Max retries exceeded with url: \/api\/2.0\/mlflow\/runs\/get? (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object&gt;: Failed to establish a new connection: [Errno 111] Connection refused'))\n<\/code><\/pre>\n<p>The way I have deployed the mlflow pod is shown below in the yaml and docker:<\/p>\n<p>Yaml:<\/p>\n<pre><code>---\napiVersion: apps\/v1\nkind: Deployment\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\nspec:\n  selector:\n    matchLabels:\n      app: mlflow-tracking-server\n  replicas: 1\n  template:\n    metadata:\n      labels:\n        app: mlflow-tracking-server\n    spec:\n      containers:\n      - name: mlflow-tracking-server\n        image: &lt;ECR_IMAGE&gt;\n        ports:\n        - containerPort: 5000\n        env:\n        - name: AWS_MLFLOW_BUCKET\n          value: &lt;S3_BUCKET&gt;\n        - name: AWS_ACCESS_KEY_ID\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_ACCESS_KEY_ID\n        - name: AWS_SECRET_ACCESS_KEY\n          valueFrom:\n            secretKeyRef:\n              name: aws-secret\n              key: AWS_SECRET_ACCESS_KEY\n\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: mlflow-tracking-server\n  namespace: default\n  labels:\n    app: mlflow-tracking-server\n  annotations:\n    service.beta.kubernetes.io\/aws-load-balancer-type: nlb\nspec:\n  externalTrafficPolicy: Local\n  type: LoadBalancer\n  selector:\n    app: mlflow-tracking-server\n  ports:\n    - name: http\n      port: 5000\n      targetPort: http\n<\/code><\/pre>\n<p>While the dockerfile calls a script that executes the mlflow server command: <code>mlflow server --default-artifact-root ${AWS_MLFLOW_BUCKET} --host 0.0.0.0 --port 5000<\/code>, I cannot connect to the service I have created using that mlflow pod.<\/p>\n<p>I have tried using the tracking uri <code>http:\/\/mlflow-tracking-server.default.svc.cluster.local:5000<\/code>, I've tried using the service EXTERNAL-IP:5000, but everything I tried cannot connect and log using the service. Is there anything that I have missed in deploying my mlflow server pod to my kubernetes cluster?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_created_time":1587495286493,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1599477403816,
        "Question_score":2,
        "Question_view_count":855.0,
        "Poster_id":4296407.0,
        "Poster_created_time":1417012835812,
        "Poster_last_access_time":1628782664572,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":945.0,
        "Poster_views":148.0,
        "Answer_body":"<p>Your <strong>mlflow-tracking-server<\/strong> service should have <em>ClusterIP<\/em> type, not <em>LoadBalancer<\/em>. <\/p>\n\n<p>Both pods are inside the same Kubernetes cluster, therefore, there is no reason to use <em>LoadBalancer<\/em> Service type.<\/p>\n\n<blockquote>\n  <p>For some parts of your application (for example, frontends) you may want to expose a Service onto an external IP address, that\u2019s outside of your cluster.\n  Kubernetes ServiceTypes allow you to specify what kind of Service you want. The default is ClusterIP.<\/p>\n  \n  <p>Type values and their behaviors are:<\/p>\n  \n  <ul>\n  <li><p><strong>ClusterIP<\/strong>: Exposes the Service on a cluster-internal IP. Choosing this\n  value makes the Service only reachable from within the cluster. This\n  is the default ServiceType. <\/p><\/li>\n  <li><p><strong>NodePort<\/strong>: Exposes the Service on each Node\u2019s IP at a static port (the NodePort). A > ClusterIP Service, to which the NodePort Service routes, is automatically created. You\u2019ll > be able to contact the NodePort Service, from outside the cluster, by\n  requesting :. <\/p><\/li>\n  <li><strong>LoadBalancer<\/strong>: Exposes the Service\n  externally using a cloud provider\u2019s load balancer. NodePort and\n  ClusterIP Services, to which the external load balancer routes, are\n  automatically created. <\/li>\n  <li><strong>ExternalName<\/strong>: Maps the Service to the contents\n  of the externalName field (e.g. foo.bar.example.com), by returning a\n  CNAME record with its value. No proxying of any kind is set up.<\/li>\n  <\/ul>\n  \n  <p><a href=\"https:\/\/kubernetes.io\/docs\/concepts\/services-networking\/service\/#publishing-services-service-types\" rel=\"nofollow noreferrer\">kubernetes.io<\/a><\/p>\n<\/blockquote>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1587499353943,
        "Answer_score":2.0,
        "Answerer_id":886498.0,
        "Answerer_created_time":1312912826288,
        "Answerer_last_access_time":1633068874412,
        "Answerer_age":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation":5843.0,
        "Answerer_views":153.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61351024",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1587498737656,
        "Question_original_content":"kubernet servic pod connect deploi build pod kubernet cluster abl port forward attempt test run follow test jupyt notebook run pod cluster import print set track server track uri http track server default svc cluster local set track uri track uri print log artifact log artifact home test exampl artifact png print run connectionerror httpconnectionpool host track server default svc cluster local port max retri exceed url api run caus newconnectionerror fail establish new connect errno connect refus wai deploi pod shown yaml docker yaml apivers app kind deploy metadata track server namespac default spec selector matchlabel app track server replica templat metadata label app track server spec contain track server imag port containerport env aw bucket valu aw access kei valuefrom secretkeyref aw secret kei aw access kei aw secret access kei valuefrom secretkeyref aw secret kei aw secret access kei apivers kind servic metadata track server namespac default label app track server annot servic beta kubernet aw load balanc type nlb spec externaltrafficpolici local type loadbalanc selector app track server port http port targetport http dockerfil call script execut server command server default artifact root aw bucket host port connect servic creat pod tri track uri http track server default svc cluster local tri servic extern tri connect log servic miss deploi server pod kubernet cluster",
        "Question_preprocessed_content":"kubernet servic pod connect deploi build pod kubernet cluster abl port forward attempt test run follow test jupyt notebook run pod cluster run wai deploi pod shown yaml docker yaml dockerfil call script execut server command connect servic creat pod tri track uri tri servic tri connect log servic miss deploi server pod kubernet cluster",
        "Question_gpt_summary_original":"The user has encountered challenges in connecting to the mlflow service pod deployed in their Kubernetes cluster. They have attempted to test the connection using a Jupyter notebook on another pod in the same cluster but encountered a ConnectionError. The user has provided the YAML and Docker files used to deploy the mlflow pod and has tried different tracking URIs but still cannot connect to the service. The user is seeking assistance in identifying any missed steps in deploying the mlflow server pod to their Kubernetes cluster.",
        "Question_gpt_summary":"user encount challeng connect servic pod deploi kubernet cluster attempt test connect jupyt notebook pod cluster encount connectionerror user provid yaml docker file deploi pod tri differ track uri connect servic user seek assist identifi miss step deploi server pod kubernet cluster",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"How to remove the entire rows if value is NULL in Azure ML studio",
        "Question_body":"<p>I am preparing the data for regression model. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LFaYl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LFaYl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I want to remove the entire row If all columns have value <code>NULL<\/code>. <\/p>\n\n<p>With Clean Missing Data module seems to me like I only able to remove missing values. But <code>NULL<\/code> is not considers mission value. <\/p>\n\n<p>So are there any other modules that simply can remove the entire row if all values are <code>NULL<\/code>'s<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1515625426463,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":1477.0,
        "Poster_id":6043544.0,
        "Poster_created_time":1457596845392,
        "Poster_last_access_time":1663977598456,
        "Poster_age":null,
        "Poster_location":"San Diego, CA, United States",
        "Poster_reputation":4046.0,
        "Poster_views":825.0,
        "Answer_body":"<p>you could use \"<strong>Execute Python Script<\/strong>\" or \"<strong>Execute R Script<\/strong>\" to archive that. Or just use \"<strong>Apply SQL Transformation<\/strong>\" -> <code>SELECT * FROM tbl1 where column1 IS NULL AND column2 IS NULL<\/code>.... <\/p>\n\n<p>Greetings,\nStefan<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1526560111352,
        "Answer_score":1.0,
        "Answerer_id":9250720.0,
        "Answerer_created_time":1516613003416,
        "Answerer_last_access_time":1537191993167,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":26.0,
        "Answerer_views":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48197524",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1516614808532,
        "Question_original_content":"remov entir row valu null studio prepar data regress model want remov entir row column valu null clean miss data modul like abl remov miss valu null consid mission valu modul simpli remov entir row valu null",
        "Question_preprocessed_content":"remov entir row valu null studio prepar data regress model want remov entir row column valu clean miss data modul like abl remov miss valu consid mission valu modul simpli remov entir row valu",
        "Question_gpt_summary_original":"The user is facing a challenge in preparing data for a regression model in Azure ML studio. They want to remove entire rows if all columns have a value of NULL, but the Clean Missing Data module only removes missing values and does not consider NULL as a missing value. The user is seeking information on other modules that can remove entire rows with all NULL values.",
        "Question_gpt_summary":"user face challeng prepar data regress model studio want remov entir row column valu null clean miss data modul remov miss valu consid null miss valu user seek inform modul remov entir row null valu",
        "Answer_original_content":"us execut python script execut script archiv us appli sql transform select tbl column null column null greet stefan",
        "Answer_preprocessed_content":"us execut python script execut script archiv us appli sql transform greet stefan",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Using \"Execute Python Script\" or \"Execute R Script\" to remove entire rows with all NULL values.\n- Using \"Apply SQL Transformation\" and the query \"SELECT * FROM tbl1 where column1 IS NULL AND column2 IS NULL\" to remove entire rows with all NULL values.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss execut python script execut script remov entir row null valu appli sql transform queri select tbl column null column null remov entir row null valu person opinion bias includ summari"
    },
    {
        "Question_title":"Dask: AttributeError: 'DataFrame' object has no attribute '_getitem_array'",
        "Question_body":"<p>I've got some data on S3 bucket that I want to work with. <\/p>\n\n<p>I've imported it using:<\/p>\n\n<pre><code>import boto3\nimport dask.dataframe as dd\n\ndef import_df(key):\n        s3 = boto3.client('s3')\n        df = dd.read_csv('s3:\/\/...\/' + key ,encoding='latin1')\n        return df\n\nkey = 'Churn\/CLEANED_data\/file.csv'\ntrain = import_df(key)\n<\/code><\/pre>\n\n<p>I can see that the data has been imported correctly using:<\/p>\n\n<pre><code>train.head()\n<\/code><\/pre>\n\n<p>but when I try simple operation (<a href=\"https:\/\/docs.dask.org\/en\/latest\/dataframe.html\" rel=\"nofollow noreferrer\">taken from this dask doc<\/a>):<\/p>\n\n<pre><code>train_churn = train[train['CON_CHURN_DECLARATION'] == 1]\ntrain_churn.compute()\n<\/code><\/pre>\n\n<p>I've got Error:<\/p>\n\n<blockquote>\n  <p>AttributeError                            Traceback (most recent call\n  last)  in ()<\/p>\n  \n  <p>1 train_churn = train[train['CON_CHURN_DECLARATION'] == 1]<\/p>\n  \n  <p>----> 2 train_churn.compute()<\/p>\n  \n  <p>~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/dask\/base.py in\n  compute(self, **kwargs)\n      152         dask.base.compute\n      153         \"\"\"\n  --> 154         (result,) = compute(self, traverse=False, **kwargs)\n      155         return result\n      156<\/p>\n  \n  <p>AttributeError: 'DataFrame' object has no attribute '_getitem_array'<\/p>\n<\/blockquote>\n\n<p>Full error here: <a href=\"https:\/\/textuploader.com\/11lg7\" rel=\"nofollow noreferrer\">Error Upload<\/a><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_created_time":1564579246737,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":2742.0,
        "Poster_id":4815601.0,
        "Poster_created_time":1429630461500,
        "Poster_last_access_time":1663951534340,
        "Poster_age":null,
        "Poster_location":"Warszawa, Polska",
        "Poster_reputation":938.0,
        "Poster_views":137.0,
        "Answer_body":"<p>I was facing a similar issue when trying to read from s3 files, ultimately solved by updating dask to most recent version (I think the one sagemaker instances start with by default is deprecated)<\/p>\n\n<h2>Install\/Upgrade packages and dependencies (from notebook)<\/h2>\n\n<pre><code>! python -m pip install --upgrade dask\n! python -m pip install fsspec\n! python -m pip install --upgrade s3fs\n<\/code><\/pre>\n\n<p>Hope this helps!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1574362465680,
        "Answer_score":1.0,
        "Answerer_id":6011629.0,
        "Answerer_created_time":1456986606312,
        "Answerer_last_access_time":1663952857883,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":757.0,
        "Answerer_views":80.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57291797",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1574121568172,
        "Question_original_content":"dask attributeerror datafram object attribut getitem arrai got data bucket want work import import boto import dask datafram def import kei boto client read csv kei encod latin return kei churn clean data file csv train import kei data import correctli train head try simpl oper taken dask doc train churn train train churn declar train churn comput got error attributeerror traceback recent train churn train train churn declar train churn comput anaconda env python lib python site packag dask base comput self kwarg dask base comput result comput self travers fals kwarg return result attributeerror datafram object attribut getitem arrai error error upload",
        "Question_preprocessed_content":"dask attributeerror datafram object attribut got data bucket want work import data import correctli try simpl oper got error attributeerror traceback comput comput return result attributeerror datafram object attribut error error upload",
        "Question_gpt_summary_original":"The user is encountering an error while trying to perform a simple operation on a Dask DataFrame imported from an S3 bucket using the dd.read_csv() function. The error message states that the 'DataFrame' object has no attribute '_getitem_array'.",
        "Question_gpt_summary":"user encount error try perform simpl oper dask datafram import bucket read csv function error messag state datafram object attribut getitem arrai",
        "Answer_original_content":"face similar issu try read file ultim solv updat dask recent version think instanc start default deprec instal upgrad packag depend notebook python pip instal upgrad dask python pip instal fsspec python pip instal upgrad sf hope help",
        "Answer_preprocessed_content":"face similar issu try read file ultim solv updat dask recent version packag depend hope help",
        "Answer_gpt_summary_original":"Solution: The user can try updating Dask to the most recent version, as the default version on Sagemaker instances may be deprecated. They can also try installing\/upgrading the necessary packages and dependencies using the provided code snippet.",
        "Answer_gpt_summary":"solut user try updat dask recent version default version instanc deprec try instal upgrad necessari packag depend provid code snippet"
    },
    {
        "Question_title":"AzureML: TabularDataset.to_pandas_dataframe() hangs when parquet file is empty",
        "Question_body":"<p>I have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I try to load the dataset using the API <code>TabularDataset.to_pandas_dataframe()<\/code>, it continues forever (hangs), if there are empty parquet files included in the Dataset. If the tabular dataset doesn't include those empty parquet files, <code>TabularDataset.to_pandas_dataframe()<\/code> completes within few minutes.<\/p>\n<p>By empty parquet file, I mean that the if I read the individual parquet file using pandas (pd.read_parquet()), it results in an empty DF (df.empty == True).<\/p>\n<p>I discovered the root cause while working on another issue mentioned <code>[here][1]<\/code>.<\/p>\n<p><strong>My question is how can make <code>TabularDataset.to_pandas_dataframe()<\/code> work even when there are empty parquet files?<\/strong><\/p>\n<p><strong>Update<\/strong>\nThe issue has been fixed in the following version:<\/p>\n<ul>\n<li>azureml-dataprep : 3.0.1<\/li>\n<li>azureml-core :  1.40.0<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1644552863947,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1648643447772,
        "Question_score":1,
        "Question_view_count":300.0,
        "Poster_id":406896.0,
        "Poster_created_time":1280505139752,
        "Poster_last_access_time":1663935737867,
        "Poster_age":null,
        "Poster_location":"Bangalore, India",
        "Poster_reputation":4265.0,
        "Poster_views":403.0,
        "Answer_body":"<p>Thanks for reporting it.\nThis is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.<\/p>\n<p>I could not repro the hang on multiple files, though, so if you could provide more info on that would be nice.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":15643694.0,
        "Answerer_created_time":1618467374027,
        "Answerer_last_access_time":1654823850247,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":61.0,
        "Answerer_views":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71075255",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1646432724768,
        "Question_original_content":"tabulardataset panda datafram hang parquet file creat tabular dataset python api data question bunch parquet file parquet file size resid azur data lake gen spread multipl partit try load dataset api tabulardataset panda datafram continu forev hang parquet file includ dataset tabular dataset includ parquet file tabulardataset panda datafram complet minut parquet file mean read individu parquet file panda read parquet result true discov root caus work issu mention question tabulardataset panda datafram work parquet file updat issu fix follow version dataprep core",
        "Question_preprocessed_content":"hang parquet file creat tabular dataset python api data question bunch parquet file resid azur data lake gen spread multipl partit try load dataset api continu forev parquet file includ dataset tabular dataset includ parquet file complet minut parquet file mean read individu parquet file panda result discov root caus work issu mention question work parquet file updat issu fix follow version dataprep core",
        "Question_gpt_summary_original":"The user is facing a challenge with AzureML's TabularDataset API when trying to load a dataset containing empty parquet files. The API hangs indefinitely when attempting to load the dataset using the to_pandas_dataframe() function. The user is seeking a solution to make the function work even when empty parquet files are included in the dataset. The issue has been resolved in newer versions of the relevant software.",
        "Question_gpt_summary":"user face challeng tabulardataset api try load dataset contain parquet file api hang indefinit attempt load dataset panda datafram function user seek solut function work parquet file includ dataset issu resolv newer version relev softwar",
        "Answer_original_content":"thank report bug handl parquet file column row set fix includ releas repro hang multipl file provid info nice",
        "Answer_preprocessed_content":"thank report bug handl parquet file column row set fix includ releas repro hang multipl file provid info nice",
        "Answer_gpt_summary_original":"Solution: The issue has been resolved in newer versions of the relevant software. The bug in handling empty parquet files with columns has been fixed and will be included in the next release. However, the user is requested to provide more information if they are still facing the issue with multiple files.",
        "Answer_gpt_summary":"solut issu resolv newer version relev softwar bug handl parquet file column fix includ releas user request provid inform face issu multipl file"
    },
    {
        "Question_title":"Mounting an S3 bucket in docker in a clearml agent",
        "Question_body":"<p>What is the best practice for mounting an S3 container inside a docker image that will be using as a ClearML agent?  I can think of 3 solutions, but have been unable to get any to work currently:<\/p>\n<ol>\n<li>Use <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/use_cases\/clearml_agent_use_case_examples.html?highlight=docker\" rel=\"nofollow noreferrer\">prefabbed configuration in ClearML<\/a>, specifically CLEARML_AGENT_K8S_HOST_MOUNT.  For this to work, the S3 bucket would be mounted separately on the host using <a href=\"https:\/\/rclone.org\/\" rel=\"nofollow noreferrer\">rclone<\/a> and then remapped into docker. This appears to only apply to Kubernetes and not Docker - and therefore would not work.<\/li>\n<li>Mount using s3fuse as specified <a href=\"https:\/\/stackoverflow.com\/questions\/35189251\/docker-mount-s3-container\">here<\/a>.  The issue is will it work with the S3 bucket secret stored in ClearML browser sessions?  This would also appear to be complicated and require custom docker images, not to mention running the docker image as --privileged or similar.<\/li>\n<li>Pass arguments to docker using &quot;docker_args and docker_bash_setup_script arguments to Task.create()&quot; as specified in the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/docs\/release_notes\/ver_1_0.html\" rel=\"nofollow noreferrer\">1.0 release notes<\/a>.  This would be similar to (1), but the arguments would be for <a href=\"https:\/\/docs.docker.com\/storage\/bind-mounts\/\" rel=\"nofollow noreferrer\">bind-mounting the volume<\/a>.  I do not see much documentation or examples on how this new feature may be used for this end.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1620788331577,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1621022250560,
        "Question_score":1,
        "Question_view_count":770.0,
        "Poster_id":2140414.0,
        "Poster_created_time":1362580980910,
        "Poster_last_access_time":1663351509150,
        "Poster_age":null,
        "Poster_location":"Akron, OH, USA",
        "Poster_reputation":4013.0,
        "Poster_views":72.0,
        "Answer_body":"<p>I was able to get another option entirely to work, namely, mount a drive on in WSL and then pass it to Docker.  Let's get to it:<\/p>\n<p>Why not host in Windows itself, why rclone in WSL?<\/p>\n<ul>\n<li>Docker running on WSL <a href=\"https:\/\/github.com\/billziss-gh\/winfsp\/issues\/61\" rel=\"nofollow noreferrer\">cannot access drives mounted through winfsp<\/a> (what rclone uses)<\/li>\n<\/ul>\n<p>Steps to mount the drive in ClearML in Windows:<\/p>\n<ul>\n<li>You can install rclone in WSL and the mount will be accessible to docker\n<ul>\n<li>create the folder <code>\/data\/my-mount<\/code> (this needs to be in <code>\/data<\/code> - I don't know why and I can't find out with a Google search, but I found out about it <a href=\"https:\/\/forum.rclone.org\/t\/fusermount-permission-denied-in-docker-rclone\/13914\/5\" rel=\"nofollow noreferrer\">here<\/a>)<\/li>\n<li>You can put the configuration file in windows (use the <code>--config<\/code> option).<\/li>\n<li>Note: ClearML will not support spaces in mounted paths, even though docker will.  Therefore your path has to be <code>\/data\/my-mount<\/code> rather than <code>\/data\/my mount<\/code>.  There is a <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/358\" rel=\"nofollow noreferrer\">bug that I opened about this<\/a>.<\/li>\n<\/ul>\n<\/li>\n<li>You can test mounting by calling docker and mounting the file.\n<ul>\n<li>Example: <code>docker run -it -v \\\\wsl$\\Ubuntu\\data:\/data my-docker-image:latest ls \/data\/my-mount<\/code><\/li>\n<li>Note: You will have to mount \/data rather than \/data\/my-mount, otherwise you may get this error: <code>docker: Error response from daemon: error while creating mount source path<\/code><\/li>\n<\/ul>\n<\/li>\n<li>Now, you can setup the clearml.conf file in <code>C:\\Users\\Myself\\clearml.conf<\/code> such that:<\/li>\n<\/ul>\n<pre><code>default_docker: {\n   # default docker image to use when running in docker mode\n   image: &quot;my-docker-image:latest&quot;\n\n   # optional arguments to pass to docker image\n   arguments: [&quot;-v&quot;,&quot;\\\\wsl$\\Ubuntu\\data:\/data&quot;, ]\n}\n<\/code><\/pre>\n<ul>\n<li>Note that you can also run clearml-agent out of WSL and then would only need to specify <code>[&quot;-v&quot;,&quot;\/data:\/data&quot;, ]<\/code>.<\/li>\n<li>Run clearml agent in cmd: <code>clearml-agent daemon --docker<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1621012052112,
        "Answer_score":0.0,
        "Answerer_id":2140414.0,
        "Answerer_created_time":1362580980910,
        "Answerer_last_access_time":1663351509150,
        "Answerer_age":null,
        "Answerer_location":"Akron, OH, USA",
        "Answerer_reputation":4013.0,
        "Answerer_views":72.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67496760",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1621009841940,
        "Question_original_content":"mount bucket docker agent best practic mount contain insid docker imag agent think solut unabl work current us prefab configur specif agent host mount work bucket mount separ host rclone remap docker appear appli kubernet docker work mount sfuse specifi issu work bucket secret store browser session appear complic requir custom docker imag mention run docker imag privileg similar pass argument docker docker arg docker bash setup script argument task creat specifi releas note similar argument bind mount volum document exampl new featur end",
        "Question_preprocessed_content":"mount bucket docker agent best practic mount contain insid docker imag agent think solut unabl work current us prefab configur specif work bucket mount separ host rclone remap docker appear appli kubernet docker work mount fuse specifi issu work bucket secret store browser session appear complic requir custom docker imag mention run docker imag similar pass argument docker argument specifi releas note similar argument volum document exampl new featur end",
        "Question_gpt_summary_original":"The user is facing challenges in mounting an S3 container inside a docker image that will be used as a ClearML agent. They have considered three solutions, but none of them have worked so far. The first solution using prefabbed configuration in ClearML only applies to Kubernetes and not Docker. The second solution using s3fuse appears to be complicated and requires custom docker images and running the docker image as --privileged or similar. The third solution involves passing arguments to docker using \"docker_args and docker_bash_setup_script arguments to Task.create()\" but there is not much documentation or examples on how this new feature may be used for this end.",
        "Question_gpt_summary":"user face challeng mount contain insid docker imag agent consid solut work far solut prefab configur appli kubernet docker second solut sfuse appear complic requir custom docker imag run docker imag privileg similar solut involv pass argument docker docker arg docker bash setup script argument task creat document exampl new featur end",
        "Answer_original_content":"abl option entir work mount drive wsl pass docker let host window rclone wsl docker run wsl access drive mount winfsp rclone us step mount drive window instal rclone wsl mount access docker creat folder data mount need data know googl search configur file window us config option note support space mount path docker path data mount data mount bug open test mount call docker mount file exampl docker run wsl ubuntu data data docker imag latest data mount note mount data data mount error docker error respons daemon error creat mount sourc path setup conf file user conf default docker default docker imag us run docker mode imag docker imag latest option argument pass docker imag argument wsl ubuntu data data note run agent wsl need specifi data data run agent cmd agent daemon docker",
        "Answer_preprocessed_content":"abl option entir work mount drive wsl pass docker let host window rclone wsl docker run wsl access drive mount winfsp step mount drive window instal rclone wsl mount access docker creat folder configur file window note support space mount path docker path bug open test mount call docker mount file exampl note mount data error setup conf file note run agent wsl need specifi run agent cmd",
        "Answer_gpt_summary_original":"Solution: The user was able to mount a drive on in WSL and then pass it to Docker. They installed rclone in WSL and created the folder \/data\/my-mount. They put the configuration file in windows (use the --config option). They tested mounting by calling docker and mounting the file. They set up the clearml.conf file in C:\\Users\\Myself\\clearml.conf such that default_docker includes the image and optional arguments to pass to docker image. They ran clearml agent in cmd: clearml-agent daemon --docker.",
        "Answer_gpt_summary":"solut user abl mount drive wsl pass docker instal rclone wsl creat folder data mount configur file window us config option test mount call docker mount file set conf file user conf default docker includ imag option argument pass docker imag ran agent cmd agent daemon docker"
    },
    {
        "Question_title":"What type should the returned scores from an R scoring script?",
        "Question_body":"<p>I am attempting to develop an Azure ML experiment that uses R to perform predictions of a continuous response variable. The initial experiment is relatively simple, incorporating only a few experiment items, including \"Create R Model\", \"Train Model\" and \"Score Model\", along with some data input.<\/p>\n\n<p>I have written a training script and a scoring script, both of which appear to execute without errors when I run the experiment within ML Studio. However, when I examine the scored dataset, the score values are all missing values. So I am concerned that my scoring script could be returning scores incorrectly. Can anyone advise what type I should be returning? Is it meant to be a single column data.frame, or something else?<\/p>\n\n<p>It is also possible that my scores are not being properly calculated within the scoring script, although I have run the training and scoring scripts within R Studio, which shows the expected results. It would also be helpful if someone could suggest how to perform debugging of my scoring script in some way, so that I could determine whereabouts the code is failing to behave as expected.<\/p>\n\n<p>Thanks, Paul<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1464593030317,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":238.0,
        "Poster_id":6398390.0,
        "Poster_created_time":1464571689107,
        "Poster_last_access_time":1470182061488,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":3.0,
        "Poster_views":2.0,
        "Answer_body":"<p>Try using this sample and compare with yours - <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Compare-Sample-5-in-R-vs-Azure-ML-1<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":4589073.0,
        "Answerer_created_time":1424453610300,
        "Answerer_last_access_time":1505365713168,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1237.0,
        "Answerer_views":116.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37519858",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1464675335140,
        "Question_original_content":"type return score score script attempt develop experi us perform predict continu respons variabl initi experi rel simpl incorpor experi item includ creat model train model score model data input written train script score script appear execut error run experi studio examin score dataset score valu miss valu concern score script return score incorrectli advis type return meant singl column data frame possibl score properli calcul score script run train score script studio show expect result help suggest perform debug score script wai determin whereabout code fail behav expect thank paul",
        "Question_preprocessed_content":"type return score score script attempt develop experi us perform predict continu respons variabl initi experi rel simpl incorpor experi item includ creat model train model score model data input written train script score script appear execut error run experi studio examin score dataset score valu miss valu concern score script return score incorrectli advis type return meant singl column possibl score properli calcul score script run train score script studio show expect result help suggest perform debug score script wai determin whereabout code fail behav expect thank paul",
        "Question_gpt_summary_original":"The user is facing challenges in developing an Azure ML experiment that uses R to predict a continuous response variable. The scoring script appears to execute without errors, but the score values are all missing values. The user is unsure about the type of data to be returned and is seeking advice on how to debug the scoring script to determine where the code is failing to behave as expected.",
        "Question_gpt_summary":"user face challeng develop experi us predict continu respons variabl score script appear execut error score valu miss valu user unsur type data return seek advic debug score script determin code fail behav expect",
        "Answer_original_content":"try sampl compar http galleri cortanaintellig com experi compar sampl azur",
        "Answer_preprocessed_content":"try sampl compar",
        "Answer_gpt_summary_original":"Solution: The discussion suggests trying a sample experiment available on the Cortana Intelligence Gallery and comparing it with the user's experiment to identify any differences and potential issues. No other solutions are mentioned.",
        "Answer_gpt_summary":"solut discuss suggest try sampl experi avail cortana intellig galleri compar user experi identifi differ potenti issu solut mention"
    },
    {
        "Question_title":"AzureML: ResolvePackageNotFound azureml-dataprep",
        "Question_body":"<p>I've got a basic ScriptStep in my AML Pipeline and it's just trying to read an attached dataset. When i execute this simple example, the pipeline fails with the following in the driver log:<\/p>\n\n<blockquote>\n  <p>ImportError: azureml-dataprep is not installed. Dataset cannot be used\n  without azureml-dataprep. Please make sure\n  azureml-dataprep[fuse,pandas] is installed by specifying it in the\n  conda dependencies. pandas is optional and should be only installed if\n  you intend to create a pandas DataFrame from the dataset.<\/p>\n<\/blockquote>\n\n<p>I then modified my step to include the conda package but then the driver fails with \"ResolvePackageNotFound: azureml-dataprep\". The entire log file can be accessed <a href=\"https:\/\/www.dropbox.com\/s\/372ht6jkvzu9loo\/conda.err.txt?dl=0\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<pre><code># create a new runconfig object\nrun_config = RunConfiguration()\nrun_config.environment.docker.enabled = True\nrun_config.environment.docker.base_image = DEFAULT_CPU_IMAGE\nrun_config.environment.python.user_managed_dependencies = False\nrun_config.environment.python.conda_dependencies = CondaDependencies.create(conda_packages=['azureml-dataprep[pandas,fuse]'])\n\nsource_directory = '.\/read-step'\nprint('Source directory for the step is {}.'.format(os.path.realpath(source_directory)))\nstep2 = PythonScriptStep(name=\"read_step\",\n                         script_name=\"Read.py\", \n                         arguments=[\"--dataFilePath\", dataset.as_named_input('local_ds').as_mount() ],\n                         compute_target=aml_compute, \n                         source_directory=source_directory,\n                         runconfig=run_config,\n                         allow_reuse=False)\n<\/code><\/pre>\n\n<p>I'm out of ideas, would deeply appreciate any help here!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1587154334353,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1591825691630,
        "Question_score":1,
        "Question_view_count":1244.0,
        "Poster_id":1228906.0,
        "Poster_created_time":1330016065408,
        "Poster_last_access_time":1662160983830,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1704.0,
        "Poster_views":232.0,
        "Answer_body":"<p>The <code>azureml-sdk<\/code> isn't available on conda, you need to install it with <code>pip<\/code>.<\/p>\n\n<pre><code>myenv = Environment(name=\"myenv\")\nconda_dep = CondaDependencies().add_pip_package(\"azureml-dataprep[pandas,fuse]\")\nmyenv.python.conda_dependencies=conda_dep\nrun_config.environment = myenv\n<\/code><\/pre>\n\n<p>For more information, about this error, the logs tab has a log named <code>20_image_build_log.txt<\/code> which Docker build logs. It contains the error where <code>conda<\/code> failed to failed to find <code>azureml-dataprep<\/code><\/p>\n\n<p>EDIT:<\/p>\n\n<p>Soon, you won't have to specify this dependency anymore. the Azure Data4ML team says <code>azureml-dataprep[pandas,fuse]<\/code> is getting added as a dependency for <code>azureml-defaults<\/code> which is automatically installed on all images. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1587416360076,
        "Answer_score":1.0,
        "Answerer_id":3842610.0,
        "Answerer_created_time":1405457120427,
        "Answerer_last_access_time":1663947733100,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":3359.0,
        "Answerer_views":555.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61279914",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1587162956780,
        "Question_original_content":"resolvepackagenotfound dataprep got basic scriptstep aml pipelin try read attach dataset execut simpl exampl pipelin fail follow driver log importerror dataprep instal dataset dataprep sure dataprep fuse panda instal specifi conda depend panda option instal intend creat panda datafram dataset modifi step includ conda packag driver fail resolvepackagenotfound dataprep entir log file access creat new runconfig object run config runconfigur run config environ docker enabl true run config environ docker base imag default cpu imag run config environ python user manag depend fals run config environ python conda depend condadepend creat conda packag dataprep panda fuse sourc directori read step print sourc directori step format path realpath sourc directori step pythonscriptstep read step script read argument datafilepath dataset name input local mount comput target aml comput sourc directori sourc directori runconfig run config allow reus fals idea deepli appreci help",
        "Question_preprocessed_content":"resolvepackagenotfound dataprep got basic scriptstep aml pipelin try read attach dataset execut simpl exampl pipelin fail follow driver log importerror dataprep instal dataset dataprep sure dataprep instal specifi conda depend panda option instal intend creat panda datafram dataset modifi step includ conda packag driver fail resolvepackagenotfound dataprep entir log file access idea deepli appreci help",
        "Question_gpt_summary_original":"The user is encountering challenges while executing a basic ScriptStep in their AML Pipeline. The pipeline fails due to the absence of the azureml-dataprep package. The user then modifies the step to include the conda package, but the driver fails with \"ResolvePackageNotFound: azureml-dataprep\". The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user encount challeng execut basic scriptstep aml pipelin pipelin fail absenc dataprep packag user modifi step includ conda packag driver fail resolvepackagenotfound dataprep user seek help resolv issu",
        "Answer_original_content":"sdk isn avail conda need instal pip myenv environ myenv conda dep condadepend add pip packag dataprep panda fuse myenv python conda depend conda dep run config environ myenv inform error log tab log name imag build log txt docker build log contain error conda fail fail dataprep edit soon won specifi depend anymor azur dataml team sai dataprep panda fuse get ad depend default automat instal imag",
        "Answer_preprocessed_content":"isn avail conda need instal inform error log tab log name docker build log contain error fail fail edit soon won specifi depend anymor azur data team sai get ad depend automat instal imag",
        "Answer_gpt_summary_original":"Solution:\n- Install the `azureml-sdk` with `pip`.\n- Modify the ScriptStep to include the `conda` package with `azureml-dataprep[pandas,fuse]`.\n- Check the logs tab for the error message where `conda` failed to find `azureml-dataprep`.\n- In the future, `azureml-dataprep[pandas,fuse]` will be added as a dependency for `azureml-defaults`, which is automatically installed on all images. \n\nNote: No personal opinions or biases were included in this summary.",
        "Answer_gpt_summary":"solut instal sdk pip modifi scriptstep includ conda packag dataprep panda fuse check log tab error messag conda fail dataprep futur dataprep panda fuse ad depend default automat instal imag note person opinion bias includ summari"
    },
    {
        "Question_title":"What is called within a sagemaker custom (training) container?",
        "Question_body":"<p>Somewhere this spring the behaviour of the sagemaker docker image changed and I cannot find the way I need to construct it now.<\/p>\n<p><strong>Directory structure<\/strong><\/p>\n<pre><code>\/src\/some\/package\n\/project1\n    \/some_entrypoint.py\n    \/some_notebook.ipynb\n\/project2\n    \/another_entrypoint.py\n    \/another_notebook.ipynb\nsetup.py\n<\/code><\/pre>\n<p><strong>Docker file<\/strong><\/p>\n<p>Note that I want to shift tensorflow version, so I changed the <code>FROM<\/code> to the latest version. This was the\nbreaking change.<\/p>\n<pre><code># Core\nFROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com\/tensorflow-training:2.3.0-cpu-py37-ubuntu18.04\n\nCOPY . \/opt\/ml\/code\/all\/\nRUN pip install \/opt\/ml\/code\/all\/\n\nWORKDIR &quot;\/opt\/ml\/code&quot;\n<\/code><\/pre>\n<p><strong>Python code<\/strong><\/p>\n<p>This code should start the entrypoints, for example here we have the code of some_notebook.ipynb. I tried all possible combinations of working directory + source_dir (None, '.', or '..'), entry_point (with or without \/), dependencies ('src')...<\/p>\n<ul>\n<li>if setup is present it tries to call my project as a module (python -m some_entrypoint)<\/li>\n<li>if not, it often is not able to find my entrypoint. Which I don't understand because the TensorFlow is supposed to add it to the container, isn't it?<\/li>\n<\/ul>\n<pre><code>estimator = TensorFlow(\n   entry_point='some_entrypoint.py', \n   image_name='ECR.dkr.ecr.eu-west-1.amazonaws.com\/overall-project\/sagemaker-training:latest',\n   source_dir='.',\n#    dependencies=['..\/src\/'],\n   script_mode=True,\n\n   train_instance_type='ml.m5.4xlarge',\n   train_instance_count=1,\n   train_max_run=60*60,  # seconds * minutes\n   train_max_wait=60*60,  # seconds * minutes. Must be &gt;= train_max_run\n   hyperparameters=hyperparameters,\n   metric_definitions=metrics,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/some-data\/&quot;}\n#   , wait=False\n)\n<\/code><\/pre>\n<p>Ideally I would want to understand the logic within: what is called given what settings?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1597694559337,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597697877660,
        "Question_score":0,
        "Question_view_count":61.0,
        "Poster_id":7441757.0,
        "Poster_created_time":1484838464572,
        "Poster_last_access_time":1663858511743,
        "Poster_age":null,
        "Poster_location":"Amsterdam, Nederland",
        "Poster_reputation":3937.0,
        "Poster_views":387.0,
        "Answer_body":"<p>when the training container runs, your entry_point script will be executed.<\/p>\n<p>Since your notebook file and entry_point script are under the same directory, your <code>source_dir<\/code> should just be &quot;.&quot;<\/p>\n<p>Does your entry_point script import any modules that are not installed by the tensorflow training container by default? Also could you share your stacktrace of the error?<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":11278913.0,
        "Answerer_created_time":1553882107003,
        "Answerer_last_access_time":1605854360676,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":294.0,
        "Answerer_views":28.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63457857",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1600272249596,
        "Question_original_content":"call custom train contain spring behaviour docker imag chang wai need construct directori structur src packag project entrypoint notebook ipynb project entrypoint notebook ipynb setup docker file note want shift tensorflow version chang latest version break chang core dkr ecr west amazonaw com tensorflow train cpu ubuntu copi opt code run pip instal opt code workdir opt code python code code start entrypoint exampl code notebook ipynb tri possibl combin work directori sourc dir entri point depend src setup present tri project modul python entrypoint abl entrypoint understand tensorflow suppos add contain isn estim tensorflow entri point entrypoint imag ecr dkr ecr west amazonaw com overal project train latest sourc dir depend src script mode true train instanc type xlarg train instanc count train max run second minut train max wait second minut train max run hyperparamet hyperparamet metric definit metric role role framework version version estim fit train data wait fals ideal want understand logic call given set",
        "Question_preprocessed_content":"call custom contain spring behaviour docker imag chang wai need construct directori structur docker file note want shift tensorflow version chang latest version break chang python code code start entrypoint exampl code tri possibl combin work directori depend setup present tri project modul abl entrypoint understand tensorflow suppos add contain isn ideal want understand logic call given set",
        "Question_gpt_summary_original":"The user is facing challenges with the behavior of the sagemaker docker image, specifically with the directory structure, docker file, and python code. They are trying to shift the tensorflow version and start the entrypoints, but have tried all possible combinations of working directory, source_dir, entry_point, and dependencies, and are still unable to find the entrypoint. The user is seeking to understand the logic within and what is called given the settings.",
        "Question_gpt_summary":"user face challeng behavior docker imag specif directori structur docker file python code try shift tensorflow version start entrypoint tri possibl combin work directori sourc dir entri point depend unabl entrypoint user seek understand logic call given set",
        "Answer_original_content":"train contain run entri point script execut notebook file entri point script directori sourc dir entri point script import modul instal tensorflow train contain default share stacktrac error",
        "Answer_preprocessed_content":"train contain run script execut notebook file script directori script import modul instal tensorflow train contain default share stacktrac error",
        "Answer_gpt_summary_original":"Solutions provided:\n- Set the source_dir to \".\" since the notebook file and entry_point script are under the same directory.\n- Check if the entry_point script imports any modules that are not installed by the tensorflow training container by default.\n- Share the stacktrace of the error to get more specific help.\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid set sourc dir notebook file entri point script directori check entri point script import modul instal tensorflow train contain default share stacktrac error specif help person opinion bias includ respons"
    },
    {
        "Question_title":"Not Getting the Expected Output in AzureML",
        "Question_body":"<p>Background: I am working on a project that aims to classify product reviews into positive and negative using Sentiment Analysis in Azure ML. I got stuck when I was classifying reviews into different departments.<\/p>\n\n<p>I am basically reading words from csv files and checking whether the review(v: list of sentences) contains these words. If some of these words are found in the review then I am noting the sentence number and pushing it into respective lists( FinanceList, QualityList, LogisticsList ). In the end I am converting the lists to strings and pushing them into a dataframe.<\/p>\n\n<p>The output is not getting logged for the print statements that I have written in the script in Azure ML.<\/p>\n\n<p>The values in the dataframe are always turning out to be 0 but when I run the code locally I get the expected output.<\/p>\n\n<p>Description of First Image: The columns of the dataframe showing 0 values.<\/p>\n\n<p>Description of Second Image: I have highlighted the expected output that I got locally for the same review which was used in AzureML.<\/p>\n\n<p><a href=\"http:\/\/imgur.com\/0C3wcYj.png\" rel=\"nofollow\">Image 1<\/a><\/p>\n\n<p><a href=\"http:\/\/i.imgur.com\/lyHsM8z.png\" rel=\"nofollow\">Image 2<\/a><\/p>\n\n<p>The things that I have already checked:<\/p>\n\n<ol>\n<li>The csv files are read properly.<\/li>\n<li>The review contains the words that I am searching.<\/li>\n<\/ol>\n\n<p>I am unable to understand where I am going wrong.<\/p>\n\n<p>'<\/p>\n\n<pre><code>import csv\nimport math\nimport pandas as pd\nimport numpy as np\n\ndef azureml_main( data, ud):\n\n   FinanceDept = []\n   LogisticsDept = []\n   QualityDept = []\n  #Reading from the csv files\n   with open('.\\Script Bundle\\\\quality1.csv', 'rb') as fin:\n      reader = csv.reader(fin)\n      QualityDept = list(reader)\n\n   with open('.\\Script Bundle\\\\finance1.csv', 'rb') as f:\n      reader = csv.reader(f)\n      FinanceDept = list(reader)\n\n   with open('.\\Script Bundle\\\\logistics1.csv', 'rb') as f:\n      reader = csv.reader(f)\n      LogisticDept = list(reader)\n\n   FinanceList = []\n   LogisticsList = []\n   QualityList = []\n\n#Initializing the Lists   \n   FinanceList.append(0)\n   LogisticsList.append(0)\n   QualityList.append(0)\n\n   rev = data['Data']\n   v = rev[0].split('.')\n\n   print FinanceDept\n\n   S = 0   \n   for sentence in v:\n      S = S + 1\n      z = sentence.split(' ')\n      for c in z:\n         c = c.lower()\n         if c in FinanceDept and S not in FinanceList:\n            FinanceList.append(S)\n         if c in LogisticsDept and S not in LogisticsList:\n            LogisticsList.append(S)\n         if c in QualityDept and S not in QualityList:\n            QualityList.append(S)\n   #Compute User Reputation Score\n   Upvotes = int(ud['upvotes'].tolist()[0])\n   Downvotes = int(ud['downvotes'].tolist()[0])\n   TotalVotes = max(1,Upvotes+Downvotes)\n\n   q = data['Score']\n\n   print FinanceList\n\n   repScore = float(Upvotes)\/TotalVotes \n   repScore = repScore*float( q[0] )\n   str1 = ','.join(str(e) for e in FinanceList) \n   str2 = ','.join(str(e) for e in QualityList)\n   str3 = ','.join(str(e) for e in LogisticsList)\n\n   x = ud['id']\n\n   #df = pd.DataFrame(  [str(repScore), str1  , str2  , str3 ], columns=[Write the columns])\n   d = {'id': x[0], 'Score': float(repScore),'Logistics':str3,'Finance':str1,'Quality':str2}\n   df = pd.DataFrame(data=d, index=np.arange(1))\n   return df,`\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1441602696040,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1441604124888,
        "Question_score":2,
        "Question_view_count":1013.0,
        "Poster_id":5306348.0,
        "Poster_created_time":1441555651888,
        "Poster_last_access_time":1553172657223,
        "Poster_age":null,
        "Poster_location":"Bangalore, Karnataka, India",
        "Poster_reputation":143.0,
        "Poster_views":45.0,
        "Answer_body":"<p>@Anuj Shankar,\nAfter my colleague tested, we can read data from <code>CSV<\/code> files and get the expected results. Please refer to this experience:<\/p>\n\n<p>1)  Input data  - It has <code>apple.zip<\/code> file which has two <code>csv<\/code> files similar to you and each csv file includes bag of words related to company.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/yrXst.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/yrXst.jpg\" alt=\"enter image description here\"><\/a>\n2)  Python script: <\/p>\n\n<pre><code># The script MUST contain a function named azureml_main\n# which is the entry point for this module.\n#\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\nimport csv\nimport numpy as np\nimport pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    # Execution logic goes here\n    #print('Input pandas.DataFrame #1:\\r\\n\\r\\n{0}'.format(dataframe1))\n\n    # If a zip file is connected to the third input port is connected,\n    # it is unzipped under \".\\Script Bundle\". This directory is added\n    # to sys.path. Therefore, if your zip file contains a Python file\n    # mymodule.py you can import it using:\n    # import mymodule\n\n    apple = {}\n    microsoft = {}\n  #Reading from the csv files\n    with open('.\\Script Bundle\\\\apple.csv', 'rb') as f:\n      reader = csv.reader(f)\n      apple = list_to_dict(list(reader)[0])\n\n    with open('.\\Script Bundle\\\\microsoft.csv', 'rb') as f:\n      reader = csv.reader(f)\n      microsoft = list_to_dict(list(reader)[0])\n\n#    print('hello world' + ' '.join(apple[0]))\n    applecount = 0\n    microsoftcount = 0\n\n    input = \"i want to buy surface which runs on windows\"\n    splitted_input = input.split(' ')\n\n    for word in splitted_input:\n        if word in apple:\n            applecount = applecount + 1\n        if word in microsoft:\n            microsoftcount = microsoftcount + 1\n\n    print(\"apple bag of words count - \" + str(applecount))\n    print(\"microsoft bag of words count - \" + str(microsoftcount))\n    mydata = [{'input words': len(splitted_input)}, {'applecount':applecount},\n        {'microsoftcount':microsoftcount}]       \n    # Return value must be of a sequence of pandas.DataFrame\n    return pd.DataFrame(mydata),\n\n\ndef list_to_dict(li):      \n    dct = {}  \n    for item in li:\n        if dct.has_key(item):              \n            dct[item] = dct[item] + 1  \n        else:  \n            dct[item] = 1  \n    return dct  \n<\/code><\/pre>\n\n<p>3)  Output  - if I consider a string \"i want to buy surface which runs on windows\". It has 2 words related to microsoft and 0 related to apple which are visualized in below snapshot.\n<a href=\"https:\/\/i.stack.imgur.com\/ifE1t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ifE1t.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1471950010496,
        "Answer_score":0.0,
        "Answerer_id":4836342.0,
        "Answerer_created_time":1430113774790,
        "Answerer_last_access_time":1589975964448,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1181.0,
        "Answerer_views":225.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32431471",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1443582461283,
        "Question_original_content":"get expect output background work project aim classifi product review posit neg sentiment analysi got stuck classifi review differ depart basic read word csv file check review list sentenc contain word word review note sentenc number push respect list financelist qualitylist logisticslist end convert list string push datafram output get log print statement written script valu datafram turn run code local expect output descript imag column datafram show valu descript second imag highlight expect output got local review imag imag thing check csv file read properli review contain word search unabl understand go wrong import csv import math import panda import numpi def main data financedept logisticsdept qualitydept read csv file open script bundl qualiti csv fin reader csv reader fin qualitydept list reader open script bundl financ csv reader csv reader financedept list reader open script bundl logist csv reader csv reader logisticdept list reader financelist logisticslist qualitylist initi list financelist append logisticslist append qualitylist append rev data data rev split print financedept sentenc sentenc split lower financedept financelist financelist append logisticsdept logisticslist logisticslist append qualitydept qualitylist qualitylist append comput user reput score upvot int upvot tolist downvot int downvot tolist totalvot max upvot downvot data score print financelist repscor float upvot totalvot repscor repscor float str join str financelist str join str qualitylist str join str logisticslist datafram str repscor str str str column write column score float repscor logist str financ str qualiti str datafram data index arang return",
        "Question_preprocessed_content":"get expect output background work project aim classifi product review posit neg sentiment analysi got stuck classifi review differ depart basic read word csv file check review contain word word review note sentenc number push respect list end convert list string push datafram output get log print statement written script valu datafram turn run code local expect output descript imag column datafram show valu descript second imag highlight expect output got local review imag imag thing check csv file read properli review contain word search unabl understand go wrong",
        "Question_gpt_summary_original":"The user is facing challenges in getting the expected output in AzureML while classifying product reviews into positive and negative using Sentiment Analysis. The user is unable to log the output for the print statements in the script, and the values in the dataframe are always turning out to be 0. The user has checked that the csv files are read properly and the review contains the words that are being searched. However, the user is unable to understand where they are going wrong.",
        "Question_gpt_summary":"user face challeng get expect output classifi product review posit neg sentiment analysi user unabl log output print statement script valu datafram turn user check csv file read properli review contain word search user unabl understand go wrong",
        "Answer_original_content":"anuj shankar colleagu test read data csv file expect result refer experi input data appl zip file csv file similar csv file includ bag word relat compani python script script contain function name main entri point modul entri point function contain input argument param panda datafram param panda datafram import csv import numpi import panda def main datafram datafram execut logic goe print input panda datafram format datafram zip file connect input port connect unzip script bundl directori ad sy path zip file contain python file mymodul import import mymodul appl microsoft read csv file open script bundl appl csv reader csv reader appl list dict list reader open script bundl microsoft csv reader csv reader microsoft list dict list reader print hello world join appl applecount microsoftcount input want bui surfac run window split input input split word split input word appl applecount applecount word microsoft microsoftcount microsoftcount print appl bag word count str applecount print microsoft bag word count str microsoftcount mydata input word len split input applecount applecount microsoftcount microsoftcount return valu sequenc panda datafram return datafram mydata def list dict dct item dct kei item dct item dct item dct item return dct output consid string want bui surfac run window word relat microsoft relat appl visual snapshot",
        "Answer_preprocessed_content":"shankar colleagu test read data file expect result refer experi input data file file similar csv file includ bag word relat compani python script output consid string want bui surfac run window word relat microsoft relat appl visual snapshot",
        "Answer_gpt_summary_original":"Solution:\n- The user can refer to the provided Python script and modify it according to their requirements.\n- The provided Python script reads data from CSV files and performs sentiment analysis on the input text.\n- The output of the sentiment analysis is logged using print statements in the script.\n- The script uses a bag of words approach to classify the input text as positive or negative.",
        "Answer_gpt_summary":"solut user refer provid python script modifi accord requir provid python script read data csv file perform sentiment analysi input text output sentiment analysi log print statement script script us bag word approach classifi input text posit neg"
    },
    {
        "Question_title":"Can I use Azure interactive mode for azure-cli-ml extension?",
        "Question_body":"<p>I'm using Azure CLI interactive mode <code>az interactive<\/code> to run below command. <br \/>\n<code>az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code><br \/><br \/>\nIt prompts me with below error message.<br \/>\n<code>az: error: unrecognized arguments: -w yhd-mlws -g yhd-mlws-rg<\/code><br \/><br \/>\nBTW, both my Machine Learning workspace <code>yhd-mlws<\/code> and resource group <code>yhd-mlws-rg<\/code> had been created in my Azure subscription. Azure CLI extension for machine learning service had also been installed via <code>az extension add -n azure-cli-ml<\/code>.<br \/><br \/>\nThen I run command <code>az ml folder attach<\/code> without any argument. I get bellow error message.<br \/><\/p>\n\n<pre><code>Message: Error, default workspace not set and workspace name parameter not provided.\nPlease set a default workspace using \"az ml folder attach -w myworkspace -g myresourcegroup\" or provide a value for the workspace name parameter.\n<\/code><\/pre>\n\n<p>The command window exit the interactive mode after above error message. Then I try the command <code>az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code> again, bingo! It works. <br \/>\nHere comes my question, does azure-cli-ml extension support Azure CLI interactive mode? You know, Azure CLI interactive mode is amazing and I want to use it whenever possible. Thanks!<br \/><br \/>\nBTW, I'm running windows command window in Windows Server 2016 Datcenter. Azure-cli version is 2.0.79.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1582642024300,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1582678582156,
        "Question_score":1,
        "Question_view_count":408.0,
        "Poster_id":12943018.0,
        "Poster_created_time":1582361231692,
        "Poster_last_access_time":1655516080367,
        "Poster_age":null,
        "Poster_location":"Guangzhou, China",
        "Poster_reputation":393.0,
        "Poster_views":58.0,
        "Answer_body":"<p>I can reproduce your issue, the interactive mode should support the <code>azure-cli-ml<\/code> extension, because when I run <code>az ml workspace list<\/code>, it works, once I pass the <code>-g<\/code> parameter, it gives the same error, maybe it is a bug, but I am not sure, the <code>interactive<\/code> is in preview currently.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4FvM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/g4FvM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you want to run <code>az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code> in the interactive mode, my workaround is to pass the <code>#<\/code>, i.e. <code># az ml folder attach -w yhd-mlws -g yhd-mlws-rg<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/pWMyH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/pWMyH.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":9455659.0,
        "Answerer_created_time":1520413126203,
        "Answerer_last_access_time":1663858820390,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":37123.0,
        "Answerer_views":4058.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60397252",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1582686710412,
        "Question_original_content":"us azur interact mode azur cli extens azur cli interact mode interact run command folder attach yhd mlw yhd mlw prompt error messag error unrecogn argument yhd mlw yhd mlw btw machin learn workspac yhd mlw resourc group yhd mlw creat azur subscript azur cli extens machin learn servic instal extens add azur cli run command folder attach argument bellow error messag messag error default workspac set workspac paramet provid set default workspac folder attach myworkspac myresourcegroup provid valu workspac paramet command window exit interact mode error messag try command folder attach yhd mlw yhd mlw bingo work come question azur cli extens support azur cli interact mode know azur cli interact mode amaz want us possibl thank btw run window command window window server datcent azur cli version",
        "Question_preprocessed_content":"us azur interact mode extens azur cli interact mode run command prompt error messag btw machin learn workspac resourc group creat azur subscript azur cli extens machin learn servic instal run command argument bellow error messag command window exit interact mode error messag try command bingo work come question extens support azur cli interact mode know azur cli interact mode amaz want us possibl thank btw run window command window window server datcent version",
        "Question_gpt_summary_original":"The user encountered challenges while using Azure CLI interactive mode to run the command 'az ml folder attach'. The prompt displayed an error message stating that the arguments were unrecognized. The user had installed the Azure CLI extension for machine learning service and created a machine learning workspace and resource group in their Azure subscription. When the user ran the command without any argument, it prompted an error message stating that the default workspace was not set. However, when the user ran the command with the workspace and resource group arguments, it worked. The user's question was whether the azure-cli-ml extension supports Azure CLI interactive mode.",
        "Question_gpt_summary":"user encount challeng azur cli interact mode run command folder attach prompt displai error messag state argument unrecogn user instal azur cli extens machin learn servic creat machin learn workspac resourc group azur subscript user ran command argument prompt error messag state default workspac set user ran command workspac resourc group argument work user question azur cli extens support azur cli interact mode",
        "Answer_original_content":"reproduc issu interact mode support azur cli extens run workspac list work pass paramet give error mayb bug sure interact preview current want run folder attach yhd mlw yhd mlw interact mode workaround pass folder attach yhd mlw yhd mlw",
        "Answer_preprocessed_content":"reproduc issu interact mode support extens run work pass paramet give error mayb bug sure preview current want run interact mode workaround pass",
        "Answer_gpt_summary_original":"Solution:\n- A workaround suggested by a user is to pass the '#' before the command in the interactive mode.",
        "Answer_gpt_summary":"solut workaround suggest user pass command interact mode"
    },
    {
        "Question_title":"Is there a cross-sectional, unified security check list for Azure?",
        "Question_body":"I have been used Azure for the first time, and I am overwelmed by the huge quantity of information about Azure.\n\nI think that the information about security on Azure is not unified.\n\nFor example, in Identity Management and access control security best practices page, sometimes there are multiple best practices per one section header.\nHowever, in Security recommendations for Blob storage page,security recommendations are documented in the form of table, one issue per one row.\n\nI wish there was a cross-sectional, unified security check list for Azure as follows.\n\nWe could select Azure services we use.\n\n\nWhen we select the services, the security check list are displayed or could be downloaded as text file.\n\n\nThe security check list are documented so that we can easily understand what we should do. (where on the Azure portal UI, which item, or how to do set the item which is related to security, etc)\n\nI have used Azure services as follows.\n\nAzure Data Factory\n\n\nAzure Data Lake Storage Gen2\n\n\nAzure Functions (App Service)\n\n\nAzure Database for MySQL\n\n\nAzure Machine Learning\n\n\nAzure Monitor (for Application Insights)\n\nEven if I take one service (for example, Azure Data Lake Storage Gen2), I think that I have to check at least two pages (here and here ).\nHowever, I'm not sure if it's covered. Do you have any good ideas?\n\nRegards.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1638435378937,
        "Question_favorite_count":21.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi @makotooda-1289,\n\nThanks for using Microsoft Q&A!!\n\nI do not think that we have a single document which can provide you a consolidated view of security across all Azure services. You may need to go through the documentation available for individual services to get the required information. However, you can try checking - Azure security documentation and Security considerations for Azure Architecture center if this helps you getting anything specific you are looking in Azure at higher level.\n\nHope this helps.\n\nThanks\nSaurabh",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/648921\/is-there-a-cross-sectional-unified-security-check.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1638553979008,
        "Question_original_content":"cross section unifi secur check list azur azur time overwelm huge quantiti inform azur think inform secur azur unifi exampl ident manag access control secur best practic page multipl best practic section header secur recommend blob storag page secur recommend document form tabl issu row wish cross section unifi secur check list azur follow select azur servic us select servic secur check list displai download text file secur check list document easili understand azur portal item set item relat secur azur servic follow azur data factori azur data lake storag gen azur function app servic azur databas mysql azur monitor applic insight servic exampl azur data lake storag gen think check page sure cover good idea regard",
        "Question_preprocessed_content":"unifi secur check list azur azur time overwelm huge quantiti inform azur think inform secur azur unifi exampl ident manag access control secur best practic page multipl best practic section header secur recommend blob storag page secur recommend document form tabl issu row wish unifi secur check list azur follow select azur servic us select servic secur check list displai download text file secur check list document easili understand azur servic follow azur data factori azur data lake storag gen azur function azur databas mysql azur monitor servic think check page sure cover good idea regard",
        "Question_gpt_summary_original":"The user is overwhelmed by the amount of information about security on Azure and finds it difficult to navigate due to the lack of a unified security checklist. The user wishes for a cross-sectional, unified security checklist that is easy to understand and can be downloaded as a text file. The user has used various Azure services and finds it challenging to check for security recommendations as they are documented differently on different pages.",
        "Question_gpt_summary":"user overwhelm inform secur azur find difficult navig lack unifi secur checklist user wish cross section unifi secur checklist easi understand download text file user azur servic find challeng check secur recommend document differ differ page",
        "Answer_original_content":"makotooda thank microsoft think singl document provid consolid view secur azur servic need document avail individu servic requir inform try check azur secur document secur consider azur architectur center help get specif look azur higher level hope help thank saurabh",
        "Answer_preprocessed_content":"thank microsoft think singl document provid consolid view secur azur servic need document avail individu servic requir inform try check azur secur document secur consider azur architectur center help get specif look azur higher level hope help thank saurabh",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"[Feature Request] W&B badge or shield for GitHub repositories",
        "Question_body":"<p>I was wondering if it would be possible to have a simple W&amp;B\/wandb badge to display on GitHub repositories, meaning: \u201cThis repository supports experiment tracking with wandb\u201d.<\/p>\n<p>By badge, I mean like below. The official wandb client repository for example uses pypi, codecov and circleci badges.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188.png\" data-download-href=\"\/uploads\/short-url\/64tMj9Dw36m9P2OBKPlPRcyIuBq.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188_2_690x151.png\" alt=\"image\" data-base62-sha1=\"64tMj9Dw36m9P2OBKPlPRcyIuBq\" width=\"690\" height=\"151\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188_2_690x151.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/2a8eb240dfb428a627280e4311324e7c0ec92188_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">775\u00d7170 36 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1648914623994,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4,
        "Question_view_count":219.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dealer56\">@dealer56<\/a>,<\/p>\n<p>I discussed this with some folks, and looks like we already have a <a href=\"https:\/\/img.shields.io\/badge\/Weights_&amp;_Biases-FFCC33?style=for-the-badge&amp;logo=WeightsAndBiases&amp;logoColor=black\" rel=\"noopener nofollow ugc\">badge<\/a> for something like this. You should also be able to generate such badges through <a href=\"http:\/\/shields.io\" rel=\"noopener nofollow ugc\">shields.io<\/a>, and we plan to have a tutorial in the future on how to use badges to present a metric on your repo.<\/p>\n<p>I\u2019ll link the tutorial once it is out.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/feature-request-w-b-badge-or-shield-for-github-repositories\/2181",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1650663975454,
        "Question_original_content":"featur request badg shield github repositori wonder possibl simpl badg displai github repositori mean repositori support experi track badg mean like offici client repositori exampl us pypi codecov circleci badg imag",
        "Question_preprocessed_content":"badg shield github repositori wonder possibl simpl badg displai github repositori mean repositori support experi track badg mean like offici client repositori exampl us pypi codecov circleci badg imag",
        "Question_gpt_summary_original":"The user is requesting a feature to have a W&B badge or shield for GitHub repositories to indicate that the repository supports experiment tracking with wandb. The user provides an example of how the badge would look like and mentions that other repositories use badges for pypi, codecov, and circleci.",
        "Question_gpt_summary":"user request featur badg shield github repositori indic repositori support experi track user provid exampl badg look like mention repositori us badg pypi codecov circleci",
        "Answer_original_content":"dealer discuss folk look like badg like abl gener badg shield plan tutori futur us badg present metric repo ill link tutori thank ramit",
        "Answer_preprocessed_content":"discuss folk look like badg like abl gener badg plan tutori futur us badg present metric repo ill link tutori thank ramit",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that there is already a badge available for W&B and that the user can generate such badges through shields.io. The team also plans to release a tutorial in the future on how to use badges to present a metric on the repository.",
        "Answer_gpt_summary":"solut discuss suggest badg avail user gener badg shield team plan releas tutori futur us badg present metric repositori"
    },
    {
        "Question_title":"ModuleNotFoundError: No module named 'azureml'",
        "Question_body":"Hi,\n\nI am doing the Challenge. https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/intro-to-azure-machine-learning-service\/\n\n\n\n\nPlease see what I have installed:\n\npip install azureml-sdk\n\nI am getting the following messages at the end:\n\n\n\n\nERROR: After October 2020 you may experience errors when installing or updating packages. This is because pip will change the way that it resolves dependency conflicts.\n\n\nWe recommend you use --use-feature=2020-resolver to test your packages with the new resolver before it becomes the default.\n\n\njupyterlab 2.2.9 requires jupyterlab-server<2.0,>=1.1.5, which is not installed.\nSuccessfully installed applicationinsights-0.11.9 azure-identity-1.4.1 azureml-automl-core-1.19.0 azureml-dataprep-2.6.3 azureml-dataprep-native-26.0.0 azureml-dataprep-rslex-1.4.0 azureml-dataset-runtime-1.19.0.post1 azureml-pipeline-1.19.0 azureml-pipeline-core-1.19.0 azureml-pipeline-steps-1.19.0 azureml-sdk-1.19.0 azureml-telemetry-1.19.0 azureml-train-1.19.0 azureml-train-automl-client-1.19.0 azureml-train-core-1.19.0 azureml-train-restclients-hyperdrive-1.19.0 distro-1.5.0 dotnetcore2-2.1.20 fusepy-3.0.1 msal-1.8.0 msal-extensions-0.2.2 numpy-1.19.3 portalocker-1.7.1 pyarrow-1.0.1 pywin32-227\n\n\n\n\nNow I am trying to start up and type the following in .py file in Visual Studio Code\n\nfrom azureml.core import Workspace\n\nThis is the error message I am getting:\n\nFile \"c:\/Users\/User\/OneDrive\/Desktop\/New folder\/Build AI Solution\/automl_python.py\", line 1, in <module>\nfrom azureml.core import Workspace\nModuleNotFoundError: No module named 'azureml'\n\n\n\n\nPlease could you help me?\n\nthanks,\n\nNaveen",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1609112074090,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"This is now solved. Thanks!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/211503\/modulenotfounderror-no-module-named-39azureml39.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1610753174427,
        "Question_original_content":"modulenotfounderror modul name challeng http doc microsoft com learn modul intro azur machin learn servic instal pip instal sdk get follow messag end error octob experi error instal updat packag pip chang wai resolv depend conflict recommend us us featur resolv test packag new resolv default jupyterlab requir jupyterlab server instal successfulli instal applicationinsight azur ident automl core dataprep dataprep nativ dataprep rslex dataset runtim post pipelin pipelin core pipelin step sdk telemetri train train automl client train core train restclient hyperdr distro dotnetcor fusepi msal msal extens numpi portalock pyarrow pywin try start type follow file visual studio code core import workspac error messag get file user user onedr desktop new folder build solut automl python line core import workspac modulenotfounderror modul name help thank naveen",
        "Question_preprocessed_content":"modulenotfounderror modul name challeng instal pip instal sdk get follow messag end error octob experi error instal updat packag pip chang wai resolv depend conflict recommend us test packag new resolv default jupyterlab requir instal successfulli instal try start type follow file visual studio code core import workspac error messag get file line core import workspac modulenotfounderror modul name help thank naveen",
        "Question_gpt_summary_original":"The user is encountering a challenge with the installation of the Azure Machine Learning SDK. They have installed the SDK using pip, but are receiving an error message when trying to import the 'azureml' module in their Python code. The error message indicates that the module is not found, suggesting that the installation may not have been successful.",
        "Question_gpt_summary":"user encount challeng instal sdk instal sdk pip receiv error messag try import modul python code error messag indic modul suggest instal success",
        "Answer_original_content":"solv thank",
        "Answer_preprocessed_content":"solv thank",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"SageMaker deploy custom script",
        "Question_body":"<p>I'm pretty new to SageMaker, so I'm sorry if I miss something obvious.<\/p>\n\n<p>I've trained a DL model which uses frames from a video to make a prediction. The current script, that runs in the SageMaker jupyter-notebook, takes a video URL as an input and uses an FFMPEG subprocess pipe to extract the frames and predict them afterwards. This works fine, but now I want to start that script from Lambda.<\/p>\n\n<p>As far as I understood, I could deploy my model with sagemaker and make predictions for every single frame from Lambda, unfortunately this is not an option, as ffprobe, ffmpeg and numpy are too large to fit into the limited lambda space.<\/p>\n\n<p>tl;dr: Is it possible to run my custom script (ffmpeg frame extraction + tensorflow model prediction) as an endpoint in SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1580859395073,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":386.0,
        "Poster_id":6191842.0,
        "Poster_created_time":1460444230316,
        "Poster_last_access_time":1663978921487,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":404.0,
        "Poster_views":127.0,
        "Answer_body":"<p>Sagemaker allows you to use a custom Docker image (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms.html\" rel=\"nofollow noreferrer\">AWS document<\/a>)<\/p>\n\n<blockquote>\n  <p>Build your own custom container image: If there is no pre-built Amazon\n  SageMaker container image that you can use or modify for an advanced\n  scenario, you can package your own script or algorithm to use with\n  Amazon SageMaker.You can use any programming language or framework to\n  develop your container<\/p>\n<\/blockquote>\n\n<ul>\n<li>Create a docker image with your code (FFmpeg, TensorFlow)<\/li>\n<li>Testing the docker container locally<\/li>\n<li>Deploying the image on Amazon ECR (Elastic Container Repository)<\/li>\n<li>Create a SageMaker model and point to the image<\/li>\n<\/ul>\n\n<p>For details, you can learn more from <a href=\"https:\/\/towardsdatascience.com\/brewing-up-custom-ml-models-on-aws-sagemaker-e09b64627722\" rel=\"nofollow noreferrer\">this tutorial<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":9618462.0,
        "Answerer_created_time":1523264005616,
        "Answerer_last_access_time":1664083645288,
        "Answerer_age":null,
        "Answerer_location":"Yokohama, Kanagawa, Japan",
        "Answerer_reputation":1635.0,
        "Answerer_views":143.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60067075",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1580863000328,
        "Question_original_content":"deploi custom script pretti new sorri miss obviou train model us frame video predict current script run jupyt notebook take video url input us ffmpeg subprocess pipe extract frame predict work fine want start script lambda far understood deploi model predict singl frame lambda unfortun option ffprobe ffmpeg numpi larg fit limit lambda space possibl run custom script ffmpeg frame extract tensorflow model predict endpoint",
        "Question_preprocessed_content":"deploi custom script pretti new sorri miss obviou train model us frame video predict current script run take video url input us ffmpeg subprocess pipe extract frame predict work fine want start script lambda far understood deploi model predict singl frame lambda unfortun option ffprobe ffmpeg numpi larg fit limit lambda space possibl run custom script endpoint",
        "Question_gpt_summary_original":"The user has encountered a challenge in deploying a custom script for a DL model in SageMaker. They want to run the script from Lambda, but the required tools such as ffprobe, ffmpeg, and numpy are too large to fit into the limited Lambda space. The user is seeking a solution to run their custom script as an endpoint in SageMaker.",
        "Question_gpt_summary":"user encount challeng deploi custom script model want run script lambda requir tool ffprobe ffmpeg numpi larg fit limit lambda space user seek solut run custom script endpoint",
        "Answer_original_content":"allow us custom docker imag aw document build custom contain imag pre built amazon contain imag us modifi advanc scenario packag script algorithm us us program languag framework develop contain creat docker imag code ffmpeg tensorflow test docker contain local deploi imag amazon ecr elast contain repositori creat model point imag detail learn tutori",
        "Answer_preprocessed_content":"allow us custom docker imag build custom contain imag amazon contain imag us modifi advanc scenario packag script algorithm us us program languag framework develop contain creat docker imag code test docker contain local deploi imag amazon ecr creat model point imag detail learn tutori",
        "Answer_gpt_summary_original":"Solution: The user can create a custom Docker image with their code and required tools such as FFmpeg, TensorFlow, and numpy. They can test the Docker container locally and deploy the image on Amazon ECR (Elastic Container Repository). Then, they can create a SageMaker model and point to the image. A tutorial is provided for more details.",
        "Answer_gpt_summary":"solut user creat custom docker imag code requir tool ffmpeg tensorflow numpi test docker contain local deploi imag amazon ecr elast contain repositori creat model point imag tutori provid detail"
    },
    {
        "Question_title":"Datadrift in Azure ML SDK v2",
        "Question_body":"I can't see a data drift module anywhere in v2 of the Azure ML Python SDK. Is this missing or what's the deal? If so, are there any plans of bringing it into v2?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1657283475557,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hello @SH-3152\n\nI have a good news for you, I just got confirmation from product team, the datadrift function will be in SDK V2 for sure. But for now we don't have an exact date for when. I have forwarded this feedback to product group and we hope we can bring this feature in near future.\n\n\n\n\nI hope this helps.\n\n\n\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/919651\/datadrift-in-azure-ml-sdk-v2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1658311324112,
        "Question_original_content":"datadrift sdk data drift modul python sdk miss deal plan bring",
        "Question_preprocessed_content":"datadrift sdk data drift modul python sdk miss deal plan bring",
        "Question_gpt_summary_original":"The user is facing a challenge in finding the data drift module in version 2 of the Azure ML Python SDK and is unsure if it is missing or not included. They are also inquiring about any plans to bring it into version 2.",
        "Question_gpt_summary":"user face challeng find data drift modul version python sdk unsur miss includ inquir plan bring version",
        "Answer_original_content":"hello good new got confirm product team datadrift function sdk sure exact date forward feedback product group hope bring featur near futur hope help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_preprocessed_content":"hello good new got confirm product team datadrift function sdk sure exact date forward feedback product group hope bring featur near futur hope help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_gpt_summary_original":"Solution: The discussion mentions that the data drift module will be included in version 2 of the Azure ML Python SDK, but there is no exact date for when it will be available.",
        "Answer_gpt_summary":"solut discuss mention data drift modul includ version python sdk exact date avail"
    },
    {
        "Question_title":"Download file using boto3 within Docker container deployed on Sagemaker Endpoint",
        "Question_body":"<p>I have built my own Docker container that provides inference code to be deployed as endpoint on Amazon Sagemaker. However, this container needs to have access to some files from s3. The used IAM role has access to all s3 buckets that I am trying to reach.<\/p>\n\n<p>Code to download files using a boto3 client:<\/p>\n\n<pre><code>import boto3\n\nmodel_bucket = 'my-bucket'\n\ndef download_file_from_s3(s3_path, local_path):\n    client = boto3.client('s3')\n    client.download_file(model_bucket, s3_path, local_path)\n<\/code><\/pre>\n\n<p>The IAM role's policies:<\/p>\n\n<pre><code>{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Action\": [\n                \"s3:GetObject\",\n                \"s3:PutObject\",\n                \"s3:DeleteObject\"\n            ],\n            \"Effect\": \"Allow\",\n            \"Resource\": [\n                \"arn:aws:s3:::my-bucket\/*\"\n            ]\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Starting the docker container locally allows me to download files from s3 just like expected. <\/p>\n\n<p>Deploying as an endpoint on Sagemaker, however, the request times out:<\/p>\n\n<pre><code>botocore.vendored.requests.exceptions.ConnectTimeout: HTTPSConnectionPool(host='my-bucket.s3.eu-central-1.amazonaws.com', port=443): Max retries exceeded with url: \/path\/to\/my-file (Caused by ConnectTimeoutError(&lt;botocore.awsrequest.AWSHTTPSConnection object at 0x7f66244e69b0&gt;, 'Connection to my-bucket.s3.eu-central-1.amazonaws.com timed out. (connect timeout=60)'))\n<\/code><\/pre>\n\n<p>Any help is appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1561982365937,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":4,
        "Question_view_count":1171.0,
        "Poster_id":5981083.0,
        "Poster_created_time":1456411465888,
        "Poster_last_access_time":1663933136732,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":105.0,
        "Poster_views":25.0,
        "Answer_body":"<p>For anyone coming across this question, when creating a model, the 'Enable Network Isolation' property defaults to True.\nFrom AWS docs:<\/p>\n<blockquote>\n<p>If you enable network isolation, the containers are not able to make any outbound network calls, even to other AWS services such as Amazon S3. Additionally, no AWS credentials are made available to the container runtime environment.<\/p>\n<\/blockquote>\n<p>So this property needs to be set to False in order to connect to any other AWS service.<\/p>\n<p><img src=\"https:\/\/i.stack.imgur.com\/5qERm.jpg\" alt=\"AWS Sagemaker UI Network Isolation set to False\" \/><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1641299334400,
        "Answer_score":0.0,
        "Answerer_id":5981083.0,
        "Answerer_created_time":1456411465888,
        "Answerer_last_access_time":1663933136732,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":105.0,
        "Answerer_views":25.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56835306",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1562142120396,
        "Question_original_content":"download file boto docker contain deploi endpoint built docker contain provid infer code deploi endpoint contain need access file iam role access bucket try reach code download file boto client import boto model bucket bucket def download file path local path client boto client client download file model bucket path local path iam role polici version statement action getobject putobject deleteobject effect allow resourc arn aw bucket start docker contain local allow download file like expect deploi endpoint request time botocor vendor request except connecttimeout httpsconnectionpool host bucket central amazonaw com port max retri exceed url path file caus connecttimeouterror connect bucket central amazonaw com time connect timeout help appreci",
        "Question_preprocessed_content":"download file boto docker contain deploi endpoint built docker contain provid infer code deploi endpoint contain need access file iam role access bucket try reach code download file boto client iam role polici start docker contain local allow download file like expect deploi endpoint request time help appreci",
        "Question_gpt_summary_original":"The user has built a Docker container for inference code to be deployed as an endpoint on Amazon Sagemaker, which needs access to files from S3. The IAM role used has access to all S3 buckets, but when the container is deployed as an endpoint on Sagemaker, the request times out while trying to download files from S3. The error message suggests a connection timeout issue.",
        "Question_gpt_summary":"user built docker contain infer code deploi endpoint need access file iam role access bucket contain deploi endpoint request time try download file error messag suggest connect timeout issu",
        "Answer_original_content":"come question creat model enabl network isol properti default true aw doc enabl network isol contain abl outbound network call aw servic amazon addition aw credenti avail contain runtim environ properti need set fals order connect aw servic",
        "Answer_preprocessed_content":"come question creat model enabl network isol properti default true aw doc enabl network isol contain abl outbound network call aw servic amazon addition aw credenti avail contain runtim environ properti need set fals order connect aw servic",
        "Answer_gpt_summary_original":"Solution: The 'Enable Network Isolation' property needs to be set to False in order to connect to any other AWS service.",
        "Answer_gpt_summary":"solut enabl network isol properti need set fals order connect aw servic"
    },
    {
        "Question_title":"Running ipynb file, but can't import a certain module",
        "Question_body":"<p>l'am running a ipynb file on sagemaker, however the error of occurs.\nl have used 'pip install tqdm' in terminals to install the tqdm so l've no idea what's happening. Is it running in a different environment?\nThanks for any answer.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/bO6zS.png\" rel=\"nofollow noreferrer\">error report from my ipynb file <\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/5tP1J.png\" rel=\"nofollow noreferrer\">what l've done in terminal<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1659151516027,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1659151556683,
        "Question_score":0,
        "Question_view_count":34.0,
        "Poster_id":19584929.0,
        "Poster_created_time":1658300099523,
        "Poster_last_access_time":1660479740076,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":3.0,
        "Poster_views":1.0,
        "Answer_body":"<p>There is a possibility you may be executing &quot;pip&quot; in a different environment.<\/p>\n<p>Try executing &quot;!pip install tqdm&quot; or &quot;!pip3 install tqdm&quot; as a code cell in the Sagemaker document itself.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":19651081.0,
        "Answerer_created_time":1659144422092,
        "Answerer_last_access_time":1663907045180,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":166.0,
        "Answerer_views":37.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73172644",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659155782776,
        "Question_original_content":"run ipynb file import certain modul run ipynb file error occur pip instal tqdm termin instal tqdm idea happen run differ environ thank answer error report ipynb file termin",
        "Question_preprocessed_content":"run ipynb file import certain modul run ipynb file error occur pip instal tqdm termin instal tqdm idea happen run differ environ thank answer error report ipynb file termin",
        "Question_gpt_summary_original":"The user is encountering challenges while running an ipynb file on Sagemaker as they are unable to import a certain module. They have tried installing the module using 'pip install tqdm' in terminals but the error still occurs. The user is unsure if the file is running in a different environment.",
        "Question_gpt_summary":"user encount challeng run ipynb file unabl import certain modul tri instal modul pip instal tqdm termin error occur user unsur file run differ environ",
        "Answer_original_content":"possibl execut pip differ environ try execut pip instal tqdm pip instal tqdm code cell document",
        "Answer_preprocessed_content":"possibl execut pip differ environ try execut pip instal tqdm pip instal tqdm code cell document",
        "Answer_gpt_summary_original":"Solution: The user can try executing \"!pip install tqdm\" or \"!pip3 install tqdm\" as a code cell in the Sagemaker document itself to install the required module.",
        "Answer_gpt_summary":"solut user try execut pip instal tqdm pip instal tqdm code cell document instal requir modul"
    },
    {
        "Question_title":"In SageMaker Studio, how to decide on which instance to open a terminal?",
        "Question_body":"Hi,\n\nI was writing some notebook on a t2.Medium Studio Notebook. Now I just switched to an m5.8xlarge. However, when I launch a terminal, it still shows up only 2 CPUs, not the 32 I expected. How to open a terminal on that m5.8xlarge instance?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1606945520000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":495.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Where do you launch the terminal from? If you use the launcher window, it would start on the t2.medium as you are experiencing.\n\nHowever, if you use the launch terminal button in the toolbar that is displayed at the top of your notebook, it will launch the image terminal on the new instance the notebook's kernel is running on (your m5.8xlarge instance).",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo5ycye8jQ7Cw8dgSAfE9RQ\/in-sage-maker-studio-how-to-decide-on-which-instance-to-open-a-terminal",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1606993756000,
        "Question_original_content":"studio decid instanc open termin write notebook medium studio notebook switch xlarg launch termin show cpu expect open termin xlarg instanc",
        "Question_preprocessed_content":"studio decid instanc open termin write notebook studio notebook switch launch termin show cpu expect open termin instanc",
        "Question_gpt_summary_original":"The user is facing a challenge in opening a terminal on an m5.8xlarge instance in SageMaker Studio, as the terminal only shows 2 CPUs instead of the expected 32.",
        "Question_gpt_summary":"user face challeng open termin xlarg instanc studio termin show cpu instead expect",
        "Answer_original_content":"launch termin us launcher window start medium experienc us launch termin button toolbar displai notebook launch imag termin new instanc notebook kernel run xlarg instanc",
        "Answer_preprocessed_content":"launch termin us launcher window start experienc us launch termin button toolbar displai notebook launch imag termin new instanc notebook kernel run",
        "Answer_gpt_summary_original":"Solution: Launch the terminal using the launch terminal button in the toolbar displayed at the top of the notebook instead of using the launcher window. This will launch the image terminal on the new instance the notebook's kernel is running on (m5.8xlarge instance).",
        "Answer_gpt_summary":"solut launch termin launch termin button toolbar displai notebook instead launcher window launch imag termin new instanc notebook kernel run xlarg instanc"
    },
    {
        "Question_title":"Unable to Create AWS Segamaker, Error: The account-level service limit 'Number of elastic inference accelerators across all notebook instances.'",
        "Question_body":"<p>I'm facing an Error Message <br> &quot;The account-level service limit 'Number of elastic inference accelerators across all notebook instances.' is 0 Accelerators, with current utilization of 0 Accelerators and a request delta of 1 Accelerators. Please contact AWS support to request an increase for this limit.&quot;\n<a href=\"https:\/\/i.stack.imgur.com\/KWvQb.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KWvQb.png\" alt=\"enter image description here\" \/><\/a>\n<br>\nI never used this account, I have 500$ in my account. But I'm unable to create the Juypter NoteBook on Aws SageMaker Instances.<\/p>\n<p>I already visited: <a href=\"https:\/\/stackoverflow.com\/questions\/53595157\/aws-sagemaker-deploy-fails\">AWS Sagemaker Deploy fails<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1649082135410,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":109.0,
        "Poster_id":12238779.0,
        "Poster_created_time":1571405791940,
        "Poster_last_access_time":1664045184740,
        "Poster_age":null,
        "Poster_location":"UK",
        "Poster_reputation":57.0,
        "Poster_views":39.0,
        "Answer_body":"<p>By default AWS limits the number of instances you can use, here you have the default <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">limits<\/a>. As the error message says, you have to request for a limit increase, you can do it from <a href=\"https:\/\/us-east-1.console.aws.amazon.com\/support\/home?region=us-east-1&amp;skipRegion=true#\/case\/create\" rel=\"nofollow noreferrer\">here<\/a>, it will take couple of days from my experience.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4267439.0,
        "Answerer_created_time":1416346350292,
        "Answerer_last_access_time":1664039219436,
        "Answerer_age":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation":2302.0,
        "Answerer_views":227.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71738894",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1649144823327,
        "Question_original_content":"unabl creat aw segamak error account level servic limit number elast infer acceler notebook instanc face error messag account level servic limit number elast infer acceler notebook instanc acceler current util acceler request delta acceler contact aw support request increas limit account account unabl creat juypter notebook instanc visit deploi fail",
        "Question_preprocessed_content":"unabl creat aw segamak error servic limit number elast infer acceler notebook face error messag servic limit number elast infer acceler notebook acceler current util acceler request delta acceler contact aw support request increas account account unabl creat juypter notebook instanc visit deploi fail",
        "Question_gpt_summary_original":"The user is unable to create an AWS SageMaker instance due to an account-level service limit error related to the number of elastic inference accelerators. The error message suggests that the user needs to contact AWS support to request an increase in the limit. The user has not used the account before and has $500 in the account. The user has already visited a related Stack Overflow post for help.",
        "Question_gpt_summary":"user unabl creat instanc account level servic limit error relat number elast infer acceler error messag suggest user need contact aw support request increas limit user account account user visit relat stack overflow post help",
        "Answer_original_content":"default aw limit number instanc us default limit error messag sai request limit increas coupl dai experi",
        "Answer_preprocessed_content":"default aw limit number instanc us default limit error messag sai request limit increas coupl dai experi",
        "Answer_gpt_summary_original":"Solution: The user needs to request an increase in the limit for elastic inference accelerators by contacting AWS support through the provided link. It may take a couple of days for the request to be processed.",
        "Answer_gpt_summary":"solut user need request increas limit elast infer acceler contact aw support provid link coupl dai request process"
    },
    {
        "Question_title":"Deploy SageMaker model to IoT Greengrass in different account?",
        "Question_body":"Is it possible to deploy a model created by SageMaker in one account to an IoT Greengrass device in a different account?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1556295446000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":38.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"For IoT Greengrass 1.x, this is possible but not trivial. From the console this is not possible, as you can only select buckets or SageMaker jobs from the same account, but you can refer to resources in other accounts if you use the CLI or the API.\n\nYou have to create a new Resource Definition Version with the correct data specifying the model resource and then add it to your group definition. For permissions in the source account, you must set up the S3 bucket policy to allow access from the destination account. For permissions in the destination account, you must update the IoT Greengrass service role policy to access the model resource in the source account.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJha7KbxOTXuhMRGbMYGC0g\/deploy-sage-maker-model-to-io-t-greengrass-in-different-account",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1556529654000,
        "Question_original_content":"deploi model iot greengrass differ account possibl deploi model creat account iot greengrass devic differ account",
        "Question_preprocessed_content":"deploi model iot greengrass differ account possibl deploi model creat account iot greengrass devic differ account",
        "Question_gpt_summary_original":"The user is facing a challenge of deploying a SageMaker model to an IoT Greengrass device in a different account. They are unsure if this is possible and seeking clarification.",
        "Question_gpt_summary":"user face challeng deploi model iot greengrass devic differ account unsur possibl seek clarif",
        "Answer_original_content":"iot greengrass possibl trivial consol possibl select bucket job account refer resourc account us cli api creat new resourc definit version correct data specifi model resourc add group definit permiss sourc account set bucket polici allow access destin account permiss destin account updat iot greengrass servic role polici access model resourc sourc account",
        "Answer_preprocessed_content":"iot greengrass possibl trivial consol possibl select bucket job account refer resourc account us cli api creat new resourc definit version correct data specifi model resourc add group definit permiss sourc account set bucket polici allow access destin account permiss destin account updat iot greengrass servic role polici access model resourc sourc account",
        "Answer_gpt_summary_original":"Solution: It is possible to deploy a SageMaker model to an IoT Greengrass device in a different account, but it is not trivial. From the console, it is not possible, but it can be done using the CLI or the API. You have to create a new Resource Definition Version with the correct data specifying the model resource and then add it to your group definition. For permissions in the source account, you must set up the S3 bucket policy to allow access from the destination account. For permissions in the destination account, you must update the IoT Greengrass service role policy to access the model resource in the source account.",
        "Answer_gpt_summary":"solut possibl deploi model iot greengrass devic differ account trivial consol possibl cli api creat new resourc definit version correct data specifi model resourc add group definit permiss sourc account set bucket polici allow access destin account permiss destin account updat iot greengrass servic role polici access model resourc sourc account"
    },
    {
        "Question_title":"How to Curl an Amazon Sagemaker Endpoint",
        "Question_body":"<p>What is a is the curl command to make a POST request to sage-maker and receive a ML inference?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_created_time":1513373969900,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":11,
        "Question_view_count":6889.0,
        "Poster_id":5377184.0,
        "Poster_created_time":1443201378360,
        "Poster_last_access_time":1653587950968,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":749.0,
        "Poster_views":49.0,
        "Answer_body":"<p>Rather than using curl, it's recommended that you use the SageMaker Runtime client to send data and get back inferences from a SageMaker Endpoint:<\/p>\n\n<p><a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Answerer_id":4941254.0,
        "Answerer_created_time":1432655047272,
        "Answerer_last_access_time":1663360066432,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":463.0,
        "Answerer_views":76.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47840209",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1514326523572,
        "Question_original_content":"curl endpoint curl command post request sage maker receiv infer",
        "Question_preprocessed_content":"curl endpoint curl command post request receiv infer",
        "Question_gpt_summary_original":"The user is facing a challenge in understanding how to use the curl command to make a POST request to Amazon Sagemaker and receive a machine learning inference.",
        "Question_gpt_summary":"user face challeng understand us curl command post request receiv machin learn infer",
        "Answer_original_content":"curl recommend us runtim client send data infer endpoint http doc aw amazon com latest api runtim invokeendpoint html",
        "Answer_preprocessed_content":"curl recommend us runtim client send data infer endpoint",
        "Answer_gpt_summary_original":"Solution: It is recommended to use the SageMaker Runtime client instead of curl to send data and receive inferences from a SageMaker Endpoint. The documentation for using the SageMaker Runtime client can be found at http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut recommend us runtim client instead curl send data receiv infer endpoint document runtim client http doc aw amazon com latest api runtim invokeendpoint html solut mention discuss"
    },
    {
        "Question_title":"Unable to upload data using sagemaker_session.upload_data in s3 bucket that I created, it is getting stored in default s3 bucket",
        "Question_body":"<p>So I am trying iris to get acquainted with was sagemaker I am following simple tutorials from <a href=\"https:\/\/towardsdatascience.com\/training-and-deploying-custom-tensorflow-models-with-aws-sagemaker-72027722ad76\" rel=\"nofollow noreferrer\">link<\/a>. I have created a bucket named &quot;tf-practise-iris-data&quot; and gave the IAM role of Sagemaker access to the s3 bucket as mentioned in the tutorial. I also tried creating a new bucket with a different name thinking there might be some problem with a bucket but still it is having the same issue, this is the snippet of my code <a href=\"https:\/\/i.stack.imgur.com\/FgjVK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FgjVK.png\" alt=\"enter image description here\" \/><\/a>. And I have turned off Block all public access from the bucket but still nothing.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1642443300117,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":242.0,
        "Poster_id":13844933.0,
        "Poster_created_time":1593579580856,
        "Poster_last_access_time":1663603162270,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":14.0,
        "Poster_views":1.0,
        "Answer_body":"<p>Solved it!!!!!!!!!!!!<\/p>\n<pre><code>prefix = &quot;checking-with-new-bucket&quot;\ntraining_input_path = sagemaker_session.upload_data('train.csv', bucket = 'checking-with-new-bucket',key_prefix = prefix + &quot;\/training&quot;)\ntraining_input_path\n<\/code><\/pre>\n<p>Which gave output as<\/p>\n<pre><code>'s3:\/\/checking-with-new-bucket\/checking-with-new-bucket\/training\/train.csv'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":13844933.0,
        "Answerer_created_time":1593579580856,
        "Answerer_last_access_time":1663603162270,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":14.0,
        "Answerer_views":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70745798",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1642446326470,
        "Question_original_content":"unabl upload data session upload data bucket creat get store default bucket try iri acquaint follow simpl tutori link creat bucket name practis iri data gave iam role access bucket mention tutori tri creat new bucket differ think problem bucket have issu snippet code turn block public access bucket",
        "Question_preprocessed_content":"unabl upload data bucket creat get store default bucket try iri acquaint follow simpl tutori link creat bucket name gave iam role access bucket mention tutori tri creat new bucket differ think problem bucket have issu snippet code turn block public access bucket",
        "Question_gpt_summary_original":"The user is facing challenges in uploading data using sagemaker_session.upload_data in an s3 bucket that they created. Despite creating a new bucket and giving Sagemaker access to the bucket, the data is still getting stored in the default s3 bucket. The user has turned off Block all public access from the bucket but the issue persists.",
        "Question_gpt_summary":"user face challeng upload data session upload data bucket creat despit creat new bucket give access bucket data get store default bucket user turn block public access bucket issu persist",
        "Answer_original_content":"solv prefix check new bucket train input path session upload data train csv bucket check new bucket kei prefix prefix train train input path gave output check new bucket check new bucket train train csv",
        "Answer_preprocessed_content":"solv gave output",
        "Answer_gpt_summary_original":"Solution: The user was able to solve the issue by specifying the bucket name in the key_prefix parameter while using the sagemaker_session.upload_data function. The code used was:\n\nprefix = \"checking-with-new-bucket\"\ntraining_input_path = sagemaker_session.upload_data('train.csv', bucket = 'checking-with-new-bucket',key_prefix = prefix + \"\/training\")\ntraining_input_path\n\nThis solution ensured that the data was uploaded to the correct bucket and the issue was resolved.",
        "Answer_gpt_summary":"solut user abl solv issu specifi bucket kei prefix paramet session upload data function code prefix check new bucket train input path session upload data train csv bucket check new bucket kei prefix prefix train train input path solut ensur data upload correct bucket issu resolv"
    },
    {
        "Question_title":"ClientError: lst should at least has three parts, but only has 1 parts for",
        "Question_body":"<p>I use SageMaker for research purpose in my study, hope somebody can help me out.\nError I get\nClientError: lst should at least has three parts, but only has 1 parts for '1 0 class_iphone6splus\/i6 (1).jpg'<\/p>\n\n<p>It is possible to create my own training job using the SageMaker GUI only?\nCause I'm totally new to AWS...\nThe built in algorithm that I wan to use is image classification. <\/p>\n\n<p>I have 400 images in JPG format for the datasets. Those images are from two different phone models which is iPhone 6s plus and iPhone7plus so that the system will classify them into two different classes. Both 200 each.<\/p>\n\n<p>S3 bucket\nIn train folder, I have two different folder to store those images which are class_iphone6splus and \nclass_iphone7plus 200 each for one class. The .lst file which are created by own use notepad++ name as data.lst are put in these two folder with the images cause I not sure where to put it.\nWhile in validation folder, I also store the same 400 images into another class_iphone6splus and \nclass_iphone7plus folder seperately by their class.<\/p>\n\n<p>Things store in .lst file<br>\nExample altogether is 400lines<br>\n1 0 class_iphone6splus\/i6 (1).jpg<br>\nuntil<br>\n200 0 class_iphone6splus\/i6 (200).jpg  <\/p>\n\n<p>201 1 class_iphone7plus\/i7 (1).jpg<br>\nuntil<br>\n400 1 class_iphone7plus\/i7 (200).jpg  <\/p>\n\n<p>Should I create two different folder in the bucket to store the .lst file which are train_lst folder &amp; validation_lst folder. These two folders also should contains 400 images?  <\/p>\n\n<p>Resource configuration:<br>\nInstance type: ml.p2.xlarge<br>\nInstance count: 1<br>\nAdditional storage: 5GB  <\/p>\n\n<p>Hyperparameters:<br>\nnum_classes:2<br>\nnum_training_samples:400<br>\nothers parameters used default value by system.  <\/p>\n\n<p>Input data configuration:<br>\nI set 4 channels which are:  <\/p>\n\n<p>1) train\nS3 location: s3:\/\/datasets-for-testing\/train<\/p>\n\n<p>2) validation\nS3 location: s3:\/\/datasets-for-testing\/validation<\/p>\n\n<p>3) train_lst\nS3 location: s3:\/\/datasets-for-testing\/train<\/p>\n\n<p>4) validation_lst\nS3 location: s3:\/\/datasets-for-testing\/validation<\/p>\n\n<p>Input mode:file\nContent type: application\/jpeg or use application\/x-image will be better<\/p>\n\n<p>S3 output path\ns3:\/\/datasets-for-testing\/output<\/p>\n\n<p>These are all the configurations I choose before click on the 'Create training job'.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1564115966253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1564126209647,
        "Question_score":1,
        "Question_view_count":239.0,
        "Poster_id":11839519.0,
        "Poster_created_time":1564115013776,
        "Poster_last_access_time":1566463993567,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":33.0,
        "Poster_views":4.0,
        "Answer_body":"<p>I create a training job that you have specified and had got the same error. To resolve the error <strong>ClientError: lst should at least has three parts, but only has 1 parts for<\/strong>, make sure that the file <strong>.lst<\/strong> is well-formatted with tab-separated like this:<\/p>\n\n<pre><code>5      1   iphone\/iphone7_1.jpg\n1000   0   iphone\/iphone6_1.jpg\n22     1   iphone\/iphone7_2.jpg\n<\/code><\/pre>\n\n<p>I used  <code>nano<\/code> on <strong>MAC OS X<\/strong> to validate the tab-separated format.<\/p>",
        "Answer_comment_count":9.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":6393053.0,
        "Answerer_created_time":1464391892936,
        "Answerer_last_access_time":1658153265652,
        "Answerer_age":null,
        "Answerer_location":"Rio de Janeiro, State of Rio de Janeiro, Brazil",
        "Answerer_reputation":2243.0,
        "Answerer_views":148.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57213293",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1564165675663,
        "Question_original_content":"clienterror lst part part us research purpos studi hope somebodi help error clienterror lst part part class iphonesplu jpg possibl creat train job gui caus total new aw built algorithm wan us imag classif imag jpg format dataset imag differ phone model iphon plu iphoneplu classifi differ class bucket train folder differ folder store imag class iphonesplu class iphoneplu class lst file creat us notepad data lst folder imag caus sure valid folder store imag class iphonesplu class iphoneplu folder seper class thing store lst file exampl altogeth line class iphonesplu jpg class iphonesplu jpg class iphoneplu jpg class iphoneplu jpg creat differ folder bucket store lst file train lst folder valid lst folder folder contain imag resourc configur instanc type xlarg instanc count addit storag hyperparamet num class num train sampl paramet default valu input data configur set channel train locat dataset test train valid locat dataset test valid train lst locat dataset test train valid lst locat dataset test valid input mode file content type applic jpeg us applic imag better output path dataset test output configur choos click creat train job",
        "Question_preprocessed_content":"clienterror lst part part us research purpos studi hope somebodi help error clienterror lst part part jpg possibl creat train job gui caus total new built algorithm wan us imag classif imag jpg format dataset imag differ phone model iphon plu iphon plu classifi differ class bucket train folder differ folder store imag class lst file creat us notepad folder imag caus sure valid folder store imag folder seper class thing store lst file exampl altogeth line jpg jpg jpg jpg creat differ folder bucket store lst file folder folder folder contain imag resourc configur instanc type instanc count addit storag hyperparamet paramet default valu input data configur set channel train locat valid locat locat locat input mode file content type us better output path configur choos click creat train job",
        "Question_gpt_summary_original":"The user is encountering an error message \"ClientError: lst should at least has three parts, but only has 1 parts for '1 0 class_iphone6splus\/i6 (1).jpg'\" while using SageMaker for image classification. They have 400 images in JPG format for the datasets, which are from two different phone models, and they want to classify them into two different classes. The user is unsure where to put the .lst file and whether to create two different folders in the bucket to store the .lst file. They have provided the resource configuration, hyperparameters, input data configuration, and S3 output path for the training job.",
        "Question_gpt_summary":"user encount error messag clienterror lst part part class iphonesplu jpg imag classif imag jpg format dataset differ phone model want classifi differ class user unsur lst file creat differ folder bucket store lst file provid resourc configur hyperparamet input data configur output path train job",
        "Answer_original_content":"creat train job specifi got error resolv error clienterror lst part part sure file lst format tab separ like iphon iphon jpg iphon iphon jpg iphon iphon jpg nano mac valid tab separ format",
        "Answer_preprocessed_content":"creat train job specifi got error resolv error clienterror lst part part sure file lst like mac valid format",
        "Answer_gpt_summary_original":"Solution: The error message \"ClientError: lst should at least has three parts, but only has 1 parts for '1 0 class_iphone6splus\/i6 (1).jpg'\" can be resolved by ensuring that the .lst file is well-formatted with tab-separated values. The format should have at least three parts, with the first part being the image index, the second part being the class index, and the third part being the image file path. The user can use a text editor like nano on MAC OS X to validate the tab-separated format.",
        "Answer_gpt_summary":"solut error messag clienterror lst part part class iphonesplu jpg resolv ensur lst file format tab separ valu format part imag index second class index imag file path user us text editor like nano mac valid tab separ format"
    },
    {
        "Question_title":"Download model artefact from Databricks workspace",
        "Question_body":"<p>How can I download a mlflow model artefact in a docker container from databricks workspace?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1645771255510,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1645795991323,
        "Question_score":0,
        "Question_view_count":370.0,
        "Poster_id":4065074.0,
        "Poster_created_time":1411361217027,
        "Poster_last_access_time":1662192778187,
        "Poster_age":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation":569.0,
        "Poster_views":123.0,
        "Answer_body":"<p>To download a model from Databricks workspace you need to do two things:<\/p>\n<ol>\n<li><p>Set MLFlow tracking URI to databricks using python API<\/p>\n<\/li>\n<li><p>Setup databricks authentication. I prefer authenticating by setting the following environment variables, you can also use databricks CLI to authenticate:<\/p>\n<pre><code>DATABRICKS_HOST\n\nDATABRICKS_TOKEN\n<\/code><\/pre>\n<\/li>\n<li><p>Here's a basic code snippet to download a model from Databricks workspace model registry:<\/p>\n<pre><code>import os\nimport mlflow\nfrom mlflow.store.artifact.models_artifact_repo import ModelsArtifactRepository\n\nmodel_name = &quot;example-model-name&quot;\nmodel_stage = &quot;Staging&quot;  # Should be either 'Staging' or 'Production'\n\nmlflow.set_tracking_uri(&quot;databricks&quot;)\n\nos.makedirs(&quot;model&quot;, exist_ok=True)\nlocal_path = ModelsArtifactRepository(\n    f'models:\/{model_name}\/{model_stage}').download_artifacts(&quot;&quot;, dst_path=&quot;model&quot;)\n\nprint(f'{model_stage} Model {model_name} is downloaded at {local_path}')\n<\/code><\/pre>\n<p>Running above python script will download an ML model in the model directory.<\/p>\n<p><strong>Containerizing MLFlow model serving with Docker<\/strong><\/p>\n<p>The next step is to package this downloaded model in a docker image and serve a model when you run the image.<\/p>\n<\/li>\n<\/ol>\n<p>Here's a basic Dockerfile to do the same:<\/p>\n<pre><code>FROM continuumio\/miniconda3\n\nENV MLFLOW_HOME \/opt\/mlflow\nENV MLFLOW_VERSION 1.12.1\nENV PORT 5000\n\nRUN conda install -c conda-forge mlflow=${MLFLOW_VERSION}\n\nCOPY model\/ ${MLFLOW_HOME}\/model\n\nWORKDIR ${MLFLOW_HOME}\n\nRUN mlflow models prepare-env -m ${MLFLOW_HOME}\/model\n\nRUN useradd -d ${MLFLOW_HOME} mlflow\nRUN chown mlflow: ${MLFLOW_HOME}\nUSER mlflow\n\nCMD mlflow models serve -m ${MLFLOW_HOME}\/model --host 0.0.0.0 --port ${PORT}\n<\/code><\/pre>\n<p>For more information you can follow this <a href=\"https:\/\/dev.to\/itachiredhair\/downloading-mlflow-model-from-databricks-and-serving-with-docker-38ip\" rel=\"nofollow noreferrer\">article<\/a> from Akshay Milmile<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1645795937627,
        "Answer_score":2.0,
        "Answerer_id":11104805.0,
        "Answerer_created_time":1550902509267,
        "Answerer_last_access_time":1664081158016,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":2669.0,
        "Answerer_views":3292.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71262010",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1645776779603,
        "Question_original_content":"download model artefact databrick workspac download model artefact docker contain databrick workspac",
        "Question_preprocessed_content":"download model artefact databrick workspac download model artefact docker contain databrick workspac",
        "Question_gpt_summary_original":"The user is facing a challenge in downloading a mlflow model artefact from a Databricks workspace in a docker container.",
        "Question_gpt_summary":"user face challeng download model artefact databrick workspac docker contain",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"How to copy notebooks between different Sagemaker instances?",
        "Question_body":"<p>My search didn't yield anything useful so I was wondering if there is any easy way to copy notebooks from one instance to another instance on Sagemaker? Of course other than manually downloading the notebooks on one instance and uploading to the other one!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_created_time":1537397018090,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1537397797832,
        "Question_score":3,
        "Question_view_count":5303.0,
        "Poster_id":5432304.0,
        "Poster_created_time":1444524456120,
        "Poster_last_access_time":1663864360907,
        "Poster_age":null,
        "Poster_location":"Los Angeles, CA, USA",
        "Poster_reputation":973.0,
        "Poster_views":82.0,
        "Answer_body":"<p>The recommended way to do this (as of 12\/16\/2018) would be to use the newly- launched Git integration for SageMaker Notebook Instances.<\/p>\n\n<ol>\n<li>Create a Git repository for your notebooks<\/li>\n<li>Commit and push changes from Notebook Instance #1 to your Git repo<\/li>\n<li>Start Notebook Instance #2 using the same Git repo<\/li>\n<\/ol>\n\n<p>This way your notebooks are persisted in the Git repo rather than on the  instance, and the Git repo can be shared by multiple instances. <\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebooks-now-support-git-integration-for-increased-persistence-collaboration-and-reproducibility\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":8899112.0,
        "Answerer_created_time":1510046220943,
        "Answerer_last_access_time":1613678255310,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":292.0,
        "Answerer_views":43.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52415136",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1544938600760,
        "Question_original_content":"copi notebook differ instanc search yield us wonder easi wai copi notebook instanc instanc cours manual download notebook instanc upload",
        "Question_preprocessed_content":"copi notebook differ instanc search yield us wonder easi wai copi notebook instanc instanc cours manual download notebook instanc upload",
        "Question_gpt_summary_original":"The user is facing a challenge of finding an easy way to copy notebooks from one Sagemaker instance to another without having to manually download and upload them.",
        "Question_gpt_summary":"user face challeng find easi wai copi notebook instanc have manual download upload",
        "Answer_original_content":"recommend wai us newli launch git integr notebook instanc creat git repositori notebook commit push chang notebook instanc git repo start notebook instanc git repo wai notebook persist git repo instanc git repo share multipl instanc http aw amazon com blog machin learn amazon notebook support git integr increas persist collabor reproduc",
        "Answer_preprocessed_content":"recommend wai us newli launch git integr notebook instanc creat git repositori notebook commit push chang notebook instanc git repo start notebook instanc git repo wai notebook persist git repo instanc git repo share multipl instanc",
        "Answer_gpt_summary_original":"Solution: The recommended solution is to use the newly-launched Git integration for SageMaker Notebook Instances. The user needs to create a Git repository for their notebooks, commit and push changes from Notebook Instance #1 to the Git repo, and start Notebook Instance #2 using the same Git repo. This way, the notebooks are persisted in the Git repo rather than on the instance, and the Git repo can be shared by multiple instances.",
        "Answer_gpt_summary":"solut recommend solut us newli launch git integr notebook instanc user need creat git repositori notebook commit push chang notebook instanc git repo start notebook instanc git repo wai notebook persist git repo instanc git repo share multipl instanc"
    },
    {
        "Question_title":"Feature Importance for XGBoost in Sagemaker",
        "Question_body":"<p>I have built an XGBoost model using Amazon Sagemaker, but I was unable to find anything which will help me interpret the model and validate if it has learned the right dependencies.<\/p>\n\n<p>Generally, we can see Feature Importance for XGBoost by get_fscore() function in the python API (<a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html<\/a>) I see nothing of that sort in the sagemaker api(<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>).<\/p>\n\n<p>I know I can build my own model and then deploy that using sagemaker but I am curious if anyone has faced this problem and how they overcame it.<\/p>\n\n<p>Thanks.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1554934002317,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":4,
        "Question_view_count":3637.0,
        "Poster_id":4388099.0,
        "Poster_created_time":1419327925267,
        "Poster_last_access_time":1663886373928,
        "Poster_age":null,
        "Poster_location":"Mountain View, CA, USA",
        "Poster_reputation":123.0,
        "Poster_views":17.0,
        "Answer_body":"<p>SageMaker XGBoost currently does not provide interface to retrieve feature importance from the model. You can write some code to get the feature importance from the XGBoost model. You have to get the booster object artifacts from the model in S3 and then use the following snippet <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle as pkl\nimport xgboost\nbooster = pkl.load(open(model_file, 'rb'))\nbooster.get_score()\nbooster.get_fscore()\n<\/code><\/pre>\n\n<p>Refer <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">XGBoost doc<\/a> for methods to get feature importance from the Booster object such as <code>get_score()<\/code> or <code>get_fscore()<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":6069517.0,
        "Answerer_created_time":1458100127643,
        "Answerer_last_access_time":1663950101183,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1083.0,
        "Answerer_views":86.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55621967",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1555001695856,
        "Question_original_content":"featur import xgboost built xgboost model unabl help interpret model valid learn right depend gener featur import xgboost fscore function python api http xgboost readthedoc latest python python api html sort api http readthedoc stabl estim html know build model deploi curiou face problem overcam thank",
        "Question_preprocessed_content":"featur import xgboost built xgboost model unabl help interpret model valid learn right depend gener featur import xgboost function python api sort know build model deploi curiou face problem overcam thank",
        "Question_gpt_summary_original":"The user has encountered challenges in interpreting and validating the XGBoost model built using Amazon Sagemaker. They are unable to find a function in the Sagemaker API that can help them see the feature importance for XGBoost, which is available in the Python API. The user is seeking advice on how to overcome this problem.",
        "Question_gpt_summary":"user encount challeng interpret valid xgboost model built unabl function api help featur import xgboost avail python api user seek advic overcom problem",
        "Answer_original_content":"xgboost current provid interfac retriev featur import model write code featur import xgboost model booster object artifact model us follow snippet import pickl pkl import xgboost booster pkl load open model file booster score booster fscore refer xgboost doc method featur import booster object score fscore",
        "Answer_preprocessed_content":"xgboost current provid interfac retriev featur import model write code featur import xgboost model booster object artifact model us follow snippet refer xgboost doc method featur import booster object",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user can write some code to get the feature importance from the XGBoost model. They can get the booster object artifacts from the model in S3 and then use the provided code snippet to retrieve the feature importance. The XGBoost documentation provides methods to get feature importance from the Booster object such as `get_score()` or `get_fscore()`.",
        "Answer_gpt_summary":"solut discuss suggest user write code featur import xgboost model booster object artifact model us provid code snippet retriev featur import xgboost document provid method featur import booster object score fscore"
    },
    {
        "Question_title":"Databricks MLFlow AutoML XGBoost can't predict_proba()",
        "Question_body":"<p>I used AutoML in Databricks Notebooks for a binary classification problem and the winning model flavor was XGBoost (big surprise).<\/p>\n<p>The outputted model is of this variety:<\/p>\n<pre><code>mlflow.pyfunc.loaded_model:\n      artifact_path: model\n      flavor: mlflow.sklearn\n      run_id: 123456789\n<\/code><\/pre>\n<p>Any idea why when I use <code>model.predict_proba(X)<\/code>, I get this response?<\/p>\n<p><code>AttributeError: 'PyFuncModel' object has no attribute 'predict_proba'<\/code><\/p>\n<p>I know it is possible to get the probabilities because ROC\/AUC is a metric used for tuning the model. Any help would be amazing!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1640912523883,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":451.0,
        "Poster_id":11768634.0,
        "Poster_created_time":1562828557216,
        "Poster_last_access_time":1663796637888,
        "Poster_age":null,
        "Poster_location":"San Francisco, CA, USA",
        "Poster_reputation":77.0,
        "Poster_views":17.0,
        "Answer_body":"<p>I had the same issue with catboost model.\nThe way I solved it was by saving the artifacts in a local dir<\/p>\n<pre><code>import os\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()\nlocal_dir = &quot;\/dbfs\/FileStore\/user\/models&quot;\nlocal_path = client.download_artifacts('run_id', &quot;model&quot;, local_dir)```\n\n```model_path = '\/dbfs\/FileStore\/user\/models\/model\/model.cb'\nmodel = CatBoostClassifier()\nmodel = model.load_model(model_path)\nmodel.predict_proba(test_set)```\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":8716240.0,
        "Answerer_created_time":1507058830472,
        "Answerer_last_access_time":1663842473432,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":36.0,
        "Answerer_views":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70538098",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1646744187860,
        "Question_original_content":"databrick automl xgboost predict proba automl databrick notebook binari classif problem win model flavor xgboost big surpris output model varieti pyfunc load model artifact path model flavor sklearn run idea us model predict proba respons attributeerror pyfuncmodel object attribut predict proba know possibl probabl roc auc metric tune model help amaz",
        "Question_preprocessed_content":"databrick automl xgboost automl databrick notebook binari classif problem win model flavor xgboost output model varieti idea us respons know possibl probabl metric tune model help amaz",
        "Question_gpt_summary_original":"The user encountered a challenge while using AutoML in Databricks Notebooks for a binary classification problem. The winning model flavor was XGBoost, but when the user tried to use the predict_proba() function, they received an AttributeError stating that the PyFuncModel object has no attribute predict_proba. The user is seeking help to understand why this is happening and how to obtain the probabilities.",
        "Question_gpt_summary":"user encount challeng automl databrick notebook binari classif problem win model flavor xgboost user tri us predict proba function receiv attributeerror state pyfuncmodel object attribut predict proba user seek help understand happen obtain probabl",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"How to only load one portion of an AzureML tabular dataset (linked to Azure Blob Storage)",
        "Question_body":"<p>I have a DataSet defined in my AzureML workspace that is linked to an Azure Blob Storage csv file of 1.6Gb.  This file contains timeseries information of around 10000 devices.  So, I could've also created 10000 smaller files (since I use ADF for the transmission pipeline).<\/p>\n\n<p>My question now is: is it possible to load a part of the AzureML DataSet in my python notebook or script instead of loading the entire file?<br>\nThe only code I have now load the full file:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>dataset = Dataset.get_by_name(workspace, name='devicetelemetry')\ndf = dataset.to_pandas_dataframe()\n<\/code><\/pre>\n\n<p>The only concept of partitions I found with regards to the AzureML datasets was around time series and partitioning of timestamps &amp; dates.  However, here I would love to partition per device, so I can very easily just do a load of all telemetry of a specific device.<\/p>\n\n<p>Any pointers to docs or any suggestions? (I couldn't find any so far)<\/p>\n\n<p>Thanks already<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1585756434767,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":560.0,
        "Poster_id":2063902.0,
        "Poster_created_time":1360655430743,
        "Poster_last_access_time":1663784892907,
        "Poster_age":null,
        "Poster_location":"Belgium",
        "Poster_reputation":2947.0,
        "Poster_views":355.0,
        "Answer_body":"<p>You're right there are the <code>.time_*()<\/code> filtering methods available with a <code>TabularDataset<\/code>.<\/p>\n\n<p>I'm not aware of anyway to do filtering as you suggest (but I agree it would be a useful feature). To get per-device partitioning, my recommendation would be to structure your container like so:<\/p>\n\n<pre><code>- device1\n    - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n- device2\n   - 2020\n        - 2020-03-31.csv\n        - 2020-04-01.csv\n<\/code><\/pre>\n\n<p>In this way you can define an all-up Dataset, but also per-device Datasets by passing folder of the device to the DataPath<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># all up dataset\nds_all = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, '*')\n)\n# device 1 dataset\nds_d1 = Dataset.Tabular.from_delimited_files(\n    path=DataPath(datastore, 'device1\/*')\n)\n<\/code><\/pre>\n\n<p><strong>CAVEAT<\/strong><\/p>\n\n<p>dataprep SDK is optimized for blobs around 200MB in size. So you can work with many small files, but sometimes it can be slower than expected, especially considering the overhead of enumerating all blobs in a container.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":3842610.0,
        "Answerer_created_time":1405457120427,
        "Answerer_last_access_time":1663947733100,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":3359.0,
        "Answerer_views":555.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60975078",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1585761136672,
        "Question_original_content":"load portion tabular dataset link azur blob storag dataset defin workspac link azur blob storag csv file file contain timeseri inform devic creat smaller file us adf transmiss pipelin question possibl load dataset python notebook script instead load entir file code load file dataset dataset workspac devicetelemetri dataset panda datafram concept partit regard dataset time seri partit timestamp date love partit devic easili load telemetri specif devic pointer doc suggest couldn far thank",
        "Question_preprocessed_content":"load portion tabular dataset dataset defin workspac link azur blob storag csv file file contain timeseri inform devic creat smaller file question possibl load dataset python notebook script instead load entir file code load file concept partit regard dataset time seri partit timestamp date love partit devic easili load telemetri specif devic pointer doc suggest thank",
        "Question_gpt_summary_original":"The user is facing a challenge of loading a large AzureML DataSet linked to an Azure Blob Storage csv file of 1.6Gb, containing timeseries information of around 10000 devices. The user wants to know if it is possible to load only a part of the DataSet in their python notebook or script instead of loading the entire file. The user is also looking for suggestions on how to partition the DataSet per device for easy loading of all telemetry of a specific device.",
        "Question_gpt_summary":"user face challeng load larg dataset link azur blob storag csv file contain timeseri inform devic user want know possibl load dataset python notebook script instead load entir file user look suggest partit dataset devic easi load telemetri specif devic",
        "Answer_original_content":"right time filter method avail tabulardataset awar filter suggest agre us featur devic partit recommend structur contain like devic csv csv devic csv csv wai defin dataset devic dataset pass folder devic datapath dataset dataset tabular delimit file path datapath datastor devic dataset dataset tabular delimit file path datapath datastor devic caveat dataprep sdk optim blob size work small file slower expect especi consid overhead enumer blob contain",
        "Answer_preprocessed_content":"right filter method avail awar filter suggest partit recommend structur contain like wai defin dataset dataset pass folder devic datapath caveat dataprep sdk optim blob size work small file slower expect especi consid overhead enumer blob contain",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use the `.time_*()` filtering methods available with a `TabularDataset` to filter the data.\n2. Structure the container in a way that allows for per-device partitioning, which can be achieved by creating a folder for each device and then creating subfolders for each year and csv files for each day. This will allow for defining an all-up Dataset, but also per-device Datasets by passing the folder of the device to the DataPath.\n3. Keep in mind that the dataprep SDK is optimized for blobs around 200MB in size, so working with many small files may be slower than expected, especially considering the overhead of enumerating all blobs in a container.\n\nNo personal opinions",
        "Answer_gpt_summary":"possibl solut mention discuss us time filter method avail tabulardataset filter data structur contain wai allow devic partit achiev creat folder devic creat subfold year csv file dai allow defin dataset devic dataset pass folder devic datapath mind dataprep sdk optim blob size work small file slower expect especi consid overhead enumer blob contain person opinion"
    },
    {
        "Question_title":"Deepar Prediction Quantiles Explained",
        "Question_body":"<p>I am working with Deepar and trying to get a better understanding of the quantile values returned. From the documentation, the likelihood hyperparameter explains that: <code>...provide quantiles of the distribution and return samples<\/code>. <\/p>\n\n<p>If I look at a single data point the quantiles returned are linear. E.g. the 0.1 quantile has the lowest predicted value and 0.9 quantile has the highest predicted value. I am having trouble understanding this. If these are samples from the distribution, shouldn't they look similar to the distribution selected with the likelihood hyperparameter (negative-binomial in my case)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1570719704473,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":568.0,
        "Poster_id":6020161.0,
        "Poster_created_time":1457123770467,
        "Poster_last_access_time":1663868088727,
        "Poster_age":null,
        "Poster_location":"Denver",
        "Poster_reputation":246.0,
        "Poster_views":24.0,
        "Answer_body":"<p>DeepAR returns probabilistic forecasts in terms of quantiles: by default, the 0.1, 0.2, 0.3, ..., 0.9 quantiles are returned. This means that, according to the model, in each future time step you have 10% chance of observing something lower than the 0.1 quantile, 20% chance of observing something lower than the 0.2 quantile, and so on. Quantiles are in fact in order, and they must be by definition of quantile. Hope this clarifies is a bit!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Answerer_id":387656.0,
        "Answerer_created_time":1278673735383,
        "Answerer_last_access_time":1663332557230,
        "Answerer_age":null,
        "Answerer_location":"Berlin, Germany",
        "Answerer_reputation":242.0,
        "Answerer_views":37.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58325923",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1573131591227,
        "Question_original_content":"deepar predict quantil explain work deepar try better understand quantil valu return document likelihood hyperparamet explain provid quantil distribut return sampl look singl data point quantil return linear quantil lowest predict valu quantil highest predict valu have troubl understand sampl distribut shouldn look similar distribut select likelihood hyperparamet neg binomi case",
        "Question_preprocessed_content":"deepar predict quantil explain work deepar try better understand quantil valu return document likelihood hyperparamet explain look singl data point quantil return linear quantil lowest predict valu quantil highest predict valu have troubl understand sampl distribut shouldn look similar distribut select likelihood hyperparamet",
        "Question_gpt_summary_original":"The user is having trouble understanding the quantile values returned by Deepar, specifically how they are linear for a single data point and do not resemble the selected distribution (negative-binomial).",
        "Question_gpt_summary":"user have troubl understand quantil valu return deepar specif linear singl data point resembl select distribut neg binomi",
        "Answer_original_content":"deepar return probabilist forecast term quantil default quantil return mean accord model futur time step chanc observ lower quantil chanc observ lower quantil quantil fact order definit quantil hope clarifi bit",
        "Answer_preprocessed_content":"deepar return probabilist forecast term quantil default quantil return mean accord model futur time step chanc observ lower quantil chanc observ lower quantil quantil fact order definit quantil hope clarifi bit",
        "Answer_gpt_summary_original":"Solution: The discussion provides an explanation of how DeepAR returns probabilistic forecasts in terms of quantiles. The quantiles returned are in order and represent the probability of observing a value lower than that quantile. No specific solution is provided to address the user's trouble in understanding the quantile values returned by DeepAR.",
        "Answer_gpt_summary":"solut discuss provid explan deepar return probabilist forecast term quantil quantil return order repres probabl observ valu lower quantil specif solut provid address user troubl understand quantil valu return deepar"
    },
    {
        "Question_title":"AzureML Model Register",
        "Question_body":"<p>I was trying to register a model using the <code>Run<\/code> Class like this:<\/p>\n<pre><code>model = run.register_model(\n    model_name=model_name,\n    model_path=model_path)\n<\/code><\/pre>\n<p>Errors with message: <code>Could not locate the provided model_path ... in the set of files uploaded to the run...<\/code><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1643643837027,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":319.0,
        "Poster_id":13526512.0,
        "Poster_created_time":1589293508567,
        "Poster_last_access_time":1663681781072,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":833.0,
        "Poster_views":55.0,
        "Answer_body":"<p>The only way I found to fix the issue was to use the <code>Model<\/code> Class instead:<\/p>\n<pre><code>        model = Model.register(\n            workspace=ws,\n            model_name=model_name,\n            model_path=model_path,\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version=sklearn.__version__,\n            description='Model Deescription',\n            tags={'Name' : 'ModelName', 'Type' : 'Production'},\n            model_framework=Model.Framework.SCIKITLEARN,\n            model_framework_version='1.0'\n            )\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":13526512.0,
        "Answerer_created_time":1589293508567,
        "Answerer_last_access_time":1663681781072,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":833.0,
        "Answerer_views":55.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70928761",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1643643837027,
        "Question_original_content":"model regist try regist model run class like model run regist model model model model path model path error messag locat provid model path set file upload run",
        "Question_preprocessed_content":"model regist try regist model class like error messag",
        "Question_gpt_summary_original":"The user encountered challenges while trying to register a model using the Run Class in AzureML. The error message indicated that the provided model_path could not be located in the set of files uploaded to the run.",
        "Question_gpt_summary":"user encount challeng try regist model run class error messag indic provid model path locat set file upload run",
        "Answer_original_content":"wai fix issu us model class instead model model regist workspac model model model path model path model framework model framework scikitlearn model framework version sklearn version descript model deescript tag modelnam type product model framework model framework scikitlearn model framework version",
        "Answer_preprocessed_content":"wai fix issu us class instead",
        "Answer_gpt_summary_original":"Solution: The user found a solution to the challenge by using the Model Class instead of the Run Class to register the model. The code snippet provided shows how to use the Model Class to register a model in AzureML.",
        "Answer_gpt_summary":"solut user solut challeng model class instead run class regist model code snippet provid show us model class regist model"
    },
    {
        "Question_title":"Can ClearML (formerly Trains) work a local server?",
        "Question_body":"<p>I am trying to start my way with <a href=\"https:\/\/github.com\/allegroai\/clearml\" rel=\"nofollow noreferrer\">ClearML<\/a> (formerly known as Trains).<\/p>\n<p>I see on the <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/getting_started\/index.html\" rel=\"nofollow noreferrer\">documentation<\/a> that I need to have server running, either on the ClearML platform itself, or on a remote machine using AWS etc.<\/p>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<p>According to <a href=\"https:\/\/allegro.ai\/clearml\/docs\/rst\/deploying_clearml\/index.html\" rel=\"nofollow noreferrer\">this<\/a> I can install the <code>trains-server<\/code> on any remote machine, so in theory I should also be able to install it on my local machine, but it still requires me to have Kubernetes or Docker, but I am not using any of them.<\/p>\n<p>Anyone had any luck using ClearML (or Trains, I think it's still quite the same API and all) on a local server?<\/p>\n<ul>\n<li>My OS is Ubuntu 18.04.<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1609343679217,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1609427009848,
        "Question_score":5,
        "Question_view_count":740.0,
        "Poster_id":2934048.0,
        "Poster_created_time":1383083414230,
        "Poster_last_access_time":1663937141383,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":2801.0,
        "Poster_views":131.0,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<blockquote>\n<p>I would really like to bypass this restriction and run experiments on my local machine, not connecting to any remote destination.<\/p>\n<\/blockquote>\n<p>A few options:<\/p>\n<ol>\n<li>The Clearml Free trier offers free hosting for your experiments, these experiment are only accessible to you, unless you specifically want to share them among your colleagues. This is probably the easiest way to <a href=\"https:\/\/app.community.clear.ml\" rel=\"nofollow noreferrer\">get started<\/a>.<\/li>\n<li>Install the ClearML-Server basically all you need is docker installed and you should be fine. There are full instructions <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/deploying_clearml\/clearml_server_linux_mac\/\" rel=\"nofollow noreferrer\">here<\/a> , this is the summary:<\/li>\n<\/ol>\n<pre class=\"lang-bash prettyprint-override\"><code>echo &quot;vm.max_map_count=262144&quot; &gt; \/tmp\/99-trains.conf\nsudo mv \/tmp\/99-trains.conf \/etc\/sysctl.d\/99-trains.conf\nsudo sysctl -w vm.max_map_count=262144\nsudo service docker restart\n\nsudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/latest\/download\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsudo chmod +x \/usr\/local\/bin\/docker-compose\n\nsudo mkdir -p \/opt\/trains\/data\/elastic_7\nsudo mkdir -p \/opt\/trains\/data\/mongo\/db\nsudo mkdir -p \/opt\/trains\/data\/mongo\/configdb\nsudo mkdir -p \/opt\/trains\/data\/redis\nsudo mkdir -p \/opt\/trains\/logs\nsudo mkdir -p \/opt\/trains\/config\nsudo mkdir -p \/opt\/trains\/data\/fileserver\n\nsudo curl https:\/\/raw.githubusercontent.com\/allegroai\/trains-server\/master\/docker-compose.yml -o \/opt\/trains\/docker-compose.yml\ndocker-compose -f \/opt\/trains\/docker-compose.yml up -d\n<\/code><\/pre>\n<ol start=\"3\">\n<li>ClearML also supports full offline mode (i.e. no outside connection is made). Once your experiment completes, you can manually import the run to your server (either self hosted or free tier server)<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.set_offline(True)\ntask = Task.init(project_name='examples', task_name='offline mode experiment')\n<\/code><\/pre>\n<p>When the process ends you will get a link to a zip file containing the output of the entire offline session:<\/p>\n<pre><code>ClearML Task: Offline session stored in \/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip\n<\/code><\/pre>\n<p>Later you can import the session with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\nTask.import_offline_session('\/home\/user\/.clearml\/cache\/offline\/offline-2d061bb57d9e408a9420c4fe81e26ad0.zip')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1661326401412,
        "Answer_score":6.0,
        "Answerer_id":11682840.0,
        "Answerer_created_time":1561143508792,
        "Answerer_last_access_time":1659992319200,
        "Answerer_age":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation":489.0,
        "Answerer_views":60.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65509754",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1609345139523,
        "Question_original_content":"train work local server try start wai known train document need server run platform remot machin aw like bypass restrict run experi local machin connect remot destin accord instal train server remot machin theori abl instal local machin requir kubernet docker luck train think api local server ubuntu",
        "Question_preprocessed_content":"work local server try start wai document need server run platform remot machin aw like bypass restrict run experi local machin connect remot destin accord instal remot machin theori abl instal local machin requir kubernet docker luck local server ubuntu",
        "Question_gpt_summary_original":"The user is facing a challenge in using ClearML (formerly Trains) on a local server. The documentation states that a server is required, either on the ClearML platform or on a remote machine using AWS. The user wants to bypass this restriction and run experiments on their local machine, but installing the trains-server on a local machine still requires Kubernetes or Docker, which the user is not using. The user is seeking advice on whether anyone has had success using ClearML on a local server.",
        "Question_gpt_summary":"user face challeng train local server document state server requir platform remot machin aw user want bypass restrict run experi local machin instal train server local machin requir kubernet docker user user seek advic success local server",
        "Answer_original_content":"disclaim member team train like bypass restrict run experi local machin connect remot destin option free trier offer free host experi experi access specif want share colleagu probabl easiest wai start instal server basic need docker instal fine instruct summari echo max map count tmp train conf sudo tmp train conf sysctl train conf sudo sysctl max map count sudo servic docker restart sudo curl http github com docker compos releas latest download docker compos unam unam usr local bin docker compos sudo chmod usr local bin docker compos sudo mkdir opt train data elast sudo mkdir opt train data mongo sudo mkdir opt train data mongo configdb sudo mkdir opt train data redi sudo mkdir opt train log sudo mkdir opt train config sudo mkdir opt train data fileserv sudo curl http raw githubusercont com allegroai train server master docker compos yml opt train docker compos yml docker compos opt train docker compos yml support offlin mode outsid connect experi complet manual import run server self host free tier server import task task set offlin true task task init project exampl task offlin mode experi process end link zip file contain output entir offlin session task offlin session store home user cach offlin offlin dbbdeacfeead zip later import session import task task import offlin session home user cach offlin offlin dbbdeacfeead zip",
        "Answer_preprocessed_content":"disclaim member team like bypass restrict run experi local machin connect remot destin option free trier offer free host experi experi access specif want share colleagu probabl easiest wai start instal server basic need docker instal fine instruct summari support offlin mode experi complet manual import run server process end link zip file contain output entir offlin session later import session",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use the ClearML Free tier to host experiments.\n2. Install the ClearML-Server on a local machine using Docker.\n3. Use ClearML's full offline mode to run experiments and manually import the run to a server later.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss us free tier host experi instal server local machin docker us offlin mode run experi manual import run server later person opinion bias includ summari"
    },
    {
        "Question_title":"Associate CodeCommit repo While creating Sagemaker notebook Instance",
        "Question_body":"<p>I am trying to create Sagemaker notebook instance using cloudformation template. Just wanted to see if there is any way i can associate codecommit repo to that note book instance.<\/p>\n\n<p>I know simple way to create repo and associate it using sagemaker via GUI easily.. However, is there any way we can associate via template. <\/p>\n\n<p>I found similar info on <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html<\/a> But it doesn't show with cloudformation<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1557513388300,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1557769171263,
        "Question_score":1,
        "Question_view_count":1194.0,
        "Poster_id":11244959.0,
        "Poster_created_time":1553287213987,
        "Poster_last_access_time":1663861715048,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":17.0,
        "Poster_views":8.0,
        "Answer_body":"<p>SageMaker now supports associating code repository including CodeCommit and any other Git repository with Notebook Instances via CloudFormation. <\/p>\n\n<p>Here's the link for more information: <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-coderepository.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-coderepository.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":11596370.0,
        "Answerer_created_time":1559607319220,
        "Answerer_last_access_time":1663134415867,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":66.0,
        "Answerer_views":12.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56083148",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1559608178472,
        "Question_original_content":"associ codecommit repo creat notebook instanc try creat notebook instanc cloudform templat want wai associ codecommit repo note book instanc know simpl wai creat repo associ gui easili wai associ templat similar info http doc aw amazon com latest nbi git repo html cloudform",
        "Question_preprocessed_content":"associ codecommit repo creat notebook instanc try creat notebook instanc cloudform templat want wai associ codecommit repo note book instanc know simpl wai creat repo associ gui wai associ templat similar info cloudform",
        "Question_gpt_summary_original":"The user is trying to create a Sagemaker notebook instance using a cloudformation template and is looking for a way to associate a CodeCommit repo to the instance. They are aware of how to do this through the Sagemaker GUI but are seeking a way to do it through the template. The user found information on how to associate a repo through Sagemaker documentation but it does not show how to do it through cloudformation.",
        "Question_gpt_summary":"user try creat notebook instanc cloudform templat look wai associ codecommit repo instanc awar gui seek wai templat user inform associ repo document cloudform",
        "Answer_original_content":"support associ code repositori includ codecommit git repositori notebook instanc cloudform link inform http doc aw amazon com awscloudform latest userguid aw resourc coderepositori html",
        "Answer_preprocessed_content":"support associ code repositori includ codecommit git repositori notebook instanc cloudform link inform",
        "Answer_gpt_summary_original":"Solution: The user can associate a CodeCommit repo to a Sagemaker notebook instance through CloudFormation by using the AWS resource \"AWS::SageMaker::CodeRepository\". The documentation link provided explains how to do this.",
        "Answer_gpt_summary":"solut user associ codecommit repo notebook instanc cloudform aw resourc aw coderepositori document link provid explain"
    },
    {
        "Question_title":"How to parse AWS Sagemaker SM_USER_ARGS with argparse into an argparse Namespace?",
        "Question_body":"<p>AWS Sagemaker uses <code>SM_USER_ARGS<\/code> (as documented <a href=\"https:\/\/github.com\/aws\/sagemaker-containers#sm-user-args\" rel=\"nofollow noreferrer\">here<\/a>) as an environment variable in which it contains a string (list) of arguments as they are passed by the user. So the environment variable value looks like this: <code>'[\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optmize\"]'<\/code>.<\/p>\n\n<p>With <code>json.loads()<\/code> I am capable of transforming that string into a python list. Although, I want to create an abstract module that returns an <strong>argparse Namespace<\/strong> in a way that rest of the code remains intact whether I run it locally or in the AWS Sagemaker service.<\/p>\n\n<p>So, basically, what I want is a code that receives the input <code>[\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optmize\"]<\/code> and output <code>Namespace(test_size=0.2, random_seed='42', not_optmize=True, &lt;other_arguments&gt;... ])<\/code>.<\/p>\n\n<p>Does python <strong>argparse<\/strong> package helps me with that? I am trying to figure out a way that I do not need to re implement the argparse parser.<\/p>\n\n<p>Here is an example, I have this config.ini file:<\/p>\n\n<pre><code>[Docker]\nhome_dir = \/opt\nSM_MODEL_DIR = %(home_dir)s\/ml\/model\nSM_CHANNELS = [\"training\"]\nSM_NUM_GPUS = 1\nSM_NUM_CPUS =\nSM_LOG_LEVEL = 20\nSM_USER_ARGS = [\"--test_size\",\"0.2\",\"--random_seed\",\"42\"]\nSM_INPUT_DIR = %(home_dir)s\/ml\/input\nSM_INPUT_CONFIG_DIR = %(home_dir)s\/ml\/input\/config\nSM_OUTPUT_DIR = %(home_dir)s\/ml\/output\nSM_OUTPUT_INTERMEDIATE_DIR = %(home_dir)s\/ml\/output\/intermediate\n<\/code><\/pre>\n\n<p>I have this Argparser class:<\/p>\n\n<pre><code>import argparse\nimport configparser\nimport datetime\nimport json\nimport multiprocessing\nimport os\nimport time\nfrom pathlib import Path\nfrom typing import Any, Dict\n\nfrom .files import JsonFile, YAMLFile\n\n\nclass ArgParser(ABC):\n\n    @abstractmethod\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        pass\n\n\nclass AWSArgParser(ArgParser):\n\n    def __init__(self):\n        configuration_file_path = 'config.ini'\n\n        self.environment = \"Sagemaker\" \\\n            if os.environ.get(\"SM_MODEL_DIR\", False) \\\n            else os.environ.get(\"ENVIRON\", \"Default\")\n\n        config = configparser.ConfigParser()\n        config.read(configuration_file_path)\n        if self.environment == \"Local\":\n            config[self.environment][\"home_dir\"] = str(pathlib.Path(__file__).parent.absolute())\n        if self.environment != 'Sagemaker':\n            config[self.environment][\"SM_NUM_CPUS\"] = str(multiprocessing.cpu_count())\n\n        for key, value in config[self.environment].items():\n            os.environ[key.upper()] = value\n\n        self.parser = argparse.ArgumentParser()\n        # AWS Sagemaker default environmental arguments\n        self.parser.add_argument(\n            '--model_dir',\n            type=str,\n            default=os.environ['SM_MODEL_DIR'],\n        )\n        self.parser.add_argument(\n            '--channel_names',\n            default=json.loads(os.environ['SM_CHANNELS']),\n        )\n        self.parser.add_argument(\n            '--num_gpus',\n            type=int,\n            default=os.environ['SM_NUM_GPUS'],\n        )\n        self.parser.add_argument(\n            '--num_cpus',\n            type=int,\n            default=os.environ['SM_NUM_CPUS'],\n        )\n        self.parser.add_argument(\n            '--user_args',\n            default=json.loads(os.environ['SM_USER_ARGS']),\n        )\n        self.parser.add_argument(\n            '--input_dir',\n            type=str,\n            default=os.environ['SM_INPUT_DIR'],\n        )\n        self.parser.add_argument(\n            '--input_config_dir',\n            type=Path,\n            default=os.environ['SM_INPUT_CONFIG_DIR'],\n        )\n        self.parser.add_argument(\n            '--output_dir',\n            type=Path,\n            default=os.environ['SM_OUTPUT_DIR'],\n        )\n\n        # Extra arguments\n        self.run_tag = datetime.datetime \\\n            .fromtimestamp(time.time()) \\\n            .strftime('%Y-%m-%d-%H-%M-%S')\n        self.parser.add_argument(\n            '--run_tag',\n            default=self.run_tag,\n            type=str,\n            help=f\"Run tag (default: 'datetime.fromtimestamp')\",\n        )\n        self.parser.add_argument(\n            '--environment',\n            type=str,\n            default=self.environment,\n        )\n\n        self.args = self.parser.parse_args()\n\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        &lt;parse self.args.user_args&gt;\n\n        return self.args\n<\/code><\/pre>\n\n<p>then I have my <code>train<\/code> script:<\/p>\n\n<pre><code>from utils.arg_parser import AWSArgParser\n\nif __name__ == '__main__':\n    logger.info(f\"Begin train.py\")\n\n    if os.environ[\"ENVIRON\"] == \"Sagemaker\":\n        arg_parser = AWSArgParser()\n        args = arg_parser.get_arguments()\n    else:\n        args = &lt;normal local parse&gt;\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_created_time":1583962823900,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1583964791167,
        "Question_score":0,
        "Question_view_count":782.0,
        "Poster_id":5924820.0,
        "Poster_created_time":1455444590636,
        "Poster_last_access_time":1663792994552,
        "Poster_age":null,
        "Poster_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Poster_reputation":903.0,
        "Poster_views":94.0,
        "Answer_body":"<p>Following <a href=\"https:\/\/stackoverflow.com\/users\/1126841\/chepner\">@chepner<\/a>'s comment an example solution would be something like this:<\/p>\n\n<p>config.ini file:<\/p>\n\n<pre><code>[Docker]\nhome_dir = \/opt\nSM_MODEL_DIR = %(home_dir)s\/ml\/model\nSM_CHANNELS = [\"training\"]\nSM_NUM_GPUS = 1\nSM_NUM_CPUS =\nSM_LOG_LEVEL = 20\nSM_USER_ARGS = [\"--test_size\",\"0.2\",\"--random_seed\",\"42\", \"--not_optimize\"]\nSM_INPUT_DIR = %(home_dir)s\/ml\/input\nSM_INPUT_CONFIG_DIR = %(home_dir)s\/ml\/input\/config\nSM_OUTPUT_DIR = %(home_dir)s\/ml\/output\nSM_OUTPUT_INTERMEDIATE_DIR = %(home_dir)s\/ml\/output\/intermediate\n<\/code><\/pre>\n\n<p>A <code>TrainArgParser<\/code> class like this:<\/p>\n\n<pre><code>class ArgParser(ABC):\n\n    @abstractmethod\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        pass\n\n\nclass TrainArgParser(ArgParser):\n\n    def __init__(self):\n        configuration_file_path = 'config.ini'\n\n        self.environment = \"Sagemaker\" \\\n            if os.environ.get(\"SM_MODEL_DIR\", False) \\\n            else os.environ.get(\"ENVIRON\", \"Default\")\n\n        config = configparser.ConfigParser()\n        config.read(configuration_file_path)\n        if self.environment == \"Local\":\n            config[self.environment][\"home_dir\"] = str(pathlib.Path(__file__).parent.absolute())\n        if self.environment != 'Sagemaker':\n            config[self.environment][\"SM_NUM_CPUS\"] = str(multiprocessing.cpu_count())\n\n        for key, value in config[self.environment].items():\n            os.environ[key.upper()] = value\n\n        self.parser = argparse.ArgumentParser()\n        # AWS Sagemaker default environmental arguments\n        self.parser.add_argument(\n            '--model_dir',\n            type=str,\n            default=os.environ['SM_MODEL_DIR'],\n        )\n        self.parser.add_argument(\n            '--channel_names',\n            default=json.loads(os.environ['SM_CHANNELS']),\n        )\n        self.parser.add_argument(\n            '--num_gpus',\n            type=int,\n            default=os.environ['SM_NUM_GPUS'],\n        )\n        self.parser.add_argument(\n            '--num_cpus',\n            type=int,\n            default=os.environ['SM_NUM_CPUS'],\n        )\n        self.parser.add_argument(\n            '--user_args',\n            default=json.loads(os.environ['SM_USER_ARGS']),\n        )\n        self.parser.add_argument(\n            '--input_dir',\n            type=str,\n            default=os.environ['SM_INPUT_DIR'],\n        )\n        self.parser.add_argument(\n            '--input_config_dir',\n            type=Path,\n            default=os.environ['SM_INPUT_CONFIG_DIR'],\n        )\n        self.parser.add_argument(\n            '--output_dir',\n            type=Path,\n            default=os.environ['SM_OUTPUT_DIR'],\n        )\n\n        # Extra arguments\n        self.run_tag = datetime.datetime \\\n            .fromtimestamp(time.time()) \\\n            .strftime('%Y-%m-%d-%H-%M-%S')\n        self.parser.add_argument(\n            '--run_tag',\n            default=self.run_tag,\n            type=str,\n            help=f\"Run tag (default: 'datetime.fromtimestamp')\",\n        )\n        self.parser.add_argument(\n            '--environment',\n            type=str,\n            default=self.environment,\n        )\n\n        self.args = self.parser.parse_args()\n\n    def get_arguments(self) -&gt; Dict[str, Any]:\n        # Not in AWS Sagemaker arguments\n        self.parser.add_argument(\n            '--test_size',\n            default=0.2,\n            type=float,\n            help=\"Test dataset size (default: '0.2')\",\n        )\n        self.parser.add_argument(\n            '--random_seed',\n            default=42,\n            type=int,\n            help=\"Random number for initialization (default: '42')\",\n        )\n        self.parser.add_argument(\n            '--secrets',\n            type=YAMLFile.parse_string,\n            default='',\n            help=\"An yaml formated string (default: '')\"\n        )\n        self.parser.add_argument(\n            '--bucket_name',\n            type=str,\n            default='',\n            help=\"Bucket name of a remote storage (default: '')\"\n        )\n        self.args = self.parser.parse_args(self.args.user_args)\n\n        return self.args\n<\/code><\/pre>\n\n<p>and a entry_script for <code>train<\/code> would start like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\n\nfrom utils.arg_parser import TrainArgParser\n\nif __name__ == '__main__':\n    logger.info(f\"Begin train.py\")\n\n    arg_parser = TrainArgParser()\n    args = arg_parser.get_arguments()\n    print(args)\n<\/code><\/pre>\n\n<p>This should output something like this:<\/p>\n\n<pre><code>Namespace(bucket_name='', channel_names=['training'], environment='Docker', input_config_dir=PosixPath('\/opt\/ml\/input\/config'), input_dir='\/opt\/ml\/input', model_dir='\/opt\/ml\/model', num_cpus=8, num_gpus=1, output_dir=PosixPath('\/opt\/ml\/output'), random_seed=42, run_tag='2020-03-11-22-18-21', secrets={}, test_size=0.2, user_args=['--test_size', '0.2', '--random_seed', '42'])\n<\/code><\/pre>\n\n<p>But that is useless if AWS Sagemaker treats <code>SM_USER_ARGS<\/code> and <code>SM_HPS<\/code> as the same thing. :(<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":5924820.0,
        "Answerer_created_time":1455444590636,
        "Answerer_last_access_time":1663792994552,
        "Answerer_age":null,
        "Answerer_location":"S\u00e3o Paulo, State of S\u00e3o Paulo, Brazil",
        "Answerer_reputation":903.0,
        "Answerer_views":94.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60644771",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1583965847987,
        "Question_original_content":"pars user arg argpars argpars namespac us user arg document environ variabl contain string list argument pass user environ variabl valu look like test size random seed optmiz json load capabl transform string python list want creat abstract modul return argpars namespac wai rest code remain intact run local servic basic want code receiv input test size random seed optmiz output namespac test size random seed optmiz true python argpars packag help try figur wai need implement argpars parser exampl config ini file docker home dir opt model dir home dir model channel train num gpu num cpu log level user arg test size random seed input dir home dir input input config dir home dir input config output dir home dir output output intermedi dir home dir output intermedi argpars class import argpars import configpars import datetim import json import multiprocess import import time pathlib import path type import dict file import jsonfil yamlfil class argpars abc abstractmethod def argument self dict str pass class awsargpars argpars def init self configur file path config ini self environ environ model dir fals environ environ default config configpars configpars config read configur file path self environ local config self environ home dir str pathlib path file parent absolut self environ config self environ num cpu str multiprocess cpu count kei valu config self environ item environ kei upper valu self parser argpars argumentpars default environment argument self parser add argument model dir type str default environ model dir self parser add argument channel name default json load environ channel self parser add argument num gpu type int default environ num gpu self parser add argument num cpu type int default environ num cpu self parser add argument user arg default json load environ user arg self parser add argument input dir type str default environ input dir self parser add argument input config dir type path default environ input config dir self parser add argument output dir type path default environ output dir extra argument self run tag datetim datetim fromtimestamp time time strftime self parser add argument run tag default self run tag type str help run tag default datetim fromtimestamp self parser add argument environ type str default self environ self arg self parser pars arg def argument self dict str return self arg train script util arg parser import awsargpars main logger info begin train environ environ arg parser awsargpars arg arg parser argument arg",
        "Question_preprocessed_content":"pars argpars argpars namespac us environ variabl contain string argument pass user environ variabl valu look like capabl transform string python list want creat abstract modul return argpars namespac wai rest code remain intact run local servic basic want code receiv input output python argpars packag help try figur wai need implement argpars parser exampl file argpars class script",
        "Question_gpt_summary_original":"The user is trying to create an abstract module that returns an argparse Namespace in a way that the rest of the code remains intact whether it is run locally or in the AWS Sagemaker service. The user is trying to figure out a way to avoid re-implementing the argparse parser and wants to know if the argparse package can help with that. The user has provided an example of a config.ini file, an Argparser class, and a train script.",
        "Question_gpt_summary":"user try creat abstract modul return argpars namespac wai rest code remain intact run local servic user try figur wai avoid implement argpars parser want know argpars packag help user provid exampl config ini file argpars class train script",
        "Answer_original_content":"follow chepner comment exampl solut like config ini file docker home dir opt model dir home dir model channel train num gpu num cpu log level user arg test size random seed optim input dir home dir input input config dir home dir input config output dir home dir output output intermedi dir home dir output intermedi trainargpars class like class argpars abc abstractmethod def argument self dict str pass class trainargpars argpars def init self configur file path config ini self environ environ model dir fals environ environ default config configpars configpars config read configur file path self environ local config self environ home dir str pathlib path file parent absolut self environ config self environ num cpu str multiprocess cpu count kei valu config self environ item environ kei upper valu self parser argpars argumentpars default environment argument self parser add argument model dir type str default environ model dir self parser add argument channel name default json load environ channel self parser add argument num gpu type int default environ num gpu self parser add argument num cpu type int default environ num cpu self parser add argument user arg default json load environ user arg self parser add argument input dir type str default environ input dir self parser add argument input config dir type path default environ input config dir self parser add argument output dir type path default environ output dir extra argument self run tag datetim datetim fromtimestamp time time strftime self parser add argument run tag default self run tag type str help run tag default datetim fromtimestamp self parser add argument environ type str default self environ self arg self parser pars arg def argument self dict str argument self parser add argument test size default type float help test dataset size default self parser add argument random seed default type int help random number initi default self parser add argument secret type yamlfil pars string default help yaml format string default self parser add argument bucket type str default help bucket remot storag default self arg self parser pars arg self arg user arg return self arg entri script train start like usr bin env python util arg parser import trainargpars main logger info begin train arg parser trainargpars arg arg parser argument print arg output like namespac bucket channel name train environ docker input config dir posixpath opt input config input dir opt input model dir opt model num cpu num gpu output dir posixpath opt output random seed run tag secret test size user arg test size random seed useless treat user arg hp thing",
        "Answer_preprocessed_content":"follow comment exampl solut like file class like start like output like useless treat thing",
        "Answer_gpt_summary_original":"The solution proposed in the discussion involves creating a TrainArgParser class that reads a config.ini file and sets environment variables based on the environment the code is running in. The class also uses the argparse package to parse arguments and returns a Namespace object. The entry_script for train would then use this class to get the arguments. However, it is noted that this solution may not work if AWS Sagemaker treats SM_USER_ARGS and SM_HPS as the same thing.",
        "Answer_gpt_summary":"solut propos discuss involv creat trainargpars class read config ini file set environ variabl base environ code run class us argpars packag pars argument return namespac object entri script train us class argument note solut work treat user arg hp thing"
    },
    {
        "Question_title":"Publish azure machine learning service with feature hashing",
        "Question_body":"<p>I have created an experiment in azure machine learning studio, this experiment is multi-class classification problem using multi-class neural network algorithm, I have also add 'feature hashing' module to transform a stream of English text into a set of features represented as integers. I have successfully run the experiment but when i publish it as web service endpoint i got message \"Reduce the total number of input and output columns to less than 1000 and try publishing again.\"\nI understood after some research that feature hashing convert text into thousands of feature but the problem is how i publish it as web service? and i don't want to remove 'feature hashing' module.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1442415823127,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":405.0,
        "Poster_id":4513812.0,
        "Poster_created_time":1422680560156,
        "Poster_last_access_time":1544934963472,
        "Poster_age":null,
        "Poster_location":"Ahmedabad, Gujarat, India",
        "Poster_reputation":410.0,
        "Poster_views":15.0,
        "Answer_body":"<p>It sounds like you are trying to output all those thousands of columns as an output. What you really only need is the scored probability or the scored label. To solve this, just drop all the feature hashed columns from the score model module. To do this add in a project columns module, and tell it to start with \"no columns\" then \"include\" by \"column names\", and just add predicted column (scored probability\/scored label). <\/p>\n\n<p>Then hook up the output of that project columns module to your web service output module. Your web service should now be returning only 1-3 columns rather than thousands.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":4582041.0,
        "Answerer_created_time":1424310919952,
        "Answerer_last_access_time":1629240006223,
        "Answerer_age":null,
        "Answerer_location":"Lynnwood, WA",
        "Answerer_reputation":451.0,
        "Answerer_views":102.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/32612311",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1442447100743,
        "Question_original_content":"publish servic featur hash creat experi studio experi multi class classif problem multi class neural network algorithm add featur hash modul transform stream english text set featur repres integ successfulli run experi publish web servic endpoint got messag reduc total number input output column try publish understood research featur hash convert text thousand featur problem publish web servic want remov featur hash modul",
        "Question_preprocessed_content":"publish servic featur hash creat experi studio experi classif problem neural network algorithm add featur hash modul transform stream english text set featur repres integ successfulli run experi publish web servic endpoint got messag reduc total number input output column try publish understood research featur hash convert text thousand featur problem publish web servic want remov featur hash modul",
        "Question_gpt_summary_original":"The user has encountered a challenge while trying to publish an experiment in Azure Machine Learning Studio as a web service endpoint. The experiment involves a multi-class classification problem using a neural network algorithm and a 'feature hashing' module to transform English text into features represented as integers. The user received an error message stating that the total number of input and output columns should be reduced to less than 1000 before publishing. The user is unsure how to publish the experiment as a web service without removing the 'feature hashing' module.",
        "Question_gpt_summary":"user encount challeng try publish experi studio web servic endpoint experi involv multi class classif problem neural network algorithm featur hash modul transform english text featur repres integ user receiv error messag state total number input output column reduc publish user unsur publish experi web servic remov featur hash modul",
        "Answer_original_content":"sound like try output thousand column output need score probabl score label solv drop featur hash column score model modul add project column modul tell start column includ column name add predict column score probabl score label hook output project column modul web servic output modul web servic return column thousand",
        "Answer_preprocessed_content":"sound like try output thousand column output need score probabl score label solv drop featur hash column score model modul add project column modul tell start column includ column name add predict column hook output project column modul web servic output modul web servic return column thousand",
        "Answer_gpt_summary_original":"Solution: The user can drop all the feature hashed columns from the score model module by adding a project columns module and selecting \"no columns\" as the starting point. Then, the user can include only the predicted column (scored probability\/scored label) by selecting \"include\" by \"column names\". The output of the project columns module can be connected to the web service output module, which will now return only 1-3 columns instead of thousands.",
        "Answer_gpt_summary":"solut user drop featur hash column score model modul ad project column modul select column start point user includ predict column score probabl score label select includ column name output project column modul connect web servic output modul return column instead thousand"
    },
    {
        "Question_title":"Compare prediction\/summary scores on a table",
        "Question_body":"<p>At the end of my run, I make predictions on the development set and log these scores to wandb with this command:<\/p>\n<pre><code class=\"lang-auto\">wandb.log({\"best_epoch\": best_epoch, \"dev_micro_acc\": dev_micro_acc, \"dev_weighted_f1\": dev_weighted_f1,\n               \"dev_f1_label1\": dev_f1_label1,\n               \"dev_f05_score\": dev_f05_score, \"dev_roc_auc\": dev_roc_auc})\n<\/code><\/pre>\n<p>These values then appear in the summary section on my overview page for each run. But how can I see them as overview to compare them with other runs? Like, it would be nice to see them as column on the run table such that I can sort there for the best score and therefore identify my best model, but I acknowledge that the config columns probably aren\u2019t the proper place for this.<\/p>\n<p>So where could I see such an overview on wandb?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1675270660403,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":72.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/harpiye\">@harpiye<\/a>, thanks for writing in! These metrics should appear in the Runs table by default but if this isn\u2019t the case, in the top right corner there\u2019s a <code>Columns<\/code> button and you can manage the visible columns. If you cannot see these metrics there, could you please send me a link to the project and so I can have a look at it?<\/p>\n<p>To be able to compare runs, I\u2019d recommend you to use the <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/panels\/run-comparer\">Run comparer plot<\/a> which you can add from <code>Add panel<\/code> and allows you to compare your different runs. Please let me know if this would work for you!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/compare-prediction-summary-scores-on-a-table\/3791",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1675341672580,
        "Question_original_content":"compar predict summari score tabl end run predict develop set log score command log best epoch best epoch dev micro acc dev micro acc dev weight dev weight dev label dev label dev score dev score dev roc auc dev roc auc valu appear summari section overview page run overview compar run like nice column run tabl sort best score identifi best model acknowledg config column probabl arent proper place overview",
        "Question_preprocessed_content":"compar score tabl end run predict develop set log score command valu appear summari section overview page run overview compar run like nice column run tabl sort best score identifi best model acknowledg config column probabl arent proper place overview",
        "Question_gpt_summary_original":"The user is facing a challenge in comparing prediction\/summary scores on a table in Wandb. They have logged scores for each run, but are unable to see them as a column on the run table to compare with other runs and identify the best model. The user is seeking guidance on where to find such an overview on Wandb.",
        "Question_gpt_summary":"user face challeng compar predict summari score tabl log score run unabl column run tabl compar run identifi best model user seek guidanc overview",
        "Answer_original_content":"harpiy thank write metric appear run tabl default isnt case right corner there column button manag visibl column metric send link project look abl compar run recommend us run compar plot add add panel allow compar differ run let know work",
        "Answer_preprocessed_content":"thank write metric appear run tabl default isnt case right corner there button manag visibl column metric send link project look abl compar run recommend us run compar plot add allow compar differ run let know work",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Check the \"Columns\" button in the top right corner of the Runs table and manage the visible columns to see the metrics.\n2. Use the \"Run comparer plot\" which can be added from \"Add panel\" to compare different runs.\n\nNo personal opinions or biases are included in the response.",
        "Answer_gpt_summary":"possibl solut mention discuss check column button right corner run tabl manag visibl column metric us run compar plot ad add panel compar differ run person opinion bias includ respons"
    },
    {
        "Question_title":"SageMaker Studio projects in VpcOnly mode without internet access",
        "Question_body":"A customer is using SageMaker Studio in VpcOnly mode (VPC, protected subnets without internet access, NO NAT gateways). The all functionality is fine. However, when I try create a SageMaker projects - as described here, SageMaker Studio is unable to list the project templates (timeout and unspecified error) resulting in empty list of the available project templates.\n\nProjects are enabled for the users - as described here. The problem is with project creation.\n\nIs internet access (e.g. via NAT gateways) is needed for SageMaker projects?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1615480055000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":323.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Figured it out. SageMaker Studio projects need Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcyhpq1pxRTmtjkDRAh_MDA\/sage-maker-studio-projects-in-vpc-only-mode-without-internet-access",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1618085440000,
        "Question_original_content":"studio project vpconli mode internet access custom studio vpconli mode vpc protect subnet internet access nat gatewai function fine try creat project describ studio unabl list project templat timeout unspecifi error result list avail project templat project enabl user describ problem project creation internet access nat gatewai need project",
        "Question_preprocessed_content":"studio project vpconli mode internet access custom studio vpconli mode function fine try creat project describ studio unabl list project templat result list avail project templat project enabl user describ problem project creation internet access need project",
        "Question_gpt_summary_original":"The user is facing challenges in creating SageMaker projects in VpcOnly mode without internet access. Although all other functionalities are working fine, SageMaker Studio is unable to list project templates resulting in an empty list of available project templates. The user has enabled projects for the users, but the problem persists with project creation. The user is unsure if internet access is required for SageMaker projects.",
        "Question_gpt_summary":"user face challeng creat project vpconli mode internet access function work fine studio unabl list project templat result list avail project templat user enabl project user problem persist project creation user unsur internet access requir project",
        "Answer_original_content":"figur studio project need servic catalog access vpce com amazonaw aw region servicecatalog",
        "Answer_preprocessed_content":"figur studio project need servic catalog access vpce",
        "Answer_gpt_summary_original":"Solution: The user found a solution to the challenge by enabling Service Catalog access and VPCE for com.amazonaws.${AWS::Region}.servicecatalog. This resolved the issue with SageMaker Studio not being able to list project templates and allowed for project creation in VpcOnly mode without internet access.",
        "Answer_gpt_summary":"solut user solut challeng enabl servic catalog access vpce com amazonaw aw region servicecatalog resolv issu studio abl list project templat allow project creation vpconli mode internet access"
    },
    {
        "Question_title":"Is there a way to access compute quotas with the Azure CLI or Python SDK?",
        "Question_body":"<p>I want to tabulate the compute quotas for each Azure ML workspace, in each Azure location, for my organization's Azure subscription. Although it is possible to look at the quotas manually through the Azure Portal (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-quotas#workspace-level-quota\" rel=\"nofollow noreferrer\">link<\/a>), I have not found a way to do this with the Azure CLI or Python SDK for Azure. Since there are many resource groups and AML workspaces for different teams under my Azure subscription, it would be much more efficient to do this programmatically rather than manually through the portal. Is this even possible, and if so how can it be done?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1597248542537,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597249191852,
        "Question_score":2,
        "Question_view_count":418.0,
        "Poster_id":2434295.0,
        "Poster_created_time":1369863777596,
        "Poster_last_access_time":1653082051763,
        "Poster_age":null,
        "Poster_location":"Cambridge, MA",
        "Poster_reputation":335.0,
        "Poster_views":23.0,
        "Answer_body":"<p>It does look like these commands are currently in the CLI or the Python SDK. The CLI uses the Python SDK, so what's missing from one does tend to be missing from the other.<\/p>\n<p>Fortunately, you can invoke the rest endpoints directly, either in Python or by using the <code>az rest<\/code> command in the CLI.<\/p>\n<p>There are a few commands that may interest you:<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspacesandcomputes\/usages\/list\" rel=\"nofollow noreferrer\">Usage<\/a> and Quotas for a region:\n<code>\/subscriptions\/{subscriptionId}\/providers\/Microsoft.MachineLearningServices\/locations\/{location}\/usages?api-version=2019-05-01<\/code>\n<code>\/subscriptions\/{subscriptionId}\/providers\/Microsoft.MachineLearningServices\/locations\/{location}\/quotas?api-version=2020-04-01<\/code><\/p>\n<p>The process for updating REST specs to the offical documentation is fairly lengthy so it isn't published yet, but if you are willing to use Swagger docs to explore what is available, the 2020-06-01 version of the API is on Github, which includes endpoints for updating quotas as well as retrieving them: <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/resource-manager\/Microsoft.MachineLearningServices\/stable\/2020-06-01\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/resource-manager\/Microsoft.MachineLearningServices\/stable\/2020-06-01<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1597439016992,
        "Answer_score":1.0,
        "Answerer_id":11002318.0,
        "Answerer_created_time":1549041651583,
        "Answerer_last_access_time":1663950965043,
        "Answerer_age":null,
        "Answerer_location":"Atlanta, GA, USA",
        "Answerer_reputation":3106.0,
        "Answerer_views":428.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63380531",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1597341970283,
        "Question_original_content":"wai access comput quota azur cli python sdk want tabul comput quota workspac azur locat organ azur subscript possibl look quota manual azur portal link wai azur cli python sdk azur resourc group aml workspac differ team azur subscript effici programmat manual portal possibl",
        "Question_preprocessed_content":"wai access comput quota azur cli python sdk want tabul comput quota workspac azur locat organ azur subscript possibl look quota manual azur portal wai azur cli python sdk azur resourc group aml workspac differ team azur subscript effici programmat manual portal possibl",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing compute quotas for each Azure ML workspace in each Azure location for their organization's Azure subscription. Although it is possible to view the quotas manually through the Azure Portal, the user has not found a way to do this programmatically using the Azure CLI or Python SDK for Azure. The user is seeking a more efficient way to access this information.",
        "Question_gpt_summary":"user face challeng access comput quota workspac azur locat organ azur subscript possibl view quota manual azur portal user wai programmat azur cli python sdk azur user seek effici wai access inform",
        "Answer_original_content":"look like command current cli python sdk cli us python sdk miss tend miss fortun invok rest endpoint directli python rest command cli command usag quota region subscript subscriptionid provid microsoft machinelearningservic locat locat usag api version subscript subscriptionid provid microsoft machinelearningservic locat locat quota api version process updat rest spec offic document fairli lengthi isn publish will us swagger doc explor avail version api github includ endpoint updat quota retriev http github com azur azur rest api spec tree master specif machinelearningservic resourc manag microsoft machinelearningservic stabl",
        "Answer_preprocessed_content":"look like command current cli python sdk cli us python sdk miss tend miss fortun invok rest endpoint directli python command cli command usag quota region process updat rest spec offic document fairli lengthi isn publish will us swagger doc explor avail version api github includ endpoint updat quota retriev",
        "Answer_gpt_summary_original":"The discussion suggests that the user can access the compute quotas for each Azure ML workspace in each Azure location for their organization's Azure subscription by invoking the rest endpoints directly using Python or the \"az rest\" command in the CLI. The discussion also provides a few commands that may interest the user, including the Usage and Quotas for a region. Additionally, the discussion mentions that the 2020-06-01 version of the API on Github includes endpoints for updating quotas as well as retrieving them.",
        "Answer_gpt_summary":"discuss suggest user access comput quota workspac azur locat organ azur subscript invok rest endpoint directli python rest command cli discuss provid command user includ usag quota region addition discuss mention version api github includ endpoint updat quota retriev"
    },
    {
        "Question_title":"How to access\/invoke a sagemaker endpoint without lambda?",
        "Question_body":"<p>based on the aws documentation, maximum timeout limit is less that 30 seconds in api gateway.so hooking up an sagemaker endpoint with api gateway wouldn't make sense, if the request\/response is going to take more than 30 seconds. is there any workaround ? adding a lambda in between api gateway and sagemaker endpoint is going to add more time to process request\/response, which i would like to avoid. also, there will be added time for lambda cold starts and sagemaker serverless endpoints are built on top of lambda so that will also add cold start time. is there a way to invoke the serverless sagemaker endpoints , without these overhead?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1645755688313,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3,
        "Question_view_count":1122.0,
        "Poster_id":13645093.0,
        "Poster_created_time":1590797441983,
        "Poster_last_access_time":1664049080543,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":525.0,
        "Poster_views":98.0,
        "Answer_body":"<p>It is indeed possible to invoke sagemaker endpoints from sagemaker without using any other AWS services and that is also manifested by the fact that they have invocation URLs.<\/p>\n<p>Here's how you set it up:<\/p>\n<ol>\n<li>create an user with only programmatic access and attach a policy json that should look something like below:<\/li>\n<\/ol>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:&lt;region&gt;:&lt;account-id&gt;:endpoint\/&lt;endpoint-name&gt;&quot;\n        }\n    ]\n} \n<\/code><\/pre>\n<p>you can replace <code>&lt;endpoint-name&gt;<\/code> with <code>*<\/code> to let this user invoke all endpoints.<\/p>\n<ol start=\"2\">\n<li><p>use the ACCESS-KEY and SECRET-ACCESS-KEY to configure authorisation in postman like shown in this screenshot. also add the parameters in advanced tab like shown in the screenshot.\n<a href=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cYkTf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>then fill up your body with the relevant content type.<\/p>\n<\/li>\n<li><p>then add or remove additional headers like variant-name or model-name, if you have them set up and the headers should look like shown in this screenshot: <a href=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/NLqkV.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<li><p>send the request to receive reponse like this\n<a href=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uA4kF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<\/li>\n<\/ol>\n<p><em><strong>URL and credentials in the above screenshots doesn't work anymore, duh!<\/strong><\/em><\/p>\n<p>and if you want code to invoke the endpoint directly using some back-end language, <a href=\"https:\/\/stackoverflow.com\/a\/70803026\/11814996\">here's code for python<\/a>.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":1649698499116,
        "Answer_score":3.0,
        "Answerer_id":11814996.0,
        "Answerer_created_time":1559910246180,
        "Answerer_last_access_time":1664039951323,
        "Answerer_age":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation":2046.0,
        "Answerer_views":369.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71260306",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1645795642143,
        "Question_original_content":"access invok endpoint lambda base aw document maximum timeout limit second api gatewai hook endpoint api gatewai wouldn sens request respons go second workaround ad lambda api gatewai endpoint go add time process request respons like avoid ad time lambda cold start serverless endpoint built lambda add cold start time wai invok serverless endpoint overhead",
        "Question_preprocessed_content":"endpoint lambda base aw document maximum timeout limit second api hook endpoint api gatewai wouldn sens go second workaround ad lambda api gatewai endpoint go add time process like avoid ad time lambda cold start serverless endpoint built lambda add cold start time wai invok serverless endpoint overhead",
        "Question_gpt_summary_original":"The user is facing challenges in accessing\/invoking a sagemaker endpoint without using lambda. The maximum timeout limit in API Gateway is less than 30 seconds, which makes it impractical to hook up an endpoint with API Gateway if the request\/response takes longer. Adding a lambda in between would add more time to process request\/response, including cold start time. The user is looking for a way to invoke serverless sagemaker endpoints without these overheads.",
        "Question_gpt_summary":"user face challeng access invok endpoint lambda maximum timeout limit api gatewai second make impract hook endpoint api gatewai request respons take longer ad lambda add time process request respons includ cold start time user look wai invok serverless endpoint overhead",
        "Answer_original_content":"possibl invok endpoint aw servic manifest fact invoc url set creat user programmat access attach polici json look like version statement sid visualeditor effect allow action invokeendpoint resourc arn aw endpoint replac let user invok endpoint us access kei secret access kei configur authoris postman like shown screenshot add paramet advanc tab like shown screenshot bodi relev content type add remov addit header like variant model set header look like shown screenshot send request receiv repons like url credenti screenshot work anymor duh want code invok endpoint directli end languag code python",
        "Answer_preprocessed_content":"possibl invok endpoint aw servic manifest fact invoc url set creat user programmat access attach polici json look like replac let user invok endpoint us configur authoris postman like shown screenshot add paramet advanc tab like shown screenshot bodi relev content type add remov addit header like set header look like shown screenshot send request receiv repons like url credenti screenshot work anymor duh want code invok endpoint directli languag code python",
        "Answer_gpt_summary_original":"Solution:\nIt is possible to invoke sagemaker endpoints from sagemaker without using any other AWS services. To set it up, create a user with only programmatic access and attach a policy json. Then use the ACCESS-KEY and SECRET-ACCESS-KEY to configure authorization in postman. Fill up the body with the relevant content type and add or remove additional headers like variant-name or model-name. Finally, send the request to receive a response. If you want code to invoke the endpoint directly using some back-end language, there is code available for python.",
        "Answer_gpt_summary":"solut possibl invok endpoint aw servic set creat user programmat access attach polici json us access kei secret access kei configur author postman bodi relev content type add remov addit header like variant model final send request receiv respons want code invok endpoint directli end languag code avail python"
    },
    {
        "Question_title":"AWS sagemaker RandomCutForest (RCF) vs scikit lean RandomForest (RF)?",
        "Question_body":"<p>Is there a difference between the two, or are they different names for the same algorithm?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1561329917317,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":244.0,
        "Poster_id":4458718.0,
        "Poster_created_time":1421343783700,
        "Poster_last_access_time":1661295265603,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1387.0,
        "Poster_views":153.0,
        "Answer_body":"<p>RandomCutForest (RCF) is an unsupervised method primarily used for anomaly detection, while RandomForest (RF) is a supervised method that can be used for regression or classification. <\/p>\n\n<p>For RCF, see documentation (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"nofollow noreferrer\">here<\/a>) and notebook example (<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/random_cut_forest\/random_cut_forest.ipynb\" rel=\"nofollow noreferrer\">here<\/a>)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":11025360.0,
        "Answerer_created_time":1549488907923,
        "Answerer_last_access_time":1624824671527,
        "Answerer_age":null,
        "Answerer_location":"USA",
        "Answerer_reputation":41.0,
        "Answerer_views":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56728230",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1561385097928,
        "Question_original_content":"randomcutforest rcf scikit lean randomforest differ differ name algorithm",
        "Question_preprocessed_content":"randomcutforest scikit lean randomforest differ differ name algorithm",
        "Question_gpt_summary_original":"The user is seeking clarification on whether AWS Sagemaker RandomCutForest (RCF) and scikit learn RandomForest (RF) are the same algorithm or if there are differences between the two.",
        "Question_gpt_summary":"user seek clarif randomcutforest rcf scikit learn randomforest algorithm differ",
        "Answer_original_content":"randomcutforest rcf unsupervis method primarili anomali detect randomforest supervis method regress classif rcf document notebook exampl",
        "Answer_preprocessed_content":"randomcutforest unsupervis method primarili anomali detect randomforest supervis method regress classif rcf document notebook exampl",
        "Answer_gpt_summary_original":"There are differences between AWS Sagemaker RandomCutForest (RCF) and scikit learn RandomForest (RF). RCF is an unsupervised method primarily used for anomaly detection, while RF is a supervised method that can be used for regression or classification. Documentation and notebook examples for RCF can be found on the AWS Sagemaker website. No solutions were provided as the user was seeking clarification on the differences between the two algorithms.",
        "Answer_gpt_summary":"differ randomcutforest rcf scikit learn randomforest rcf unsupervis method primarili anomali detect supervis method regress classif document notebook exampl rcf websit solut provid user seek clarif differ algorithm"
    },
    {
        "Question_title":"Rerun a deleted run in wandb sweep",
        "Question_body":"<p>Hi,<\/p>\n<p>Assuming I have a sweep of runs. For some reason, I wanna rerun a few of the runs. So I go ahead and delete those runs in the dashboard. But then even if I rerun the command (<code>wandb agent ...<\/code>), wandb is not able to rerun those runs. It will show all runs have been completed. Could wandb add the feature to rerun the runs that are not in the dashboards (for example, those that are deleted)?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1676042396664,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":29.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/taochen\">@taochen<\/a>, rerunning deleted runs of a sweep is supported for grid search only. Please see the <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/faq#can-i-rerun-a-grid-search\">following guide<\/a> on the steps to take to execute correctly. If you find that does not work for you, provide a link to your workspace and we\u2019ll take a closer look.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/rerun-a-deleted-run-in-wandb-sweep\/3860",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1676070775536,
        "Question_original_content":"rerun delet run sweep assum sweep run reason wanna rerun run ahead delet run dashboard rerun command agent abl rerun run run complet add featur rerun run dashboard exampl delet",
        "Question_preprocessed_content":"rerun delet run sweep assum sweep run reason wanna rerun run ahead delet run dashboard rerun command abl rerun run run complet add featur rerun run dashboard",
        "Question_gpt_summary_original":"The user encountered a challenge with rerunning deleted runs in a wandb sweep. After deleting some runs in the dashboard, the user was unable to rerun those runs using the wandb agent command as wandb showed all runs as completed. The user suggests adding a feature to rerun deleted runs.",
        "Question_gpt_summary":"user encount challeng rerun delet run sweep delet run dashboard user unabl rerun run agent command show run complet user suggest ad featur rerun delet run",
        "Answer_original_content":"taochen rerun delet run sweep support grid search follow guid step execut correctli work provid link workspac closer look",
        "Answer_preprocessed_content":"rerun delet run sweep support grid search follow guid step execut correctli work provid link workspac closer look",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that rerunning deleted runs of a sweep is supported for grid search only. The user is directed to follow the steps provided in the guide on how to execute it correctly. If the suggested solution does not work, the user is advised to provide a link to their workspace for further assistance.",
        "Answer_gpt_summary":"solut discuss suggest rerun delet run sweep support grid search user direct follow step provid guid execut correctli suggest solut work user advis provid link workspac assist"
    },
    {
        "Question_title":"Azure Batch API rising 'AttributeError' in ML notebook",
        "Question_body":"<p>I am trying to interact with Azure Batch with python API, in the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azure.batch import BatchServiceClient\nbatch = BatchServiceClient('&lt;mycredential&gt;','https:\/\/&lt;mybatchaccount&gt;.&lt;region&gt;.batch.azure.com')\nnext(batch.job.list())\n<\/code><\/pre>\n<p>This is run in a ML Studio notebook.<\/p>\n<p>However the following error appears: <code>AttributeError: 'str' object has no attribute 'signed_session'<\/code>.<br \/>\nI am taking the url and credentials from my batch console UI:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lc9n4m.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lc9n4m.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As a credential I tried both Primary and Secondary access keys amd &quot;URL&quot; as batch url.<br \/>\nAm I doing anything wrong?<br \/>\nThanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1622712274873,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1622714028187,
        "Question_score":0,
        "Question_view_count":52.0,
        "Poster_id":5080195.0,
        "Poster_created_time":1436006890427,
        "Poster_last_access_time":1663945829127,
        "Poster_age":null,
        "Poster_location":"Amsterdam, Paesi Bassi",
        "Poster_reputation":959.0,
        "Poster_views":204.0,
        "Answer_body":"<p><code>&lt;mycredential&gt;<\/code> should not be your bare auth key string. You need to create a shared auth key object.<\/p>\n<pre><code>credentials = batchauth.SharedKeyCredentials(BATCH_ACCOUNT_NAME, BATCH_ACCOUNT_KEY)\nbatch_client = batch.BatchServiceClient(credentials, base_url=BATCH_ACCOUNT_URL)\n<\/code><\/pre>\n<p>Please see the <a href=\"https:\/\/docs.microsoft.com\/azure\/batch\/tutorial-parallel-python\" rel=\"nofollow noreferrer\">Azure Batch Python tutorial<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4896852.0,
        "Answerer_created_time":1431537216680,
        "Answerer_last_access_time":1663810198763,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":2243.0,
        "Answerer_views":231.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67818831",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1622734661128,
        "Question_original_content":"azur batch api rise attributeerror notebook try interact azur batch python api follow wai azur batch import batchservicecli batch batchservicecli http batch azur com batch job list run studio notebook follow error appear attributeerror str object attribut sign session take url credenti batch consol credenti tri primari secondari access kei amd url batch url wrong thank",
        "Question_preprocessed_content":"azur batch api rise attributeerror notebook try interact azur batch python api follow wai run studio notebook follow error appear take url credenti batch consol credenti tri primari secondari access kei amd url batch url wrong thank",
        "Question_gpt_summary_original":"The user is encountering an 'AttributeError' while trying to interact with Azure Batch using the Python API in an ML Studio notebook. The error message states that a 'str' object has no attribute 'signed_session'. The user is using the URL and credentials from their batch console UI, including both Primary and Secondary access keys and 'URL' as the batch URL. The user is seeking assistance in identifying the issue.",
        "Question_gpt_summary":"user encount attributeerror try interact azur batch python api studio notebook error messag state str object attribut sign session user url credenti batch consol includ primari secondari access kei url batch url user seek assist identifi issu",
        "Answer_original_content":"bare auth kei string need creat share auth kei object credenti batchauth sharedkeycredenti batch account batch account kei batch client batch batchservicecli credenti base url batch account url azur batch python tutori",
        "Answer_preprocessed_content":"bare auth kei string need creat share auth kei object azur batch python tutori",
        "Answer_gpt_summary_original":"Solution: The user needs to create a shared auth key object using the credentials and base URL. They can use the following code to create the object:\n\n```\ncredentials = batchauth.SharedKeyCredentials(BATCH_ACCOUNT_NAME, BATCH_ACCOUNT_KEY)\nbatch_client = batch.BatchServiceClient(credentials, base_url=BATCH_ACCOUNT_URL)\n```\n\nThe user can refer to the Azure Batch Python tutorial for more information.",
        "Answer_gpt_summary":"solut user need creat share auth kei object credenti base url us follow code creat object credenti batchauth sharedkeycredenti batch account batch account kei batch client batch batchservicecli credenti base url batch account url user refer azur batch python tutori inform"
    },
    {
        "Question_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Question_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1592577511000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":32.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: [1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2] [2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow. For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and [Using Amazon SageMaker to build a machine learning platform with just three engineers][4]. [3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/ [4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1592579742000,
        "Question_original_content":"share success stori aw custom perform want creat simpl templat scientist fit model easili continu integr continu deliveri pipelin want know success stori aw custom perform machin learn pipelin",
        "Question_preprocessed_content":"share success stori aw custom perform want creat simpl templat scientist fit model easili continu deliveri pipelin want know success stori aw custom perform machin learn pipelin",
        "Question_gpt_summary_original":"The user is seeking success stories of AWS customers who have implemented continuous integration\/continuous delivery (CI\/CD) on machine learning pipelines. They aim to create simple templates for scientists to easily fit their models into a CI\/CD pipeline.",
        "Question_gpt_summary":"user seek success stori aw custom implement continu integr continu deliveri machin learn pipelin aim creat simpl templat scientist easili fit model pipelin",
        "Answer_original_content":"amazon releas pipelin purpos built servic machin learn http aw amazon com pipelin inform new pipelin bring devop capabl machin learn project http aw amazon com blog aw amazon pipelin bring devop machin learn project addition case studi custom creat model develop airflow inform nerdwallet us machin learn aw power recommend platform build machin learn platform engin http aw amazon com solut case studi nerdwallet case studi http nerdwallet com blog engin machin learn platform amazon",
        "Answer_preprocessed_content":"amazon releas servic machin learn inform addition custom creat model develop airflow inform",
        "Answer_gpt_summary_original":"Solution: Amazon has released the Amazon SageMaker Pipelines, which is the first purpose-built CI\/CD service for machine learning. There is also a case-study where a customer created their own CI\/CD pipeline for model development using Airflow.",
        "Answer_gpt_summary":"solut amazon releas pipelin purpos built servic machin learn case studi custom creat pipelin model develop airflow"
    },
    {
        "Question_title":"Importing a file into jupyterlabs from s3",
        "Question_body":"<p>I have a file I want to import into a Sagemaker Jupyter notebook python 3 instance for use.  The exact code would be 'import lstm.'  I can store the file in s3 (which would probably be ideal) or locally, whichever you prefer.  I have been searching the internet for a while and have been unable to find a solution to this.  I am actually just trying to run\/understand this code from Suraj Raval's youtube channel: <a href=\"https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot\" rel=\"nofollow noreferrer\">https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot<\/a>.  The 'import lstm' line is failing when I run, and I am trying to figure out how to make this work.  <\/p>\n\n<p>I have tried:\nfrom s3:\/\/... import lstm.  failed\nI have tried some boto3 methods and wasn't able to get it to work.  <\/p>\n\n<pre><code>import time\nimport threading\nimport lstm, etl, json. ##this line\nimport numpy as np\nimport pandas as pd\nimport h5py\nimport matplotlib.pyplot as plt\nconfigs = json.loads(open('configs.json').read())\ntstart = time.time()\n<\/code><\/pre>\n\n<p>I would just like to be able to import the lstm file and all the others into a Jupyter notebook instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1555242786160,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3,
        "Question_view_count":1863.0,
        "Poster_id":11358899.0,
        "Poster_created_time":1555241852167,
        "Poster_last_access_time":1635370371112,
        "Poster_age":null,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation":393.0,
        "Poster_views":81.0,
        "Answer_body":"<p>I think you should  be cloning the Github repo in SageMaker instance and not importing the files from S3. I was able to reproduce the Bitcoin Trading Bot notebook from SageMaker by cloning it. You can follow the below steps<\/p>\n\n<h3>Cloning Github Repo to SageMaker Notebook<\/h3>\n\n<ol>\n<li>Open JupyterLab from the AWS SageMaker console.<\/li>\n<li>From the JupyterLab  Launcher, open the Terminal.<\/li>\n<li>Change directory to SageMaker<\/li>\n<\/ol>\n\n<pre><code>cd ~\/SageMaker\n<\/code><\/pre>\n\n<ol start=\"4\">\n<li>Clone the BitCoin Trading Bot <a href=\"https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot\" rel=\"nofollow noreferrer\">git repo<\/a><\/li>\n<\/ol>\n\n<pre><code>git clone https:\/\/github.com\/llSourcell\/Bitcoin_Trading_Bot.git\ncd Bitcoin_Trading_Bot\n<\/code><\/pre>\n\n<ol start=\"5\">\n<li>Now you can open the notebook <code>Bitcoin LSTM Prediction.ipynb<\/code> and select the Tensorflow Kernel to run the notebook.<\/li>\n<\/ol>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/YbKic.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YbKic.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<h3>Adding files from local machine to SageMaker Notebook<\/h3>\n\n<p>To add files from your local machine to SageMaker Notebook instance, you can use <a href=\"https:\/\/jupyterlab.readthedocs.io\/en\/stable\/user\/files.html\" rel=\"nofollow noreferrer\">file upload<\/a> functionality in JupyterLab<\/p>\n\n<h3>Adding files from S3 to SageMaker Notebook<\/h3>\n\n<p>To add files from S3 to SageMaker Notebook instance, use AWS CLI or Python SDK to upload\/download files. <\/p>\n\n<p>For example, to download <code>lstm.py<\/code> file from S3 to SageMaker using AWS CLI<\/p>\n\n<pre><code>aws s3 cp s3:\/\/mybucket\/bot\/src\/lstm.py .\n<\/code><\/pre>\n\n<p>Using <code>boto3<\/code> API<\/p>\n\n<pre><code>import boto3\ns3 = boto3.resource('s3')\ns3.meta.client.download_file('mybucket', 'bot\/src\/lstm.py', '.\/lstm.py')\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1555346571920,
        "Answer_score":4.0,
        "Answerer_id":6069517.0,
        "Answerer_created_time":1458100127643,
        "Answerer_last_access_time":1663950101183,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1083.0,
        "Answerer_views":86.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55674959",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1555252725128,
        "Question_original_content":"import file jupyterlab file want import jupyt notebook python instanc us exact code import lstm store file probabl ideal local whichev prefer search internet unabl solut actual try run understand code suraj raval youtub channel http github com llsourcel bitcoin trade bot import lstm line fail run try figur work tri import lstm fail tri boto method wasn abl work import time import thread import lstm etl json line import numpi import panda import hpy import matplotlib pyplot plt config json load open config json read tstart time time like abl import lstm file jupyt notebook instanc",
        "Question_preprocessed_content":"import file jupyterlab file want import jupyt notebook python instanc us exact code import store file local whichev prefer search internet unabl solut actual try code suraj raval youtub channel import lstm line fail run try figur work tri import lstm fail tri boto method wasn abl work like abl import lstm file jupyt notebook instanc",
        "Question_gpt_summary_original":"The user is facing challenges in importing a file named 'lstm' into a Sagemaker Jupyter notebook python 3 instance for use. The user has tried importing the file from s3 and using boto3 methods, but both have failed. The 'import lstm' line is failing when the user runs the code, and the user is trying to figure out how to make it work. The user wants to import the lstm file and all the others into a Jupyter notebook instance.",
        "Question_gpt_summary":"user face challeng import file name lstm jupyt notebook python instanc us user tri import file boto method fail import lstm line fail user run code user try figur work user want import lstm file jupyt notebook instanc",
        "Answer_original_content":"think clone github repo instanc import file abl reproduc bitcoin trade bot notebook clone follow step clone github repo notebook open jupyterlab consol jupyterlab launcher open termin chang directori clone bitcoin trade bot git repo git clone http github com llsourcel bitcoin trade bot git bitcoin trade bot open notebook bitcoin lstm predict ipynb select tensorflow kernel run notebook ad file local machin notebook add file local machin notebook instanc us file upload function jupyterlab ad file notebook add file notebook instanc us aw cli python sdk upload download file exampl download lstm file aw cli aw mybucket bot src lstm boto api import boto boto resourc meta client download file mybucket bot src lstm lstm",
        "Answer_preprocessed_content":"think clone github repo instanc import file abl reproduc bitcoin trade bot notebook clone follow step clone github repo notebook open jupyterlab consol jupyterlab launcher open termin chang directori clone bitcoin trade bot git repo open notebook select tensorflow kernel run notebook ad file local machin notebook add file local machin notebook instanc us file upload function jupyterlab ad file notebook add file notebook instanc us aw cli python sdk file exampl download file aw cli api",
        "Answer_gpt_summary_original":"The solution suggested in the discussion is to clone the Github repo in SageMaker instance instead of importing the files from S3. The user can follow the steps mentioned in the discussion to clone the BitCoin Trading Bot git repo and open the notebook Bitcoin LSTM Prediction.ipynb. The user can also add files from their local machine to SageMaker Notebook instance using file upload functionality in JupyterLab or use AWS CLI or Python SDK to upload\/download files from S3.",
        "Answer_gpt_summary":"solut suggest discuss clone github repo instanc instead import file user follow step mention discuss clone bitcoin trade bot git repo open notebook bitcoin lstm predict ipynb user add file local machin notebook instanc file upload function jupyterlab us aw cli python sdk upload download file"
    },
    {
        "Question_title":"pip install azureml-sdk with latest patches to underlying libraries",
        "Question_body":"<p>How do I upgrade <code>azureml-sdk<\/code> such that the newest release of <code>azureml-core<\/code>, <code>1.1.5.5<\/code>, is installed? \nIf <code>azureml-sdk<\/code> is not installed, <code>pip install --upgrade azureml-sdk<\/code> will install <code>azureml-core==1.1.5.5<\/code>. If it is already installed, then it won't.<\/p>\n\n<pre><code>$ pip list --format=freeze | grep 'azureml-core'`\n&gt; azureml-core==1.1.5.1\n$ pip install --upgrade azureml-sdk[interpret,notebooks]\n$ pip list --format=freeze | grep 'azureml-core'`\n&gt; azureml-core==1.1.5.1\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1584723850063,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1584724828660,
        "Question_score":0,
        "Question_view_count":1142.0,
        "Poster_id":3842610.0,
        "Poster_created_time":1405457120427,
        "Poster_last_access_time":1663947733100,
        "Poster_age":null,
        "Poster_location":"Seattle, WA, USA",
        "Poster_reputation":3359.0,
        "Poster_views":555.0,
        "Answer_body":"<p>You can use the eager strategy to force an upgrade of requirements:<\/p>\n\n<pre><code>pip install -U --upgrade-strategy eager azureml-sdk\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":5784983.0,
        "Answerer_created_time":1452696930640,
        "Answerer_last_access_time":1603992113372,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":746.0,
        "Answerer_views":112.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60778546",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1584735835707,
        "Question_original_content":"pip instal sdk latest patch underli librari upgrad sdk newest releas core instal sdk instal pip instal upgrad sdk instal core instal won pip list format freez grep core core pip instal upgrad sdk interpret notebook pip list format freez grep core core",
        "Question_preprocessed_content":"pip instal sdk latest patch underli librari upgrad newest releas instal instal instal instal won",
        "Question_gpt_summary_original":"The user is facing a challenge in upgrading the azureml-sdk to the latest release of azureml-core, version 1.1.5.5. The user has tried upgrading using pip install --upgrade azureml-sdk[interpret,notebooks], but the upgrade did not install the latest version of azureml-core.",
        "Question_gpt_summary":"user face challeng upgrad sdk latest releas core version user tri upgrad pip instal upgrad sdk interpret notebook upgrad instal latest version core",
        "Answer_original_content":"us eager strategi forc upgrad requir pip instal upgrad strategi eager sdk",
        "Answer_preprocessed_content":"us eager strategi forc upgrad requir",
        "Answer_gpt_summary_original":"Solution: The user can use the eager strategy to force an upgrade of requirements by running the command \"pip install -U --upgrade-strategy eager azureml-sdk\".",
        "Answer_gpt_summary":"solut user us eager strategi forc upgrad requir run command pip instal upgrad strategi eager sdk"
    },
    {
        "Question_title":"Remotely execute ClearML task using local-only repo",
        "Question_body":"<p>I want to execute ClearML task remotely. According to docs there are 2 options: 1) execute single python file; 2) ClearML would identify that script is part of repo, that repo will be cloned and installed into docker and executed on the worker.<\/p>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<p>I think it is somewhat extending scenario 1, where not a single file is passed for execution but whole directory with file in it.<\/p>\n<p>PS: i understand reproducibility concerns that arise, but repo is really not accessible from worker :(<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1653499480970,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":25.0,
        "Poster_id":19200449.0,
        "Poster_created_time":1653498830776,
        "Poster_last_access_time":1662113390107,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":3.0,
        "Poster_views":0.0,
        "Answer_body":"<p>Disclaimer: I'm a team members of ClearML<\/p>\n<blockquote>\n<p>In this second scenario it is assumed that repo has remote url and it is accessible by worker. What if it isn't the case? Is it possible to somehow pack the local repo and send it for remote execution.<\/p>\n<\/blockquote>\n<p>well, no :( if your code is a single script, then yes ClearML would store the entire script, then the worker will reproduce it on the remote machine. But if your code base is composed of more than a single file, then why not use git? it is free hosted by GitHub, Bitbucket, GitLab etc.<\/p>\n<p>In theory this is doable and if you feel the need, I urge you to PR this feature. Basically you would store the entire folder as an artifact (ClearML will auto zip it for you), then the agent needs to unzip the artifact and run it. The main issue would be that cloning the Task will not clone the artifact...<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":11682840.0,
        "Answerer_created_time":1561143508792,
        "Answerer_last_access_time":1659992319200,
        "Answerer_age":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation":489.0,
        "Answerer_views":60.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72381916",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659992618003,
        "Question_original_content":"remot execut task local repo want execut task remot accord doc option execut singl python file identifi script repo repo clone instal docker execut worker second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut think somewhat extend scenario singl file pass execut directori file understand reproduc concern aris repo access worker thank advanc",
        "Question_preprocessed_content":"remot execut task repo want execut task remot accord doc option execut singl python file identifi script repo repo clone instal docker execut worker second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut think somewhat extend scenario singl file pass execut directori file understand reproduc concern aris repo access worker thank advanc",
        "Question_gpt_summary_original":"The user wants to execute a ClearML task remotely, but faces a challenge when the repo is not accessible by the worker. The two options provided by ClearML are to execute a single python file or to clone and install the repo into docker and execute it on the worker. However, the user wonders if it is possible to pack the local repo and send it for remote execution, similar to passing a directory with a file for execution. The user acknowledges the reproducibility concerns but states that the repo is not accessible from the worker.",
        "Question_gpt_summary":"user want execut task remot face challeng repo access worker option provid execut singl python file clone instal repo docker execut worker user wonder possibl pack local repo send remot execut similar pass directori file execut user acknowledg reproduc concern state repo access worker",
        "Answer_original_content":"disclaim team member second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut code singl script ye store entir script worker reproduc remot machin code base compos singl file us git free host github bitbucket gitlab theori doabl feel need urg featur basic store entir folder artifact auto zip agent need unzip artifact run main issu clone task clone artifact",
        "Answer_preprocessed_content":"disclaim team member second scenario assum repo remot url access worker isn case possibl pack local repo send remot execut code singl script ye store entir script worker reproduc remot machin code base compos singl file us git free host github bitbucket gitlab theori doabl feel need urg featur basic store entir folder artifact agent need unzip artifact run main issu clone task clone",
        "Answer_gpt_summary_original":"Solution: The discussion mentions two possible solutions. The first solution is to execute a single python file or to clone and install the repo into docker and execute it on the worker, which are the options provided by ClearML. The second solution is to use git to store the entire code base, which is free and hosted by GitHub, Bitbucket, GitLab, etc. However, if the user wants to pack the local repo and send it for remote execution, it is not currently possible with ClearML. Although, in theory, it is doable and the user can submit a feature request to ClearML to store the entire folder as an artifact, which will be auto-zipped by ClearML, and then the agent needs to unzip the artifact and",
        "Answer_gpt_summary":"solut discuss mention possibl solut solut execut singl python file clone instal repo docker execut worker option provid second solut us git store entir code base free host github bitbucket gitlab user want pack local repo send remot execut current possibl theori doabl user submit featur request store entir folder artifact auto zip agent need unzip artifact"
    },
    {
        "Question_title":"Can I use AWS Sagemaker without S3",
        "Question_body":"<p>If I am not using the notebook on AWS but instead just the Sagemaker CLI and want to train a model, can I specify a local path to read from and write to?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1530312595250,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":826.0,
        "Poster_id":9916433.0,
        "Poster_created_time":1528500562963,
        "Poster_last_access_time":1635459100792,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":13.0,
        "Poster_views":2.0,
        "Answer_body":"<p>If you use local mode with the SageMaker Python SDK, you can train using local data:<\/p>\n\n<pre><code>from sagemaker.mxnet import MXNet\n\nmxnet_estimator = MXNet('train.py',\n                        train_instance_type='local',\n                        train_instance_count=1)\n\nmxnet_estimator.fit('file:\/\/\/tmp\/my_training_data')\n<\/code><\/pre>\n\n<p>However, this only works if you are training a model locally, not on SageMaker. If you want to train on SageMaker, then yes, you do need to use S3.<\/p>\n\n<p>For more about local mode: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Answerer_id":9074534.0,
        "Answerer_created_time":1512770138847,
        "Answerer_last_access_time":1662131461223,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":493.0,
        "Answerer_views":47.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51110274",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1530570724340,
        "Question_original_content":"us notebook aw instead cli want train model specifi local path read write",
        "Question_preprocessed_content":"us notebook aw instead cli want train model specifi local path read write",
        "Question_gpt_summary_original":"The user is facing a challenge of whether they can use AWS Sagemaker without S3 by specifying a local path to read from and write to while training a model using the Sagemaker CLI instead of the notebook on AWS.",
        "Question_gpt_summary":"user face challeng us specifi local path read write train model cli instead notebook aw",
        "Answer_original_content":"us local mode python sdk train local data mxnet import mxnet mxnet estim mxnet train train instanc type local train instanc count mxnet estim fit file tmp train data work train model local want train ye need us local mode http github com aw python sdk local mode",
        "Answer_preprocessed_content":"us local mode python sdk train local data work train model local want train ye need us local mode",
        "Answer_gpt_summary_original":"Solution: The user can use local mode with the SageMaker Python SDK to train using local data by specifying a local path to read from and write to. However, this only works if the user is training a model locally, not on SageMaker. If the user wants to train on SageMaker, then they need to use S3.",
        "Answer_gpt_summary":"solut user us local mode python sdk train local data specifi local path read write work user train model local user want train need us"
    },
    {
        "Question_title":".txt altered after save leads to CSV reader seeing too many fields",
        "Question_body":"<p>I am running a <code>JupyterLab<\/code> on <code>AWS SageMaker<\/code>. Kernel: <code>conda_amazonei_mxnet_p27<\/code><\/p>\n<p>The number of fields found: <code>saw 9<\/code> increments by 1, each run.<\/p>\n<p><strong>Error:<\/strong> <code>ParserError: Error tokenizing data. C error: Expected 2 fields in line 50, saw 9<\/code><\/p>\n<hr \/>\n<h3>Code:<\/h3>\n<p>Invocation (Error doesn't appear when running all cells before this but does when this is ran):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train = open('train_textcorrupted.csv', 'a')\nval = open('val.csv', 'a')\nclasses = open('classes.txt', 'a')\nuni_label = 'Organisation\\tUniversity'\nn_pad = 4\nfor i in range(len(unis)-n_pad):\n    record = ' '.join(unis[i:(i+n_pad)])\n    full_record = f'{uni_label}\\t{record}\\n'\n    if random.random() &gt; 0.9:\n        val.write(full_record)\n    else:\n        train.write(full_record) \n\nclasses.write(uni_label)\nclasses.close() \nval.close()\ntrain.close()                      \n<\/code><\/pre>\n<h3>Traceback:<\/h3>\n<pre class=\"lang-sh prettyprint-override\"><code>---------------------------------------------------------------------------\nParserError                               Traceback (most recent call last)\n&lt;ipython-input-8-89b1728bd5a6&gt; in &lt;module&gt;\n      7       --gpus 1\n      8     &quot;&quot;&quot;.split()\n----&gt; 9 run_training(args)\n&lt;ipython-input-5-091daf2638a1&gt; in run_training(input)\n     55     csv_logger = pl.loggers.CSVLogger(save_dir=f'{args.modeldir}\/csv_logs')\n     56     loggers = [logger, csv_logger]\n---&gt; 57     dm = OntologyTaggerDataModule.from_argparse_args(args)\n     58     if args.model_uri:\n     59         local_model_uri = os.environ.get('SM_CHANNEL_MODEL', '.')\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pytorch_lightning\/core\/datamodule.py in from_argparse_args(cls, args, **kwargs)\n    324         datamodule_kwargs.update(**kwargs)\n    325 \n--&gt; 326         return cls(**datamodule_kwargs)\n    327 \n    328     @classmethod\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pytorch_lightning\/core\/datamodule.py in __call__(cls, *args, **kwargs)\n     47 \n     48         # Get instance of LightningDataModule by mocking its __init__ via __call__\n---&gt; 49         obj = type.__call__(cls, *args, **kwargs)\n     50 \n     51         return obj\n&lt;ipython-input-3-66ee2be72e78&gt; in __init__(self, traindir, train_file, validate_file, model_name, labels, batch_size)\n     30         print('tokenizer', tokenizer)\n     31         print('labels_file', labels_file)\n---&gt; 32         label_mapper = LabelMapper(labels_file)\n     33         self.batch_size = batch_size\n     34         self.num_classes = label_mapper.num_classes\n&lt;ipython-input-3-66ee2be72e78&gt; in __init__(self, classes_file)\n    102 \n    103     def __init__(self, classes_file):\n--&gt; 104         self._raw_labels = pd.read_csv(classes_file, header=None, sep='\\t')\n    105 \n    106         self._map = []\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\n    686     )\n    687 \n--&gt; 688     return _read(filepath_or_buffer, kwds)\n    689 \n    690 \n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in _read(filepath_or_buffer, kwds)\n    458 \n    459     try:\n--&gt; 460         data = parser.read(nrows)\n    461     finally:\n    462         parser.close()\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   1196     def read(self, nrows=None):\n   1197         nrows = _validate_integer(&quot;nrows&quot;, nrows)\n-&gt; 1198         ret = self._engine.read(nrows)\n   1199 \n   1200         # May alter columns \/ col_dict\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   2155     def read(self, nrows=None):\n   2156         try:\n-&gt; 2157             data = self._reader.read(nrows)\n   2158         except StopIteration:\n   2159             if self._first_chunk:\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader.read()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.raise_parser_error()\nParserError: Error tokenizing data. C error: Expected 2 fields in line 50, saw 9\n<\/code><\/pre>\n<hr \/>\n<p><code>classes.txt<\/code> (tab-separated) Before runtime<\/p>\n<pre><code>Activity    Event\nActor   Person\nAgent   Person\nAlbum   Product\nAnimal  Object\nArchitecturalStructure  Location\nArtist  Person\nAthlete Person\nAutomobileEngine    Product\nAward   Object\nBiomolecule Object\nBird    Object\nBodyOfWater Location\nBuilding    Location\nChemicalSubstance   Object\nCompany Organisation\nCompetition Event\nDevice  Product\nDisease Object\nDistrict    Location\nEukaryote   Object\nEvent   Event\nFilm    Object\nFood    Object\nLanguage    Object\nLocation    Location\nMeanOfTransportation    Product\nMotorsportSeason    Event\nMunicipality    Location\nMusicalWork Product\nOrganisation    Organisation\nPainter Person\nPeriodicalLiterature    Product\nPerson  Person\nPersonFunction  Person\nPlant   Object\nPoet    Person\nPolitician  Person\nRiver   Location\nSchool  Organisation\nSettlement  Location\nSoftware    Product\nSong    Product\nSpecies Object\nSportsSeason    Event\nStation Location\nTown    Location\nVillage Location\nWriter  Person\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_created_time":1630938895957,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1631000955160,
        "Question_score":0,
        "Question_view_count":87.0,
        "Poster_id":16105404.0,
        "Poster_created_time":1622632545867,
        "Poster_last_access_time":1646839538183,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1.0,
        "Poster_views":111.0,
        "Answer_body":"<p>Problem Found:<\/p>\n<p>So no fault of my own, I keep ensuring these fields are on their own lines in <code>classes.txt<\/code> and <code>Ctrl+S<\/code>. Then when I reopen the file, <strong>after runtime<\/strong>, it'll have fields be on the same line again.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UbRTm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UbRTm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To fix this, on line <code>classes.write(uni_label)<\/code>.<\/p>\n<p>I replaced it with <code>classes.write('\\n'+uni_label)<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1631001042847,
        "Answer_score":1.0,
        "Answerer_id":16105404.0,
        "Answerer_created_time":1622632545867,
        "Answerer_last_access_time":1646839538183,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1.0,
        "Answerer_views":111.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69076270",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1631000840376,
        "Question_original_content":"txt alter save lead csv reader see field run jupyterlab kernel conda amazonei mxnet number field saw increment run error parsererror error token data error expect field line saw code invoc error appear run cell ran train open train textcorrupt csv val open val csv class open class txt uni label organis tunivers pad rang len uni pad record join uni pad record uni label record random random val write record train write record class write uni label class close val close train close traceback parsererror traceback recent gpu split run train arg run train input csv logger logger csvlogger save dir arg modeldir csv log logger logger csv logger ontologytaggerdatamodul argpars arg arg arg model uri local model uri environ channel model anaconda env pytorch latest lib python site packag pytorch lightn core datamodul argpars arg cl arg kwarg datamodul kwarg updat kwarg return cl datamodul kwarg classmethod anaconda env pytorch latest lib python site packag pytorch lightn core datamodul cl arg kwarg instanc lightningdatamodul mock init obj type cl arg kwarg return obj init self traindir train file valid file model label batch size print token token print label file label file label mapper labelmapp label file self batch size batch size self num class label mapper num class init self class file def init self class file self raw label read csv class file header sep self map anaconda env pytorch latest lib python site packag panda parser read csv filepath buffer sep delimit header name index col usecol squeez prefix mangl dupe col dtype engin convert true valu fals valu skipinitialspac skiprow skipfoot nrow valu default filter verbos skip blank line pars date infer datetim format date col date parser dayfirst cach date iter chunksiz compress thousand decim linetermin quotechar quot doublequot escapechar comment encod dialect error bad line warn bad line delim whitespac low memori memori map float precis return read filepath buffer kwd anaconda env pytorch latest lib python site packag panda parser read filepath buffer kwd try data parser read nrow final parser close anaconda env pytorch latest lib python site packag panda parser read self nrow def read self nrow nrow valid integ nrow nrow ret self engin read nrow alter column col dict anaconda env pytorch latest lib python site packag panda parser read self nrow def read self nrow try data self reader read nrow stopiter self chunk panda lib parser pyx panda lib parser textread read panda lib parser pyx panda lib parser textread read low memori panda lib parser pyx panda lib parser textread read row panda lib parser pyx panda lib parser textread token row panda lib parser pyx panda lib parser rais parser error parsererror error token data error expect field line saw class txt tab separ runtim activ event actor person agent person album product anim object architecturalstructur locat artist person athlet person automobileengin product award object biomolecul object bird object bodyofwat locat build locat chemicalsubst object compani organis competit event devic product diseas object district locat eukaryot object event event film object food object languag object locat locat meanoftransport product motorsportseason event municip locat musicalwork product organis organis painter person periodicalliteratur product person person personfunct person plant object poet person politician person river locat school organis settlement locat softwar product song product speci object sportsseason event station locat town locat villag locat writer person organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers",
        "Question_preprocessed_content":"txt alter save lead csv reader see field run kernel number field increment run error code invoc traceback runtim",
        "Question_gpt_summary_original":"The user is encountering an error where the CSV reader is seeing too many fields due to the .txt file being altered after save. The error message shows that the expected number of fields is 2, but it saw 9. The traceback indicates that the error occurred when reading the classes_file in line 104 of the code. The classes.txt file is tab-separated and contains a list of labels. Before runtime, the file contains a list of labels with their corresponding categories.",
        "Question_gpt_summary":"user encount error csv reader see field txt file alter save error messag show expect number field saw traceback indic error occur read class file line code class txt file tab separ contain list label runtim file contain list label correspond categori",
        "Answer_original_content":"problem fault ensur field line class txt ctrl reopen file runtim field line fix line class write uni label replac class write uni label",
        "Answer_preprocessed_content":"problem fault ensur field line reopen file runtim field line fix line replac",
        "Answer_gpt_summary_original":"Solution: The user found that the issue was caused by the fields being on the same line in the classes.txt file after runtime. To fix this, they added a newline character before writing each label to the file using the code \"classes.write('\\n'+uni_label)\".",
        "Answer_gpt_summary":"solut user issu caus field line class txt file runtim fix ad newlin charact write label file code class write uni label"
    },
    {
        "Question_title":"AWS Sagemaker Multiple Training Jobs",
        "Question_body":"<p>We currently have a system running on AWS Sagemaker whereby several units have their own trained machine learning model artifact (using an SKLearn training script with the Sagemaker SKLearn estimator).<\/p>\n<p>Through the use of Sagemaker's multi-model endpoints, we are able to host all of these units on a single instance.<\/p>\n<p>The problem we have is that we need to scale this system up such that we can train individual models for hundreds of thousand of units and then host the resulting model artifacts on a multi-model endpoint. But, Sagemaker has a limit to the number of models you can train in parallel (our limit is 30).<\/p>\n<p>Aside from training our models in batches, does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit?<\/p>\n<p>Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/p>\n<p>Furthermore, how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1603715439387,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1603716498123,
        "Question_score":0,
        "Question_view_count":1053.0,
        "Poster_id":13756183.0,
        "Poster_created_time":1592311727163,
        "Poster_last_access_time":1647692327232,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":153.0,
        "Poster_views":15.0,
        "Answer_body":"<p>Here are some ideas:<\/p>\n<p><em><strong>1. does anyone have any ideas how to go about implementing a system in AWS Sagemaker whereby for hundreds of thousands of units, we can have a separate trained model artifact for each unit? Is there a way to output multiple model artifacts for 1 sagemaker training job with the use of an SKLearn estimator?<\/strong><\/em><\/p>\n<p>I don't know if the 30-training job concurrency is a hard limit, if it is a blocker you should try and open a support ticket to ask if it is and try and get it raised. Otherwise as you can point out, you can try and train multiple models in one job, and produce multiple artifacts that you can either (a) send to S3 manually, or (b) save to <code>opt\/ml\/model<\/code> so that they all get sent to the model.tar.gz artifact in S3. Note that if this artifact gets too big this could get impractical though<\/p>\n<p><em><strong>2. how does Sagemaker make use of multiple CPUs when a training script is submitted? Does this have to be specified in the training script\/estimator object or is this handled automatically?<\/strong><\/em><\/p>\n<p>This depends on the type of training container you are using. SageMaker built-in containers are developed by Amazon teams and designed to efficiently use available resources. If you use your own code such as custom python in the Sklearn container, you are responsible for making sure that your code is efficiently written and uses available hardware. Hence framework choice is quite important :) for example, some sklearn models support explicitly using multiple CPUs (eg the <code>n_jobs<\/code> parameter in the <a href=\"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\" rel=\"nofollow noreferrer\">random forest<\/a>), but I don't think that Sklearn natively supports GPU, multi-GPU or multi-node training.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":5331834.0,
        "Answerer_created_time":1442180190107,
        "Answerer_last_access_time":1664048608876,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64537150",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1603790179687,
        "Question_original_content":"multipl train job current run unit train machin learn model artifact sklearn train script sklearn estim us multi model endpoint abl host unit singl instanc problem need scale train individu model hundr thousand unit host result model artifact multi model endpoint limit number model train parallel limit asid train model batch idea implement hundr thousand unit separ train model artifact unit wai output multipl model artifact train job us sklearn estim furthermor us multipl cpu train script submit specifi train script estim object handl automat",
        "Question_preprocessed_content":"multipl train job current run unit train machin learn model artifact us endpoint abl host unit singl instanc problem need scale train individu model hundr thousand unit host result model artifact endpoint limit number model train parallel asid train model batch idea implement hundr thousand unit separ train model artifact unit wai output multipl model artifact train job us sklearn estim furthermor us multipl cpu train script submit specifi train object handl automat",
        "Question_gpt_summary_original":"The user is facing challenges in scaling up their system on AWS Sagemaker to train individual models for hundreds of thousands of units and host resulting model artifacts on a multi-model endpoint. However, Sagemaker has a limit of training only 30 models in parallel. The user is seeking ideas to implement a system in Sagemaker to have a separate trained model artifact for each unit and output multiple model artifacts for one Sagemaker training job using an SKLearn estimator. Additionally, the user is unsure about how Sagemaker makes use of multiple CPUs when a training script is submitted and whether it needs to be specified in the training script\/estimator object or is handled automatically.",
        "Question_gpt_summary":"user face challeng scale train individu model hundr thousand unit host result model artifact multi model endpoint limit train model parallel user seek idea implement separ train model artifact unit output multipl model artifact train job sklearn estim addition user unsur make us multipl cpu train script submit need specifi train script estim object handl automat",
        "Answer_original_content":"idea idea implement hundr thousand unit separ train model artifact unit wai output multipl model artifact train job us sklearn estim know train job concurr hard limit blocker try open support ticket ask try rais point try train multipl model job produc multipl artifact send manual save opt model sent model tar artifact note artifact get big impract us multipl cpu train script submit specifi train script estim object handl automat depend type train contain built contain develop amazon team design effici us avail resourc us code custom python sklearn contain respons make sure code effici written us avail hardwar framework choic import exampl sklearn model support explicitli multipl cpu job paramet random forest think sklearn nativ support gpu multi gpu multi node train",
        "Answer_preprocessed_content":"idea idea implement hundr thousand unit separ train model artifact unit wai output multipl model artifact train job us sklearn estim know job concurr hard limit blocker try open support ticket ask try rais point try train multipl model job produc multipl artifact send manual save sent artifact note artifact get big impract us multipl cpu train script submit specifi train object handl automat depend type train contain contain develop amazon team design effici us avail resourc us code custom python sklearn contain respons make sure code effici written us avail hardwar framework choic import exampl sklearn model support explicitli multipl cpu think sklearn nativ support gpu train",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. The user can try and train multiple models in one job and produce multiple artifacts that can be saved to opt\/ml\/model so that they all get sent to the model.tar.gz artifact in S3. However, if this artifact gets too big, it could get impractical.\n2. The type of training container used in Sagemaker determines how it makes use of multiple CPUs. SageMaker built-in containers are developed by Amazon teams and designed to efficiently use available resources. If the user uses their own code such as custom python in the Sklearn container, they are responsible for making sure that their code is efficiently written and uses available hardware. Some Sklearn models support explicitly using multiple CPUs, but Sklearn",
        "Answer_gpt_summary":"possibl solut mention discuss user try train multipl model job produc multipl artifact save opt model sent model tar artifact artifact get big impract type train contain determin make us multipl cpu built contain develop amazon team design effici us avail resourc user us code custom python sklearn contain respons make sure code effici written us avail hardwar sklearn model support explicitli multipl cpu sklearn"
    },
    {
        "Question_title":"PowerBI and MLflow integration (through AzureML)",
        "Question_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1600261190477,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1600855880503,
        "Question_score":0,
        "Question_view_count":405.0,
        "Poster_id":14287756.0,
        "Poster_created_time":1600260166047,
        "Poster_last_access_time":1615561616230,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":15.0,
        "Poster_views":4.0,
        "Answer_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1600855957376,
        "Answer_score":0.0,
        "Answerer_id":455814.0,
        "Answerer_created_time":1285219808283,
        "Answerer_last_access_time":1664071160590,
        "Answerer_age":null,
        "Answerer_location":"Perth WA, Australia",
        "Answerer_reputation":6770.0,
        "Answerer_views":1127.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1600604920243,
        "Question_original_content":"powerbi integr current try integr model current deploi webservic powerbi integr model requir addit schema file deploi webservic model view powerbi problem come us log model perform subsequ deploi select model webservic integr deploi unfortun option defin schema file model deploi result model avail powerbi lack requir schema file option workaround possibl work rest api model power queri rewrit deploy code handl webservic deploy step azur instead thought ask mayb miss workaround current code defin schema file deploi deploi",
        "Question_preprocessed_content":"powerbi integr current try integr model current deploi webservic powerbi integr model requir addit schema file deploi webservic model view powerbi problem come us log model perform subsequ deploi select model webservic integr unfortun option defin schema file model deploi result model avail powerbi lack requir schema file option workaround possibl work rest api model power queri rewrit deploy code handl webservic deploy step azur instead thought ask mayb miss workaround current code defin schema file deploi",
        "Question_gpt_summary_original":"The user is facing challenges in integrating an ML model deployed on AzureML with PowerBI due to the requirement of a schema file. The user uses MLflow to log ML model performances and deploy a selected model onto AzureML as a webservice using mlflow.azureml.deploy(), which does not have the option to define a schema file before deployment, resulting in no model being available in PowerBI. The user's options are to find a workaround using the REST API of the model in a power query or rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.",
        "Question_gpt_summary":"user face challeng integr model deploi powerbi requir schema file user us log model perform deploi select model webservic deploi option defin schema file deploy result model avail powerbi user option workaround rest api model power queri rewrit deploy code handl webservic deploy step azur instead",
        "Answer_original_content":"point number wai solv issu instead deploi score servic azur wrote custom code load model contain initialis score code like import import json pyfunc import load model infer schema schema decor import input schema output schema infer schema paramet type numpi paramet type import numpyparametertyp def init global model model load model path join environ model dir awesom model input schema data numpyparametertyp input sampl output schema numpyparametertyp output sampl def run data return model predict data",
        "Answer_preprocessed_content":"point number wai solv issu instead deploi score servic azur wrote custom code load model contain initialis score code like",
        "Answer_gpt_summary_original":"Solution: The user can rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow. This can be done by writing a custom code that loads the MLflow model when the container is initialized. The scoring code can be written to accept input and output schema using the REST API of the model in a power query.",
        "Answer_gpt_summary":"solut user rewrit deploy code handl webservic deploy step azur instead write custom code load model contain initi score code written accept input output schema rest api model power queri"
    },
    {
        "Question_title":"Replace values in a column based on a condition in Azure ML Studio",
        "Question_body":"<p>How do I replace the values in a specific column with a particular value based on a condition in Azure ML Studio. I can do this using pandas in python as foolows:<\/p>\n\n<pre><code>df.loc[df['col_name'] &gt; 1990, 'col_name'] = 1\n<\/code><\/pre>\n\n<p>I'm trying to find a Module in Azure Machine Learning Studio that does the equivalent of this. <\/p>\n\n<p>I understand there is a replace option under the ConverToDataset module and a Replace Discrete Values module. But neither of these seems to do what I want. Is there an option to replace the values in just one column to a specific value based on a condition?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1557229871357,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3,
        "Question_view_count":564.0,
        "Poster_id":5686015.0,
        "Poster_created_time":1450260166772,
        "Poster_last_access_time":1663955203356,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1587.0,
        "Poster_views":540.0,
        "Answer_body":"<p>You can use either the more general <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/apply-sql-transformation\" rel=\"nofollow noreferrer\">Apply SQL Transformation<\/a>, or the dedicated <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/clip-values\" rel=\"nofollow noreferrer\">Clip Values<\/a> module. If all else fails, there's also <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/execute-python-script\" rel=\"nofollow noreferrer\">Execute Python Script<\/a>.<\/p>\n\n<p>Personally, for your example I'd use <code>Clip Values<\/code> with <code>Clip Peaks<\/code> and <code>Upper Threshold<\/code> set. For more complex rules I'd use either <code>Apply SQL Transformation<\/code> or <code>Execute Python Script<\/code>, depending on the rules but favouring SQL :).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":155697.0,
        "Answerer_created_time":1250158552416,
        "Answerer_last_access_time":1663847198323,
        "Answerer_age":null,
        "Answerer_location":"Romania",
        "Answerer_reputation":7916.0,
        "Answerer_views":801.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56021977",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1557391929596,
        "Question_original_content":"replac valu column base condit studio replac valu specif column particular valu base condit studio panda python foolow loc col col try modul studio equival understand replac option convertodataset modul replac discret valu modul want option replac valu column specif valu base condit",
        "Question_preprocessed_content":"replac valu column base condit studio replac valu specif column particular valu base condit studio panda python foolow try modul studio equival understand replac option convertodataset modul replac discret valu modul want option replac valu column specif valu base condit",
        "Question_gpt_summary_original":"The user is facing challenges in replacing values in a specific column based on a condition in Azure ML Studio. They are familiar with how to do this using pandas in Python but are struggling to find a module in Azure ML Studio that does the same thing. The user has tried using the ConvertToDataset and Replace Discrete Values modules but neither of them seems to do what they want. They are looking for an option to replace the values in just one column to a specific value based on a condition.",
        "Question_gpt_summary":"user face challeng replac valu specif column base condit studio familiar panda python struggl modul studio thing user tri converttodataset replac discret valu modul want look option replac valu column specif valu base condit",
        "Answer_original_content":"us gener appli sql transform dedic clip valu modul fail execut python script person exampl us clip valu clip peak upper threshold set complex rule us appli sql transform execut python script depend rule favour sql",
        "Answer_preprocessed_content":"us gener appli sql transform dedic clip valu modul fail execut python script person exampl us set complex rule us depend rule favour sql",
        "Answer_gpt_summary_original":"Solutions provided in the discussion include using the Apply SQL Transformation module, the Clip Values module with Clip Peaks and Upper Threshold set, and the Execute Python Script module. The Clip Values module is recommended for the specific example given, while Apply SQL Transformation or Execute Python Script can be used for more complex rules.",
        "Answer_gpt_summary":"solut provid discuss includ appli sql transform modul clip valu modul clip peak upper threshold set execut python script modul clip valu modul recommend specif exampl given appli sql transform execut python script complex rule"
    },
    {
        "Question_title":"Does SageMaker built-in LightGBM algorithm support distributed training?",
        "Question_body":"<p>Does  <strong>Amazon SageMaker built-in LightGBM<\/strong> algorithm support <strong>distributed training<\/strong>?<\/p>\n<p>I use Databricks for distributed training of LightGBM today. If SageMaker built-in LightGBM supports distributed training, I would consider migrating to SageMaker. It is not clear in the Amazon SageMaker's built-in <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">LightGBM<\/a>'s documentation on whether it supports distributed training.<\/p>\n<p>Thanks very much for any suggestion or clarification on this.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1662878679543,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":27.0,
        "Poster_id":3203213.0,
        "Poster_created_time":1389887039672,
        "Poster_last_access_time":1664076128463,
        "Poster_age":null,
        "Poster_location":"Singapore",
        "Poster_reputation":5854.0,
        "Poster_views":794.0,
        "Answer_body":"<p>I went through the LightGBM section of SageMaker documentation and there are no references that it supports distributed training. One of the example\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/lightgbm.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0uses single instance type. Also looked at lightGBM documentation\u00a0<a href=\"https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parallel-Learning-Guide.html\" rel=\"nofollow noreferrer\">here<\/a>\u00a0. Here are the parameters that you need to specify<\/p>\n<p>tree_learner=your_parallel_algorithm,<\/p>\n<p>num_machines=your_num_machines,<\/p>\n<p>Given I couldnt find any reference of above in SageMaker documentation, I assume its not supported.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":20046744.0,
        "Answerer_created_time":1663710134840,
        "Answerer_last_access_time":1663785274460,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":16.0,
        "Answerer_views":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73677347",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1663711220203,
        "Question_original_content":"built lightgbm algorithm support distribut train built lightgbm algorithm support distribut train us databrick distribut train lightgbm todai built lightgbm support distribut train consid migrat clear built lightgbm document support distribut train thank suggest clarif",
        "Question_preprocessed_content":"lightgbm algorithm support distribut train lightgbm algorithm support distribut train us databrick distribut train lightgbm todai lightgbm support distribut train consid migrat clear lightgbm document support distribut train thank suggest clarif",
        "Question_gpt_summary_original":"The user is unsure if the Amazon SageMaker built-in LightGBM algorithm supports distributed training and is seeking clarification on this matter. They currently use Databricks for distributed training of LightGBM and would consider migrating to SageMaker if it supports distributed training.",
        "Question_gpt_summary":"user unsur built lightgbm algorithm support distribut train seek clarif matter current us databrick distribut train lightgbm consid migrat support distribut train",
        "Answer_original_content":"went lightgbm section document refer support distribut train examplehereus singl instanc type look lightgbm documentationher paramet need specifi tree learner parallel algorithm num machin num machin given refer document assum support",
        "Answer_preprocessed_content":"went lightgbm section document refer support distribut train examplehereus singl instanc type look lightgbm documentationher paramet need specifi given refer document assum support",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The user is seeking clarification on whether the Amazon SageMaker built-in LightGBM algorithm supports distributed training, but the discussion concludes that there are no references to support this in the documentation.",
        "Answer_gpt_summary":"solut solut provid discuss user seek clarif built lightgbm algorithm support distribut train discuss conclud refer support document"
    },
    {
        "Question_title":"'Enter Data' as list instead of list of lists in Azure ML Web Service",
        "Question_body":"<p>In Azure ML, I want to enter data to a model through a published Web Service. \nThe way to tell this to the Web Service, as far as I can tell, it to have an 'Enter Data' box coming into the same input as the Web service. <\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/m1x5x.png\" alt=\"enter image description here\"><\/p>\n\n<p>You can then set you data format in the 'Enter Data' properties:<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/VQJ9V.png\" alt=\"enter image description here\"><\/p>\n\n<p>I want that list to be an arbitrary-length array of samples. This works if your input is:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1\n        ],\n        [\n          2\n        ],\n        [\n          3\n        ],\n        [\n          4\n        ],\n        [\n          5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>This is ok, but ideally it would be easier, and (more importantly) more network-efficient, if I could send them as:<\/p>\n\n<pre><code>{\n  \"Inputs\": {\n    \"input1\": {\n      \"ColumnNames\": [\n        \"samples\"\n      ],\n      \"Values\": [\n        [\n          1,2,3,4,5\n        ]\n      ]\n    }\n  },\n  \"GlobalParameters\": {}\n}\n<\/code><\/pre>\n\n<p>Is there a correct syntax to implement this? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1437737046243,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":174.0,
        "Poster_id":277113.0,
        "Poster_created_time":1266595927520,
        "Poster_last_access_time":1633931623067,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":7681.0,
        "Poster_views":361.0,
        "Answer_body":"<p>I have worked internally to request a confirmation of your concern - <\/p>\n\n<blockquote>\n  <p>'Enter Data' as list instead of list of lists in Azure ML Web Service<\/p>\n<\/blockquote>\n\n<p>but you expected feature is not available today in Azure ML Studio (The reason behind is Azure ML has to be able to read the input data as a tabular format, rows and columns). Such being the case, I would like to suggest you to submit a new feature request via below option:<\/p>\n\n<p>On Azure ML Studio -> the upper right corner, there is a smiley face, please click that and send the feedback.<\/p>\n\n<p>Should you have any further concerns, please feel free to let me know.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":1338161.0,
        "Answerer_created_time":1334649856036,
        "Answerer_last_access_time":1524030637968,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":2091.0,
        "Answerer_views":501.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/31609319",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1438871325836,
        "Question_original_content":"enter data list instead list list web servic want enter data model publish web servic wai tell web servic far tell enter data box come input web servic set data format enter data properti want list arbitrari length arrai sampl work input input input columnnam sampl valu globalparamet ideal easier importantli network effici send input input columnnam sampl valu globalparamet correct syntax implement",
        "Question_preprocessed_content":"enter data list instead list list web servic want enter data model publish web servic wai tell web servic far tell enter data box come input web servic set data format enter data properti want list arrai sampl work input ideal easier send correct syntax implement",
        "Question_gpt_summary_original":"The user is facing a challenge in entering data to a model through a published Web Service in Azure ML. They want to send an arbitrary-length array of samples as a list instead of a list of lists, which would be more network-efficient. The user is seeking guidance on the correct syntax to implement this.",
        "Question_gpt_summary":"user face challeng enter data model publish web servic want send arbitrari length arrai sampl list instead list list network effici user seek guidanc correct syntax implement",
        "Answer_original_content":"work intern request confirm concern enter data list instead list list web servic expect featur avail todai studio reason abl read input data tabular format row column case like suggest submit new featur request option studio upper right corner smilei face click send feedback concern feel free let know",
        "Answer_preprocessed_content":"work intern request confirm concern enter data list instead list list web servic expect featur avail todai studio case like suggest submit new featur request option studio upper right corner smilei face click send feedback concern feel free let know",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The user is advised to submit a new feature request via Azure ML Studio's feedback option.",
        "Answer_gpt_summary":"solut solut provid discuss user advis submit new featur request studio feedback option"
    },
    {
        "Question_title":"How to deploy an existing pytorch model previously trained with Amazon Sagemaker and stored in S3 bucket",
        "Question_body":"<p>I have trained a Pytorch model using SageMaker and the model is now stored in an S3 bucket.\nI am trying to retrieve that model and deploying it.<\/p>\n\n<p>This is the code I am using:<\/p>\n\n<pre><code>estimator = sagemaker.model.FrameworkModel(\n    model_data= #link to  model location in s3\n    image=  # image\n    role=role,\n    entry_point='train.py', \n    source_dir='pytorch_source',\n    sagemaker_session = sagemaker_session\n) \n\npredictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")\n<\/code><\/pre>\n\n<p>But after the deployment process (that seems to run smoothly), the predictor is just a NoneType.\nI haven't found any weird message in the logs...<\/p>\n\n<p>I have also made another attempt with the following code:<\/p>\n\n<pre><code>estimator = PyTorchModel(model_data= #link to model location in s3 \n                             role=role,\n                             image= #image\n                             entry_point='pytorch_source\/train.py',\n                            predictor_cls = 'pytorch_source\/train.py',\n                           framework_version = '1.1.0')\n\npredictor = estimator.deploy(initial_instance_count=1, instance_type=\"ml.p2.xlarge\")\n<\/code><\/pre>\n\n<p>But it doesn't even complete the deployment.<\/p>\n\n<p>Can anyone help with this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1583334481857,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1583398637700,
        "Question_score":0,
        "Question_view_count":360.0,
        "Poster_id":9961790.0,
        "Poster_created_time":1529408888483,
        "Poster_last_access_time":1649930832187,
        "Poster_age":null,
        "Poster_location":"London, UK",
        "Poster_reputation":79.0,
        "Poster_views":5.0,
        "Answer_body":"<p>I actually solved using PyTorchModel with the following settings:<\/p>\n\n<pre><code>estimator = PyTorchModel(model_data='#path to model, \n                             role=role,\n                             source_dir='pytorch_source',\n                             entry_point='deploy.py',\n                            predictor_cls = ImgPredictor,\n                           framework_version = '1.1.0')\n<\/code><\/pre>\n\n<p>where ImgPredictor is<\/p>\n\n<pre><code>from sagemaker.predictor import RealTimePredictor, json_deserializer\n\nclass ImgPredictor(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super(ImgPredictor, self).__init__(endpoint_name, sagemaker_session, content_type='application\/x-image', \n                                           deserializer = json_deserializer ,accept='application\/json')\n<\/code><\/pre>\n\n<p>and deploy.py contains the required functions input_fn, output_fn, model_fn and predict_fn.\nAlso, a requirements.txt file was missing in the source directory.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":9961790.0,
        "Answerer_created_time":1529408888483,
        "Answerer_last_access_time":1649930832187,
        "Answerer_age":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation":79.0,
        "Answerer_views":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60529048",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1583499635092,
        "Question_original_content":"deploi exist pytorch model previous train store bucket train pytorch model model store bucket try retriev model deploi code estim model frameworkmodel model data link model locat imag imag role role entri point train sourc dir pytorch sourc session session predictor estim deploi initi instanc count instanc type xlarg deploy process run smoothli predictor nonetyp haven weird messag log attempt follow code estim pytorchmodel model data link model locat role role imag imag entri point pytorch sourc train predictor cl pytorch sourc train framework version predictor estim deploi initi instanc count instanc type xlarg complet deploy help",
        "Question_preprocessed_content":"deploi exist pytorch model previous train store bucket train pytorch model model store bucket try retriev model deploi code deploy process predictor nonetyp haven weird messag attempt follow code complet deploy help",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a Pytorch model that was previously trained using Amazon SageMaker and stored in an S3 bucket. The user has tried two different deployment codes, but the predictor is either a NoneType or the deployment process does not complete. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user face challeng deploi pytorch model previous train store bucket user tri differ deploy code predictor nonetyp deploy process complet user seek help resolv issu",
        "Answer_original_content":"actual solv pytorchmodel follow set estim pytorchmodel model data path model role role sourc dir pytorch sourc entri point deploi predictor cl imgpredictor framework version imgpredictor predictor import realtimepredictor json deseri class imgpredictor realtimepredictor def init self endpoint session super imgpredictor self init endpoint session content type applic imag deseri json deseri accept applic json deploi contain requir function input output model predict requir txt file miss sourc directori",
        "Answer_preprocessed_content":"actual solv pytorchmodel follow set imgpredictor contain requir function file miss sourc directori",
        "Answer_gpt_summary_original":"Solution:\nThe user can try using PyTorchModel with the provided settings and ImgPredictor class. They should also ensure that the deploy.py file contains the required functions input_fn, output_fn, model_fn, and predict_fn. Additionally, the user should make sure that the source directory contains a requirements.txt file.",
        "Answer_gpt_summary":"solut user try pytorchmodel provid set imgpredictor class ensur deploi file contain requir function input output model predict addition user sure sourc directori contain requir txt file"
    },
    {
        "Question_title":"Request help with Azure machine learning workspace",
        "Question_body":"I have enrolled myself in Azure Machine Learning course and the first step there is to create an azure ML workspace with subscription, resource group, region, storage account etc. I am a new joiner and I am doing this for my learning. Not sure which option to select. Is there any guidance or doc to follow? I have checked with my team and they are suggesting to use my personal account to get a demo account and free azure subscription to do the course and not my microsoft credentials. Require assistance in this regard.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1655371059130,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@SanjanaDas-0274 Thanks for the question. Here is the document to Create workspace resources you need to get started with Azure Machine Learning. You can use your personal account to get free azure subscription.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/quickstart-create-resources\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/891716\/request-help-with-azure-machine-learning-workspace.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1655384032827,
        "Question_original_content":"request help workspac enrol cours step creat workspac subscript resourc group region storag account new joiner learn sure option select guidanc doc follow check team suggest us person account demo account free azur subscript cours microsoft credenti requir assist regard",
        "Question_preprocessed_content":"request help workspac enrol cours step creat workspac subscript resourc group region storag account new joiner learn sure option select guidanc doc follow check team suggest us person account demo account free azur subscript cours microsoft credenti requir assist regard",
        "Question_gpt_summary_original":"The user is facing challenges in creating an Azure Machine Learning workspace for a course they have enrolled in. They are unsure which options to select and have reached out for guidance. Additionally, they are unsure whether to use their personal account or Microsoft credentials and require assistance in this regard.",
        "Question_gpt_summary":"user face challeng creat workspac cours enrol unsur option select reach guidanc addition unsur us person account microsoft credenti requir assist regard",
        "Answer_original_content":"sanjanada thank question document creat workspac resourc need start us person account free azur subscript http doc microsoft com azur machin learn quickstart creat resourc forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"thank question document creat workspac resourc need start us person account free azur subscript forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to a document that guides the user on how to create workspace resources for Azure Machine Learning. It also suggests that the user can use their personal account to get a free Azure subscription. No other solutions are mentioned in the discussion.",
        "Answer_gpt_summary":"solut discuss provid link document guid user creat workspac resourc suggest user us person account free azur subscript solut mention discuss"
    },
    {
        "Question_title":"Upload Microsoft Excel Workbook with Many Sheets into Azure ML Studio",
        "Question_body":"<p>I want to upload my <code>Excel<\/code> Workbook into Azure Machine Learning Studio. The reason is I have some data that I would like to join into my other <code>.csv<\/code> files to create a training data set. \nWhen I upload my <code>Excel<\/code>, I don't get <code>.xlsx<\/code>, or <code>.xls<\/code>, but other extensions such as <code>.csv<\/code>, <code>.txt<\/code> etc.. <\/p>\n\n<p>This is how it looks,\n<a href=\"https:\/\/i.stack.imgur.com\/SrSon.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SrSon.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I uploaded anyways and now, I am getting weird characters. How can I get excel workbook uploaded and get my sheets, so, I can join data and do, data preparation. Any suggestions?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1530198936390,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":577.0,
        "Poster_id":9431057.0,
        "Poster_created_time":1519936486960,
        "Poster_last_access_time":1634702229683,
        "Poster_age":null,
        "Poster_location":"Minneapolis, MN, USA",
        "Poster_reputation":1113.0,
        "Poster_views":122.0,
        "Answer_body":"<p>You could save the workbook as a (set of) CSV file(s) and upload them separately.<\/p>\n\n<p>A CSV file, a '<a href=\"https:\/\/en.wikipedia.org\/wiki\/Comma-separated_values\" rel=\"nofollow noreferrer\">Comma Separated Values<\/a>' file, is exactly that. A flat file with some values separated by a comma. If you load an Excel file it will mess up since there's way more information in an Excel file than just values separated by comma's. Have a look at <code>File<\/code> -> <code>Save as<\/code> -> <code>Save as type<\/code> where you can select 'CSV (comma delimited) (*.csv)'<\/p>\n\n<p><em>Disclaimer: no, it's not always a comma...<\/em>  <\/p>\n\n<blockquote>\n  <p>In addition, the term \"CSV\" also denotes some closely related delimiter-separated formats that use different field delimiters. These include tab-separated values and space-separated values. A delimiter that is not present in the field data (such as tab) keeps the format parsing simple. These alternate delimiter-separated files are often even given a .csv extension despite the use of a non-comma field separator.<\/p>\n<\/blockquote>\n\n<p><strong>Edit<\/strong><br>\nSo apparently Excel files <em>are<\/em> supported: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/desktop-workbench\/data-prep-appendix2-supported-data-sources\" rel=\"nofollow noreferrer\">Supported data sources for Azure Machine Learning data preparation<\/a>  <\/p>\n\n<p><em>Excel (.xls\/.xlsx)<\/em><br>\nRead an Excel file one sheet at a time by specifying sheet name or number.<\/p>\n\n<p>But also, only UTF-8 is supported: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/import-data#bkmk_Notes\" rel=\"nofollow noreferrer\">Import Data - Technical notes<\/a><\/p>\n\n<blockquote>\n  <p>Azure Machine Learning requires UTF-8 encoding. If the data you are importing uses a different encoding, or was exported from a data source that uses a different default encoding, various problems might appear in the text.<\/p>\n<\/blockquote>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1530200496840,
        "Answer_score":1.0,
        "Answerer_id":1945525.0,
        "Answerer_created_time":1357215436787,
        "Answerer_last_access_time":1663998148808,
        "Answerer_age":null,
        "Answerer_location":"Netherlands",
        "Answerer_reputation":12551.0,
        "Answerer_views":1671.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51086377",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1530199230416,
        "Question_original_content":"upload microsoft excel workbook sheet studio want upload excel workbook studio reason data like join csv file creat train data set upload excel xlsx xl extens csv txt look upload anywai get weird charact excel workbook upload sheet join data data prepar suggest",
        "Question_preprocessed_content":"upload microsoft excel workbook sheet studio want upload workbook studio reason data like join file creat train data set upload extens look upload anywai get weird charact excel workbook upload sheet join data data prepar suggest",
        "Question_gpt_summary_original":"The user is facing challenges in uploading a Microsoft Excel workbook with multiple sheets into Azure Machine Learning Studio. When the user uploads the workbook, they do not get the expected file extensions and encounter issues with weird characters. The user is seeking suggestions on how to upload the workbook and access the sheets for data preparation.",
        "Question_gpt_summary":"user face challeng upload microsoft excel workbook multipl sheet studio user upload workbook expect file extens encount issu weird charact user seek suggest upload workbook access sheet data prepar",
        "Answer_original_content":"save workbook set csv file upload separ csv file comma separ valu file exactli flat file valu separ comma load excel file mess wai inform excel file valu separ comma look file save save type select csv comma delimit csv disclaim comma addit term csv denot close relat delimit separ format us differ field delimit includ tab separ valu space separ valu delimit present field data tab keep format pars simpl altern delimit separ file given csv extens despit us non comma field separ edit appar excel file support support data sourc data prepar excel xl xlsx read excel file sheet time specifi sheet number utf support import data technic note requir utf encod data import us differ encod export data sourc us differ default encod problem appear text",
        "Answer_preprocessed_content":"save workbook csv file upload separ csv file comma separ valu file exactli flat file valu separ comma load excel file mess wai inform excel file valu separ comma look select csv disclaim addit term csv denot close relat format us differ field delimit includ valu valu delimit present field data keep format pars simpl altern file given csv extens despit us field separ edit appar excel file support support data sourc data prepar excel read excel file sheet time specifi sheet number support import data technic note requir encod data import us differ encod export data sourc us differ default encod problem appear text",
        "Answer_gpt_summary_original":"One solution suggested is to save the workbook as a CSV file and upload the sheets separately. Another solution is to upload the Excel file directly, but it should be noted that only UTF-8 encoding is supported.",
        "Answer_gpt_summary":"solut suggest save workbook csv file upload sheet separ solut upload excel file directli note utf encod support"
    },
    {
        "Question_title":"Artifacts (local) caching - how does it really work?",
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m trying to figure out how does the caching  of artifacts work. Let\u2019s say I want to download a model artifact to run some evaluation on. I don\u2019t need the file on disk to persist rather I just want to load it into memory. What I do right now in my evaluation script is:<\/p>\n<pre><code class=\"lang-auto\">import tempfile\nimport wandb\n\nartifact = wandb.use_artifact(model_weights_uri)\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    artifact.download(tmpdirname)\n    model_weights = load_pickle(os.path.join(tmpdirname, \"model_weights.pickle\"))\n<\/code><\/pre>\n<p>And from that point on I use the <code>model_weights<\/code> as it was loaded into memory.<\/p>\n<p>My first question is: if I run the code twice (on the same machine), <strong>will the model-weights be downloaded again<\/strong> or are they cached somewhere? assuming the logged artifact wasn\u2019t changed of course. And if they are cached, where are they cached?<br>\nI\u2019m also not clear about the <code>artifact<\/code> directory (which is used if I run <code>artifact.download()<\/code> without any argument). Does that directory serve as cache? if so, what does the <code>.cache<\/code> directory used for?<\/p>\n<p>I would appreciate answers to my questions and perhaps a  general explanation of the artifact caching mechanism &amp; best practices.<\/p>\n<p>Thanks!<br>\nRan<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1650203452610,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":680.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ranshadmi-nexite\">@ranshadmi-nexite<\/a>,<\/p>\n<p>Thank you for your question. You are right, all Artifacts are cached on your system under <code>~\/.cache\/wandb\/artifacts<\/code> and organized by their checksum. So if you try to download a file with checksum <code>x<\/code> and that file has been logged in an Artifact from your machine or downloaded to your machine as part of an artifact before, we just pull it from the cache by checking if there is a cached Artifact file with checksum <code>x<\/code>.<\/p>\n<p>So, if you run the same code twice, assuming the version of the artifact you are trying to download has not changed, the artifact can simply be pickked up from your cache directory.<\/p>\n<p>Also, when calling <code>artifact.download()<\/code> without any arguments, the artifact is saved in the directory in which the code is running. This, however,  is not the directory that serves as a cache, that still remains <code>.cache<\/code> which acts as a central location to look for artifacts before fetching it.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/artifacts-local-caching-how-does-it-really-work\/2255",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1650312955392,
        "Question_original_content":"artifact local cach work try figur cach artifact work let want download model artifact run evalu dont need file disk persist want load memori right evalu script import tempfil import artifact us artifact model weight uri tempfil temporarydirectori tmpdirnam artifact download tmpdirnam model weight load pickl path join tmpdirnam model weight pickl point us model weight load memori question run code twice machin model weight download cach assum log artifact wasnt chang cours cach cach clear artifact directori run artifact download argument directori serv cach cach directori appreci answer question gener explan artifact cach mechan best practic thank ran",
        "Question_preprocessed_content":"artifact cach work try figur cach artifact work let want download model artifact run evalu dont need file disk persist want load memori right evalu script point us load memori question run code twice download cach assum log artifact wasnt chang cours cach cach clear directori directori serv cach directori appreci answer question gener explan artifact cach mechan best practic thank ran",
        "Question_gpt_summary_original":"The user is trying to understand how the caching of artifacts works. They want to know if the model-weights will be downloaded again if they run the code twice on the same machine and where the cached files are stored. The user is also unclear about the artifact directory and the .cache directory and is seeking a general explanation of the artifact caching mechanism and best practices.",
        "Question_gpt_summary":"user try understand cach artifact work want know model weight download run code twice machin cach file store user unclear artifact directori cach directori seek gener explan artifact cach mechan best practic",
        "Answer_original_content":"ranshadmi nexit thank question right artifact cach cach artifact organ checksum try download file checksum file log artifact machin download machin artifact pull cach check cach artifact file checksum run code twice assum version artifact try download chang artifact simpli pick cach directori call artifact download argument artifact save directori code run directori serv cach remain cach act central locat look artifact fetch thank ramit",
        "Answer_preprocessed_content":"thank question right artifact cach organ checksum try download file checksum file log artifact machin download machin artifact pull cach check cach artifact file checksum run code twice assum version artifact try download chang artifact simpli pick cach directori call argument artifact save directori code run directori serv cach remain act central locat look artifact fetch thank ramit",
        "Answer_gpt_summary_original":"Solution:\n- All Artifacts are cached on the user's system under `~\/.cache\/wandb\/artifacts` and organized by their checksum.\n- If the user runs the same code twice and the version of the artifact they are trying to download has not changed, the artifact can be picked up from the cache directory.\n- When calling `artifact.download()` without any arguments, the artifact is saved in the directory in which the code is running.\n- The `.cache` directory acts as a central location to look for artifacts before fetching it.",
        "Answer_gpt_summary":"solut artifact cach user cach artifact organ checksum user run code twice version artifact try download chang artifact pick cach directori call artifact download argument artifact save directori code run cach directori act central locat look artifact fetch"
    },
    {
        "Question_title":"Is there a workaround to make opencensus work with MLFlow?",
        "Question_body":"<p>I'm not able to import mlflow after having launched a log with opencensus Azure.\nThe MLFlow import runs forever.<\/p>\n<p>My environment is the following:<\/p>\n<ul>\n<li>Python 3.7<\/li>\n<li>opencensus-ext-azure 1.0.7<\/li>\n<li>opencensus-ext-logging 0.1.0<\/li>\n<li>mlflow 1.15.0<\/li>\n<\/ul>\n<p>Here is the code to repoduce the bug:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import logging\n\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))\nlogger.warning('Hello, World!')\n\nimport mlflow\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1619011270043,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":41.0,
        "Poster_id":4553735.0,
        "Poster_created_time":1423640080283,
        "Poster_last_access_time":1663943557963,
        "Poster_age":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation":457.0,
        "Poster_views":125.0,
        "Answer_body":"<p>I found a workaround, not the cleanest one though.<\/p>\n<p>I import mlflow at the beginning even if it's not useful this way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import mlflow\nimport logging\n\nfrom opencensus.ext.azure.log_exporter import AzureLogHandler\n\nlogger = logging.getLogger(__name__)\nlogger.addHandler(AzureLogHandler(connection_string='InstrumentationKey=&lt;your-key&gt;'))\nlogger.warning('Hello, World!')\n\nimport mlflow\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4553735.0,
        "Answerer_created_time":1423640080283,
        "Answerer_last_access_time":1663943557963,
        "Answerer_age":null,
        "Answerer_location":"Lyon, France",
        "Answerer_reputation":457.0,
        "Answerer_views":125.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67196775",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1619077377892,
        "Question_original_content":"workaround opencensu work abl import have launch log opencensu azur import run forev environ follow python opencensu ext azur opencensu ext log code repoduc bug import log opencensu ext azur log export import azureloghandl logger log getlogg logger addhandl azureloghandl connect string instrumentationkei logger warn hello world import",
        "Question_preprocessed_content":"workaround opencensu work abl import have launch log opencensu azur import run forev environ follow python code repoduc bug",
        "Question_gpt_summary_original":"The user is facing a challenge in importing MLFlow after launching a log with opencensus Azure. The MLFlow import runs indefinitely, and the user has provided the environment details and code to reproduce the issue.",
        "Question_gpt_summary":"user face challeng import launch log opencensu azur import run indefinit user provid environ detail code reproduc issu",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"SparkR not working",
        "Question_body":"I am trying to control a Spark cluster (using SparkR) from a Sagemaker notebook. I followed these instructions closely: https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/ and got it to work.\n\nToday when I try to run the SageMaker notebook (using the exact same code as before) I inexplicably get this error:\n\nAn error was encountered:\n[1] \"Error in callJMethod(sparkSession, \\\"read\\\"): Invalid jobj 1. If SparkR was restarted, Spark operations need to be re-executed.\"\n\n\nDoes anyone know why this is? I terminated the SparkR kernel and am still getting this error.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1591029652000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":44.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"You cannot have multiple SparkContexts in one JVM. The issue is resolved as WON'T FIX. You have to stop the spark session which spawned the sparkcontext (which you have already done).\n\nsparkR.session.stop()\n\nhttps:\/\/issues.apache.org\/jira\/browse\/SPARK-2243",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqyiKb_XvRhGxm1RxwjXhJQ\/spark-r-not-working",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1591041375000,
        "Question_original_content":"sparkr work try control spark cluster sparkr notebook follow instruct close http aw amazon com blog machin learn build amazon notebook back spark amazon emr got work todai try run notebook exact code inexplic error error encount error calljmethod sparksess read invalid jobj sparkr restart spark oper need execut know termin sparkr kernel get error",
        "Question_preprocessed_content":"sparkr work try control spark cluster notebook follow instruct close got work todai try run notebook inexplic error error encount error calljmethod invalid jobj sparkr restart spark oper need know termin sparkr kernel get error",
        "Question_gpt_summary_original":"The user is encountering an error while trying to control a Spark cluster using SparkR from a Sagemaker notebook. The error message states that there is an invalid jobj and suggests that Spark operations need to be re-executed. The user has already terminated the SparkR kernel but is still getting the same error.",
        "Question_gpt_summary":"user encount error try control spark cluster sparkr notebook error messag state invalid jobj suggest spark oper need execut user termin sparkr kernel get error",
        "Answer_original_content":"multipl sparkcontext jvm issu resolv won fix stop spark session spawn sparkcontext sparkr session stop http issu apach org jira brows spark",
        "Answer_preprocessed_content":"multipl sparkcontext jvm issu resolv won fix stop spark session spawn sparkcontext",
        "Answer_gpt_summary_original":"Solution: The issue is resolved as WON'T FIX. The solution is to stop the spark session which spawned the sparkcontext by using the command \"sparkR.session.stop()\".",
        "Answer_gpt_summary":"solut issu resolv won fix solut stop spark session spawn sparkcontext command sparkr session stop"
    },
    {
        "Question_title":"Preparing ML object detction dataset for deep learning in PyTorch or similar",
        "Question_body":"The intent of what I'm trying to achieve is:\n\nExport data labelling project as a Dataset\n\n\nConsume the Dataset in a notebook (converting to a Pandas dataframe)\n\n\nPerform a custom train \/ test split that maintains particular file groupings\n\n\nRegister the resulting training and testing dataframes as Datasets\n\n\nUse these Datasets to train and test a custom object detection model\n\n\n\n\nI need help in preparing the data for that final step. I'm familiar with different deep learning libraries, but have never implemented them in the Azure environment before. I've managed to complete 1 to 4. For step 4, I ended up writing the data to csv files and uploading these to the datastore.\n\n # define path for training data file and create new delimited file\n train_path = '.\/data\/train.csv'\n train_dataframe.to_csv(train_path, sep = ';', index = False)\n    \n # repeat for testing\n test_path = '.\/data\/test.csv'\n test_dataframe.to_csv(test_path, sep = ';', index = False)\n    \n # get the datastore to upload prepared data\n datastore = Datastore.get(ws, datastore_name='learningdata')\n    \n # upload the local files from src_dir to the target_path in datastore\n datastore.upload(src_dir='data', target_path='train-test', overwrite=True)\n    \n # create and register training dataset from datastore files\n training_ds = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train-test\/train.csv')], separator=';')\n training_ds = training_ds.register(workspace=ws, name = 'train', description = 'training dataset sampled from labelled data', create_new_version=True)\n    \n # create and register testing dataset from datastore files\n testing_ds = Dataset.Tabular.from_delimited_files(path = [(datastore, 'train-test\/test.csv')], separator=';')\n testing_ds = testing_ds.register(workspace=ws, name = 'test', description = 'testing dataset sampled from labelled data', create_new_version=True)\n\n\n\nThe approach I was intending to use for step 5 was to use to_torchvision() to convert it into a Torchvision dataset. This doesn't work, I receive the following error:\n\n UserErrorException: UserErrorException:\n  Message: Cannot perform torchvision conversion on dataset without labeled columns defined\n  InnerException None\n  ErrorResponse \n {\n     \"error\": {\n         \"code\": \"UserError\",\n         \"message\": \"Cannot perform torchvision conversion on dataset without labeled columns defined\"\n     }\n }\n\n\n\nI suspect that the issue has to do with DataTypes. The original Dataset (exported from the data labelling project) has the DataTypes displayed below. By comparison, all column types in the train and test Datasets are parsed as strings. From my understanding, there's no way to convert to these data types.\n\nimage_url = Stream\n\n\nlabel = List\n\n\nlabel_confidence = List\n\nAny advice on how to prepare this dataset for use in PyTorch or recommendation for an alternative approach would be greatly appreciated.\n\n\n\n\n\n\nUpdate as per comment below:\n\nI'm currently mounting the dataframe rather than downloading it due to data size.\n\n\nI can view images from the originally mounted Dataset, but when loading the newly registered training Dataset I can't access images as '\/tmp\/tmpog809x4v\/[...].jpg' is no longer relevant.\n\n\nI can't perform random split because I'm using clustered sampling.\n\n\nI'm working on creating a class object to define the dataset, but I cannot currently create the PIL Image object as required by PyTorch (https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html#defining-the-dataset)",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1607567444300,
        "Question_favorite_count":7.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"I modified the methodology and was able to successfully resolve this issue as follows:\n\nExport data labelling project as Dataset\n\n\nConsume the Dataset in the notebook by creating both a PyTorch dataset and a Pandas dataframe\n\n\nUse the Pandas dataframe to determine indices for the train \/ test split based on required sampling\n\n\nUse the indices as an input to torch.utils.data.Subset() to split the PyTorch dataset into train and test",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/192973\/preparing-ml-object-detction-dataset-for-deep-lear.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1608071413792,
        "Question_original_content":"prepar object detction dataset deep learn pytorch similar intent try achiev export data label project dataset consum dataset notebook convert panda datafram perform custom train test split maintain particular file group regist result train test datafram dataset us dataset train test custom object detect model need help prepar data final step familiar differ deep learn librari implement azur environ manag complet step end write data csv file upload datastor defin path train data file creat new delimit file train path data train csv train datafram csv train path sep index fals repeat test test path data test csv test datafram csv test path sep index fals datastor upload prepar data datastor datastor datastor learningdata upload local file src dir target path datastor datastor upload src dir data target path train test overwrit true creat regist train dataset datastor file train dataset tabular delimit file path datastor train test train csv separ train train regist workspac train descript train dataset sampl label data creat new version true creat regist test dataset datastor file test dataset tabular delimit file path datastor train test test csv separ test test regist workspac test descript test dataset sampl label data creat new version true approach intend us step us torchvis convert torchvis dataset work receiv follow error usererrorexcept usererrorexcept messag perform torchvis convers dataset label column defin innerexcept errorrespons error code usererror messag perform torchvis convers dataset label column defin suspect issu datatyp origin dataset export data label project datatyp displai comparison column type train test dataset pars string understand wai convert data type imag url stream label list label confid list advic prepar dataset us pytorch recommend altern approach greatli appreci updat comment current mount datafram download data size view imag origin mount dataset load newli regist train dataset access imag tmp tmpogxv jpg longer relev perform random split cluster sampl work creat class object defin dataset current creat pil imag object requir pytorch http pytorch org tutori intermedi torchvis tutori html defin dataset",
        "Question_preprocessed_content":"prepar object detction dataset deep learn pytorch similar intent try achiev export data label project dataset consum dataset notebook perform custom train test split maintain particular file group regist result train test datafram dataset us dataset train test custom object detect model need help prepar data final step familiar differ deep learn librari implement azur environ manag complet step end write data csv file upload datastor defin path train data file creat new delimit file sep index fals repeat test sep index fals datastor upload prepar data datastor upload local file datastor overwrit true creat regist train dataset datastor file separ train descript train dataset sampl label data creat regist test dataset datastor file separ test descript test dataset sampl label data approach intend us step us convert torchvis dataset work receiv follow error usererrorexcept usererrorexcept messag perform torchvis convers dataset label column defin innerexcept errorrespons suspect issu datatyp origin dataset datatyp displai comparison column type train test dataset pars string understand wai convert data type stream label list list advic prepar dataset us pytorch recommend altern approach greatli appreci updat comment current mount datafram download data size view imag origin mount dataset load newli regist train dataset access imag longer relev perform random split cluster sampl work creat class object defin dataset current creat pil imag object requir pytorch",
        "Question_gpt_summary_original":"The user is facing challenges in preparing a machine learning object detection dataset for deep learning in PyTorch or similar. They have completed steps 1 to 4, but are having difficulty with step 5, which involves converting the dataset to a Torchvision dataset. They are receiving an error message stating that the dataset does not have labeled columns defined, and suspect that the issue may be related to data types. The user is seeking advice on how to prepare the dataset for use in PyTorch or recommendations for an alternative approach. They are also facing challenges with accessing images and creating a PIL Image object as required by PyTorch.",
        "Question_gpt_summary":"user face challeng prepar machin learn object detect dataset deep learn pytorch similar complet step have difficulti step involv convert dataset torchvis dataset receiv error messag state dataset label column defin suspect issu relat data type user seek advic prepar dataset us pytorch recommend altern approach face challeng access imag creat pil imag object requir pytorch",
        "Answer_original_content":"modifi methodolog abl successfulli resolv issu follow export data label project dataset consum dataset notebook creat pytorch dataset panda datafram us panda datafram determin indic train test split base requir sampl us indic input torch util data subset split pytorch dataset train test",
        "Answer_preprocessed_content":"modifi methodolog abl successfulli resolv issu follow export data label project dataset consum dataset notebook creat pytorch dataset panda datafram us panda datafram determin indic train test split base requir sampl us indic input split pytorch dataset train test",
        "Answer_gpt_summary_original":"Solution: The user was able to resolve the issue by modifying the methodology and following the steps below:\n\n1. Export data labelling project as Dataset\n2. Consume the Dataset in the notebook by creating both a PyTorch dataset and a Pandas dataframe\n3. Use the Pandas dataframe to determine indices for the train \/ test split based on required sampling\n4. Use the indices as an input to torch.utils.data.Subset() to split the PyTorch dataset into train and test.",
        "Answer_gpt_summary":"solut user abl resolv issu modifi methodolog follow step export data label project dataset consum dataset notebook creat pytorch dataset panda datafram us panda datafram determin indic train test split base requir sampl us indic input torch util data subset split pytorch dataset train test"
    },
    {
        "Question_title":"Azure ML's web service asking for label?",
        "Question_body":"<p>I built a linear regression algorithm in Azure ML. On the &quot;Score Model&quot; module I can actually see the predictions and the rest of the features. However, when I deploy this project as a web service, the service is expecting the actual label of the data (e.g. I'm trying to predict a house's price and it asks me for the price of the house to make the prediction), which doesn't make any sense to me... What am I doing wrong? On the &quot;Train Model&quot; module I set that the label column is the HousePrice, which is what I'm trying to predict...<\/p>\n<p>This is my model:\n<a href=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kI8qu.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I tried leaving that field blank but the prediction returns null...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1476382562787,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1592644375060,
        "Question_score":4,
        "Question_view_count":1012.0,
        "Poster_id":3700322.0,
        "Poster_created_time":1401729936860,
        "Poster_last_access_time":1662997148480,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1102.0,
        "Poster_views":120.0,
        "Answer_body":"<p>The input schema (names\/types of required input) based on the location in the graph where you attach the \"Web Service Input\" module. To get the schema you want, you will need to find -- or if necessary, create -- a place in the experiment where the data has the column names\/types you desire.<\/p>\n\n<p>Consider this simple example experiment that predicts whether a field called \"income\" will be above or below $50k\/year:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/nWaN2.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When we click \"Set up web service\", the following graph is automatically generated:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/NMMpV.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Since the input dataset and \"Web service input\" modules are connected to the same port, the web service schema will perfectly match the schema of the input dataset. This is unfortunate because the input dataset contains a column called \"income\", which is what our web service is supposed to predict -- this is equivalent to the problem that you are having.<\/p>\n\n<p>To get around it, we need to create a place in our experiment graph where we've dropped the unneeded \"income\" field from the input dataset, and attach the \"Web service input\" module there:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" rel=\"nofollow\"><img src=\"https:\/\/i.stack.imgur.com\/WPeSB.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>With this arrangement, the web service only requests the features actually needed to score the model. I'm sure you can use a similar method to create a predictive experiment with whatever input schema you need for your own work.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":6722050.0,
        "Answerer_created_time":1468179475927,
        "Answerer_last_access_time":1660080085528,
        "Answerer_age":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation":795.0,
        "Answerer_views":210.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40028165",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1476730527012,
        "Question_original_content":"web servic ask label built linear regress algorithm score model modul actual predict rest featur deploi project web servic servic expect actual label data try predict hous price ask price hous predict sens wrong train model modul set label column housepric try predict model tri leav field blank predict return null",
        "Question_preprocessed_content":"web servic ask label built linear regress algorithm score model modul actual predict rest featur deploi project web servic servic expect actual label data sens wrong train model modul set label column housepric try model tri leav field blank predict return",
        "Question_gpt_summary_original":"The user built a linear regression algorithm in Azure ML and is facing challenges while deploying the project as a web service. The web service is asking for the actual label of the data, which is not making sense to the user. The user has set the label column as HousePrice in the Train Model module, but leaving the field blank results in null predictions.",
        "Question_gpt_summary":"user built linear regress algorithm face challeng deploi project web servic web servic ask actual label data make sens user user set label column housepric train model modul leav field blank result null predict",
        "Answer_original_content":"input schema name type requir input base locat graph attach web servic input modul schema want need necessari creat place experi data column name type desir consid simpl exampl experi predict field call incom year click set web servic follow graph automat gener input dataset web servic input modul connect port web servic schema perfectli match schema input dataset unfortun input dataset contain column call incom web servic suppos predict equival problem have need creat place experi graph drop unneed incom field input dataset attach web servic input modul arrang web servic request featur actual need score model sure us similar method creat predict experi input schema need work",
        "Answer_preprocessed_content":"input schema base locat graph attach web servic input modul schema want need necessari creat place experi data column desir consid simpl exampl experi predict field call incom click set web servic follow graph automat gener input dataset web servic input modul connect port web servic schema perfectli match schema input dataset unfortun input dataset contain column call incom web servic suppos predict equival problem have need creat place experi graph drop unneed incom field input dataset attach web servic input modul arrang web servic request featur actual need score model sure us similar method creat predict experi input schema need work",
        "Answer_gpt_summary_original":"Solution: The user needs to create a place in the experiment graph where the unneeded column is dropped from the input dataset and attach the \"Web service input\" module there. This will ensure that the web service only requests the features actually needed to score the model.",
        "Answer_gpt_summary":"solut user need creat place experi graph unneed column drop input dataset attach web servic input modul ensur web servic request featur actual need score model"
    },
    {
        "Question_title":"Azure ML Error: TimeSeriesImputer object has no attribute '_known_df'",
        "Question_body":"<p>Running <a href=\"http:\/\/%20https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-orange-juice-sales\/auto-ml-forecasting-orange-juice-sales.ipynb\" rel=\"nofollow noreferrer\">this orange juice sales notebook<\/a> I get the below error with the <code>.forecast()<\/code> method.<\/p>\n<h3>code<\/h3>\n<pre class=\"lang-py prettyprint-override\"><code># The featurized data, aligned to y, will also be returned.\n# This contains the assumptions that were made in the forecast\n# and helps align the forecast to the original data\ny_predictions, X_trans = fitted_model.forecast(X_test)\n<\/code><\/pre>\n<h3>Error (<a href=\"https:\/\/gist.github.com\/swanderz\/201819978b6719bbed1826a02bb2fb47\" rel=\"nofollow noreferrer\">full stacktrace<\/a>):<\/h3>\n<pre><code>**AttributeError: 'TimeSeriesImputer' object has no attribute '_known_df'**\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1593350881953,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1594142196016,
        "Question_score":1,
        "Question_view_count":256.0,
        "Poster_id":5432796.0,
        "Poster_created_time":1444548825376,
        "Poster_last_access_time":1641987792040,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":303.0,
        "Poster_views":60.0,
        "Answer_body":"<p>This is commonly fixed by upgrading to the latest SDK. You can do this by running <code>pip install --upgrade azureml-sdk[explain,automl]<\/code>.<\/p>\n<p>Thanks,\nSabina<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":12875447.0,
        "Answerer_created_time":1581378257476,
        "Answerer_last_access_time":1624564204928,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":131.0,
        "Answerer_views":14.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62623166",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1593542625630,
        "Question_original_content":"error timeseriesimput object attribut known run orang juic sale notebook error forecast method code featur data align return contain assumpt forecast help align forecast origin data predict tran fit model forecast test error stacktrac attributeerror timeseriesimput object attribut known",
        "Question_preprocessed_content":"error timeseriesimput object attribut run orang juic sale notebook error method code error",
        "Question_gpt_summary_original":"The user encountered an error while running a notebook for automated machine learning forecasting of orange juice sales using Azure ML. The error occurred with the \".forecast()\" method and was caused by an attribute error in the \"TimeSeriesImputer\" object, which had no \"_known_df\" attribute.",
        "Question_gpt_summary":"user encount error run notebook autom machin learn forecast orang juic sale error occur forecast method caus attribut error timeseriesimput object known attribut",
        "Answer_original_content":"commonli fix upgrad latest sdk run pip instal upgrad sdk explain automl thank sabina",
        "Answer_preprocessed_content":"commonli fix upgrad latest sdk run thank sabina",
        "Answer_gpt_summary_original":"Solution: The error can be fixed by upgrading to the latest SDK using the command \"pip install --upgrade azureml-sdk[explain,automl]\".",
        "Answer_gpt_summary":"solut error fix upgrad latest sdk command pip instal upgrad sdk explain automl"
    },
    {
        "Question_title":"About the end of Machine Learning Studio (classic)#2",
        "Question_body":"hello.\n\nI am currently using Machine Learning Studio (classic).\n\n'From now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic) experiments and web services. Beginning 1 December 2021, new creation of Machine Learning Studio (classic) resources will not be available.\n\nIs the following interpretation correct?\n\nThings you can't do from 1 December 2021\n-Creating a workspace for Machine Learning Studio (classic)\n-Creating a web service plan for Machine Learning Studio (classic)\n\nWhat you can do until 1 December 2021\n-Creating new Machine Learning Studio (classic) experiments\n-Creating new Machine Learning Studio (classic) trained models\n-Creating a new Machine Learning Studio (classic) web service",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1636119906810,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi, customers will not be able to create new ML Studio(classic) workspaces after Dec 1, 2021. However, customers can create or update experiments\/web services in existing workspaces until Aug 31, 2024.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/616952\/about-the-end-of-machine-learning-studio-classic2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1636209342128,
        "Question_original_content":"end machin learn studio classic hello current machin learn studio classic august continu us exist machin learn studio classic experi web servic begin decemb new creation machin learn studio classic resourc avail follow interpret correct thing decemb creat workspac machin learn studio classic creat web servic plan machin learn studio classic decemb creat new machin learn studio classic experi creat new machin learn studio classic train model creat new machin learn studio classic web servic",
        "Question_preprocessed_content":"end machin learn studio hello current machin learn studio august continu us exist machin learn studio experi web servic begin decemb new creation machin learn studio resourc avail follow interpret correct thing decemb creat workspac machin learn studio creat web servic plan machin learn studio decemb creat new machin learn studio experi creat new machin learn studio train model creat new machin learn studio web servic",
        "Question_gpt_summary_original":"The user is currently using Machine Learning Studio (classic) and has been informed that new creation of resources for this platform will not be available starting from 1 December 2021. This means that the user will not be able to create a workspace or web service plan for Machine Learning Studio (classic) after this date. However, until 1 December 2021, the user can still create new experiments, trained models, and web services using Machine Learning Studio (classic).",
        "Question_gpt_summary":"user current machin learn studio classic inform new creation resourc platform avail start decemb mean user abl creat workspac web servic plan machin learn studio classic date decemb user creat new experi train model web servic machin learn studio classic",
        "Answer_original_content":"custom abl creat new studio classic workspac dec custom creat updat experi web servic exist workspac aug kindli accept answer inform help thank",
        "Answer_preprocessed_content":"custom abl creat new studio workspac dec custom creat updat servic exist workspac aug kindli accept answer inform help thank",
        "Answer_gpt_summary_original":"Solution: The discussion mentions that users can still create or update experiments and web services in existing workspaces until August 31, 2024. However, no other solutions are provided.",
        "Answer_gpt_summary":"solut discuss mention user creat updat experi web servic exist workspac august solut provid"
    },
    {
        "Question_title":"Blankspace and colon not found in firstline",
        "Question_body":"<p>I have a jupyter notebook in SageMaker in which I want to run the XGBoost algorithm. The data has to match 3 criteria: \n-No header row\n-Outcome variable in the first column, features in the rest of the columns \n-All columns need to be numeric<\/p>\n\n<p>The error I get is the following:<\/p>\n\n<pre><code>    Error for Training job xgboost-2019-03-13-16-21-25-000: \n    Failed Reason: ClientError: Blankspace and colon not found in firstline \n'0.0,0.0,99.0,314.07,1.0,0.0,0.0,0.0,0.48027846,0.0...' of file 'train.csv'\n<\/code><\/pre>\n\n<p>In the error itself it can be seen that there are no headers, the output is the first column (it just takes 1.0 and 0.0 values) and all features are numerical. The data is stored in its own bucket. <\/p>\n\n<p>I have seen a related question in GitHub but there are no solution there. Also, the example notebook that Amazon has does not take care of change the default sep or anything when saving a dataframe to csv for using it later on. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1552497791307,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":915.0,
        "Poster_id":9621172.0,
        "Poster_created_time":1523298968403,
        "Poster_last_access_time":1663934452963,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1754.0,
        "Poster_views":197.0,
        "Answer_body":"<p>The error message indicated XGBoost was expecting the input data set as libsvm format instead of csv. SageMaker XGBoost by default assumed the input data set was in libsvm format. For using input data set in csv, please explicitly specify <code>content-type<\/code> as <code>text\/csv<\/code>.<\/p>\n\n<p>For more information: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Answerer_id":4876041.0,
        "Answerer_created_time":1431018627572,
        "Answerer_last_access_time":1553295848552,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":86.0,
        "Answerer_views":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55147861",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1553278582280,
        "Question_original_content":"blankspac colon firstlin jupyt notebook want run xgboost algorithm data match criteria header row outcom variabl column featur rest column column need numer error follow error train job xgboost fail reason clienterror blankspac colon firstlin file train csv error seen header output column take valu featur numer data store bucket seen relat question github solut exampl notebook amazon care chang default sep save datafram csv later",
        "Question_preprocessed_content":"blankspac colon firstlin jupyt notebook want run xgboost algorithm data match criteria header row outcom variabl column featur rest column column need numer error follow error seen header output column featur numer data store bucket seen relat question github solut exampl notebook amazon care chang default sep save datafram csv later",
        "Question_gpt_summary_original":"The user is encountering an error while trying to run the XGBoost algorithm in a Jupyter notebook in SageMaker. The data needs to meet specific criteria, including having no header row, the outcome variable in the first column, and all columns being numeric. The error message indicates that there are no headers, the output is in the first column, and all features are numerical. The data is stored in its own bucket, and the user has not found a solution to the problem.",
        "Question_gpt_summary":"user encount error try run xgboost algorithm jupyt notebook data need meet specif criteria includ have header row outcom variabl column column numer error messag indic header output column featur numer data store bucket user solut problem",
        "Answer_original_content":"error messag indic xgboost expect input data set libsvm format instead csv xgboost default assum input data set libsvm format input data set csv explicitli specifi content type text csv inform http doc aw amazon com latest xgboost html inputoutput xgboost",
        "Answer_preprocessed_content":"error messag indic xgboost expect input data set libsvm format instead csv xgboost default assum input data set libsvm format input data set csv explicitli specifi inform",
        "Answer_gpt_summary_original":"Solution: The error message indicated that XGBoost was expecting the input data set as libsvm format instead of csv. SageMaker XGBoost by default assumed the input data set was in libsvm format. For using input data set in csv, the user needs to explicitly specify content-type as text\/csv. For more information, the user can refer to the SageMaker XGBoost documentation.",
        "Answer_gpt_summary":"solut error messag indic xgboost expect input data set libsvm format instead csv xgboost default assum input data set libsvm format input data set csv user need explicitli specifi content type text csv inform user refer xgboost document"
    },
    {
        "Question_title":"Azure Data Lake Storage Gen2 (ADLS Gen2) as a data source for Kedro pipeline",
        "Question_body":"<p>According to Kedro's <a href=\"https:\/\/kedro.readthedocs.io\/en\/0.15.7\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\">documentation<\/a>, Azure Blob Storage is one of the available data sources. Does this extend to ADLS Gen2 ?<\/p>\n<p>Haven't tried Kedro yet, but before I invest some time on it, I wanted to make sure I could connect to ADLS Gen2.<\/p>\n<p>Thank you in advance !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1636709020827,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":350.0,
        "Poster_id":13278114.0,
        "Poster_created_time":1586517832390,
        "Poster_last_access_time":1660328393328,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":127.0,
        "Poster_views":20.0,
        "Answer_body":"<p>Yes this works with Kedro. You're actually pointing a really old version of the docs, nowadays all filesystem based datasets in Kedro use <a href=\"https:\/\/github.com\/fsspec\/filesystem_spec\" rel=\"nofollow noreferrer\">fsspec<\/a> under the hood which means they work with S3, HDFS, local and many more filesystems seamlessly.<\/p>\n<p>The ADLS Gen2 is supported by <code>ffspec<\/code> via the underlying <code>adlfs<\/code> library which is <a href=\"https:\/\/github.com\/fsspec\/adlfs\" rel=\"nofollow noreferrer\">documented here<\/a>.<\/p>\n<p>From a Kedro point of view all you need to do is declare your catalog entry like so:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code> motorbikes:\n     type: pandas.CSVDataSet\n     filepath: abfs:\/\/your_bucket\/data\/02_intermediate\/company\/motorbikes.csv\n     credentials: dev_az\n<\/code><\/pre>\n<p>We also have more examples <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/01_data_catalog.html\" rel=\"nofollow noreferrer\">here<\/a>, particularly example 15.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":2010808.0,
        "Answerer_created_time":1359113510580,
        "Answerer_last_access_time":1663614437208,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1076.0,
        "Answerer_views":81.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69940562",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1636712829167,
        "Question_original_content":"azur data lake storag gen adl gen data sourc pipelin accord document azur blob storag avail data sourc extend adl gen haven tri invest time want sure connect adl gen thank advanc",
        "Question_preprocessed_content":"azur data lake storag gen data sourc pipelin accord document azur blob storag avail data sourc extend adl gen haven tri invest time want sure connect adl gen thank advanc",
        "Question_gpt_summary_original":"The user is unsure if Azure Data Lake Storage Gen2 can be used as a data source for Kedro pipeline and is seeking clarification before investing time in it.",
        "Question_gpt_summary":"user unsur azur data lake storag gen data sourc pipelin seek clarif invest time",
        "Answer_original_content":"ye work actual point old version doc nowadai filesystem base dataset us fsspec hood mean work hdf local filesystem seamlessli adl gen support ffspec underli adlf librari document point view need declar catalog entri like motorbik type panda csvdataset filepath abf bucket data intermedi compani motorbik csv credenti dev exampl particularli exampl",
        "Answer_preprocessed_content":"ye work actual point old version doc nowadai filesystem base dataset us fsspec hood mean work hdf local filesystem seamlessli adl gen support underli librari document point view need declar catalog entri like exampl particularli exampl",
        "Answer_gpt_summary_original":"Solution: Azure Data Lake Storage Gen2 can be used as a data source for Kedro pipeline. The ADLS Gen2 is supported by `fsspec` via the underlying `adlfs` library. All filesystem based datasets in Kedro use `fsspec` under the hood which means they work with S3, HDFS, local and many more filesystems seamlessly. From a Kedro point of view, all that needs to be done is to declare the catalog entry with the appropriate filepath and credentials. More examples can be found in the Kedro documentation.",
        "Answer_gpt_summary":"solut azur data lake storag gen data sourc pipelin adl gen support fsspec underli adlf librari filesystem base dataset us fsspec hood mean work hdf local filesystem seamlessli point view need declar catalog entri appropri filepath credenti exampl document"
    },
    {
        "Question_title":"Hyperparameter tunning with wandb - CommError: Sweep user not valid when trying to initial the sweep",
        "Question_body":"<p>I'mt rying to use wandb for hyperparameter tunning as described in <a href=\"https:\/\/colab.research.google.com\/github\/wandb\/examples\/blob\/master\/colabs\/pytorch\/Organizing_Hyperparameter_Sweeps_in_PyTorch_with_W%26B.ipynb?authuser=1#scrollTo=hoAi-idR1DQk\" rel=\"nofollow noreferrer\">this notebook<\/a> (but using my dataframe and trying to do it on random forest regressor instead).<\/p>\n<p>I'm trying to initial the sweep but I get the error:<\/p>\n<pre><code>sweep_configuration = {\n    &quot;name&quot;: &quot;test-project&quot;,\n    &quot;method&quot;: &quot;random&quot;,\n    &quot;entity&quot;:&quot;my_name&quot;\u05ea\n    &quot;metric&quot;: {\n        &quot;name&quot;: &quot;loss&quot;,\n        &quot;goal&quot;: &quot;minimize&quot;\n    }\n    \n}\n\nparameters_dict = {\n    'n_estimators': {\n        'values': [100,200,300]\n        },\n    'max_depth': {\n        'values': [4,7,10,14]\n        },\n    'min_samples_split': {\n          'values': [2,4,8]\n        },\n    \n    'min_samples_leaf': {\n          'values': [2,4,8]\n        },\n    \n    \n    'max_features': {\n          'values': [1,7,10]\n        },\n\n    }\n\nsweep_configuration['parameters'] = parameters_dict\n\nsweep_id = wandb.sweep(sweep_configuration)\n\n\n<\/code><\/pre>\n<blockquote>\n<p>400 response executing GraphQL. {&quot;errors&quot;:[{&quot;message&quot;:&quot;Sweep user not\nvalid&quot;,&quot;path&quot;:[&quot;upsertSweep&quot;]}],&quot;data&quot;:{&quot;upsertSweep&quot;:null}} wandb:\nERROR Error while calling W&amp;B API: Sweep user not valid (&lt;Response\n[400]&gt;)<br \/>\nCommError: Sweep user not valid<\/p>\n<\/blockquote>\n<p>My end goal : to inital the sweep<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1655994282873,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1656234648443,
        "Question_score":1,
        "Question_view_count":182.0,
        "Poster_id":12285101.0,
        "Poster_created_time":1572256318027,
        "Poster_last_access_time":1663676968963,
        "Poster_age":null,
        "Poster_location":"Israel",
        "Poster_reputation":1387.0,
        "Poster_views":224.0,
        "Answer_body":"<p>Two things to try:<\/p>\n<ul>\n<li><p>Like in the notebook, you should pass <code>project=&quot;your-project-name&quot;<\/code> like <code>wandb.sweep(sweep_configuration, project=&quot;your-project-name&quot;)<\/code><\/p>\n<\/li>\n<li><p>Have you logged in to W&amp;B (using <code>wandb.login()<\/code>)?<\/p>\n<\/li>\n<\/ul>\n<p>Finally, once you've successfully created the sweep, you should pass the <code>sweep_id<\/code> and your function (here <code>train<\/code>) like:\n<code>wandb.agent(sweep_id, train, count=5)<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":3959708.0,
        "Answerer_created_time":1408529239847,
        "Answerer_last_access_time":1663674307907,
        "Answerer_age":null,
        "Answerer_location":"Dublin, Ireland",
        "Answerer_reputation":1652.0,
        "Answerer_views":293.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72731861",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_closed_time":1656329528480,
        "Question_original_content":"hyperparamet tun commerror sweep user valid try initi sweep ry us hyperparamet tun describ notebook datafram try random forest regressor instead try initi sweep error sweep configur test project method random entiti metric loss goal minim paramet dict estim valu max depth valu min sampl split valu min sampl leaf valu max featur valu sweep configur paramet paramet dict sweep sweep sweep configur respons execut graphql error messag sweep user valid path upsertsweep data upsertsweep null error error call api sweep user valid commerror sweep user valid end goal init sweep",
        "Question_preprocessed_content":"hyperparamet tun commerror sweep user valid try initi sweep ry us hyperparamet tun describ notebook try initi sweep error respons execut graphql data upsertsweep null error error call api sweep user valid commerror sweep user valid end goal init sweep",
        "Question_gpt_summary_original":"The user is encountering a challenge while trying to initialize a hyperparameter sweep using wandb. The error message \"Sweep user not valid\" is displayed while executing GraphQL, preventing the user from initializing the sweep. The user's end goal is to successfully initialize the sweep.",
        "Question_gpt_summary":"user encount challeng try initi hyperparamet sweep error messag sweep user valid displai execut graphql prevent user initi sweep user end goal successfulli initi sweep",
        "Answer_original_content":"thing try like notebook pass project project like sweep sweep configur project project log login final successfulli creat sweep pass sweep function train like agent sweep train count",
        "Answer_preprocessed_content":"thing try like notebook pass like log final successfulli creat sweep pass function like",
        "Answer_gpt_summary_original":"Two possible solutions were suggested in the discussion. The first solution is to pass the project name while initializing the sweep using wandb. The second solution is to ensure that the user is logged in to W&B using wandb.login(). Once the sweep is successfully created, the user should pass the sweep_id and the function to wandb.agent().",
        "Answer_gpt_summary":"possibl solut suggest discuss solut pass project initi sweep second solut ensur user log login sweep successfulli creat user pass sweep function agent"
    },
    {
        "Question_title":"Update SageMaker Jupyterlab environment",
        "Question_body":"<p>How can I update my SageMaker notebook's jupyter environment to the latest alpha release and then restart the process?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1554750635030,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":4,
        "Question_view_count":1720.0,
        "Poster_id":2089899.0,
        "Poster_created_time":1361339272692,
        "Poster_last_access_time":1663965928400,
        "Poster_age":null,
        "Poster_location":"NYC",
        "Poster_reputation":6281.0,
        "Poster_views":958.0,
        "Answer_body":"<p>Hi and thank you for using SageMaker!<\/p>\n\n<p>To restart Jupyter from within a SageMaker Notebook Instance, you can issue the following command: <code>sudo initctl restart jupyter-server --no-wait<\/code>.<\/p>\n\n<p>Best,\nKevin<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Answerer_id":977443.0,
        "Answerer_created_time":1317676233236,
        "Answerer_last_access_time":1650916041003,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation":2348.0,
        "Answerer_views":206.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55580232",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1555007316016,
        "Question_original_content":"updat jupyterlab environ updat notebook jupyt environ latest alpha releas restart process",
        "Question_preprocessed_content":"updat jupyterlab environ updat notebook jupyt environ latest alpha releas restart process",
        "Question_gpt_summary_original":"The user is facing a challenge in updating their SageMaker notebook's Jupyter environment to the latest alpha release and restarting the process.",
        "Question_gpt_summary":"user face challeng updat notebook jupyt environ latest alpha releas restart process",
        "Answer_original_content":"thank restart jupyt notebook instanc issu follow command sudo initctl restart jupyt server wait best kevin",
        "Answer_preprocessed_content":"thank restart jupyt notebook instanc issu follow command best kevin",
        "Answer_gpt_summary_original":"Solution:\n- To restart Jupyter from within a SageMaker Notebook Instance, the user can issue the following command: <code>sudo initctl restart jupyter-server --no-wait<\/code>.",
        "Answer_gpt_summary":"solut restart jupyt notebook instanc user issu follow command sudo initctl restart jupyt server wait"
    },
    {
        "Question_title":"How to select a target column in a Vertex AI AutoML time series model",
        "Question_body":"<p>I am testing out Google Cloud Vertex AI with a time series AutoML model.<\/p>\n<p>I have created a dataset, from a Biguery table, with 2 columns, one of a timestamp and another of a numeric value I want to predict:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><code>salesorderdate<\/code> is my <code>TIMESTAMP<\/code> column and <code>orders<\/code> is the value I want to predict.<\/p>\n<p>When I proceed to the next step I cannot select <code>orders<\/code> as my value to predict, there are no available options for this field:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/HOed3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HOed3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I missing here? Surely the time series value <em>is<\/em> the target value in this case? Is there an expectation of more fields here, and can one actually add additional features as columns to a time series model in this way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1624627510520,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1624971874476,
        "Question_score":1,
        "Question_view_count":606.0,
        "Poster_id":7813005.0,
        "Poster_created_time":1491294824967,
        "Poster_last_access_time":1663926174400,
        "Poster_age":null,
        "Poster_location":"Herefordshire, UK",
        "Poster_reputation":3107.0,
        "Poster_views":434.0,
        "Answer_body":"<p>I guess from your question that you are using &quot;forecasting models&quot;. Please note that it is in &quot;Preview&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a> with all consequences of that fact.<\/p>\n<p>In the documentation you may find <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-tabular#data-structure\" rel=\"nofollow noreferrer\">Training data structure<\/a> following information:<\/p>\n<blockquote>\n<ul>\n<li>There must be at least two and no more than 1,000 columns.<\/li>\n<\/ul>\n<p>For datasets that train AutoML models, one column must be the target,\nand there must be at least one feature available to train the model.\nIf the training data does not include the target column, Vertex AI\ncannot associate the training data with the desired result.<\/p>\n<\/blockquote>\n<p>I suppose you are using AutoML models so in this situation you need to have 3 columns in the data set:<\/p>\n<ul>\n<li>Time column - used to place the observation represented by that row in time<\/li>\n<li>time series identifier column as &quot;Forecasting training data usually includes multiple time series&quot;<\/li>\n<li>and target column which is value that model should learn to predict.<\/li>\n<\/ul>\n<p>If you want to predict <code>orders<\/code> this should be target column. But before you are choosing this target this &quot;time series identifier column&quot; is already chosen in previous step, so you do not have available column to choose.<\/p>\n<p>So you need to add to your BigQuery table at least one additional column with will be used as time series column. You can add to your data set column with the same value in each row. This concept is presented in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">Forecasting data preparation best practices<\/a>:<\/p>\n<blockquote>\n<p>You can train a forecasting model on a single time series (in other\nwords, the time series identifier column contains the same value for\nall rows). However, Vertex AI is a better fit for training data that\ncontains two or more time series. For best results, you should have at\nleast 10 time series for every column used to train the model.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1624960598496,
        "Answer_score":1.0,
        "Answerer_id":12410332.0,
        "Answerer_created_time":1574340010727,
        "Answerer_last_access_time":1658996767036,
        "Answerer_age":null,
        "Answerer_location":"Krak\u00f3w, Poland",
        "Answerer_reputation":3988.0,
        "Answerer_views":615.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68131742",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1624889338667,
        "Question_original_content":"select target column automl time seri model test googl cloud time seri automl model creat dataset bigueri tabl column timestamp numer valu want predict salesorderd timestamp column order valu want predict proce step select order valu predict avail option field miss sure time seri valu target valu case expect field actual add addit featur column time seri model wai",
        "Question_preprocessed_content":"select target column automl time seri model test googl cloud time seri automl model creat dataset bigueri tabl column timestamp numer valu want predict column valu want predict proce step select valu predict avail option field miss sure time seri valu target valu case expect field actual add addit featur column time seri model wai",
        "Question_gpt_summary_original":"The user is facing challenges in selecting a target column in a Vertex AI AutoML time series model. They have created a dataset with a timestamp and a numeric value they want to predict, but when they proceed to the next step, they cannot select the value they want to predict. The user is unsure if they are missing something and if they can add additional features as columns to a time series model.",
        "Question_gpt_summary":"user face challeng select target column automl time seri model creat dataset timestamp numer valu want predict proce step select valu want predict user unsur miss add addit featur column time seri model",
        "Answer_original_content":"guess question forecast model note preview product launch stage consequ fact document train data structur follow inform column dataset train automl model column target featur avail train model train data includ target column associ train data desir result suppos automl model situat need column data set time column place observ repres row time time seri identifi column forecast train data usual includ multipl time seri target column valu model learn predict want predict order target column choos target time seri identifi column chosen previou step avail column choos need add bigqueri tabl addit column time seri column add data set column valu row concept present forecast data prepar best practic train forecast model singl time seri word time seri identifi column contain valu row better fit train data contain time seri best result time seri column train model",
        "Answer_preprocessed_content":"guess question forecast model note preview product launch stage consequ fact document train data structur follow inform column dataset train automl model column target featur avail train model train data includ target column associ train data desir result suppos automl model situat need column data set time column place observ repres row time time seri identifi column forecast train data usual includ multipl time seri target column valu model learn predict want predict target column choos target time seri identifi column chosen previou step avail column choos need add bigqueri tabl addit column time seri column add data set column valu row concept present forecast data prepar best practic train forecast model singl time seri better fit train data contain time seri best result time seri column train model",
        "Answer_gpt_summary_original":"Solution:\n- The user needs to have at least three columns in the dataset for AutoML time series models: a time column, a time series identifier column, and a target column.\n- If the user wants to predict a specific value, that value should be the target column.\n- The time series identifier column is already chosen in the previous step, so the user needs to add at least one additional column to the BigQuery table to use as the time series column.\n- The user can add a column with the same value in each row to use as the time series column.\n- It is recommended to have at least 10 time series for every column used to train the model for best results.",
        "Answer_gpt_summary":"solut user need column dataset automl time seri model time column time seri identifi column target column user want predict specif valu valu target column time seri identifi column chosen previou step user need add addit column bigqueri tabl us time seri column user add column valu row us time seri column recommend time seri column train model best result"
    },
    {
        "Question_title":"Saving data with DataCatalog",
        "Question_body":"<p>I was looking at <code>iris<\/code> project example provided by kedro. Apart from logging the accuracy I also wanted to save the <code>predictions<\/code> and <code>test_y<\/code> as a csv.<\/p>\n<p>This is the example node provided by kedro.<\/p>\n<pre><code>def report_accuracy(predictions: np.ndarray, test_y: pd.DataFrame) -&gt; None:\n    &quot;&quot;&quot;Node for reporting the accuracy of the predictions performed by the\n    previous node. Notice that this function has no outputs, except logging.\n    &quot;&quot;&quot;\n    # Get true class index\n    target = np.argmax(test_y.to_numpy(), axis=1)\n    # Calculate accuracy of predictions\n    accuracy = np.sum(predictions == target) \/ target.shape[0]\n    # Log the accuracy of the model\n    log = logging.getLogger(__name__)\n    log.info(&quot;Model accuracy on test set: %0.2f%%&quot;, accuracy * 100)\n<\/code><\/pre>\n<p>I added the following to save the data.<\/p>\n<pre><code>data = pd.DataFrame({&quot;target&quot;: target , &quot;prediction&quot;: predictions})\ndata_set = CSVDataSet(filepath=&quot;data\/test.csv&quot;)\ndata_set.save(data)\n<\/code><\/pre>\n<p>This works as intended, however, my question is &quot;is it the kedro way of doing thing&quot; ? Can I provide the <code>data_set <\/code> in <code>catalog.yml<\/code> and later save <code>data<\/code> to it? If I want to do it, how do I access the <code>data_set<\/code> from <code>catalog.yml<\/code> inside a node.<\/p>\n<p>Is there a way to save data without creating a catalog inside a node like this <code>data_set = CSVDataSet(filepath=&quot;data\/test.csv&quot;)<\/code> ? I want this in <code>catalog.yml<\/code>, if possible and if it follows kedro convention!.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1629897723887,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1629897818943,
        "Question_score":0,
        "Question_view_count":333.0,
        "Poster_id":9417884.0,
        "Poster_created_time":1519724643532,
        "Poster_last_access_time":1646603612996,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":453.0,
        "Poster_views":79.0,
        "Answer_body":"<p>Kedro actually abstracts this part for you. You don't need to access the datasets via their Python API.<\/p>\n<p>Your <code>report_accuracy<\/code> method does need to be tweaked to return the <code>DataFrame<\/code> instead of <code>None<\/code>.<\/p>\n<p>Your node needs to be defined as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>node(\n  func=report_accuracy,\n  inputs='dataset_a',\n  outputs='dataset_b'\n)\n<\/code><\/pre>\n<p>Kedro then looks at your catalog and will load\/save <code>dataset_a<\/code> and <code>dataset_b<\/code> as required:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset_a:\n   type: pandas.CSVDataSet\n   path: xxxx.csv\n\ndataset_b:\n   type: pandas.ParquetDataSet\n   path: yyyy.pq\n<\/code><\/pre>\n<p>As you run the node\/pipeline Kedro will handle the load\/save operations for you. You also don't need to save every dataset if it's only used mid-way in a pipeline, you can read about <code>MemoryDataSet<\/code>s <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/11_tools_integration\/01_pyspark.html#use-memorydataset-for-intermediary-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1643038306147,
        "Answer_score":7.0,
        "Answerer_id":2010808.0,
        "Answerer_created_time":1359113510580,
        "Answerer_last_access_time":1663614437208,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1076.0,
        "Answerer_views":81.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68923747",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1629898742263,
        "Question_original_content":"save data datacatalog look iri project exampl provid apart log accuraci want save predict test csv exampl node provid def report accuraci predict ndarrai test datafram node report accuraci predict perform previou node notic function output log true class index target argmax test numpi axi calcul accuraci predict accuraci sum predict target target shape log accuraci model log log getlogg log info model accuraci test set accuraci ad follow save data data datafram target target predict predict data set csvdataset filepath data test csv data set save data work intend question wai thing provid data set catalog yml later save data want access data set catalog yml insid node wai save data creat catalog insid node like data set csvdataset filepath data test csv want catalog yml possibl follow convent",
        "Question_preprocessed_content":"save data datacatalog look project exampl provid apart log accuraci want save csv exampl node provid ad follow save data work intend question wai thing provid later save want access insid node wai save data creat catalog insid node like want possibl follow convent",
        "Question_gpt_summary_original":"The user encountered a challenge in saving data in the Kedro project example provided by Kedro. They were able to save the data as intended, but they are unsure if it is the \"Kedro way\" of doing things. They are looking for a way to provide the data set in catalog.yml and access it from a node, without creating a catalog inside a node.",
        "Question_gpt_summary":"user encount challeng save data project exampl provid abl save data intend unsur wai thing look wai provid data set catalog yml access node creat catalog insid node",
        "Answer_original_content":"actual abstract need access dataset python api report accuraci method need tweak return datafram instead node need defin node func report accuraci input dataset output dataset look catalog load save dataset dataset requir dataset type panda csvdataset path csv dataset type panda parquetdataset path run node pipelin handl load save oper need save dataset mid wai pipelin read memorydataset",
        "Answer_preprocessed_content":"actual abstract need access dataset python api method need tweak return instead node need defin look catalog requir run handl oper need save dataset pipelin read",
        "Answer_gpt_summary_original":"Solution: The user can define their node with inputs and outputs in the catalog.yml file and Kedro will handle the load\/save operations for them. They can also use MemoryDataSet for intermediary dataframes if the dataset is only used mid-way in a pipeline.",
        "Answer_gpt_summary":"solut user defin node input output catalog yml file handl load save oper us memorydataset intermediari datafram dataset mid wai pipelin"
    },
    {
        "Question_title":"MLFlow tracking ui not showing experiments on local machine (laptop)",
        "Question_body":"<p>I am a beginner in mlflow and was trying to set it up locally using Anaconda 3.\nI have created a new environment in anaconda and install mlflow and sklearn in it. Now I am using jupyter notebook to run my sample code for mlflow.<\/p>\n<p>'''<\/p>\n<pre><code>import os\nimport warnings\nimport sys\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import ElasticNet\nfrom urllib.parse import urlparse\nimport mlflow\nimport mlflow.sklearn\n\nimport logging\n\nlogging.basicConfig(level=logging.WARN)\nlogger = logging.getLogger(__name__)\n\nwarnings.filterwarnings(&quot;ignore&quot;)\nnp.random.seed(40)\n\n\nmlflow.set_tracking_uri(&quot;file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun&quot;)\n\nmlflow.get_tracking_uri()\n\nmlflow.get_experiment\n\n#experiment_id = mlflow.create_experiment(&quot;Mlflow_demo&quot;)\nexperiment_id = mlflow.create_experiment(&quot;Demo3&quot;)\nexperiment = mlflow.get_experiment(experiment_id)\nprint(&quot;Name: {}&quot;.format(experiment.name))\nprint(&quot;Experiment_id: {}&quot;.format(experiment.experiment_id))\nprint(&quot;Artifact Location: {}&quot;.format(experiment.artifact_location))\nprint(&quot;Tags: {}&quot;.format(experiment.tags))\nprint(&quot;Lifecycle_stage: {}&quot;.format(experiment.lifecycle_stage))\n\nmlflow.set_experiment(&quot;Demo3&quot;)\n\ndef eval_metrics(actual, pred):\n    rmse = np.sqrt(mean_squared_error(actual, pred))\n    mae = mean_absolute_error(actual, pred)\n    r2 = r2_score(actual, pred)\n    return rmse, mae, r2\n\n# Read the wine-quality csv file from the URL\ncsv_url =\\\n    'http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine-quality\/winequality-red.csv'\ntry:\n    data = pd.read_csv(csv_url, sep=';')\nexcept Exception as e:\n    logger.exception(\n        &quot;Unable to download training &amp; test CSV, check your internet connection. Error: %s&quot;, e)\n\ndata.head(2)\n\n\ndef train_model(data, alpha, l1_ratio):\n    \n    # Split the data into training and test sets. (0.75, 0.25) split.\n    train, test = train_test_split(data)\n\n    # The predicted column is &quot;quality&quot; which is a scalar from [3, 9]\n    train_x = train.drop([&quot;quality&quot;], axis=1)\n    test_x = test.drop([&quot;quality&quot;], axis=1)\n    train_y = train[[&quot;quality&quot;]]\n    test_y = test[[&quot;quality&quot;]]\n\n    # Set default values if no alpha is provided\n    alpha = alpha\n    l1_ratio = l1_ratio\n\n\n    # Execute ElasticNet\n    lr = ElasticNet(alpha=alpha, l1_ratio=l1_ratio, random_state=42)\n    lr.fit(train_x, train_y)\n\n    # Evaluate Metrics\n    predicted_qualities = lr.predict(test_x)\n    (rmse, mae, r2) = eval_metrics(test_y, predicted_qualities)\n\n    # Print out metrics\n    print(&quot;Elasticnet model (alpha=%f, l1_ratio=%f):&quot; % (alpha, l1_ratio))\n    print(&quot;  RMSE: %s&quot; % rmse)\n    print(&quot;  MAE: %s&quot; % mae)\n    print(&quot;  R2: %s&quot; % r2)\n    \n    # Log parameter, metrics, and model to MLflow\n    with mlflow.start_run(experiment_id = experiment_id):\n        mlflow.log_param(&quot;alpha&quot;, alpha)\n        mlflow.log_param(&quot;l1_ratio&quot;, l1_ratio)\n        mlflow.log_metric(&quot;rmse&quot;, rmse)\n        mlflow.log_metric(&quot;r2&quot;, r2)\n        mlflow.log_metric(&quot;mae&quot;, mae)\n        mlflow.sklearn.log_model(lr, &quot;model&quot;)\n        \n\ntrain_model(data, 0.5, 0.5)\n\ntrain_model(data, 0.5, 0.3)\n\ntrain_model(data, 0.4, 0.3)\n<\/code><\/pre>\n<p>'''<\/p>\n<p>using above code, I am successfully able to create 3 different experiment as I can see the folders created in my local directory as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKqgX.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Now, I am trying to run the mlflow ui using the jupyter terminal in my chrome browser and I am able to open the mlflow ui but cannot see and experiments as shown below:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6KaQK.png\" rel=\"nofollow noreferrer\">enter image description here<\/a><\/p>\n<p>Could you help me in finding where I am going wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1648821712310,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3,
        "Question_view_count":936.0,
        "Poster_id":18664602.0,
        "Poster_created_time":1648820658172,
        "Poster_last_access_time":1663932565808,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":33.0,
        "Poster_views":0.0,
        "Answer_body":"<p>Where do you run <code>mlflow ui<\/code> command?<\/p>\n<p>I think if you pass tracking ui path in the arguments, it would work:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>mlflow ui --backend-store-uri file:\/\/\/Users\/Swapnil\/Documents\/LocalPython\/MLFLowDemo\/mlrun\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1659992652860,
        "Answer_score":3.0,
        "Answerer_id":11232272.0,
        "Answerer_created_time":1553088438367,
        "Answerer_last_access_time":1664054397820,
        "Answerer_age":null,
        "Answerer_location":"Tehran, Tehran Province, Iran",
        "Answerer_reputation":415.0,
        "Answerer_views":37.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71708147",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1650761211590,
        "Question_original_content":"track show experi local machin laptop beginn try set local anaconda creat new environ anaconda instal sklearn jupyt notebook run sampl code import import warn import sy import panda import numpi sklearn metric import mean squar error mean absolut error score sklearn model select import train test split sklearn linear model import elasticnet urllib pars import urlpars import import sklearn import log log basicconfig level log warn logger log getlogg warn filterwarn ignor random seed set track uri file user swapnil document localpython demo mlrun track uri experi experi creat experi demo experi creat experi demo experi experi experi print format experi print experi format experi experi print artifact locat format experi artifact locat print tag format experi tag print lifecycl stage format experi lifecycl stage set experi demo def eval metric actual pred rmse sqrt mean squar error actual pred mae mean absolut error actual pred score actual pred return rmse mae read wine qualiti csv file url csv url http archiv ic uci edu machin learn databas wine qualiti winequ red csv try data read csv csv url sep except logger except unabl download train test csv check internet connect error data head def train model data alpha ratio split data train test set split train test train test split data predict column qualiti scalar train train drop qualiti axi test test drop qualiti axi train train qualiti test test qualiti set default valu alpha provid alpha alpha ratio ratio execut elasticnet elasticnet alpha alpha ratio ratio random state fit train train evalu metric predict qualiti predict test rmse mae eval metric test predict qualiti print metric print elasticnet model alpha ratio alpha ratio print rmse rmse print mae mae print log paramet metric model start run experi experi log param alpha alpha log param ratio ratio log metric rmse rmse log metric log metric mae mae sklearn log model model train model data train model data train model data code successfulli abl creat differ experi folder creat local directori shown enter imag descript try run jupyt termin chrome browser abl open experi shown enter imag descript help find go wrong",
        "Question_preprocessed_content":"track show experi local machin beginn try set local anaconda creat new environ anaconda instal sklearn jupyt notebook run sampl code code successfulli abl creat differ experi folder creat local directori shown enter imag descript try run jupyt termin chrome browser abl open experi shown enter imag descript help find go wrong",
        "Question_gpt_summary_original":"The user is a beginner in MLFlow and has set it up locally using Anaconda 3. They have created a new environment in Anaconda and installed MLFlow and Sklearn in it. The user has successfully created three different experiments using Jupyter Notebook, but when trying to run the MLFlow UI using the Jupyter terminal in their Chrome browser, they are unable to see any experiments. The user is seeking help in finding where they are going wrong.",
        "Question_gpt_summary":"user beginn set local anaconda creat new environ anaconda instal sklearn user successfulli creat differ experi jupyt notebook try run jupyt termin chrome browser unabl experi user seek help find go wrong",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"How to create Sagemaker studio project using aws cdk",
        "Question_body":"<p>I am trying to create sagemaker studio project using aws cdk following below steps:<\/p>\n<p>create domain (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate user (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate jupyter app\ncreate project<\/p>\n<p>Code for creating jupyter app:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             app_name: str,\n             app_type: str,\n             domain_id: str,\n             user_profile_name: str,\n             depends_on=None, **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_jupyter_app = sg.CfnApp(self, construct_id,\n                                      app_name=app_name,\n                                      app_type=app_type,\n                                      domain_id=domain_id,\n                                      user_profile_name=user_profile_name\n                                      )\n    sagemaker_jupyter_app.add_depends_on(depends_on_user_creation)\n<\/code><\/pre>\n<p>Code for creating project:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             project_name: str,\n             project_description: str,\n             product_id: str,\n             depends_on=None,\n             **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_studio_project = sg.CfnProject(self, construct_id,\n                                             project_name=project_name,\n                                             service_catalog_provisioning_details={\n                                                 &quot;ProductId&quot;: &quot;prod-7tjedn5dz4jrw&quot;\n                                             },\n                                             project_description=project_description\n                                             )\n<\/code><\/pre>\n<p>Domain, user, jupyter app all gets created successfully. The problem comes in with project.\nBelow is the error :<\/p>\n<blockquote>\n<p>Resource handler returned message: &quot;Product prod-7tjedn5dz4jrw does\nnot exist or access was denied (Service: SageMaker, Status Code: 400,\nRequest ID: 768116aa-e77b-4691-a972-38b83093fdc4)&quot; (RequestToken:\n45ca2a0c-3f03-e3e0-f29d-d9443ff4dfc1, HandlerErrorCode:\nGeneralServiceException)<\/p>\n<\/blockquote>\n<p>I am running this code from an ec2 instance that has SagemakerFullAccess\nI also tried attaching SagemakerFullAccess execution role with project...but got the same error.\nI have also attached below policy to my domain:<\/p>\n<ul>\n<li>AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1650541371883,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":491.0,
        "Poster_id":10214532.0,
        "Poster_created_time":1534058291092,
        "Poster_last_access_time":1663146934510,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":116.0,
        "Poster_views":33.0,
        "Answer_body":"<p>Basically this was an issue related to IAM.\nRunning cdk program requires bootstrapping it using the command <code>cdk bootstrap<\/code>\nAfter running this command cdk was creating a bunch of roles out of which one role will be related to cloudformation's execution role. Something like<\/p>\n<blockquote>\n<p>cdk-serialnumber-cfn-exec-role-Id-region<\/p>\n<\/blockquote>\n<p>Now this role was used by cloudformation to run the stack.<\/p>\n<p>Using sagemaker from console automatically adds the role associated with domain\/user at<\/p>\n<blockquote>\n<p>ServiceCatalog -&gt; Portfolios -&gt; Imported -&gt; Amazon SageMaker Solutions and ML Ops products -&gt; Groups, roles, and users<\/p>\n<\/blockquote>\n<p>Thats was the reason why product id was accessible from console.<\/p>\n<p>After adding the role created by cdk bootsrap to the above path I was able to run my stack.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":10214532.0,
        "Answerer_created_time":1534058291092,
        "Answerer_last_access_time":1663146934510,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":116.0,
        "Answerer_views":33.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71953876",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1650740870208,
        "Question_original_content":"creat studio project aw cdk try creat studio project aw cdk follow step creat domain exampl creat user exampl creat jupyt app creat project code creat jupyt app def init self scope construct construct str app str app type str domain str user profil str depend kwarg super init scope construct jupyt app cfnapp self construct app app app type app type domain domain user profil user profil jupyt app add depend depend user creation code creat project def init self scope construct construct str project str project descript str product str depend kwarg super init scope construct studio project cfnproject self construct project project servic catalog provis detail productid prod tjedndzjrw project descript project descript domain user jupyt app get creat successfulli problem come project error resourc handler return messag product prod tjedndzjrw exist access deni servic statu code request bfdc requesttoken caac dffdfc handlererrorcod generalserviceexcept run code instanc fullaccess tri attach fullaccess execut role project got error attach polici domain amazonadmin servicecatalogproductsservicerolepolici",
        "Question_preprocessed_content":"creat studio project aw cdk try creat studio project aw cdk follow step creat domain creat user creat jupyt app creat project code creat jupyt app code creat project domain user jupyt app get creat successfulli problem come project error resourc handler return messag product exist access deni run code instanc fullaccess tri attach fullaccess execut role got error attach polici domain",
        "Question_gpt_summary_original":"The user is encountering an error while trying to create a Sagemaker studio project using AWS CDK. The domain, user, and Jupyter app are created successfully, but the project creation fails with an error message stating that the product does not exist or access was denied. The user has tried running the code from an EC2 instance with SagemakerFullAccess and attaching the SagemakerFullAccess execution role to the project, but the error persists. The user has also attached the AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy to the domain.",
        "Question_gpt_summary":"user encount error try creat studio project aw cdk domain user jupyt app creat successfulli project creation fail error messag state product exist access deni user tri run code instanc fullaccess attach fullaccess execut role project error persist user attach amazonadmin servicecatalogproductsservicerolepolici domain",
        "Answer_original_content":"basic issu relat iam run cdk program requir bootstrap command cdk bootstrap run command cdk creat bunch role role relat cloudform execut role like cdk serialnumb cfn exec role region role cloudform run stack consol automat add role associ domain user servicecatalog portfolio import solut op product group role user that reason product access consol ad role creat cdk bootsrap path abl run stack",
        "Answer_preprocessed_content":"basic issu relat iam run cdk program requir bootstrap command run command cdk creat bunch role role relat cloudform execut role like role cloudform run stack consol automat add role associ servicecatalog portfolio import solut op product group role user that reason product access consol ad role creat cdk bootsrap path abl run stack",
        "Answer_gpt_summary_original":"Solution: The issue was related to IAM. Running the CDK program requires bootstrapping it using the command \"cdk bootstrap\". After running this command, CDK creates a bunch of roles, out of which one role will be related to CloudFormation's execution role. This role is used by CloudFormation to run the stack. Using SageMaker from the console automatically adds the role associated with the domain\/user at \"ServiceCatalog -> Portfolios -> Imported -> Amazon SageMaker Solutions and ML Ops products -> Groups, roles, and users\". After adding the role created by CDK bootstrap to the above path, the user was able to run the stack.",
        "Answer_gpt_summary":"solut issu relat iam run cdk program requir bootstrap command cdk bootstrap run command cdk creat bunch role role relat cloudform execut role role cloudform run stack consol automat add role associ domain user servicecatalog portfolio import solut op product group role user ad role creat cdk bootstrap path user abl run stack"
    },
    {
        "Question_title":"Azure Recommendations API's Parameter",
        "Question_body":"<p>I would like to make a recommendation model using Recommendations API on Azure MS Cognitive Services. I can't understand three API's parameters below for \"Create\/Trigger a build.\" What do these parameters mean?<\/p>\n\n<p><a href=\"https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0\" rel=\"nofollow\">https:\/\/westus.dev.cognitive.microsoft.com\/docs\/services\/Recommendations.V4.0\/operations\/56f30d77eda5650db055a3d0<\/a><\/p>\n\n<blockquote>\n  <p>EnableModelingInsights<br> Allows you to compute metrics on the\n  recommendation model. <br> Valid Values: True\/False<\/p>\n  \n  <p>AllowColdItemPlacement<br> Indicates if the recommendation should also\n  push cold items via feature similarity. <br> Valid Values: True\/False<\/p>\n  \n  <p>ReasoningFeatureList<br> Comma-separated list of feature names to be\n  used for reasoning sentences (e.g. recommendation explanations).<br>\n  Valid Values: Feature names, up to 512 chars<\/p>\n<\/blockquote>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1459942893017,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":279.0,
        "Poster_id":6166347.0,
        "Poster_created_time":1459941581603,
        "Poster_last_access_time":1469314422527,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":23.0,
        "Poster_views":0.0,
        "Answer_body":"<p>That page is missing references to content mentioned at other locations.  See this page for a more complete guide...<\/p>\n\n<p><a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/machine-learning-recommendation-api-documentation\/<\/a><\/p>\n\n<p>It describes Cold Items in the Rank Build section in the document as...<\/p>\n\n<p>Features can enhance the recommendation model, but to do so requires the use of meaningful features. For this purpose a new build was introduced - a rank build. This build will rank the usefulness of features. A meaningful feature is a feature with a rank score of 2 and up. After understanding which of the features are meaningful, trigger a recommendation build with the list (or sublist) of meaningful features. It is possible to use these feature for the enhancement of both warm items and cold items. In order to use them for warm items, the UseFeatureInModel build parameter should be set up. In order to use features for cold items, the AllowColdItemPlacement build parameter should be enabled. Note: It is not possible to enable AllowColdItemPlacement without enabling UseFeatureInModel.<\/p>\n\n<p>It also describes the ReasoningFeatureList in the Recommendation Reasoning section as...<\/p>\n\n<p>Recommendation reasoning is another aspect of feature usage. Indeed, the Azure Machine Learning Recommendations engine can use features to provide recommendation explanations (a.k.a. reasoning), leading to more confidence in the recommended item from the recommendation consumer. To enable reasoning, the AllowFeatureCorrelation and ReasoningFeatureList parameters should be setup prior to requesting a recommendation build.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":6195563.0,
        "Answerer_created_time":1460494806016,
        "Answerer_last_access_time":1480536797910,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":46.0,
        "Answerer_views":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36450108",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1460495281968,
        "Question_original_content":"azur recommend api paramet like recommend model recommend api azur cognit servic understand api paramet creat trigger build paramet mean http westu dev cognit microsoft com doc servic recommend oper fdedadbad enablemodelinginsight allow comput metric recommend model valid valu true fals allowcolditemplac indic recommend push cold item featur similar valid valu true fals reasoningfeaturelist comma separ list featur name reason sentenc recommend explan valid valu featur name char thank",
        "Question_preprocessed_content":"azur recommend api paramet like recommend model recommend api azur cognit servic understand api paramet paramet mean enablemodelinginsight allow comput metric recommend model valid valu allowcolditemplac indic recommend push cold item featur similar valid valu reasoningfeaturelist list featur name reason sentenc valid valu featur name char thank",
        "Question_gpt_summary_original":"The user is facing challenges in understanding three parameters of the Azure Recommendations API, specifically \"EnableModelingInsights,\" \"AllowColdItemPlacement,\" and \"ReasoningFeatureList.\" The user is seeking clarification on the meaning and valid values of these parameters.",
        "Question_gpt_summary":"user face challeng understand paramet azur recommend api specif enablemodelinginsight allowcolditemplac reasoningfeaturelist user seek clarif mean valid valu paramet",
        "Answer_original_content":"page miss refer content mention locat page complet guid http azur microsoft com document articl machin learn recommend api document describ cold item rank build section document featur enhanc recommend model requir us meaning featur purpos new build introduc rank build build rank us featur meaning featur featur rank score understand featur meaning trigger recommend build list sublist meaning featur possibl us featur enhanc warm item cold item order us warm item usefeatureinmodel build paramet set order us featur cold item allowcolditemplac build paramet enabl note possibl enabl allowcolditemplac enabl usefeatureinmodel describ reasoningfeaturelist recommend reason section recommend reason aspect featur usag recommend engin us featur provid recommend explan reason lead confid recommend item recommend consum enabl reason allowfeaturecorrel reasoningfeaturelist paramet setup prior request recommend build",
        "Answer_preprocessed_content":"page miss refer content mention locat page complet describ cold item rank build section document featur enhanc recommend model requir us meaning featur purpos new build introduc rank build build rank us featur meaning featur featur rank score understand featur meaning trigger recommend build list meaning featur possibl us featur enhanc warm item cold item order us warm item usefeatureinmodel build paramet set order us featur cold item allowcolditemplac build paramet enabl note possibl enabl allowcolditemplac enabl usefeatureinmodel describ reasoningfeaturelist recommend reason section recommend reason aspect featur usag recommend engin us featur provid recommend explan lead confid recommend item recommend consum enabl reason allowfeaturecorrel reasoningfeaturelist paramet setup prior request recommend build",
        "Answer_gpt_summary_original":"Solution:\n- The \"AllowColdItemPlacement\" parameter is used to enable the use of features for cold items in the recommendation model. It can only be enabled if the \"UseFeatureInModel\" parameter is also enabled.\n- The \"EnableModelingInsights\" parameter is used to enable the collection of data about the recommendation model's performance and usage.\n- The \"ReasoningFeatureList\" parameter is used to enable the recommendation engine to provide reasoning or explanation for the recommended item. It should be set up along with the \"AllowFeatureCorrelation\" parameter.",
        "Answer_gpt_summary":"solut allowcolditemplac paramet enabl us featur cold item recommend model enabl usefeatureinmodel paramet enabl enablemodelinginsight paramet enabl collect data recommend model perform usag reasoningfeaturelist paramet enabl recommend engin provid reason explan recommend item set allowfeaturecorrel paramet"
    },
    {
        "Question_title":"Multiple tables",
        "Question_body":"<p>I know how to create a table with a data frame programmatically. However, I have two data frames, and they have different number of rows, so I cannot combine them into a single data frame. How do I upload two different tables to a Weights&amp;Biases project? Somehow, I suspect that the following is not the correct approach:<\/p>\n<pre><code class=\"lang-python\">    train_df = pd.DataFrame({\n        'tx':train_x,\n        'ty':train_y,\n    })\n    valid_df = pd.DataFrame({\n        'vx':valid_x,\n        'vy':valid_y\n    })\n\n    # How to add multiple tables\n\n    wandb.log({\"table\": train_df}, commit=False)\n    wandb.log({\"table\": valid_df}, commit=False)\n<\/code><\/pre>\n<p>Any help is greatly appreciated.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1659744551959,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":160.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a> , see <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#create-tables\">this document<\/a> on how to create tables from dataframes and please let me know if you have any questions.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/multiple-tables\/2856",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1660166829976,
        "Question_original_content":"multipl tabl know creat tabl data frame programmat data frame differ number row combin singl data frame upload differ tabl weight bias project suspect follow correct approach train datafram train train valid datafram valid valid add multipl tabl log tabl train commit fals log tabl valid commit fals help greatli appreci",
        "Question_preprocessed_content":"multipl tabl know creat tabl data frame programmat data frame differ number row combin singl data frame upload differ tabl weight bias project suspect follow correct approach help greatli appreci",
        "Question_gpt_summary_original":"The user is facing a challenge of how to upload two different tables with different numbers of rows to a Weights&Biases project. They have tried creating two data frames programmatically but are unsure if logging them separately is the correct approach.",
        "Question_gpt_summary":"user face challeng upload differ tabl differ number row weight bias project tri creat data frame programmat unsur log separ correct approach",
        "Answer_original_content":"erlebach document creat tabl datafram let know question",
        "Answer_preprocessed_content":"document creat tabl datafram let know question",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to a document that explains how to create tables from data frames. The user can refer to this document to learn how to upload two different tables with different numbers of rows to a Weights&Biases project. No other solutions are mentioned in the discussion.",
        "Answer_gpt_summary":"solut discuss provid link document explain creat tabl data frame user refer document learn upload differ tabl differ number row weight bias project solut mention discuss"
    },
    {
        "Question_title":"comet (comet-ml) fails to run with Keras",
        "Question_body":"<p>Im running the keras examples from <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\/blob\/master\/comet_keras_example.py\" rel=\"nofollow noreferrer\">Comet github project<\/a> .<\/p>\n\n<p>I add the import and create a new experiment:<\/p>\n\n<pre><code>def train(x_train,y_train,x_test,y_test):\nmodel = build_model_graph()\n\nfrom comet_ml import Experiment\n\nexperiment = Experiment(api_key=\"XXXX\", log_code=True)\n\nmodel.fit(x_train, y_train, batch_size=128, epochs=50, validation_data=(x_test, y_test))\n\nscore = model.evaluate(x_test, y_test, verbose=0)\n<\/code><\/pre>\n\n<p>and when i run my training code it fails.<\/p>\n\n<p>error:<\/p>\n\n<pre><code>Using TensorFlow backend.\nTraceback (most recent call last):\n  File \"\/Users\/nimrodlahav\/Code\/semantica\/experiment-logger-client\/train-examples\/keras-example.py\", line 21, in &lt;module&gt;\n    from comet_ml import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/__init__.py\", line 3, in &lt;module&gt;\n    from .comet import Experiment\n  File \"..\/.\/comet-client-lib\/comet_ml\/comet.py\", line 29, in &lt;module&gt;\n    from comet_ml import keras_logger\n  File \"..\/.\/comet-client-lib\/comet_ml\/keras_logger.py\", line 31, in &lt;module&gt;\n    raise SyntaxError(\"Please import Comet before importing any keras modules\")\nSyntaxError: Please import Comet before importing any keras modules\n<\/code><\/pre>\n\n<p>what am I missing?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1506024659377,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1506491876827,
        "Question_score":2,
        "Question_view_count":601.0,
        "Poster_id":8635661.0,
        "Poster_created_time":1505841491572,
        "Poster_last_access_time":1506108675088,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":55.0,
        "Poster_views":4.0,
        "Answer_body":"<p>I don't see start of the code but it looks like you have imported Keras before you have imported Comet.<\/p>\n\n<p>From the error message it looks like just need to switch the import lines (Comet first Keras second), like in your example:<\/p>\n\n<pre><code>from comet_ml import Experiment\n\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import RMSprop \n<\/code><\/pre>\n\n<p>view the full source code <a href=\"https:\/\/github.com\/comet-ml\/comet-keras-example\" rel=\"nofollow noreferrer\">example<\/a> .<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1506066589407,
        "Answer_score":3.0,
        "Answerer_id":2126740.0,
        "Answerer_created_time":1362230894870,
        "Answerer_last_access_time":1627657805790,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":165.0,
        "Answerer_views":33.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46352435",
        "Tool":"Comet",
        "Platform":"Stack Overflow",
        "Question_closed_time":1506026797448,
        "Question_original_content":"fail run kera run kera exampl github project add import creat new experi def train train train test test model build model graph import experi experi experi api kei log code true model fit train train batch size epoch valid data test test score model evalu test test verbos run train code fail error tensorflow backend traceback recent file user nimrodlahav code semantica experi logger client train exampl kera exampl line import experi file client lib init line import experi file client lib line import kera logger file client lib kera logger line rais syntaxerror import import kera modul syntaxerror import import kera modul miss",
        "Question_preprocessed_content":"fail run kera run kera exampl github project add import creat new experi run train code fail error miss",
        "Question_gpt_summary_original":"The user is encountering an error when trying to run the Keras examples from the Comet GitHub project. The error message indicates that the user needs to import Comet before importing any Keras modules.",
        "Question_gpt_summary":"user encount error try run kera exampl github project error messag indic user need import import kera modul",
        "Answer_original_content":"start code look like import kera import error messag look like need switch import line kera second like exampl import experi import kera kera dataset import mnist kera model import sequenti kera layer import dens dropout kera optim import rmsprop view sourc code exampl",
        "Answer_preprocessed_content":"start code look like import kera import error messag look like need switch import line like exampl view sourc code exampl",
        "Answer_gpt_summary_original":"Solution: The error message indicates that the user needs to import Comet before importing any Keras modules. The solution is to switch the import lines, so that Comet is imported first and Keras is imported second.",
        "Answer_gpt_summary":"solut error messag indic user need import import kera modul solut switch import line import kera import second"
    },
    {
        "Question_title":"AWS Sagemaker inference endpoint doesn't scale in with autoscaling",
        "Question_body":"<p>I have an AWS Sagemaker inference endpoint with autoscaling enabled with SageMakerVariantInvocationsPerInstance target metric. When I send a lot of requests to the endpoint the number of instances correctly scales out to the maximum instance count. But after I stop sending the requests the number of instances doesn't scale in to 1, minimum instance count. I waited for many hours. Is there a reason for this behaviour?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1608117787513,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":1028.0,
        "Poster_id":2928970.0,
        "Poster_created_time":1382978984190,
        "Poster_last_access_time":1664012645480,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1311.0,
        "Poster_views":49.0,
        "Answer_body":"<p>AutoScaling requires a cloudwatch alarm to trigger to scale in.  Sagemaker doesn't push 0 value metrics when there's no activity (it just doesn't push anything).  This leads to the alarm being put into insufficient data and not triggering the autoscaling scale in action when your workload suddenly ends.<\/p>\n<p>Workarounds are either:<\/p>\n<ol>\n<li>Have a step scaling policy using the cloudwatch metric math FILL() function for your scale in.  This way you can tell CloudWatch &quot;if there's no data, pretend this was the metric value when evaluating the alarm.  This is only possible with step scaling since target tracking creates the alarms for you (and AutoScaling will periodically recreate them, so if you make manual changes they'll get deleted)<\/li>\n<li>Have scheduled scaling set the size back down to 1 every evening<\/li>\n<li>Make sure traffic continues at a low level for some times<\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Answerer_id":7097192.0,
        "Answerer_created_time":1477948823888,
        "Answerer_last_access_time":1629417219760,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":721.0,
        "Answerer_views":57.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65322286",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1608300540056,
        "Question_original_content":"infer endpoint scale autosc infer endpoint autosc enabl variantinvocationsperinst target metric send lot request endpoint number instanc correctli scale maximum instanc count stop send request number instanc scale minimum instanc count wait hour reason behaviour thank",
        "Question_preprocessed_content":"infer endpoint scale autosc infer endpoint autosc enabl variantinvocationsperinst target metric send lot request endpoint number instanc correctli scale maximum instanc count stop send request number instanc scale minimum instanc count wait hour reason behaviour thank",
        "Question_gpt_summary_original":"The user is facing a challenge with their AWS Sagemaker inference endpoint, which has autoscaling enabled with SageMakerVariantInvocationsPerInstance target metric. The endpoint scales out to the maximum instance count when a lot of requests are sent, but it does not scale in to the minimum instance count of 1 after the requests stop, even after waiting for many hours. The user is seeking a reason for this behavior.",
        "Question_gpt_summary":"user face challeng infer endpoint autosc enabl variantinvocationsperinst target metric endpoint scale maximum instanc count lot request sent scale minimum instanc count request stop wait hour user seek reason behavior",
        "Answer_original_content":"autosc requir cloudwatch alarm trigger scale push valu metric activ push lead alarm insuffici data trigger autosc scale action workload suddenli end workaround step scale polici cloudwatch metric math function scale wai tell cloudwatch data pretend metric valu evalu alarm possibl step scale target track creat alarm autosc period recreat manual chang delet schedul scale set size even sure traffic continu low level time",
        "Answer_preprocessed_content":"autosc requir cloudwatch alarm trigger scale push valu metric activ lead alarm insuffici data trigger autosc scale action workload suddenli end workaround step scale polici cloudwatch metric math function scale wai tell cloudwatch data pretend metric valu evalu alarm possibl step scale target track creat alarm schedul scale set size even sure traffic continu low level time",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are:\n\n1. Use a step scaling policy with the CloudWatch metric math FILL() function for scale-in.\n2. Set scheduled scaling to reduce the size back down to 1 every evening.\n3. Ensure that traffic continues at a low level for some time.\n\nThe reason for the behavior is that Sagemaker does not push 0 value metrics when there is no activity, which leads to the alarm being put into insufficient data and not triggering the autoscaling scale-in action when the workload suddenly ends.",
        "Answer_gpt_summary":"solut provid discuss us step scale polici cloudwatch metric math function scale set schedul scale reduc size even ensur traffic continu low level time reason behavior push valu metric activ lead alarm insuffici data trigger autosc scale action workload suddenli end"
    },
    {
        "Question_title":"Python: How to retrive the best model from Optuna LightGBM study?",
        "Question_body":"<p>I would like to get the best model to use later in the notebook to predict using a different test batch.<\/p>\n\n<p>reproducible example (taken from Optuna Github) :<\/p>\n\n<pre><code>import lightgbm as lgb\nimport numpy as np\nimport sklearn.datasets\nimport sklearn.metrics\nfrom sklearn.model_selection import train_test_split\n\nimport optuna\n\n\n# FYI: Objective functions can take additional arguments\n# (https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#objective-func-additional-args).\ndef objective(trial):\n    data, target = sklearn.datasets.load_breast_cancer(return_X_y=True)\n    train_x, valid_x, train_y, valid_y = train_test_split(data, target, test_size=0.25)\n    dtrain = lgb.Dataset(train_x, label=train_y)\n    dvalid = lgb.Dataset(valid_x, label=valid_y)\n\n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"auc\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"lambda_l1\": trial.suggest_loguniform(\"lambda_l1\", 1e-8, 10.0),\n        \"lambda_l2\": trial.suggest_loguniform(\"lambda_l2\", 1e-8, 10.0),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"feature_fraction\": trial.suggest_uniform(\"feature_fraction\", 0.4, 1.0),\n        \"bagging_fraction\": trial.suggest_uniform(\"bagging_fraction\", 0.4, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n    }\n\n    # Add a callback for pruning.\n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, \"auc\")\n    gbm = lgb.train(\n        param, dtrain, valid_sets=[dvalid], verbose_eval=False, callbacks=[pruning_callback]\n    )\n\n    preds = gbm.predict(valid_x)\n    pred_labels = np.rint(preds)\n    accuracy = sklearn.metrics.accuracy_score(valid_y, pred_labels)\n    return accuracy\n\n<\/code><\/pre>\n\n<p>my understanding is that the study below will tune for accuracy. I would like to somehow retrieve the best model from the study (not just the parameters) without saving it as a pickle, I just want to use the model somewhere else in my notebook. <\/p>\n\n<pre><code>\nif __name__ == \"__main__\":\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, n_trials=100)\n\n    print(\"Best trial:\")\n    trial = study.best_trial\n\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))\n\n<\/code><\/pre>\n\n<p>desired output would be <\/p>\n\n<pre><code>best_model = ~model from above~\nnew_target_pred = best_model.predict(new_data_test)\nmetrics.accuracy_score(new_target_test, new__target_pred)\n\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1591072505577,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":1591072893872,
        "Question_score":14,
        "Question_view_count":8921.0,
        "Poster_id":9989308.0,
        "Poster_created_time":1529932143432,
        "Poster_last_access_time":1663484597720,
        "Poster_age":null,
        "Poster_location":"Melbourne VIC, Australia",
        "Poster_reputation":525.0,
        "Poster_views":54.0,
        "Answer_body":"<p>I think you can use the <code>callback<\/code> argument of <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/reference\/study.html#optuna.study.Study.optimize\" rel=\"noreferrer\"><code>Study.optimize<\/code><\/a> to save the best model. In the following code example, the callback checks if a given trial is corresponding to the best trial and saves the model as a global variable <code>best_booster<\/code>.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>best_booster = None\ngbm = None\n\ndef objective(trial):\n    global gbm\n    # ...\n\ndef callback(study, trial):\n    global best_booster\n    if study.best_trial == trial:\n        best_booster = gbm\n\nif __name__ == \"__main__\":\n    study = optuna.create_study(\n        pruner=optuna.pruners.MedianPruner(n_warmup_steps=10), direction=\"maximize\"\n    )\n    study.optimize(objective, n_trials=100, callbacks=[callback])\n\n<\/code><\/pre>\n\n<p>If you define your objective function as a class, you can remove the global variables. I created a notebook as a code example. Please take a look at it:\n<a href=\"https:\/\/colab.research.google.com\/drive\/1ssjXp74bJ8bCAbvXFOC4EIycBto_ONp_?usp=sharing\" rel=\"noreferrer\">https:\/\/colab.research.google.com\/drive\/1ssjXp74bJ8bCAbvXFOC4EIycBto_ONp_?usp=sharing<\/a><\/p>\n\n<blockquote>\n  <p>I would like to somehow retrieve the best model from the study (not just the parameters) without saving it as a pickle<\/p>\n<\/blockquote>\n\n<p>FYI, if you can pickle the boosters, I think you can make the code simple by following <a href=\"https:\/\/optuna.readthedocs.io\/en\/latest\/faq.html#how-to-save-machine-learning-models-trained-in-objective-functions\" rel=\"noreferrer\">this FAQ<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Answerer_id":11337652.0,
        "Answerer_created_time":1554860971800,
        "Answerer_last_access_time":1635124280540,
        "Answerer_age":null,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation":301.0,
        "Answerer_views":65.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62144904",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Question_closed_time":1591153873300,
        "Question_original_content":"python retriv best model lightgbm studi like best model us later notebook predict differ test batch reproduc exampl taken github import lightgbm lgb import numpi import sklearn dataset import sklearn metric sklearn model select import train test split import fyi object function addit argument http readthedoc stabl faq html object func addit arg def object trial data target sklearn dataset load breast cancer return true train valid train valid train test split data target test size dtrain lgb dataset train label train dvalid lgb dataset valid label valid param object binari metric auc verbos boost type gbdt lambda trial suggest loguniform lambda lambda trial suggest loguniform lambda num leav trial suggest int num leav featur fraction trial suggest uniform featur fraction bag fraction trial suggest uniform bag fraction bag freq trial suggest int bag freq min child sampl trial suggest int min child sampl add callback prune prune callback integr lightgbmpruningcallback trial auc gbm lgb train param dtrain valid set dvalid verbos eval fals callback prune callback pred gbm predict valid pred label rint pred accuraci sklearn metric accuraci score valid pred label return accuraci understand studi tune accuraci like retriev best model studi paramet save pickl want us model notebook main studi creat studi pruner pruner medianprun warmup step direct maxim studi optim object trial print best trial trial studi best trial print param kei valu trial param item print format kei valu desir output best model model new target pred best model predict new data test metric accuraci score new target test new target pred",
        "Question_preprocessed_content":"python retriv best model lightgbm studi like best model us later notebook predict differ test batch reproduc exampl understand studi tune accuraci like retriev best model studi save pickl want us model notebook desir output",
        "Question_gpt_summary_original":"The user wants to retrieve the best model from an Optuna LightGBM study to use it later in the notebook to predict using a different test batch. The study tunes for accuracy, and the user wants to retrieve the best model without saving it as a pickle. The desired output is the best model and its accuracy score on new test data.",
        "Question_gpt_summary":"user want retriev best model lightgbm studi us later notebook predict differ test batch studi tune accuraci user want retriev best model save pickl desir output best model accuraci score new test data",
        "Answer_original_content":"think us callback argument studi optim save best model follow code exampl callback check given trial correspond best trial save model global variabl best booster best booster gbm def object trial global gbm def callback studi trial global best booster studi best trial trial best booster gbm main studi creat studi pruner pruner medianprun warmup step direct maxim studi optim object trial callback callback defin object function class remov global variabl creat notebook code exampl look http colab research googl com drive ssjxpbjbcabvxfoceiycbto onp usp share like retriev best model studi paramet save pickl fyi pickl booster think code simpl follow faq",
        "Answer_preprocessed_content":"think us argument save best model follow code exampl callback check given trial correspond best trial save model global variabl defin object function class remov global variabl creat notebook code exampl look like retriev best model studi save pickl fyi pickl booster think code simpl follow faq",
        "Answer_gpt_summary_original":"Solution:\n- Use the `callback` argument of `Study.optimize` to save the best model. The callback checks if a given trial is corresponding to the best trial and saves the model as a global variable `best_booster`.\n- If you define your objective function as a class, you can remove the global variables.\n- If you can pickle the boosters, you can make the code simple by following the FAQ provided in the discussion.",
        "Answer_gpt_summary":"solut us callback argument studi optim save best model callback check given trial correspond best trial save model global variabl best booster defin object function class remov global variabl pickl booster code simpl follow faq provid discuss"
    },
    {
        "Question_title":"[Azure][ML][Python SDK][Environment][Docker] Docker copy missing context",
        "Question_body":"Hello,\n\nI am trying to create an Azure ML Environment using a Dockerfile but it contains the 'COPY' instruction.\n\nFrom the documentation of Environment.from_dockerfile ( https:\/\/docs.microsoft.com\/fr-fr\/python\/api\/azureml-core\/azureml.core.environment(class)?view=azure-ml-py#from-dockerfile-name--dockerfile--conda-specification-none--pip-requirements-none- ), I can not find a way to give it some files along with the Dockerfile itself.\n\nSo, how to pass context to enable using COPY in the Dockerfile ?\n\nThank you for your time !",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1634739305317,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Docker context is not supported with AzureML Python SDK at the moment. Context support will added later this year",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/597612\/azuremlpython-sdkenvironmentdocker-docker-copy-mis.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1634774742763,
        "Question_original_content":"azur python sdk environ docker docker copi miss context hello try creat environ dockerfil contain copi instruct document environ dockerfil http doc microsoft com python api core core environ class view azur dockerfil dockerfil conda specif pip requir wai file dockerfil pass context enabl copi dockerfil thank time",
        "Question_preprocessed_content":"docker copi miss context hello try creat environ dockerfil contain copi instruct document wai file dockerfil pass context enabl copi dockerfil thank time",
        "Question_gpt_summary_original":"The user is facing a challenge in creating an Azure ML Environment using a Dockerfile that contains the 'COPY' instruction. They are unable to find a way to give files along with the Dockerfile itself and are seeking a solution to pass context to enable using COPY in the Dockerfile.",
        "Question_gpt_summary":"user face challeng creat environ dockerfil contain copi instruct unabl wai file dockerfil seek solut pass context enabl copi dockerfil",
        "Answer_original_content":"docker context support python sdk moment context support ad later year",
        "Answer_preprocessed_content":"docker context support python sdk moment context support ad later year",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. It is mentioned that context support for Docker will be added later this year in AzureML Python SDK.",
        "Answer_gpt_summary":"solut solut provid discuss mention context support docker ad later year python sdk"
    },
    {
        "Question_title":"Creating a dataframe in Azure ML Notebook with R kernel",
        "Question_body":"<p>I have written some scripts in <code>R<\/code> which I have to run in azure ml notebook but I have not found much documentation how to create a dataset by running code in notebook with R kernel. I have written the following python code which works with <code>python<\/code> kernel as:<\/p>\n<pre><code>from azureml.core import Dataset, Datastore,Workspace\n\nsubscription_id = 'abc'\nresource_group = 'pqr'\nworkspace_name = 'xyz'\n\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndatastore = Datastore.get(workspace, 'workspaceblobstore')\n\n# create tabular dataset from all parquet files in the directory\ntabular_dataset_3 = Dataset.Tabular.from_parquet_files(path=(datastore,'\/UI\/09-17-2022_125003_UTC\/userdata1.parquet'))\n\ndf=tabular_dataset_3.to_pandas_dataframe()\n<\/code><\/pre>\n<p>It works fine with python kernel but I want to execute the equivalent R code in notebook with R kernel.<\/p>\n<p>Can anyone please help me what is the equivalent R code ? Any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1663429668780,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":19.0,
        "Poster_id":10537579.0,
        "Poster_created_time":1540154634483,
        "Poster_last_access_time":1664030893903,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":237.0,
        "Poster_views":173.0,
        "Answer_body":"<p>To create an R script and use the dataset, first we need to register the dataset to the portal. Once the dataset is added to the portal, we need to get the dataset URL and open the notebook and use the R kernel.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/j1q3J.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/j1q3J.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Upload the dataset and get the data source URL<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/waBr3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/waBr3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Go to Machine Learning studio and create a new notebook.<\/p>\n<p>Use the below R script to get the dataset and convert that to dataframe.<\/p>\n<pre><code>azureml_main &lt;- function(dataframe1, dataframe2){\n  print(&quot;R script run.&quot;)\n  run = get_current_run()\n  ws = workspacename\n  dataset = azureml$core$dataset$Dataset$get_by_name(ws, \u201c.\/path\/insurance.csv&quot;)\n  dataframe2 &lt;- dataset$to_pandas_dataframe()\n  # Return datasets as a Named List\n  return(list(dataset1=dataframe1, dataset2=dataframe2))\n  \n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":18428148.0,
        "Answerer_created_time":1646907459852,
        "Answerer_last_access_time":1664066617772,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":1624.0,
        "Answerer_views":1376.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73756301",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1663579958747,
        "Question_original_content":"creat datafram notebook kernel written script run notebook document creat dataset run code notebook kernel written follow python code work python kernel core import dataset datastor workspac subscript abc resourc group pqr workspac xyz workspac workspac subscript resourc group workspac datastor datastor workspac workspaceblobstor creat tabular dataset parquet file directori tabular dataset dataset tabular parquet file path datastor utc userdata parquet tabular dataset panda datafram work fine python kernel want execut equival code notebook kernel help equival code help appreci",
        "Question_preprocessed_content":"creat datafram notebook kernel written script run notebook document creat dataset run code notebook kernel written follow python code work kernel work fine python kernel want execut equival code notebook kernel help equival code help appreci",
        "Question_gpt_summary_original":"The user is facing challenges in creating a dataset in Azure ML Notebook with R kernel. They have written scripts in R but have not found sufficient documentation on how to create a dataset by running code in the notebook with R kernel. They have provided a Python code that works with the Python kernel but are seeking help to execute the equivalent R code in the R kernel.",
        "Question_gpt_summary":"user face challeng creat dataset notebook kernel written script suffici document creat dataset run code notebook kernel provid python code work python kernel seek help execut equival code kernel",
        "Answer_original_content":"creat script us dataset need regist dataset portal dataset ad portal need dataset url open notebook us kernel upload dataset data sourc url machin learn studio creat new notebook us script dataset convert datafram main function datafram datafram print script run run current run workspacenam dataset core dataset dataset path insur csv datafram dataset panda datafram return dataset name list return list dataset datafram dataset datafram",
        "Answer_preprocessed_content":"creat script us dataset need regist dataset portal dataset ad portal need dataset url open notebook us kernel upload dataset data sourc url machin learn studio creat new notebook us script dataset convert datafram",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to create a dataset in Azure ML Notebook with R kernel. The solution involves registering the dataset to the portal, getting the dataset URL, and using the R script to get the dataset and convert it to a dataframe. The R script is provided in the discussion.",
        "Answer_gpt_summary":"solut discuss provid solut creat dataset notebook kernel solut involv regist dataset portal get dataset url script dataset convert datafram script provid discuss"
    },
    {
        "Question_title":"What is a trait in Azure ML matchbox recommender?",
        "Question_body":"<p>Azure Machine Learning has an item called <code>Train Matchbox Recommender<\/code>. It can be configured with a <code>Number of traits<\/code>. Unfortunately, the documentation does not describe what such a trait is.<\/p>\n\n<p>What are traits? Is this related to <a href=\"https:\/\/en.wikipedia.org\/wiki\/Latent_variable\" rel=\"nofollow noreferrer\">latent variables<\/a>?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1525162879793,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1526046231816,
        "Question_score":0,
        "Question_view_count":859.0,
        "Poster_id":520957.0,
        "Poster_created_time":1290750251812,
        "Poster_last_access_time":1663965312343,
        "Poster_age":null,
        "Poster_location":"Nimes, France",
        "Poster_reputation":55864.0,
        "Poster_views":4960.0,
        "Answer_body":"<p><a href=\"http:\/\/apprize.info\/microsoft\/azure_1\/9.html\" rel=\"nofollow noreferrer\">This<\/a> page may have better descriptions on it.<\/p>\n\n<p>Basically, traits are the features the algorithm will learn about each user related to each item. For example, in the <a href=\"https:\/\/gallery.azure.ai\/Experiment\/Recommender-Restaurant-ratings-2\" rel=\"nofollow noreferrer\">restaurant ratings recommender<\/a> traits could include a user's birth year, if they're a student or working professional, martial status, etc.<\/p>\n\n<p>Hope that helps!<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":186013.0,
        "Answerer_created_time":1254957460063,
        "Answerer_last_access_time":1659941040212,
        "Answerer_age":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation":2484.0,
        "Answerer_views":362.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50113374",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1525171563376,
        "Question_original_content":"trait matchbox recommend item call train matchbox recommend configur number trait unfortun document trait trait relat latent variabl",
        "Question_preprocessed_content":"trait matchbox recommend item call configur unfortun document trait trait relat latent variabl",
        "Question_gpt_summary_original":"The user is facing challenges in understanding the concept of \"traits\" in Azure ML's Train Matchbox Recommender. The documentation does not provide any information about what traits are and whether they are related to latent variables.",
        "Question_gpt_summary":"user face challeng understand concept trait train matchbox recommend document provid inform trait relat latent variabl",
        "Answer_original_content":"page better descript basic trait featur algorithm learn user relat item exampl restaur rate recommend trait includ user birth year student work profession martial statu hope help",
        "Answer_preprocessed_content":"page better descript basic trait featur algorithm learn user relat item exampl restaur rate recommend trait includ user birth year student work profession martial statu hope help",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by explaining that traits are the features that the algorithm learns about each user related to each item. The example given is that in the restaurant ratings recommender, traits could include a user's birth year, if they're a student or working professional, martial status, etc.",
        "Answer_gpt_summary":"solut discuss provid solut challeng explain trait featur algorithm learn user relat item exampl given restaur rate recommend trait includ user birth year student work profession martial statu"
    },
    {
        "Question_title":"Can you configure Amazon ECR containers to be immutable?",
        "Question_body":"Is there a way to configure Amazon ECR containers so that they can't be changed once they're created? Here are our requirements:\n\nContainers can't be changed after their built.\nContainers can't receive updates.\nChanges in the containerized application must require the building and deployment of a new container image.\nRuntime data and configurations must be stored outside of the container environment.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1607710511000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":27.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Yes, you can configure Amazon ECR containers to be immutable. Amazon ECR uses resource-based permissions to control access to repositories. The resource-based permissions let you specify which IAM users or roles have access to a repository and what actions they can perform on it. By default, only the repository owner has access to a repository.\n\nFor more information, see Repository policies and Image tag mutability in the Amazon ECR user guide.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUPiBylRCSe6_ax_u_4g-oA\/can-you-configure-amazon-ecr-containers-to-be-immutable",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1608306412000,
        "Question_original_content":"configur amazon ecr contain immut wai configur amazon ecr contain chang creat requir contain chang built contain receiv updat chang container applic requir build deploy new contain imag runtim data configur store outsid contain environ",
        "Question_preprocessed_content":"configur amazon ecr contain immut wai configur amazon ecr contain chang creat requir contain chang built contain receiv updat chang container applic requir build deploy new contain imag runtim data configur store outsid contain environ",
        "Question_gpt_summary_original":"The user is looking for a way to configure Amazon ECR containers to be immutable, meaning they cannot be changed or updated once they are created. They require that any changes to the containerized application must require building and deploying a new container image, and that runtime data and configurations must be stored outside of the container environment.",
        "Question_gpt_summary":"user look wai configur amazon ecr contain immut mean chang updat creat requir chang container applic requir build deploi new contain imag runtim data configur store outsid contain environ",
        "Answer_original_content":"ye configur amazon ecr contain immut amazon ecr us resourc base permiss control access repositori resourc base permiss let specifi iam user role access repositori action perform default repositori owner access repositori inform repositori polici imag tag mutabl amazon ecr user guid",
        "Answer_preprocessed_content":"ye configur amazon ecr contain immut amazon ecr us permiss control access repositori permiss let specifi iam user role access repositori action perform default repositori owner access repositori inform repositori polici imag tag mutabl amazon ecr user guid",
        "Answer_gpt_summary_original":"Solution: The user can configure Amazon ECR containers to be immutable by using resource-based permissions to control access to repositories. The resource-based permissions let the user specify which IAM users or roles have access to a repository and what actions they can perform on it. The user can also refer to the Amazon ECR user guide for more information on Repository policies and Image tag mutability.",
        "Answer_gpt_summary":"solut user configur amazon ecr contain immut resourc base permiss control access repositori resourc base permiss let user specifi iam user role access repositori action perform user refer amazon ecr user guid inform repositori polici imag tag mutabl"
    },
    {
        "Question_title":"Is there a limit on input json string for aws sagemaker endpoint?",
        "Question_body":"<p>I have ~5MB json string that I want to send to my endpoint. I am using boto3.client to invoke the endpoint from my python client. It throws ConnectionResetError. <\/p>\n\n<pre><code>    File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 600, in urlopen\n    chunked=chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\urllib3\\connectionpool.py\", line 354, in _make_request\n    conn.request(method, url, **httplib_request_kw)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1229, in request\n    self._send_request(method, url, body, headers, encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 92, in _send_request\n    method, url, body, headers, *args, **kwargs)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1275, in _send_request\n    self.endheaders(body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 1224, in endheaders\n    self._send_output(message_body, encode_chunked=encode_chunked)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 119, in _send_output\n    self.send(msg)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\botocore\\awsrequest.py\", line 203, in send\n    return super(AWSConnection, self).send(str)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\http\\client.py\", line 977, in send\n    self.sock.sendall(data)\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ssl.py\", line 1012, in sendall\n    v = self.send(byte_view[count:])\n  File \"C:\\Users\\corona\\AppData\\Local\\Programs\\Python\\Python37\\lib\\ssl.py\", line 981, in send\n    return self._sslobj.write(data)\nConnectionResetError: [WinError 10054] An existing connection was forcibly closed by the remote host\n<\/code><\/pre>\n\n<p>Looking at the trace, I am guessing it is due to json string size. Could someone please help me how to get around this? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1586140541570,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1586146490856,
        "Question_score":1,
        "Question_view_count":1281.0,
        "Poster_id":13077501.0,
        "Poster_created_time":1584461369092,
        "Poster_last_access_time":1663592296568,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":13.0,
        "Poster_views":4.0,
        "Answer_body":"<p>Exceeding the payload size limit does result in a connection reset from the SageMaker Runtime service.<\/p>\n\n<p>From the SageMaker <a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Maximum payload size for endpoint invocation |    5 MB<\/p>\n<\/blockquote>\n\n<p>There are likely more space-efficient data formats than JSON that you could use to transmit the payload, but the available options will depend on the type of data and what model image you are using (i.e. whether Amazon-provided or a custom implementation).<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":10286863.0,
        "Answerer_created_time":1535490052056,
        "Answerer_last_access_time":1663802199163,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":81.0,
        "Answerer_views":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61052173",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1586421549190,
        "Question_original_content":"limit input json string endpoint json string want send endpoint boto client invok endpoint python client throw connectionreseterror file user corona appdata local program python python lib site packag urllib connectionpool line urlopen chunk chunk file user corona appdata local program python python lib site packag urllib connectionpool line request conn request method url httplib request file user corona appdata local program python python lib http client line request self send request method url bodi header encod chunk file user corona appdata local program python python lib site packag botocor awsrequest line send request method url bodi header arg kwarg file user corona appdata local program python python lib http client line send request self endhead bodi encod chunk encod chunk file user corona appdata local program python python lib http client line endhead self send output messag bodi encod chunk encod chunk file user corona appdata local program python python lib site packag botocor awsrequest line send output self send msg file user corona appdata local program python python lib site packag botocor awsrequest line send return super awsconnect self send str file user corona appdata local program python python lib http client line send self sock sendal data file user corona appdata local program python python lib ssl line sendal self send byte view count file user corona appdata local program python python lib ssl line send return self sslobj write data connectionreseterror winerror exist connect forcibl close remot host look trace guess json string size help",
        "Question_preprocessed_content":"limit input json string endpoint json string want send endpoint invok endpoint python client throw connectionreseterror look trace guess json string size help",
        "Question_gpt_summary_original":"The user is encountering a ConnectionResetError while trying to send a 5MB JSON string to an AWS Sagemaker endpoint using boto3.client from their Python client. The user suspects that the error is due to the size of the JSON string and is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount connectionreseterror try send json string endpoint boto client python client user suspect error size json string seek help resolv issu",
        "Answer_original_content":"exceed payload size limit result connect reset runtim servic document maximum payload size endpoint invoc like space effici data format json us transmit payload avail option depend type data model imag amazon provid custom implement",
        "Answer_preprocessed_content":"exceed payload size limit result connect reset runtim servic document maximum payload size endpoint invoc like data format json us transmit payload avail option depend type data model imag",
        "Answer_gpt_summary_original":"Summary: The user is encountering a ConnectionResetError while trying to send a 5MB JSON string to an AWS Sagemaker endpoint using boto3.client from their Python client. The maximum payload size for endpoint invocation is 5 MB, and exceeding this limit results in a connection reset from the SageMaker Runtime service. There are likely more space-efficient data formats than JSON that could be used to transmit the payload, but the available options will depend on the type of data and what model image is being used. No specific solution is provided.",
        "Answer_gpt_summary":"summari user encount connectionreseterror try send json string endpoint boto client python client maximum payload size endpoint invoc exceed limit result connect reset runtim servic like space effici data format json transmit payload avail option depend type data model imag specif solut provid"
    },
    {
        "Question_title":"Distributed Unsupervised Learning in SageMaker",
        "Question_body":"<p>I am running local unsupervised learning (predominantly clustering) on a large, single node with GPU.<\/p>\n<p>Does SageMaker support <strong>distributed unsupervised learning<\/strong> using <strong>clustering<\/strong>?<\/p>\n<p>If yes, please provide the relevant example (preferably non-TensorFlow).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1663402564440,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":14.0,
        "Poster_id":3203213.0,
        "Poster_created_time":1389887039672,
        "Poster_last_access_time":1664076128463,
        "Poster_age":null,
        "Poster_location":"Singapore",
        "Poster_reputation":5854.0,
        "Poster_views":794.0,
        "Answer_body":"<p>SageMaker Training allow you to bring your own training scripts, and supports various forms of distributed training, like data\/model parallel, and frameworks like PyTorch DDP, Horovod, DeepSpeed, etc.\nAdditionally, if you want to bring your data, but not code, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algorithms-unsupervised.html\" rel=\"nofollow noreferrer\">SageMaker training offers various unsupervised built-in algorithms<\/a>, some of which are parallelizable.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":121956.0,
        "Answerer_created_time":1244808478036,
        "Answerer_last_access_time":1663981477123,
        "Answerer_age":null,
        "Answerer_location":"Israel",
        "Answerer_reputation":4932.0,
        "Answerer_views":405.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73753271",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1663485400223,
        "Question_original_content":"distribut unsupervis learn run local unsupervis learn predominantli cluster larg singl node gpu support distribut unsupervis learn cluster ye provid relev exampl prefer non tensorflow",
        "Question_preprocessed_content":"distribut unsupervis learn run local unsupervis learn larg singl node gpu support distribut unsupervis learn cluster ye provid relev exampl",
        "Question_gpt_summary_original":"The user is inquiring about whether SageMaker supports distributed unsupervised learning using clustering and is seeking a relevant example, preferably non-TensorFlow.",
        "Question_gpt_summary":"user inquir support distribut unsupervis learn cluster seek relev exampl prefer non tensorflow",
        "Answer_original_content":"train allow bring train script support form distribut train like data model parallel framework like pytorch ddp horovod deepspe addition want bring data code train offer unsupervis built algorithm paralleliz",
        "Answer_preprocessed_content":"train allow bring train script support form distribut train like parallel framework like pytorch ddp horovod deepspe addition want bring data code train offer unsupervis algorithm paralleliz",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- SageMaker Training allows users to bring their own training scripts and supports various forms of distributed training, including data\/model parallel, and frameworks like PyTorch DDP, Horovod, DeepSpeed, etc.\n- SageMaker Training offers various unsupervised built-in algorithms, some of which are parallelizable, for users who want to bring their data but not code.\n\nTherefore, SageMaker supports distributed unsupervised learning using clustering, and users can use either their own training scripts or built-in algorithms.",
        "Answer_gpt_summary":"possibl solut mention discuss train allow user bring train script support form distribut train includ data model parallel framework like pytorch ddp horovod deepspe train offer unsupervis built algorithm paralleliz user want bring data code support distribut unsupervis learn cluster user us train script built algorithm"
    },
    {
        "Question_title":"TensorFlow Serving vs. TensorFlow Inference (container type for SageMaker model)",
        "Question_body":"<p>I am fairly new to TensorFlow (and SageMaker) and am stuck in the process of deploying a SageMaker endpoint. I have just recently succeeded in creating a Saved Model type model, which is currently being used to service a sample endpoint (the model was created externally). However, when I checked the image I am using for the endpoint, it says '...\/tensorflow-inference', which is not the direction I want to go in because I want to use a SageMaker TensorFlow serving container (I followed tutorials from the official TensorFlow serving GitHub repo-using sample models, and they are deployed correcting using the TensorFlow serving framework).<\/p>\n<p>Am I encountering this issue because my Saved Model does not have the correct 'serving' tag? I have not checked my tag sets yet but wanted to know if this would be the core reason to the problem. Also, most importantly, <strong>what are the differences between the two container types<\/strong>-I think having a better understanding of these two concepts would show me why I am unable to produce the correct image.<\/p>\n<hr \/>\n<p>This is how I deployed the sample endpoint:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>model = Model(model_data =...)\n\npredictor = model.deploy(initial_instance_count=...)\n<\/code><\/pre>\n<p>When I run the code, I get a model, an endpoint configuration, and an endpoint. I got the container type by clicking on model details within the AWS SageMaker console.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_created_time":1594709403670,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1594788261467,
        "Question_score":2,
        "Question_view_count":1223.0,
        "Poster_id":13850966.0,
        "Poster_created_time":1593662684510,
        "Poster_last_access_time":1599824259692,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":41.0,
        "Poster_views":11.0,
        "Answer_body":"<p>There are different versions for the framework containers. Since the framework version I'm using is 1.15, the image I got had to be in a tensorflow-inference container. If I used versions &lt;= 1.13, then I would get sagemaker-tensorflow-serving images. The two aren't the same, but there's no 'correct' container type.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":13850966.0,
        "Answerer_created_time":1593662684510,
        "Answerer_last_access_time":1599824259692,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":41.0,
        "Answerer_views":11.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62889537",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1595219623720,
        "Question_original_content":"tensorflow serv tensorflow infer contain type model fairli new tensorflow stuck process deploi endpoint recent succeed creat save model type model current servic sampl endpoint model creat extern check imag endpoint sai tensorflow infer direct want want us tensorflow serv contain follow tutori offici tensorflow serv github repo sampl model deploi correct tensorflow serv framework encount issu save model correct serv tag check tag set want know core reason problem importantli differ contain type think have better understand concept unabl produc correct imag deploi sampl endpoint model model model data predictor model deploi initi instanc count run code model endpoint configur endpoint got contain type click model detail consol",
        "Question_preprocessed_content":"tensorflow serv tensorflow infer fairli new tensorflow stuck process deploi endpoint recent succeed creat save model type model current servic sampl endpoint check imag endpoint sai direct want want us tensorflow serv contain encount issu save model correct serv tag check tag set want know core reason problem importantli differ contain think have better understand concept unabl produc correct imag deploi sampl endpoint run code model endpoint configur endpoint got contain type click model detail consol",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a SageMaker endpoint using TensorFlow. They have successfully created a Saved Model type model but the image they are using for the endpoint is 'tensorflow-inference' instead of the desired SageMaker TensorFlow serving container. The user is unsure if the issue is due to incorrect tag sets and is seeking a better understanding of the differences between the two container types.",
        "Question_gpt_summary":"user face challeng deploi endpoint tensorflow successfulli creat save model type model imag endpoint tensorflow infer instead desir tensorflow serv contain user unsur issu incorrect tag set seek better understand differ contain type",
        "Answer_original_content":"differ version framework contain framework version imag got tensorflow infer contain version tensorflow serv imag aren correct contain type",
        "Answer_preprocessed_content":"differ version framework contain framework version imag got contain version imag aren correct contain type",
        "Answer_gpt_summary_original":"Solution: The user is using the correct container type for their framework version. There is no \"correct\" container type, as it depends on the framework version being used.",
        "Answer_gpt_summary":"solut user correct contain type framework version correct contain type depend framework version"
    },
    {
        "Question_title":"Cannot execute AWS Sagemaker Notebook",
        "Question_body":"<p>I cannot execute sagemaker notebook anymore.<br>\nThe following error occurs.<\/p>\n\n<pre><code>Failed to start kernel\nAn error occurred (ThrottlingException) when calling the CreateApp operation (reached max retries: 4): \nRate exceeded\n<\/code><\/pre>\n\n<p>I checked my app list and there are only two.\nOne app is trying to delete but never stops, this could be one of the problem.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/M0iqo.png\" rel=\"nofollow noreferrer\">image<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1590373205720,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":965.0,
        "Poster_id":10126783.0,
        "Poster_created_time":1532422348876,
        "Poster_last_access_time":1606441872016,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":27.0,
        "Poster_views":4.0,
        "Answer_body":"<p>Happened to me too. Contact support and ask them to delete the kernel behind the scenes.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":121956.0,
        "Answerer_created_time":1244808478036,
        "Answerer_last_access_time":1663981477123,
        "Answerer_age":null,
        "Answerer_location":"Israel",
        "Answerer_reputation":4932.0,
        "Answerer_views":405.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61994821",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1590563891208,
        "Question_original_content":"execut notebook execut notebook anymor follow error occur fail start kernel error occur throttlingexcept call createapp oper reach max retri rate exceed check app list app try delet stop problem imag",
        "Question_preprocessed_content":"execut notebook execut notebook anymor follow error occur check app list app try delet stop problem imag",
        "Question_gpt_summary_original":"The user is facing challenges in executing AWS Sagemaker Notebook due to a ThrottlingException error that occurs when calling the CreateApp operation. The user suspects that the issue could be related to an app that is trying to delete but never stops.",
        "Question_gpt_summary":"user face challeng execut notebook throttlingexcept error occur call createapp oper user suspect issu relat app try delet stop",
        "Answer_original_content":"happen contact support ask delet kernel scene",
        "Answer_preprocessed_content":"happen contact support ask delet kernel scene",
        "Answer_gpt_summary_original":"Solution: Contact AWS support and request them to delete the kernel behind the scenes.",
        "Answer_gpt_summary":"solut contact aw support request delet kernel scene"
    },
    {
        "Question_title":"ClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.",
        "Question_body":"Hi Dears,\n\nI am building ML model using DeepAR Algorithm.\n\nI faced this error while i reached to this point : Error :\n\nClientError: An error occurred (UnknownOperationException) when calling the CreateHyperParameterTuningJob operation: The requested operation is not supported in the called region.\n\nCode: from sagemaker.tuner import ( IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner, ) from sagemaker import image_uris\n\ncontainer = image_uris.retrieve(region= 'af-south-1', framework=\"forecasting-deepar\")\n\ndeepar = sagemaker.estimator.Estimator( container, role, instance_count=1, instance_type=\"ml.m5.2xlarge\", use_spot_instances=True, # use spot instances max_run=1800, # max training time in seconds max_wait=1800, # seconds to wait for spot instance output_path=\"s3:\/\/{}\/{}\".format(bucket, output_path), sagemaker_session=sess, ) freq = \"D\" context_length = 300\n\ndeepar.set_hyperparameters( time_freq=freq, context_length=str(context_length), prediction_length=str(prediction_length) )\n\nCan you please help in solving the error? I have to do that in af-south-1 region.\n\nThanks Basem\n\nhyperparameter_ranges = { \"mini_batch_size\": IntegerParameter(100, 400), \"epochs\": IntegerParameter(200, 400), \"num_cells\": IntegerParameter(30, 100), \"likelihood\": CategoricalParameter([\"negative-binomial\", \"student-T\"]), \"learning_rate\": ContinuousParameter(0.0001, 0.1), }\n\nobjective_metric_name = \"test:RMSE\"\n\ntuner = HyperparameterTuner( deepar, objective_metric_name, hyperparameter_ranges, max_jobs=10, strategy=\"Bayesian\", objective_type=\"Minimize\", max_parallel_jobs=10, early_stopping_type=\"Auto\", )\n\ns3_input_train = sagemaker.inputs.TrainingInput( s3_data=\"s3:\/\/{}\/{}\/train\/\".format(bucket, prefix), content_type=\"json\" ) s3_input_test = sagemaker.inputs.TrainingInput( s3_data=\"s3:\/\/{}\/{}\/test\/\".format(bucket, prefix), content_type=\"json\" )\n\ntuner.fit({\"train\": s3_input_train, \"test\": s3_input_test}, include_cls_metadata=False) tuner.wait()",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1652655830031,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":135.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"The error message indicates that CreateHyperParameterTuningJob operation is not supported in the region you're currently using. If possible, try the notebook in a region that supports HPO jobs.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUkAwy2tG8QreIWmLTGIUAqg\/client-error-an-error-occurred-unknown-operation-exception-when-calling-the-create-hyper-parameter-tuning-job-operation-the-requested-operation-is-not-supported-in-the-called-region",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1653062956983,
        "Question_original_content":"clienterror error occur unknownoperationexcept call createhyperparametertuningjob oper request oper support call region dear build model deepar algorithm face error reach point error clienterror error occur unknownoperationexcept call createhyperparametertuningjob oper request oper support call region code tuner import integerparamet categoricalparamet continuousparamet hyperparametertun import imag uri contain imag uri retriev region south framework forecast deepar deepar estim estim contain role instanc count instanc type xlarg us spot instanc true us spot instanc max run max train time second max wait second wait spot instanc output path format bucket output path session sess freq context length deepar set hyperparamet time freq freq context length str context length predict length str predict length help solv error south region thank basem hyperparamet rang mini batch size integerparamet epoch integerparamet num cell integerparamet likelihood categoricalparamet neg binomi student learn rate continuousparamet object metric test rmse tuner hyperparametertun deepar object metric hyperparamet rang max job strategi bayesian object type minim max parallel job earli stop type auto input train input traininginput data train format bucket prefix content type json input test input traininginput data test format bucket prefix content type json tuner fit train input train test input test includ cl metadata fals tuner wait",
        "Question_preprocessed_content":"clienterror error occur call createhyperparametertuningjob oper request oper support call region dear build model deepar algorithm face error reach point error clienterror error occur call createhyperparametertuningjob oper request oper support call region code tuner import import contain deepar contain role us spot instanc max train time second second wait spot instanc freq help solv error region thank basem test rmse tuner hyperparametertun prefix prefix test",
        "Question_gpt_summary_original":"The user encountered an error while building an ML model using DeepAR Algorithm. The error message states that the requested operation is not supported in the called region. The user is trying to perform hyperparameter tuning in the af-south-1 region, but the operation is not supported in that region. The user is seeking help to solve the error.",
        "Question_gpt_summary":"user encount error build model deepar algorithm error messag state request oper support call region user try perform hyperparamet tune south region oper support region user seek help solv error",
        "Answer_original_content":"error messag indic createhyperparametertuningjob oper support region current possibl try notebook region support hpo job",
        "Answer_preprocessed_content":"error messag indic createhyperparametertuningjob oper support region current possibl try notebook region support hpo job",
        "Answer_gpt_summary_original":"Solution: The suggested solution is to try the notebook in a region that supports Hyperparameter Optimization (HPO) jobs since the CreateHyperParameterTuningJob operation is not supported in the current region.",
        "Answer_gpt_summary":"solut suggest solut try notebook region support hyperparamet optim hpo job createhyperparametertuningjob oper support current region"
    },
    {
        "Question_title":"Problems connecting to workspace using Azure Machine Learning SDK for Python",
        "Question_body":"I am trying to connect to my Azure ML workspace using SDK for python, using Virtual Studio Code to do so. After pip installing the needed SDK packages:\npip install azureml-sdk\npip install azureml-sdk[notebooks,automl,explain]\n\nI downloaded the .json configuration file for my workspace, made sure it was in the correct location for the file path and tried the following code (with my subscription id, resource group and workspace name in place of the fillers in this chunk of code):\n\n {\n     \"subscription_id\": \"1234567-abcde-890-fgh...\",\n     \"resource_group\": \"aml-resources\",\n     \"workspace_name\": \"aml-workspace\"\n }\n\n\n\nUpon executing this in my ipy kernel in Virutal Studio Code I got a UserErrorException (see image below, I have blocked out subscription id's and other sensitive information):\n\n\n\n\n\n\nI then tried this alternative way to connect to my workspace using the following code (again with my info filled in instead of the fillers in the code):\nfrom azureml.core import Workspace\n\n from azureml.core import Workspace\n    \n ws = Workspace.get(name='aml-workspace',\n                    subscription_id='1234567-abcde-890-fgh...',\n                    resource_group='aml-resources')\n    \n ws = Workspace.from_config()\n\n\n\nThis produced the same error upon execution. I have tried using different subscriptions with different workspace names and resource groups and it gives me the same error every time. It appears to be telling me I do not have access to the subscription that I am logged in to? I am unsure how to fix this. I am trying to do this as part of the lessons in the Microsoft Azure Data Scientist certification if anyone is familiar with that or has run into the same problem while trying to complete the modules for that certification provided through Microsoft.",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_created_time":1606083056563,
        "Question_favorite_count":13.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"can you try using InteractiveLoginAuthentication?\n\nbelow code might help you\n\n from azureml.core.authentication import InteractiveLoginAuthentication\n ia = InteractiveLoginAuthentication(tenant_id='YourTenant id')\n # You can find tenant id under azure active directory->properties\n ws = Workspace.get(name='aml-workspace',\n                     subscription_id='1234567-abcde-890-fgh...',\n                     resource_group='aml-resources',auth=ia)",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/171465\/problems-connecting-to-workspace-using-azure-machi.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1606097262372,
        "Question_original_content":"problem connect workspac sdk python try connect workspac sdk python virtual studio code pip instal need sdk packag pip instal sdk pip instal sdk notebook automl explain download json configur file workspac sure correct locat file path tri follow code subscript resourc group workspac place filler chunk code subscript abcd fgh resourc group aml resourc workspac aml workspac execut ipi kernel virut studio code got usererrorexcept imag block subscript sensit inform tri altern wai connect workspac follow code info fill instead filler code core import workspac core import workspac workspac aml workspac subscript abcd fgh resourc group aml resourc workspac config produc error execut tri differ subscript differ workspac name resourc group give error time appear tell access subscript log unsur fix try lesson microsoft azur data scientist certif familiar run problem try complet modul certif provid microsoft",
        "Question_preprocessed_content":"problem connect workspac sdk python try connect workspac sdk python virtual studio code pip instal need sdk packag pip instal sdk pip instal sdk download json configur file workspac sure correct locat file path tri follow code execut ipi kernel virut studio code got usererrorexcept tri altern wai connect workspac follow code core import workspac core import workspac produc error execut tri differ subscript differ workspac name resourc group give error time appear tell access subscript log unsur fix try lesson microsoft azur data scientist certif familiar run problem try complet modul certif provid microsoft",
        "Question_gpt_summary_original":"The user is facing challenges in connecting to their Azure ML workspace using SDK for Python in Virtual Studio Code. They have tried installing the necessary SDK packages and using two different codes to connect to their workspace, but both attempts resulted in a UserErrorException. The error message suggests that the user does not have access to the subscription they are logged in to, and they are unsure how to fix this issue.",
        "Question_gpt_summary":"user face challeng connect workspac sdk python virtual studio code tri instal necessari sdk packag differ code connect workspac attempt result usererrorexcept error messag suggest user access subscript log unsur fix issu",
        "Answer_original_content":"try interactiveloginauthent code help core authent import interactiveloginauthent interactiveloginauthent tenant yourten tenant azur activ directori properti workspac aml workspac subscript abcd fgh resourc group aml resourc auth",
        "Answer_preprocessed_content":"try interactiveloginauthent code help import interactiveloginauthent tenant azur activ",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to try using InteractiveLoginAuthentication. The user can use the provided code snippet to authenticate their login and access their Azure ML workspace.",
        "Answer_gpt_summary":"solut possibl solut mention discuss try interactiveloginauthent user us provid code snippet authent login access workspac"
    },
    {
        "Question_title":"SageMaker XGBoost Parquet Example Code Fails and Errors out. Bug?",
        "Question_body":"Hi, I'm trying to run the SageMaker XGBoost Parquet example linked here. I followed the exact same steps but using my own data. I uploaded my data, converted it to a pandas df. The train_df shape is (15279798, 32) while the test_df shape is (150848, 32). I then converted it to parquet files and uploaded it to an S3 bucket - per example instructions.\n\nMy error is as follows:\n\nFailure reason\nAlgorithmError: framework error: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/data_utils.py\", line 422, in _get_parquet_dmatrix_pipe_mode data = np.vstack(examples) File \"<__array_function__ internals>\", line 6, in vstack File \"\/miniconda3\/lib\/python3.7\/site-packages\/numpy\/core\/shape_base.py\", line 283, in vstack return _nx.concatenate(arrs, 0) File \"<__array_function__ internals>\", line 6, in concatenate ValueError: all the input array dimensions for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 32 and the array at index 1 has size 9 During handling of the above exception, another exception occurred: Traceback (most recent call last): File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_containers\/_trainer.py\", line 84, in train entrypoint() File \"\/miniconda3\/lib\/python3.7\/site-packages\/sagemaker_xgboost_container\/training.py\", line 94, in main train(framework.tr\n\n\n\nBut I'm confused because the train and test are the same shape and I added no extra code. My code below:\n\n# requires PyArrow installed\ntrain.to_parquet(\"Xgb_train.parquet\")\ntest.to_parquet(\"Xgb_test.parquet\")\n\n%%time\nsagemaker.Session().upload_data(\n    \"Xgb_train.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptrain\"\n)\n\nsagemaker.Session().upload_data(\n    \"Xgb_test.parquet\", bucket=bucket, key_prefix=prefix + \"\/\" + \"Ptest\"\n)\n\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-2\")\n\n%%time\nimport time\nfrom time import gmtime, strftime\n\njob_name = \"xgboost-parquet-example-training-\" + strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nprint(\"Training job\", job_name)\n\n# Ensure that the training and validation data folders generated above are reflected in the \"InputDataConfig\" parameter below.\n\ncreate_training_params = {\n    \"AlgorithmSpecification\": {\"TrainingImage\": container, \"TrainingInputMode\": \"Pipe\"},\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\"S3OutputPath\": bucket_path + \"\/\" + prefix + \"\/single-xgboost\"},\n    \"ResourceConfig\": {\"InstanceCount\": 1, \"InstanceType\": \"ml.m5.2xlarge\", \"VolumeSizeInGB\": 20},\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": {\n        \"max_depth\": \"5\",\n        \"eta\": \"0.2\",\n        \"gamma\": \"4\",\n        \"min_child_weight\": \"6\",\n        \"subsample\": \"0.7\",\n        \"objective\": \"reg:linear\",\n        \"num_round\": \"10\",\n        \"verbosity\": \"2\",\n    },\n    \"StoppingCondition\": {\"MaxRuntimeInSeconds\": 3600},\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptrain\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"S3Prefix\",\n                    \"S3Uri\": bucket_path + \"\/\" + prefix + \"\/Ptest\",\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                }\n            },\n            \"ContentType\": \"application\/x-parquet\",\n            \"CompressionType\": \"None\",\n        },\n    ],\n}\n\n\nclient = boto3.client(\"sagemaker\", region_name=region)\nclient.create_training_job(**create_training_params)\nprint(client)\nstatus = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\nprint(status)\nwhile status != \"Completed\" and status != \"Failed\":\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)[\"TrainingJobStatus\"]\n    print(status)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1648146766576,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":117.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"I just changed my bucket name and file names. It worked now.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqqbIbodsT42efRxxi1FLzw\/sage-maker-xg-boost-parquet-example-code-fails-and-errors-out-bug",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1648149098276,
        "Question_original_content":"xgboost parquet exampl code fail error bug try run xgboost parquet exampl link follow exact step data upload data convert panda train shape test shape convert parquet file upload bucket exampl instruct error follow failur reason algorithmerror framework error traceback recent file miniconda lib python site packag xgboost contain data util line parquet dmatrix pipe mode data vstack exampl file line vstack file miniconda lib python site packag numpi core shape base line vstack return concaten arr file line concaten valueerror input arrai dimens concaten axi match exactli dimens arrai index size arrai index size handl except except occur traceback recent file miniconda lib python site packag contain trainer line train entrypoint file miniconda lib python site packag xgboost contain train line main train framework confus train test shape ad extra code code requir pyarrow instal train parquet xgb train parquet test parquet xgb test parquet time session upload data xgb train parquet bucket bucket kei prefix prefix ptrain session upload data xgb test parquet bucket bucket kei prefix prefix ptest contain imag uri retriev xgboost region time import time time import gmtime strftime job xgboost parquet exampl train strftime gmtime print train job job ensur train valid data folder gener reflect inputdataconfig paramet creat train param algorithmspecif trainingimag contain traininginputmod pipe rolearn role outputdataconfig soutputpath bucket path prefix singl xgboost resourceconfig instancecount instancetyp xlarg volumesizeingb trainingjobnam job hyperparamet max depth eta gamma min child weight subsampl object reg linear num round verbos stoppingcondit maxruntimeinsecond inputdataconfig channelnam train datasourc sdatasourc sdatatyp sprefix suri bucket path prefix ptrain sdatadistributiontyp fullyrepl contenttyp applic parquet compressiontyp channelnam valid datasourc sdatasourc sdatatyp sprefix suri bucket path prefix ptest sdatadistributiontyp fullyrepl contenttyp applic parquet compressiontyp client boto client region region client creat train job creat train param print client statu client train job trainingjobnam job trainingjobstatu print statu statu complet statu fail time sleep statu client train job trainingjobnam job trainingjobstatu print statu",
        "Question_preprocessed_content":"xgboost parquet exampl code fail error bug try run xgboost parquet exampl link follow exact step data upload data convert panda shape shape convert parquet file upload bucket exampl instruct error follow failur reason algorithmerror framework error traceback file line data file intern line vstack file line vstack return file intern line concaten valueerror input arrai dimens concaten axi match exactli dimens arrai index size arrai index size handl except except occur traceback file line train entrypoint file line main confus train test shape ad extra code code requir pyarrow instal time bucket bucket ptrain bucket bucket ptest contain region time import time time import gmtime strftime gmtime print ensur train valid data folder gener reflect inputdataconfig paramet rolearn role outputdataconfig resourceconfig trainingjobnam hyperparamet stoppingcondit inputdataconfig client print statu print statu complet statu fail statu print",
        "Question_gpt_summary_original":"The user encountered an error while running the SageMaker XGBoost Parquet example code using their own data. The error message indicates a concatenation issue due to mismatched dimensions between the train and test data. The user is unsure why this error occurred since the train and test data have the same shape and no extra code was added.",
        "Question_gpt_summary":"user encount error run xgboost parquet exampl code data error messag indic concaten issu mismatch dimens train test data user unsur error occur train test data shape extra code ad",
        "Answer_original_content":"chang bucket file name work",
        "Answer_preprocessed_content":"chang bucket file name work",
        "Answer_gpt_summary_original":"Solution: The user was able to resolve the error by changing the bucket name and file names.",
        "Answer_gpt_summary":"solut user abl resolv error chang bucket file name"
    },
    {
        "Question_title":"Track to database, artifacts to specific destination",
        "Question_body":"<p>I am running <code>mlflow ui<\/code> and PostgreSQL db in docker compose.<\/p>\n<p>Mlflow UI container runs like this: <code>mlflow ui --backend-store-uri &quot;postgresql+psycopg2:\/\/postgres:passw0rd@database:5432\/postgres&quot; --host 0.0.0.0<\/code><\/p>\n<p>Then I run my models locally from jupyter, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>remote_server_uri = &quot;postgresql+psycopg2:\/\/postgres:passw0rd@localhost:5432\/postgres&quot;\nmlflow.set_tracking_uri(remote_server_uri)\nmlflow.set_experiment(&quot;exp2&quot;)\n\nX = np.array([-2, -1, 0, 1, 2, 1]).reshape(-1, 1)\ny = np.array([0, 0, 1, 1, 1, 0])\nlr = LogisticRegression()\nlr.fit(X, y)\nscore = lr.score(X, y)\nprint(&quot;Score: %s&quot; % score)\nwith mlflow.start_run():\n    mlflow.log_metric(&quot;score&quot;, score)\n<\/code><\/pre>\n<p>Everything works fine - experiments get logged into PostgreSQL and mlflow UI can read it from PostgreSQL .<\/p>\n<p>One thing that bothers me is that artifacts are stored locally into .\/mlruns folder. How to change it to save it somewhere else?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1642929527630,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1642929807488,
        "Question_score":0,
        "Question_view_count":361.0,
        "Poster_id":6156353.0,
        "Poster_created_time":1459778195087,
        "Poster_last_access_time":1664034598608,
        "Poster_age":null,
        "Poster_location":"Czech Republic",
        "Poster_reputation":622.0,
        "Poster_views":59.0,
        "Answer_body":"<p>So apparently <code>--default-artifact-root<\/code> argument has to be used when launching server\/ui. The only downside is that that default artifact root is relative to development environment, so if you are running mlflow server in docker and specify default-artifact-root to e.g. <code>some\/path<\/code> then the artifacts are going to be saved to your <strong>local machine<\/strong> to that path (<strong>not inside docker container<\/strong>). Probably the best solution is to use remote storage such as S3\/Blob.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":6156353.0,
        "Answerer_created_time":1459778195087,
        "Answerer_last_access_time":1664034598608,
        "Answerer_age":null,
        "Answerer_location":"Czech Republic",
        "Answerer_reputation":622.0,
        "Answerer_views":59.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70820661",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1642961026120,
        "Question_original_content":"track databas artifact specif destin run postgresql docker compos contain run like backend store uri postgresql psycopg postgr passwrd databas postgr host run model local jupyt remot server uri postgresql psycopg postgr passwrd localhost postgr set track uri remot server uri set experi exp arrai reshap arrai logisticregress fit score score print score score start run log metric score score work fine experi log postgresql read postgresql thing bother artifact store local mlrun folder chang save",
        "Question_preprocessed_content":"track databas artifact specif destin run postgresql docker compos contain run like run model local jupyt work fine experi log postgresql read postgresql thing bother artifact store local folder chang save",
        "Question_gpt_summary_original":"The user is running mlflow ui and PostgreSQL db in docker compose. They are able to run their models locally from jupyter and log experiments into PostgreSQL. However, the user is facing a challenge of storing artifacts locally into the .\/mlruns folder and wants to know how to change it to save it somewhere else.",
        "Question_gpt_summary":"user run postgresql docker compos abl run model local jupyt log experi postgresql user face challeng store artifact local mlrun folder want know chang save",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"ClearML SSH port forwarding fileserver not available in WEB Ui",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5 with SSH Port Forwarding and not beeing able to see my debug samples.<\/p>\n<p>My setup:<\/p>\n<ul>\n<li>ClearML server on hostA<\/li>\n<li>SSH Tunnel connections to access Web App from working machine via localhost:18080<\/li>\n<li>Web App: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<li>Fileserver: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<\/ul>\n<p>In Web App under Task-&gt;Results-&gt;Debug Samples the Images are still refrenced by localhost:8081<\/p>\n<p>Where can I set the fileserver URL to be localhost:18081 in Web App?\nI tried ~\/clearml.conf, but this did not work ( I think it is for my python script ).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1610384778623,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":121.0,
        "Poster_id":14569139.0,
        "Poster_created_time":1604391794420,
        "Poster_last_access_time":1663670435372,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":89.0,
        "Poster_views":4.0,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<p>In ClearML, debug images' URL is registered once they are uploaded to the fileserver. The WebApp doesn't actually decide on the URL for each debug image, but rather obtains it for each debug image from the server. This allows you to potentially upload debug images to a variety of storage targets, ClearML File Server simply being the most convenient, built-in option.<\/p>\n<p>So, the WebApp will always look for <code>localhost:8008<\/code> for debug images that have already been uploaded to the fileserver and contain <code>localhost:8080<\/code> in their URL.\nA possible solution is to simply add another tunnel in the form of <code>ssh -N -L 8081:127.0.0.1:8081 user@hostA<\/code>.<\/p>\n<p>For future experiments, you can choose to keep using <code>8081<\/code> (and keep using this new tunnel), or to change the default fileserver URL in <code>clearml.conf<\/code> to point to port <code>localhost:18081<\/code>, assuming you're running your experiments from the same machine where the tunnel to <code>18081<\/code> exists.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":11682840.0,
        "Answerer_created_time":1561143508792,
        "Answerer_last_access_time":1659992319200,
        "Answerer_age":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation":489.0,
        "Answerer_views":60.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65671395",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1610390345336,
        "Question_original_content":"ssh port forward fileserv avail web try us server ubuntu ssh port forward bee abl debug sampl setup server hosta ssh tunnel connect access web app work machin localhost web app ssh user hosta fileserv ssh user hosta web app task result debug sampl imag refrenc localhost set fileserv url localhost web app tri conf work think python script",
        "Question_preprocessed_content":"ssh port forward fileserv avail web try us server ubuntu ssh port forward bee abl debug sampl setup server hosta ssh tunnel connect access web app work machin localhost web app fileserv web app sampl imag refrenc localhost set fileserv url localhost web app tri work",
        "Question_gpt_summary_original":"The user is facing challenges while using ClearML server on their Ubuntu 18.04.5 with SSH Port Forwarding. They are unable to see their debug samples and are trying to set the fileserver URL to localhost:18081 in the Web App, but are unable to find the correct location to do so. The images are still referenced by localhost:8081 in the Web App under Task->Results->Debug Samples.",
        "Question_gpt_summary":"user face challeng server ubuntu ssh port forward unabl debug sampl try set fileserv url localhost web app unabl correct locat imag referenc localhost web app task result debug sampl",
        "Answer_original_content":"disclaim member team train debug imag url regist upload fileserv webapp actual decid url debug imag obtain debug imag server allow potenti upload debug imag varieti storag target file server simpli conveni built option webapp look localhost debug imag upload fileserv contain localhost url possibl solut simpli add tunnel form ssh user hosta futur experi choos new tunnel chang default fileserv url conf point port localhost assum run experi machin tunnel exist",
        "Answer_preprocessed_content":"disclaim member team debug imag url regist upload fileserv webapp actual decid url debug imag obtain debug imag server allow potenti upload debug imag varieti storag target file server simpli conveni option webapp look debug imag upload fileserv contain url possibl solut simpli add tunnel form futur experi choos chang default fileserv url point port assum run experi machin tunnel exist",
        "Answer_gpt_summary_original":"Solution:\n- The WebApp obtains the URL for each debug image from the server, so changing the fileserver URL to localhost:18081 in the Web App will not work.\n- A possible solution is to add another tunnel in the form of `ssh -N -L 8081:127.0.0.1:8081 user@hostA`.\n- For future experiments, the user can choose to keep using 8081 (and keep using the new tunnel), or change the default fileserver URL in `clearml.conf` to point to port localhost:18081, assuming they are running their experiments from the same machine where the tunnel to 18081 exists.",
        "Answer_gpt_summary":"solut webapp obtain url debug imag server chang fileserv url localhost web app work possibl solut add tunnel form ssh user hosta futur experi user choos new tunnel chang default fileserv url conf point port localhost assum run experi machin tunnel exist"
    },
    {
        "Question_title":"Gluonnlp installation not found on Sagemaker jupyter notebook",
        "Question_body":"<p><a href=\"https:\/\/i.stack.imgur.com\/I8c93.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/I8c93.png\" alt=\"enter image description here\"><\/a>I am attempting to install Gluonnlp to a sagemaker jupyter notebook. Im using the command <code>!sudo pip3 install gluonnlp<\/code> to install.  Which is successful.  However on import I get <code>ModuleNotFoundError: No module named 'gluonnlp'<\/code><\/p>\n\n<p>I got the same issue when attempting to install mxnet with pip in the same notebook.  It was resolved when I conda installed mxnet instead.  However conda install has not been working for gluonnlp as it cannot find the package.  I can't seem to find a way to conda install gluonnlp.  Any suggestions would be highly appreciated.<\/p>\n\n<p>Here are some of the commands I have tried<\/p>\n\n<p><code>!sudo pip3 install gluonnlp<\/code><\/p>\n\n<p><code>!conda install gluonnlp<\/code> --> Anaconda cant find the package in any channels<\/p>\n\n<pre><code>!conda install pip --y\n!sudo pip3 install gluonnlp\n\n!sudo pip3 install gluonnlp\n\n!conda install -c conda-forge gluonnlp --y\n<\/code><\/pre>\n\n<p>All these commands on my import \nimport warnings<\/p>\n\n<pre><code>warnings.filterwarnings('ignore')\n\nimport io\nimport random\nimport numpy as np\nimport mxnet as mx\nimport gluonnlp as nlp\nfrom bert import data, model\n<\/code><\/pre>\n\n<p>result in the error<\/p>\n\n<pre><code>ModuleNotFoundError: No module named 'gluonnlp'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_created_time":1564111030610,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1564413127092,
        "Question_score":1,
        "Question_view_count":2015.0,
        "Poster_id":10094585.0,
        "Poster_created_time":1531840489147,
        "Poster_last_access_time":1636067734716,
        "Poster_age":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation":425.0,
        "Poster_views":92.0,
        "Answer_body":"<p>this is as simple as creating a Jupyter notebook using the 'conda_mxnet_p36' kernel, and adding a cell containing:<\/p>\n\n<pre><code>!pip install gluonnlp\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":4686192.0,
        "Answerer_created_time":1426694564423,
        "Answerer_last_access_time":1659633906700,
        "Answerer_age":null,
        "Answerer_location":"Paris",
        "Answerer_reputation":2425.0,
        "Answerer_views":459.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57212696",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1564315813180,
        "Question_original_content":"gluonnlp instal jupyt notebook attempt instal gluonnlp jupyt notebook command sudo pip instal gluonnlp instal success import modulenotfounderror modul name gluonnlp got issu attempt instal mxnet pip notebook resolv conda instal mxnet instead conda instal work gluonnlp packag wai conda instal gluonnlp suggest highli appreci command tri sudo pip instal gluonnlp conda instal gluonnlp anaconda packag channel conda instal pip sudo pip instal gluonnlp sudo pip instal gluonnlp conda instal conda forg gluonnlp command import import warn warn filterwarn ignor import import random import numpi import mxnet import gluonnlp nlp bert import data model result error modulenotfounderror modul name gluonnlp",
        "Question_preprocessed_content":"gluonnlp instal jupyt notebook attempt instal gluonnlp jupyt notebook command instal success import got issu attempt instal mxnet pip notebook resolv conda instal mxnet instead conda instal work gluonnlp packag wai conda instal gluonnlp suggest highli appreci command tri anaconda packag channel command import import warn result error",
        "Question_gpt_summary_original":"The user is facing challenges installing and importing Gluonnlp on a Sagemaker Jupyter notebook. The user has successfully installed Gluonnlp using pip3, but on import, the user encounters a \"ModuleNotFoundError\". The user has also tried to install Gluonnlp using conda, but it cannot find the package in any channels. The user is seeking suggestions on how to resolve the issue.",
        "Question_gpt_summary":"user face challeng instal import gluonnlp jupyt notebook user successfulli instal gluonnlp pip import user encount modulenotfounderror user tri instal gluonnlp conda packag channel user seek suggest resolv issu",
        "Answer_original_content":"simpl creat jupyt notebook conda mxnet kernel ad cell contain pip instal gluonnlp",
        "Answer_preprocessed_content":"simpl creat jupyt notebook kernel ad cell contain",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to create a Jupyter notebook using the 'conda_mxnet_p36' kernel and install Gluonnlp using pip by adding a cell containing \"!pip install gluonnlp\".",
        "Answer_gpt_summary":"solut solut suggest discuss creat jupyt notebook conda mxnet kernel instal gluonnlp pip ad cell contain pip instal gluonnlp"
    },
    {
        "Question_title":"SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ?",
        "Question_body":"Hi, does SageMaker PyTorch Hosting 1.6 works only with artifacts named model.pth ? I'm trying this sample with 1.6 and the deployment fails with error\n\nFileNotFoundError: [Errno 2] No such file or directory: '\/opt\/ml\/model\/model.pth'\n\n\nthe documentation doesn't mention such a constraint",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1610118101000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":174.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Yes - from the thread on this open issue: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/issues\/86\n\nIn the issue thread they note that the new Pytorch 1.6 image requires that the model filename is model.pth, linking to the relevant code where this default is set: https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/9a6869e\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L121\n\nAlso noted in the thread is that users have successfully adapted their code to use torchserve in Pytorch 1.6 by changing it to save their model in a file named model.pth. Once renamed, they were still able to use custom inference scripts to load their model by defining a custom model_fn: https:\/\/github.com\/data-science-on-aws\/workshop\/blob\/374329adf15bf1810bfc4a9e73501ee5d3b4e0f5\/09_deploy\/wip\/pytorch\/code\/inference.py",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUn-IyC9nySDKKibLL3Lvw3A\/sage-maker-py-torch-hosting-1-6-works-only-with-artifacts-named-model-pth",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1610121735000,
        "Question_original_content":"pytorch host work artifact name model pth pytorch host work artifact name model pth try sampl deploy fail error filenotfounderror errno file directori opt model model pth document mention constraint",
        "Question_preprocessed_content":"pytorch host work artifact name pytorch host work artifact name try sampl deploy fail error filenotfounderror file directori document mention constraint",
        "Question_gpt_summary_original":"The user is facing a challenge with SageMaker PyTorch Hosting 1.6 as the deployment fails with an error stating that the model.pth file cannot be found. The user is questioning if the hosting only works with artifacts named model.pth, as this constraint is not mentioned in the documentation.",
        "Question_gpt_summary":"user face challeng pytorch host deploy fail error state model pth file user question host work artifact name model pth constraint mention document",
        "Answer_original_content":"ye thread open issu http github com aw pytorch infer toolkit issu issu thread note new pytorch imag requir model filenam model pth link relev code default set http github com aw pytorch infer toolkit blob src pytorch serv contain torchserv note thread user successfulli adapt code us torchserv pytorch chang save model file name model pth renam abl us custom infer script load model defin custom model http github com data scienc aw workshop blob adfbfbfcaeeedbef deploi wip pytorch code infer",
        "Answer_preprocessed_content":"ye thread open issu issu thread note new pytorch imag requir model filenam link relev code default set note thread user successfulli adapt code us torchserv pytorch chang save model file name renam abl us custom infer script load model defin custom",
        "Answer_gpt_summary_original":"The solution to the challenge is that the new PyTorch 1.6 image requires the model filename to be model.pth. This constraint is not mentioned in the documentation. Users have successfully adapted their code to use torchserve in PyTorch 1.6 by changing it to save their model in a file named model.pth. Once renamed, they were still able to use custom inference scripts to load their model by defining a custom model_fn.",
        "Answer_gpt_summary":"solut challeng new pytorch imag requir model filenam model pth constraint mention document user successfulli adapt code us torchserv pytorch chang save model file name model pth renam abl us custom infer script load model defin custom model"
    },
    {
        "Question_title":"Customer Error: Additional hyperparameters are not allowed - Image classification training- Sagemaker",
        "Question_body":"<p>I'm learning image classification with Amazon SageMaker. I was trying to follow their  learning demo <strong>Image classification transfer learning demo<\/strong> (<code>Image-classification-transfer-learning-highlevel.ipynb<\/code>)<\/p>\n\n<p>I got up to Start the training. Executed below.<\/p>\n\n<pre><code>ic.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>Set the hyper parameters as given in the demo<\/p>\n\n<pre><code>ic.set_hyperparameters(num_layers=18,\n                             use_pretrained_model=1,\n                             image_shape = \"3,224,224\",\n                             num_classes=257,\n                             num_training_samples=15420,\n                             mini_batch_size=128,\n                             epochs=1,\n                             learning_rate=0.01,\n                             precission_dtype='float32')\n<\/code><\/pre>\n\n<p>Got the client error<\/p>\n\n<pre><code>ERROR 140291262150464] Customer Error: Additional hyperparameters are not allowed (u'precission_dtype' was unexpected) (caused by ValidationError)\n\nCaused by: Additional properties are not allowed (u'precission_dtype' was unexpected)\n<\/code><\/pre>\n\n<p>Does anyone know how to overcome this? I'm also reporting this to aws support. Posting here for sharing and get a fix. Thanks !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1539753916957,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":146.0,
        "Poster_id":3756762.0,
        "Poster_created_time":1403185902747,
        "Poster_last_access_time":1547107848667,
        "Poster_age":null,
        "Poster_location":"Colombo, Sri Lanka",
        "Poster_reputation":169.0,
        "Poster_views":81.0,
        "Answer_body":"<p>Assuming you are referring to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb<\/a> - you have a typo, it's <code>precision_dtype<\/code>, not <code>precission_dtype<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":1502599.0,
        "Answerer_created_time":1341441916656,
        "Answerer_last_access_time":1635983985347,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":5985.0,
        "Answerer_views":161.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52847777",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1539831993640,
        "Question_original_content":"custom error addit hyperparamet allow imag classif train learn imag classif try follow learn demo imag classif transfer learn demo imag classif transfer learn highlevel ipynb got start train execut fit input data channel log true set hyper paramet given demo set hyperparamet num layer us pretrain model imag shape num class num train sampl mini batch size epoch learn rate preciss dtype float got client error error custom error addit hyperparamet allow preciss dtype unexpect caus validationerror caus addit properti allow preciss dtype unexpect know overcom report aw support post share fix thank",
        "Question_preprocessed_content":"custom error addit hyperparamet allow imag classif train learn imag classif try follow learn demo imag classif transfer learn demo got start train execut set hyper paramet given demo got client error know overcom report aw support post share fix thank",
        "Question_gpt_summary_original":"The user encountered an error while trying to set hyperparameters for image classification training using Amazon SageMaker. The error message stated that additional hyperparameters are not allowed, specifically the \"precission_dtype\" parameter. The user is seeking help to overcome this issue and has reported it to AWS support.",
        "Question_gpt_summary":"user encount error try set hyperparamet imag classif train error messag state addit hyperparamet allow specif preciss dtype paramet user seek help overcom issu report aw support",
        "Answer_original_content":"assum refer http github com awslab amazon exampl blob master introduct amazon algorithm imageclassif caltech imag classif transfer learn highlevel ipynb typo precis dtype preciss dtype",
        "Answer_preprocessed_content":"assum refer typo",
        "Answer_gpt_summary_original":"Solution: The error message was caused by a typo in the hyperparameter name. The correct parameter name is \"precision_dtype\" instead of \"precission_dtype\".",
        "Answer_gpt_summary":"solut error messag caus typo hyperparamet correct paramet precis dtype instead preciss dtype"
    },
    {
        "Question_title":"git-ignore dvc.lock in repositories where only the DVC pipelines are used",
        "Question_body":"<p>I want to use the pipeline functionality of dvc in a git repository. The data is managed otherwise and should not be versioned by dvc. The only functionality which is needed is that dvc reproduces the needed steps of the pipeline when <code>dvc repro<\/code> is called. Checking out the repository on a new system should lead to an 'empty' repository, where none of the pipeline steps are stored.<\/p>\n<p>Thus, - if I understand correctly - there is no need to track the dvc.lock file in the repository. However, adding dvc.lock to the .gitginore file leads to an error message:<\/p>\n<pre><code>ERROR: 'dvc.lock' is git-ignored.\n<\/code><\/pre>\n<p>Is there any way to disable the dvc.lock in .gitignore check for this usecase?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1624362030233,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":493.0,
        "Poster_id":10670040.0,
        "Poster_created_time":1542537900087,
        "Poster_last_access_time":1663940500712,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":147.0,
        "Poster_views":5.0,
        "Answer_body":"<p>This is definitely possible, as DVC features are loosely coupled to one another. You can do pipelining by writing your dvc.yaml file(s), but avoid data management\/versioning by using <code>cache: false<\/code> in the stage outputs (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#output-subfields\" rel=\"nofollow noreferrer\"><code>outs<\/code> field<\/a>). See also helper <code>dvc stage add -O<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/stage\/add#options\" rel=\"nofollow noreferrer\">big O<\/a>, alias of <code>--outs-no-cache<\/code>).<\/p>\n<p>And the same for initial data dependencies, you can <code>dvc add --no-commit<\/code> them (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#options\" rel=\"nofollow noreferrer\">ref<\/a>).<\/p>\n<p>You do want to track <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#dvclock-file\" rel=\"nofollow noreferrer\">dvc.lock<\/a> in Git though, so that DVC can determine the latest stage of the pipeline associated with the Git commit in every repo copy or branch.<\/p>\n<p>You'll be responsible for placing the right data files\/dirs (matching .dvc files and dvc.lock) in the workspace for <code>dvc repro<\/code> or <code>dvc exp run<\/code> to behave as expected. <code>dvc checkout<\/code> won't be able to help you.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":761963.0,
        "Answerer_created_time":1305851487736,
        "Answerer_last_access_time":1663864920647,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":5993.0,
        "Answerer_views":457.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68082912",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_closed_time":1624393487732,
        "Question_original_content":"git ignor lock repositori pipelin want us pipelin function git repositori data manag version function need reproduc need step pipelin repro call check repositori new lead repositori pipelin step store understand correctli need track lock file repositori ad lock gitginor file lead error messag error lock git ignor wai disabl lock gitignor check usecas",
        "Question_preprocessed_content":"lock repositori pipelin want us pipelin function git repositori data manag version function need reproduc need step pipelin call check repositori new lead repositori pipelin step store understand correctli need track lock file repositori ad lock gitginor file lead error messag wai disabl lock gitignor check usecas",
        "Question_gpt_summary_original":"The user wants to use the pipeline functionality of dvc in a git repository without versioning the data by dvc. However, adding dvc.lock to the .gitignore file leads to an error message, and the user is looking for a way to disable the dvc.lock in .gitignore check for this use case.",
        "Question_gpt_summary":"user want us pipelin function git repositori version data ad lock gitignor file lead error messag user look wai disabl lock gitignor check us case",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"How do I extend the waiting time of Azure speech-to-text API in Python?",
        "Question_body":"When using speech-to-text to transfer audio file to text, I found that the function would stop working if human voices haven't occurred for about 5 seconds. In my case, what I want to transfer is audios of interviews, which would often contain some advertisements or music in the middle of it, and when this happens, the speech-to-text would only transfer the first half of the whole audio, and report an error that \"No speech could be recognized\".\nIn this case, how can I extend the waiting time of that in order to transfer the whole file in Python codes?",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1649087644937,
        "Question_favorite_count":12.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hello @MuyaoHu-4139\n\nI think there are two solutions you can have a try in Python SDK:\n\nThere is a 'set_property' method on the config to allow you to set parameters to your request, which can change the default silence time:: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azure-cognitiveservices-speech\/azure.cognitiveservices.speech.propertycollection?view=azure-python#azure-cognitiveservices-speech-propertycollection-set-property\n\nThis way you can set the EndSilenceTimeout (PropertyIDs in Pyhton: https:\/\/docs.microsoft.com\/en-us\/python\/api\/azure-cognitiveservices-speech\/azure.cognitiveservices.speech.propertyid?view=azure-python#fields)\n\n\n\n\nPlease notice, the time is as \"ms\". Hope above helps!\n\n\n\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/799565\/how-do-i-extend-the-waiting-time-of-azure-speech-t.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1649205144236,
        "Question_original_content":"extend wait time azur speech text api python speech text transfer audio file text function stop work human voic haven occur second case want transfer audio interview contain advertis music middl happen speech text transfer half audio report error speech recogn case extend wait time order transfer file python code",
        "Question_preprocessed_content":"extend wait time azur api python transfer audio file text function stop work human voic haven occur second case want transfer audio interview contain advertis music middl happen transfer half audio report error speech recogn case extend wait time order transfer file python code",
        "Question_gpt_summary_original":"The user is facing challenges with the Azure speech-to-text API in Python, as the function stops working if there is no human voice for about 5 seconds. This is causing issues when transferring audio files of interviews that contain advertisements or music in the middle, as the API only transfers the first half of the audio and reports an error. The user is seeking a solution to extend the waiting time in Python codes to transfer the whole file.",
        "Question_gpt_summary":"user face challeng azur speech text api python function stop work human voic second caus issu transfer audio file interview contain advertis music middl api transfer half audio report error user seek solut extend wait time python code transfer file",
        "Answer_original_content":"hello muyaohu think solut try python sdk set properti method config allow set paramet request chang default silenc time http doc microsoft com python api azur cognitiveservic speech azur cognitiveservic speech propertycollect view azur python azur cognitiveservic speech propertycollect set properti wai set endsilencetimeout propertyid pyhton http doc microsoft com python api azur cognitiveservic speech azur cognitiveservic speech propertyid view azur python field notic time hope help regard yutong kindli accept answer feel help thank",
        "Answer_preprocessed_content":"hello think solut try python sdk method config allow set paramet request chang default silenc time wai set endsilencetimeout notic time hope help regard yutong kindli accept answer feel help thank",
        "Answer_gpt_summary_original":"Two solutions were suggested in the discussion. The first solution is to use the 'set_property' method on the config to change the default silence time. This can be done by setting the EndSilenceTimeout parameter in milliseconds. The second solution is not provided.",
        "Answer_gpt_summary":"solut suggest discuss solut us set properti method config chang default silenc time set endsilencetimeout paramet millisecond second solut provid"
    },
    {
        "Question_title":"Azure ML PipelineData with DataTransferStep results in 0 bytes file",
        "Question_body":"<p>I am building an Azure ML pipeline with the azureml Python SDK. The pipeline calls a PythonScriptStep which stores data on the workspaceblobstore of the AML workspace. <\/p>\n\n<p>I would like to extend the pipeline to export the pipeline data to an Azure Data Lake (Gen 1). Connecting the output of the PythonScriptStep directly to Azure Data Lake (Gen 1) is not supported by Azure ML as far as I understand. Therefore, I added an extra DataTransferStep to the pipeline, which takes the output from the PythonScriptStep as input directly into the DataTransferStep. According to the Microsoft documentation this should be possible.<\/p>\n\n<p>So far I have built this solution, only this results in a file of 0 bytes on the Gen 1 Data Lake. I think the output_export_blob PipelineData does not correctly references the test.csv, and therefore the DataTransferStep cannot find the input. How can I connect the DataTransferStep correctly with the PipelineData output from the PythonScriptStep?<\/p>\n\n<p>Example I followed:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-data-dependency-steps.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-with-data-dependency-steps.ipynb<\/a><\/p>\n\n<p>pipeline.py<\/p>\n\n<pre><code>input_dataset = delimited_dataset(\n    datastore=prdadls_datastore,\n    folderpath=FOLDER_PATH_INPUT,\n    filepath=INPUT_PATH\n)\n\noutput_export_blob = PipelineData(\n    'export_blob',\n    datastore=workspaceblobstore_datastore,\n)\n\ntest_step = PythonScriptStep(\n    script_name=\"test_upload_stackoverflow.py\",\n    arguments=[\n        \"--output_extract\", output_export_blob,\n    ],\n    inputs=[\n        input_dataset.as_named_input('input'),\n    ],\n    outputs=[output_export_blob],\n    compute_target=aml_compute,\n    source_directory=\".\"\n)\n\noutput_export_adls = DataReference(\n    datastore=prdadls_datastore, \n    path_on_datastore=os.path.join(FOLDER_PATH_OUTPUT, 'test.csv'),\n    data_reference_name='export_adls'        \n)\n\nexport_to_adls = DataTransferStep(\n    name='export_output_to_adls',\n    source_data_reference=output_export_blob,\n    source_reference_type='file',\n    destination_data_reference=output_export_adls,\n    compute_target=adf_compute\n)\n\npipeline = Pipeline(\n    workspace=aml_workspace, \n    steps=[\n        test_step, \n        export_to_adls\n    ]\n)\n<\/code><\/pre>\n\n<p>test_upload_stackoverflow.py<\/p>\n\n<pre><code>import os\nimport pathlib\nfrom azureml.core import Datastore, Run\n\nparser = argparse.ArgumentParser(\"train\")\nparser.add_argument(\"--output_extract\", type=str)\nargs = parser.parse_args() \n\nrun = Run.get_context()\ndf_data_all = (\n    run\n    .input_datasets[\"input\"]\n    .to_pandas_dataframe()\n)\n\nos.makedirs(args.output_extract, exist_ok=True)\ndf_data_all.to_csv(\n    os.path.join(args.output_extract, \"test.csv\"), \n    index=False\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1591811660073,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1591812659792,
        "Question_score":2,
        "Question_view_count":917.0,
        "Poster_id":8858149.0,
        "Poster_created_time":1509387489888,
        "Poster_last_access_time":1662618596132,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":73.0,
        "Poster_views":24.0,
        "Answer_body":"<p>The code example is immensely helpful. Thanks for that. You're right that it can be confusing to get <code>PythonScriptStep -&gt; PipelineData<\/code>. Working initially even without the <code>DataTransferStep<\/code>.<\/p>\n\n<p>I don't know 100% what's going on, but I thought I'd spitball some ideas:<\/p>\n\n<ol>\n<li>Does your <code>PipelineData<\/code>,  <code>export_blob<\/code>, contain the \"test.csv\" file? I would verify that before troubleshooting the <code>DataTransferStep<\/code>. You can verify this using the SDK, or more easily with the UI.\n\n<ol>\n<li>Go to the PipelineRun page, click on the <code>PythonScriptStep<\/code> in question.<\/li>\n<li>On \"Outputs + Logs\" page, there's a \"Data Outputs\" Section (that is slow to load initially)<\/li>\n<li>Open it and you'll see the output PipelineDatas then click on \"View Output\"<\/li>\n<li>Navigate to given path either in the Azure Portal or Azure Storage Explorer.\n<a href=\"https:\/\/i.stack.imgur.com\/9LaEq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9LaEq.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/XbnhC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XbnhC.png\" alt=\"enter image description here\"><\/a><\/li>\n<\/ol><\/li>\n<li>In <code>test_upload_stackoverflow.py<\/code> you are treating the <code>PipelineData<\/code> as a directory when call <code>.to_csv()<\/code> as opposed to a file which would be you just calling <code>df_data_all.to_csv(args.output_extract, index=False)<\/code>. Perhaps try defining the <code>PipelineData<\/code> with <code>is_directory=True<\/code>. Not sure if this is required though.<\/li>\n<\/ol>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":3842610.0,
        "Answerer_created_time":1405457120427,
        "Answerer_last_access_time":1663947733100,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":3359.0,
        "Answerer_views":555.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62310010",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1591813300647,
        "Question_original_content":"pipelinedata datatransferstep result byte file build pipelin python sdk pipelin call pythonscriptstep store data workspaceblobstor aml workspac like extend pipelin export pipelin data azur data lake gen connect output pythonscriptstep directli azur data lake gen support far understand ad extra datatransferstep pipelin take output pythonscriptstep input directli datatransferstep accord microsoft document possibl far built solut result file byte gen data lake think output export blob pipelinedata correctli refer test csv datatransferstep input connect datatransferstep correctli pipelinedata output pythonscriptstep exampl follow http github com azur machinelearningnotebook blob master us machin learn pipelin intro pipelin aml pipelin data depend step ipynb pipelin input dataset delimit dataset datastor prdadl datastor folderpath folder path input filepath input path output export blob pipelinedata export blob datastor workspaceblobstor datastor test step pythonscriptstep script test upload stackoverflow argument output extract output export blob input input dataset name input input output output export blob comput target aml comput sourc directori output export adl datarefer datastor prdadl datastor path datastor path join folder path output test csv data refer export adl export adl datatransferstep export output adl sourc data refer output export blob sourc refer type file destin data refer output export adl comput target adf comput pipelin pipelin workspac aml workspac step test step export adl test upload stackoverflow import import pathlib core import datastor run parser argpars argumentpars train parser add argument output extract type str arg parser pars arg run run context data run input dataset input panda datafram makedir arg output extract exist true data csv path join arg output extract test csv index fals",
        "Question_preprocessed_content":"pipelinedata datatransferstep result byte file build pipelin python sdk pipelin call pythonscriptstep store data workspaceblobstor aml workspac like extend pipelin export pipelin data azur data lake connect output pythonscriptstep directli azur data lake support far understand ad extra datatransferstep pipelin take output pythonscriptstep input directli datatransferstep accord microsoft document possibl far built solut result file byte gen data lake think pipelinedata correctli refer datatransferstep input connect datatransferstep correctli pipelinedata output pythonscriptstep exampl follow",
        "Question_gpt_summary_original":"The user is building an Azure ML pipeline with the azureml Python SDK and wants to export pipeline data to an Azure Data Lake (Gen 1). However, connecting the output of the PythonScriptStep directly to Azure Data Lake (Gen 1) is not supported by Azure ML. Therefore, the user added an extra DataTransferStep to the pipeline, which takes the output from the PythonScriptStep as input directly into the DataTransferStep. However, this results in a file of 0 bytes on the Gen 1 Data Lake, and the user suspects that the output_export_blob PipelineData does not correctly reference the test.csv, making it impossible for the DataTransferStep to find the input. The user is seeking guidance on how to connect the DataTransferStep correctly with the PipelineData output from the PythonScriptStep.",
        "Question_gpt_summary":"user build pipelin python sdk want export pipelin data azur data lake gen connect output pythonscriptstep directli azur data lake gen support user ad extra datatransferstep pipelin take output pythonscriptstep input directli datatransferstep result file byte gen data lake user suspect output export blob pipelinedata correctli refer test csv make imposs datatransferstep input user seek guidanc connect datatransferstep correctli pipelinedata output pythonscriptstep",
        "Answer_original_content":"code exampl immens help thank right confus pythonscriptstep pipelinedata work initi datatransferstep know go thought spitbal idea pipelinedata export blob contain test csv file verifi troubleshoot datatransferstep verifi sdk easili pipelinerun page click pythonscriptstep question output log page data output section slow load initi open output pipelinedata click view output navig given path azur portal azur storag explor test upload stackoverflow treat pipelinedata directori csv oppos file call data csv arg output extract index fals try defin pipelinedata directori true sure requir",
        "Answer_preprocessed_content":"code exampl immens help thank right confus work initi know go thought spitbal idea contain file verifi troubleshoot verifi sdk easili pipelinerun page click question output log page data output section open output pipelinedata click view output navig given path azur portal azur storag explor treat directori oppos file call try defin sure requir",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Verify if the PipelineData, export_blob, contains the \"test.csv\" file before troubleshooting the DataTransferStep. This can be done using the SDK or the UI.\n2. Define the PipelineData with is_directory=True in the test_upload_stackoverflow.py file when calling .to_csv() to treat it as a directory instead of a file.",
        "Answer_gpt_summary":"possibl solut mention discuss verifi pipelinedata export blob contain test csv file troubleshoot datatransferstep sdk defin pipelinedata directori true test upload stackoverflow file call csv treat directori instead file"
    },
    {
        "Question_title":"Does the sagemaker official tutorial generate an AttributeError, and how to solve it?",
        "Question_body":"<p>I'm following the <a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">AWS Sagemaker tutorial<\/a>, but I think there's an error in the step 4a. Particularly, at line 3 I'm instructed to type:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>     s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket_name, prefix), content_type='csv')\n<\/code><\/pre>\n<p>and I get the error<\/p>\n<pre><code>----&gt; 3 s3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket_name, prefix), content_type='csv')\n\nAttributeError: module 'sagemaker' has no attribute 's3_input'\n<\/code><\/pre>\n<p>Indeed, using <code>dir<\/code> shows that sagemaker has no attribute called s3_input. How can fix this so that I can keep advancing in the tutorial? I tried using <code>session.inputs<\/code>, but this redirects me to a page saying that <code>session<\/code> is deprecated and suggesting that I use <code>sagemaker.inputs.TrainingInput<\/code> instead of <code>sagemaker.s3_inputs<\/code>. Is this a good way of going forward?<\/p>\n<p>Thanks everyone for the help and patience!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1609421425757,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":625.0,
        "Poster_id":1714385.0,
        "Poster_created_time":1349183356160,
        "Poster_last_access_time":1664043290936,
        "Poster_age":null,
        "Poster_location":"Oxford, UK",
        "Poster_reputation":3803.0,
        "Poster_views":240.0,
        "Answer_body":"<p>Using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/inputs.html#inputs\" rel=\"noreferrer\"><code>sagemaker.inputs.TrainingInput<\/code><\/a> instead of <code>sagemaker.s3_inputs<\/code> worked to get that code cell functioning. It is an appropriate solution, though there may be another approach.<\/p>\n<p>Step 4.b also had code which needed updating<\/p>\n<pre><code>sess = sagemaker.Session()\nxgb = sagemaker.estimator.Estimator(containers[my_region],role, train_instance_count=1, train_instance_type='ml.m4.xlarge',output_path='s3:\/\/{}\/{}\/output'.format(bucket_name, prefix),sagemaker_session=sess)\nxgb.set_hyperparameters(max_depth=5,eta=0.2,gamma=4,min_child_weight=6,subsample=0.8,silent=0,objective='binary:logistic',num_round=100)\n<\/code><\/pre>\n<p>uses parameters <code>train_instance_count<\/code> and <code>train_instance_type<\/code> which have been changed in a later version (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html#parameter-and-class-name-changes\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/v2.html#parameter-and-class-name-changes<\/a>).<\/p>\n<p>Making these changes resolved the errors for the tutorial using <code>conda_python3<\/code> kernel.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1609455178296,
        "Answer_score":6.0,
        "Answerer_id":2118138.0,
        "Answerer_created_time":1362025964372,
        "Answerer_last_access_time":1625541376390,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":78.0,
        "Answerer_views":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65521556",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1609450565407,
        "Question_original_content":"offici tutori gener attributeerror solv follow tutori think error step particularli line instruct type input train input data train format bucket prefix content type csv error input train input data train format bucket prefix content type csv attributeerror modul attribut input dir show attribut call input fix advanc tutori tri session input redirect page sai session deprec suggest us input traininginput instead input good wai go forward thank help patienc",
        "Question_preprocessed_content":"offici tutori gener attributeerror solv follow tutori think error step particularli line instruct type error show attribut call fix advanc tutori tri redirect page sai deprec suggest us instead good wai go forward thank help patienc",
        "Question_gpt_summary_original":"The user is encountering an AttributeError while following the AWS Sagemaker tutorial in step 4a. The error occurs when trying to use the sagemaker.s3_input function, which is not recognized as an attribute of the sagemaker module. The user is seeking advice on how to fix this error and continue with the tutorial, and has tried using session.inputs but was redirected to use sagemaker.inputs.TrainingInput instead of sagemaker.s3_inputs.",
        "Question_gpt_summary":"user encount attributeerror follow tutori step error occur try us input function recogn attribut modul user seek advic fix error continu tutori tri session input redirect us input traininginput instead input",
        "Answer_original_content":"input traininginput instead input work code cell function appropri solut approach step code need updat sess session xgb estim estim contain region role train instanc count train instanc type xlarg output path output format bucket prefix session sess xgb set hyperparamet max depth eta gamma min child weight subsampl silent object binari logist num round us paramet train instanc count train instanc type chang later version http readthedoc stabl html paramet class chang make chang resolv error tutori conda python kernel",
        "Answer_preprocessed_content":"instead work code cell function appropri solut approach step code need updat us paramet chang later version make chang resolv error tutori kernel",
        "Answer_gpt_summary_original":"Solution:\n- Using sagemaker.inputs.TrainingInput instead of sagemaker.s3_inputs worked to get the code cell functioning.\n- Updating the code in step 4.b by changing the parameters train_instance_count and train_instance_type resolved the errors for the tutorial using conda_python3 kernel.",
        "Answer_gpt_summary":"solut input traininginput instead input work code cell function updat code step chang paramet train instanc count train instanc type resolv error tutori conda python kernel"
    },
    {
        "Question_title":"How to convert jupyter notebook (ipython) to slideshow NOT using command line",
        "Question_body":"<p>I am new with <code>Jupyter<\/code>, and I use Amazon SageMaker so that everything is cloud based and not local.  I cannot use any resources locally, nor can I install <code>Jupyter<\/code> on this local computer that I want to do this on, so I cannot use the command line to put :<\/p>\n\n<pre><code>jupyter nbconvert Jupyter\\ Slides.ipynb --to slides --post serve\n<\/code><\/pre>\n\n<p>So, I am struggling to find a way to convert my notebook to a slideshow NOT using command line. Thanks in advance!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1532649972710,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1532675039560,
        "Question_score":1,
        "Question_view_count":4610.0,
        "Poster_id":9919003.0,
        "Poster_created_time":1528567336707,
        "Poster_last_access_time":1652301904528,
        "Poster_age":null,
        "Poster_location":"Kelowna, BC, Canada",
        "Poster_reputation":165.0,
        "Poster_views":16.0,
        "Answer_body":"<p>You can follow below steps to convert your notebook to slides on AWS Sagemaker (tried on sagemaker notebook instance) without installing any extensions.<\/p>\n\n<p><strong>Step 1:<\/strong> Follow this <a href=\"https:\/\/medium.com\/@mjspeck\/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67\" rel=\"nofollow noreferrer\">article<\/a> to chose which cells in your notebook can be presented or skipped.\n  - Go to View \u2192 Cell Toolbar \u2192 Slideshow\n  - A light gray bar will appear above each cell with a scroll down window on the top right\n  - Select type of slide each cell should be - regular slide, sub-slide, skip, notes<\/p>\n\n<p><strong>Step 2:<\/strong> Go to Sagemaker notebook home page and open terminal<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 3:<\/strong> Change directory in the instance where your notebook exists<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 4:<\/strong> Clone <code>reveal.js<\/code> in the directory where notebook exists from <a href=\"https:\/\/github.com\/hakimel\/reveal.js\" rel=\"nofollow noreferrer\">github<\/a>. <code>reveal.js<\/code> is used for rendering HTML file as presentation.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dillF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dillF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 5:<\/strong> Run the below command (same as in your question) to convert the notebook to slides without serving them (since there is no browser on the Sagemaker instance). This will just convert notebook to slides html.<\/p>\n\n<pre><code>jupyter nbconvert Image-classification-fulltraining.ipynb --to slides\n[NbConvertApp] Converting notebook Image-classification-fulltraining.ipynb to slides\n[NbConvertApp] Writing 346423 bytes to Image-classification-fulltraining.slides.html\n<\/code><\/pre>\n\n<p><strong>Step 6:<\/strong> Now open the html file from Sagemaker notebook file browser <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fykyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fykyl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now you can see the notebook rendered as slides based on how setup each cell in your notebook in Step 1<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":6069517.0,
        "Answerer_created_time":1458100127643,
        "Answerer_last_access_time":1663950101183,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1083.0,
        "Answerer_views":86.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51549048",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1532732192903,
        "Question_original_content":"convert jupyt notebook ipython slideshow command line new jupyt us cloud base local us resourc local instal jupyt local want us command line jupyt nbconvert jupyt slide ipynb slide post serv struggl wai convert notebook slideshow command line thank advanc",
        "Question_preprocessed_content":"convert jupyt notebook slideshow command line new us cloud base local us resourc local instal local want us command line struggl wai convert notebook slideshow command line thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in converting a Jupyter notebook to a slideshow without using the command line. They are unable to use any resources locally or install Jupyter on their local computer as they are using Amazon SageMaker, which is cloud-based.",
        "Question_gpt_summary":"user face challeng convert jupyt notebook slideshow command line unabl us resourc local instal jupyt local cloud base",
        "Answer_original_content":"follow step convert notebook slide tri notebook instanc instal extens step follow articl chose cell notebook present skip view cell toolbar slideshow light grai bar appear cell scroll window right select type slide cell regular slide sub slide skip note step notebook home page open termin step chang directori instanc notebook exist step clone reveal directori notebook exist github reveal render html file present step run command question convert notebook slide serv browser instanc convert notebook slide html jupyt nbconvert imag classif fulltrain ipynb slide nbconvertapp convert notebook imag classif fulltrain ipynb slide nbconvertapp write byte imag classif fulltrain slide html step open html file notebook file browser notebook render slide base setup cell notebook step hope help",
        "Answer_preprocessed_content":"follow step convert notebook slide instal extens step follow articl chose cell notebook present skip view cell toolbar slideshow light grai bar appear cell scroll window right select type slide cell regular slide skip note step notebook home page open termin step chang directori instanc notebook exist step clone directori notebook exist github render html file present step run command convert notebook slide serv convert notebook slide html step open html file notebook file browser notebook render slide base setup cell notebook step hope help",
        "Answer_gpt_summary_original":"Solution: The user can follow the steps mentioned below to convert their Jupyter notebook to a slideshow on AWS SageMaker without using the command line or installing any extensions. \n\nStep 1: Choose which cells in the notebook can be presented or skipped by going to View \u2192 Cell Toolbar \u2192 Slideshow and selecting the type of slide each cell should be - regular slide, sub-slide, skip, notes.\n\nStep 2: Open the terminal on the SageMaker notebook home page.\n\nStep 3: Change the directory to where the notebook exists.\n\nStep 4: Clone reveal.js in the directory where the notebook exists from GitHub. reveal.js is used for rendering the HTML file as a presentation.\n\nStep 5: Run the command \"jupyter nbconvert",
        "Answer_gpt_summary":"solut user follow step mention convert jupyt notebook slideshow command line instal extens step choos cell notebook present skip go view cell toolbar slideshow select type slide cell regular slide sub slide skip note step open termin notebook home page step chang directori notebook exist step clone reveal directori notebook exist github reveal render html file present step run command jupyt nbconvert"
    },
    {
        "Question_title":"entry_point file using XGBoost as a framework in sagemaker",
        "Question_body":"<p>Looking at the following source code taken from <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">here<\/a> (SDK v2):<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker.xgboost.estimator import XGBoost\nfrom sagemaker.session import Session\nfrom sagemaker.inputs import TrainingInput\n\n# initialize hyperparameters\nhyperparameters = {\n        &quot;max_depth&quot;:&quot;5&quot;,\n        &quot;eta&quot;:&quot;0.2&quot;,\n        &quot;gamma&quot;:&quot;4&quot;,\n        &quot;min_child_weight&quot;:&quot;6&quot;,\n        &quot;subsample&quot;:&quot;0.7&quot;,\n        &quot;verbosity&quot;:&quot;1&quot;,\n        &quot;objective&quot;:&quot;reg:linear&quot;,\n        &quot;num_round&quot;:&quot;50&quot;}\n\n# set an output path where the trained model will be saved\nbucket = sagemaker.Session().default_bucket()\nprefix = 'DEMO-xgboost-as-a-framework'\noutput_path = 's3:\/\/{}\/{}\/{}\/output'.format(bucket, prefix, 'abalone-xgb-framework')\n\n# construct a SageMaker XGBoost estimator\n# specify the entry_point to your xgboost training script\nestimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n                    framework_version='1.2-2',\n                    hyperparameters=hyperparameters,\n                    role=sagemaker.get_execution_role(),\n                    instance_count=1,\n                    instance_type='ml.m5.2xlarge',\n                    output_path=output_path)\n\n# define the data type and paths to the training and validation datasets\ncontent_type = &quot;libsvm&quot;\ntrain_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'train'), content_type=content_type)\nvalidation_input = TrainingInput(&quot;s3:\/\/{}\/{}\/{}\/&quot;.format(bucket, prefix, 'validation'), content_type=content_type)\n\n# execute the XGBoost training job\nestimator.fit({'train': train_input, 'validation': validation_input})\n<\/code><\/pre>\n<p>I wonder where the your_xgboost_abalone_script.py file has to be placed please? So far I used XGBoost as a built-in algorithm from my local machine with similar code (i.e. I span up a training job remotely). Thanks!<\/p>\n<p>PS:<\/p>\n<p>Looking at <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">this<\/a>, and source_dir, I wonder if one can upload Python files to S3. In this case, I take it is has to be tar.gz? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1653478466700,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1653482394636,
        "Question_score":0,
        "Question_view_count":74.0,
        "Poster_id":283538.0,
        "Poster_created_time":1267440784443,
        "Poster_last_access_time":1664045779312,
        "Poster_age":null,
        "Poster_location":"Somewhere",
        "Poster_reputation":15705.0,
        "Poster_views":2150.0,
        "Answer_body":"<p><code>your_xgboost_abalone_script.py<\/code> can be created locally. The path you provide is relative to where the code is running.<\/p>\n<p>I.e. <code>your_xgboost_abalone_script.py<\/code> can be located in the same directory where you are running the SageMaker SDK (&quot;source code&quot;).<\/p>\n<p>For example if you have <code>your_xgboost_abalone_script.py<\/code> in the same directory as the source code:<\/p>\n<pre><code>.\n\u251c\u2500\u2500 source_code.py\n\u2514\u2500\u2500 your_xgboost_abalone_script.py\n<\/code><\/pre>\n<p>Then you can point to this file exactly how the documentation depicts:<\/p>\n<pre><code>estimator = XGBoost(entry_point = &quot;your_xgboost_abalone_script.py&quot;, \n.\n.\n.\n)\n<\/code><\/pre>\n<p>The SDK will take <code>your_xgboost_abalone_script.py<\/code> repackage it into a model tar ball and upload it to S3 on your behalf.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1654796658207,
        "Answer_score":1.0,
        "Answerer_id":18272224.0,
        "Answerer_created_time":1645475560783,
        "Answerer_last_access_time":1663885040952,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":466.0,
        "Answerer_views":32.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72376872",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1654714910812,
        "Question_original_content":"entri point file xgboost framework look follow sourc code taken sdk import boto import xgboost estim import xgboost session import session input import traininginput initi hyperparamet hyperparamet max depth eta gamma min child weight subsampl verbos object reg linear num round set output path train model save bucket session default bucket prefix demo xgboost framework output path output format bucket prefix abalon xgb framework construct xgboost estim specifi entri point xgboost train script estim xgboost entri point xgboost abalon script framework version hyperparamet hyperparamet role execut role instanc count instanc type xlarg output path output path defin data type path train valid dataset content type libsvm train input traininginput format bucket prefix train content type content type valid input traininginput format bucket prefix valid content type content type execut xgboost train job estim fit train train input valid valid input wonder xgboost abalon script file place far xgboost built algorithm local machin similar code span train job remot thank look sourc dir wonder upload python file case tar thank",
        "Question_preprocessed_content":"file xgboost framework look follow sourc code taken wonder file place far xgboost algorithm local machin similar code thank look wonder upload python file case thank",
        "Question_gpt_summary_original":"The user is unsure where to place the \"your_xgboost_abalone_script.py\" file in the provided source code for using XGBoost as a framework in Sagemaker. They also have a question about uploading Python files to S3 and whether it needs to be in tar.gz format.",
        "Question_gpt_summary":"user unsur place xgboost abalon script file provid sourc code xgboost framework question upload python file need tar format",
        "Answer_original_content":"xgboost abalon script creat local path provid rel code run xgboost abalon script locat directori run sdk sourc code exampl xgboost abalon script directori sourc code sourc code xgboost abalon script point file exactli document depict estim xgboost entri point xgboost abalon script sdk xgboost abalon script repackag model tar ball upload behalf",
        "Answer_preprocessed_content":"creat local path provid rel code run locat directori run sdk exampl directori sourc code point file exactli document depict sdk repackag model tar ball upload behalf",
        "Answer_gpt_summary_original":"Solution: The user can create the \"your_xgboost_abalone_script.py\" file locally and place it in the same directory as the source code. The path provided is relative to where the code is running. The SDK will repackage the file into a model tar ball and upload it to S3 on the user's behalf. It is not necessary to upload the file in tar.gz format.",
        "Answer_gpt_summary":"solut user creat xgboost abalon script file local place directori sourc code path provid rel code run sdk repackag file model tar ball upload user behalf necessari upload file tar format"
    },
    {
        "Question_title":"kedro run as a python command instead of command line",
        "Question_body":"<p>I am getting started with Kedro, so I created the new kedro project for default iris dataset.<\/p>\n<p>I am able to succesfully run it with <code>kedro run<\/code> command. My question now is how do I run it as a python command? From the documentation I read that the command <code>kedro run<\/code>  runs the <code>src\/project-name\/run.py<\/code>. However, if I run the <code>run.py<\/code> I get <code>ModuleNotFoundError: No module named 'iris_workflow'<\/code>. I get the same error if I run the <code>run<\/code> method from <code>src\/project-name\/cli.py<\/code>.<\/p>\n<p>Everything works fine If I run <code>kedro run<\/code> in terminal.<\/p>\n<p>How do I run <code>kedro run<\/code> from a python script without <code>subprocess.run()<\/code>. If I import the <code>run.py<\/code> or <code>cli.py<\/code> in a script and run it, I get the same error <code>ModuleNotFoundError: No module named 'iris_workflow'<\/code>.<\/p>\n<p>This is the default workflow I created with <code>kedro new --starter=pandas-iris<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1615034237927,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":805.0,
        "Poster_id":9417884.0,
        "Poster_created_time":1519724643532,
        "Poster_last_access_time":1646603612996,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":453.0,
        "Poster_views":79.0,
        "Answer_body":"<p>The problem is that your <code>src\/<\/code> folder which is where your project python package lives isn't on your Python path, so if you modify your <code>PYTHONPATH<\/code> first, you'll be able to run <code>run.py<\/code>:<\/p>\n<pre><code>~\/code\/kedro\/test-project\ntest-project \u276f PYTHONPATH=$PYTHONPATH:$pwd\/src python3 src\/test_project\/run.py\n<\/code><\/pre>\n<p>To more concretely answer your question, if you wanted to run Kedro from a Python script, you'd do something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\n\nsys.path.append(&quot;&lt;path-to-your-project-src&quot;)\nwith KedroSession.create(package_path.name) as session:\n    session.run()\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":9999106.0,
        "Answerer_created_time":1530092504712,
        "Answerer_last_access_time":1652101318888,
        "Answerer_age":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation":915.0,
        "Answerer_views":288.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66505695",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1615038763867,
        "Question_original_content":"run python command instead command line get start creat new project default iri dataset abl succesfulli run run command question run python command document read command run run src project run run run modulenotfounderror modul name iri workflow error run run method src project cli work fine run run termin run run python script subprocess run import run cli script run error modulenotfounderror modul name iri workflow default workflow creat new starter panda iri",
        "Question_preprocessed_content":"run python command instead command line get start creat new project default iri dataset abl succesfulli run command question run python command document read command run run error run method work fine run termin run python script import script run error default workflow creat",
        "Question_gpt_summary_original":"The user is facing challenges in running Kedro project as a Python command instead of a command line. They are encountering a \"ModuleNotFoundError\" when trying to run the \"run.py\" or \"cli.py\" methods from the project directory. The user is seeking guidance on how to run the project as a Python command without using \"subprocess.run()\".",
        "Question_gpt_summary":"user face challeng run project python command instead command line encount modulenotfounderror try run run cli method project directori user seek guidanc run project python command subprocess run",
        "Answer_original_content":"problem src folder project python packag live isn python path modifi pythonpath abl run run code test project test project pythonpath pythonpath pwd src python src test project run concret answer question want run python script like import sy sy path append path project src session creat packag path session session run",
        "Answer_preprocessed_content":"problem folder project python packag live isn python path modifi abl run concret answer question want run python script like",
        "Answer_gpt_summary_original":"Solution:\n- Modify the PYTHONPATH to include the \"src\/\" folder where the project python package lives, and then run the \"run.py\" method using the modified PYTHONPATH.\n- To run Kedro from a Python script, append the path to the \"src\/\" folder to the sys.path list and use the KedroSession.create() method to create a session and run the project.",
        "Answer_gpt_summary":"solut modifi pythonpath includ src folder project python packag live run run method modifi pythonpath run python script append path src folder sy path list us session creat method creat session run project"
    },
    {
        "Question_title":"ImportError: No module named cassandra in Azure Machine Learning Studio",
        "Question_body":"<p>I am trying to install python package cassandra driver in Azure Machine Learning studio. I am following this answer from <a href=\"https:\/\/stackoverflow.com\/questions\/44371692\/install-python-packages-in-azure-ml\">here<\/a>. Unfortunately i don't see any wheel file for cassandra-driver <a href=\"https:\/\/pypi.python.org\/pypi\/cassandra-driver\/\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/cassandra-driver\/<\/a> so i downloaded the .tar file and converted to zip.<\/p>\n<p>I included this .zip file as dataset and connected to python script<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/omsO9.jpg\" alt=\"jpg1\" \/><\/a><\/p>\n<p>But when i run it, it says No module named cassandra\n<a href=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4DKTB.jpg\" alt=\"jpg2\" \/><\/a><\/p>\n<p>Does this work only with wheel file? Any solution is much appreciated.<\/p>\n<p>I am using Python Version :  Anoconda 4.0\/Python 3.5<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1519110374087,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1592644375060,
        "Question_score":1,
        "Question_view_count":500.0,
        "Poster_id":7719284.0,
        "Poster_created_time":1489644560420,
        "Poster_last_access_time":1646025882008,
        "Poster_age":null,
        "Poster_location":"Planet Earth",
        "Poster_reputation":791.0,
        "Poster_views":253.0,
        "Answer_body":"<p>I got it working. Changed the folder inside .zip file to <code>\"cassandra\"<\/code> (just like cassandra package). <\/p>\n\n<p>And in the Python script, i added <\/p>\n\n<pre><code>from cassandra import *\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":7719284.0,
        "Answerer_created_time":1489644560420,
        "Answerer_last_access_time":1646025882008,
        "Answerer_age":null,
        "Answerer_location":"Planet Earth",
        "Answerer_reputation":791.0,
        "Answerer_views":253.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48879595",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1519124227916,
        "Question_original_content":"importerror modul name cassandra studio try instal python packag cassandra driver studio follow answer unfortun wheel file cassandra driver http pypi python org pypi cassandra driver download tar file convert zip includ zip file dataset connect python script run sai modul name cassandra work wheel file solut appreci python version anoconda python",
        "Question_preprocessed_content":"importerror modul name cassandra studio try instal python packag cassandra driver studio follow answer unfortun wheel file download tar file convert zip includ zip file dataset connect python script run sai modul name cassandra work wheel file solut appreci python version anoconda",
        "Question_gpt_summary_original":"The user is facing challenges installing the cassandra driver python package in Azure Machine Learning Studio. They downloaded the .tar file and converted it to a zip file, included it as a dataset, and connected it to a python script. However, when they run it, they receive an ImportError stating that there is no module named cassandra. The user is unsure if this only works with a wheel file and is seeking a solution.",
        "Question_gpt_summary":"user face challeng instal cassandra driver python packag studio download tar file convert zip file includ dataset connect python script run receiv importerror state modul name cassandra user unsur work wheel file seek solut",
        "Answer_original_content":"got work chang folder insid zip file cassandra like cassandra packag python script ad cassandra import",
        "Answer_preprocessed_content":"got work chang folder insid zip file python script ad",
        "Answer_gpt_summary_original":"Solution: The user was able to resolve the issue by changing the folder name inside the .zip file to \"cassandra\" and adding \"from cassandra import *\" in the Python script.",
        "Answer_gpt_summary":"solut user abl resolv issu chang folder insid zip file cassandra ad cassandra import python script"
    },
    {
        "Question_title":"Use data in AWS Athena table and real time data to make prediction through Sagemaker inference",
        "Question_body":"<p>It is like time series prediction, but not exactly. Our customer sends patient current vital information to AWS SageMaker inference endpoint to predict the patient future health condition.  The patient data is saved to AWS S3 bucket,  and we use Athena to build a table from saved data.  We did off-line training, and found the vital information change is an import feature to predict patient future health condition.\nDoes anyone know how I can query Athena table to extract a given patient historical vital information, and send both historical and current vital information to the AWS Sagemaker inference endpoint to make prediction?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1654642349553,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":108.0,
        "Poster_id":13751111.0,
        "Poster_created_time":1592239501487,
        "Poster_last_access_time":1663126794552,
        "Poster_age":null,
        "Poster_location":"California, USA",
        "Poster_reputation":105.0,
        "Poster_views":16.0,
        "Answer_body":"<p>Do you have a SageMaker real-time endpoint created? One route is using an AWS SDK (Ex: boto3 for Python) and coupling it with a Lambda function to invoke that endpoint. The Lambda function can grab data from S3 or Athena or whatever data source using the SDK and then invoke that endpoint.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":16504640.0,
        "Answerer_created_time":1626973229036,
        "Answerer_last_access_time":1663708000863,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":199.0,
        "Answerer_views":37.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72538301",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1654800055123,
        "Question_original_content":"us data aw athena tabl real time data predict infer like time seri predict exactli custom send patient current vital inform infer endpoint predict patient futur health condit patient data save aw bucket us athena build tabl save data line train vital inform chang import featur predict patient futur health condit know queri athena tabl extract given patient histor vital inform send histor current vital inform infer endpoint predict",
        "Question_preprocessed_content":"us data aw athena tabl real time data predict infer like time seri predict exactli custom send patient current vital inform infer endpoint predict patient futur health condit patient data save aw bucket us athena build tabl save data train vital inform chang import featur predict patient futur health condit know queri athena tabl extract given patient histor vital inform send histor current vital inform infer endpoint predict",
        "Question_gpt_summary_original":"The user is facing a challenge in querying an Athena table to extract a patient's historical vital information and sending both historical and current vital information to the AWS Sagemaker inference endpoint to make predictions about the patient's future health condition. The user has already done offline training and found that changes in vital information are important for predicting future health conditions.",
        "Question_gpt_summary":"user face challeng queri athena tabl extract patient histor vital inform send histor current vital inform infer endpoint predict patient futur health condit user offlin train chang vital inform import predict futur health condit",
        "Answer_original_content":"real time endpoint creat rout aw sdk boto python coupl lambda function invok endpoint lambda function grab data athena data sourc sdk invok endpoint",
        "Answer_preprocessed_content":"endpoint creat rout aw sdk coupl lambda function invok endpoint lambda function grab data athena data sourc sdk invok endpoint",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use an AWS SDK (such as boto3 for Python) to create a Lambda function that can grab data from S3 or Athena and send it to the AWS Sagemaker inference endpoint. This can be done by coupling the SDK with the Lambda function to invoke the endpoint.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us aw sdk boto python creat lambda function grab data athena send infer endpoint coupl sdk lambda function invok endpoint"
    },
    {
        "Question_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Question_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_created_time":1583933260433,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1584005920356,
        "Question_score":4,
        "Question_view_count":1681.0,
        "Poster_id":2063902.0,
        "Poster_created_time":1360655430743,
        "Poster_last_access_time":1663784892907,
        "Poster_age":null,
        "Poster_location":"Belgium",
        "Poster_reputation":2947.0,
        "Poster_views":355.0,
        "Answer_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1584011988083,
        "Answer_score":-2.0,
        "Answerer_id":11297406.0,
        "Answerer_created_time":1554186784008,
        "Answerer_last_access_time":1662988357176,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":2175.0,
        "Answerer_views":434.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1584005785480,
        "Question_original_content":"pass argument score file deploi model deploi train model aci endpoint python sdk creat score file like file call argument pass like train file interpret argpars pass argument code creat inferenceconfig environ obvious work fall extra docker file step core conda depend import condadepend core environ import environ core model import inferenceconfig env environ host environ env python conda depend condadepend creat conda packag scikit learn pip packag default score script score model model infer config inferenceconfig entri script score script environ env ad score refer love us argument script remov import import argpars def init global model parser argpars argumentpars descript load sklearn model parser add argument model dest model requir true arg parser pars known arg model path model model path model arg model model joblib load model path def run raw data try data json load raw data data data arrai data result model predict data return result tolist except result str return result interest hear thought",
        "Question_preprocessed_content":"pass argument score file deploi model deploi train model aci endpoint python sdk creat file like file call argument pass interpret pass argument code creat inferenceconfig environ obvious work fall extra docker file step ad refer love us argument script interest hear thought",
        "Question_gpt_summary_original":"The user is facing a challenge in passing arguments to the score.py file when deploying a trained model to an ACI endpoint on Azure Machine Learning using the Python SDK. They have created the score.py file and want it to be called with an argument that can be interpreted using argparse, but they are unable to find a way to pass arguments. The user has shared their code for creating the InferenceConfig environment and the score.py file for reference.",
        "Question_gpt_summary":"user face challeng pass argument score file deploi train model aci endpoint python sdk creat score file want call argument interpret argpars unabl wai pass argument user share code creat inferenceconfig environ score file refer",
        "Answer_original_content":"deploi environ model regist deploi ipynb inferenceconfig class accept sourc directori entri script paramet sourc directori path folder contain file score addit file creat imag multi model regist deploi ipynb code snippet creat inferenceconfig sourc directori entri script core webservic import webservic core model import inferenceconfig core environ import environ myenv environ conda specif myenv file path myenv yml infer config inferenceconfig entri script score environ myenv servic model deploi workspac sklearn mnist svc model model infer config infer config deploy config aciconfig servic wait deploy output true print servic score uri",
        "Answer_preprocessed_content":"deploi environ inferenceconfig class accept paramet path folder contain addit file creat imag code snippet creat inferenceconfig",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Sagemaker doesn't inference in an async manner",
        "Question_body":"<p>I've deployed a custom model with an async endpoint. I want to process video files with it because videos can have ~5-10 minutes I can't load all frames to memory. Of course, I want to make an inference on each frame.\nI've written<br \/>\n<code>input_fn<\/code> - download video file from s3 using boto and creates generator which loads video frames with a given batch size - return a generator - written with OpenCV<br \/>\n<code>predict_fn<\/code> - iterate over generator batched frames and generate prediction using model - save prediction in list<br \/>\n<code>output_fn<\/code> - transform prediction into json format, gzip all to reduce the size<\/p>\n<p>Endpoint works well, but the problem is concurrency. The sagemaker endpoint processes request after request (from cloudwatch and s3 save file time). I don't know why this happens.\nmax_concurrent_invocations_per_instance is set to 1000. Other settings from PyTorch serving are as follows:<\/p>\n<pre><code>SAGEMAKER_MODEL_SERVER_TIMEOUT: 100000\nSAGEMAKER_TS_MAX_BATCH_DELAY: 10000\nSAGEMAKER_TS_BATCH_SIZE: 1000\nSAGEMAKER_TS_MAX_WORKERS: 4\nSAGEMAKER_TS_RESPONSE_TIMEOUT: 100000\n<\/code><\/pre>\n<p>And still, it doesn't work. So how can I create an async inference endpoint with PyTorch to get concurrency?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1653569251380,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":109.0,
        "Poster_id":8049757.0,
        "Poster_created_time":1495477835623,
        "Poster_last_access_time":1662212097747,
        "Poster_age":null,
        "Poster_location":"Rzeszow, Poland",
        "Poster_reputation":148.0,
        "Poster_views":68.0,
        "Answer_body":"<p>The concurrency settings for TorchServe DLC are controlled by such mechanisms as # of workers, which can be set by defining the appropriate variables, such as <code>SAGEMAKER_TS_*<\/code>, and <code>SAGEMAKER_MODEL_*<\/code> (see, e.g., <a href=\"https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\" rel=\"nofollow noreferrer\">this page<\/a> for details on their meaning and implications).<\/p>\n<p>While the latter are agnostic to any particular serving stack and are defined in the <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a>, the former are TorchServe-specific and are defined in <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\" rel=\"nofollow noreferrer\">TorchServe Inference Toolkit<\/a>. Moreover, since the TorchServe Inference Toolkit is built on top of the SageMaker Inference Toolkit, there is a non-trivial interplay between these two sets of params.<\/p>\n<p>Thus you may also want to experiment with such params as, e.g., <code>SAGEMAKER_MODEL_SERVER_WORKERS<\/code> to properly set up the concurrency setting of the SageMaker Async Endpoint.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":8468326.0,
        "Answerer_created_time":1502816769670,
        "Answerer_last_access_time":1662134982640,
        "Answerer_age":null,
        "Answerer_location":"Darmstadt, Germany",
        "Answerer_reputation":1998.0,
        "Answerer_views":76.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72392070",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1653766025390,
        "Question_original_content":"infer async manner deploi custom model async endpoint want process video file video minut load frame memori cours want infer frame written input download video file boto creat gener load video frame given batch size return gener written opencv predict iter gener batch frame gener predict model save predict list output transform predict json format gzip reduc size endpoint work problem concurr endpoint process request request cloudwatch save file time know happen max concurr invoc instanc set set pytorch serv follow model server timeout max batch delai batch size max worker respons timeout work creat async infer endpoint pytorch concurr",
        "Question_preprocessed_content":"infer async manner deploi custom model async endpoint want process video file video minut load frame memori cours want infer frame written download video file boto creat gener load video frame given batch size return gener written opencv iter gener batch frame gener predict model save predict list transform predict json format gzip reduc size endpoint work problem concurr endpoint process request request know happen set set pytorch serv follow work creat async infer endpoint pytorch concurr",
        "Question_gpt_summary_original":"The user has deployed a custom model with an async endpoint on Sagemaker to process video files with multiple frames. However, the Sagemaker endpoint processes requests one after the other, causing concurrency issues. The user has tried adjusting the settings, but the problem persists. The user is seeking a solution to create an async inference endpoint with PyTorch to achieve concurrency.",
        "Question_gpt_summary":"user deploi custom model async endpoint process video file multipl frame endpoint process request caus concurr issu user tri adjust set problem persist user seek solut creat async infer endpoint pytorch achiev concurr",
        "Answer_original_content":"concurr set torchserv dlc control mechan worker set defin appropri variabl model page detail mean implic agnost particular serv stack defin infer toolkit torchserv specif defin torchserv infer toolkit torchserv infer toolkit built infer toolkit non trivial interplai set param want experi param model server worker properli set concurr set async endpoint",
        "Answer_preprocessed_content":"concurr set torchserv dlc control mechan worker set defin appropri variabl agnost particular serv stack defin infer toolkit defin torchserv infer toolkit torchserv infer toolkit built infer toolkit interplai set param want experi param properli set concurr set async endpoint",
        "Answer_gpt_summary_original":"Solution: The user can experiment with adjusting the concurrency settings for TorchServe DLC by defining appropriate variables such as <code>SAGEMAKER_TS_*<\/code> and <code>SAGEMAKER_MODEL_*<\/code>. Additionally, the user can experiment with adjusting the <code>SAGEMAKER_MODEL_SERVER_WORKERS<\/code> parameter to properly set up the concurrency setting of the SageMaker Async Endpoint.",
        "Answer_gpt_summary":"solut user experi adjust concurr set torchserv dlc defin appropri variabl model addition user experi adjust model server worker paramet properli set concurr set async endpoint"
    },
    {
        "Question_title":"Azure Machine learning - Strip top X rows from dataset",
        "Question_body":"<p>I have a plain text csv file, which i am trying to read in Azure ML studio - the file format is pretty much like this<\/p>\n\n<pre><code>Geolife trajectory\nWGS 84\nAltitude is in Feet\nReserved 3\n0,2,255,My Track,0,0,2,8421376\n0\n39.984702,116.318417,0,492,39744.1201851852,2008-10-23,02:53:04\n39.984683,116.31845,0,492,39744.1202546296,2008-10-23,02:53:10\n39.984686,116.318417,0,492,39744.1203125,2008-10-23,02:53:15\n39.984688,116.318385,0,492,39744.1203703704,2008-10-23,02:53:20\n39.984655,116.318263,0,492,39744.1204282407,2008-10-23,02:53:25\n39.984611,116.318026,0,493,39744.1204861111,2008-10-23,02:53:30\n<\/code><\/pre>\n\n<p>The real data starts from Line 7, how could i strip it off, these files need to be downloaded on the fly so I don't think i would like to strip off the data by some code.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1463839280200,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":53.0,
        "Poster_id":1251674.0,
        "Poster_created_time":1331022293612,
        "Poster_last_access_time":1662439094590,
        "Poster_age":null,
        "Poster_location":"India",
        "Poster_reputation":4743.0,
        "Poster_views":811.0,
        "Answer_body":"<p>What is your source location - SQL or Blob or http?<\/p>\n\n<p>If SQL, then you can use query to start from line 6.<\/p>\n\n<p>If Blob\/http, I would suggest reading a file as a single column TSV format, use simple R\/Python script to drop first 6 rows and convert to csv<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4589073.0,
        "Answerer_created_time":1424453610300,
        "Answerer_last_access_time":1505365713168,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1237.0,
        "Answerer_views":116.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37363883",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1463946230187,
        "Question_original_content":"strip row dataset plain text csv file try read studio file format pretti like geolif trajectori wg altitud feet reserv track real data start line strip file need download fly think like strip data code",
        "Question_preprocessed_content":"strip row dataset plain text csv file try read studio file format pretti like real data start line strip file need download fly think like strip data code",
        "Question_gpt_summary_original":"The user is facing a challenge in reading a plain text csv file in Azure ML studio. The file contains some unnecessary data in the first few rows, and the user wants to strip it off without using any code as the files need to be downloaded on the fly.",
        "Question_gpt_summary":"user face challeng read plain text csv file studio file contain unnecessari data row user want strip code file need download fly",
        "Answer_original_content":"sourc locat sql blob http sql us queri start line blob http suggest read file singl column tsv format us simpl python script drop row convert csv",
        "Answer_preprocessed_content":"sourc locat sql blob http sql us queri start line suggest read file singl column tsv format us simpl script drop row convert csv",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- If the source location is SQL, the user can use a query to start reading the file from line 6.\n- If the source location is Blob or http, the user can read the file as a single column TSV format, use a simple R\/Python script to drop the first 6 rows, and convert it to CSV. \n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss sourc locat sql user us queri start read file line sourc locat blob http user read file singl column tsv format us simpl python script drop row convert csv person opinion bias includ summari"
    },
    {
        "Question_title":"Blocked Algorithms still run for AutoML Experiment",
        "Question_body":"I have been trying to run an AutoML Forecasting Experiment with only allowing one algorithm (FBProphet) to run and no other supported algorithms. The issue I run into is that even though I specify the blocked algorithms, they still run in the experiment taking up unnecessary runtime. For eg, my experiment should run only for 1-2 hours but it ends up running for 24-30 hours because it still runs the undesired algorithms. Is there any way I can stop making the blocked algorithms from running in my experiment so I can save up on significant runtime? I have attached a screenshot and my AutoML config code to help understand this issue better.\n\nCode:\nn_test_periods = 60\nblocked_algos = ['ExtremeRandomTrees','DecisionTree','ElasticNet','LassoLars']\ntime_series_settings = {\n    'time_column_name': time_column_name,\n    'grain_column_names': grain_column_names ,\n    'forecast_horizon': n_test_periods\n}\nautoml_config = AutoMLConfig(task='forecasting',\n                             debug_log='Logs\/prophet_forecasting_errors.log',\n                             primary_metric='normalized_mean_absolute_error',\n                             training_data=train_data,\n                             label_column_name=target_column_name,\n                             compute_target=compute_target,\n                             featurization= 'off',\n                             blocked_model = blocked_algos,\n                             allowed_models = ['Prophet'],\n                             n_cross_validations= 3,\n                             verbosity=logging.INFO,\n                             max_cores_per_iteration=6,\n                             **time_series_settings)\nremote_run = experiment.submit(automl_config, show_output=True)\n\n\nScreenshot of the Experiment: (This took 32h when it should ideally take 56 mins)",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_created_time":1605815289883,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@MeghaLokanadham-7021 it's always good to have the latest version if possible, latest version is 1.18.0 as of today.\n\nCan you please try the following 1. Correct the name of the parameter( blocked_model) to \"blocked_models\" and keep both \"blocked_models\" and \"allowed_models\".",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/168932\/blocked-algorithms-still-run-for-automl-experiment.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1606310431887,
        "Question_original_content":"block algorithm run automl experi try run automl forecast experi allow algorithm fbprophet run support algorithm issu run specifi block algorithm run experi take unnecessari runtim experi run hour end run hour run undesir algorithm wai stop make block algorithm run experi save signific runtim attach screenshot automl config code help understand issu better code test period block algo extremerandomtre decisiontre elasticnet lassolar time seri set time column time column grain column name grain column name forecast horizon test period automl config automlconfig task forecast debug log log prophet forecast error log primari metric normal mean absolut error train data train data label column target column comput target comput target featur block model block algo allow model prophet cross valid verbos log info max core iter time seri set remot run experi submit automl config output true screenshot experi took ideal min",
        "Question_preprocessed_content":"block algorithm run automl experi try run automl forecast experi allow algorithm run support algorithm issu run specifi block algorithm run experi take unnecessari runtim experi run hour end run hour run undesir algorithm wai stop make block algorithm run experi save signific runtim attach screenshot automl config code help understand issu better code automlconfig screenshot experi",
        "Question_gpt_summary_original":"The user is facing a challenge while running an AutoML Forecasting Experiment with only one allowed algorithm (FBProphet) and blocking other supported algorithms. Even after specifying the blocked algorithms, they still run in the experiment, leading to unnecessary runtime. The experiment should ideally run for 1-2 hours, but it ends up running for 24-30 hours due to the undesired algorithms. The user is seeking a solution to prevent the blocked algorithms from running in the experiment to save significant runtime.",
        "Question_gpt_summary":"user face challeng run automl forecast experi allow algorithm fbprophet block support algorithm specifi block algorithm run experi lead unnecessari runtim experi ideal run hour end run hour undesir algorithm user seek solut prevent block algorithm run experi save signific runtim",
        "Answer_original_content":"meghalokanadham good latest version possibl latest version todai try follow correct paramet block model block model block model allow model",
        "Answer_preprocessed_content":"good latest version possibl latest version todai try follow correct paramet",
        "Answer_gpt_summary_original":"Solution: The discussion suggests correcting the name of the parameter from \"blocked_model\" to \"blocked_models\" and keeping both \"blocked_models\" and \"allowed_models\" to prevent the undesired algorithms from running in the experiment.",
        "Answer_gpt_summary":"solut discuss suggest correct paramet block model block model keep block model allow model prevent undesir algorithm run experi"
    },
    {
        "Question_title":"SageMaker Tensorflow - how to write my serving_input_fn()",
        "Question_body":"<p>I'm pretty new to Tensorflow and SageMaker and I'm trying to figure out how to write my <code>serving_input_fn()<\/code>. I've tried a number of ways to do it, but to no avail. <\/p>\n\n<p>my input function has 3 feature columns: <code>amount_normalized, x_month and y_month<\/code>:<\/p>\n\n<pre><code>def construct_feature_columns():\n    amount_normalized = tf.feature_column.numeric_column(key='amount_normalized')\n    x_month = tf.feature_column.numeric_column(key='x_month')\n    y_month = tf.feature_column.numeric_column(key='y_month')\n    return set([amount_normalized, x_month, y_month])\n<\/code><\/pre>\n\n<p>I want to be able to call my deployed model using something like <code>deployed_model.predict([1.23,0.3,0.8])<\/code> <\/p>\n\n<p>Where the first element is <code>amount_normalized<\/code>, second is <code>x_month<\/code> third is <code>y_month<\/code><\/p>\n\n<p>I've tried this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(params):\n    feature_placeholders = {\n      key : tf.placeholder(tf.float32, [None]) \\\n        for key in FEATURES\n    }\nreturn tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)()\n<\/code><\/pre>\n\n<p>But all I get is:\n<code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\".<\/code><\/p>\n\n<p>Any help would be <strong>really<\/strong> appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1524856485467,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":1337.0,
        "Poster_id":4760261.0,
        "Poster_created_time":1428432018420,
        "Poster_last_access_time":1552394246967,
        "Poster_age":null,
        "Poster_location":"New York, United States",
        "Poster_reputation":301.0,
        "Poster_views":9.0,
        "Answer_body":"<p>Posting this here in case anyone else has this issue.<\/p>\n\n<p>After a bunch of trial and error I managed to solve my issue by writing my serving input function like this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(hyperparameters):\n    feature_spec = {\n        key : tf.FixedLenFeature(shape=[], dtype = tf.float32) \\\n          for key in FEATURES\n    }\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\n<\/code><\/pre>\n\n<p>I can then call my deployed model by passing in a hash:<\/p>\n\n<pre><code>deployed_model.predict({\"amount_normalized\": 2.3, \"x_month\": 0.2, \"y_month\": -0.3})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":4760261.0,
        "Answerer_created_time":1428432018420,
        "Answerer_last_access_time":1552394246967,
        "Answerer_age":null,
        "Answerer_location":"New York, United States",
        "Answerer_reputation":301.0,
        "Answerer_views":9.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50068941",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1525019634972,
        "Question_original_content":"tensorflow write serv input pretti new tensorflow try figur write serv input tri number wai avail input function featur column normal month month def construct featur column normal featur column numer column kei normal month featur column numer column kei month month featur column numer column kei month return set normal month month want abl deploi model like deploi model predict element normal second month month tri featur normal month month def serv input param featur placehold kei placehold float kei featur return estim export build raw serv input receiv featur placehold error occur modelerror call invokeendpoint oper receiv server error model messag help appreci",
        "Question_preprocessed_content":"tensorflow write pretti new tensorflow try figur write tri number wai avail input function featur column want abl deploi model like element second tri help appreci",
        "Question_gpt_summary_original":"The user is facing challenges in writing the serving_input_fn() for their Tensorflow model in SageMaker. They have tried different methods but have not been successful. The input function has three feature columns, and the user wants to call the deployed model using specific values for each column. The user has attempted to write the serving_input_fn() using placeholders, but they are encountering an error when calling the model.",
        "Question_gpt_summary":"user face challeng write serv input tensorflow model tri differ method success input function featur column user want deploi model specif valu column user attempt write serv input placehold encount error call model",
        "Answer_original_content":"post case issu bunch trial error manag solv issu write serv input function like featur normal month month def serv input hyperparamet featur spec kei fixedlenfeatur shape dtype float kei featur return estim export build pars serv input receiv featur spec deploi model pass hash deploi model predict normal month month",
        "Answer_preprocessed_content":"post case issu bunch trial error manag solv issu write serv input function like deploi model pass hash",
        "Answer_gpt_summary_original":"Solution:\nThe user can write the serving_input_fn() using the provided code snippet. They can define the feature columns in the FEATURES list and create a feature_spec dictionary using tf.FixedLenFeature. They can then call the exported model by passing in a hash with specific values for each column.",
        "Answer_gpt_summary":"solut user write serv input provid code snippet defin featur column featur list creat featur spec dictionari fixedlenfeatur export model pass hash specif valu column"
    },
    {
        "Question_title":"Negative Samples in ML Assisted Image Labeling",
        "Question_body":"We are evaluating the Azure ML Assisted Object detection labeling and I have some questions:\n1. How do I mark an image as a negative?\n2. How do I rename a label?\n3. How do I go back to a skipped image?\n4. When labeling if I discover that an image should not be in the dataset, how do I delete it from the dataset? The id of the image is no where to be found.\n5. For an autolabeled image, if I accidentally delete the bounding box, how do I undo this operation?\n6. Sometime the autolabeler creates small bounding boxes without any labels. Is this a bug?\n\nThank you",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658973322783,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@PrashantSaraswat-9512 I think I can answer some of your questions from some of the projects I used for labeling.\n\nHow do I mark an image as a negative?\nUnlike the Azure custom vision labeling experience, there isn't a feature to mark a label as negative. I believe you can add another label and use it as a negative label and tag images.\n\nHow do I go back to a skipped image?\nGo to the Data tab of your project and select Review Labels tab from the side. Using the filters option on right hand side, set the Asset Type as \"Skipped\". This should pull any skipped images and you should be able to assign the required label and a button should be enabled to update label. The same applies for updating any labeled image or bounding box.\n\n\nHow do I rename a label?\nI think a label cannot be renamed after it is created. You can delete all labels and create a new set though. Just stop your project and select the Details-> Label Classes tab and click Add label option to see this screen.\n\n\nWhen labeling if I discover that an image should not be in the dataset, how do I delete it from the dataset? The id of the image is no where to be found.\nI think you can skip the image since the dataset is registered while creating a project there is no option to delete certain images after this action.\n\nFor an autolabeled image, if I accidentally delete the bounding box, how do I undo this operation?\nI have not used auto labeling before but the same step to update the skipped image or label should help you with this step.\n\nSometime the autolabeler creates small bounding boxes without any labels. Is this a bug?\nNot sure about this issue since I haven't come across it. You could report through support or through portal using the smiley image on the top right hand corner.\n\nI hope this helps!!\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/945298\/negative-samples-in-ml-assisted-image-labeling.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1659001066756,
        "Question_original_content":"neg sampl assist imag label evalu assist object detect label question mark imag neg renam label skip imag label discov imag dataset delet dataset imag autolabel imag accident delet bound box undo oper autolabel creat small bound box label bug thank",
        "Question_preprocessed_content":"neg sampl assist imag label evalu assist object detect label question mark imag neg renam label skip imag label discov imag dataset delet dataset imag autolabel imag accident delet bound box undo oper autolabel creat small bound box label bug thank",
        "Question_gpt_summary_original":"The user is facing several challenges while evaluating the Azure ML Assisted Object detection labeling. These challenges include difficulty in marking an image as negative, renaming a label, going back to a skipped image, deleting an image from the dataset, undoing the accidental deletion of a bounding box, and encountering small bounding boxes without any labels created by the autolabeler.",
        "Question_gpt_summary":"user face challeng evalu assist object detect label challeng includ difficulti mark imag neg renam label go skip imag delet imag dataset undo accident delet bound box encount small bound box label creat autolabel",
        "Answer_original_content":"prashantsaraswat think answer question project label mark imag neg unlik azur custom vision label experi isn featur mark label neg believ add label us neg label tag imag skip imag data tab project select review label tab filter option right hand set asset type skip pull skip imag abl assign requir label button enabl updat label appli updat label imag bound box renam label think label renam creat delet label creat new set stop project select detail label class tab click add label option screen label discov imag dataset delet dataset imag think skip imag dataset regist creat project option delet certain imag action autolabel imag accident delet bound box undo oper auto label step updat skip imag label help step autolabel creat small bound box label bug sure issu haven come report support portal smilei imag right hand corner hope help answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"think answer question project label mark imag neg unlik azur custom vision label experi isn featur mark label neg believ add label us neg label tag imag skip imag data tab project select review label tab filter option right hand set asset type skip pull skip imag abl assign requir label button enabl updat label appli updat label imag bound box renam label think label renam creat delet label creat new set stop project select label class tab click add label option screen label discov imag dataset delet dataset imag think skip imag dataset regist creat project option delet certain imag action autolabel imag accident delet bound box undo oper auto label step updat skip imag label help step autolabel creat small bound box label bug sure issu haven come report support portal smilei imag right hand corner hope help answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- To mark an image as negative, add another label and use it as a negative label and tag images.\n- To go back to a skipped image, go to the Data tab of the project, select Review Labels tab, set the Asset Type as \"Skipped\", assign the required label, and update the label.\n- Labels cannot be renamed after they are created. Delete all labels and create a new set instead.\n- If an image should not be in the dataset, skip the image since there is no option to delete certain images after the dataset is registered while creating a project.\n- To undo the accidental deletion of a bounding box, use the same step to update the skipped image or label.\n- For",
        "Answer_gpt_summary":"possibl solut mention discuss mark imag neg add label us neg label tag imag skip imag data tab project select review label tab set asset type skip assign requir label updat label label renam creat delet label creat new set instead imag dataset skip imag option delet certain imag dataset regist creat project undo accident delet bound box us step updat skip imag label"
    },
    {
        "Question_title":"Sagemaker lifecycle configuration for installing pandas not working",
        "Question_body":"<p>I am trying to update pandas within a lifecycle configuration, and following the example of AWS I have the next code:<\/p>\n\n<pre><code>#!\/bin\/bash\n\nset -e\n\n# OVERVIEW\n# This script installs a single pip package in a single SageMaker conda environments.\n\nsudo -u ec2-user -i &lt;&lt;EOF\n# PARAMETERS\nPACKAGE=pandas\nENVIRONMENT=python3\nsource \/home\/ec2-user\/anaconda3\/bin\/activate \"$ENVIRONMENT\"\npip install --upgrade \"$PACKAGE\"==0.25.3\nsource \/home\/ec2-user\/anaconda3\/bin\/deactivate\nEOF\n<\/code><\/pre>\n\n<p>Then I attach it to a notebook and when I enter the notebook and open a notebook file, I see that pandas have not been updated. Using <code>!pip show pandas<\/code> I get:<\/p>\n\n<pre><code>Name: pandas\nVersion: 0.24.2\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: http:\/\/pandas.pydata.org\nAuthor: None\nAuthor-email: None\nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: pytz, python-dateutil, numpy\nRequired-by: sparkmagic, seaborn, odo, hdijupyterutils, autovizwidget\n<\/code><\/pre>\n\n<p>So we can see that I am indeed in the python3 env although the version is 0.24. <\/p>\n\n<p>However, the log in cloudwatch shows that it has been installed:<\/p>\n\n<pre><code>Collecting pandas==0.25.3 Downloading https:\/\/files.pythonhosted.org\/packages\/52\/3f\/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d\/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl (10.4MB)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: pytz&gt;=2017.2 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (2018.4)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: python-dateutil&gt;=2.6.1 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (2.7.3)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in .\/anaconda3\/lib\/python3.6\/site-packages (from pandas==0.25.3) (1.16.4)\n2020-02-03T12:33:09.065+01:00\nRequirement already satisfied, skipping upgrade: six&gt;=1.5 in .\/anaconda3\/lib\/python3.6\/site-packages (from python-dateutil&gt;=2.6.1-&gt;pandas==0.25.3) (1.13.0)\n2020-02-03T12:33:09.065+01:00\nInstalling collected packages: pandas Found existing installation: pandas 0.24.2 Uninstalling pandas-0.24.2: Successfully uninstalled pandas-0.24.2\n2020-02-03T12:33:12.066+01:00\nSuccessfully installed pandas-0.25.3\n<\/code><\/pre>\n\n<p>What could be the problem? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1580724156233,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1580746498476,
        "Question_score":2,
        "Question_view_count":1493.0,
        "Poster_id":9621172.0,
        "Poster_created_time":1523298968403,
        "Poster_last_access_time":1663934452963,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1754.0,
        "Poster_views":197.0,
        "Answer_body":"<p>if you want to install the  packages only in for the python3 environment, use the following script in your <strong>Create Sagemaker Lifecycle<\/strong> configurations. <\/p>\n\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called \"conda_python3\".\nsource activate python3\n\n# Replace myPackage with the name of the package you want to install.\npip install pandas==0.25.3\n# You can also perform \"conda install\" here as well.\nsource deactivate\nEOF\n<\/code><\/pre>\n\n<p>Reference : \"<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">Lifecycle Configuration Best Practices<\/a>\" <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":9064794.0,
        "Answerer_created_time":1512609233800,
        "Answerer_last_access_time":1660782650707,
        "Answerer_age":null,
        "Answerer_location":"Washington D.C., DC, USA",
        "Answerer_reputation":916.0,
        "Answerer_views":181.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60036916",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1581352236820,
        "Question_original_content":"lifecycl configur instal panda work try updat panda lifecycl configur follow exampl aw code bin bash set overview script instal singl pip packag singl conda environ sudo user anaconda lib python site packag panda requir satisfi skip upgrad python dateutil anaconda lib python site packag panda requir satisfi skip upgrad numpi anaconda lib python site packag panda requir satisfi skip upgrad anaconda lib python site packag python dateutil panda instal collect packag panda exist instal panda uninstal panda successfulli uninstal panda successfulli instal panda problem",
        "Question_preprocessed_content":"lifecycl configur instal panda work try updat panda lifecycl configur follow exampl aw code attach notebook enter notebook open notebook file panda updat python env version log cloudwatch show instal problem",
        "Question_gpt_summary_original":"The user is facing a challenge with updating pandas within a lifecycle configuration in AWS Sagemaker. Although the log in cloudwatch shows that pandas has been installed, when the user enters the notebook and opens a notebook file, pandas has not been updated. The version of pandas is still 0.24.2, even though the log shows that pandas 0.25.3 has been installed.",
        "Question_gpt_summary":"user face challeng updat panda lifecycl configur log cloudwatch show panda instal user enter notebook open notebook file panda updat version panda log show panda instal",
        "Answer_original_content":"want instal packag python environ us follow script creat lifecycl configur bin bash sudo user eof affect jupyt kernel call conda python sourc activ python replac mypackag packag want instal pip instal panda perform conda instal sourc deactiv eof refer lifecycl configur best practic",
        "Answer_preprocessed_content":"want instal packag python environ us follow script creat lifecycl configur refer lifecycl configur best practic",
        "Answer_gpt_summary_original":"Solution: The user can use the script provided in the discussion to install pandas 0.25.3 in the python3 environment of the Sagemaker Lifecycle Configuration. The script can be added to the Create Sagemaker Lifecycle configurations.",
        "Answer_gpt_summary":"solut user us script provid discuss instal panda python environ lifecycl configur script ad creat lifecycl configur"
    },
    {
        "Question_title":"Artifact storage and MLFLow on remote server",
        "Question_body":"<p>I am trying to get MLFlow on another machine in a local network to run and I would like to ask for some help because I don't know what to do now.<\/p>\n\n<p>I have a mlflow server running on a <em>server<\/em>. The mlflow server is running under my user on the <em>server<\/em> and has been started like this: <\/p>\n\n<pre><code>mlflow server --host 0.0.0.0 --port 9999 --default-artifact-root sftp:\/\/&lt;MYUSERNAME&gt;@&lt;SERVER&gt;:&lt;PATH\/TO\/DIRECTORY\/WHICH\/EXISTS&gt;\n<\/code><\/pre>\n\n<p>My program which should log all the data to the mlflow server looks like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from mlflow import log_metric, log_param, log_artifact, set_tracking_uri\n\nif __name__ == \"__main__\":\n    remote_server_uri = '&lt;SERVER&gt;' # this value has been replaced\n    set_tracking_uri(remote_server_uri)\n    # Log a parameter (key-value pair)\n    log_param(\"param1\", 5)\n\n    # Log a metric; metrics can be updated throughout the run\n    log_metric(\"foo\", 1)\n    log_metric(\"foo\", 2)\n    log_metric(\"foo\", 3)\n\n    # Log an artifact (output file)\n    with open(\"output.txt\", \"w\") as f:\n        f.write(\"Hello world!\")\n    log_artifact(\"output.txt\")\n\n<\/code><\/pre>\n\n<p>The parameters get and metrics get transfered to the server but not the artifacts. Why is that so?<\/p>\n\n<p>Note on the SFTP part:\nI can log in via SFTP and the pysftp package is installed<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1574429522373,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1574433828620,
        "Question_score":9,
        "Question_view_count":3483.0,
        "Poster_id":3446982.0,
        "Poster_created_time":1395413944963,
        "Poster_last_access_time":1656608868552,
        "Poster_age":null,
        "Poster_location":"Germany",
        "Poster_reputation":422.0,
        "Poster_views":13.0,
        "Answer_body":"<p>I don't know if I will get an answer to my problem but I did <em>solved<\/em> it this way.<\/p>\n\n<p>On the server I created the directory <code>\/var\/mlruns<\/code>. I pass this directory to mlflow via <code>--backend-store-uri file:\/\/\/var\/mlruns<\/code><\/p>\n\n<p>Then I mount this directory via e.g. <code>sshfs<\/code> on my local machine under the same path.<\/p>\n\n<p>I don't like this solution but it solved the problem good enough for now.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":3446982.0,
        "Answerer_created_time":1395413944963,
        "Answerer_last_access_time":1656608868552,
        "Answerer_age":null,
        "Answerer_location":"Germany",
        "Answerer_reputation":422.0,
        "Answerer_views":13.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58995329",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1575967801236,
        "Question_original_content":"artifact storag remot server try machin local network run like ask help know server run server server run user server start like server host port default artifact root sftp program log data server look like import log metric log param log artifact set track uri main remot server uri valu replac set track uri remot server uri log paramet kei valu pair log param param log metric metric updat run log metric foo log metric foo log metric foo log artifact output file open output txt write hello world log artifact output txt paramet metric transfer server artifact note sftp log sftp pysftp packag instal",
        "Question_preprocessed_content":"artifact storag remot server try machin local network run like ask help know server run server server run user server start like program log data server look like paramet metric transfer server artifact note log py packag instal",
        "Question_gpt_summary_original":"The user is facing a challenge in transferring artifacts to a remote server using MLFlow. The parameters and metrics are successfully transferred, but the artifacts are not. The MLFlow server is running on a server under the user's account, and the program to log data to the server is written in Python. The user has noted that they can log in via SFTP, and the pysftp package is installed.",
        "Question_gpt_summary":"user face challeng transfer artifact remot server paramet metric successfulli transfer artifact server run server user account program log data server written python user note log sftp pysftp packag instal",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"How to fix error \"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\" - strange behaviour in optuna",
        "Question_body":"<p>I am trying to use optuna lib in Python to optimise parameters for recommender systems' models. Those models are custom and look like standard fit-predict sklearn models (with methods get\/set params). <\/p>\n\n<p>What I do: simple objective function that selects two parameters from uniform int distribution, set these params to model, predicts the model (there no fit stage as it simple model that uses params only in predict stage) and calculates some metric. <\/p>\n\n<p>What I get: the first trial runs normal, it samples params and prints results to log. But on the second and next trial I have some strange errors (look code below) that I can't solve or google. When I run study on just 1 trial everything is okay.<\/p>\n\n<p>What I tried: to rearrange parts of objective function, put fit stage inside, try to calculate more simpler metrics - nothing helps. <\/p>\n\n<p>Here is my objective function: <\/p>\n\n<pre><code># getting train, test\n# fitting model\nself.model = SomeRecommender()\nself.model.fit(train, some_other_params)\n\ndef objective(trial: optuna.Trial):\n    # save study\n    if path is not None:\n        joblib.dump(study, some_path)\n\n    # sampling params\n    alpha = trial.suggest_uniform('alpha', 0, 100)\n    beta = trial.suggest_uniform('beta', 0, 100)\n\n    # setting params to model\n    params = {'alpha': alpha,\n              'beta': beta}\n    self.model.set_params(**params)\n\n    # getting predict\n    recs = self.model.predict(some_other_params)\n\n    # metric computing\n    metric_result = Metrics.hit_rate_at_k(recs, test, k=k)\n\n    return metric_result\n\n# starting study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3, n_jobs=1)\n<\/code><\/pre>\n\n<p>That's what I get on three trials:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>[I 2019-10-01 12:53:59,019] Finished trial#0 resulted in value: 0.1. Current best value is 0.1 with parameters: {'alpha': 59.6135986324444, 'beta': 40.714559720597585}.\n[W 2019-10-01 13:39:58,140] Setting status of trial#1 as TrialState.FAIL because of the following error: AttributeError(\"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\")\nTraceback (most recent call last):\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/study.py\", line 448, in _run_trial\n    result = func(trial)\n  File \"\/Users\/roseaysina\/code\/project\/model.py\", line 100, in objective\n    'alpha', 0, 100)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 180, in suggest_uniform\n    return self._suggest(name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 453, in _suggest\n    self.study, trial, name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 127, in sample_independent\n    values, scores = _get_observation_pairs(study, param_name)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 558, in _get_observation_pairs\n    param_value = distribution.to_internal_repr(trial.params[param_name])\nAttributeError: '_BaseUniformDistribution' object has no attribute 'to_internal_repr'\n[W 2019-10-01 13:39:58,206] Setting status of trial#2 as TrialState.FAIL because of the following error: AttributeError(\"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\")\nTraceback (most recent call last):\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/study.py\", line 448, in _run_trial\n    result = func(trial)\n  File \"\/Users\/roseaysina\/code\/project\/model.py\", line 100, in objective\n    'alpha', 0, 100)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 180, in suggest_uniform\n    return self._suggest(name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/trial.py\", line 453, in _suggest\n    self.study, trial, name, distribution)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 127, in sample_independent\n    values, scores = _get_observation_pairs(study, param_name)\n  File \"\/Users\/roseaysina\/anaconda3\/envs\/sauvage\/lib\/python3.7\/site-packages\/optuna\/samplers\/tpe\/sampler.py\", line 558, in _get_observation_pairs\n    param_value = distribution.to_internal_repr(trial.params[param_name])\nAttributeError: '_BaseUniformDistribution' object has no attribute 'to_internal_repr'\n<\/code><\/pre>\n\n<p>I can't understand where is the problem and why the first trial is working. Please, help. <\/p>\n\n<p>Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1569926663223,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1569927339700,
        "Question_score":3,
        "Question_view_count":666.0,
        "Poster_id":4952975.0,
        "Poster_created_time":1432898903270,
        "Poster_last_access_time":1659613238212,
        "Poster_age":null,
        "Poster_location":"Moscow, Russia",
        "Poster_reputation":125.0,
        "Poster_views":20.0,
        "Answer_body":"<p>Your code seems to have no problems.<\/p>\n\n<p>I ran a simplified version of your code (see below), and it worked well in my environment:<\/p>\n\n<pre><code>import optuna\n\ndef objective(trial: optuna.Trial):\n    # sampling params\n    alpha = trial.suggest_uniform('alpha', 0, 100)\n    beta = trial.suggest_uniform('beta', 0, 100)\n\n    # evaluating params\n    return alpha + beta\n\n# starting study\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=3, n_jobs=1)\n<\/code><\/pre>\n\n<p>Could you tell me about your environment in order to investigate the problem? (e.g., OS, Python version, Python interpreter (CPython, PyPy, IronPython or Jython), Optuna version)<\/p>\n\n<blockquote>\n  <p>why the first trial is working.<\/p>\n<\/blockquote>\n\n<p>This error is raised by <a href=\"https:\/\/github.com\/pfnet\/optuna\/blob\/389a176c8cd1c860001a7a4562670006643e5e11\/optuna\/samplers\/tpe\/sampler.py#L558\" rel=\"noreferrer\">optuna\/samplers\/tpe\/sampler.py#558<\/a>, and this line is only executed when the number of completed trials in the study is greater than zero.<\/p>\n\n<p>BTW, you might be able to avoid this problem by using <code>RandomSampler<\/code> as follows:<\/p>\n\n<pre><code>sampler = optuna.samplers.RandomSampler()\nstudy = optuna.create_study(direction='maximize', sampler=sampler)\n<\/code><\/pre>\n\n<p>Notice that the optimization performance of <code>RandomSampler<\/code> tends to be worse than <code>TPESampler<\/code> that is the default sampler of Optuna.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Answerer_id":12152103.0,
        "Answerer_created_time":1569996433956,
        "Answerer_last_access_time":1636607223656,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":191.0,
        "Answerer_views":17.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58183158",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Question_closed_time":1570006754996,
        "Question_original_content":"fix error baseuniformdistribut object attribut intern repr strang behaviour try us lib python optimis paramet recommend system model model custom look like standard fit predict sklearn model method set param simpl object function select paramet uniform int distribut set param model predict model fit stage simpl model us param predict stage calcul metric trial run normal sampl param print result log second trial strang error look code solv googl run studi trial okai tri rearrang part object function fit stage insid try calcul simpler metric help object function get train test fit model self model somerecommend self model fit train param def object trial trial save studi path joblib dump studi path sampl param alpha trial suggest uniform alpha beta trial suggest uniform beta set param model param alpha alpha beta beta self model set param param get predict rec self model predict param metric comput metric result metric hit rate rec test return metric result start studi studi creat studi direct maxim studi optim object trial job trial finish trial result valu current best valu paramet alpha beta set statu trial trialstat fail follow error attributeerror baseuniformdistribut object attribut intern repr traceback recent file user roseaysina anaconda env sauvag lib python site packag studi line run trial result func trial file user roseaysina code project model line object alpha file user roseaysina anaconda env sauvag lib python site packag trial line suggest uniform return self suggest distribut file user roseaysina anaconda env sauvag lib python site packag trial line suggest self studi trial distribut file user roseaysina anaconda env sauvag lib python site packag sampler tpe sampler line sampl independ valu score observ pair studi param file user roseaysina anaconda env sauvag lib python site packag sampler tpe sampler line observ pair param valu distribut intern repr trial param param attributeerror baseuniformdistribut object attribut intern repr set statu trial trialstat fail follow error attributeerror baseuniformdistribut object attribut intern repr traceback recent file user roseaysina anaconda env sauvag lib python site packag studi line run trial result func trial file user roseaysina code project model line object alpha file user roseaysina anaconda env sauvag lib python site packag trial line suggest uniform return self suggest distribut file user roseaysina anaconda env sauvag lib python site packag trial line suggest self studi trial distribut file user roseaysina anaconda env sauvag lib python site packag sampler tpe sampler line sampl independ valu score observ pair studi param file user roseaysina anaconda env sauvag lib python site packag sampler tpe sampler line observ pair param valu distribut intern repr trial param param attributeerror baseuniformdistribut object attribut intern repr understand problem trial work help thank",
        "Question_preprocessed_content":"fix error object attribut strang behaviour try us lib python optimis paramet recommend system model model custom look like standard sklearn model simpl object function select paramet uniform int distribut set param model predict model calcul metric trial run normal sampl param print result log second trial strang error solv googl run studi trial okai tri rearrang part object function fit stage insid try calcul simpler metric help object function trial understand problem trial work help thank",
        "Question_gpt_summary_original":"The user is encountering an error while using the optuna library in Python to optimize parameters for recommender systems' models. The first trial runs normally, but on the second and subsequent trials, the user encounters an error that they cannot solve or find on Google. The error message is \"'_BaseUniformDistribution' object has no attribute 'to_internal_repr'\". The user has tried rearranging parts of the objective function, putting the fit stage inside, and calculating simpler metrics, but nothing has helped.",
        "Question_gpt_summary":"user encount error librari python optim paramet recommend system model trial run normal second subsequ trial user encount error solv googl error messag baseuniformdistribut object attribut intern repr user tri rearrang part object function put fit stage insid calcul simpler metric help",
        "Answer_original_content":"code problem ran simplifi version code work environ import def object trial trial sampl param alpha trial suggest uniform alpha beta trial suggest uniform beta evalu param return alpha beta start studi studi creat studi direct maxim studi optim object trial job tell environ order investig problem python version python interpret cpython pypi ironpython jython version trial work error rais sampler tpe sampler line execut number complet trial studi greater zero btw abl avoid problem randomsampl follow sampler sampler randomsampl studi creat studi direct maxim sampler sampler notic optim perform randomsampl tend wors tpesampl default sampler",
        "Answer_preprocessed_content":"code problem ran simplifi version code work environ tell environ order investig problem version trial work error rais line execut number complet trial studi greater zero btw abl avoid problem follow notic optim perform tend wors default sampler",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- The user can try using the RandomSampler instead of the default TPESampler to avoid the error.\n- The user can provide more information about their environment to investigate the problem further.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss user try randomsampl instead default tpesampl avoid error user provid inform environ investig problem person opinion bias includ summari"
    },
    {
        "Question_title":"How to reenable automatic synchronisation",
        "Question_body":"<p>I made a change to my script and now I have to manually synchronise my runs, my script contains<\/p>\n<pre><code>if args.dry_run:\n    os.environ['WANDB_MODE'] = 'dryrun'\n\nwandb.init(project=args.project_name, notes=args.notes)\n\n# log all experimental args to wandb\nwandb.config.update(args)\n<\/code><\/pre>\n<p>The change I made was the first line, setting <code>WANDB_MODE=dryrun<\/code>. From that point on I cannot re-enable automatic synchronisation.<\/p>\n<p>I\u2019ve run <code>wandb online<\/code> and run my script with <code>dryrun=False<\/code>. I also realised that this doesn\u2019t unset WANDB_MODE so I tried setting it to \u2018online\u2019 when <code>dryrun==False<\/code>. But it always ends up logging to <code>wandb\/offline-run-*<\/code> and I have to manually sync it.<\/p>\n<p>Is there another step to re-enable sync\u2019ing?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1662254971968,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":130.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>I\u2019ve found a way around this - I\u2019m not really sure why it\u2019s happening but I noticed that the huggingface trainer logs the metrics at the end of training as follows:<\/p>\n<pre><code>                if not args.load_best_model_at_end\n                else {\n                    f\"eval\/{args.metric_for_best_model}\": state.best_metric,\n                    \"train\/total_floss\": state.total_flos,\n                }\n<\/code><\/pre>\n<p>Meaning it logs the validation loss, but only if you train with <code>load_best_model_at_end=True<\/code> and set <code>save_strategy==evaluation_strategy<\/code> (epoch or steps) and <code>save_steps=eval_steps<\/code>.<\/p>\n<p>Doing this means I didn\u2019t need to perform the separate eval step since it\u2019s already logged the evaluation loss from the best model during training.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-reenable-automatic-synchronisation\/3061",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1662355289728,
        "Question_original_content":"reenabl automat synchronis chang script manual synchronis run script contain arg dry run environ mode dryrun init project arg project note arg note log experiment arg config updat arg chang line set mode dryrun point enabl automat synchronis iv run onlin run script dryrun fals realis doesnt unset mode tri set onlin dryrun fals end log offlin run manual sync step enabl sync",
        "Question_preprocessed_content":"reenabl automat synchronis chang script manual synchronis run script contain chang line set point automat synchronis iv run run script realis doesnt unset tri set onlin end log manual sync step sync",
        "Question_gpt_summary_original":"The user made a change to their script by setting WANDB_MODE to 'dryrun', which disabled automatic synchronization. They have tried running the script with 'dryrun=False' and setting WANDB_MODE to 'online' when 'dryrun==False', but the script still logs to 'wandb\/offline-run-*' and requires manual synchronization. The user is seeking advice on how to re-enable automatic synchronization.",
        "Question_gpt_summary":"user chang script set mode dryrun disabl automat synchron tri run script dryrun fals set mode onlin dryrun fals script log offlin run requir manual synchron user seek advic enabl automat synchron",
        "Answer_original_content":"iv wai sure happen notic huggingfac trainer log metric end train follow arg load best model end eval arg metric best model state best metric train total floss state total flo mean log valid loss train load best model end true set save strategi evalu strategi epoch step save step eval step mean didnt need perform separ eval step log evalu loss best model train",
        "Answer_preprocessed_content":"iv wai sure happen notic huggingfac trainer log metric end train follow mean log valid loss train set mean didnt need perform separ eval step log evalu loss best model train",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Sagemaker Batch Transform entry point",
        "Question_body":"<p>Before the AWS Sagemaker batch transform I need to do some transform. is it possible to have an custom script and associate as entry point to BatchTransformer?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1645114921967,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":230.0,
        "Poster_id":11314562.0,
        "Poster_created_time":1554425457572,
        "Poster_last_access_time":1659094784667,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":63.0,
        "Poster_views":2.0,
        "Answer_body":"<p>The inference code and requirement.txt should be stored as part of model.gz while training.  They will be used in the batch transform!!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":11314562.0,
        "Answerer_created_time":1554425457572,
        "Answerer_last_access_time":1659094784667,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":63.0,
        "Answerer_views":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71161777",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1646173336163,
        "Question_original_content":"batch transform entri point batch transform need transform possibl custom script associ entri point batchtransform",
        "Question_preprocessed_content":"batch transform entri point batch transform need transform possibl custom script associ entri point batchtransform",
        "Question_gpt_summary_original":"The user is facing a challenge with AWS Sagemaker batch transform and is seeking to use a custom script as an entry point to the BatchTransformer.",
        "Question_gpt_summary":"user face challeng batch transform seek us custom script entri point batchtransform",
        "Answer_original_content":"infer code requir txt store model train batch transform",
        "Answer_preprocessed_content":"infer code store train batch transform",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to store the inference code and requirement.txt as part of model.gz while training. This will ensure that they are used in the batch transform.",
        "Answer_gpt_summary":"solut solut mention discuss store infer code requir txt model train ensur batch transform"
    },
    {
        "Question_title":"Azure Machine Learning Designer Error: JobConfigurationMaxSizeExceeded",
        "Question_body":"<p>I have an Azure Machine Learning Designer pipeline that I've run successfully many dozens of times.  Suddenly, today, The pipeline is getting down to the 'Train Model' node and failing with the following error:<\/p>\n<p><code>JobConfigurationMaxSizeExceeded: The specified job configuration exceeds the max allowed size of 32768 characters. Please reduce the size of the job's command line arguments and environment settings<\/code><\/p>\n<p>How do I address this error in designer-built pipelines?<\/p>\n<p>I have even gone back to previously successful runs of this pipeline and resubmitted one of these runs which also failed with the exact same error.  A resubmitted run should have the exact same pipeline architecture and input data (afaik), so it seems like a problem outside my control.<\/p>\n<p>Pipeline with error:\n<a href=\"https:\/\/i.stack.imgur.com\/uLoIe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uLoIe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Pipeline run overview:\n<a href=\"https:\/\/i.stack.imgur.com\/eTzTA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eTzTA.png\" alt=\"enter image description here\" \/><\/a>\nAny ideas?<\/p>\n<p>EDIT:  I'm able to repro this with a really simple pipeline.  Simply trying to exclude columns in a <code>Select Columns<\/code> node from a dataset gives me this error:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qZKj1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qZKj1.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1638827883783,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1638837173276,
        "Question_score":3,
        "Question_view_count":358.0,
        "Poster_id":1486998.0,
        "Poster_created_time":1340833876128,
        "Poster_last_access_time":1663795160110,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":751.0,
        "Poster_views":73.0,
        "Answer_body":"<p>This appears to be a bug introduced by Microsoft's rollout of their new Compute Common Runtime.<\/p>\n<p>If I go into any nodes failing with the <code>JobConfigurationMaxSizeExceeded<\/code> exception and manually set <code>AZUREML_COMPUTE_USE_COMMON_RUNTIME:false<\/code> in their  <code>Environment JSON<\/code> field, then they work correctly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":1486998.0,
        "Answerer_created_time":1340833876128,
        "Answerer_last_access_time":1663795160110,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":751.0,
        "Answerer_views":73.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70252478",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1638852273883,
        "Question_original_content":"design error jobconfigurationmaxsizeexceed design pipelin run successfulli dozen time suddenli todai pipelin get train model node fail follow error jobconfigurationmaxsizeexceed specifi job configur exce max allow size charact reduc size job command line argument environ set address error design built pipelin gone previous success run pipelin resubmit run fail exact error resubmit run exact pipelin architectur input data afaik like problem outsid control pipelin error pipelin run overview idea edit abl repro simpl pipelin simpli try exclud column select column node dataset give error",
        "Question_preprocessed_content":"design error jobconfigurationmaxsizeexceed design pipelin run successfulli dozen time suddenli todai pipelin get train model node fail follow error address error pipelin gone previous success run pipelin resubmit run fail exact error resubmit run exact pipelin architectur input data like problem outsid control pipelin error pipelin run overview idea edit abl repro simpl pipelin simpli try exclud column node dataset give error",
        "Question_gpt_summary_original":"The user is encountering an error in Azure Machine Learning Designer pipeline while running the 'Train Model' node. The error message indicates that the job configuration exceeds the maximum allowed size of 32768 characters. The user has tried resubmitting previously successful runs of the pipeline, but the error persists. The user is seeking advice on how to address this error in designer-built pipelines.",
        "Question_gpt_summary":"user encount error design pipelin run train model node error messag indic job configur exce maximum allow size charact user tri resubmit previous success run pipelin error persist user seek advic address error design built pipelin",
        "Answer_original_content":"appear bug introduc microsoft rollout new comput common runtim node fail jobconfigurationmaxsizeexceed except manual set comput us common runtim fals environ json field work correctli",
        "Answer_preprocessed_content":"appear bug introduc microsoft rollout new comput common runtim node fail except manual set field work correctli",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to manually set \"AZUREML_COMPUTE_USE_COMMON_RUNTIME:false\" in the Environment JSON field of the nodes failing with the JobConfigurationMaxSizeExceeded exception. This has been reported to work correctly.",
        "Answer_gpt_summary":"solut possibl solut mention discuss manual set comput us common runtim fals environ json field node fail jobconfigurationmaxsizeexceed except report work correctli"
    },
    {
        "Question_title":"Model generated by AWS autopilot",
        "Question_body":"<p>After AWS Autopilot creates a model, How to use that model to the training data set offline?<\/p>\n<p>How to use that .tar.gz model file?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1637321691430,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1658782461116,
        "Question_score":0,
        "Question_view_count":125.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>The .tar.gz file is a model artifact<\/p>\n<p>To create a model, you combine the algorithm container and the model artifact<\/p>\n<p>You can do so in the Console, under Inference &gt; Models &gt; Create Model<\/p>\n<p>What do you mean by &quot;How to use that model to the training data set offline?&quot;<\/p>\n<p>If you mean, &quot;run a batch transformation&quot;, then once you create a model, you can select the model in the console, then click 'Create batch transform job'<\/p>\n<p>If you want to do it locally, then you can use the SageMaker Python SDK in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> and run the transform on your local computer (requires Docker)<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1637865568940,
        "Answer_score":1.0,
        "Answerer_id":223478.0,
        "Answerer_created_time":1259808393296,
        "Answerer_last_access_time":1664074603568,
        "Answerer_age":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation":44706.0,
        "Answerer_views":4356.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70034212",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1637338122312,
        "Question_original_content":"model gener aw autopilot aw autopilot creat model us model train data set offlin us tar model file",
        "Question_preprocessed_content":"model gener aw autopilot aw autopilot creat model us model train data set offlin us model file",
        "Question_gpt_summary_original":"The user is facing challenges in understanding how to use a model generated by AWS Autopilot to train data offline and how to use the .tar.gz model file.",
        "Question_gpt_summary":"user face challeng understand us model gener aw autopilot train data offlin us tar model file",
        "Answer_original_content":"tar file model artifact creat model combin algorithm contain model artifact consol infer model creat model mean us model train data set offlin mean run batch transform creat model select model consol click creat batch transform job want local us python sdk local mode run transform local requir docker",
        "Answer_preprocessed_content":"file model artifact creat model combin algorithm contain model artifact consol infer model creat model mean us model train data set offlin mean run batch transform creat model select model consol click creat batch transform job want local us python sdk local mode run transform local",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- To use the .tar.gz model file, the user needs to combine it with the algorithm container to create a model. This can be done in the AWS Console under Inference > Models > Create Model.\n- If the user wants to run a batch transformation on the training data set offline, they can select the model in the console and click 'Create batch transform job'.\n- If the user wants to do it locally, they can use the SageMaker Python SDK in Local Mode and run the transform on their local computer (requires Docker).\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss us tar model file user need combin algorithm contain creat model aw consol infer model creat model user want run batch transform train data set offlin select model consol click creat batch transform job user want local us python sdk local mode run transform local requir docker person opinion bias includ summari"
    },
    {
        "Question_title":"How to access DVC-controlled files from Oracle?",
        "Question_body":"<p>I have been storing my large files in CLOBs within Oracle, but I am thinking of storing my large files in a shared drive, then having a column in Oracle contain pointers to the files. This would use DVC.<\/p>\n<p>When I do this,<\/p>\n<p>(a) are the paths in Oracle paths that point to the files in my shared drive, as in, the actual files themselves?<\/p>\n<p>(b) or do the paths in Oracle point somehow to the DVC metafile?<\/p>\n<p>Any insight would help me out!<\/p>\n<p>Thanks :)\nJustin<\/p>\n<hr \/>\n<p>EDIT to provide more clarity:<\/p>\n<p>I checked here (<a href=\"https:\/\/dvc.org\/doc\/api-reference\/open\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/api-reference\/open<\/a>), and it helped, but I'm not fully there yet ...<\/p>\n<p>I want to pull a file from a remote dvc repository using python (which I have connected to the Oracle database). So, if we can make that work, I think I will be good. But, I am confused. If I specify 'remote' below, then how do I name the file (e.g., 'activity.log') when the remote files are all encoded?<\/p>\n<pre><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>(NOTE: For testing purposes, my &quot;remote&quot; DVC directory is just another folder on my MacBook.)<\/p>\n<p>I feel like I'm missing a key concept about getting remote files ...<\/p>\n<p>I hope that adds more clarity. Any help figuring out remote file access is appreciated! :)<\/p>\n<p>Justin<\/p>\n<hr \/>\n<p>EDIT to get insights on 'rev' parameter:<\/p>\n<p>Before my question, some background\/my setup:\n(a) I have a repo on my MacBook called 'basics'.\n(b) I copied into 'basics' a directory of 501 files (called 'surface_files') that I subsequently pushed to a remote storage folder called 'gss'. After the push, 'gss' contains 220 hash directories.<\/p>\n<p>The steps I used to get here are as follows:<\/p>\n<pre><code>&gt; cd ~\/Desktop\/Work\/basics\n&gt; git init\n&gt; dvc init\n&gt; dvc add ~\/Desktop\/Work\/basics\/surface_files\n&gt; git add .gitignore surface_files.dvc\n&gt; git commit -m &quot;Add raw data&quot;\n&gt; dvc remote add -d remote_storage ~\/Desktop\/Work\/gss\n&gt; git commit .dvc\/config -m &quot;Configure remote storage&quot;\n&gt; dvc push\n&gt; rm -rf .\/.dvc\/cache\n&gt; rm -rf .\/surface_files\n<\/code><\/pre>\n<p>Next, I ran the following Python code to take one of my surface files, named <code>surface_100141.dat<\/code>, and used <code>dvc.api.get_url()<\/code> to get the corresponding remote storage file name. I then copied this remote storage file into my desktop under the file's original name, i.e., <code>surface_100141.dat<\/code>.<\/p>\n<p>The code that does all this is as follows, but FIRST, MY QUESTION --- when I run the code as it is shown below, no problems; but when I uncomment the 'rev=' line, it fails. I am not sure why this is happening. I used <code>git log<\/code> and <code>cat .git\/refs\/heads\/master<\/code> to make sure that I was getting the right hash. WHY IS THIS FAILING? That is my question.<\/p>\n<p>(In full disclosure, my git knowledge is not too strong yet. I'm getting there, but it's still a work in progress! :))<\/p>\n<pre><code>import dvc.api\nimport os.path\nfrom os import path\nimport shutil\n\nfilename = 'surface_100141.dat' # This file name would be stored in my Oracle database\nhome_dir = os.path.expanduser('~')+'\/' # This simply expanding '~' into '\/Users\/ricej\/'\n\nresource_url = dvc.api.get_url(\n    path=f'surface_files\/{filename}', # Works when 'surface_files.dvc' exists, even when 'surface_files' directory and .dvc\/cache do not\n    repo=f'{home_dir}Desktop\/Work\/basics',\n    # rev='5c92710e68c045d75865fa24f1b56a0a486a8a45', # Commit hash, found using 'git log' or 'cat .git\/refs\/heads\/master'\n    remote='remote_storage')\nresource_url = home_dir+resource_url\nprint(f'Remote file: {resource_url}')\n\nnew_dir = f'{home_dir}Desktop\/' # Will copy fetched file to desktop, for demonstration\nnew_file = new_dir+filename\nprint(f'Remote file copy: {new_file}')\n\nif path.exists(new_file):\n    os.remove(new_file)\n    \ndest = shutil.copy(resource_url, new_file) # Check your desktop after this to see remote file copy\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1617399923477,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1617643343263,
        "Question_score":1,
        "Question_view_count":389.0,
        "Poster_id":3904518.0,
        "Poster_created_time":1407091594728,
        "Poster_last_access_time":1654654035420,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":53.0,
        "Poster_views":7.0,
        "Answer_body":"<p>I'm not 100% sure that I understand the question (it would be great to expand it a bit on the actual use case you are trying to solve with this database), but I can share a few thoughts.<\/p>\n<p>When we talk about DVC, I think you need to specify a few things to identify the file\/directory:<\/p>\n<ol>\n<li>Git commit + path (actual path like <code>data\/data\/xml<\/code>). Commit (or to be precise any Git revision) is needed to identify the version of the data file.<\/li>\n<li>Or path in the DVC storage (<code>\/mnt\/shared\/storage\/00\/198493ef2343ao<\/code> ...<code>) + actual name of this file. This way you would be saving info that <\/code>.dvc` files have.<\/li>\n<\/ol>\n<p>I would say that second way is <em>not<\/em> recommended since to some extent it's an implementation detail - how does DVC store files internally. The public interface to DVC organized data storage is its repository URL + commit + file name.<\/p>\n<p>Edit (example):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'activity.log',\n        repo='location\/of\/dvc\/project',\n        remote='my-s3-bucket'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p><code>location\/of\/dvc\/project<\/code> this path must point to an actual Git repo. This repo should have a <code>.dvc<\/code> or <code>dvc.lock<\/code> file that has <code>activity.log<\/code> name in it + its hash in the remote storage:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>outs:\n  - md5: a304afb96060aad90176268345e10355\n    path: activity.log\n<\/code><\/pre>\n<p>By reading this Git repo and analyzing let's say <code>activity.log.dvc<\/code> DVC will be able to create the right path <code>s3:\/\/my-bucket\/storage\/a3\/04afb96060aad90176268345e10355<\/code><\/p>\n<p><code>remote='my-s3-bucket'<\/code> argument is optional. By default it will use the one that is defined in the repo itself.<\/p>\n<p>Let's take another real example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>with dvc.api.open(\n        'get-started\/data.xml',\n        repo='https:\/\/github.com\/iterative\/dataset-registry'\n        ) as fd:\n    for line in fd:\n        match = re.search(r'user=(\\w+)', line)\n        # ... Process users activity log\n<\/code><\/pre>\n<p>In the <code>https:\/\/github.com\/iterative\/dataset-registry<\/code> you could find the <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/get-started\/data.xml.dvc\" rel=\"nofollow noreferrer\"><code>.dvc<\/code> file<\/a> that is enough for DVC to create a path to the file by also analyzing its <a href=\"https:\/\/github.com\/iterative\/dataset-registry\/blob\/master\/.dvc\/config\" rel=\"nofollow noreferrer\">config<\/a><\/p>\n<pre><code>https:\/\/remote.dvc.org\/dataset-registry\/a3\/04afb96060aad90176268345e10355\n<\/code><\/pre>\n<p>you could run <code>wget<\/code> on this file to download it<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1617478114567,
        "Answer_score":2.0,
        "Answerer_id":298182.0,
        "Answerer_created_time":1250347954880,
        "Answerer_last_access_time":1664073072270,
        "Answerer_age":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation":5575.0,
        "Answerer_views":358.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66925614",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_closed_time":1617404822540,
        "Question_original_content":"access control file oracl store larg file clob oracl think store larg file share drive have column oracl contain pointer file us path oracl path point file share drive actual file path oracl point metafil insight help thank justin edit provid clariti check http org doc api refer open help fulli want pull file remot repositori python connect oracl databas work think good confus specifi remot file activ log remot file encod api open activ log repo locat project remot bucket line match search user line process user activ log note test purpos remot directori folder macbook feel like miss kei concept get remot file hope add clariti help figur remot file access appreci justin edit insight rev paramet question background setup repo macbook call basic copi basic directori file call surfac file subsequ push remot storag folder call gss push gss contain hash directori step follow desktop work basic git init init add desktop work basic surfac file git add gitignor surfac file git commit add raw data remot add remot storag desktop work gss git commit config configur remot storag push cach surfac file ran follow python code surfac file name surfac dat api url correspond remot storag file copi remot storag file desktop file origin surfac dat code follow question run code shown problem uncom rev line fail sure happen git log cat git ref head master sure get right hash fail question disclosur git knowledg strong get work progress import api import path import path import shutil filenam surfac dat file store oracl databas home dir path expandus simpli expand user ricej resourc url api url path surfac file filenam work surfac file exist surfac file directori cach repo home dir desktop work basic rev cecdfafbaaaa commit hash git log cat git ref head master remot remot storag resourc url home dir resourc url print remot file resourc url new dir home dir desktop copi fetch file desktop demonstr new file new dir filenam print remot file copi new file path exist new file remov new file dest shutil copi resourc url new file check desktop remot file copi",
        "Question_preprocessed_content":"access control file oracl store larg file clob oracl think store larg file share drive have column oracl contain pointer file us path oracl path point file share drive actual file path oracl point metafil insight help thank justin edit provid clariti check help fulli want pull file remot repositori python work think good confus specifi remot file remot file encod note test purpos remot directori folder feel like miss kei concept get remot file hope add clariti help figur remot file access appreci justin edit insight rev paramet question setup repo macbook call basic copi basic directori file subsequ push remot storag folder call gss push gss contain hash directori step follow ran follow python code surfac file name correspond remot storag file copi remot storag file desktop file origin code follow question run code shown problem uncom rev line fail sure happen sure get right hash fail question disclosur git knowledg strong get work progress",
        "Question_gpt_summary_original":"The user is trying to access DVC-controlled files from Oracle and is considering storing large files in a shared drive with a column in Oracle containing pointers to the files. The user is seeking clarification on whether the paths in Oracle point to the files in the shared drive or to the DVC metafile. Additionally, the user is trying to pull a file from a remote DVC repository using Python and is confused about how to name the file when the remote files are all encoded. Finally, the user is seeking insights on the 'rev' parameter and why uncommenting it causes the code to fail.",
        "Question_gpt_summary":"user try access control file oracl consid store larg file share drive column oracl contain pointer file user seek clarif path oracl point file share drive metafil addition user try pull file remot repositori python confus file remot file encod final user seek insight rev paramet uncom caus code fail",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"ClientError: train channel is not specified with AWS object_detection_augmented_manifest_training using ground truth images",
        "Question_body":"<p>I have completed a labelling job in AWS ground truth and started working on the notebook template for object detection.<\/p>\n\n<p>I have 2 manifests which has 293 labeled images for birds in a train and validation set like this:<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\",\"Bird-Label-Train\":{\"workerId\":XXXXXXXX,\"imageSource\":{\"s3Uri\":\"s3:\/\/XXXXXXX\/Train\/Blackbird_1.JPG\"},\"boxesInfo\":{\"annotatedResult\":{\"boundingBoxes\":[{\"width\":1612,\"top\":841,\"label\":\"Blackbird\",\"left\":1276,\"height\":757}],\"inputImageProperties\":{\"width\":3872,\"height\":2592}}}},\"Bird-Label-Train-metadata\":{\"type\":\"groundtruth\/custom\",\"job-name\":\"bird-label-train\",\"human-annotated\":\"yes\",\"creation-date\":\"2019-01-16T17:28:23+0000\"}}\n<\/code><\/pre>\n\n<p>Below are the parameters I am using for the notebook instance:<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 5\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"1\",\n         \"mini_batch_size\": \"16\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n \"InputDataConfig\": [\n    {\n        \"ChannelName\": \"train\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_train_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label-Train\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    },\n    {\n        \"ChannelName\": \"validation\",\n        \"DataSource\": {\n            \"S3DataSource\": {\n                \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                \"S3Uri\": s3_validation_data_path,\n                \"S3DataDistributionType\": \"FullyReplicated\",\n                \"AttributeNames\": [\"source-ref\",\"Bird-Label\"] # NB. This must correspond to the JSON field names in your augmented manifest.\n            }\n        },\n        \"ContentType\": \"image\/jpeg\",\n        \"RecordWrapperType\": \"None\",\n        \"CompressionType\": \"None\"\n    }\n]\n<\/code><\/pre>\n\n<p>I would end up with this being printed after running my ml.p3.2xlarge instance:<\/p>\n\n<pre><code>InProgress Starting\nInProgress Starting\nInProgress Starting\nInProgress Training\nFailed Failed\n<\/code><\/pre>\n\n<p>Followed by this error message: \n<strong>'ClientError: train channel is not specified.'<\/strong><\/p>\n\n<p>Does anyone have any thoughts for how I can get this running with no errors? Any help is much apreciated!<\/p>\n\n<p><strong>Successful run:<\/strong> Below is the paramaters that were used, along with the Augmented Manifest JSON Objects for a successful run.<\/p>\n\n<pre><code>training_params = \\\n{\n    \"AlgorithmSpecification\": {\n        \"TrainingImage\": training_image, # NB. This is one of the named constants defined in the first cell.\n        \"TrainingInputMode\": \"Pipe\"\n    },\n    \"RoleArn\": role,\n    \"OutputDataConfig\": {\n        \"S3OutputPath\": s3_output_path\n    },\n    \"ResourceConfig\": {\n        \"InstanceCount\": 1,   \n        \"InstanceType\": \"ml.p3.2xlarge\",\n        \"VolumeSizeInGB\": 50\n    },\n    \"TrainingJobName\": job_name,\n    \"HyperParameters\": { # NB. These hyperparameters are at the user's discretion and are beyond the scope of this demo.\n         \"base_network\": \"resnet-50\",\n         \"use_pretrained_model\": \"1\",\n         \"num_classes\": \"3\",\n         \"mini_batch_size\": \"1\",\n         \"epochs\": \"5\",\n         \"learning_rate\": \"0.001\",\n         \"lr_scheduler_step\": \"3,6\",\n         \"lr_scheduler_factor\": \"0.1\",\n         \"optimizer\": \"rmsprop\",\n         \"momentum\": \"0.9\",\n         \"weight_decay\": \"0.0005\",\n         \"overlap_threshold\": \"0.5\",\n         \"nms_threshold\": \"0.45\",\n         \"image_shape\": \"300\",\n         \"label_width\": \"350\",\n         \"num_training_samples\": str(num_training_samples)\n    },\n    \"StoppingCondition\": {\n        \"MaxRuntimeInSeconds\": 86400\n    },\n    \"InputDataConfig\": [\n        {\n            \"ChannelName\": \"train\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_train_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": attribute_names # NB. This must correspond to the JSON field names in your **TRAIN** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        },\n        {\n            \"ChannelName\": \"validation\",\n            \"DataSource\": {\n                \"S3DataSource\": {\n                    \"S3DataType\": \"AugmentedManifestFile\", # NB. Augmented Manifest\n                    \"S3Uri\": s3_validation_data_path,\n                    \"S3DataDistributionType\": \"FullyReplicated\",\n                    \"AttributeNames\": [\"source-ref\",\"ValidateBird\"] # NB. This must correspond to the JSON field names in your **VALIDATION** augmented manifest.\n                }\n            },\n            \"ContentType\": \"application\/x-recordio\",\n            \"RecordWrapperType\": \"RecordIO\",\n            \"CompressionType\": \"None\"\n        }\n    ]\n}\n<\/code><\/pre>\n\n<p>Training Augmented Manifest File generated during the running of the training job<\/p>\n\n<pre><code>Line 1\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_1.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1613,\"top\":840,\"height\":766,\"left\":1293}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:21:29.829003\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 2\n{\"source-ref\":\"s3:\/\/xxxxx\/Train\/Blackbird_2.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":897,\"top\":665,\"height\":1601,\"left\":1598}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:22:34.502274\",\"type\":\"groundtruth\/object-detection\"}}\n\n\nLine 3\n{\"source-ref\":\"s3:\/\/XXXXX\/Train\/Blackbird_3.JPG\",\"TrainBird\":{\"annotations\":[{\"class_id\":0,\"width\":1040,\"top\":509,\"height\":1695,\"left\":1548}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"TrainBird-metadata\":{\"job-name\":\"labeling-job\/trainbird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:20:26.660164\",\"type\":\"groundtruth\/object-detection\"}}\n<\/code><\/pre>\n\n<p>I then unzip the model.tar file to get the following files:hyperparams.JSON, model_algo_1-0000.params and model_algo_1-symbol<\/p>\n\n<p>hyperparams.JSON looks like this:<\/p>\n\n<pre><code>{\"label_width\": \"350\", \"early_stopping_min_epochs\": \"10\", \"epochs\": \"5\", \"overlap_threshold\": \"0.5\", \"lr_scheduler_factor\": \"0.1\", \"_num_kv_servers\": \"auto\", \"weight_decay\": \"0.0005\", \"mini_batch_size\": \"1\", \"use_pretrained_model\": \"1\", \"freeze_layer_pattern\": \"\", \"lr_scheduler_step\": \"3,6\", \"early_stopping\": \"False\", \"early_stopping_patience\": \"5\", \"momentum\": \"0.9\", \"num_training_samples\": \"11\", \"optimizer\": \"rmsprop\", \"_tuning_objective_metric\": \"\", \"early_stopping_tolerance\": \"0.0\", \"learning_rate\": \"0.001\", \"kv_store\": \"device\", \"nms_threshold\": \"0.45\", \"num_classes\": \"1\", \"base_network\": \"resnet-50\", \"nms_topk\": \"400\", \"_kvstore\": \"device\", \"image_shape\": \"300\"}\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1547399704507,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1551005141750,
        "Question_score":3,
        "Question_view_count":1312.0,
        "Poster_id":10908571.0,
        "Poster_created_time":1547398724312,
        "Poster_last_access_time":1565032201830,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":41.0,
        "Poster_views":6.0,
        "Answer_body":"<p>Thank you again for your help. All of which were valid in helping me get further. Having received a response on the AWS forum pages, I finally got it working.<\/p>\n\n<p>I understood that my JSON was slightly different to the augmented manifest training guide. Having gone back to basics, I created another labelling job, but used the 'Bounding Box' type as opposed to the 'Custom - Bounding box template'. My output matched what was expected. This ran with no errors!<\/p>\n\n<p>As my purpose was to have multiple labels, I was able to edit the files and mapping of my output manifests, which also worked!<\/p>\n\n<p>i.e.<\/p>\n\n<pre><code>{\"source-ref\":\"s3:\/\/xxxxx\/Blackbird_15.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":0,\"width\":2023,\"top\":665,\"height\":1421,\"left\":1312}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"0\":\"Blackbird\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.174131\",\"type\":\"groundtruth\/object-detection\"}}\n{\"source-ref\":\"s3:\/\/xxxx\/Pigeon_19.JPG\",\"ValidateBird\":{\"annotations\":[{\"class_id\":2,\"width\":784,\"top\":634,\"height\":1657,\"left\":1306}],\"image_size\":[{\"width\":3872,\"depth\":3,\"height\":2592}]},\"ValidateBird-metadata\":{\"job-name\":\"labeling-job\/validatebird\",\"class-map\":{\"2\":\"Pigeon\"},\"human-annotated\":\"yes\",\"objects\":[{\"confidence\":0.09}],\"creation-date\":\"2019-02-09T14:23:51.074809\",\"type\":\"groundtruth\/object-detection\"}} \n<\/code><\/pre>\n\n<p>The original mapping was 0:'Bird' for all images through the labelling job.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":10908571.0,
        "Answerer_created_time":1547398724312,
        "Answerer_last_access_time":1565032201830,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":41.0,
        "Answerer_views":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54171261",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1549801330896,
        "Question_original_content":"clienterror train channel specifi aw object detect augment manifest train ground truth imag complet label job aw ground truth start work notebook templat object detect manifest label imag bird train valid set like sourc ref train blackbird jpg bird label train workerid imagesourc suri train blackbird jpg boxesinfo annotatedresult boundingbox width label blackbird left height inputimageproperti width height bird label train metadata type groundtruth custom job bird label train human annot ye creation date paramet notebook instanc train param algorithmspecif trainingimag train imag name constant defin cell traininginputmod pipe rolearn role outputdataconfig soutputpath output path resourceconfig instancecount instancetyp xlarg volumesizeingb trainingjobnam job hyperparamet hyperparamet user discret scope demo base network resnet us pretrain model num class mini batch size epoch learn rate schedul step schedul factor optim rmsprop momentum weight decai overlap threshold nm threshold imag shape label width num train sampl str num train sampl stoppingcondit maxruntimeinsecond inputdataconfig channelnam train datasourc sdatasourc sdatatyp augmentedmanifestfil augment manifest suri train data path sdatadistributiontyp fullyrepl attributenam sourc ref bird label train correspond json field name augment manifest contenttyp imag jpeg recordwrappertyp compressiontyp channelnam valid datasourc sdatasourc sdatatyp augmentedmanifestfil augment manifest suri valid data path sdatadistributiontyp fullyrepl attributenam sourc ref bird label correspond json field name augment manifest contenttyp imag jpeg recordwrappertyp compressiontyp end print run xlarg instanc inprogress start inprogress start inprogress start inprogress train fail fail follow error messag clienterror train channel specifi thought run error help apreci success run paramat augment manifest json object success run train param algorithmspecif trainingimag train imag name constant defin cell traininginputmod pipe rolearn role outputdataconfig soutputpath output path resourceconfig instancecount instancetyp xlarg volumesizeingb trainingjobnam job hyperparamet hyperparamet user discret scope demo base network resnet us pretrain model num class mini batch size epoch learn rate schedul step schedul factor optim rmsprop momentum weight decai overlap threshold nm threshold imag shape label width num train sampl str num train sampl stoppingcondit maxruntimeinsecond inputdataconfig channelnam train datasourc sdatasourc sdatatyp augmentedmanifestfil augment manifest suri train data path sdatadistributiontyp fullyrepl attributenam attribut name correspond json field name train augment manifest contenttyp applic recordio recordwrappertyp recordio compressiontyp channelnam valid datasourc sdatasourc sdatatyp augmentedmanifestfil augment manifest suri valid data path sdatadistributiontyp fullyrepl attributenam sourc ref validatebird correspond json field name valid augment manifest contenttyp applic recordio recordwrappertyp recordio compressiontyp train augment manifest file gener run train job line sourc ref train blackbird jpg trainbird annot class width height left imag size width depth height trainbird metadata job label job trainbird class map blackbird human annot ye object confid creation date type groundtruth object detect line sourc ref train blackbird jpg trainbird annot class width height left imag size width depth height trainbird metadata job label job trainbird class map blackbird human annot ye object confid creation date type groundtruth object detect line sourc ref train blackbird jpg trainbird annot class width height left imag size width depth height trainbird metadata job label job trainbird class map blackbird human annot ye object confid creation date type groundtruth object detect unzip model tar file follow file hyperparam json model algo param model algo symbol hyperparam json look like label width earli stop min epoch epoch overlap threshold schedul factor num server auto weight decai mini batch size us pretrain model freez layer pattern schedul step earli stop fals earli stop patienc momentum num train sampl optim rmsprop tune object metric earli stop toler learn rate store devic nm threshold num class base network resnet nm topk kvstore devic imag shape",
        "Question_preprocessed_content":"clienterror train channel specifi aw ground truth imag complet label job aw ground truth start work notebook templat object detect manifest label imag bird train valid set like paramet notebook instanc end print run instanc follow error messag clienterror train channel thought run error help apreci success run paramat augment manifest json object success run train augment manifest file gener run train job unzip file follow look like",
        "Question_gpt_summary_original":"The user encountered an error message \"ClientError: train channel is not specified\" while working on a notebook template for object detection in AWS ground truth. The error occurred after running an ml.p3.2xlarge instance and was caused by a missing train channel in the input data configuration. The user provided the parameters used for a successful run, which included an augmented manifest file and hyperparameters.",
        "Question_gpt_summary":"user encount error messag clienterror train channel specifi work notebook templat object detect aw ground truth error occur run xlarg instanc caus miss train channel input data configur user provid paramet success run includ augment manifest file hyperparamet",
        "Answer_original_content":"thank help valid help have receiv respons aw forum page final got work understood json slightli differ augment manifest train guid have gone basic creat label job bound box type oppos custom bound box templat output match expect ran error purpos multipl label abl edit file map output manifest work sourc ref blackbird jpg validatebird annot class width height left imag size width depth height validatebird metadata job label job validatebird class map blackbird human annot ye object confid creation date type groundtruth object detect sourc ref pigeon jpg validatebird annot class width height left imag size width depth height validatebird metadata job label job validatebird class map pigeon human annot ye object confid creation date type groundtruth object detect origin map bird imag label job",
        "Answer_preprocessed_content":"thank help valid help have receiv respons aw forum page final got work understood json slightli differ augment manifest train guid have gone basic creat label job bound box type oppos custom bound box templat output match expect ran error purpos multipl label abl edit file map output manifest work origin map bird imag label job",
        "Answer_gpt_summary_original":"Solution:\n- The user found a solution by creating another labeling job using the 'Bounding Box' type instead of the 'Custom - Bounding box template' and editing the files and mapping of the output manifests to have multiple labels.",
        "Answer_gpt_summary":"solut user solut creat label job bound box type instead custom bound box templat edit file map output manifest multipl label"
    },
    {
        "Question_title":"Azure Machine Learning Studio Designer Error: code_expired",
        "Question_body":"<p>I am trying to register a data set via the Azure Machine Learning Studio designer but keep getting an error. Here is my code, used in a &quot;Execute Python Script&quot; module:<\/p>\n<pre><code>import pandas as pd\nfrom azureml.core.dataset import Dataset\nfrom azureml.core import Workspace\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    ws = Workspace.get(name = &lt;my_workspace_name&gt;, subscription_id = &lt;my_id&gt;, resource_group = &lt;my_RG&gt;)\n    ds = Dataset.from_pandas_dataframe(dataframe1)\n    ds.register(workspace = ws,\n                name = &quot;data set name&quot;,\n                description = &quot;example description&quot;,\n                create_new_version = True)\n    return dataframe1, \n<\/code><\/pre>\n<p>But I get the following error in the Workspace.get line:<\/p>\n<pre><code>Authentication Exception: Unknown error occurred during authentication. Error detail: Unexpected polling state code_expired.\n<\/code><\/pre>\n<p>Since I am inside the workspace and in the designer, I do not usually need to do any kind of authentication (or even reference the workspace). Can anybody offer some direction? Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1628037272883,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1628038626927,
        "Question_score":1,
        "Question_view_count":321.0,
        "Poster_id":2494795.0,
        "Poster_created_time":1371499229816,
        "Poster_last_access_time":1664024865427,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1111.0,
        "Poster_views":191.0,
        "Answer_body":"<p>when you're inside a &quot;Execute Python Script&quot; module or <code>PythonScriptStep<\/code>, the authentication for fetching the workspace is already done for you (unless you're trying to authenticate to different Azure ML workspace.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Run\nrun = Run.get_context()\n\nws = run.experiment.workspace\n<\/code><\/pre>\n<p>You should be able to use that <code>ws<\/code> object to register a Dataset.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":3842610.0,
        "Answerer_created_time":1405457120427,
        "Answerer_last_access_time":1663947733100,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":3359.0,
        "Answerer_views":555.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68644137",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1628038438487,
        "Question_original_content":"studio design error code expir try regist data set studio design get error code execut python script modul import panda core dataset import dataset core import workspac def main datafram datafram workspac subscript resourc group dataset panda datafram datafram regist workspac data set descript exampl descript creat new version true return datafram follow error workspac line authent except unknown error occur authent error unexpect poll state code expir insid workspac design usual need kind authent refer workspac anybodi offer direct thank",
        "Question_preprocessed_content":"studio design error try regist data set studio design get error code execut python script modul follow error line insid workspac design usual need kind authent anybodi offer direct thank",
        "Question_gpt_summary_original":"The user is encountering an error while trying to register a data set via the Azure Machine Learning Studio designer. The error occurs in the Workspace.get line and is related to authentication, with the error message stating \"Unexpected polling state code_expired.\" The user is unsure why authentication is necessary since they are already inside the workspace and seeking guidance to resolve the issue.",
        "Question_gpt_summary":"user encount error try regist data set studio design error occur workspac line relat authent error messag state unexpect poll state code expir user unsur authent necessari insid workspac seek guidanc resolv issu",
        "Answer_original_content":"insid execut python script modul pythonscriptstep authent fetch workspac try authent differ workspac core import run run run context run experi workspac abl us object regist dataset",
        "Answer_preprocessed_content":"insid execut python script modul authent fetch workspac try authent differ workspac abl us object regist dataset",
        "Answer_gpt_summary_original":"Solution: The user can use the \"Execute Python Script\" module or PythonScriptStep to authenticate and fetch the workspace. They can then use the \"ws\" object to register a dataset.",
        "Answer_gpt_summary":"solut user us execut python script modul pythonscriptstep authent fetch workspac us object regist dataset"
    },
    {
        "Question_title":"Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Question_body":"Does AutoML support optimizing convolutional neural network over the number of layers and pool layer parameters?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1607093437353,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"AutoML doesn't currently support CNNs publicly, it's on our roadmap and it will come with optimizations across different parameters, so stay tuned. Hope this helps.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/186789\/does-automl-support-optimizing-convolutional-neura.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1607117314596,
        "Question_original_content":"automl support optim convolut neural network number layer pool layer paramet automl support optim convolut neural network number layer pool layer paramet",
        "Question_preprocessed_content":"automl support optim convolut neural network number layer pool layer paramet automl support optim convolut neural network number layer pool layer paramet",
        "Question_gpt_summary_original":"The user is seeking information on whether AutoML can optimize convolutional neural networks based on the number of layers and pool layer parameters.",
        "Question_gpt_summary":"user seek inform automl optim convolut neural network base number layer pool layer paramet",
        "Answer_original_content":"automl current support cnn publicli roadmap come optim differ paramet stai tune hope help",
        "Answer_preprocessed_content":"automl current support cnn publicli roadmap come optim differ paramet stai tune hope help",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Add Security groups in Amazon SageMaker for distributed training jobs",
        "Question_body":"<p>We would like to enforce specific security groups to be set on the SageMaker training jobs (XGBoost in script mode).\nHowever, distributed training, in this case, won\u2019t work out of the box, since the containers need to communicate with each other. What are the minimum inbound\/outbound rules (ports) that we need to specify for training jobs so that they can communicate?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1662733216593,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":19.0,
        "Poster_id":19947257.0,
        "Poster_created_time":1662621266503,
        "Poster_last_access_time":1663966999636,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":48.0,
        "Poster_views":6.0,
        "Answer_body":"<p>setting up training in VPC including specifying security groups is documented here:\u00a0<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/train-vpc.html#train-vpc-groups<\/a><\/p>\n<p>Normally you would allow all communication between the training nodes. To do this you specify the security group source and destination to the name of the security group itself, and allow all IPv4 traffic. If you want to figure out what ports are used, you could: 1\/ define the permissive security group. 2\/ Turn on VPC flow logs 3\/ run training. 4\/ examine VPC Flow logs 5\/ update the security group only to the required ports.<\/p>\n<p>I must say restricting communication between the training nodes might be an extreme, so I would challenge the customer why it's really needed, as all nodes carry the same job, have the same IAM role, and are transiate by nature.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":8754522.0,
        "Answerer_created_time":1507661294190,
        "Answerer_last_access_time":1663873580420,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":98.0,
        "Answerer_views":16.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73663585",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1662835019252,
        "Question_original_content":"add secur group distribut train job like enforc specif secur group set train job xgboost script mode distribut train case wont work box contain need commun minimum inbound outbound rule port need specifi train job commun",
        "Question_preprocessed_content":"add secur group distribut train job like enforc specif secur group set train job distribut train case wont work box contain need commun minimum rule need specifi train job commun",
        "Question_gpt_summary_original":"The user is facing challenges in adding security groups to Amazon SageMaker for distributed training jobs, specifically for XGBoost in script mode. The issue is that distributed training won't work out of the box because the containers need to communicate with each other, and the user needs to specify the minimum inbound\/outbound rules (ports) for training jobs to enable communication.",
        "Question_gpt_summary":"user face challeng ad secur group distribut train job specif xgboost script mode issu distribut train won work box contain need commun user need specifi minimum inbound outbound rule port train job enabl commun",
        "Answer_original_content":"set train vpc includ specifi secur group document http doc aw amazon com latest train vpc html train vpc group normal allow commun train node specifi secur group sourc destin secur group allow ipv traffic want figur port defin permiss secur group turn vpc flow log run train examin vpc flow log updat secur group requir port restrict commun train node extrem challeng custom need node carri job iam role transiat natur",
        "Answer_preprocessed_content":"set train vpc includ specifi secur group document normal allow commun train node specifi secur group sourc destin secur group allow ipv traffic want figur port defin permiss secur group turn vpc flow log run train examin vpc flow log updat secur group requir port restrict commun train node extrem challeng custom need node carri job iam role transiat natur",
        "Answer_gpt_summary_original":"Solution:\n- The user can set up training in VPC including specifying security groups by following the documentation provided by Amazon SageMaker.\n- Normally, the user would allow all communication between the training nodes by specifying the security group source and destination to the name of the security group itself and allowing all IPv4 traffic.\n- To figure out what ports are used, the user can define the permissive security group, turn on VPC flow logs, run training, examine VPC Flow logs, and update the security group only to the required ports.\n- It is suggested to challenge the customer why restricting communication between the training nodes is really needed, as all nodes carry the same job, have the same IAM role, and are transient by nature.",
        "Answer_gpt_summary":"solut user set train vpc includ specifi secur group follow document provid normal user allow commun train node specifi secur group sourc destin secur group allow ipv traffic figur port user defin permiss secur group turn vpc flow log run train examin vpc flow log updat secur group requir port suggest challeng custom restrict commun train node need node carri job iam role transient natur"
    },
    {
        "Question_title":"AWS SageMaker PyTorch: no module named 'sagemaker'",
        "Question_body":"<p>I have deployed a PyTorch model on AWS with SageMaker, and I try to send a request to test the service. However, I got a very vague error message saying \"no module named 'sagemaker'\". I have tried to search online, but cannot find posts about similar message.<\/p>\n\n<p>My client code: <\/p>\n\n<pre><code>import numpy as np\nfrom sagemaker.pytorch.model import PyTorchPredictor\n\nENDPOINT = '&lt;endpoint name&gt;'\n\npredictor = PyTorchPredictor(ENDPOINT)\npredictor.predict(np.random.random_sample([1, 3, 224, 224]).tobytes())\n<\/code><\/pre>\n\n<p>Detailed error message:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"client.py\", line 7, in &lt;module&gt;\n    predictor.predict(np.random.random_sample([1, 3, 224, 224]).tobytes())\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/sagemaker\/predictor.py\", line 110, in predict\n    response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 276, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \"\/Users\/jiashenc\/Env\/py3\/lib\/python3.7\/site-packages\/botocore\/client.py\", line 586, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"No module named 'sagemaker'\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/&lt;endpoint name&gt; in account xxxxxxxxxxxxxx for more information.\n<\/code><\/pre>\n\n<p>This bug is because I merge both the serving script and my deploy script together, see below<\/p>\n\n<pre><code>import os\nimport torch\nimport numpy as np\nfrom sagemaker.pytorch.model import PyTorchModel\nfrom torch import cuda\nfrom torchvision.models import resnet50\n\n\ndef model_fn(model_dir):\n    device = torch.device('cuda' if cuda.is_available() else 'cpu')\n    model = resnet50()\n    with open(os.path.join(model_dir, 'model.pth'), 'rb') as f:\n        model.load_state_dict(torch.load(f, map_location=device))\n    return model.to(device)\n\ndef predict_fn(input_data, model):\n    device = torch.device('cuda' if cuda.is_available() else 'cpu')\n    model.eval()\n    with torch.no_grad():\n        return model(input_data.to(device))\n\n\nif __name__ == '__main__':\n    pytorch_model = PyTorchModel(model_data='s3:\/\/&lt;bucket name&gt;\/resnet50\/model.tar.gz',\n                                    entry_point='serve.py', role='jiashenC-sagemaker',\n                                    py_version='py3', framework_version='1.3.1')\n    predictor = pytorch_model.deploy(instance_type='ml.t2.medium', initial_instance_count=1)\n    print(predictor.predict(np.random.random_sample([1, 3, 224, 224]).astype(np.float32)))\n<\/code><\/pre>\n\n<p>The root cause is the 4th line in my code. It tries to import sagemaker, which is an unavailable library. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1581115266463,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1581260842870,
        "Question_score":1,
        "Question_view_count":5568.0,
        "Poster_id":7718669.0,
        "Poster_created_time":1489634112740,
        "Poster_last_access_time":1664069959960,
        "Poster_age":null,
        "Poster_location":"Atlanta, GA, USA",
        "Poster_reputation":1653.0,
        "Poster_views":72.0,
        "Answer_body":"<p><em>(edit 2\/9\/2020 with extra code snippets)<\/em><\/p>\n\n<p>Your serving code tries to use the <code>sagemaker<\/code> module internally. The <code>sagemaker<\/code> module (also called <a href=\"http:\/\/sagemaker.readthedocs.io\" rel=\"nofollow noreferrer\">SageMaker Python SDK<\/a>, one of the numerous orchestration SDKs for SageMaker) is not designed to be used in model containers, but instead out of models, to orchestrate their activity (train, deploy, bayesian tuning, etc). In your specific example, you shouldn't include the deployment and model call code to server code, as those are actually actions that will be conducted from outside the server to orchestrate its lifecyle and interact with it. For model deployment with the Sagemaker Pytorch container, your entry point script just needs to contain the required <code>model_fn<\/code> function for model deserialization, and optionally an <code>input_fn<\/code>, <code>predict_fn<\/code> and <code>output_fn<\/code>, respectively for pre-processing, inference and post-processing (<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#the-sagemaker-pytorch-model-server\" rel=\"nofollow noreferrer\">detailed in the documentation here<\/a>). This logic is beautiful :) : you don't need anything else to deploy a production-ready deep learning server! (MMS in the case of Pytorch and MXNet, Flask+Gunicorn in the case of sklearn).<\/p>\n\n<p>In summary, this is how your code should be split:<\/p>\n\n<p>An entry_point script <code>serve.py<\/code> that contains model serving code and looks like this:<\/p>\n\n<pre><code>import os\n\nimport numpy as np\nimport torch\nfrom torch import cuda\nfrom torchvision.models import resnet50\n\ndef model_fn(model_dir):\n    # TODO instantiate a model from its artifact stored in model_dir\n    return model\n\ndef predict_fn(input_data, model):\n    # TODO apply model to the input_data, return result of interest\n    return result\n<\/code><\/pre>\n\n<p>and some orchestration code to instantiate a SageMaker Model object, deploy it to a server and query it. This is run from the orchestration runtime of your choice, which could be a SageMaker Notebook, your laptop, an AWS Lambda function, an Apache Airflow operator, etc - and with the SDK for your choice; don't need to use python for this.<\/p>\n\n<pre><code>import numpy as np\nfrom sagemaker.pytorch.model import PyTorchModel\n\npytorch_model = PyTorchModel(\n    model_data='s3:\/\/&lt;bucket name&gt;\/resnet50\/model.tar.gz',\n    entry_point='serve.py',\n    role='jiashenC-sagemaker',\n    py_version='py3',\n    framework_version='1.3.1')\n\npredictor = pytorch_model.deploy(instance_type='ml.t2.medium', initial_instance_count=1)\n\nprint(predictor.predict(np.random.random_sample([1, 3, 224, 224]).astype(np.float32)))\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1581269375956,
        "Answer_score":4.0,
        "Answerer_id":5331834.0,
        "Answerer_created_time":1442180190107,
        "Answerer_last_access_time":1664048608876,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60122070",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1581189783900,
        "Question_original_content":"pytorch modul name deploi pytorch model aw try send request test servic got vagu error messag sai modul name tri search onlin post similar messag client code import numpi pytorch model import pytorchpredictor endpoint predictor pytorchpredictor endpoint predictor predict random random sampl tobyt detail error messag traceback recent file client line predictor predict random random sampl tobyt file user jiashenc env lib python site packag predictor line predict respons self session runtim client invok endpoint request arg file user jiashenc env lib python site packag botocor client line api return self api oper kwarg file user jiashenc env lib python site packag botocor client line api rais error class pars respons oper botocor errorfactori modelerror error occur modelerror call invokeendpoint oper receiv server error model messag modul name http east consol aw amazon com cloudwatch home region east logeventview group aw endpoint account inform bug merg serv script deploi script import import torch import numpi pytorch model import pytorchmodel torch import cuda torchvis model import resnet def model model dir devic torch devic cuda cuda avail cpu model resnet open path join model dir model pth model load state dict torch load map locat devic return model devic def predict input data model devic torch devic cuda cuda avail cpu model eval torch grad return model input data devic main pytorch model pytorchmodel model data resnet model tar entri point serv role jiashenc version framework version predictor pytorch model deploi instanc type medium initi instanc count print predictor predict random random sampl astyp float root caus line code tri import unavail librari",
        "Question_preprocessed_content":"pytorch modul name deploi pytorch model aw try send request test servic got vagu error messag sai modul name tri search onlin post similar messag client code detail error messag bug merg serv script deploi script root caus line code tri import unavail librari",
        "Question_gpt_summary_original":"The user encountered an error message \"no module named 'sagemaker'\" while trying to send a request to test a PyTorch model deployed on AWS SageMaker. The error occurred due to the user merging the serving script and deploy script together, resulting in the import of an unavailable library.",
        "Question_gpt_summary":"user encount error messag modul name try send request test pytorch model deploi error occur user merg serv script deploi script result import unavail librari",
        "Answer_original_content":"edit extra code snippet serv code tri us modul intern modul call python sdk numer orchestr sdk design model contain instead model orchestr activ train deploi bayesian tune specif exampl shouldn includ deploy model code server code actual action conduct outsid server orchestr lifecyl interact model deploy pytorch contain entri point script need contain requir model function model deseri option input predict output respect pre process infer post process detail document logic beauti need deploi product readi deep learn server mm case pytorch mxnet flask gunicorn case sklearn summari code split entri point script serv contain model serv code look like import import numpi import torch torch import cuda torchvis model import resnet def model model dir todo instanti model artifact store model dir return model def predict input data model todo appli model input data return result return result orchestr code instanti model object deploi server queri run orchestr runtim choic notebook laptop aw lambda function apach airflow oper sdk choic need us python import numpi pytorch model import pytorchmodel pytorch model pytorchmodel model data resnet model tar entri point serv role jiashenc version framework version predictor pytorch model deploi instanc type medium initi instanc count print predictor predict random random sampl astyp float",
        "Answer_preprocessed_content":"serv code tri us modul intern modul design model contain instead model orchestr activ specif exampl shouldn includ deploy model code server code actual action conduct outsid server orchestr lifecyl interact model deploy pytorch contain entri point script need contain requir function model deseri option respect infer logic beauti need deploi deep learn server summari code split script contain model serv code look like orchestr code instanti model object deploi server queri run orchestr runtim choic notebook laptop aw lambda function apach airflow oper sdk choic need us python",
        "Answer_gpt_summary_original":"Solution: The user should split the code into two parts: an entry point script `serve.py` that contains model serving code and orchestration code to instantiate a SageMaker Model object, deploy it to a server, and query it. The entry point script should contain the required `model_fn` function for model deserialization, and optionally an `input_fn`, `predict_fn`, and `output_fn`, respectively for pre-processing, inference, and post-processing. The `sagemaker` module is not designed to be used in model containers, but instead out of models, to orchestrate their activity.",
        "Answer_gpt_summary":"solut user split code part entri point script serv contain model serv code orchestr code instanti model object deploi server queri entri point script contain requir model function model deseri option input predict output respect pre process infer post process modul design model contain instead model orchestr activ"
    },
    {
        "Question_title":"Uploading 1000s of files to AWS Notebook Instance",
        "Question_body":"<p>When I open AWS Notebook Instance-&gt; Jupyter Notebook. It gives me a storage (probably called an S3 bucket). I created a folder there and tried to upload 1000s of data. However, it asks me to manually click on the upload button next to every single file. Is it possible to upload that data much easier way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1599628300940,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":382.0,
        "Poster_id":2421555.0,
        "Poster_created_time":1369539919747,
        "Poster_last_access_time":1663810191950,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":311.0,
        "Poster_views":35.0,
        "Answer_body":"<p>You could use the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/userguide\/cli-services-s3-commands.html#using-s3-commands-managing-objects-move\" rel=\"nofollow noreferrer\">AWS-CLI<\/a> or the <a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/S3.html\" rel=\"nofollow noreferrer\">AWS-S3 SDK<\/a> (JS in this example).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":12638118.0,
        "Answerer_created_time":1577919980176,
        "Answerer_last_access_time":1664044981843,
        "Answerer_age":null,
        "Answerer_location":"Hamburg, Germany",
        "Answerer_reputation":5588.0,
        "Answerer_views":398.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63805114",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1599632498016,
        "Question_original_content":"upload file aw notebook instanc open aw notebook instanc jupyt notebook give storag probabl call bucket creat folder tri upload data ask manual click upload button singl file possibl upload data easier wai",
        "Question_preprocessed_content":"upload file aw notebook instanc open aw notebook jupyt notebook give storag creat folder tri upload data ask manual click upload button singl file possibl upload data easier wai",
        "Question_gpt_summary_original":"The user is facing a challenge while uploading thousands of files to an S3 bucket in AWS Notebook Instance. The system requires the user to manually click on the upload button next to every single file, making the process time-consuming and tedious. The user is seeking a more efficient way to upload the data.",
        "Question_gpt_summary":"user face challeng upload thousand file bucket aw notebook instanc requir user manual click upload button singl file make process time consum tediou user seek effici wai upload data",
        "Answer_original_content":"us aw cli aw sdk exampl",
        "Answer_preprocessed_content":"us sdk",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are to use the AWS-CLI or the AWS-S3 SDK to upload the files to the S3 bucket in AWS Notebook Instance.",
        "Answer_gpt_summary":"possibl solut mention discuss us aw cli aw sdk upload file bucket aw notebook instanc"
    },
    {
        "Question_title":"Running a request against all variants in an endpoint",
        "Question_body":"I have a customer asking me about the Rendezvous architecture. What I'm thinking is, we could implement this in a number of ways, all using endpoint variants:\n\nLambda (and probably SQS) around the endpoint;\nA custom monitoring job;\nStep Functions\n\nWithout going into details of the above options or of how the evaluation and SLA check will be done, it looks like the several models would fit very well as variants of an endpoint. The thing is, the architecture expects to call them all. Is there a way to directly call all variants of a model, or will a wrapper to identify the variants, call them all and process the results be needed?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1604486652000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":14.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"When I last looked into it, it was not possible to query all versions\/variants of the model automatically. You can specify what variant to use when using the invoke_endpoint method. I would therefore write a lambda function to invoke each of the endpoints one-by-one (see here: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_runtime_InvokeEndpoint.html). To be especially rigorous about it, you can add a function in your lambda code that first retrieves all the endpoint variants (see here: https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.describe_endpoint) then queries them one-by-one, and returns all the results.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6bm-EMtOQV6robgbTXClLQ\/running-a-request-against-all-variants-in-an-endpoint",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1604506964000,
        "Question_original_content":"run request variant endpoint custom ask rendezv architectur think implement number wai endpoint variant lambda probabl sq endpoint custom monitor job step function go detail option evalu sla check look like model fit variant endpoint thing architectur expect wai directli variant model wrapper identifi variant process result need",
        "Question_preprocessed_content":"run request variant endpoint custom ask rendezv architectur think implement number wai endpoint variant lambda endpoint custom monitor job step function go detail option evalu sla check look like model fit variant endpoint thing architectur expect wai directli variant model wrapper identifi variant process result need",
        "Question_gpt_summary_original":"The user is facing a challenge of how to call all variants of a model in the Rendezvous architecture without a wrapper to identify the variants and process the results. They are considering implementing Lambda, SQS, custom monitoring job, or Step Functions as endpoint variants, but are unsure of how to call them all directly.",
        "Question_gpt_summary":"user face challeng variant model rendezv architectur wrapper identifi variant process result consid implement lambda sq custom monitor job step function endpoint variant unsur directli",
        "Answer_original_content":"look possibl queri version variant model automat specifi variant us invok endpoint method write lambda function invok endpoint http doc aw amazon com latest apirefer api runtim invokeendpoint html especi rigor add function lambda code retriev endpoint variant http boto amazonaw com document api latest refer servic html client endpoint queri return result",
        "Answer_preprocessed_content":"look possibl queri model automat specifi variant us method write lambda function invok endpoint especi rigor add function lambda code retriev endpoint variant queri return result",
        "Answer_gpt_summary_original":"Solution: The user can write a lambda function to invoke each of the endpoints one-by-one using the invoke_endpoint method. They can also add a function in their lambda code that first retrieves all the endpoint variants and then queries them one-by-one, returning all the results.",
        "Answer_gpt_summary":"solut user write lambda function invok endpoint invok endpoint method add function lambda code retriev endpoint variant queri return result"
    },
    {
        "Question_title":"Vertex AI forecasting AutoML datatype mismatch",
        "Question_body":"<p>I could train the vertex AI AutoML forecating model but when I do batch prediction I get following error<\/p>\n<blockquote>\n<p>Batch prediction job batch_prediction encountered the following\nerrors:<\/p>\n<pre><code>Column &quot;sales&quot; expects type: NUMBER, the actual type is: STRING.\n<\/code><\/pre>\n<\/blockquote>\n<p>Below is a sample of test set I am passing for batch prediction in big query.<\/p>\n<p>According to the documentation for batch prediction we have to send some training\/historical data and forecasting dates. I did just that.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/J7eYT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/J7eYT.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_created_time":1644227346773,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1644237395456,
        "Question_score":1,
        "Question_view_count":222.0,
        "Poster_id":5687866.0,
        "Poster_created_time":1450288149287,
        "Poster_last_access_time":1661168365350,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":500.0,
        "Poster_views":72.0,
        "Answer_body":"<p>Google recommend you to use the same input format for ingraining and prediction. Seams you have trained your model using a input format here the column sales were a <code>numeric<\/code> type, and now in the prediction you a using a BigQuery table with the <code>sales<\/code> column as <code>string<\/code>.<\/p>\n<p>Delete this table and import the data again defining the schema <strong>manually<\/strong>, and set sales as a numeric field, as following:<\/p>\n<pre><code>date:DATE,\nstore_product_id:STRING,\nsales:NUMERIC\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":13248738.0,
        "Answerer_created_time":1586263306992,
        "Answerer_last_access_time":1663926938332,
        "Answerer_age":null,
        "Answerer_location":"Krak\u00f3w, Poland",
        "Answerer_reputation":1622.0,
        "Answerer_views":654.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71016472",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1644420672183,
        "Question_original_content":"forecast automl datatyp mismatch train automl forec model batch predict follow error batch predict job batch predict encount follow error column sale expect type number actual type string sampl test set pass batch predict big queri accord document batch predict send train histor data forecast date",
        "Question_preprocessed_content":"forecast automl datatyp mismatch train automl forec model batch predict follow error batch predict job encount follow error sampl test set pass batch predict big queri accord document batch predict send data forecast date",
        "Question_gpt_summary_original":"The user encountered a datatype mismatch error while performing batch prediction using Vertex AI AutoML forecasting model. The error occurred because the column \"sales\" expected a NUMBER datatype, but the actual datatype was STRING. The user followed the documentation and provided training\/historical data and forecasting dates for batch prediction in BigQuery.",
        "Question_gpt_summary":"user encount datatyp mismatch error perform batch predict automl forecast model error occur column sale expect number datatyp actual datatyp string user follow document provid train histor data forecast date batch predict bigqueri",
        "Answer_original_content":"googl recommend us input format ingrain predict seam train model input format column sale numer type predict bigqueri tabl sale column string delet tabl import data defin schema manual set sale numer field follow date date store product string sale numer",
        "Answer_preprocessed_content":"googl recommend us input format ingrain predict seam train model input format column sale type predict bigqueri tabl column delet tabl import data defin schema manual set sale numer field follow",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to delete the table and import the data again with a manually defined schema where the \"sales\" column is set as a numeric field. The suggested schema is:\n\ndate:DATE,\nstore_product_id:STRING,\nsales:NUMERIC",
        "Answer_gpt_summary":"solut solut suggest discuss delet tabl import data manual defin schema sale column set numer field suggest schema date date store product string sale numer"
    },
    {
        "Question_title":"Google Translate API and Serbian Latin script",
        "Question_body":"Hi there,In Serbia we are using 2 scripts side by side - Cyrillic and Latin script.I am heaving an issue with translation to Serbian Latin.\nBy default Google translate offer translation to Serbian Cyrillic , but bellow that default translation there is a translation to Serbian Latin.\nTake a look at this example:\nhttps:\/\/translate.google.com\/?sl=en&tl=sr&text=Hello%20world!&op=translateI have found this post from early 2019.\nhttps:\/\/support.google.com\/translate\/thread\/1836538?hl=enLike in that post my question is the same:\nI need it to support Serbian Latin, for some projects I don`t use the Cyrillic script. Also there is a problem with translating pages or similar plugins, e.g.: Google Language Translator for WordPress and some others CMS system like Kopage you can translate only to Serbian Cyrillic script.As I found this post on stackoverflow:\nhttps:\/\/stackoverflow.com\/questions\/73699065\/google-cloud-translate-serbian-latin-not-workingIt seems, according to the poster of that article, that there was a workaround.\nInstead of \"sr\" ISO-639 code you can put \"sr_Latn\" - and you will get translation into Serbian Latin script.\nBut that workaround stop working several weeks ago - according to the poster.Is there a workaround to translate into Serbian Latin characters rather into Serbian Cyrillic characters?Regards,\nBranislav",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1665455160000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":128.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"It appears that translation to the Serbian Latin Alphabet is not officially supported by the Cloud Translate API, as discussed in this recent issue. Therefore it\u2019s not assured that any possible workaround will be functional or reliable. You can see the list of supported language codes for translation here.\n\nYou can, however, submit a Feature Request to the public Google issue tracker for Cloud Translation API. The higher the number of users who bring attention to this request, the more likely it is for it to be eventually built into the API.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-API-and-Serbian-Latin-script\/td-p\/476723\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1665562320000,
        "Question_original_content":"googl translat api serbian latin script serbia script cyril latin script heav issu translat serbian latin default googl translat offer translat serbian cyril bellow default translat translat serbian latin look exampl http translat googl com text hello world translatei post earli http support googl com translat thread enlik post question need support serbian latin project us cyril script problem translat page similar plugin googl languag translat wordpress cm like kopag translat serbian cyril script post stackoverflow http stackoverflow com question googl cloud translat serbian latin workingit accord poster articl workaround instead iso code latn translat serbian latin script workaround stop work week ago accord poster workaround translat serbian latin charact serbian cyril charact regard branislav",
        "Question_preprocessed_content":"googl translat api serbian latin script serbia script cyril latin heav issu translat serbian latin default googl translat offer translat serbian cyril bellow default translat translat serbian latin look exampl post earli post question need support serbian latin project us cyril script problem translat page similar plugin googl languag translat wordpress cm like kopag translat serbian cyril post stackoverflow accord poster articl workaround instead code translat serbian latin script workaround stop work week ago accord workaround translat serbian latin charact serbian cyril charact regard branislav",
        "Question_gpt_summary_original":"The user is facing challenges with the Google Translate API when translating to Serbian Latin script. By default, the API offers translation to Serbian Cyrillic, and there is a problem with translating pages or plugins to Serbian Latin script. The user has found a workaround by using \"sr_Latn\" instead of \"sr\" ISO-639 code, but it has stopped working several weeks ago. The user is seeking a solution to translate into Serbian Latin characters rather than Serbian Cyrillic characters.",
        "Question_gpt_summary":"user face challeng googl translat api translat serbian latin script default api offer translat serbian cyril problem translat page plugin serbian latin script user workaround latn instead iso code stop work week ago user seek solut translat serbian latin charact serbian cyril charact",
        "Answer_original_content":"appear translat serbian latin alphabet offici support cloud translat api discuss recent issu assur possibl workaround function reliabl list support languag code translat submit featur request public googl issu tracker cloud translat api higher number user bring attent request like eventu built api view solut origin post",
        "Answer_preprocessed_content":"appear translat serbian latin alphabet offici support cloud translat api discuss recent issu assur possibl workaround function reliabl list support languag code translat submit featur request public googl issu tracker cloud translat api higher number user bring attent request like eventu built api view solut origin post",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include submitting a Feature Request to the public Google issue tracker for Cloud Translation API to add support for Serbian Latin script. However, it is not guaranteed that any possible workaround will be functional or reliable since translation to the Serbian Latin Alphabet is not officially supported by the Cloud Translate API.",
        "Answer_gpt_summary":"possibl solut mention discuss includ submit featur request public googl issu tracker cloud translat api add support serbian latin script guarante possibl workaround function reliabl translat serbian latin alphabet offici support cloud translat api"
    },
    {
        "Question_title":"Clienterror: An error occured when calling the CreateModel operation",
        "Question_body":"<p>I want to deploy sklearn model in sagemaker. I created a training script.<\/p>\n\n<p>scripPath=' sklearn.py'<\/p>\n\n<p><code>sklearn=SKLearn(entry_point=scripPath,\n                                 train_instance_type='ml.m5.xlarge',\n                                   role=role,                  output_path='s3:\/\/{}\/{}\/output'.format(bucket,prefix), sagemaker_session=session)\nsklearn.fit({\"train-dir' : train_input})<\/code><\/p>\n\n<p>When I deploy it\n<code>predictor=sklearn.deploy(initial_count=1,instance_type='ml.m5.xlarge')<\/code><\/p>\n\n<p>It throws,\n<code>Clienterror: An error occured when calling the CreateModel operation:Could not find model data at s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code><\/p>\n\n<p>Can anyone say how to solve this issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1560943786943,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":1033.0,
        "Poster_id":11621597.0,
        "Poster_created_time":1560085651596,
        "Poster_last_access_time":1561731351963,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":155.0,
        "Poster_views":5.0,
        "Answer_body":"<p>When deploying models, SageMaker looks up S3 to find your trained model artifact. It seems that there is no trained model artifact at <code>s3:\/\/tree\/sklearn\/output\/model.tar.gz<\/code>. Make sure to persist your model artifact in your training script at the appropriate local location in docker which is <code>\/opt\/ml\/model<\/code>.\nfor example, in your training script this could look like:<\/p>\n\n<pre><code>joblib.dump(model, \/opt\/ml\/model\/mymodel.joblib)\n<\/code><\/pre>\n\n<p>After training, SageMaker will copy the content of <code>\/opt\/ml\/model<\/code> to s3 at the <code>output_path<\/code> location.<\/p>\n\n<p>If you deploy in the same session a <code>model.deploy()<\/code> will map automatically to the artifact path. If you want to deploy a model that you trained elsewhere, possibly during a different session or in a different hardware, you need to explicitly instantiate a model before deploying<\/p>\n\n<pre><code>from sagemaker.sklearn.model import SKLearnModel\n\nmodel = SKLearnModel(\n    model_data='s3:\/\/...model.tar.gz',  # your artifact\n    role=get_execution_role(),\n    entry_point='script.py')  # script containing inference functions\n\nmodel.deploy(\n    instance_type='ml.m5.xlarge',\n    initial_instance_count=1,\n    endpoint_name='your_endpoint_name')\n<\/code><\/pre>\n\n<p>See more about Sklearn in SageMaker here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":5331834.0,
        "Answerer_created_time":1442180190107,
        "Answerer_last_access_time":1664048608876,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56666667",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1560972233448,
        "Question_original_content":"clienterror error occur call createmodel oper want deploi sklearn model creat train script scrippath sklearn sklearn sklearn entri point scrippath train instanc type xlarg role role output path output format bucket prefix session session sklearn fit train dir train input deploi predictor sklearn deploi initi count instanc type xlarg throw clienterror error occur call createmodel oper model data tree sklearn output model tar solv issu",
        "Question_preprocessed_content":"clienterror error occur call createmodel oper want deploi sklearn model creat train script scrippath deploi throw solv issu",
        "Question_gpt_summary_original":"The user is encountering an error when trying to deploy a sklearn model in sagemaker. The error message states that the model data cannot be found at the specified S3 location. The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user encount error try deploi sklearn model error messag state model data specifi locat user seek assist resolv issu",
        "Answer_original_content":"deploi model look train model artifact train model artifact tree sklearn output model tar sure persist model artifact train script appropri local locat docker opt model exampl train script look like joblib dump model opt model mymodel joblib train copi content opt model output path locat deploi session model deploi map automat artifact path want deploi model train possibl differ session differ hardwar need explicitli instanti model deploi sklearn model import sklearnmodel model sklearnmodel model data model tar artifact role execut role entri point script script contain infer function model deploi instanc type xlarg initi instanc count endpoint endpoint sklearn http readthedoc stabl sklearn html",
        "Answer_preprocessed_content":"deploi model look train model artifact train model artifact sure persist model artifact train script appropri local locat docker exampl train script look like train copi content locat deploi session map automat artifact path want deploi model train possibl differ session differ hardwar need explicitli instanti model deploi sklearn",
        "Answer_gpt_summary_original":"Solution: The user needs to make sure that the trained model artifact is persisted in the appropriate local location in docker which is `\/opt\/ml\/model`. After training, SageMaker will copy the content of `\/opt\/ml\/model` to S3 at the `output_path` location. If the user wants to deploy a model that they trained elsewhere, they need to explicitly instantiate a model before deploying. They can do this by using the `SKLearnModel` class and passing in the `model_data`, `role`, and `entry_point` parameters.",
        "Answer_gpt_summary":"solut user need sure train model artifact persist appropri local locat docker opt model train copi content opt model output path locat user want deploi model train need explicitli instanti model deploi sklearnmodel class pass model data role entri point paramet"
    },
    {
        "Question_title":"Vertex AI Pipeline Failed Precondition",
        "Question_body":"<p>I have been following this video:\n<a href=\"https:\/\/www.youtube.com\/watch?v=1ykDWsnL2LE&amp;t=310s\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=1ykDWsnL2LE&amp;t=310s<\/a><\/p>\n<p>Code located at:\n<a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5\" rel=\"nofollow noreferrer\">https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#5<\/a>\n(I have done the last two steps as per the video which isn't an issue for google_cloud_pipeline_components version: 0.1.1)<\/p>\n<p>I have created a pipeline in vertex ai which ran and used the following code to create the pipeline (from video not code extract in link above):<\/p>\n<pre><code>#run pipeline\nresponse = api_client.create_run_from_job_spec(\n    &quot;tab_classif_pipeline.json&quot;, pipeline_root = PIPELINE_ROOT,\n    parameter_values = {\n    &quot;project&quot; : PROJECT_ID,\n    &quot;display_name&quot; : DISPLAY_NAME\n    }\n)\n    \n<\/code><\/pre>\n<p>and in the GCP logs I get the following error:<\/p>\n<pre><code>&quot;google.api_core.exceptions.FailedPrecondition: 400 BigQuery Dataset location `eu` must be in the same location as the service location `us-central1`.\n<\/code><\/pre>\n<p>I get the error at the dataset_create_op stage:<\/p>\n<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n    project = project, display_name = display_name, bq_source = bq_source\n)\n<\/code><\/pre>\n<p>My dataset is configured in EU (the whole region) so I don't understand where us-central1 is coming from (or what the service location is?).<\/p>\n<p>Here is the all the code I have used:<\/p>\n<pre><code> PROJECT_ID = &quot;marketingtown&quot;\n BUCKET_NAME = f&quot;gs:\/\/lookalike_model&quot;\n from typing import NamedTuple\n import kfp\n from kfp import dsl\n from kfp.v2 import compiler\n from kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                            OutputPath, ClassificationMetrics, \n Metrics, component)\n from kfp.v2.components.types.artifact_types import Dataset\n from kfp.v2.google.client import AIPlatformClient\n from google.cloud import aiplatform\n from google_cloud_pipeline_components import aiplatform as gcc_aip\n\n #set environment variables\n PATH = %env PATH\n %env PATH = (PATH):\/\/home\/jupyter\/.local\/bin\n REGION = &quot;europe-west2&quot;\n    \n #cloud storage path where artifact is created by pipeline\n PIPELINE_ROOT = f&quot;{BUCKET_NAME}\/pipeline_root\/&quot;\n PIPELINE_ROOT\n import time\n DISPLAY_NAME = f&quot;lookalike_model_pipeline_{str(int(time.time()))}&quot;\n print(DISPLAY_NAME)\n \n@kfp.dsl.pipeline(name = &quot;lookalike-model-training-v2&quot;, \npipeline_root = PIPELINE_ROOT)\n\ndef pipeline(\n    bq_source : str = f&quot;bq:\/\/{PROJECT_ID}.MLOp_pipeline_temp.lookalike_training_set&quot;,\n    display_name : str = DISPLAY_NAME,\n    project : str = PROJECT_ID,\n    gcp_region : str = &quot;europe-west2&quot;,\n    api_endpoint : str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str : str = '{&quot;auPrc&quot; : 0.3}'\n):\n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project = project, display_name = display_name, bq_source = bq_source\n    )\n    \n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;], #dataset from previous step\n        target_column=&quot;sale&quot;,\n    )\n    \n    #outputted evaluation metrics\n    model_eval_task = classification_model_eval_metrics(\n        project,\n        gcp_region,\n        api_endpoint,\n        thresholds_dict_str,\n        training_op.outputs[&quot;model&quot;],\n    )\n    \n    #if deployment threshold is mean, deploy\n    with dsl.Condition(\n        model_eval_task.outputs[&quot;dep_decision&quot;] == &quot;true&quot;,\n        name=&quot;deploy_decision&quot;,\n    ):\n        \n    endpoint_op = gcc_aip.EndpointCreateOp(\n        project=project,\n        location=gcp_region,\n        display_name=&quot;train-automl-beans&quot;,\n    )\n        \n    #deploys model to an endpoint\n    gcc_aip.ModelDeployOp(\n        model=training_op.outputs[&quot;model&quot;],\n        endpoint=endpoint_op.outputs[&quot;endpoint&quot;],\n        min_replica_count=1,\n        max_replica_count=1,\n        machine_type=&quot;n1-standard-4&quot;,\n        )\n   \n\n     compiler.Compiler().compile(\n        pipeline_func = pipeline, package_path = &quot;tab_classif_pipeline.json&quot;\n    )\n\n    #run pipeline\n    response = api_client.create_run_from_job_spec(\n        &quot;tab_classif_pipeline.json&quot;, pipeline_root = PIPELINE_ROOT,\n        parameter_values = {\n        &quot;project&quot; : PROJECT_ID,\n        &quot;display_name&quot; : DISPLAY_NAME\n        }\n    )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_created_time":1645657042940,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1645697298212,
        "Question_score":1,
        "Question_view_count":420.0,
        "Poster_id":11761839.0,
        "Poster_created_time":1562706291280,
        "Poster_last_access_time":1664033431527,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":31.0,
        "Poster_views":9.0,
        "Answer_body":"<p>I solved this issue by adding the location to the TabularDatasetCreateJob:<\/p>\n<pre><code>    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n    project=project,\n    display_name=display_name, \n    bq_source=bq_source,\n    location = gcp_region\n)\n<\/code><\/pre>\n<p>I now have the same issue with the model training job but I have learnt that a lot of the functions in the above code take a location parameter, or default to us-central1. I will update if I get any further.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":11761839.0,
        "Answerer_created_time":1562706291280,
        "Answerer_last_access_time":1664033431527,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":31.0,
        "Answerer_views":9.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71245000",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1646828083056,
        "Question_original_content":"pipelin fail precondit follow video http youtub com watch ykdwsnlle code locat http codelab develop googl com vertex pipelin intro step video isn issu googl cloud pipelin compon version creat pipelin ran follow code creat pipelin video code extract link run pipelin respons api client creat run job spec tab classif pipelin json pipelin root pipelin root paramet valu project project displai displai gcp log follow error googl api core except failedprecondit bigqueri dataset locat locat servic locat central error dataset creat stage dataset creat gcc aip tabulardatasetcreateop project project displai displai sourc sourc dataset configur region understand central come servic locat code project marketingtown bucket lookalik model type import namedtupl import kfp kfp import dsl kfp import compil kfp dsl import artifact input inputpath model output outputpath classificationmetr metric compon kfp compon type artifact type import dataset kfp googl client import aiplatformcli googl cloud import aiplatform googl cloud pipelin compon import aiplatform gcc aip set environ variabl path env path env path path home jupyt local bin region europ west cloud storag path artifact creat pipelin pipelin root bucket pipelin root pipelin root import time displai lookalik model pipelin str int time time print displai kfp dsl pipelin lookalik model train pipelin root pipelin root def pipelin sourc str project mlop pipelin temp lookalik train set displai str displai project str project gcp region str europ west api endpoint str europ west aiplatform googleapi com threshold dict str str auprc dataset creat gcc aip tabulardatasetcreateop project project displai displai sourc sourc train gcc aip automltabulartrainingjobrunop project project displai displai optim predict type classif budget milli node hour column transform categor column agentid categor column postcod categor column ismobil categor column gender categor column timeofdai categor column sale dataset dataset creat output dataset dataset previou step target column sale output evalu metric model eval task classif model eval metric project gcp region api endpoint threshold dict str train output model deploy threshold mean deploi dsl condit model eval task output dep decis true deploi decis endpoint gcc aip endpointcreateop project project locat gcp region displai train automl bean deploi model endpoint gcc aip modeldeployop model train output model endpoint endpoint output endpoint min replica count max replica count machin type standard compil compil compil pipelin func pipelin packag path tab classif pipelin json run pipelin respons api client creat run job spec tab classif pipelin json pipelin root pipelin root paramet valu project project displai displai",
        "Question_preprocessed_content":"pipelin fail precondit follow video code locat step video isn issu version creat pipelin ran follow code creat pipelin gcp log follow error error stage dataset configur understand come code",
        "Question_gpt_summary_original":"The user encountered an error while creating a pipeline in Vertex AI. The error message indicates that the BigQuery dataset location 'eu' must be in the same location as the service location 'us-central1'. The user's dataset is configured in the EU region, so they are unsure where 'us-central1' is coming from or what the service location is. The error occurs at the dataset_create_op stage. The user provided the code they used to create the pipeline.",
        "Question_gpt_summary":"user encount error creat pipelin error messag indic bigqueri dataset locat locat servic locat central user dataset configur region unsur central come servic locat error occur dataset creat stage user provid code creat pipelin",
        "Answer_original_content":"solv issu ad locat tabulardatasetcreatejob dataset creat gcc aip tabulardatasetcreateop project project displai displai sourc sourc locat gcp region issu model train job learnt lot function code locat paramet default central updat",
        "Answer_preprocessed_content":"solv issu ad locat tabulardatasetcreatejob issu model train job learnt lot function code locat paramet default updat",
        "Answer_gpt_summary_original":"Solution: The user solved the issue by adding the location to the TabularDatasetCreateJob in the code. They added the parameter 'location = gcp_region' to the dataset_create_op. However, no solution was provided for the issue with the model training job.",
        "Answer_gpt_summary":"solut user solv issu ad locat tabulardatasetcreatejob code ad paramet locat gcp region dataset creat solut provid issu model train job"
    },
    {
        "Question_title":"AWS SageMaker - Realtime Data Processing",
        "Question_body":"<p>My company does online consumer behavior analysis and we do realtime predictions using the data we collected from various websites (with our java script embedded). <\/p>\n\n<p>We have been using AWS ML for real time prediction but now we are experimenting with AWS SageMaker it occurred to us that the realtime data processing is a problem compared to AWS ML. For example we have some string variables that AWS ML can convert to numerics and use them for real time prediction in AWS ML automatically. But it does not look like SageMaker can do it. <\/p>\n\n<p>Does anyone have any experience with real time data processing and prediction in AWS SageMaker?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1541421249963,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":4,
        "Question_view_count":1508.0,
        "Poster_id":7292363.0,
        "Poster_created_time":1481650459980,
        "Poster_last_access_time":1661430875243,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1446.0,
        "Poster_views":107.0,
        "Answer_body":"<p>AWS SageMaker is a robust machine learning service in AWS that manages every major aspect of machine learning implementation, including data preparation, model construction, training and fine-tuning, and deployment.<\/p>\n<p><strong>Preparation<\/strong><\/p>\n<p>SageMaker uses a range of resources to make it simple to prepare data for machine learning models, even though it comes from many sources or is in a variety of formats.<\/p>\n<p>It's simple to mark data, including video, images, and text, that's automatically processed into usable data, with SageMaker Ground Truth. GroundWork will process and merge this data using auto-segmentation and a suite of tools to create a single data label that can be used in machine learning models. AWS, in conjunction with SageMaker Data Wrangler and SageMaker Processing, reduces a data preparation phase that may take weeks or months to a matter of days, if not hours.<\/p>\n<p><strong>Build<\/strong><\/p>\n<p>SageMaker Studio Notebooks centralize everything relevant to your machine learning models, allowing them to be conveniently shared along with their associated data. You can choose from a variety of built-in, open-source algorithms to start processing your data with SageMaker JumpStart, or you can build custom parameters for your machine learning model.<\/p>\n<p>Once you've chosen a model, SageMaker starts processing data automatically and offers a simple, easy-to-understand interface for tracking your model's progress and performance.<\/p>\n<p><strong>Training<\/strong><\/p>\n<p>SageMaker provides a range of tools for training your model from the data you've prepared, including a built-in debugger for detecting possible errors.<\/p>\n<p>Machine Learning\nThe training job's results are saved in an Amazon S3 bucket, where they can be viewed using other AWS services including AWS Quicksight.<\/p>\n<p><strong>Deployment<\/strong><\/p>\n<p>It's pointless to have strong machine learning models if they can't be easily deployed to your hosting infrastructure. Fortunately, SageMaker allows deploying machine learning models to your current services and applications as easy as a single click.<\/p>\n<p>SageMaker allows for real-time data processing and prediction after installation. This has far-reaching consequences in a variety of areas, including finance and health. Businesses operating in the stock market, for example, may make real-time financial decisions about stock and make more attractive acquisitions by pinpointing the best time to buy.<\/p>\n<p>Incorporation with Amazon Comprehend, allows for natural language processing, transforming human speech into usable data to train better models, or provide a chatbot to customers through Amazon Lex.<\/p>\n<p><strong>In conclusion\u2026<\/strong><\/p>\n<p>Machine Learning is no longer a niche technological curiosity; it now plays a critical role in the decision-making processes of thousands of companies around the world. There has never been a better time to start your Machine Learning journey than now, with virtually unlimited frameworks and simple integration into the AWS system.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":15397939.0,
        "Answerer_created_time":1615788942376,
        "Answerer_last_access_time":1645167571500,
        "Answerer_age":null,
        "Answerer_location":"USA",
        "Answerer_reputation":129.0,
        "Answerer_views":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53154542",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1620131934800,
        "Question_original_content":"realtim data process compani onlin consum behavior analysi realtim predict data collect websit java script embed aw real time predict experi occur realtim data process problem compar aw exampl string variabl aw convert numer us real time predict aw automat look like experi real time data process predict",
        "Question_preprocessed_content":"realtim data process compani onlin consum behavior analysi realtim predict data collect websit aw real time predict experi occur realtim data process problem compar aw exampl string variabl aw convert numer us real time predict aw automat look like experi real time data process predict",
        "Question_gpt_summary_original":"The user is facing challenges with real-time data processing and prediction in AWS SageMaker compared to AWS ML. They are having difficulty converting string variables to numerics for real-time prediction in SageMaker. The user is seeking advice and experiences from others who have worked with real-time data processing and prediction in AWS SageMaker.",
        "Question_gpt_summary":"user face challeng real time data process predict compar aw have difficulti convert string variabl numer real time predict user seek advic experi work real time data process predict",
        "Answer_original_content":"robust machin learn servic aw manag major aspect machin learn implement includ data prepar model construct train fine tune deploy prepar us rang resourc simpl prepar data machin learn model come sourc varieti format simpl mark data includ video imag text automat process usabl data ground truth groundwork process merg data auto segment suit tool creat singl data label machin learn model aw conjunct data wrangler process reduc data prepar phase week month matter dai hour build studio notebook central relev machin learn model allow conveni share associ data choos varieti built open sourc algorithm start process data jumpstart build custom paramet machin learn model chosen model start process data automat offer simpl easi understand interfac track model progress perform train provid rang tool train model data prepar includ built debugg detect possibl error machin learn train job result save amazon bucket view aw servic includ aw quicksight deploy pointless strong machin learn model easili deploi host infrastructur fortun allow deploi machin learn model current servic applic easi singl click allow real time data process predict instal far reach consequ varieti area includ financ health busi oper stock market exampl real time financi decis stock attract acquisit pinpoint best time bui incorpor amazon comprehend allow natur languag process transform human speech usabl data train better model provid chatbot custom amazon lex conclus machin learn longer nich technolog curios plai critic role decis make process thousand compani world better time start machin learn journei virtual unlimit framework simpl integr aw",
        "Answer_preprocessed_content":"robust machin learn servic aw manag major aspect machin learn implement includ data prepar model construct train deploy prepar us rang resourc simpl prepar data machin learn model come sourc varieti format simpl mark data includ video imag text automat process usabl data ground truth groundwork process merg data suit tool creat singl data label machin learn model aw conjunct data wrangler process reduc data prepar phase week month matter dai hour build studio notebook central relev machin learn model allow conveni share associ data choos varieti algorithm start process data jumpstart build custom paramet machin learn model chosen model start process data automat offer simpl interfac track model progress perform train provid rang tool train model data prepar includ debugg detect possibl error machin learn train job result save amazon bucket view aw servic includ aw quicksight deploy pointless strong machin learn model easili deploi host infrastructur fortun allow deploi machin learn model current servic applic easi singl click allow data process predict instal consequ varieti area includ financ health busi oper stock market exampl financi decis stock attract acquisit pinpoint best time bui incorpor amazon comprehend allow natur languag process transform human speech usabl data train better model provid chatbot custom amazon lex conclus machin learn longer nich technolog curios plai critic role process thousand compani world better time start machin learn journei virtual unlimit framework simpl integr aw",
        "Answer_gpt_summary_original":"No solutions are provided for the specific challenge of converting string variables to numerics for real-time prediction in AWS SageMaker. The discussion provides an overview of the features and benefits of AWS SageMaker for machine learning implementation, including data preparation, model construction, training, and deployment. The discussion also highlights the importance of real-time data processing and prediction in various industries and the integration of natural language processing with Amazon Comprehend.",
        "Answer_gpt_summary":"solut provid specif challeng convert string variabl numer real time predict discuss provid overview featur benefit machin learn implement includ data prepar model construct train deploy discuss highlight import real time data process predict industri integr natur languag process amazon comprehend"
    },
    {
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_body":"I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs & returns the desired results. However, no files are written to the specified S3 location.\n\nEndpoint Configuration\n\nThe endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a ml.m4.xlarge instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried s3:\/\/<bucket-name> as well as s3:\/\/<bucket-name>\/<some-other-path>. With the \"Capture content type\" I tried leaving everything blank, setting text\/csv in \"CSV\/Text\" and application\/json in \"JSON\".\n\nEndpoint Invokation\n\nThe endpoint is invoked in a Lambda function with a client. Here's the call:\n\nsagemaker_body_source = {\n            \"segments\": segments,\n            \"language\": language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[\"predictions\"]\n\n\nInternally, the endpoint uses a Flask API with an \/invocation path that returns the result.\n\nLogs\n\nThe endpoint itself works fine and the Flask API is logging input and output:\n\nINFO:api:body: {'segments': [<strings...>], 'language': 'de'}\n\nINFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660135320930,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":52.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"So the issue seemed to be related to the IAM role. The default role (ModelEndpoint-Role) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKWPP4eXTTZe5qIUDJAXnsQ\/sagemaker-data-capture-does-not-write-files",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1660656368966,
        "Question_original_content":"data captur write file want enabl data captur specif endpoint far consol endpoint work fine log return desir result file written specifi locat endpoint configur endpoint base train job scikit learn classifi variant xlarg instanc type data captur enabl sampl percentag data captur storag locat tri captur content type tri leav blank set text csv csv text applic json json endpoint invok endpoint invok lambda function client bodi sourc segment segment languag languag payload json dump bodi sourc encod respons self client invok endpoint endpointnam endpoint bodi payload contenttyp applic json accept applic json result json load respons bodi read decod return result predict intern endpoint us flask api invoc path return result log endpoint work fine flask api log input output info api bodi segment languag info api output predict text text label label",
        "Question_preprocessed_content":"data captur write file want enabl data captur specif endpoint endpoint work fine log return desir result file written specifi locat endpoint configur endpoint base train job scikit learn classifi variant instanc type data captur enabl sampl percentag data captur storag locat tri captur content type tri leav blank set json endpoint invok endpoint invok lambda function client payload respons bodi payload result return result intern endpoint us flask api invoc path return result log endpoint work fine flask api log input output info api bodi info api output",
        "Question_gpt_summary_original":"The user is facing a challenge with Sagemaker Data Capture as no files are being written to the specified S3 location despite enabling data capture for a specific endpoint. The endpoint is based on a training job with a scikit learn classifier and has only one variant which is a ml.m4.xlarge instance type. The endpoint is invoked in a Lambda function with a client and internally uses a Flask API with an \/invocation path that returns the result. The endpoint works fine and logs input and output, but no files are being written to the specified S3 location.",
        "Question_gpt_summary":"user face challeng data captur file written specifi locat despit enabl data captur specif endpoint endpoint base train job scikit learn classifi variant xlarg instanc type endpoint invok lambda function client intern us flask api invoc path return result endpoint work fine log input output file written specifi locat",
        "Answer_original_content":"issu relat iam role default role modelendpoint role access write file work sdk us role studio receiv error messag",
        "Answer_preprocessed_content":"issu relat iam role default role access write file work sdk us role studio receiv error messag",
        "Answer_gpt_summary_original":"Solution: The issue was related to the IAM role. The default role (ModelEndpoint-Role) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. The solution is to create a new IAM role with the necessary permissions to write to the specified S3 location and assign it to the endpoint.",
        "Answer_gpt_summary":"solut issu relat iam role default role modelendpoint role access write file work sdk us role studio solut creat new iam role necessari permiss write specifi locat assign endpoint"
    },
    {
        "Question_title":"Azure-ML Deployment does NOT see AzureML Environment (wrong version number)",
        "Question_body":"<p>I've followed the documentation pretty well as outlined <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-custom-docker-image\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>I've setup my azure machine learning environment the following way:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n# Connect to the workspace\nws = Workspace.from_config()\n\nfrom azureml.core import Environment\nfrom azureml.core import ContainerRegistry\n\nmyenv = Environment(name = &quot;myenv&quot;)\n\nmyenv.inferencing_stack_version = &quot;latest&quot;  # This will install the inference specific apt packages.\n\n# Docker\nmyenv.docker.enabled = True\nmyenv.docker.base_image_registry.address = &quot;myazureregistry.azurecr.io&quot;\nmyenv.docker.base_image_registry.username = &quot;myusername&quot;\nmyenv.docker.base_image_registry.password = &quot;mypassword&quot;\nmyenv.docker.base_image = &quot;4fb3...&quot; \nmyenv.docker.arguments = None\n\n# Environment variable (I need python to look at folders \nmyenv.environment_variables = {&quot;PYTHONPATH&quot;:&quot;\/root&quot;}\n\n# python\nmyenv.python.user_managed_dependencies = True\nmyenv.python.interpreter_path = &quot;\/opt\/miniconda\/envs\/myenv\/bin\/python&quot; \n\nfrom azureml.core.conda_dependencies import CondaDependencies\nconda_dep = CondaDependencies()\nconda_dep.add_pip_package(&quot;azureml-defaults&quot;)\nmyenv.python.conda_dependencies=conda_dep\n\nmyenv.register(workspace=ws) # works!\n<\/code><\/pre>\n<p>I have a score.py file configured for inference (not relevant to the problem I'm having)...<\/p>\n<p>I then setup inference configuration<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import InferenceConfig\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=myenv)\n<\/code><\/pre>\n<p>I setup my compute cluster:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.compute import ComputeTarget, AksCompute\nfrom azureml.exceptions import ComputeTargetException\n\n# Choose a name for your cluster\naks_name = &quot;theclustername&quot; \n\n# Check to see if the cluster already exists\ntry:\n    aks_target = ComputeTarget(workspace=ws, name=aks_name)\n    print('Found existing compute target')\nexcept ComputeTargetException:\n    print('Creating a new compute target...')\n    prov_config = AksCompute.provisioning_configuration(vm_size=&quot;Standard_NC6_Promo&quot;)\n\n    aks_target = ComputeTarget.create(workspace=ws, name=aks_name, provisioning_configuration=prov_config)\n\n    aks_target.wait_for_completion(show_output=True)\n\nfrom azureml.core.webservice import AksWebservice\n\n# Example\ngpu_aks_config = AksWebservice.deploy_configuration(autoscale_enabled=False,\n                                                    num_replicas=3,\n                                                    cpu_cores=4,\n                                                    memory_gb=10)\n<\/code><\/pre>\n<p>Everything succeeds; then I try and deploy the model for inference:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.model import Model\n\nmodel = Model(ws, name=&quot;thenameofmymodel&quot;)\n\n# Name of the web service that is deployed\naks_service_name = 'tryingtodeply'\n\n# Deploy the model\naks_service = Model.deploy(ws,\n                           aks_service_name,\n                           models=[model],\n                           inference_config=inference_config,\n                           deployment_config=gpu_aks_config,\n                           deployment_target=aks_target,\n                           overwrite=True)\n\naks_service.wait_for_deployment(show_output=True)\nprint(aks_service.state)\n<\/code><\/pre>\n<p>And it fails saying that it can't find the environment. More specifically, my environment version is <strong>version 11<\/strong>, but it keeps trying to find an environment with a version number that is 1 higher (i.e., <strong>version 12<\/strong>) than the current environment:<\/p>\n<pre><code>FailedERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 0f03a025-3407-4dc1-9922-a53cc27267d4\nMore information can be found here: \nError:\n{\n  &quot;code&quot;: &quot;BadRequest&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;The request is invalid&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;EnvironmentDetailsFetchFailedUserError&quot;,\n      &quot;message&quot;: &quot;Failed to fetch details for Environment with Name: myenv Version: 12.&quot;\n    }\n  ]\n}\n\n<\/code><\/pre>\n<p>I have tried to manually edit the environment JSON to match the version that azureml is trying to fetch, but nothing works. Can anyone see anything wrong with this code?<\/p>\n<h1>Update<\/h1>\n<p>Changing the name of the environment (e.g., <code>my_inference_env<\/code>) and passing it to <code>InferenceConfig<\/code> seems to be on the right track. However, the error now changes to the following<\/p>\n<pre><code>Running..........\nFailed\nERROR - Service deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: f0dfc13b-6fb6-494b-91a7-de42b9384692\nMore information can be found here: https:\/\/some_long_http_address_that_leads_to_nothing\nError:\n{\n  &quot;code&quot;: &quot;DeploymentFailed&quot;,\n  &quot;statusCode&quot;: 404,\n  &quot;message&quot;: &quot;Deployment not found&quot;\n}\n<\/code><\/pre>\n<h1>Solution<\/h1>\n<p>The answer from Anders below is <strong>indeed correct<\/strong> regarding the use of azure ML environments. However, the last error I was getting was because I was setting the <em>container image<\/em> using the digest value (a sha) and NOT the image name and tag (e.g., <code>imagename:tag<\/code>). Note the line of code in the first block:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;4fb3...&quot; \n<\/code><\/pre>\n<p>I reference the digest value, but it should be changed to<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv.docker.base_image = &quot;imagename:tag&quot;\n<\/code><\/pre>\n<p>Once I made that change, the deployment succeeded! :)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_created_time":1597699827673,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1599771558392,
        "Question_score":3,
        "Question_view_count":1768.0,
        "Poster_id":4932636.0,
        "Poster_created_time":1432406490590,
        "Poster_last_access_time":1663530034716,
        "Poster_age":null,
        "Poster_location":"Milwaukee, WI",
        "Poster_reputation":381.0,
        "Poster_views":62.0,
        "Answer_body":"<p>One concept that took me a while to get was the bifurcation of registering and using an Azure ML <code>Environment<\/code>. If you have already registered your env, <code>myenv<\/code>, and none of the details of the your environment have changed, there is no need re-register it with <code>myenv.register()<\/code>. You can simply get the already register env using <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.environment?view=azure-ml-py#get-workspace--name--version-none-\" rel=\"nofollow noreferrer\"><code>Environment.get()<\/code><\/a> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>myenv = Environment.get(ws, name='myenv', version=11)\n<\/code><\/pre>\n<p>My recommendation would be to name your environment something new: like <code>&quot;model_scoring_env&quot;<\/code>. Register it once, then pass it to the <code>InferenceConfig<\/code>.<\/p>",
        "Answer_comment_count":9.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":3842610.0,
        "Answerer_created_time":1405457120427,
        "Answerer_last_access_time":1663947733100,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":3359.0,
        "Answerer_views":555.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63458904",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1597702121696,
        "Question_original_content":"azur deploy environ wrong version number follow document pretti outlin setup environ follow wai core import workspac connect workspac workspac config core import environ core import containerregistri myenv environ myenv myenv inferenc stack version latest instal infer specif apt packag docker myenv docker enabl true myenv docker base imag registri address myazureregistri azurecr myenv docker base imag registri usernam myusernam myenv docker base imag registri password mypassword myenv docker base imag myenv docker argument environ variabl need python look folder myenv environ variabl pythonpath root python myenv python user manag depend true myenv python interpret path opt miniconda env myenv bin python core conda depend import condadepend conda dep condadepend conda dep add pip packag default myenv python conda depend conda dep myenv regist workspac work score file configur infer relev problem have setup infer configur core model import inferenceconfig infer config inferenceconfig entri script score environ myenv setup comput cluster core comput import computetarget akscomput except import computetargetexcept choos cluster ak theclusternam check cluster exist try ak target computetarget workspac ak print exist comput target computetargetexcept print creat new comput target prov config akscomput provis configur size standard promo ak target computetarget creat workspac ak provis configur prov config ak target wait complet output true core webservic import akswebservic exampl gpu ak config akswebservic deploi configur autoscal enabl fals num replica cpu core memori succe try deploi model infer core model import model model model thenameofmymodel web servic deploi ak servic tryingtodepli deploi model ak servic model deploi ak servic model model infer config infer config deploy config gpu ak config deploy target ak target overwrit true ak servic wait deploy output true print ak servic state fail sai environ specif environ version version keep try environ version number higher version current environ failederror servic deploy poll reach non success termin state current servic state fail oper accd inform error code badrequest statuscod messag request invalid detail code environmentdetailsfetchfailedusererror messag fail fetch detail environ myenv version tri manual edit environ json match version try fetch work wrong code updat chang environ infer env pass inferenceconfig right track error chang follow run fail error servic deploy poll reach non success termin state current servic state fail oper fdfcb deb inform http long http address lead error code deploymentfail statuscod messag deploy solut answer ander correct us environ error get set contain imag digest valu sha imag tag imagenam tag note line code block myenv docker base imag refer digest valu chang myenv docker base imag imagenam tag chang deploy succeed",
        "Question_preprocessed_content":"deploy environ follow document pretti outlin setup environ follow wai file configur infer setup infer configur setup comput cluster succe try deploi model infer fail sai environ specif environ version version keep try environ version number higher current environ tri manual edit environ json match version try fetch work wrong code updat chang environ pass right track error chang follow solut answer ander correct us environ error get set contain imag digest valu imag tag note line code block refer digest valu chang chang deploy succeed",
        "Question_gpt_summary_original":"The user is encountering challenges with Azure-ML deployment as the deployment is unable to see the AzureML environment due to a wrong version number. The user has tried to manually edit the environment JSON to match the version that AzureML is trying to fetch, but it did not work. The user has also changed the name of the environment and passed it to InferenceConfig, but it resulted in a new error. The solution to the problem was to set the container image using the image name and tag instead of the digest value.",
        "Question_gpt_summary":"user encount challeng azur deploy deploy unabl environ wrong version number user tri manual edit environ json match version try fetch work user chang environ pass inferenceconfig result new error solut problem set contain imag imag tag instead digest valu",
        "Answer_original_content":"concept took bifurc regist environ regist env myenv detail environ chang need regist myenv regist simpli regist env environ like myenv environ myenv version recommend environ new like model score env regist pass inferenceconfig",
        "Answer_preprocessed_content":"concept took bifurc regist regist env detail environ chang need simpli regist env like recommend environ new like regist pass",
        "Answer_gpt_summary_original":"Solution: The solution to the problem was to set the container image using the image name and tag instead of the digest value. Additionally, it is recommended to name the environment something new, register it once, and then pass it to the InferenceConfig.",
        "Answer_gpt_summary":"solut solut problem set contain imag imag tag instead digest valu addition recommend environ new regist pass inferenceconfig"
    },
    {
        "Question_title":"Does Amazon SageMaker XGBoost support parallel training across multiple machines?",
        "Question_body":"I'd like to set up Amazon SageMaker XGBoost to train datasets on multiple machines. Is that possible? If so, how?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1583496984000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":51.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Yes, using Amazon SageMaker hosting with XGBoost allows you to train datasets on multiple machines.\n\nFor more information, see Docker registry paths and example code in the Amazon SageMaker developer guide.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUOKZq2V_RQaaFzQkapcWpsA\/does-amazon-sage-maker-xg-boost-support-parallel-training-across-multiple-machines",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1583654787000,
        "Question_original_content":"xgboost support parallel train multipl machin like set xgboost train dataset multipl machin possibl",
        "Question_preprocessed_content":"xgboost support parallel train multipl machin like set xgboost train dataset multipl machin possibl",
        "Question_gpt_summary_original":"The user is seeking information on whether Amazon SageMaker XGBoost supports parallel training across multiple machines and how to set it up if possible.",
        "Question_gpt_summary":"user seek inform xgboost support parallel train multipl machin set possibl",
        "Answer_original_content":"ye host xgboost allow train dataset multipl machin inform docker registri path exampl code develop guid",
        "Answer_preprocessed_content":"ye host xgboost allow train dataset multipl machin inform docker registri path exampl code develop guid",
        "Answer_gpt_summary_original":"Solution: Yes, Amazon SageMaker hosting with XGBoost supports parallel training across multiple machines. The Amazon SageMaker developer guide provides information on Docker registry paths and example code to set it up.",
        "Answer_gpt_summary":"solut ye host xgboost support parallel train multipl machin develop guid provid inform docker registri path exampl code set"
    },
    {
        "Question_title":"Code for Best Run and Model from previous experiment",
        "Question_body":"<p>For the best run and fitted model from a previously run experiment, Looking for the python code.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1590161357353,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":103.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Below is the code you can reuse\n<a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/diabetes_regression\/evaluate\/evaluate_model.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/microsoft\/MLOpsPython\/blob\/master\/diabetes_regression\/evaluate\/evaluate_model.py<\/a><\/p>\n\n<p>Assuming in each previous experiment run, a model was registered with a tag that contains a metric of interest (test_mae for example), below is the code to retrieve the version with lowest mae.<\/p>\n\n<pre><code>from azureml.core.model import Model\n\nmodel_name = \"YOUR_MODEL_NAME\"\nmodel_path = \"LOCAL_PATH\u201d\nmodel_version_list = [(model.version,float(model.tags[\"test_mae\"])) for model in Model.list(workspace = ws,name =model_name)]\nmodel_version_list.sort(key = lambda a: a[0])\nlowest_mae_version =model_version_list[0][0]\nprint(\"best version is {} with mae at {}\".format(lowest_mae_version,model_version_list[0][1]))\nmodel = Model(name = model_name,workspace = ws, version =lowest_mae_version)\nmodel.download(model_path, exist_ok=True)\n<\/code><\/pre>\n\n<p>when the model has not been registered,models in an automl run and  would like to get all the models and compare the results depending on featurization, method used, and metrics, also with other data sets. The models are all inside the workspace with the GUI you can see them and download them by hand. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":11297406.0,
        "Answerer_created_time":1554186784008,
        "Answerer_last_access_time":1662988357176,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":2175.0,
        "Answerer_views":434.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61958473",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1590393765270,
        "Question_original_content":"code best run model previou experi best run fit model previous run experi look python code",
        "Question_preprocessed_content":"code best run model previou experi best run fit model previous run experi look python code",
        "Question_gpt_summary_original":"The user is searching for the python code for the best run and fitted model from a previous experiment.",
        "Question_gpt_summary":"user search python code best run fit model previou experi",
        "Answer_original_content":"code reus http github com microsoft mlopspython blob master diabet regress evalu evalu model assum previou experi run model regist tag contain metric test mae exampl code retriev version lowest mae core model import model model model model path local path model version list model version float model tag test mae model model list workspac model model version list sort kei lambda lowest mae version model version list print best version mae format lowest mae version model version list model model model workspac version lowest mae version model download model path exist true model regist model automl run like model compar result depend featur method metric data set model insid workspac gui download hand",
        "Answer_preprocessed_content":"code reus assum previou experi run model regist tag contain metric code retriev version lowest mae model regist model automl run like model compar result depend featur method metric data set model insid workspac gui download hand",
        "Answer_gpt_summary_original":"Solution:\n- If a model was registered with a tag that contains a metric of interest, the code can be used to retrieve the version with the lowest metric value.\n- If the model has not been registered, the models in an automl run can be compared based on featurization, method used, and metrics, and can be downloaded manually from the workspace GUI.",
        "Answer_gpt_summary":"solut model regist tag contain metric code retriev version lowest metric valu model regist model automl run compar base featur method metric download manual workspac gui"
    },
    {
        "Question_title":"Returning nearest neighbors from SKLearn model deployed in AWS SageMaker",
        "Question_body":"<p>I built an unsupervised NearestNeighbors model in AWS Sagemaker, and deployed this to an endpoint. Now, I am trying to use the model endpoint to generate the k-nearest neighbors for a given input vector. <\/p>\n\n<p>However, I am getting the following error:<\/p>\n\n<pre><code>AttributeError                            Traceback (most recent call last)\n&lt;ipython-input-31-f595a603f928&gt; in &lt;module&gt;()\n     12 # print(predictor.predict(sample_vector))\n     13 \n---&gt; 14 distance, indice = pred.kneighbors(sample_vector, n_neighbors=11)\n\nAttributeError: 'SKLearnPredictor' object has no attribute 'kneighbors'\n<\/code><\/pre>\n\n<p>The SKLearn NearestNeighbors learner does not have a predict method. Trying to use the 'predict' method instead of '.kneighbors' therefore also yields an error:<\/p>\n\n<pre><code>ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"&lt;!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\"&gt;\n&lt;title&gt;500 Internal Server Error&lt;\/title&gt;\n&lt;h1&gt;Internal Server Error&lt;\/h1&gt;\n&lt;p&gt;The server encountered an internal error and was unable to complete your request. Either the server is overloaded or there is an error in the application.&lt;\/p&gt;\n\". See https:\/\/us-east-2.console.aws.amazon.com\/cloudwatch\/home?region=us-east-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/sagemaker-scikit-learn-2019-06-29-13-11-50-512 in account 820407560908 for more information.\n<\/code><\/pre>\n\n<p>Is there a way to call this endpoint within Sagemaker, or does the Sagemaker SKLearn SDK only allow for models with a 'predict' method?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1561818096170,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":694.0,
        "Poster_id":9016649.0,
        "Poster_created_time":1511812140112,
        "Poster_last_access_time":1654180635567,
        "Poster_age":null,
        "Poster_location":"Chicago, IL, USA",
        "Poster_reputation":169.0,
        "Poster_views":36.0,
        "Answer_body":"<p>At inference, 3 functions are used one after the other: <code>input_fn<\/code>, <code>predict_fn<\/code>, <code>output_fn<\/code>. They take default values, but you can override them to do desired custom actions. In your case, you can for example override the <code>predict_fn<\/code> to run the desired command. See more details here <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_sklearn.html#deploying-scikit-learn-models<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":5331834.0,
        "Answerer_created_time":1442180190107,
        "Answerer_last_access_time":1664048608876,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56818280",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1561838537720,
        "Question_original_content":"return nearest neighbor sklearn model deploi built unsupervis nearestneighbor model deploi endpoint try us model endpoint gener nearest neighbor given input vector get follow error attributeerror traceback recent print predictor predict sampl vector distanc indic pred kneighbor sampl vector neighbor attributeerror sklearnpredictor object attribut kneighbor sklearn nearestneighbor learner predict method try us predict method instead kneighbor yield error modelerror error occur modelerror call invokeendpoint oper receiv server error model messag intern server error intern server error server encount intern error unabl complet request server overload error applic http east consol aw amazon com cloudwatch home region east logeventview group aw endpoint scikit learn account inform wai endpoint sklearn sdk allow model predict method",
        "Question_preprocessed_content":"return nearest neighbor sklearn model deploi built unsupervis nearestneighbor model deploi endpoint try us model endpoint gener neighbor given input vector get follow error sklearn nearestneighbor learner predict method try us predict method instead yield error wai endpoint sklearn sdk allow model predict method",
        "Question_gpt_summary_original":"The user built an unsupervised NearestNeighbors model in AWS Sagemaker and deployed it to an endpoint. However, when trying to generate the k-nearest neighbors for a given input vector, the user encountered an error indicating that the SKLearn NearestNeighbors learner does not have a predict method. Attempting to use the predict method instead of '.kneighbors' also resulted in an error. The user is unsure if there is a way to call this endpoint within Sagemaker or if the Sagemaker SKLearn SDK only allows for models with a 'predict' method.",
        "Question_gpt_summary":"user built unsupervis nearestneighbor model deploi endpoint try gener nearest neighbor given input vector user encount error indic sklearn nearestneighbor learner predict method attempt us predict method instead kneighbor result error user unsur wai endpoint sklearn sdk allow model predict method",
        "Answer_original_content":"infer function input predict output default valu overrid desir custom action case exampl overrid predict run desir command detail http readthedoc stabl sklearn html deploi scikit learn model",
        "Answer_preprocessed_content":"infer function default valu overrid desir custom action case exampl overrid run desir command detail",
        "Answer_gpt_summary_original":"Solution: The user can override the predict_fn function to run the desired command. More details can be found in the Sagemaker documentation.",
        "Answer_gpt_summary":"solut user overrid predict function run desir command detail document"
    },
    {
        "Question_title":"aws textract human review flow, failed to load image",
        "Question_body":"Hi,\n\nI`m using aws textract to extract key-value pairs from an pdf. Because sometimes the accucary is low i use augmented AI (human review worflows) to involve a human worker. That works fine with png files, but when I use pdf files (which textract supports), I get an \"Failed to load image\". How do I get around this? I tried using a custom template, but can't find a way to insert the file type.\n\nBest regards,\n\nPaul",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1665415509779,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":48.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"The underlying challenge here is that, while modern browsers can natively render PDFs, they require different embedding methods for PDFs vs images. To my knowledge there's no built-in SageMaker Crowd HTML Element that's capable of handling both types interchangeably - and your experience with the pre-built UI seems to confirm this.\n\nDisplaying PDFs in A2I\/SMGT\n\nThis simple sample suggests to use an <iframe type=\"application\/pdf\"> to display PDFs via the browser's native renderer. You could try this approach... but as of ~March 2022, I found support was patchy because some browsers' default security policies didn't like loading a cross-origin iframe with interactive content.\n\nIf relying on the browser native renderer won't work for your users, you can use the open-source PDF.js renderer instead. Here is a more complex sample template that does that. PDF.js is powerful, but can be pretty tricky to get started with from my experience... Note that the basic process in this sample is:\n\nTag the <script>s and stylesheets for PDF.js in from a CDN\nInclude a PDF viewer structure in your HTML\nPass your A2I object URL in through JavaScript and set up your viewer there - including any interactivity you need\n(The second inline script tag there you can probably ignore: It's specific to what that data that template collects)\nScaling template complexity\n\nAlthough the situation has improved a lot in recent years, writing direct-to-browser inline JavaScript in HTML can be tricky due to browser diversity and developer tooling limitations. If you want to build more advanced, interactive task templates, you might want to explore using front-end frameworks like React\/Angular\/Vue within A2I\/Ground Truth.\n\nThe above-mentioned PDF.js template is actually a legacy that's since been replaced by this VueJS app in the sample that uses it. In that case, the switch was made because we wanted to customize the PDF viewer (rendering detection boxes over the document), and the complexity of the app justified setting up a proper toolchain. You can find discussion there about using frameworks in general and VueJS in particular with A2I, and could use the app as a starting point for building your own complex template in advance. Note if I was re-building that from scratch, I'd probably use much less liquid templating, and implement more within the JS framework itself as discussed here.\n\nYou can see the complex template being built\/deployed from (SageMaker) Python notebook here, and a screenshot of it in action here. This end-to-end sample is discussed further in an AWS ML blog post.\n\nHandling mixed PDF\/Image content\n\nIf you need your template to handle both PDFs and images, this will add extra complexity. Could your JavaScript infer from the object URL (filename) which category the input object falls into, and dynamically set up either an <img> tag or a PDF viewer? Could you fetch the object from JS and check the Content-Type response header? Might it be simpler to add the file type as an input to your A2I loop, and pass it in that way? (e.g. using conditional liquid template to either render an <img> or not?)\n\nDepending on what points in the flow you know the file type, there are multiple different ways you might tackle this. Ultimately though, you'll probably be switching between either generating an img or a PDF viewer: Whether those HTML elements are created by static Liquid templating or by dynamic JS.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUo48ev4bTTvO-GjsezfAmuQ\/aws-textract-human-review-flow-failed-to-load-image",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1665462672324,
        "Question_original_content":"aw textract human review flow fail load imag aw textract extract kei valu pair pdf accucari low us augment human review worflow involv human worker work fine png file us pdf file textract support fail load imag tri custom templat wai insert file type best regard paul",
        "Question_preprocessed_content":"aw textract human review flow fail load imag aw textract extract pair pdf accucari low us augment involv human worker work fine png file us pdf file fail load imag tri custom templat wai insert file type best regard paul",
        "Question_gpt_summary_original":"The user is facing a challenge with using AWS Textract to extract key-value pairs from PDF files. They are attempting to use augmented AI (human review workflows) to improve accuracy, but are encountering an error message (\"Failed to load image\") when attempting to use PDF files. The user has tried using a custom template but is unable to insert the file type.",
        "Question_gpt_summary":"user face challeng aw textract extract kei valu pair pdf file attempt us augment human review workflow improv accuraci encount error messag fail load imag attempt us pdf file user tri custom templat unabl insert file type",
        "Answer_original_content":"underli challeng modern browser nativ render pdf requir differ embed method pdf imag knowledg built crowd html element capabl handl type interchang experi pre built confirm displai pdf smgt simpl sampl suggest us displai pdf browser nativ render try approach march support patchi browser default secur polici like load cross origin ifram interact content reli browser nativ render won work user us open sourc pdf render instead complex sampl templat pdf power pretti tricki start experi note basic process sampl tag",
        "Answer_preprocessed_content":"underli challeng modern browser nativ render pdf requir differ embed method pdf imag knowledg crowd html element capabl handl type interchang experi confirm displai pdf simpl sampl suggest us displai pdf browser nativ render try march support patchi browser default secur polici like load ifram interact content reli browser nativ render won work user us render instead complex sampl templat power pretti tricki start note basic process sampl tag",
        "Answer_gpt_summary_original":"The discussion provides several solutions to the challenge of using AWS Textract to extract key-value pairs from PDF files. One solution is to use an <iframe type=\"application\/pdf\"> to display PDFs via the browser's native renderer. Another solution is to use the open-source PDF.js renderer instead. The discussion also suggests using front-end frameworks like React\/Angular\/Vue within A2I\/Ground Truth to build more advanced, interactive task templates. If handling mixed PDF\/Image content, the solution could be to infer from the object URL (filename) which category the input object falls into, and dynamically set up either an <img> tag or a PDF viewer. Alternatively, the file type can be added as an input to the A2I loop",
        "Answer_gpt_summary":"discuss provid solut challeng aw textract extract kei valu pair pdf file solut us displai pdf browser nativ render solut us open sourc pdf render instead discuss suggest end framework like react angular vue ground truth build advanc interact task templat handl mix pdf imag content solut infer object url filenam categori input object fall dynam set tag pdf viewer altern file type ad input loop"
    },
    {
        "Question_title":"Trouble in creating graphics with matplotlib in a Jupyter notebook",
        "Question_body":"<p>Following the pandas documentation for visualization (<a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html#visualization-hist\" rel=\"nofollow noreferrer\">https:\/\/pandas.pydata.org\/pandas-docs\/stable\/visualization.html#visualization-hist<\/a>) I am trying to create the following graphics:<\/p>\n\n<pre><code>import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\n## A data set in my AzureML workplace experiment \ndf = ds.to_dataframe()\nplt.figure(); \ndf.plot.hist(stacked=True, bins=20) \nplt.figure();df.boxplot()\n<\/code><\/pre>\n\n<p>However, the output is limited to <code>\"&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd12e15dc18&gt;\"<\/code> (for the histogram(=) and <code>&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd12e0ce828&gt;\"<\/code> (to the box plot), but no image appearing. Can anyone help me to identify what I'm missing out? Thanks!<\/p>\n\n<p>I'm using Python 3 in Jupyter Notebook in AzureML. <\/p>\n\n<p>The <code>df.describe()<\/code> method works properly (there is a dataFrame)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1516035090403,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1516035342392,
        "Question_score":0,
        "Question_view_count":378.0,
        "Poster_id":955883.0,
        "Poster_created_time":1316565559336,
        "Poster_last_access_time":1655585549088,
        "Poster_age":null,
        "Poster_location":"Brazil",
        "Poster_reputation":2563.0,
        "Poster_views":503.0,
        "Answer_body":"<p>Have you set the backend?<\/p>\n\n<pre><code>%matplotlib inline\n<\/code><\/pre>\n\n<p>Worth reading about what this does for a notebook here too\n<a href=\"https:\/\/stackoverflow.com\/questions\/43027980\/purpose-of-matplotlib-inline\/43028034\">Purpose of &quot;%matplotlib inline&quot;<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":8412492.0,
        "Answerer_created_time":1501772252047,
        "Answerer_last_access_time":1592821757947,
        "Answerer_age":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation":922.0,
        "Answerer_views":49.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48267427",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1516035353070,
        "Question_original_content":"troubl creat graphic matplotlib jupyt notebook follow panda document visual http panda pydata org panda doc stabl visual html visual hist try creat follow graphic import numpi import panda import matplotlib pyplot plt data set workplac experi datafram plt figur plot hist stack true bin plt figur boxplot output limit histogram box plot imag appear help identifi miss thank python jupyt notebook method work properli datafram",
        "Question_preprocessed_content":"troubl creat graphic matplotlib jupyt notebook follow panda document visual try creat follow graphic output limit imag appear help identifi miss thank python jupyt notebook method work properli",
        "Question_gpt_summary_original":"The user is having trouble creating graphics with matplotlib in a Jupyter notebook. They are following the pandas documentation for visualization but the output is limited to text and no image appears. They are using Python 3 in Jupyter Notebook in AzureML and the df.describe() method works properly.",
        "Question_gpt_summary":"user have troubl creat graphic matplotlib jupyt notebook follow panda document visual output limit text imag appear python jupyt notebook method work properli",
        "Answer_original_content":"set backend matplotlib inlin worth read notebook purpos matplotlib inlin",
        "Answer_preprocessed_content":"set backend worth read notebook purpos matplotlib inlin",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to set the backend using the following code: \n\n%matplotlib inline\n\nThis code will enable the user to create graphics with matplotlib in a Jupyter notebook.",
        "Answer_gpt_summary":"solut solut suggest discuss set backend follow code matplotlib inlin code enabl user creat graphic matplotlib jupyt notebook"
    },
    {
        "Question_title":"multiuser public jupyter notebook on AWS sagemaker",
        "Question_body":"<p>I know there is a good tutorial on how to create jupyter notebooks on AWS sagemaker \"the easy way\".<\/p>\n\n<p>Do you know if it is possible to allow 10 students to create jupyter-notebooks who do not have an AWS accounts, and also allow them to edit jupyter-notebooks?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1554137000223,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1554180914787,
        "Question_score":2,
        "Question_view_count":1259.0,
        "Poster_id":3385325.0,
        "Poster_created_time":1394050586807,
        "Poster_last_access_time":1664048888332,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":568.0,
        "Poster_views":67.0,
        "Answer_body":"<p>Enabling multiple users to leverage the same notebook (in this case, without authentication) will involve managing your Security Groups to enable open access. You can filter, allowing access for a known IP address range, if your students are accessing it from a classroom or campus, for example.<\/p>\n\n<p>Tips for this are available in <a href=\"https:\/\/stackoverflow.com\/questions\/42617692\/is-it-possible-to-grant-multiple-users-to-jupyter-notebook\">this answer<\/a> and this page from the documentation, diving into <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/\" rel=\"nofollow noreferrer\">network configurations for SageMaker hosted notebook instances<\/a>.<\/p>\n\n<p>As for enabling students to spin up their own notebooks, I'm not sure if it's possible to enable completely unauthenticated AWS-level resource provisioning -- however once you've spun up a single managed notebook instance yourself, students can create their own notebooks directly from the browser in Jupyter, once they've navigated to the publicly available IP. You may need to attach a new SageMaker IAM role that enables notebook creation (amongst other things, depending on the workload requirements). Depending on the computational needs (number, duration, and types of concurrent workloads), there will be different optimal setups of number of managed instances and instance type to prevent computational bottlenecking.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1554169700776,
        "Answer_score":1.0,
        "Answerer_id":10885720.0,
        "Answerer_created_time":1546969667040,
        "Answerer_last_access_time":1649085825632,
        "Answerer_age":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation":1689.0,
        "Answerer_views":170.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55459903",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1554143903380,
        "Question_original_content":"multius public jupyt notebook know good tutori creat jupyt notebook easi wai know possibl allow student creat jupyt notebook aw account allow edit jupyt notebook",
        "Question_preprocessed_content":"multius public jupyt notebook know good tutori creat jupyt notebook easi wai know possibl allow student creat aw account allow edit",
        "Question_gpt_summary_original":"The user is seeking information on whether it is possible to allow 10 students to create and edit Jupyter notebooks on AWS Sagemaker without requiring them to have AWS accounts.",
        "Question_gpt_summary":"user seek inform possibl allow student creat edit jupyt notebook requir aw account",
        "Answer_original_content":"enabl multipl user leverag notebook case authent involv manag secur group enabl open access filter allow access known address rang student access classroom campu exampl tip avail answer page document dive network configur host notebook instanc enabl student spin notebook sure possibl enabl complet unauthent aw level resourc provis spun singl manag notebook instanc student creat notebook directli browser jupyt navig publicli avail need attach new iam role enabl notebook creation thing depend workload requir depend comput need number durat type concurr workload differ optim setup number manag instanc instanc type prevent comput bottleneck",
        "Answer_preprocessed_content":"enabl multipl user leverag notebook involv manag secur group enabl open access filter allow access known address rang student access classroom campu exampl tip avail answer page document dive network configur host notebook instanc enabl student spin notebook sure possibl enabl complet unauthent resourc provis spun singl manag notebook instanc student creat notebook directli browser jupyt navig publicli avail need attach new iam role enabl notebook creation depend comput need differ optim setup number manag instanc instanc type prevent comput bottleneck",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include managing Security Groups to enable open access for a known IP address range, following tips available in the provided links, and allowing students to create their own notebooks directly from the browser in Jupyter once a single managed notebook instance has been spun up. It is not clear if it is possible to enable completely unauthenticated AWS-level resource provisioning.",
        "Answer_gpt_summary":"possibl solut mention discuss includ manag secur group enabl open access known address rang follow tip avail provid link allow student creat notebook directli browser jupyt singl manag notebook instanc spun clear possibl enabl complet unauthent aw level resourc provis"
    },
    {
        "Question_title":"how to save uncompressed outputs from a training job in using aws Sagemaker python SDK?",
        "Question_body":"<p>I'm trying to upload training job artifacts to S3 in a non-compressed manner.<\/p>\n<p>I am familiar with the output_dir one can provide to a sagemaker Estimator, then everything saved under \/opt\/ml\/output is uploaded compressed to the S3 output dir.<\/p>\n<p>I want to have the option to access a specific artifact without having to decompress the output every time. Is there a clean way to go about it? if not any workaround in mind?\nThe artifacts of my interest are small meta-data files .txt or .csv, while in my case the rest of the artifacts can be ~1GB so downloading and decompressing is quite excessive.<\/p>\n<p>any help would be appreciated<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1608711307120,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":313.0,
        "Poster_id":11487739.0,
        "Poster_created_time":1557646363768,
        "Poster_last_access_time":1663926361952,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":51.0,
        "Poster_views":14.0,
        "Answer_body":"<p>I ended up using the checkpoint path that is by default being synced with the specified S3 path in an uncompressed manner.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":11487739.0,
        "Answerer_created_time":1557646363768,
        "Answerer_last_access_time":1663926361952,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":51.0,
        "Answerer_views":14.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65421005",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1612085605603,
        "Question_original_content":"save uncompress output train job python sdk try upload train job artifact non compress manner familiar output dir provid estim save opt output upload compress output dir want option access specif artifact have decompress output time clean wai workaround mind artifact small meta data file txt csv case rest artifact download decompress excess help appreci",
        "Question_preprocessed_content":"save uncompress output train job python sdk try upload train job artifact manner familiar provid estim save upload compress output dir want option access specif artifact have decompress output time clean wai workaround mind artifact small file txt csv case rest artifact download decompress excess help appreci",
        "Question_gpt_summary_original":"The user is facing a challenge in uploading training job artifacts to S3 in a non-compressed manner using AWS Sagemaker Python SDK. The compressed artifacts are saved under \/opt\/ml\/output and the user wants to access specific artifacts without having to decompress the output every time. The user is looking for a clean way to do this or a workaround as the artifacts of interest are small meta-data files while the rest of the artifacts can be around 1GB, making downloading and decompressing excessive.",
        "Question_gpt_summary":"user face challeng upload train job artifact non compress manner python sdk compress artifact save opt output user want access specif artifact have decompress output time user look clean wai workaround artifact small meta data file rest artifact make download decompress excess",
        "Answer_original_content":"end checkpoint path default sync specifi path uncompress manner",
        "Answer_preprocessed_content":"end checkpoint path default sync specifi path uncompress manner",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use the checkpoint path that is by default being synced with the specified S3 path in an uncompressed manner.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us checkpoint path default sync specifi path uncompress manner"
    },
    {
        "Question_title":"Why is Crowd HTML breaking this image?",
        "Question_body":"<p>I'm using Crowd HTML Elements to perform bounding box annotation, but when I attempt to load some of my images, I get this error in the dev tools console:<\/p>\n<pre><code>crowd-html-elements.js:1 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements.js:1\nerror (async)\ne @ crowd-html-elements.js:1\ne @ crowd-html-elements.js:1\n.\/src\/crowd-html-elements-loader.ts @ crowd-html-elements.js:1\ns @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\n(anonymous) @ crowd-html-elements.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 window.onError received an event without an error:  {event: ErrorEvent}\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:6282\nerror (async)\ne @ crowd-html-elements-without-ce-polyfill.js:6282\ne @ crowd-html-elements-without-ce-polyfill.js:6282\n.\/src\/index.ts @ crowd-html-elements-without-ce-polyfill.js:6282\nr @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:1\ncrowd-html-elements-without-ce-polyfill.js:6282 Uncaught Error: Unexpected image dimensions during normalization\n    at Function.normalizeHeight (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Function.normalizeDimensions (crowd-html-elements-without-ce-polyfill.js:6282)\n    at new a (crowd-html-elements-without-ce-polyfill.js:6282)\n    at ie.handleTargetImageLoaded (crowd-html-elements-without-ce-polyfill.js:6282)\n    at Image.r.onload (crowd-html-elements-without-ce-polyfill.js:6282)\nnormalizeHeight @ crowd-html-elements-without-ce-polyfill.js:6282\nnormalizeDimensions @ crowd-html-elements-without-ce-polyfill.js:6282\na @ crowd-html-elements-without-ce-polyfill.js:6282\nhandleTargetImageLoaded @ crowd-html-elements-without-ce-polyfill.js:6282\nr.onload @ crowd-html-elements-without-ce-polyfill.js:6282\nload (async)\nsetBackgroundImage @ crowd-html-elements-without-ce-polyfill.js:6282\nrenderImageSrcChange @ crowd-html-elements-without-ce-polyfill.js:6282\nshouldComponentUpdate @ crowd-html-elements-without-ce-polyfill.js:6282\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\nS @ crowd-html-elements-without-ce-polyfill.js:6278\ne.reactMount @ crowd-html-elements-without-ce-polyfill.js:3\ne.updateRegion @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\n(anonymous) @ crowd-html-elements-without-ce-polyfill.js:3\ne.reactBatchUpdate @ crowd-html-elements-without-ce-polyfill.js:3\ni @ crowd-html-elements-without-ce-polyfill.js:3\nf.componentDidUpdate @ crowd-html-elements-without-ce-polyfill.js:3\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nq @ crowd-html-elements-without-ce-polyfill.js:6278\nB @ crowd-html-elements-without-ce-polyfill.js:6278\nF @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nE @ crowd-html-elements-without-ce-polyfill.js:6278\nN @ crowd-html-elements-without-ce-polyfill.js:6278\nT @ crowd-html-elements-without-ce-polyfill.js:6278\nG @ crowd-html-elements-without-ce-polyfill.js:6278\nw @ crowd-html-elements-without-ce-polyfill.js:6278\n_renderReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\n_updateReactComponent @ crowd-html-elements-without-ce-polyfill.js:6282\nY @ crowd-html-elements-without-ce-polyfill.js:5984\nC @ crowd-html-elements-without-ce-polyfill.js:5984\nk @ crowd-html-elements-without-ce-polyfill.js:5984\n_propertiesChanged @ crowd-html-elements-without-ce-polyfill.js:5984\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5954\n_flushProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_invalidateProperties @ crowd-html-elements-without-ce-polyfill.js:5984\n_setProperty @ crowd-html-elements-without-ce-polyfill.js:5984\nObject.defineProperty.set @ crowd-html-elements-without-ce-polyfill.js:5954\n(anonymous) @ labeling.html:199\nasync function (async)\n(anonymous) @ labeling.html:198\nPromise.then (async)\n(anonymous) @ labeling.html:196\n<\/code><\/pre>\n<p>The <strong>Unexpected image dimensions during normalization<\/strong> portion seems like the issue, but I've found nothing with regard to troubleshooting.  Can someone explain what expected image dimensions are and why some are failing?<\/p>\n<p>Here's a snippet of the code that's throwing the error.<\/p>\n<pre><code>            static normalizeHeight(e) {\n                if (e.height === e.naturalHeight)\n                    return e.height;\n                if (e.height === e.naturalWidth)\n                    return e.height;\n                if (Math.abs(e.height - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                if (Math.abs(e.height - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n            }\n            static normalizeWidth(e) {\n                if (e.width === e.naturalWidth)\n                    return e.width;\n                if (e.width === e.naturalHeight)\n                    return e.width;\n                if (Math.abs(e.width - e.naturalWidth) &lt; 2)\n                    return e.naturalWidth;\n                if (Math.abs(e.width - e.naturalHeight) &lt; 2)\n                    return e.naturalHeight;\n                throw new Error(&quot;Unexpected image dimensions during normalization&quot;)\n<\/code><\/pre>\n<p>Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_created_time":1601930941820,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1602868269100,
        "Question_score":1,
        "Question_view_count":168.0,
        "Poster_id":7369248.0,
        "Poster_created_time":1483444144907,
        "Poster_last_access_time":1643982821672,
        "Poster_age":null,
        "Poster_location":"Hoth",
        "Poster_reputation":312.0,
        "Poster_views":65.0,
        "Answer_body":"<p>The issue turned out to be related to the css styling that was being applied to the canvas portion of my site that was loading the labeling tools.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":7369248.0,
        "Answerer_created_time":1483444144907,
        "Answerer_last_access_time":1643982821672,
        "Answerer_age":null,
        "Answerer_location":"Hoth",
        "Answerer_reputation":312.0,
        "Answerer_views":65.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64215998",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1603367517400,
        "Question_original_content":"crowd html break imag crowd html element perform bound box annot attempt load imag error dev tool consol crowd html element window onerror receiv event error event errorev anonym crowd html element error async crowd html element crowd html element src crowd html element loader crowd html element crowd html element anonym crowd html element anonym crowd html element crowd html element polyfil window onerror receiv event error event errorev anonym crowd html element polyfil error async crowd html element polyfil crowd html element polyfil src index crowd html element polyfil crowd html element polyfil anonym crowd html element polyfil anonym crowd html element polyfil crowd html element polyfil uncaught error unexpect imag dimens normal function normalizeheight crowd html element polyfil function normalizedimens crowd html element polyfil new crowd html element polyfil handletargetimageload crowd html element polyfil imag onload crowd html element polyfil normalizeheight crowd html element polyfil normalizedimens crowd html element polyfil crowd html element polyfil handletargetimageload crowd html element polyfil onload crowd html element polyfil load async setbackgroundimag crowd html element polyfil renderimagesrcchang crowd html element polyfil shouldcomponentupd crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil reactmount crowd html element polyfil updateregion crowd html element polyfil anonym crowd html element polyfil anonym crowd html element polyfil anonym crowd html element polyfil reactbatchupd crowd html element polyfil crowd html element polyfil componentdidupd crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil renderreactcompon crowd html element polyfil updatereactcompon crowd html element polyfil crowd html element polyfil crowd html element polyfil crowd html element polyfil propertieschang crowd html element polyfil flushproperti crowd html element polyfil flushproperti crowd html element polyfil invalidateproperti crowd html element polyfil setproperti crowd html element polyfil object defineproperti set crowd html element polyfil anonym label html async function async anonym label html promis async anonym label html unexpect imag dimens normal portion like issu regard troubleshoot explain expect imag dimens fail snippet code throw error static normalizeheight height naturalheight return height height naturalwidth return height math ab height naturalheight return naturalheight math ab height naturalwidth return naturalwidth throw new error unexpect imag dimens normal static normalizewidth width naturalwidth return width width naturalheight return width math ab width naturalwidth return naturalwidth math ab width naturalheight return naturalheight throw new error unexpect imag dimens normal thank",
        "Question_preprocessed_content":"crowd html break imag crowd html element perform bound box annot attempt load imag error dev tool consol unexpect imag dimens normal portion like issu regard troubleshoot explain expect imag dimens fail snippet code throw error thank",
        "Question_gpt_summary_original":"The user is encountering an error while using Crowd HTML Elements for bounding box annotation. The error message \"Unexpected image dimensions during normalization\" is displayed in the console. The user is seeking an explanation of expected image dimensions and why some images are failing. The provided code snippet shows the function that is throwing the error.",
        "Question_gpt_summary":"user encount error crowd html element bound box annot error messag unexpect imag dimens normal displai consol user seek explan expect imag dimens imag fail provid code snippet show function throw error",
        "Answer_original_content":"issu turn relat css style appli canva portion site load label tool",
        "Answer_preprocessed_content":"issu turn relat css style appli canva portion site load label tool",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Python click incorrectly parses arguments when called in Vertex AI Pipeline",
        "Question_body":"<p>I'm trying to run a simple Ada-boosted Decision Tree regressor on GCP Vertex AI. To parse hyperparams and other arguments I use Click for Python, a very simple CLI library. Here's the setup for my task function:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@click.command()\n@click.argument(&quot;input_path&quot;, type=str)\n@click.option(&quot;--output-path&quot;, type=str, envvar='AIP_MODEL_DIR')\n@click.option('--gcloud', is_flag=True, help='Run as if in Google Cloud Vertex AI Pipeline')\n@click.option('--grid', is_flag=True, help='Perform a grid search instead of a single run. Ignored with --gcloud')\n@click.option(&quot;--max_depth&quot;, type=int, default=4, help='Max depth of decision tree', show_default=True)\n@click.option(&quot;--n_estimators&quot;, type=int, default=50, help='Number of AdaBoost boosts', show_default=True)\ndef click_main(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators)\n\n\ndef train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    print(input_path, output_path, gcloud)\n    logger = logging.getLogger(__name__)\n    logger.info(&quot;training models from processed data&quot;)\n    ...\n<\/code><\/pre>\n<p>When I run it locally like below, Click correctly grabs the params both from console and environment and proceeds with model training (<code>AIP_MODEL_DIR<\/code> is <code>gs:\/\/(BUCKET_NAME)\/models<\/code>)<\/p>\n<pre><code>\u276f python3 -m src.models.train_model gs:\/\/(BUCKET_NAME)\/data\/processed --gcloud\n\ngs:\/\/(BUCKET_NAME)\/data\/processed gs:\/\/(BUCKET_NAME)\/models True\n\n<\/code><\/pre>\n<p>However, when I put this code on the Vertex AI Pipeline, it throws an error, namely<\/p>\n<pre><code>FileNotFoundError: b\/(BUCKET_NAME)\/o\/data%2Fprocessed%20%20--gcloud%2Fprocessed_features.csv\n<\/code><\/pre>\n<p>As it is clearly seen, Click grabs both the parameter and the <code>--gcloud<\/code> option and assigns it to <code>input_path<\/code>. The print statement before that confirms it, both by having one too many spaces and <code>--gcloud<\/code> being parsed as false.<\/p>\n<pre><code>gs:\/\/(BUCKET_NAME)\/data\/processed  --gcloud gs:\/\/(BUCKET_NAME)\/models\/1\/model\/ False\n<\/code><\/pre>\n<p>Has anyone here encountered this issue or have any idea how to solve it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1641797901177,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1641896853596,
        "Question_score":1,
        "Question_view_count":129.0,
        "Poster_id":12770693.0,
        "Poster_created_time":1579801831103,
        "Poster_last_access_time":1663207732260,
        "Poster_age":null,
        "Poster_location":"Tempe, AZ, USA",
        "Poster_reputation":71.0,
        "Poster_views":30.0,
        "Answer_body":"<p>I think is due the nature of <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/arguments\/?highlight=arguments\" rel=\"nofollow noreferrer\">arguments<\/a> and <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/options\/?highlight=options\" rel=\"nofollow noreferrer\">options<\/a>, you are mixing arguments and options although is not implicit stated in the documentation but argument will eat up the options that follow. If nargs is not allocated it will default to 1 considering everything after it follows as string which it looks like this is the case.<\/p>\n<blockquote>\n<p>nargs \u2013 the number of arguments to match. If not 1 the return value is a tuple instead of single value. The default for nargs is 1 (except if the type is a tuple, then it\u2019s the arity of the tuple).<\/p>\n<\/blockquote>\n<p>I think you should first use options followed by the argument as display on the <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/documentation\/?highlight=arguments\" rel=\"nofollow noreferrer\">documentation page<\/a>. Other approach is to group it under a command as show on this <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/commands\/\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":14551567.0,
        "Answerer_created_time":1604093818187,
        "Answerer_last_access_time":1664028741440,
        "Answerer_age":null,
        "Answerer_location":"Krakow, Poland",
        "Answerer_reputation":1200.0,
        "Answerer_views":263.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70648776",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1641810988040,
        "Question_original_content":"python click incorrectli pars argument call pipelin try run simpl ada boost decis tree regressor gcp pars hyperparam argument us click python simpl cli librari setup task function click command click argument input path type str click option output path type str envvar aip model dir click option gcloud flag true help run googl cloud pipelin click option grid flag true help perform grid search instead singl run ignor gcloud click option max depth type int default help max depth decis tree default true click option estim type int default help number adaboost boost default true def click main input path output path gcloud grid max depth estim train model input path output path gcloud grid max depth estim def train model input path output path gcloud grid max depth estim print input path output path gcloud logger log getlogg logger info train model process data run local like click correctli grab param consol environ proce model train aip model dir bucket model python src model train model bucket data process gcloud bucket data process bucket model true code pipelin throw error filenotfounderror bucket data fprocess gcloud fprocess featur csv clearli seen click grab paramet gcloud option assign input path print statement confirm have space gcloud pars fals bucket data process gcloud bucket model model fals encount issu idea solv",
        "Question_preprocessed_content":"python click incorrectli pars argument call pipelin try run simpl decis tree regressor gcp pars hyperparam argument us click python simpl cli librari setup task function run local like click correctli grab param consol environ proce model train code pipelin throw error clearli seen click grab paramet option assign print statement confirm have space pars fals encount issu idea solv",
        "Question_gpt_summary_original":"The user is encountering an issue with Python Click library incorrectly parsing arguments when called in Vertex AI Pipeline. Click is grabbing both the parameter and the --gcloud option and assigning it to input_path, causing an error. The issue is not present when running the code locally. The user is seeking help to solve this issue.",
        "Question_gpt_summary":"user encount issu python click librari incorrectli pars argument call pipelin click grab paramet gcloud option assign input path caus error issu present run code local user seek help solv issu",
        "Answer_original_content":"think natur argument option mix argument option implicit state document argument eat option follow narg alloc default consid follow string look like case narg number argument match return valu tupl instead singl valu default narg type tupl ariti tupl think us option follow argument displai document page approach group command link",
        "Answer_preprocessed_content":"think natur argument option mix argument option implicit state document argument eat option follow narg alloc default consid follow string look like case narg number argument match return valu tupl instead singl valu default narg think us option follow argument displai document page approach group command link",
        "Answer_gpt_summary_original":"Solutions provided:\n- Use options followed by the argument as displayed on the documentation page.\n- Group the options and arguments under a command.",
        "Answer_gpt_summary":"solut provid us option follow argument displai document page group option argument command"
    },
    {
        "Question_title":"How to write Azure machine learning batch scoring results to data lake?",
        "Question_body":"<p>I'm trying to write the output of batch scoring into datalake:<\/p>\n<pre><code>    parallel_step_name = &quot;batchscoring-&quot; + datetime.now().strftime(&quot;%Y%m%d%H%M&quot;)\n    \n    output_dir = PipelineData(name=&quot;scores&quot;, \n                              datastore=def_ADL_store,\n                              output_mode=&quot;upload&quot;,\n                              output_path_on_compute=&quot;path in data lake&quot;)\n\nparallel_run_config = ParallelRunConfig(\n    environment=curated_environment,\n    entry_script=&quot;use_model.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    output_action=&quot;append_row&quot;,\n    mini_batch_size=&quot;20&quot;,\n    error_threshold=1,\n    compute_target=compute_target,\n    process_count_per_node=2,\n    node_count=2\n)\n    \n    batch_score_step = ParallelRunStep(\n        name=parallel_step_name,\n        inputs=[test_data.as_named_input(&quot;test_data&quot;)],\n        output=output_dir,\n        parallel_run_config=parallel_run_config,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<p>However I meet the error: &quot;code&quot;: &quot;UserError&quot;,\n&quot;message&quot;: &quot;User program failed with Exception: Missing argument --output or its value is empty.&quot;<\/p>\n<p>How can I write results of batch score to data lake?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1596780750297,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":277.0,
        "Poster_id":9125948.0,
        "Poster_created_time":1513841518107,
        "Poster_last_access_time":1663924369323,
        "Poster_age":null,
        "Poster_location":"China",
        "Poster_reputation":71.0,
        "Poster_views":11.0,
        "Answer_body":"<p>I don\u2019t think ADLS is supported for <code>PipelineData<\/code>. My suggestion is to use the workspace\u2019s default blob store for the <code>PipelineData<\/code>, then use a <code>DataTransferStep<\/code> for after the <code>ParallelRunStep<\/code> is completed.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":3842610.0,
        "Answerer_created_time":1405457120427,
        "Answerer_last_access_time":1663947733100,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":3359.0,
        "Answerer_views":555.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63296185",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1596781453976,
        "Question_original_content":"write batch score result data lake try write output batch score datalak parallel step batchscor datetim strftime output dir pipelinedata score datastor def adl store output mode upload output path comput path data lake parallel run config parallelrunconfig environ curat environ entri script us model sourc directori output action append row mini batch size error threshold comput target comput target process count node node count batch score step parallelrunstep parallel step input test data name input test data output output dir parallel run config parallel run config allow reus fals meet error code usererror messag user program fail except miss argument output valu write result batch score data lake",
        "Question_preprocessed_content":"write batch score result data lake try write output batch score datalak meet error code usererror messag user program fail except miss argument valu write result batch score data lake",
        "Question_gpt_summary_original":"The user is encountering an error while trying to write the output of batch scoring into a data lake. The error message indicates that the user is missing an argument or its value is empty. The user is seeking guidance on how to write the results of batch scoring to the data lake.",
        "Question_gpt_summary":"user encount error try write output batch score data lake error messag indic user miss argument valu user seek guidanc write result batch score data lake",
        "Answer_original_content":"dont think adl support pipelinedata suggest us workspac default blob store pipelinedata us datatransferstep parallelrunstep complet",
        "Answer_preprocessed_content":"dont think adl support suggest us workspac default blob store us complet",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use the workspace's default blob store for the PipelineData and then use a DataTransferStep after the ParallelRunStep is completed. It is suggested that ADLS is not supported for PipelineData.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us workspac default blob store pipelinedata us datatransferstep parallelrunstep complet suggest adl support pipelinedata"
    },
    {
        "Question_title":"What is the best way to deploy my machine learning model using GPUs, specifically as a web based API?",
        "Question_body":"I am trying to find the best way to run my machine learning models on GPUs for inference as an http request. Do Azure functions support GPUs? if not, what are other options I can look into?\n\nnote: I also want to use packaged models, not necessarily ones of my own creation (such as easyOCR for python)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1630916125883,
        "Question_favorite_count":14.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi,\n\nIf you need GPU support on ML inference the only supported option is the Azure Kubernetes Service as stated in this documentation\n\nFor guidance on deploying an ML model to AKS, please refer to this documenation on deploying to AKS",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/541074\/what-is-a-the-best-way-to-deploy-my-machine-learni.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1630921359623,
        "Question_original_content":"best wai deploi machin learn model gpu specif web base api try best wai run machin learn model gpu infer http request azur function support gpu option look note want us packag model necessarili on creation easyocr python",
        "Question_preprocessed_content":"best wai deploi machin learn model gpu specif web base api try best wai run machin learn model gpu infer http request azur function support gpu option look note want us packag model necessarili on creation",
        "Question_gpt_summary_original":"The user is facing challenges in finding the best way to deploy their machine learning models on GPUs for inference as an http request. They are specifically looking for options to use packaged models and are unsure if Azure functions support GPUs.",
        "Question_gpt_summary":"user face challeng find best wai deploi machin learn model gpu infer http request specif look option us packag model unsur azur function support gpu",
        "Answer_original_content":"need gpu support infer support option azur kubernet servic state document guidanc deploi model ak refer documen deploi ak",
        "Answer_preprocessed_content":"need gpu support infer support option azur kubernet servic state document guidanc deploi model ak refer documen deploi ak",
        "Answer_gpt_summary_original":"Solution: The only supported option for GPU support on ML inference is the Azure Kubernetes Service (AKS). The user can refer to the documentation on deploying an ML model to AKS for guidance. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut support option gpu support infer azur kubernet servic ak user refer document deploi model ak guidanc solut mention discuss"
    },
    {
        "Question_title":"SageMaker Data Wrangler UI Features",
        "Question_body":"The SageMaker Data Wrangler UI in SageMaker Studio doesn't seem to support all the features that the API does. When will the UI support:\n\nLoading all s3 objects under a prefix? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_csv.html#awswrangler.s3.read_csv\nLoading JSON objects in addition to CSV and Parquet files? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_json.html#awswrangler.s3.read_json",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1641943985816,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":61.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"As mentioned by Tulio Alberto in comments, Amazon SageMaker Data Wrangler (the graphical data preparation feature inside Amazon SageMaker) is separate from AWS Data Wrangler (an open-source data prep utility published by AWS Labs): The two tools are based on different technologies and don't necessarily aim for full feature parity - they just happen to share similar names.\n\nTo my knowledge there's no committed timeline we can share at the moment for when these particular features will make it to SageMaker Data Wrangler, but I think as feature requests they make sense and the reasoning for both is pretty clear: I'm aware that both have been discussed to some extent internally already, and I'd personally like to see them launch too!\n\nThanks for sharing the feedback, and apologies for the naming confusion!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcsIt78jnSTW8Ta9__kUm-w\/sage-maker-data-wrangler-ui-features",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1642021146280,
        "Question_original_content":"data wrangler featur data wrangler studio support featur api support load object prefix http aw data wrangler readthedoc stabl stub awswrangl read csv html awswrangl read csv load json object addit csv parquet file http aw data wrangler readthedoc stabl stub awswrangl read json html awswrangl read json",
        "Question_preprocessed_content":"data wrangler featur data wrangler studio support featur api support load object prefix load json object addit csv parquet file",
        "Question_gpt_summary_original":"The user is facing challenges with the SageMaker Data Wrangler UI as it does not support all the features that the API does, specifically loading all s3 objects under a prefix and loading JSON objects in addition to CSV and Parquet files. The user is seeking information on when these features will be supported in the UI.",
        "Question_gpt_summary":"user face challeng data wrangler support featur api specif load object prefix load json object addit csv parquet file user seek inform featur support",
        "Answer_original_content":"mention tulio alberto comment data wrangler graphic data prepar featur insid separ aw data wrangler open sourc data prep util publish aw lab tool base differ technolog necessarili aim featur pariti happen share similar name knowledg commit timelin share moment particular featur data wrangler think featur request sens reason pretti clear awar discuss extent intern person like launch thank share feedback apolog name confus",
        "Answer_preprocessed_content":"mention tulio alberto comment data wrangler separ aw data wrangler tool base differ technolog necessarili aim featur pariti happen share similar name knowledg commit timelin share moment particular featur data wrangler think featur request sens reason pretti clear awar discuss extent intern person like launch thank share feedback apolog name confus",
        "Answer_gpt_summary_original":"Solutions provided: There are no committed timelines for when the specific features of loading all s3 objects under a prefix and loading JSON objects will be supported in the SageMaker Data Wrangler UI. However, the features have been discussed internally and are considered reasonable feature requests. The SageMaker Data Wrangler and AWS Data Wrangler are separate tools based on different technologies and do not necessarily aim for full feature parity.",
        "Answer_gpt_summary":"solut provid commit timelin specif featur load object prefix load json object support data wrangler featur discuss intern consid reason featur request data wrangler aw data wrangler separ tool base differ technolog necessarili aim featur pariti"
    },
    {
        "Question_title":"Invoking an endpoint in AWS with a multidimensional array",
        "Question_body":"<p>I have deployed a Tensorflow-Model in SageMaker Studio following this tutorial:\n<a href=\"https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/de\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/<\/a>\nThe Model needs a Multidimensional Array as input. Invoking it from the Notebook itself is working:<\/p>\n<pre><code>import numpy as np\nimport json\ndata = np.load(&quot;testValues.npy&quot;)\npred=predictor.predict(data)\n<\/code><\/pre>\n<p>But I wasnt able to invoke it from a boto 3 client using this code:<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n \nclient = boto3.client('runtime.sagemaker')\ndatain = np.load(&quot;testValues.npy&quot;)\ndata=datain.tolist();\nresponse = client.invoke_endpoint(EndpointName=endpoint_name, Body=json.dumps(data))\nresponse_body = response['Body']\nprint(response_body.read())\n<\/code><\/pre>\n<p>This throws the Error:<\/p>\n<pre><code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (415) from model with message &quot;{&quot;error&quot;: &quot;Unsupported Media Type: Unknown&quot;}&quot;.\n<\/code><\/pre>\n<p>I guess the reason is the json Media Type but i have no clue how to get it back in shape.\nI tried this:<a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/644<\/a> but it doesnt seem to change anything<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1605617435417,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":550.0,
        "Poster_id":10674521.0,
        "Poster_created_time":1542628542703,
        "Poster_last_access_time":1643066729968,
        "Poster_age":null,
        "Poster_location":"Germany",
        "Poster_reputation":11.0,
        "Poster_views":0.0,
        "Answer_body":"<p>This fixed it for me:\nThe Content Type was missing.<\/p>\n<pre><code>import json\nimport boto3\nimport numpy as np\nimport io\n\nclient = boto3.client('runtime.sagemaker',aws_access_key_id=..., aws_secret_access_key=...,region_name=...)\nendpoint_name = '...'\n\ndata = np.load(&quot;testValues.npy&quot;)\n\n\npayload = json.dumps(data.tolist())\nresponse = client.invoke_endpoint(EndpointName=endpoint_name,\n                                  ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nres = result['predictions']\nprint(&quot;test&quot;)\n\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":10674521.0,
        "Answerer_created_time":1542628542703,
        "Answerer_last_access_time":1643066729968,
        "Answerer_age":null,
        "Answerer_location":"Germany",
        "Answerer_reputation":11.0,
        "Answerer_views":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64875623",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1605773068536,
        "Question_original_content":"invok endpoint aw multidimension arrai deploi tensorflow model studio follow tutori http aw amazon com blog machin learn deploi train kera tensorflow model amazon model need multidimension arrai input invok notebook work import numpi import json data load testvalu npy pred predictor predict data wasnt abl invok boto client code import json import boto import numpi import client boto client runtim datain load testvalu npy data datain tolist respons client invok endpoint endpointnam endpoint bodi json dump data respons bodi respons bodi print respons bodi read throw error error occur modelerror call invokeendpoint oper receiv client error model messag error unsupport media type unknown guess reason json media type clue shape tri http github com aw amazon exampl issu doesnt chang",
        "Question_preprocessed_content":"invok endpoint aw multidimension arrai deploi studio follow tutori model need multidimension arrai input invok notebook work wasnt abl invok boto client code throw error guess reason json media type clue shape tri doesnt chang",
        "Question_gpt_summary_original":"The user has encountered a challenge while invoking an endpoint in AWS with a multidimensional array. The user was able to invoke the model from the Notebook but received an error when trying to invoke it from a boto3 client. The error message suggests an unsupported media type, and the user suspects it is due to the JSON media type but is unsure how to fix it. The user tried a solution from a GitHub issue but it did not resolve the problem.",
        "Question_gpt_summary":"user encount challeng invok endpoint aw multidimension arrai user abl invok model notebook receiv error try invok boto client error messag suggest unsupport media type user suspect json media type unsur fix user tri solut github issu resolv problem",
        "Answer_original_content":"fix content type miss import json import boto import numpi import client boto client runtim aw access kei aw secret access kei region endpoint data load testvalu npy payload json dump data tolist respons client invok endpoint endpointnam endpoint contenttyp applic json bodi payload result json load respons bodi read decod re result predict print test",
        "Answer_preprocessed_content":"fix content type miss",
        "Answer_gpt_summary_original":"Solution: The user was able to fix the issue by adding the Content Type to the code. The solution involved importing the necessary libraries, converting the data to a JSON format, and invoking the endpoint with the correct Content Type.",
        "Answer_gpt_summary":"solut user abl fix issu ad content type code solut involv import necessari librari convert data json format invok endpoint correct content type"
    },
    {
        "Question_title":"Import error while Executing AWS Predictive Maintenance Using Machine Learning Sample",
        "Question_body":"<p>We are trying to execute and check what kind of output is provided by Predictive Maintenance Using Machine Learning on AWS sample data. We are referring <a href=\"https:\/\/aws.amazon.com\/solutions\/predictive-maintenance-using-machine-learning\/\" rel=\"nofollow noreferrer\">Predictive Maintenance Using Machine Learning<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/solutions\/latest\/predictive-maintenance-using-machine-learning\/welcome.html\" rel=\"nofollow noreferrer\">AWS Guide<\/a> to launch the sample template provided by the AWS. The template is executed properly and we can see the resources in account. Whenever we run the sagemaker notebook for the given example we are getting the error in CloudWatch logs as follows<\/p>\n\n<pre><code>ImportError: cannot import name 'replace_file' on line from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file.\n<\/code><\/pre>\n\n<p>This is the stage where the invoke the training job. We have tried following options to resolve the issue.<\/p>\n\n<ul>\n<li>Upgrading the mxnet module<\/li>\n<li>Upgrading the tensorflow module<\/li>\n<\/ul>\n\n<p>But no success.<\/p>\n\n<p>Thanks in advance.<\/p>\n\n<p>Error Traceback is as follows<\/p>\n\n<pre><code>  File \"\/usr\/lib\/python3.5\/runpy.py\", line 184, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"\/usr\/lib\/python3.5\/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/ml\/code\/sagemaker_predictive_maintenance_entry_point.py\", line 10, in &lt;module&gt;\n    import gluonnlp\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/__init__.py\", line 25, in &lt;module&gt;\n    from . import data\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/__init__.py\", line 23, in &lt;module&gt;\n    from . import (batchify, candidate_sampler, conll, corpora, dataloader,\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/gluonnlp\/data\/question_answering.py\", line 31, in &lt;module&gt;\n    from mxnet.gluon.utils import download, check_sha1, _get_repo_file_url, replace_file\n    ImportError: cannot import name 'replace_file'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1587652058893,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":398.0,
        "Poster_id":6826576.0,
        "Poster_created_time":1473770138816,
        "Poster_last_access_time":1662976446232,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":23.0,
        "Poster_views":3.0,
        "Answer_body":"<p>A fix for this issue is being deployed to the official solution. In the meantime, you can make the changes described <a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/pull\/7\/files\" rel=\"nofollow noreferrer\">here<\/a> in your SageMaker environment by following the instructions below:<\/p>\n\n<p>1) In the notebook, please change the <code>framework_version<\/code> to <code>1.6.0<\/code>.<\/p>\n\n<pre><code>MXNet(entry_point='sagemaker_predictive_maintenance_entry_point.py',\n          source_dir='sagemaker_predictive_maintenance_entry_point',\n          py_version='py3',\n          role=role, \n          train_instance_count=1, \n          train_instance_type=train_instance_type,\n          output_path=output_location,\n          hyperparameters={'num-datasets' : len(train_df),\n                           'num-gpus': 1,\n                           'epochs': 500,\n                           'optimizer': 'adam',\n                           'batch-size':1,\n                           'log-interval': 100},\n         input_mode='File',\n         train_max_run=7200,\n         framework_version='1.6.0')  &lt;- Change this to 1.6.0.\n<\/code><\/pre>\n\n<p>2) This will likely fix things, but just to be sure you don't have any stale packages, change the <code>requirements.txt<\/code> file as well.<\/p>\n\n<p>You'll need to open up a terminal in SageMaker.\n<a href=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0Vn6l.png\" alt=\"enter image description here\"><\/a>\nimage taken from <a href=\"https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439\" rel=\"nofollow noreferrer\">https:\/\/medium.com\/swlh\/jupyter-notebook-on-amazon-sagemaker-getting-started-55489f500439<\/a><\/p>\n\n<p>and run<\/p>\n\n<pre><code>cd SageMaker\/sagemaker_predictive_maintenance_entry_point\/\nsudo vim requirements.txt  # (or sudo nano requirements.txt)\n<\/code><\/pre>\n\n<p>Change the contents to:<\/p>\n\n<pre><code>gluonnlp==0.9.1\npandas==0.22\n<\/code><\/pre>\n\n<p>Save it, and then run the example again.<\/p>\n\n<p>Feel free to comment on the issue as well:\n<a href=\"https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/predictive-maintenance-using-machine-learning\/issues\/6<\/a> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":8274165.0,
        "Answerer_created_time":1499498135632,
        "Answerer_last_access_time":1663784192956,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":675.0,
        "Answerer_views":26.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61389632",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1588063849632,
        "Question_original_content":"import error execut aw predict mainten machin learn sampl try execut check kind output provid predict mainten machin learn aw sampl data refer predict mainten machin learn aw guid launch sampl templat provid aw templat execut properli resourc account run notebook given exampl get error cloudwatch log follow importerror import replac file line mxnet gluon util import download check sha repo file url replac file stage invok train job tri follow option resolv issu upgrad mxnet modul upgrad tensorflow modul success thank advanc error traceback follow file usr lib python runpi line run modul main main mod spec file usr lib python runpi line run code exec code run global file opt code predict mainten entri point line import gluonnlp file usr local lib python dist packag gluonnlp init line import data file usr local lib python dist packag gluonnlp data init line import batchifi candid sampler conll corpora dataload file usr local lib python dist packag gluonnlp data question answer line mxnet gluon util import download check sha repo file url replac file importerror import replac file",
        "Question_preprocessed_content":"import error execut aw predict mainten machin learn sampl try execut check kind output provid predict mainten machin learn aw sampl data refer predict mainten machin learn aw guid launch sampl templat provid aw templat execut properli resourc account run notebook given exampl get error cloudwatch log follow stage invok train job tri follow option resolv issu upgrad mxnet modul upgrad tensorflow modul success thank advanc error traceback follow",
        "Question_gpt_summary_original":"The user is encountering an import error while executing AWS Predictive Maintenance Using Machine Learning sample data. The error occurs when the sagemaker notebook is run and the error traceback indicates that the mxnet.gluon.utils module cannot import the 'replace_file' function. The user has attempted to resolve the issue by upgrading the mxnet and tensorflow modules, but without success.",
        "Question_gpt_summary":"user encount import error execut aw predict mainten machin learn sampl data error occur notebook run error traceback indic mxnet gluon util modul import replac file function user attempt resolv issu upgrad mxnet tensorflow modul success",
        "Answer_original_content":"fix issu deploi offici solut meantim chang describ environ follow instruct notebook chang framework version mxnet entri point predict mainten entri point sourc dir predict mainten entri point version role role train instanc count train instanc type train instanc type output path output locat hyperparamet num dataset len train num gpu epoch optim adam batch size log interv input mode file train max run framework version chang like fix thing sure stale packag chang requir txt file need open termin imag taken http medium com swlh jupyt notebook amazon get start run predict mainten entri point sudo vim requir txt sudo nano requir txt chang content gluonnlp panda save run exampl feel free comment issu http github com awslab predict mainten machin learn issu",
        "Answer_preprocessed_content":"fix issu deploi offici solut meantim chang describ environ follow instruct notebook chang like fix thing sure stale packag chang file need open termin imag taken run chang content save run exampl feel free comment issu",
        "Answer_gpt_summary_original":"The fix for the import error is being deployed to the official solution. In the meantime, the user can make the changes described in the discussion by changing the framework version to 1.6.0 and updating the requirements.txt file. The user can follow the instructions provided in the discussion to make these changes. No other solutions are mentioned.",
        "Answer_gpt_summary":"fix import error deploi offici solut meantim user chang describ discuss chang framework version updat requir txt file user follow instruct provid discuss chang solut mention"
    },
    {
        "Question_title":"How to make the inputs and model have the same shape (RLlib Ray Sagemaker reinforcement learning)",
        "Question_body":"<p>I have a mismatch in shapes between inputs and the model of my reinforcement learning project.<\/p>\n\n<p>I have been closely following the AWS examples, specifically the cartpole example. However I have built my own custom environment. What I am struggling to understand is how to change my environment so that it is able to work with the prebuilt Ray RLEstimator.<\/p>\n\n<p>Here is the code for the environment:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\n\nimport numpy as np\n\n# from float_space import FloatSpace\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        self.observation_space = Box(np.array(0.0),np.array(1000))\n#         self.observation_space = FloatSpace(price)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n        next_state = self.price\n#         next_state = self.observation_space.sample()\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n\n#         self.observation_space.value = np.random.rand()\n#         return self.observation_space.sample()\n        self.price = np.random.rand()\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        self.price += PRICE_ADJUSTMENT[action]\n\n    def _get_reward(self,price):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    env = gym.make('ArrivalSim-v0')\n\n    env.reset()\n    for _ in range(20):\n        test = env.action_space.sample()\n        print(test)\n        print(env.observation_space)\n        env.step(test)  # take a random action\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n\n<\/code><\/pre>\n\n<p>Here is the training script<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nfrom ray.tune.registry import register_env\nfrom gym.envs.registration import register\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\n\n\nclass MyLauncher(SageMakerRayLauncher):\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n\n            }\n          }\n        }\n\nif __name__ == \"__main__\":\n    MyLauncher().train_main()\n<\/code><\/pre>\n\n<p>Here is the code I run in Jupyter:<\/p>\n\n<pre><code>metric_definitions = RLEstimator.default_metric_definitions(RLToolkit.RAY)\nenvironment = env = {\n    'SAGEMAKER_REQUIREMENTS': 'requirements.txt', # path relative to `source_dir` below.\n}\n\nestimator = RLEstimator(entry_point=\"train.py\",\n                        source_dir='.',\n                        toolkit=RLToolkit.RAY,\n                        toolkit_version='0.6.5',\n                        framework=RLFramework.TENSORFLOW,\n                        dependencies=[\"sagemaker_rl\"],\n#                         image_name='price-response-ray-cpu',\n                        role=role,\n#                         train_instance_type=\"ml.c5.2xlarge\",\n                        train_instance_type='local',\n                        train_instance_count=1,\n#                         output_path=s3_output_path,\n#                         base_job_name=job_name_prefix,\n                        metric_definitions=metric_definitions\n#                         hyperparameters={\n                          # Attention scientists!  You can override any Ray algorithm parameter here:\n                          #\"rl.training.config.horizon\": 5000,\n                          #\"rl.training.config.num_sgd_iter\": 10,\n                        #}\n                    )\n\nestimator.fit(wait=True)\njob_name = estimator.latest_training_job.job_name\nprint(\"Training job: %s\" % job_name)\n<\/code><\/pre>\n\n<p>The error message I have been receiving has been the following:<\/p>\n\n<pre><code>algo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.1\/4.1 GB\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | == Status ==\nalgo-1-dxwxx_1  | Using FIFO scheduling algorithm.\nalgo-1-dxwxx_1  | Resources requested: 2\/3 CPUs, 0\/0 GPUs\nalgo-1-dxwxx_1  | Memory usage on this node: 1.4\/4.1 GB\nalgo-1-dxwxx_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-dxwxx_1  | Number of trials: 1 ({'RUNNING': 1})\nalgo-1-dxwxx_1  | RUNNING trials:\nalgo-1-dxwxx_1  |  - PPO_ArrivalSim-v0_0:   RUNNING\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,030  WARNING ppo.py:172 -- FYI: By default, the value function will not share layers with the policy model ('vf_share_layers': False).\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,063   ERROR trial_runner.py:460 -- Error processing event.\nalgo-1-dxwxx_1  | Traceback (most recent call last):\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trial_runner.py\", line 409, in _process_trial\nalgo-1-dxwxx_1  |     result = self.trial_executor.fetch_result(trial)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/ray_trial_executor.py\", line 314, in fetch_result\nalgo-1-dxwxx_1  |     result = ray.get(trial_future[0])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/worker.py\", line 2316, in get\nalgo-1-dxwxx_1  |     raise value\nalgo-1-dxwxx_1  | ray.exceptions.RayTaskError: ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 83, in __init__\nalgo-1-dxwxx_1  |     restored, num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 135, in _build_layers_v2\nalgo-1-dxwxx_1  |     raise NotImplementedError\nalgo-1-dxwxx_1  | NotImplementedError\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | During handling of the above exception, another exception occurred:\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | ray_worker (pid=72, host=b9b15d495b68)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 276, in __init__\nalgo-1-dxwxx_1  |     Trainable.__init__(self, config, logger_creator)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/tune\/trainable.py\", line 88, in __init__\nalgo-1-dxwxx_1  |     self._setup(copy.deepcopy(self.config))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 373, in _setup\nalgo-1-dxwxx_1  |     self._init()\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo.py\", line 77, in _init\nalgo-1-dxwxx_1  |     self.env_creator, self._policy_graph)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 506, in make_local_evaluator\nalgo-1-dxwxx_1  |     extra_config or {}))\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/agent.py\", line 714, in _make_evaluator\nalgo-1-dxwxx_1  |     async_remote_worker_envs=config[\"async_remote_worker_envs\"])\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 288, in __init__\nalgo-1-dxwxx_1  |     self._build_policy_map(policy_dict, policy_config)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/evaluation\/policy_evaluator.py\", line 661, in _build_policy_map\nalgo-1-dxwxx_1  |     policy_map[name] = cls(obs_space, act_space, merged_conf)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/agents\/ppo\/ppo_policy_graph.py\", line 176, in __init__\nalgo-1-dxwxx_1  |     seq_lens=existing_seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 215, in get_model\nalgo-1-dxwxx_1  |     seq_lens)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/catalog.py\", line 255, in _get_model\nalgo-1-dxwxx_1  |     num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/model.py\", line 86, in __init__\nalgo-1-dxwxx_1  |     input_dict[\"obs\"], num_outputs, options)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/ray\/rllib\/models\/fcnet.py\", line 37, in _build_layers\nalgo-1-dxwxx_1  |     scope=label)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/framework\/python\/ops\/arg_scope.py\", line 182, in func_with_args\nalgo-1-dxwxx_1  |     return func(*args, **current_args)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/contrib\/layers\/python\/layers\/layers.py\", line 1854, in fully_connected\nalgo-1-dxwxx_1  |     outputs = layer.apply(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 817, in apply\nalgo-1-dxwxx_1  |     return self.__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/layers\/base.py\", line 374, in __call__\nalgo-1-dxwxx_1  |     outputs = super(Layer, self).__call__(inputs, *args, **kwargs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 730, in __call__\nalgo-1-dxwxx_1  |     self._assert_input_compatibility(inputs)\nalgo-1-dxwxx_1  |   File \"\/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/keras\/engine\/base_layer.py\", line 1493, in _assert_input_compatibility\nalgo-1-dxwxx_1  |     str(x.shape.as_list()))\nalgo-1-dxwxx_1  | ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]\nalgo-1-dxwxx_1  | \nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,064   INFO ray_trial_executor.py:178 -- Destroying actor for trial PPO_ArrivalSim-v0_0. If your trainable is slow to initialize, consider setting reuse_actors=True to reduce actor creation overheads.\nalgo-1-dxwxx_1  | 2019-08-30 09:35:13,076   INFO trial_runner.py:497 -- Attempting to recover trial state from last checkpoint.\nalgo-1-dxwxx_1  | (pid=72) 2019-08-30 09:35:13,041  INFO policy_evaluator.py:278 -- Creating policy evaluation worker 0 on CPU (please ignore any CUDA init errors)\n<\/code><\/pre>\n\n<p>I am not sure how to change the input the environment gives to the model or the models setup itself. It seems the documentations are quite obscure. I have a hunch that problem lies with the observation and action spaces<\/p>\n\n<p>Here is the reference to the original aws project example:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/reinforcement_learning\/rl_roboschool_ray<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1567158570927,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1567160893503,
        "Question_score":2,
        "Question_view_count":1492.0,
        "Poster_id":11889909.0,
        "Poster_created_time":1565097950652,
        "Poster_last_access_time":1663855024663,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":95.0,
        "Poster_views":55.0,
        "Answer_body":"<p><strong>Possible reason:<\/strong><\/p>\n\n<p>The error message:<\/p>\n\n<p><code>ValueError: Input 0 of layer default\/fc1 is incompatible with the layer: : expected min_ndim=2, found ndim=1. Full shape received: [None]<\/code><\/p>\n\n<p>Your original environment obs space is <code>self.observation_space = Box(np.array(0.0),np.array(1000))<\/code>.<\/p>\n\n<p>Displaying the shape of your environment obs space gives:<\/p>\n\n<p><code>print(Box(np.array(0.0), np.array(1000), dtype=np.float32).shape)<\/code> = <code>()<\/code><\/p>\n\n<p>This could be indicated by <code>Full shape received: [None]<\/code> in the error message.<\/p>\n\n<p>If you pass the shape <code>(1,1)<\/code> into <code>np.zeros<\/code>, you get the expected  <code>min_ndim=2<\/code>:<\/p>\n\n<p><code>x = np.zeros((1, 1))\nprint(x)\n[[0.]]\nprint(x.ndim)\n2<\/code><\/p>\n\n<p><strong>Suggested solution:<\/strong><\/p>\n\n<p>I assume that you want your environment obs space to range from 0.0 to 1000.0 as indicated by the <code>self.price = np.random.rand()<\/code> in your <code>reset<\/code> function.<\/p>\n\n<p>Try using the following for your environment obs space:<\/p>\n\n<p><code>self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)<\/code><\/p>\n\n<p>I hope that by setting the <code>Box<\/code> with an explicit <code>shape<\/code> helps.<\/p>\n\n<p><strike>\n<strong>EDIT (20190903):<\/strong><\/p>\n\n<p>I have modified your training script. This modification includes new imports, custom model class, model registration &amp; addition of registered custom model to config. For readability, only sections added are shown below. The entire modified training script is available in this <a href=\"https:\/\/gist.github.com\/ChuaCheowHuan\/ddf70654bd928d70e5415c947d4d43f3\" rel=\"nofollow noreferrer\">gist<\/a>. Please run with the proposed obs space as describe above.<\/p>\n\n<p>New additional imports:<\/p>\n\n<pre><code># new imports\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport numpy as np\nfrom ray.rllib.models import ModelCatalog\nfrom ray.rllib.models.tf.tf_modelv2 import TFModelV2\nfrom ray.rllib.models.tf.fcnet_v2 import FullyConnectedNetwork\n\nfrom ray.rllib.utils import try_import_tf\nfrom ray.tune import grid_search\n\ntf = try_import_tf()\n# end new imports\n<\/code><\/pre>\n\n<p>Custom model class:<\/p>\n\n<pre><code># Custom model class (fcnet)\nclass CustomModel(TFModelV2):\n    \"\"\"Example of a custom model that just delegates to a fc-net.\"\"\"\n\n    def __init__(self, obs_space, action_space, num_outputs, model_config,\n                 name):\n        super(CustomModel, self).__init__(obs_space, action_space, num_outputs,\n                                          model_config, name)\n        self.model = FullyConnectedNetwork(obs_space, action_space,\n                                           num_outputs, model_config, name)\n        self.register_variables(self.model.variables())\n\n    def forward(self, input_dict, state, seq_lens):\n        return self.model.forward(input_dict, state, seq_lens)\n\n    def value_function(self):\n        return self.model.value_function()\n<\/code><\/pre>\n\n<p>Registered &amp; add custom model:<\/p>\n\n<pre><code>    def get_experiment_config(self):\n\n\n        # Register custom model\n        ModelCatalog.register_custom_model(\"my_model\", CustomModel)\n\n\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"episode_reward_mean\": 5000,\n            },\n            \"config\": {\n\n\n              \"model\": {\"custom_model\": \"my_model\"}, # Add registered custom model\n\n\n              \"gamma\": 0.995,\n              \"kl_coeff\": 1.0,\n              \"num_sgd_iter\": 10,\n              \"lr\": 0.0001,\n              \"sgd_minibatch_size\": 32768,\n              \"train_batch_size\": 320000,\n              \"monitor\": False,  # Record videos.\n              \"model\": {\n                \"free_log_std\": False\n              },\n              \"use_gae\": False,\n              \"num_workers\": (self.num_cpus-1),\n              \"num_gpus\": self.num_gpus,\n              \"batch_mode\": \"complete_episodes\"\n            }\n          }\n        }\n<\/code><\/pre>\n\n<p><\/strike><\/p>\n\n<p><strong>EDIT 2 (20190910):<\/strong><\/p>\n\n<p>To show that it works, truncated output from Sagemaker (Jupyter notebook instance):<\/p>\n\n<pre><code>.\n.\n.\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 10.103020961393266 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.439261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.43926178] 9.663759180463124 False {}\nalgo-1-y2ayw_1  | price b = 0.439261780930142\nalgo-1-y2ayw_1  | price a = 0.189261780930142\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [0.18926178] 5.67785342790426 False {}\nalgo-1-y2ayw_1  | price b = 0.189261780930142\nalgo-1-y2ayw_1  | price a = -0.06073821906985799\nalgo-1-y2ayw_1  | (self.price).shape = (1,)\nalgo-1-y2ayw_1  | [-0.06073822] 0 True {}\nalgo-1-y2ayw_1  | Result for PPO_ArrivalSim-v0_0:\nalgo-1-y2ayw_1  |   date: 2019-09-10_11-51-13\nalgo-1-y2ayw_1  |   done: true\nalgo-1-y2ayw_1  |   episode_len_mean: 126.72727272727273\nalgo-1-y2ayw_1  |   episode_reward_max: 15772.677709596366\nalgo-1-y2ayw_1  |   episode_reward_mean: 2964.4609668691965\nalgo-1-y2ayw_1  |   episode_reward_min: 0.0\nalgo-1-y2ayw_1  |   episodes: 5\nalgo-1-y2ayw_1  |   experiment_id: 5d3b9f2988854a0db164a2e5e9a7550f\nalgo-1-y2ayw_1  |   hostname: 2dae585dcc65\nalgo-1-y2ayw_1  |   info:\nalgo-1-y2ayw_1  |     cur_lr: 4.999999873689376e-05\nalgo-1-y2ayw_1  |     entropy: 1.0670874118804932\nalgo-1-y2ayw_1  |     grad_time_ms: 1195.066\nalgo-1-y2ayw_1  |     kl: 3.391784191131592\nalgo-1-y2ayw_1  |     load_time_ms: 44.725\nalgo-1-y2ayw_1  |     num_steps_sampled: 463\nalgo-1-y2ayw_1  |     num_steps_trained: 463\nalgo-1-y2ayw_1  |     policy_loss: -0.05383850634098053\nalgo-1-y2ayw_1  |     sample_time_ms: 621.282\nalgo-1-y2ayw_1  |     total_loss: 2194493.5\nalgo-1-y2ayw_1  |     update_time_ms: 145.352\nalgo-1-y2ayw_1  |     vf_explained_var: -5.519390106201172e-05\nalgo-1-y2ayw_1  |     vf_loss: 2194492.5\nalgo-1-y2ayw_1  |   iterations_since_restore: 2\nalgo-1-y2ayw_1  |   node_ip: 172.18.0.2\nalgo-1-y2ayw_1  |   pid: 77\nalgo-1-y2ayw_1  |   policy_reward_mean: {}\nalgo-1-y2ayw_1  |   time_since_restore: 4.55129861831665\nalgo-1-y2ayw_1  |   time_this_iter_s: 1.3484764099121094\nalgo-1-y2ayw_1  |   time_total_s: 4.55129861831665\nalgo-1-y2ayw_1  |   timestamp: 1568116273\nalgo-1-y2ayw_1  |   timesteps_since_restore: 463\nalgo-1-y2ayw_1  |   timesteps_this_iter: 234\nalgo-1-y2ayw_1  |   timesteps_total: 463\nalgo-1-y2ayw_1  |   training_iteration: 2\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000781a7b5b94a203683f8f789e593abbb1.\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000d3507bc6b41ee1c9fc36292eeae69557.\nalgo-1-y2ayw_1  | == Status ==\nalgo-1-y2ayw_1  | Using FIFO scheduling algorithm.\nalgo-1-y2ayw_1  | Resources requested: 0\/3 CPUs, 0\/0 GPUs\nalgo-1-y2ayw_1  | Result logdir: \/opt\/ml\/output\/intermediate\/training\nalgo-1-y2ayw_1  | TERMINATED trials:\nalgo-1-y2ayw_1  |  - PPO_ArrivalSim-v0_0:   TERMINATED [pid=77], 4 s, 2 iter, 463 ts, 2.96e+03 rew\nalgo-1-y2ayw_1  |\nalgo-1-y2ayw_1  | Saved model configuration.\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.extra_data as \/opt\/ml\/model\/checkpoint.extra_data\nalgo-1-y2ayw_1  | Saved the checkpoint file \/opt\/ml\/output\/intermediate\/training\/PPO_ArrivalSim-v0_0_2019-09-10_11-50-53vd32vlux\/checkpoint-2.tune_metadata as \/opt\/ml\/model\/checkpoint.tune_metadata\nalgo-1-y2ayw_1  | Created LogSyncer for \/root\/ray_results\/PPO_ArrivalSim-v0_2019-09-10_11-51-13xdn_5i34 -&gt; None\nalgo-1-y2ayw_1  | 2019-09-10 11:51:13.941718: I tensorflow\/core\/common_runtime\/process_util.cc:71] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | LocalMultiGPUOptimizer devices ['\/cpu:0']\nalgo-1-y2ayw_1  | reset -&gt; (self.price).shape =  (1,)\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to save.\nalgo-1-y2ayw_1  | No assets to save.\nalgo-1-y2ayw_1  | INFO:tensorflow:No assets to write.\nalgo-1-y2ayw_1  | No assets to write.\nalgo-1-y2ayw_1  | INFO:tensorflow:SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | SavedModel written to: \/opt\/ml\/model\/1\/saved_model.pb\nalgo-1-y2ayw_1  | Saved TensorFlow serving model!\nalgo-1-y2ayw_1  | A worker died or was killed while executing task 00000000f352d985b807ca399460941fe2264899.\n\nalgo-1-y2ayw_1  | 2019-09-10 11:51:20,075 sagemaker-containers INFO\n\n Reporting training SUCCESS\n\ntmpwwb4b358_algo-1-y2ayw_1 exited with code 0\n\nAborting on container exit...\nFailed to delete: \/tmp\/tmpwwb4b358\/algo-1-y2ayw Please remove it manually.\n\n===== Job Complete =====\n<\/code><\/pre>\n\n<p>This time I make edits in all 3 files. Your environment, training script &amp; the Jupyter notebook but it turns out that there isn't a need to define custom models for your custom environment. However, that remains viable. And you're right, the main cause of the issue is still in the obs space.<\/p>\n\n<p>I set <code>self.price<\/code> to be a 1D numpy array to make it talk better with Ray RLlib. The creation of the custom environment in the training script was done in a simpler way as shown below. As for the notebook, I used version 0.5.3 instead of 0.6.5 for toolkit_version &amp; the training is done in local mode (in the docker container on the Sagemaker Jupyter notebook instance, still on AWS) with CPU only. However, it will also work with any ML instance (e.g ml.m4.xlarge) with GPU.<\/p>\n\n<p>The entire package along with all dependencies is in <a href=\"https:\/\/github.com\/ChuaCheowHuan\/sagemaker_Ray_RLlib_custom_env\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The edited env:<\/p>\n\n<pre><code># new\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n# end new\n\n\nfrom enum import Enum\nimport math\n\nimport gym\nfrom gym import error, spaces, utils, wrappers\nfrom gym.utils import seeding\nfrom gym.envs.registration import register\nfrom gym.spaces import Discrete, Box\n\nimport numpy as np\n\n\ndef sigmoid_price_fun(x, maxcust, gamma):\n    return maxcust \/ (1 + math.exp(gamma * max(0, x)))\n\n\nclass Actions(Enum):\n    DECREASE_PRICE = 0\n    INCREASE_PRICE = 1\n    HOLD = 2\n\n\nPRICE_ADJUSTMENT = {\n    Actions.DECREASE_PRICE: -0.25,\n    Actions.INCREASE_PRICE: 0.25,\n    Actions.HOLD: 0\n}\n\n\nclass ArrivalSim(gym.Env):\n    \"\"\" Simple environment for price optimising RL learner. \"\"\"\n\n    def __init__(self, price):\n        \"\"\"\n        Parameters\n        ----------\n        price : float\n            The initial price to use.\n        \"\"\"\n        super().__init__()\n\n        self.price = price\n        self.revenue = 0\n        self.action_space = Discrete(3)  # [0, 1, 2]  #increase or decrease\n        # original obs space:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space initially suggested:\n        #self.observation_space = Box(0.0, 1000.0, shape=(1,1), dtype=np.float32)\n        # obs space suggested in this edit:\n        self.observation_space = spaces.Box(np.array([0.0]), np.array([1000.0]), dtype=np.float32)\n\n    def step(self, action):\n        \"\"\" Enacts the specified action in the environment.\n\n        Returns the new price, reward, whether we're finished and an empty dict for compatibility with Gym's\n        interface. \"\"\"\n\n        self._take_action(Actions(action))\n\n        next_state = self.price\n        print('(self.price).shape =', (self.price).shape)\n        #next_state = self.observation_space.sample()\n\n        reward = self._get_reward()\n        done = False\n\n        if next_state &lt; 0 or reward == 0:\n            done = True\n\n        print(next_state, reward, done, {})\n\n        return np.array(next_state), reward, done, {}\n\n    def reset(self):\n        \"\"\" Resets the environment, selecting a random initial price. Returns the price. \"\"\"\n        #self.observation_space.value = np.random.rand()\n        #return self.observation_space.sample()\n\n        self.price = np.random.rand(1)\n\n        print('reset -&gt; (self.price).shape = ', (self.price).shape)\n\n        return self.price\n\n    def _take_action(self, action):\n#         self.observation_space.value += PRICE_ADJUSTMENT[action]\n        #print('price b =', self.price)\n        print('price b =', self.price[0])\n        #print('price b =', self.price[[0]])\n        #self.price += PRICE_ADJUSTMENT[action]\n        self.price[0] += PRICE_ADJUSTMENT[action]\n        #self.price[[0]] += PRICE_ADJUSTMENT[action]\n        #print('price a =', self.price)\n        print('price a =', self.price[0])\n        #print('price a =', self.price[[0]])\n\n    #def _get_reward(self, price):\n    def _get_reward(self):\n#         price = self.observation_space.value\n#         return max(np.random.poisson(sigmoid_price_fun(price, 50, 0.5)) * price, 0)\n        #self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        #return max(np.random.poisson(sigmoid_price_fun(self.price, 50, 0.5)) * self.price, 0)\n        self.revenue = max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n        return max(np.random.poisson(sigmoid_price_fun(self.price[0], 50, 0.5)) * self.price[0], 0)\n\n#     def render(self, mode='human'):\n#         super().render(mode)\n\ndef testEnv():\n    \"\"\"\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40.0}\n    )\n    env = gym.make('ArrivalSim-v0')\n    \"\"\"\n    env = ArrivalSim(30.0)\n\n    val = env.reset()\n    print('val.shape = ', val.shape)\n\n    for _ in range(5):\n        print('env.observation_space =', env.observation_space)\n        act = env.action_space.sample()\n        print('\\nact =', act)\n        next_state, reward, done, _ = env.step(act)  # take a random action\n        print('next_state = ', next_state)\n    env.close()\n\n\n\nif __name__ =='__main__':\n\n    testEnv()\n<\/code><\/pre>\n\n<p>The edited training script:<\/p>\n\n<pre><code>import json\nimport os\n\nimport gym\nimport ray\nfrom ray.tune import run_experiments\nimport ray.rllib.agents.a3c as a3c\nimport ray.rllib.agents.ppo as ppo\nfrom ray.tune.registry import register_env\nfrom mod_op_env import ArrivalSim\n\nfrom sagemaker_rl.ray_launcher import SageMakerRayLauncher\n\n\"\"\"\ndef create_environment(env_config):\n    import gym\n#     from gym.spaces import Space\n    from gym.envs.registration import register\n\n    # This import must happen inside the method so that worker processes import this code\n    register(\n        id='ArrivalSim-v0',\n        entry_point='env:ArrivalSim',\n        kwargs= {'price' : 40}\n    )\n    return gym.make('ArrivalSim-v0')\n\"\"\"\ndef create_environment(env_config):\n    price = 30.0\n    # This import must happen inside the method so that worker processes import this code\n    from mod_op_env import ArrivalSim\n    return ArrivalSim(price)\n\n\nclass MyLauncher(SageMakerRayLauncher):\n    def __init__(self):        \n        super(MyLauncher, self).__init__()\n        self.num_gpus = int(os.environ.get(\"SM_NUM_GPUS\", 0))\n        self.hosts_info = json.loads(os.environ.get(\"SM_RESOURCE_CONFIG\"))[\"hosts\"]\n        self.num_total_gpus = self.num_gpus * len(self.hosts_info)\n\n    def register_env_creator(self):\n        register_env(\"ArrivalSim-v0\", create_environment)\n\n    def get_experiment_config(self):\n        return {\n          \"training\": {\n            \"env\": \"ArrivalSim-v0\",\n            \"run\": \"PPO\",\n            \"stop\": {\n              \"training_iteration\": 3,\n            },\n\n            \"local_dir\": \"\/opt\/ml\/model\/\",\n            \"checkpoint_freq\" : 3,\n\n            \"config\": {                                \n              #\"num_workers\": max(self.num_total_gpus-1, 1),\n              \"num_workers\": max(self.num_cpus-1, 1),\n              #\"use_gpu_for_workers\": False,\n              \"train_batch_size\": 128, #5,\n              \"sample_batch_size\": 32, #1,\n              \"gpu_fraction\": 0.3,\n              \"optimizer\": {\n                \"grads_per_step\": 10\n              },\n            },\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 1), \"extra_cpu\": 0},\n            #\"trial_resources\": {\"cpu\": 1, \"gpu\": 0, \"extra_gpu\": max(self.num_total_gpus-1, 0),\n            #                    \"extra_cpu\": max(self.num_cpus-1, 1)},\n            \"trial_resources\": {\"cpu\": 1,\n                                \"extra_cpu\": max(self.num_cpus-1, 1)},              \n          }\n        }\n\nif __name__ == \"__main__\":\n    os.environ[\"LC_ALL\"] = \"C.UTF-8\"\n    os.environ[\"LANG\"] = \"C.UTF-8\"\n    os.environ[\"RAY_USE_XRAY\"] = \"1\"\n    print(ppo.DEFAULT_CONFIG)\n    MyLauncher().train_main()\n\n<\/code><\/pre>\n\n<p>The notebook code:<\/p>\n\n<pre><code>!\/bin\/bash .\/setup.sh\n\nfrom time import gmtime, strftime\nimport sagemaker \nrole = sagemaker.get_execution_role()\n\nsage_session = sagemaker.session.Session()\ns3_bucket = sage_session.default_bucket()  \ns3_output_path = 's3:\/\/{}\/'.format(s3_bucket)\nprint(\"S3 bucket path: {}\".format(s3_output_path))\n\njob_name_prefix = 'ArrivalSim'\n\nfrom sagemaker.rl import RLEstimator, RLToolkit, RLFramework\n\nestimator = RLEstimator(entry_point=\"mod_op_train.py\", # Our launcher code\n                        source_dir='src', # Directory where the supporting files are at. All of this will be\n                                          # copied into the container.\n                        dependencies=[\"common\/sagemaker_rl\"], # some other utils files.\n                        toolkit=RLToolkit.RAY, # We want to run using the Ray toolkit against the ray container image.\n                        framework=RLFramework.TENSORFLOW, # The code is in tensorflow backend.\n                        toolkit_version='0.5.3', # Toolkit version. This will also choose an apporpriate tf version.                                               \n                        #toolkit_version='0.6.5', # Toolkit version. This will also choose an apporpriate tf version.                        \n                        role=role, # The IAM role that we created at the begining.\n                        #train_instance_type=\"ml.m4.xlarge\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_type=\"local\", # Since we want to run fast, lets run on GPUs.\n                        train_instance_count=1, # Single instance will also work, but running distributed makes things \n                                                # fast, particularly in the case of multiple rollout training.\n                        output_path=s3_output_path, # The path where we can expect our trained model.\n                        base_job_name=job_name_prefix, # This is the name we setup above to be to track our job.\n                        hyperparameters = {      # Some hyperparameters for Ray toolkit to operate.\n                          \"s3_bucket\": s3_bucket,\n                          \"rl.training.stop.training_iteration\": 2, # Number of iterations.\n                          \"rl.training.checkpoint_freq\": 2,\n                        },\n                        #metric_definitions=metric_definitions, # This will bring all the logs out into the notebook.\n                    )\n\nestimator.fit()\n<\/code><\/pre>",
        "Answer_comment_count":19.0,
        "Answer_last_edit_time":1568837974432,
        "Answer_score":1.0,
        "Answerer_id":11427968.0,
        "Answerer_created_time":1556553177963,
        "Answerer_last_access_time":1650785543707,
        "Answerer_age":null,
        "Answerer_location":"Singapore",
        "Answerer_reputation":347.0,
        "Answerer_views":93.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57724414",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1567460466780,
        "Question_original_content":"input model shape rllib rai reinforc learn mismatch shape input model reinforc learn project close follow aw exampl specif cartpol exampl built custom environ struggl understand chang environ abl work prebuilt rai rlestim code environ enum import enum import math import gym gym import error space util wrapper gym util import seed gym env registr import regist gym space import discret box import numpi float space import floatspac def sigmoid price fun maxcust gamma return maxcust math exp gamma max class action enum decreas price increas price hold price adjust action decreas price action increas price action hold class arrivalsim gym env simpl environ price optimis learner def init self price paramet price float initi price us super init self price price self revenu self action space discret increas decreas self observ space box arrai arrai self observ space floatspac price def step self action enact specifi action environ return new price reward finish dict compat gym interfac self action action action state self price state self observ space sampl reward self reward fals state reward true print state reward return arrai state reward def reset self reset environ select random initi price return price self observ space valu random rand return self observ space sampl self price random rand return self price def action self action self observ space valu price adjust action self price price adjust action def reward self price price self observ space valu return max random poisson sigmoid price fun price price self revenu max random poisson sigmoid price fun self price self price return max random poisson sigmoid price fun self price self price def render self mode human super render mode def testenv regist arrivalsim entri point env arrivalsim kwarg price env gym arrivalsim env reset rang test env action space sampl print test print env observ space env step test random action env close main testenv train script import json import import gym import rai rai tune import run experi rai tune registri import regist env gym env registr import regist rai launcher import raylaunch def creat environ env config import gym gym space import space gym env registr import regist import happen insid method worker process import code regist arrivalsim entri point env arrivalsim kwarg price return gym arrivalsim class mylaunch raylaunch def regist env creator self regist env arrivalsim creat environ def experi config self return train env arrivalsim run ppo stop episod reward mean config gamma coeff num sgd iter sgd minibatch size train batch size monitor fals record video model free log std fals us gae fals num worker self num cpu num gpu self num gpu batch mode complet episod main mylaunch train main code run jupyt metric definit rlestim default metric definit rltoolkit rai environ env requir requir txt path rel sourc dir estim rlestim entri point train sourc dir toolkit rltoolkit rai toolkit version framework rlframework tensorflow depend imag price respons rai cpu role role train instanc type xlarg train instanc type local train instanc count output path output path base job job prefix metric definit metric definit hyperparamet attent scientist overrid rai algorithm paramet train config horizon train config num sgd iter estim fit wait true job estim latest train job job print train job job error messag receiv follow algo dxwxx statu algo dxwxx fifo schedul algorithm algo dxwxx resourc request cpu gpu algo dxwxx memori usag node algo dxwxx algo dxwxx statu algo dxwxx fifo schedul algorithm algo dxwxx resourc request cpu gpu algo dxwxx memori usag node algo dxwxx result logdir opt output intermedi train algo dxwxx number trial run algo dxwxx run trial algo dxwxx ppo arrivalsim run algo dxwxx algo dxwxx pid warn ppo fyi default valu function share layer polici model share layer fals algo dxwxx error trial runner error process event algo dxwxx traceback recent algo dxwxx file usr local lib python dist packag rai tune trial runner line process trial algo dxwxx result self trial executor fetch result trial algo dxwxx file usr local lib python dist packag rai tune rai trial executor line fetch result algo dxwxx result rai trial futur algo dxwxx file usr local lib python dist packag rai worker line algo dxwxx rais valu algo dxwxx rai except raytaskerror rai worker pid host bbdb algo dxwxx file usr local lib python dist packag rai rllib model model line init algo dxwxx restor num output option algo dxwxx file usr local lib python dist packag rai rllib model model line build layer algo dxwxx rais notimplementederror algo dxwxx notimplementederror algo dxwxx algo dxwxx handl except except occur algo dxwxx algo dxwxx rai worker pid host bbdb algo dxwxx file usr local lib python dist packag rai rllib agent agent line init algo dxwxx trainabl init self config logger creator algo dxwxx file usr local lib python dist packag rai tune trainabl line init algo dxwxx self setup copi deepcopi self config algo dxwxx file usr local lib python dist packag rai rllib agent agent line setup algo dxwxx self init algo dxwxx file usr local lib python dist packag rai rllib agent ppo ppo line init algo dxwxx self env creator self polici graph algo dxwxx file usr local lib python dist packag rai rllib agent agent line local evalu algo dxwxx extra config algo dxwxx file usr local lib python dist packag rai rllib agent agent line evalu algo dxwxx async remot worker env config async remot worker env algo dxwxx file usr local lib python dist packag rai rllib evalu polici evalu line init algo dxwxx self build polici map polici dict polici config algo dxwxx file usr local lib python dist packag rai rllib evalu polici evalu line build polici map algo dxwxx polici map cl ob space act space merg conf algo dxwxx file usr local lib python dist packag rai rllib agent ppo ppo polici graph line init algo dxwxx seq len exist seq len algo dxwxx file usr local lib python dist packag rai rllib model catalog line model algo dxwxx seq len algo dxwxx file usr local lib python dist packag rai rllib model catalog line model algo dxwxx num output option algo dxwxx file usr local lib python dist packag rai rllib model model line init algo dxwxx input dict ob num output option algo dxwxx file usr local lib python dist packag rai rllib model fcnet line build layer algo dxwxx scope label algo dxwxx file usr local lib python dist packag tensorflow contrib framework python op arg scope line func arg algo dxwxx return func arg current arg algo dxwxx file usr local lib python dist packag tensorflow contrib layer python layer layer line fulli connect algo dxwxx output layer appli input algo dxwxx file usr local lib python dist packag tensorflow python kera engin base layer line appli algo dxwxx return self input arg kwarg algo dxwxx file usr local lib python dist packag tensorflow python layer base line algo dxwxx output super layer self input arg kwarg algo dxwxx file usr local lib python dist packag tensorflow python kera engin base layer line algo dxwxx self assert input compat input algo dxwxx file usr local lib python dist packag tensorflow python kera engin base layer line assert input compat algo dxwxx str shape list algo dxwxx valueerror input layer default incompat layer expect min ndim ndim shape receiv algo dxwxx algo dxwxx info rai trial executor destroi actor trial ppo arrivalsim trainabl slow initi consid set reus actor true reduc actor creation overhead algo dxwxx info trial runner attempt recov trial state checkpoint algo dxwxx pid info polici evalu creat polici evalu worker cpu ignor cuda init error sure chang input environ give model model setup document obscur hunch problem li observ action space refer origin aw project exampl http github com awslab amazon exampl tree master reinforc learn roboschool rai",
        "Question_preprocessed_content":"input model shape mismatch shape input model reinforc learn project close follow aw exampl specif cartpol exampl built custom environ struggl understand chang environ abl work prebuilt rai rlestim code environ train script code run jupyt error messag receiv follow sure chang input environ give model model setup document obscur hunch problem li observ action space refer origin aw project exampl",
        "Question_gpt_summary_original":"the user is encountering a challenge with mismatched shapes between the inputs and model of their reinforcement learning project, and is struggling to understand how to change their environment to work with the prebuilt ray rlestimator.",
        "Question_gpt_summary":"user encount challeng mismatch shape input model reinforc learn project struggl understand chang environ work prebuilt rai rlestim",
        "Answer_original_content":"possibl reason error messag valueerror input layer default incompat layer expect min ndim ndim shape receiv origin environ ob space self observ space box arrai arrai displai shape environ ob space give print box arrai arrai dtype float shape indic shape receiv error messag pass shape zero expect min ndim zero print print ndim suggest solut assum want environ ob space rang indic self price random rand reset function try follow environ ob space self observ space box shape dtype float hope set box explicit shape help edit modifi train script modif includ new import custom model class model registr addit regist custom model config readabl section ad shown entir modifi train script avail gist run propos ob space new addit import new import futur import absolut import futur import divis futur import print function import numpi rai rllib model import modelcatalog rai rllib model modelv import tfmodelv rai rllib model fcnet import fullyconnectednetwork rai rllib util import try import rai tune import grid search try import end new import custom model class custom model class fcnet class custommodel tfmodelv exampl custom model deleg net def init self ob space action space num output model config super custommodel self init ob space action space num output model config self model fullyconnectednetwork ob space action space num output model config self regist variabl self model variabl def forward self input dict state seq len return self model forward input dict state seq len def valu function self return self model valu function regist add custom model def experi config self regist custom model modelcatalog regist custom model model custommodel return train env arrivalsim run ppo stop episod reward mean config model custom model model add regist custom model gamma coeff num sgd iter sgd minibatch size train batch size monitor fals record video model free log std fals us gae fals num worker self num cpu num gpu self num gpu batch mode complet episod edit work truncat output jupyt notebook instanc algo yayw price algo yayw price algo yayw self price shape algo yayw fals algo yayw price algo yayw price algo yayw self price shape algo yayw fals algo yayw price algo yayw price algo yayw self price shape algo yayw fals algo yayw price algo yayw price algo yayw self price shape algo yayw true algo yayw result ppo arrivalsim algo yayw date algo yayw true algo yayw episod len mean algo yayw episod reward max algo yayw episod reward mean algo yayw episod reward min algo yayw episod algo yayw experi dbfadbaeeaf algo yayw hostnam daedcc algo yayw info algo yayw cur algo yayw entropi algo yayw grad time algo yayw algo yayw load time algo yayw num step sampl algo yayw num step train algo yayw polici loss algo yayw sampl time algo yayw total loss algo yayw updat time algo yayw explain var algo yayw loss algo yayw iter restor algo yayw node algo yayw pid algo yayw polici reward mean algo yayw time restor algo yayw time iter algo yayw time total algo yayw timestamp algo yayw timestep restor algo yayw timestep iter algo yayw timestep total algo yayw train iter algo yayw algo yayw worker di kill execut task abbaffea algo yayw worker di kill execut task dbcbeecfceea algo yayw statu algo yayw fifo schedul algorithm algo yayw resourc request cpu gpu algo yayw result logdir opt output intermedi train algo yayw termin trial algo yayw ppo arrivalsim termin pid iter rew algo yayw algo yayw save model configur algo yayw save checkpoint file opt output intermedi train ppo arrivalsim vdvlux checkpoint extra data opt model checkpoint extra data algo yayw save checkpoint file opt output intermedi train ppo arrivalsim vdvlux checkpoint tune metadata opt model checkpoint tune metadata algo yayw creat logsync root rai result ppo arrivalsim xdn algo yayw tensorflow core common runtim process util creat new thread pool default inter set tune inter parallel thread best perform algo yayw reset self price shape algo yayw localmultigpuoptim devic cpu algo yayw reset self price shape algo yayw info tensorflow asset save algo yayw asset save algo yayw info tensorflow asset write algo yayw asset write algo yayw info tensorflow savedmodel written opt model save model algo yayw savedmodel written opt model save model algo yayw save tensorflow serv model algo yayw worker di kill execut task fdbcafe algo yayw contain info report train success tmpwwbb algo yayw exit code abort contain exit fail delet tmp tmpwwbb algo yayw remov manual job complet time edit file environ train script jupyt notebook turn isn need defin custom model custom environ remain viabl right main caus issu ob space set self price numpi arrai talk better rai rllib creation custom environ train script simpler wai shown notebook version instead toolkit version train local mode docker contain jupyt notebook instanc aw cpu work instanc xlarg gpu entir packag depend edit env new futur import absolut import futur import divis futur import print function end new enum import enum import math import gym gym import error space util wrapper gym util import seed gym env registr import regist gym space import discret box import numpi def sigmoid price fun maxcust gamma return maxcust math exp gamma max class action enum decreas price increas price hold price adjust action decreas price action increas price action hold class arrivalsim gym env simpl environ price optimis learner def init self price paramet price float initi price us super init self price price self revenu self action space discret increas decreas origin ob space self observ space box shape dtype float ob space initi suggest self observ space box shape dtype float ob space suggest edit self observ space space box arrai arrai dtype float def step self action enact specifi action environ return new price reward finish dict compat gym interfac self action action action state self price print self price shape self price shape state self observ space sampl reward self reward fals state self price shape self price shape return self price def action self action self observ space valu price adjust action print price self price print price self price print price self price self price price adjust action self price price adjust action self price price adjust action print price self price print price self price print price self price def reward self price def reward self price self observ space valu return max random poisson sigmoid price fun price price self revenu max random poisson sigmoid price fun self price self price return max random poisson sigmoid price fun self price self price self revenu max random poisson sigmoid price fun self price self price return max random poisson sigmoid price fun self price self price def render self mode human super render mode def testenv regist arrivalsim entri point env arrivalsim kwarg price env gym arrivalsim env arrivalsim val env reset print val shape val shape rang print env observ space env observ space act env action space sampl print nact act state reward env step act random action print state state env close main testenv edit train script import json import import gym import rai rai tune import run experi import rai rllib agent import rai rllib agent ppo ppo rai tune registri import regist env mod env import arrivalsim rai launcher import raylaunch def creat environ env config import gym gym space import space gym env registr import regist import happen insid method worker process import code regist arrivalsim entri point env arrivalsim kwarg price return gym arrivalsim def creat environ env config price import happen insid method worker process import code mod env import arrivalsim return arrivalsim price class mylaunch raylaunch def init self super mylaunch self init self num gpu int environ num gpu self host info json load environ resourc config host self num total gpu self num gpu len self host info def regist env creator self regist env arrivalsim creat environ def experi config self return train env arrivalsim run ppo stop train iter local dir opt model checkpoint freq config num worker max self num total gpu num worker max self num cpu us gpu worker fals train batch size sampl batch size gpu fraction optim grad step trial resourc cpu gpu extra gpu max self num total gpu extra cpu trial resourc cpu gpu extra gpu max self num total gpu extra cpu max self num cpu trial resourc cpu extra cpu max self num cpu main environ utf environ lang utf environ rai us xrai print ppo default config mylaunch train main notebook code bin bash setup time import gmtime strftime import role execut role sage session session session bucket sage session default bucket output path format bucket print bucket path format output path job prefix arrivalsim import rlestim rltoolkit rlframework estim rlestim entri point mod train launcher code sourc dir src directori support file copi contain depend common util file toolkit rltoolkit rai want run rai toolkit rai contain imag framework rlframework tensorflow code tensorflow backend toolkit version toolkit version choos apporpri version toolkit version toolkit version choos apporpri version role role iam role creat begin train instanc type xlarg want run fast let run gpu train instanc type local want run fast let run gpu train instanc count singl instanc work run distribut make thing fast particularli case multipl rollout train output path output path path expect train model base job job prefix setup track job hyperparamet hyperparamet rai toolkit oper bucket bucket train stop train iter number iter train checkpoint freq metric definit metric definit bring log notebook estim fit",
        "Answer_preprocessed_content":"possibl reason error messag origin environ ob space displai shape environ ob space give indic error messag pass shape expect suggest solut assum want environ ob space rang indic function try follow environ ob space hope set explicit help edit modifi train script modif includ new import custom model class model registr addit regist custom model config readabl section ad shown entir modifi train script avail gist run propos ob space new addit import custom model class regist add custom model edit work truncat output time edit file environ train script jupyt notebook turn isn need defin custom model custom environ remain viabl right main caus issu ob space set numpi arrai talk better rai rllib creation custom environ train script simpler wai shown notebook version instead train local mode cpu work instanc gpu entir packag depend edit env edit train script notebook code",
        "Answer_gpt_summary_original":"the solution to the user's challenge with mismatched shapes between the inputs and model of their reinforcement learning project is to set the environment obs space to range from 0.0 to 1000.0 using the following code: self.observation_space = box(0.0, 1000.0, shape=(1,1), dtype=np.float32).",
        "Answer_gpt_summary":"solut user challeng mismatch shape input model reinforc learn project set environ ob space rang follow code self observ space box shape dtype float"
    },
    {
        "Question_title":"Is AWS Sage Maker Auto Pilot suitable for NLP?",
        "Question_body":"<p>Is AWS Sage Maker Auto Pilot suitable for NLP?<\/p>\n\n<p>We currently have a tensorflow model that does classification on input of a sequence of URLS (\nWe transform the URLs to Word vec and Char vec to feed it to the model).<\/p>\n\n<p>Looking at Sage Maker Auto Pilot documentation it says that it works on input in tabular form.\nI was wondering if we could use it to for our use case.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1578226347093,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":261.0,
        "Poster_id":6092553.0,
        "Poster_created_time":1458550179920,
        "Poster_last_access_time":1658058794840,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":383.0,
        "Poster_views":19.0,
        "Answer_body":"<p>No. SageMaker AutoPilot doesn't support deep learning at the moment, only classification and regression problems on tabular data. Technically, I guess you could pass embeddings in CSV format, and pray that XGBoost figures them out, but I seriously doubt that this would deliver meaningful results :)<\/p>\n\n<p>Amazon Comprehend does support fully managed custom classification models <a href=\"https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/comprehend\/latest\/dg\/how-document-classification.html<\/a>. It may be worth taking a look at it.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":4686192.0,
        "Answerer_created_time":1426694564423,
        "Answerer_last_access_time":1659633906700,
        "Answerer_age":null,
        "Answerer_location":"Paris",
        "Answerer_reputation":2425.0,
        "Answerer_views":459.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59599721",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1578756147747,
        "Question_original_content":"aw sage maker auto pilot suitabl nlp aw sage maker auto pilot suitabl nlp current tensorflow model classif input sequenc url transform url word vec char vec feed model look sage maker auto pilot document sai work input tabular form wonder us us case",
        "Question_preprocessed_content":"aw sage maker auto pilot suitabl nlp aw sage maker auto pilot suitabl nlp current tensorflow model classif input sequenc url look sage maker auto pilot document sai work input tabular form wonder us us case",
        "Question_gpt_summary_original":"The user is questioning whether AWS Sage Maker Auto Pilot is suitable for their Natural Language Processing (NLP) task, as their current tensorflow model uses a sequence of URLs transformed into Word and Char vectors for classification. They are unsure if Auto Pilot, which works on tabular input, can be used for their use case.",
        "Question_gpt_summary":"user question aw sage maker auto pilot suitabl natur languag process nlp task current tensorflow model us sequenc url transform word char vector classif unsur auto pilot work tabular input us case",
        "Answer_original_content":"autopilot support deep learn moment classif regress problem tabular data technic guess pass embed csv format prai xgboost figur serious doubt deliv meaning result amazon comprehend support fulli manag custom classif model http doc aw amazon com comprehend latest document classif html worth take look",
        "Answer_preprocessed_content":"autopilot support deep learn moment classif regress problem tabular data technic guess pass embed csv format prai xgboost figur serious doubt deliv meaning result amazon comprehend support fulli manag custom classif model worth take look",
        "Answer_gpt_summary_original":"SageMaker AutoPilot is not suitable for the user's NLP task as it only supports classification and regression problems on tabular data. It is suggested that the user looks into Amazon Comprehend, which supports fully managed custom classification models.",
        "Answer_gpt_summary":"autopilot suitabl user nlp task support classif regress problem tabular data suggest user look amazon comprehend support fulli manag custom classif model"
    },
    {
        "Question_title":"Amazon SageMaker: ClientError: Data download failed:NoSuchKey (404): The specified key does not exist",
        "Question_body":"<p>I'm running a SageMaker Training Job with a custom algorithm and the input data from s3. The SagaMaker AIM role ARN has a Read\/Put policy on the specified S3 bucket folder, but while creating the job I get a client error:<\/p>\n\n<p><code>ClientError: Data download failed:NoSuchKey (404): The specified key does not exist.<\/code><\/p>\n\n<p>Unfortunately no more error info is provided in the SageMaker dashboard to investigate further. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1541002596347,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":1219.0,
        "Poster_id":758836.0,
        "Poster_created_time":1305708350447,
        "Poster_last_access_time":1663804694443,
        "Poster_age":null,
        "Poster_location":"Bologna, Italy",
        "Poster_reputation":14823.0,
        "Poster_views":1847.0,
        "Answer_body":"<p>SageMaker team member here.<\/p>\n\n<p>The problem here is that the training job in question was setup with S3DataType=ManifestFile. In this case SageMaker expects to be able to download a single manifest file from the location specified by the S3Uri, if the file does not exist in S3 we get a 404 which is what we're sending back as the error here.<\/p>\n\n<p>See here for documentation on S3DataType\/S3Uri and manifests: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html#SageMaker-Type-S3DataSource-S3DataType\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_S3DataSource.html#SageMaker-Type-S3DataSource-S3DataType<\/a><\/p>\n\n<p>We will work to make this error message a bit more user-friendly, thanks for calling this out!<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":1502599.0,
        "Answerer_created_time":1341441916656,
        "Answerer_last_access_time":1635983985347,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":5985.0,
        "Answerer_views":161.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53087851",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1541230379527,
        "Question_original_content":"clienterror data download fail nosuchkei specifi kei exist run train job custom algorithm input data sagamak aim role arn read polici specifi bucket folder creat job client error clienterror data download fail nosuchkei specifi kei exist unfortun error info provid dashboard investig",
        "Question_preprocessed_content":"clienterror data download fail nosuchkei specifi kei exist run train job custom algorithm input data sagamak aim role arn polici specifi bucket folder creat job client error unfortun error info provid dashboard investig",
        "Question_gpt_summary_original":"The user encountered a challenge while running a SageMaker Training Job with a custom algorithm and input data from S3. The specified key does not exist, resulting in a client error with no further information provided in the SageMaker dashboard to investigate further.",
        "Question_gpt_summary":"user encount challeng run train job custom algorithm input data specifi kei exist result client error inform provid dashboard investig",
        "Answer_original_content":"team member problem train job question setup sdatatyp manifestfil case expect abl download singl manifest file locat specifi suri file exist send error document sdatatyp suri manifest http doc aw amazon com latest api sdatasourc html type sdatasourc sdatatyp work error messag bit user friendli thank call",
        "Answer_preprocessed_content":"team member problem train job question setup datatyp manifestfil case expect abl download singl manifest file locat specifi uri file exist send error document manifest work error messag bit thank call",
        "Answer_gpt_summary_original":"Solution: The issue occurred because the training job was set up with S3DataType=ManifestFile, and SageMaker expected to download a single manifest file from the location specified by the S3Uri. If the file does not exist in S3, SageMaker sends back a 404 error. The SageMaker team member acknowledged the issue and will work to make the error message more user-friendly. No further solutions were provided.",
        "Answer_gpt_summary":"solut issu occur train job set sdatatyp manifestfil expect download singl manifest file locat specifi suri file exist send error team member acknowledg issu work error messag user friendli solut provid"
    },
    {
        "Question_title":"strange behavior of roc_auc_score, 'roc_auc', 'auc'",
        "Question_body":"<p>While optimizing parameters for xgboost I encountered a problem with the roc_auc_score metric. I get significantly different results during cross-validation compared to the results on the training data.<\/p>\n\n<pre><code>class OptunaHyperparamsSearch:\ndef __init__(self, X_train, y_train, **kwargs):\n    ...\n\ndef objective(self, trial):\n\n    ...\n\n    cv_results = xgb.cv(param, self.dtrain, num_boost_round=5, metrics=['auc'], nfold=5, verbose_eval=True)\n\n    mean_auc = cv_results['test-auc-mean'].max()\n    boost_rounds = cv_results['test-auc-mean'].idxmax()\n\n    param['n_estimators'] = boost_rounds\n    trial.set_user_attr('param', param)\n\n    print('boost_rounds: ', boost_rounds)\n    print('train-auc-mean', cv_results['train-auc-mean'][boost_rounds])\n\n    return mean_auc\n\ndef best_model(self, n_trials=100, save_path=None):\n\n    study = optuna.create_study(direction=\"maximize\")\n    study.optimize(self.objective, n_trials=n_trials)\n\n    best_params = study.best_trial.user_attrs['param']\n    best_model = xgb.XGBClassifier(**best_params)\n    best_model.fit(self.X_train, self.y_train)\n\n    return best_model\n<\/code><\/pre>\n\n<p>After running code:<\/p>\n\n<pre><code>search = OptunaHyperparamsSearch(X_train, y_train)\nmodel = search.best_model(n_trials=1)\n<\/code><\/pre>\n\n<p>I received:<\/p>\n\n<pre><code>[0] train-auc:0.777869+0.00962852   test-auc:0.771169+0.025347\n[1] train-auc:0.786905+0.00865646   test-auc:0.777492+0.0255523\n[2] train-auc:0.793305+0.00480249   test-auc:0.785307+0.0198732\n[3] train-auc:0.79595+0.00349561    test-auc:0.789897+0.0158569\n[4] train-auc:0.796818+0.00407504   test-auc:0.789997+0.016069\nboost_rounds:  4\ntrain-auc-mean 0.796818\n[I 2020-06-04 10:12:25,093] Finished trial#0 with value: 0.7899968 with parameters: {'booster': 'dart', 'reg_lambda': 0.8001057111479173, 'reg_alpha': 0.0016960618598770582, 'max_depth': 8, 'min_child_weight': 4, 'learning_rate': 0.0602235073221647, 'gamma': 0.0011248451567255984, 'colsample_bytree': 0.911487203002922, 'subsample': 0.9057485217255851, 'grow_policy': 'lossguide', 'scale_pos_weight': 0.5865962792358733, 'sample_type': 'weighted', 'normalize_type': 'tree', 'rate_drop': 0.0009459988874640169, 'skip_drop': 8.103200442539776e-05}. Best is trial#0 with value: 0.7899968.\n<\/code><\/pre>\n\n<p>So the result is about 0.8 (<strong>train-auc-mean 0.796818<\/strong>). And after that running:<\/p>\n\n<pre><code>y_pred = model.predict(X_train)\nprint(roc_auc_score(y_train, y_pred))\n<\/code><\/pre>\n\n<p>I received:<\/p>\n\n<pre><code>0.598231710442728\n<\/code><\/pre>\n\n<p>So it's impossible. I tried also use customize function:<\/p>\n\n<pre><code>from sklearn.metrics import roc_auc_score\n\ndef PyAUC(predt: np.ndarray, dtrain: xgb.DMatrix):\n    y = dtrain.get_label()\n    return 'PyAUC', roc_auc_score(y, predt)\n<\/code><\/pre>\n\n<p>and pass them by <code>feval<\/code> to <code>xgb.cv<\/code>, setting <code>param['disable_default_eval_metric'] = 1<\/code> and without defining metrics and the result was the same.<\/p>\n\n<p>Then I tried to use RandomizedSearchCV:<\/p>\n\n<pre><code>params = {\n    'min_child_weight': [1, 5, 10],\n    'gamma': [0.5, 1, 1.5, 2, 5],\n    'subsample': [0.6, 0.8, 1.0],\n    'colsample_bytree': [0.6, 0.8, 1.0],\n    'max_depth': [3, 4, 5]\n    }\nalg = XGBClassifier(learning_rate=0.01, n_estimators=5, objective='binary:logistic',\n                silent=True, nthread=1)\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 1001)\n\nrandom_search = RandomizedSearchCV(alg, param_distributions=params, n_iter=10, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train, y_train), verbose=3, random_state=1001 )\n\nrandom_search.fit(X_train, y_train)\n\nprint('\\n All results:')\nprint(random_search.cv_results_)\n\ny_pred = random_search.predict(X_train)\nprint(roc_auc_score(y_train, y_pred))\n<\/code><\/pre>\n\n<p>The output was:<\/p>\n\n<pre><code>All results:\n{'mean_fit_time': array([0.27621794, 0.40631523, 0.36202598, 0.32188687, 0.34574351,\n   0.2747798 , 0.31780529, 0.32190156, 0.34060073, 0.25945067]), 'std_fit_time': array([0.02603387, 0.04572275, 0.09460844, 0.01841953, 0.08391794,\n   0.03654419, 0.01583525, 0.03670047, 0.01035465, 0.03085039]), 'mean_score_time': array([0.01927972, 0.0143033 , 0.01697631, 0.01260743, 0.02442002,\n   0.02089334, 0.0182806 , 0.0132216 , 0.01498265, 0.01320119]), 'std_score_time': array([0.00609847, 0.00671443, 0.00613005, 0.00410744, 0.00384849,\n   0.00516041, 0.00505873, 0.00276774, 0.00023382, 0.00546102]), 'param_subsample': masked_array(data=[1.0, 0.6, 0.8, 1.0, 0.8, 1.0, 1.0, 0.8, 0.8, 0.8],\n         mask=[False, False, False, False, False, False, False, False,\n               False, False],\n   fill_value='?',\n        dtype=object), 'param_min_child_weight': masked_array(data=[5, 1, 5, 5, 1, 10, 1, 1, 1, 1],\n         mask=[False, False, False, False, False, False, False, False,\n               False, False],\n   fill_value='?',\n        dtype=object), 'param_max_depth': masked_array(data=[3, 5, 5, 5, 4, 4, 5, 3, 5, 4],\n         mask=[False, False, False, False, False, False, False, False,\n               False, False],\n   fill_value='?',\n        dtype=object), 'param_gamma': masked_array(data=[5, 1.5, 1, 5, 1, 1.5, 5, 2, 0.5, 1.5],\n         mask=[False, False, False, False, False, False, False, False,\n               False, False],\n   fill_value='?',\n        dtype=object), 'param_colsample_bytree': masked_array(data=[1.0, 0.8, 0.8, 0.6, 1.0, 0.6, 0.6, 0.8, 0.6, 0.6],\n         mask=[False, False, False, False, False, False, False, False,\n               False, False],\n   fill_value='?',\n        dtype=object), 'params': [{'subsample': 1.0, 'min_child_weight': 5, 'max_depth': 3, 'gamma': 5, 'colsample_bytree': 1.0}, {'subsample': 0.6, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 1.5, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'min_child_weight': 5, 'max_depth': 5, 'gamma': 1, 'colsample_bytree': 0.8}, {'subsample': 1.0, 'min_child_weight': 5, 'max_depth': 5, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'min_child_weight': 1, 'max_depth': 4, 'gamma': 1, 'colsample_bytree': 1.0}, {'subsample': 1.0, 'min_child_weight': 10, 'max_depth': 4, 'gamma': 1.5, 'colsample_bytree': 0.6}, {'subsample': 1.0, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'min_child_weight': 1, 'max_depth': 3, 'gamma': 2, 'colsample_bytree': 0.8}, {'subsample': 0.8, 'min_child_weight': 1, 'max_depth': 5, 'gamma': 0.5, 'colsample_bytree': 0.6}, {'subsample': 0.8, 'min_child_weight': 1, 'max_depth': 4, 'gamma': 1.5, 'colsample_bytree': 0.6}], 'split0_test_score': array([0.75734333, 0.78965043, 0.78929122, 0.77842559, 0.78669592,\n   0.77856369, 0.7803955 , 0.77733652, 0.78884686, 0.77706318]), 'split1_test_score': array([0.7564997 , 0.78553601, 0.78621578, 0.77250155, 0.78589665,\n   0.77237991, 0.77235486, 0.77187115, 0.78573708, 0.77046652]), 'split2_test_score': array([0.75575839, 0.77356843, 0.79002323, 0.77134164, 0.76641651,\n   0.76965581, 0.77133806, 0.76749842, 0.79029943, 0.77043647]), 'split3_test_score': array([0.74596394, 0.77188117, 0.76967513, 0.76816388, 0.76832059,\n   0.76795065, 0.76942182, 0.76217902, 0.76846871, 0.75720452]), 'split4_test_score': array([0.78099172, 0.80616938, 0.80491224, 0.80371433, 0.81990511,\n   0.82052725, 0.80327483, 0.80598102, 0.8171982 , 0.8052647 ]), 'mean_test_score': array([0.75931142, 0.78536108, 0.78802352, 0.7788294 , 0.78544696,\n   0.78181546, 0.77935701, 0.77697323, 0.79011006, 0.77608708]), 'std_test_score': array([0.01159822, 0.0124273 , 0.0112318 , 0.01287854, 0.01920727,\n   0.01968907, 0.01253142, 0.0153379 , 0.01563886, 0.01595216]), 'rank_test_score': array([10,  4,  2,  7,  3,  5,  6,  8,  1,  9], dtype=int32)}\n0.6093407594278569\n<\/code><\/pre>\n\n<p>So still the same problem: during cross-validation score about 0.8 and after that 0.6. I suppose that different metrics are used.<\/p>\n\n<p>The solution I found was to pass in RandomizedSearchCV: <code>scoring=make_scorer(roc_auc_score)<\/code>. This solved the problem giving the same result in cross-validation and after that about 0.6.<\/p>\n\n<p>Can anyone explain what the problem was because I still don't understand it? And I still don't know how to solve it using optuna optimalization.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1591267109627,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":387.0,
        "Poster_id":13678785.0,
        "Poster_created_time":1591262821660,
        "Poster_last_access_time":1595886265207,
        "Poster_age":null,
        "Poster_location":"Cracow, Polska",
        "Poster_reputation":3.0,
        "Poster_views":1.0,
        "Answer_body":"<p>You're using <code>model.predict<\/code>, but the ROC curve and <code>roc_auc_score<\/code> needs the predicted probabilities (or other confidence measures, maybe); use <code>model.predict_proba<\/code>.<\/p>\n\n<p><a href=\"https:\/\/stackoverflow.com\/q\/30623637\/10495893\">Scikit-learn : roc_auc_score<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":10495893.0,
        "Answerer_created_time":1539356592836,
        "Answerer_last_access_time":1664058553476,
        "Answerer_age":null,
        "Answerer_location":"Chicago, IL, USA",
        "Answerer_reputation":8552.0,
        "Answerer_views":772.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62192616",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Question_closed_time":1591279313660,
        "Question_original_content":"strang behavior roc auc score roc auc auc optim paramet xgboost encount problem roc auc score metric significantli differ result cross valid compar result train data class hyperparamssearch def init self train train kwarg def object self trial result xgb param self dtrain num boost round metric auc nfold verbos eval true mean auc result test auc mean max boost round result test auc mean idxmax param estim boost round trial set user attr param param print boost round boost round print train auc mean result train auc mean boost round return mean auc def best model self trial save path studi creat studi direct maxim studi optim self object trial trial best param studi best trial user attr param best model xgb xgbclassifi best param best model fit self train self train return best model run code search hyperparamssearch train train model search best model trial receiv train auc test auc train auc test auc train auc test auc train auc test auc train auc test auc boost round train auc mean finish trial valu paramet booster dart reg lambda reg alpha max depth min child weight learn rate gamma colsampl bytre subsampl grow polici lossguid scale po weight sampl type weight normal type tree rate drop skip drop best trial valu result train auc mean run pred model predict train print roc auc score train pred receiv imposs tri us custom function sklearn metric import roc auc score def pyauc predt ndarrai dtrain xgb dmatrix dtrain label return pyauc roc auc score predt pass feval xgb set param disabl default eval metric defin metric result tri us randomizedsearchcv param min child weight gamma subsampl colsampl bytre max depth alg xgbclassifi learn rate estim object binari logist silent true nthread skf stratifiedkfold split shuffl true random state random search randomizedsearchcv alg param distribut param iter score roc auc job skf split train train verbos random state random search fit train train print result print random search result pred random search predict train print roc auc score train pred output result mean fit time arrai std fit time arrai mean score time arrai std score time arrai param subsampl mask arrai data mask fals fals fals fals fals fals fals fals fals fals valu dtype object param min child weight mask arrai data mask fals fals fals fals fals fals fals fals fals fals valu dtype object param max depth mask arrai data mask fals fals fals fals fals fals fals fals fals fals valu dtype object param gamma mask arrai data mask fals fals fals fals fals fals fals fals fals fals valu dtype object param colsampl bytre mask arrai data mask fals fals fals fals fals fals fals fals fals fals valu dtype object param subsampl min child weight max depth gamma colsampl bytre subsampl min child weight max depth gamma colsampl bytre subsampl min child weight max depth gamma colsampl bytre subsampl min child weight max depth gamma colsampl bytre subsampl min child weight max depth gamma colsampl bytre subsampl min child weight max depth gamma colsampl bytre subsampl min child weight max depth gamma colsampl bytre subsampl min child weight max depth gamma colsampl bytre subsampl min child weight max depth gamma colsampl bytre subsampl min child weight max depth gamma colsampl bytre split test score arrai split test score arrai split test score arrai split test score arrai split test score arrai mean test score arrai std test score arrai rank test score arrai dtype int problem cross valid score suppos differ metric solut pass randomizedsearchcv score scorer roc auc score solv problem give result cross valid explain problem understand know solv optim",
        "Question_preprocessed_content":"strang behavior auc optim paramet xgboost encount problem metric significantli differ result compar result train data run code receiv result run receiv imposs tri us custom function pass set defin metric result tri us randomizedsearchcv output problem score suppos differ metric solut pass randomizedsearchcv solv problem give result explain problem understand know solv optim",
        "Question_gpt_summary_original":"The user encountered a problem with the roc_auc_score metric while optimizing parameters for xgboost. The results during cross-validation were significantly different from the results on the training data. The user tried using a customized function and RandomizedSearchCV, but the problem persisted. The solution was to pass in RandomizedSearchCV: scoring=make_scorer(roc_auc_score). The user is still unsure about the cause of the problem and how to solve it using optuna optimization.",
        "Question_gpt_summary":"user encount problem roc auc score metric optim paramet xgboost result cross valid significantli differ result train data user tri custom function randomizedsearchcv problem persist solut pass randomizedsearchcv score scorer roc auc score user unsur caus problem solv optim",
        "Answer_original_content":"model predict roc curv roc auc score need predict probabl confid measur mayb us model predict proba scikit learn roc auc score",
        "Answer_preprocessed_content":"roc curv need predict probabl us",
        "Answer_gpt_summary_original":"Solution: The user should use `model.predict_proba` instead of `model.predict` to get the predicted probabilities needed for ROC curve and `roc_auc_score`. Additionally, passing in `scoring=make_scorer(roc_auc_score)` in `RandomizedSearchCV` can help solve the problem. No solution was provided for using optuna optimization.",
        "Answer_gpt_summary":"solut user us model predict proba instead model predict predict probabl need roc curv roc auc score addition pass score scorer roc auc score randomizedsearchcv help solv problem solut provid optim"
    },
    {
        "Question_title":"GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set",
        "Question_body":"<p>I'm new to Google Cloud Platform and I'm trying to create a Feature Store to fill with values from a csv file from Google Cloud Storage. The aim is to do that from a local notebook in Python.\nI'm basically following the code <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/vertex-ai-samples\/blob\/master\/notebooks\/official\/feature_store\/gapic-feature-store.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, making the appropriate changes since I'm working with the credit card public dataset.\nThe error that raises when I run the code is the following:<\/p>\n<pre><code>GoogleAPICallError: None Unexpected state: Long-running operation had neither response nor error set.\n<\/code><\/pre>\n<p>and it happens during the ingestion of the data from the csv file.<\/p>\n<p>Here it is the code I'm working on:<\/p>\n<pre><code>import os\nfrom datetime import datetime\nfrom google.cloud import bigquery\nfrom google.cloud import aiplatform\nfrom google.cloud.aiplatform_v1.types import feature as feature_pb2\nfrom google.cloud.aiplatform_v1.types import featurestore as featurestore_pb2\nfrom google.cloud.aiplatform_v1.types import \\\n    featurestore_service as featurestore_service_pb2\nfrom google.cloud.aiplatform_v1.types import entity_type as entity_type_pb2\nfrom google.cloud.aiplatform_v1.types import FeatureSelector, IdMatcher\n\ncredential_path = r&quot;C:\\Users\\...\\.json&quot;\nos.environ['GOOGLE_APPLICATION_CREDENTIALS'] = credential_path\n\n## Constants\nPROJECT_ID = &quot;my-project-ID&quot;\nREGION = &quot;us-central1&quot;\nAPI_ENDPOINT = &quot;us-central1-aiplatform.googleapis.com&quot;\nINPUT_CSV_FILE = &quot;my-input-file.csv&quot;\nFEATURESTORE_ID = &quot;fraud_detection&quot;\n\n## Output dataset\nDESTINATION_DATA_SET = &quot;fraud_predictions&quot;\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\nDESTINATION_DATA_SET = &quot;{prefix}_{timestamp}&quot;.format(\n    prefix=DESTINATION_DATA_SET, timestamp=TIMESTAMP\n)\n\n## Output table. Make sure that the table does NOT already exist; \n## the BatchReadFeatureValues API cannot overwrite an existing table\nDESTINATION_TABLE_NAME = &quot;training_data&quot;\n\nDESTINATION_PATTERN = &quot;bq:\/\/{project}.{dataset}.{table}&quot;\nDESTINATION_TABLE_URI = DESTINATION_PATTERN.format(\n    project=PROJECT_ID, dataset=DESTINATION_DATA_SET, \n    table=DESTINATION_TABLE_NAME\n)\n\n## Create dataset\nclient = bigquery.Client(project=PROJECT_ID)\ndataset_id = &quot;{}.{}&quot;.format(client.project, DESTINATION_DATA_SET)\ndataset = bigquery.Dataset(dataset_id)\ndataset.location = REGION\ndataset = client.create_dataset(dataset)\nprint(&quot;Created dataset {}.{}&quot;.format(client.project, dataset.dataset_id))\n\n## Create client for CRUD and data_client for reading feature values.\nclient = aiplatform.gapic.FeaturestoreServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\ndata_client = aiplatform.gapic.FeaturestoreOnlineServingServiceClient(\n    client_options={&quot;api_endpoint&quot;: API_ENDPOINT})\nBASE_RESOURCE_PATH = client.common_location_path(PROJECT_ID, REGION)\n\n## Create featurestore (only the first time)\ncreate_lro = client.create_featurestore(\n    featurestore_service_pb2.CreateFeaturestoreRequest(\n        parent=BASE_RESOURCE_PATH,\n        featurestore_id=FEATURESTORE_ID,\n        featurestore=featurestore_pb2.Featurestore(\n            online_serving_config=featurestore_pb2.Featurestore.OnlineServingConfig(\n                fixed_node_count=1\n            ),\n        ),\n    )\n)\n\n## Wait for LRO to finish and get the LRO result.\nprint(create_lro.result())\n\nclient.get_featurestore(\n    name=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID)\n)\n\n## Create credit card entity type (only the first time)\ncc_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;creditcards&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Credit card entity&quot;,\n        ),\n    )\n)\n\n## Create fraud entity type (only the first time)\nfraud_entity_type_lro = client.create_entity_type(\n    featurestore_service_pb2.CreateEntityTypeRequest(\n        parent=client.featurestore_path(PROJECT_ID, REGION, FEATURESTORE_ID),\n        entity_type_id=&quot;frauds&quot;,\n        entity_type=entity_type_pb2.EntityType(\n            description=&quot;Fraud entity&quot;,\n        ),\n    )\n)\n\n## Create features for credit card type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v1&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v2&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v3&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v4&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v5&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v6&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v7&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v8&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v9&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v10&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v11&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v12&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v13&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v14&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v15&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v16&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v17&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v18&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v19&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v20&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v21&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v22&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v23&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v24&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v25&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v26&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v27&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;v28&quot;,\n        ),\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;amount&quot;,\n        ),\n    ],\n).result()\n\n## Create features for fraud type (only the first time)\nclient.batch_create_features(\n    parent=client.entity_type_path(PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    requests=[\n        featurestore_service_pb2.CreateFeatureRequest(\n            feature=feature_pb2.Feature(\n                value_type=feature_pb2.Feature.ValueType.DOUBLE, description=&quot;&quot;,\n            ),\n            feature_id=&quot;class&quot;,\n        ),\n    ],\n).result()\n\n## Import features values for credit cards\nimport_cc_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;creditcards&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/cc_details_train.csv&quot;])),\n    entity_id_field=&quot;cc_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v1&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v2&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v3&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v4&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v5&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v6&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v7&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v8&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v9&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v10&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v11&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v12&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v13&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v14&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v15&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v16&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v17&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v18&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v19&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v20&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v21&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v22&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v23&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v24&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v25&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v26&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v27&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;v28&quot;),\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;amount&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_cc_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n\n## Import features values for frauds\nimport_fraud_request = aiplatform.gapic.ImportFeatureValuesRequest(\n    entity_type=client.entity_type_path(\n        PROJECT_ID, REGION, FEATURESTORE_ID, &quot;frauds&quot;),\n    csv_source=aiplatform.gapic.CsvSource(gcs_source=aiplatform.gapic.GcsSource(\n        uris=[&quot;gs:\/\/fraud-detection-19102021\/dataset\/data_fraud_train.csv&quot;])),\n    entity_id_field=&quot;fraud_id&quot;,\n    feature_specs=[\n        aiplatform.gapic.ImportFeatureValuesRequest.FeatureSpec(id=&quot;class&quot;),\n    ],\n    feature_time_field='time',\n    worker_count=1,\n)\n\n## Start to import\ningestion_lro = client.import_feature_values(import_fraud_request)\n\n## Polls for the LRO status and prints when the LRO has completed\ningestion_lro.result()\n<\/code><\/pre>\n<p>When I check the <code>Ingestion Jobs<\/code> from the <code>Feature<\/code> section of Google Cloud Console I see that the job has finished but no values are added to my features.<\/p>\n<p>Any advice it is really precious.<\/p>\n<p>Thank you all.<\/p>\n<p><strong>EDIT 1<\/strong>\nIn the image below there is an example of the first row of the csv file I used as input (<code>cc_details_train.csv<\/code>). All the unseen features  are similar, the feature <code>class<\/code> can assume 0 or 1 values.\nThe injection job lasts about 5 minutes to import (ideally) 3000 rows, but it ends without error and without importing any value.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Z34hG.png\" alt=\"Rows of my csv file\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_created_time":1635242363740,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1636446864247,
        "Question_score":1,
        "Question_view_count":357.0,
        "Poster_id":15469635.0,
        "Poster_created_time":1616589293616,
        "Poster_last_access_time":1663860372103,
        "Poster_age":null,
        "Poster_location":"Alatri, Frosinone, FR",
        "Poster_reputation":67.0,
        "Poster_views":33.0,
        "Answer_body":"<p><strong>VERTEX AI recomendations when using CSV to ImportValues \/ using ImportFeatureValuesRequest<\/strong><\/p>\n<p>Its possible that when using this feature you might end not able to import any data at all. You must pay attention to the time field you are using as it must be in compliance with google time formats.<\/p>\n<ol>\n<li>feature_time_field, must follow the time constraint rule set by google which is RFC3339, ie: '2021-04-15T08:28:14Z'. You can check details about the field <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.featurestores.entityTypes\/importFeatureValues#request-body\" rel=\"nofollow noreferrer\">here<\/a> and details about timestamp format can be found <a href=\"https:\/\/developers.google.com\/protocol-buffers\/docs\/reference\/google.protobuf#timestamp\" rel=\"nofollow noreferrer\">here<\/a>.<\/li>\n<li>Other columns, fields must match is designed value. One exception is field entity_id_field, As it can be any value.<\/li>\n<\/ol>\n<p>Note: I my test i found that if i do not properly set up the time field as google recommended date format it will just not upload any feature value at all.<\/p>\n<p><em>test.csv<\/em><\/p>\n<pre><code>cc_id,time,v1,v2,v3,v4,v5,v6,v7,v8,v9,v10,v11,v12,v13,v14,v15,v16,v17,v18,v19,v20,v21,v22,v23,v24,v25,v26,v27,v28,amount\n100,2021-04-15T08:28:14Z,-1.359807,-0.072781,2.534897,1.872351,2.596267,0.465238,0.923123,0.347986,0.987354,1.234657,2.128645,1.958237,0.876123,-1.712984,-0.876436,1.74699,-1.645877,-0.936121,1.456327,0.087623,1.900872,2.876234,1.874123,0.923451,0.123432,0.000012,1.212121,0.010203,1000\n<\/code><\/pre>\n<p><em>output:<\/em><\/p>\n<pre><code>imported_entity_count: 1\nimported_feature_value_count: 29\n<\/code><\/pre>\n<p><strong>About optimization and working with features<\/strong><\/p>\n<p>You can check the official documentation <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-text#single-label-classification\" rel=\"nofollow noreferrer\">here<\/a> to see the min and max amount of records recommended for processing. As a piece of advice you should only use the actual working features to run and the recommended amount of values for it.<\/p>\n<p><strong>See your running ingested job<\/strong><\/p>\n<p>Either if you use VertexUI or code to generated the ingested job. You can track its run by going into the UI to this path:<\/p>\n<pre><code>VertexAI &gt; Features &gt; View Ingested Jobs \n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1638471768683,
        "Answer_score":1.0,
        "Answerer_id":14551567.0,
        "Answerer_created_time":1604093818187,
        "Answerer_last_access_time":1664028741440,
        "Answerer_age":null,
        "Answerer_location":"Krakow, Poland",
        "Answerer_reputation":1200.0,
        "Answerer_views":263.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69721067",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1636364178672,
        "Question_original_content":"googleapicallerror unexpect state long run oper respons error set new googl cloud platform try creat featur store valu csv file googl cloud storag aim local notebook python basic follow code make appropri chang work credit card public dataset error rais run code follow googleapicallerror unexpect state long run oper respons error set happen ingest data csv file code work import datetim import datetim googl cloud import bigqueri googl cloud import aiplatform googl cloud aiplatform type import featur featur googl cloud aiplatform type import featurestor featurestor googl cloud aiplatform type import featurestor servic featurestor servic googl cloud aiplatform type import entiti type entiti type googl cloud aiplatform type import featureselector idmatch credenti path user json environ googl applic credenti credenti path constant project project region central api endpoint central aiplatform googleapi com input csv file input file csv featurestor fraud detect output dataset destin data set fraud predict timestamp datetim strftime destin data set prefix timestamp format prefix destin data set timestamp timestamp output tabl sure tabl exist batchreadfeaturevalu api overwrit exist tabl destin tabl train data destin pattern project dataset tabl destin tabl uri destin pattern format project project dataset destin data set tabl destin tabl creat dataset client bigqueri client project project dataset format client project destin data set dataset bigqueri dataset dataset dataset locat region dataset client creat dataset dataset print creat dataset format client project dataset dataset creat client crud data client read featur valu client aiplatform gapic featurestoreservicecli client option api endpoint api endpoint data client aiplatform gapic featurestoreonlineservingservicecli client option api endpoint api endpoint base resourc path client common locat path project region creat featurestor time creat lro client creat featurestor featurestor servic createfeaturestorerequest parent base resourc path featurestor featurestor featurestor featurestor featurestor onlin serv config featurestor featurestor onlineservingconfig fix node count wait lro finish lro result print creat lro result client featurestor client featurestor path project region featurestor creat credit card entiti type time entiti type lro client creat entiti type featurestor servic createentitytyperequest parent client featurestor path project region featurestor entiti type creditcard entiti type entiti type entitytyp descript credit card entiti creat fraud entiti type time fraud entiti type lro client creat entiti type featurestor servic createentitytyperequest parent client featurestor path project region featurestor entiti type fraud entiti type entiti type entitytyp descript fraud entiti creat featur credit card type time client batch creat featur parent client entiti type path project region featurestor creditcard request featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur result creat featur fraud type time client batch creat featur parent client entiti type path project region featurestor fraud request featurestor servic createfeaturerequest featur featur featur valu type featur featur valuetyp doubl descript featur class result import featur valu credit card import request aiplatform gapic importfeaturevaluesrequest entiti type client entiti type path project region featurestor creditcard csv sourc aiplatform gapic csvsourc gc sourc aiplatform gapic gcssourc uri fraud detect dataset detail train csv entiti field featur spec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec aiplatform gapic importfeaturevaluesrequest featurespec featur time field time worker count start import ingest lro client import featur valu import request poll lro statu print lro complet ingest lro result import featur valu fraud import fraud request aiplatform gapic importfeaturevaluesrequest entiti type client entiti type path project region featurestor fraud csv sourc aiplatform gapic csvsourc gc sourc aiplatform gapic gcssourc uri fraud detect dataset data fraud train csv entiti field fraud featur spec aiplatform gapic importfeaturevaluesrequest featurespec class featur time field time worker count start import ingest lro client import featur valu import fraud request poll lro statu print lro complet ingest lro result check ingest job featur section googl cloud consol job finish valu ad featur advic preciou thank edit imag exampl row csv file input detail train csv unseen featur similar featur class assum valu inject job last minut import ideal row end error import valu",
        "Question_preprocessed_content":"googleapicallerror unexpect state oper respons error set new googl cloud platform try creat featur store valu csv file googl cloud storag aim local notebook python basic follow code make appropri chang work credit card public dataset error rais run code follow happen ingest data csv file code work check section googl cloud consol job finish valu ad featur advic preciou thank edit imag exampl row csv file input unseen featur similar featur assum valu inject job last minut import row end error import valu",
        "Question_gpt_summary_original":"the user is encountering an error while attempting to create a feature store from a local notebook in python and ingest data from a csv file on google cloud storage.",
        "Question_gpt_summary":"user encount error attempt creat featur store local notebook python ingest data csv file googl cloud storag",
        "Answer_original_content":"recomend csv importvalu importfeaturevaluesrequest possibl featur end abl import data pai attent time field complianc googl time format featur time field follow time constraint rule set googl rfc check detail field detail timestamp format column field match design valu except field entiti field valu note test properli set time field googl recommend date format upload featur valu test csv time output import entiti count import featur valu count optim work featur check offici document min max record recommend process piec advic us actual work featur run recommend valu run ingest job us vertexui code gener ingest job track run go path vertexai featur view ingest job",
        "Answer_preprocessed_content":"recomend csv importvalu importfeaturevaluesrequest possibl featur end abl import data pai attent time field complianc googl time format follow time constraint rule set googl rfc check detail field detail timestamp format column field match design valu except field valu note test properli set time field googl recommend date format upload featur valu output optim work featur check offici document min max record recommend process piec advic us actual work featur run recommend valu run ingest job us vertexui code gener ingest job track run go path",
        "Answer_gpt_summary_original":"Solutions provided:\n- The time field used for importing data from a CSV file to a feature store must follow the time constraint rule set by Google, which is RFC3339. If the time field is not properly set up, no feature value will be uploaded.\n- Other columns and fields must match their designed value, except for the entity_id_field, which can be any value.\n- It is recommended to use the actual working features and the recommended amount of values for processing.\n- The ingested job can be tracked through the VertexUI or by going to VertexAI > Features > View Ingested Jobs.",
        "Answer_gpt_summary":"solut provid time field import data csv file featur store follow time constraint rule set googl rfc time field properli set featur valu upload column field match design valu entiti field valu recommend us actual work featur recommend valu process ingest job track vertexui go vertexai featur view ingest job"
    },
    {
        "Question_title":"what is the difference between using a hugging face estimator with training script and directly using a notebook in AWS sagemaker?",
        "Question_body":"<p>in tutorials like <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/fine-tuning-a-pytorch-bert-model-and-deploying-it-with-amazon-elastic-inference-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Fine-tuning a pytorch bert model and deploying it with sagemaker<\/a> and <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/fine-tune-and-host-hugging-face-bert-models-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">fine-tune and host huggingface models on sagemaker<\/a>, a hugging face estimator is used to call a training script. What would be the difference if I just directly ran the script's code in the notebook itself? is it because the estimator makes it easier to deploy the model?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1646317438317,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":97.0,
        "Poster_id":14142074.0,
        "Poster_created_time":1597997723910,
        "Poster_last_access_time":1661243820287,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":115.0,
        "Poster_views":2.0,
        "Answer_body":"<p>You could run the script in the notebook itself but it would not deploy with SageMaker provided capabilities then. The estimator that you are seeing is what specifies to SageMaker what framework you are using and the training script that you are passing in. If you ran the script code in the notebook that would be like training in your local environment. By passing in the script to the Estimator you are running a SageMaker training job. The estimator is meant to encapsulate training on SageMaker.<\/p>\n<p>SageMaker Estimator Overview: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":16504640.0,
        "Answerer_created_time":1626973229036,
        "Answerer_last_access_time":1663708000863,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":199.0,
        "Answerer_views":37.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71338750",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1646330906792,
        "Question_original_content":"differ hug face estim train script directli notebook tutori like fine tune pytorch bert model deploi fine tune host huggingfac model hug face estim train script differ directli ran script code notebook estim make easier deploi model",
        "Question_preprocessed_content":"differ hug face estim train script directli notebook tutori like pytorch bert model deploi host huggingfac model hug face estim train script differ directli ran script code notebook estim make easier deploi model",
        "Question_gpt_summary_original":"The user is seeking to understand the difference between using a hugging face estimator with a training script and directly using a notebook in AWS Sagemaker. They are questioning whether the estimator makes it easier to deploy the model and are wondering if there are any other differences between the two methods.",
        "Question_gpt_summary":"user seek understand differ hug face estim train script directli notebook question estim make easier deploi model wonder differ method",
        "Answer_original_content":"run script notebook deploi provid capabl estim see specifi framework train script pass ran script code notebook like train local environ pass script estim run train job estim meant encapsul train estim overview http readthedoc stabl overview html",
        "Answer_preprocessed_content":"run script notebook deploi provid capabl estim see specifi framework train script pass ran script code notebook like train local environ pass script estim run train job estim meant encapsul train estim overview",
        "Answer_gpt_summary_original":"Solution: The discussion explains that running the script in the notebook itself would not deploy with SageMaker provided capabilities. The estimator specifies to SageMaker what framework is being used and the training script that is being passed in. By passing in the script to the Estimator, a SageMaker training job is run. The estimator is meant to encapsulate training on SageMaker. Therefore, using the estimator makes it easier to deploy the model and there are differences between using a hugging face estimator with a training script and directly using a notebook in AWS Sagemaker.",
        "Answer_gpt_summary":"solut discuss explain run script notebook deploi provid capabl estim specifi framework train script pass pass script estim train job run estim meant encapsul train estim make easier deploi model differ hug face estim train script directli notebook"
    },
    {
        "Question_title":"Track power\/energy consumption?",
        "Question_body":"<p>Hey all,<\/p>\n<p>I wondered if there was a way to track system power consumption caused by model development? I\u2019ve checked the W&amp;B docs and can\u2019t see anything.<br>\nIdeally I\u2019d love to be able to keep track of runs to see how much power is used by different runs but also the whole project.<\/p>\n<p>Elsewhere I\u2019ve seen packages such as <a href=\"https:\/\/pypi.org\/project\/energyusage\/\" rel=\"noopener nofollow ugc\">energyusage<\/a> but ideally would like to use something more integrated and could be aggregated across runs for whole projects.<br>\nIf something already exists I\u2019d love to hear about it, otherwise either if W&amp;B fancied adding this functionality that would be great or if it came to it if anyone would like to help me with this project.<\/p>\n<p>Thanks,<\/p>\n<p>Jeff.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1658216876213,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":50.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Hi,<br>\nWe have this example that shows how to do this with CodeCarbon.<br>\n<a href=\"https:\/\/wandb.ai\/amanarora\/codecarbon\/reports\/Tracking-CO2-Emissions-of-Your-Deep-Learning-Models-with-CodeCarbon-and-Weights-Biases--VmlldzoxMzM1NDg3\">https:\/\/wandb.ai\/amanarora\/codecarbon\/reports\/Tracking-CO2-Emissions-of-Your-Deep-Learning-Models-with-CodeCarbon-and-Weights-Biases\u2013VmlldzoxMzM1NDg3<\/a><\/p>\n<p>You would use that library and log the info yourself. I do appreciate the feature request to integrate these more tightly.<br>\nThanks, hope this helps<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/track-power-energy-consumption\/2774",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1658218095948,
        "Question_original_content":"track power energi consumpt hei wonder wai track power consumpt caus model develop iv check doc ideal love abl track run power differ run project iv seen packag energyusag ideal like us integr aggreg run project exist love hear fanci ad function great came like help project thank jeff",
        "Question_preprocessed_content":"track consumpt hei wonder wai track power consumpt caus model develop iv check doc ideal love abl track run power differ run project iv seen packag energyusag ideal like us integr aggreg run project exist love hear fanci ad function great came like help project thank jeff",
        "Question_gpt_summary_original":"The user is looking for a way to track the power\/energy consumption caused by model development. They have checked the W&B docs but couldn't find anything. They are looking for a more integrated solution that can be aggregated across runs for whole projects. The user is open to suggestions and is willing to work on a project to develop this functionality.",
        "Question_gpt_summary":"user look wai track power energi consumpt caus model develop check doc couldn look integr solut aggreg run project user open suggest will work project develop function",
        "Answer_original_content":"exampl show codecarbon http amanarora codecarbon report track emiss deep learn model codecarbon weight biasesvmlldzoxmzmndg us librari log info appreci featur request integr tightli thank hope help",
        "Answer_preprocessed_content":"exampl show codecarbon us librari log info appreci featur request integr tightli thank hope help",
        "Answer_gpt_summary_original":"Solution: The user can use CodeCarbon library to track power\/energy consumption caused by model development. They can log the information themselves and use it to aggregate across runs for whole projects. However, there is no integrated solution available at the moment.",
        "Answer_gpt_summary":"solut user us codecarbon librari track power energi consumpt caus model develop log inform us aggreg run project integr solut avail moment"
    },
    {
        "Question_title":"What should be used for deployed_model in DeployModelRequest in Vertex AI pipeline?",
        "Question_body":"<p>i am trying to deploy a model in Vertex AI pipeline component using <code>DeployModelRequest<\/code>. I try to get the model using <code>GetModelRequest<\/code><\/p>\n<pre><code>    model_name = f'projects\/{project}\/locations\/{location}\/models\/{model_id}'\n    model_request = aiplatform_v1.types.GetModelRequest(name=model_name)\n    model_info = client_model.get_model(request=model_request)       \n    \n    deploy_request = aiplatform_v1.types.DeployModelRequest(endpoint=end_point, \n                                                                deployed_model=model_info)\n    client.deploy_model(request=deploy_request)\n<\/code><\/pre>\n<p>but this gives:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected \ngoogle.cloud.aiplatform.v1.DeployedModel got Model\n<\/code><\/pre>\n<p>I have also tried <code>deployed_model=model_info.deployed_models[0]<\/code> but this gave:<\/p>\n<pre><code>TypeError: Parameter to MergeFrom() must be instance of same class: expected\n google.cloud.aiplatform.v1.DeployedModel got DeployedModelRef.\n<\/code><\/pre>\n<p>So what do I use for <code>deployed_model<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_created_time":1663254259473,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":39.0,
        "Poster_id":1773592.0,
        "Poster_created_time":1351154914716,
        "Poster_last_access_time":1663927832783,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":2564.0,
        "Poster_views":451.0,
        "Answer_body":"<p>As simple as:<\/p>\n<pre><code>machine_spec = MachineSpec(machine_type=&quot;n1-standard-2&quot;)\ndedicated_resources = DedicatedResources(machine_spec=machine_spec, \n                                         min_replica_count=1, \n                                         max_replica_count=1)\ndepmodel= DeployedModel(model=model_name, dedicated_resources=dedicated_resources) \n\ndeploy_request = aiplatform_v1.types.DeployModelRequest(\n                   endpoint=end_point, deployed_model=depmodel, \n                   traffic_split={'0':100})\nclient.deploy_model(request=deploy_request)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":1773592.0,
        "Answerer_created_time":1351154914716,
        "Answerer_last_access_time":1663927832783,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":2564.0,
        "Answerer_views":451.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73733464",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1663344904267,
        "Question_original_content":"deploi model deploymodelrequest pipelin try deploi model pipelin compon deploymodelrequest try model getmodelrequest model project project locat locat model model model request aiplatform type getmodelrequest model model info client model model request model request deploi request aiplatform type deploymodelrequest endpoint end point deploi model model info client deploi model request deploi request give typeerror paramet mergefrom instanc class expect googl cloud aiplatform deployedmodel got model tri deploi model model info deploi model gave typeerror paramet mergefrom instanc class expect googl cloud aiplatform deployedmodel got deployedmodelref us deploi model",
        "Question_preprocessed_content":"deploymodelrequest pipelin try deploi model pipelin compon try model give tri gave us",
        "Question_gpt_summary_original":"The user is facing challenges while trying to deploy a model in Vertex AI pipeline component using DeployModelRequest. They are trying to get the model using GetModelRequest, but are encountering a TypeError while passing the deployed_model parameter. The user has tried passing model_info.deployed_models[0] as the parameter, but it resulted in another TypeError. The user is seeking guidance on what to use for the deployed_model parameter.",
        "Question_gpt_summary":"user face challeng try deploi model pipelin compon deploymodelrequest try model getmodelrequest encount typeerror pass deploi model paramet user tri pass model info deploi model paramet result typeerror user seek guidanc us deploi model paramet",
        "Answer_original_content":"simpl machin spec machinespec machin type standard dedic resourc dedicatedresourc machin spec machin spec min replica count max replica count depmodel deployedmodel model model dedic resourc dedic resourc deploi request aiplatform type deploymodelrequest endpoint end point deploi model depmodel traffic split client deploi model request deploi request",
        "Answer_preprocessed_content":"simpl",
        "Answer_gpt_summary_original":"Solution: The user can use the following code to deploy a model in Vertex AI pipeline component using DeployModelRequest:\n\n```\nmachine_spec = MachineSpec(machine_type=\"n1-standard-2\")\ndedicated_resources = DedicatedResources(machine_spec=machine_spec, \n                                         min_replica_count=1, \n                                         max_replica_count=1)\ndepmodel= DeployedModel(model=model_name, dedicated_resources=dedicated_resources) \n\ndeploy_request = aiplatform_v1.types.DeployModelRequest(\n                   endpoint=end_point, deployed_model=depmodel, \n                   traffic_split={'0':100})\nclient.deploy_model(request=deploy_request)\n```\n\nThis code creates a machine specification, dedicated resources, and a deployed model object, and then uses",
        "Answer_gpt_summary":"solut user us follow code deploi model pipelin compon deploymodelrequest machin spec machinespec machin type standard dedic resourc dedicatedresourc machin spec machin spec min replica count max replica count depmodel deployedmodel model model dedic resourc dedic resourc deploi request aiplatform type deploymodelrequest endpoint end point deploi model depmodel traffic split client deploi model request deploi request code creat machin specif dedic resourc deploi model object us"
    },
    {
        "Question_title":"What does this python function signature means in Kedro Tutorial?",
        "Question_body":"<p>I am looking at Kedro Library as my team are looking into using it for our data pipeline.<\/p>\n<p>While going to the offical tutorial - Spaceflight.<\/p>\n<p>I came across this function:<\/p>\n<pre><code>def preprocess_companies(companies: pd.DataFrame) -&gt; pd.DataFrame:\n&quot;&quot;&quot;Preprocess the data for companies.\n\n    Args:\n        companies: Source data.\n    Returns:\n        Preprocessed data.\n\n&quot;&quot;&quot;\n\ncompanies[&quot;iata_approved&quot;] = companies[&quot;iata_approved&quot;].apply(_is_true)\n\ncompanies[&quot;company_rating&quot;] = companies[&quot;company_rating&quot;].apply(_parse_percentage)\n\nreturn companies\n<\/code><\/pre>\n<ul>\n<li>companies is the name of the csv file containing the data<\/li>\n<\/ul>\n<p>Looking at the function, my assumption is that <code>(companies: pd.Dafarame)<\/code> is the shorthand to read the &quot;companies&quot; dataset as a dataframe. If so, I do not understand what does <code>-&gt; pd.Dataframe<\/code> at the end means<\/p>\n<p>I tried looking at python documentation regarding such style of code but I did not managed to find any<\/p>\n<p>Much help is appreciated to assist me in understanding this.<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1613060367020,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":135.0,
        "Poster_id":8557751.0,
        "Poster_created_time":1504515330836,
        "Poster_last_access_time":1652156152220,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":5.0,
        "Poster_views":6.0,
        "Answer_body":"<p>The <code>-&gt;<\/code> notation is <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html\" rel=\"nofollow noreferrer\">type hinting<\/a>, as is the <code>:<\/code> part in the <code>companies: pd.DataFrame<\/code> function definition. This is not essential to do in Python but many people like to include it. The function definition would work exactly the same if it didn't contain this but instead read:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def preprocess_companies(companies):\n<\/code><\/pre>\n<p>This is a general Python thing rather than anything kedro-specific.<\/p>\n<p>The way that kedro registers <code>companies<\/code> as a kedro dataset is completely separate from this function definition and is done through the catalog.yml file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>companies:\n  type: pandas.CSVDataSet\n  filepath: data\/01_raw\/companies.csv\n<\/code><\/pre>\n<p>There will then a <em>node<\/em> defined (in pipeline.py) to specify that the <code>preprocess_companies<\/code> function should take as input the kedro dataset <code>companies<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>node(\n    func=preprocess_companies,\n    inputs=&quot;companies&quot;,  # THIS LINE REFERS TO THE DATASET NAME\n    outputs=&quot;preprocessed_companies&quot;,\n    name=&quot;preprocessing_companies&quot;,\n),\n<\/code><\/pre>\n<p>In theory the name of the parameter in the function itself could be completely different, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def preprocess_companies(anything_you_want):\n<\/code><\/pre>\n<p>... although it is very common to give it the same name as the dataset.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1613067465070,
        "Answer_score":0.0,
        "Answerer_id":15192378.0,
        "Answerer_created_time":1613062428296,
        "Answerer_last_access_time":1664006854880,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":141.0,
        "Answerer_views":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66158536",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1613062868172,
        "Question_original_content":"python function signatur mean tutori look librari team look data pipelin go offic tutori spaceflight came function def preprocess compani compani datafram datafram preprocess data compani arg compani sourc data return preprocess data compani iata approv compani iata approv appli true compani compani rate compani compani rate appli pars percentag return compani compani csv file contain data look function assumpt compani dafaram shorthand read compani dataset datafram understand datafram end mean tri look python document style code manag help appreci assist understand thank",
        "Question_preprocessed_content":"python function signatur mean tutori look librari team look data pipelin go offic tutori spaceflight came function compani csv file contain data look function assumpt shorthand read compani dataset datafram understand end mean tri look python document style code manag help appreci assist understand thank",
        "Question_gpt_summary_original":"The user is encountering challenges in understanding the meaning of the function signature in Kedro Tutorial. Specifically, they are unsure about the purpose of the \"-> pd.DataFrame\" at the end of the function signature. They have attempted to find information on this style of code in Python documentation but have been unsuccessful.",
        "Question_gpt_summary":"user encount challeng understand mean function signatur tutori specif unsur purpos datafram end function signatur attempt inform style code python document unsuccess",
        "Answer_original_content":"notat type hint compani datafram function definit essenti python peopl like includ function definit work exactli contain instead read def preprocess compani compani gener python thing specif wai regist compani dataset complet separ function definit catalog yml file compani type panda csvdataset filepath data raw compani csv node defin pipelin specifi preprocess compani function input dataset compani node func preprocess compani input compani line refer dataset output preprocess compani preprocess compani theori paramet function complet differ def preprocess compani want common dataset",
        "Answer_preprocessed_content":"notat type hint function definit essenti python peopl like includ function definit work exactli contain instead read gener python thing specif wai regist dataset complet separ function definit file node defin specifi function input dataset theori paramet function complet differ common dataset",
        "Answer_gpt_summary_original":"Solution:\n- The \"-> pd.DataFrame\" at the end of the function signature is a type hinting notation, which is not essential but many people like to include it.\n- The function definition would work exactly the same if it didn't contain this notation.\n- The way that Kedro registers datasets is completely separate from this function definition and is done through the catalog.yml file.\n- A node is defined in pipeline.py to specify that the function should take as input the Kedro dataset. \n- In theory, the name of the parameter in the function itself could be completely different, although it is very common to give it the same name as the dataset.",
        "Answer_gpt_summary":"solut datafram end function signatur type hint notat essenti peopl like includ function definit work exactli contain notat wai regist dataset complet separ function definit catalog yml file node defin pipelin specifi function input dataset theori paramet function complet differ common dataset"
    },
    {
        "Question_title":"How to handle a .csv input for use in Tensorflow Serving batch transform?",
        "Question_body":"<p><strong>Information:<\/strong>\nI am loading an existing trained model.tar.gz from an S3 bucket, and want to perform a batch transform with a .csv containing the input data. The data.csv is structured in such a way that reading it into a pandas DataFrame gives me rows of complete prediction inputs.<\/p>\nNotes:\n<ul>\n<li>This is done on Amazon Sagemaker using the Python SDK<\/li>\n<li>BATCH_TRANSFORM_INPUT is the path to data.csv.<\/li>\n<li>I'm able to load the contents inside model.tar.gz and use them for inference on my local machine using tensorflow, and the logs show <code>2020-08-04 13:35:01.123557: I tensorflow_serving\/core\/loader_harness.cc:87] Successfully loaded servable version {name: model version: 1}<\/code>so the model seems to have been trained and saved properly.<\/li>\n<li>The data.csv is in the exact same format as the training data, which means one row per &quot;prediction&quot; where all columns in that row represents the different features.<\/li>\n<li>Changing the argument strategy to 'MultiRecord' gives the same error<\/li>\n<li>[path in s3] is a substitute for the real path as i don't want to reveal any bucket information.<\/li>\n<li>TensorFlow ModelServer: 2.0.0+dev.sha.ab786af<\/li>\n<li>TensorFlow Library: 2.0.2<\/li>\n<\/ul>\n<p>Where 1-5 are features, the file data.csv looks like:<\/p>\n<pre><code>+------+-------------------------+---------+----------+---------+----------+----------+\n| UNIT | TS                      | 1       | 2        | 3       | 4        | 5        |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:01:00.000 | 1.81766 | 0.178043 | 1.33607 | 25.42162 | 12.85445 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:02:00.000 | 1.81673 | 0.178168 | 1.30159 | 25.48204 | 12.87305 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:03:00.000 | 1.8155  | 0.176242 | 1.38399 | 25.35309 | 12.47222 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:04:00.000 | 1.81530 | 0.176398 | 1.39781 | 25.18216 | 12.16837 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n| 110  | 2018-01-01 00:05:00.000 | 1.81505 | 0.151682 | 1.38451 | 25.22351 | 12.41623 |\n+------+-------------------------+---------+----------+---------+----------+----------+\n<\/code><\/pre>\n<p>inference.py currently looks like:<\/p>\n<pre><code>def input_handler(data, context):\n    import pandas as pd\n    if context.request_content_type == 'text\/csv':\n        payload = pd.read_csv(data)\n        instance = [{&quot;dataset&quot;: payload}]\n        return json.dumps({&quot;instances&quot;: instance})\n    else:\n        _return_error(416, 'Unsupported content type &quot;{}&quot;'.format(context.request_content_type or 'Unknown'))\n<\/code><\/pre>\n<h3>The problem:<\/h3>\n<p>When the following code runs in my jupyter Notebook:<\/p>\n<pre><code>sagemaker_model = Model(model_data = '[path in s3]\/savedmodel\/model.tar.gz'),  \n                        sagemaker_session=sagemaker_session,\n                        role = role,\n                        framework_version='2.0',\n                        entry_point = os.path.join('training', 'inference.py')\n                        )\n\ntf_serving_transformer = sagemaker_model.transformer(instance_count=1,\n                                                     instance_type='ml.p2.xlarge',\n                                                     max_payload=1,\n                                                     output_path=BATCH_TRANSFORM_OUTPUT_DIR,\n                                                     strategy='SingleRecord')\n\n\ntf_serving_transformer.transform(data=BATCH_TRANSFORM_INPUT, data_type='S3Prefix', content_type='text\/csv')\ntf_serving_transformer.wait()\n<\/code><\/pre>\n<p>The model seems to get loaded, but I end up with the following error:\n<code>2020-08-04T09:54:27.415:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=1, BatchStrategy=SINGLE_RECORD 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: ClientError: 400 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv:  2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: Message: 2020-08-04T09:54:27.503:[sagemaker logs]: [path in s3]\/data.csv: { &quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: \\&quot;\\&quot; Type: String is not of expected type: float&quot; } <\/code><\/p>\n<p>Error more clearly:<\/p>\n<p><strong>ClientError: 400\nMessage: {&quot;error&quot;: &quot;Failed to process element: 0 of 'instances' list. Error: Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float&quot;}<\/strong><\/p>\n<p>If i understand this error correctly, something is wrong with the way my data is structured, so that sagemaker fails to deliver the input data to the TFS model. I suppose there is some &quot;input handling&quot; missing in my inference.py. Maybe the csv data has to somehow be translated into a compatible JSON, for TFS to use it? What exactly has to be done in input_handler() ?<\/p>\n<p>I appreciate all help, and am sorry for this confusing case. If there is any additional information needed, please ask and I'll gladly provide what I can.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1596549653107,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1596696699176,
        "Question_score":2,
        "Question_view_count":774.0,
        "Poster_id":14048343.0,
        "Poster_created_time":1596548853443,
        "Poster_last_access_time":1597839282567,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":41.0,
        "Poster_views":2.0,
        "Answer_body":"<p><strong>Solution:<\/strong> The problem was solved by saving the dataframe as .csv using the arguments header=False, index=False. This makes the saved csv not include the dataframe indexing labels. TFS accepted a clean .csv with only float values (without labels). I assume the error message <em>Invalid argument: JSON Value: &quot;&quot; Type: String is not of expected type: float<\/em> refers to the first cell in the csv, which if the csv was exported with labels is just an empty cell. When it got an empty string instead of a float value it got confused.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":14048343.0,
        "Answerer_created_time":1596548853443,
        "Answerer_last_access_time":1597839282567,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":41.0,
        "Answerer_views":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63248562",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1596696668183,
        "Question_original_content":"handl csv input us tensorflow serv batch transform inform load exist train model tar bucket want perform batch transform csv contain input data data csv structur wai read panda datafram give row complet predict input note python sdk batch transform input path data csv abl load content insid model tar us infer local machin tensorflow log tensorflow serv core loader har successfulli load servabl version model version model train save properli data csv exact format train data mean row predict column row repres differ featur chang argument strategi multirecord give error path substitut real path want reveal bucket inform tensorflow modelserv dev sha abaf tensorflow librari featur file data csv look like unit infer current look like def input handler data context import panda context request content type text csv payload read csv data instanc dataset payload return json dump instanc instanc return error unsupport content type format context request content type unknown problem follow code run jupyt notebook model model model data path savedmodel model tar session session role role framework version entri point path join train infer serv transform model transform instanc count instanc type xlarg max payload output path batch transform output dir strategi singlerecord serv transform transform data batch transform input data type sprefix content type text csv serv transform wait model load end follow error log maxconcurrenttransform maxpayloadinmb batchstrategi singl record log path data csv clienterror log path data csv log path data csv messag log path data csv error fail process element instanc list error invalid argument json valu type string expect type float error clearli clienterror messag error fail process element instanc list error invalid argument json valu type string expect type float understand error correctli wrong wai data structur fail deliv input data tf model suppos input handl miss infer mayb csv data translat compat json tf us exactli input handler appreci help sorri confus case addit inform need ask gladli provid",
        "Question_preprocessed_content":"handl csv input us tensorflow serv batch transform inform load exist train bucket want perform batch transform csv contain input data structur wai read panda datafram give row complet predict input note python sdk path abl load content insid us infer local machin tensorflow log model train save properli exact format train data mean row predict column row repres differ featur chang argument strategi multirecord give error path substitut real path want reveal bucket inform tensorflow modelserv tensorflow librari featur file look like current look like problem follow code run jupyt notebook model load end follow error error clearli clienterror messag understand error correctli wrong wai data structur fail deliv input data tf model suppos input handl miss mayb csv data translat compat json tf us exactli appreci help sorri confus case addit inform need ask gladli provid",
        "Question_gpt_summary_original":"The user is encountering challenges in performing a batch transform with a .csv file containing input data for a pre-trained model in Amazon Sagemaker using the Python SDK. The data.csv is structured in a way that reading it into a pandas DataFrame gives rows of complete prediction inputs. The error message suggests that there is something wrong with the way the data is structured, and the user suspects that there is some input handling missing in the inference.py file. The user is seeking help in translating the csv data into a compatible JSON for the TensorFlow Serving model to use.",
        "Question_gpt_summary":"user encount challeng perform batch transform csv file contain input data pre train model python sdk data csv structur wai read panda datafram give row complet predict input error messag suggest wrong wai data structur user suspect input handl miss infer file user seek help translat csv data compat json tensorflow serv model us",
        "Answer_original_content":"solut problem solv save datafram csv argument header fals index fals make save csv includ datafram index label tf accept clean csv float valu label assum error messag invalid argument json valu type string expect type float refer cell csv csv export label cell got string instead float valu got confus",
        "Answer_preprocessed_content":"solut problem solv save datafram csv argument header fals index fals make save csv includ datafram index label tf accept clean csv float valu assum error messag invalid argument json valu type string expect type float refer cell csv csv export label cell got string instead float valu got confus",
        "Answer_gpt_summary_original":"Solution: The problem was solved by saving the dataframe as .csv using the arguments header=False, index=False. This makes the saved csv not include the dataframe indexing labels. TFS accepted a clean .csv with only float values (without labels).",
        "Answer_gpt_summary":"solut problem solv save datafram csv argument header fals index fals make save csv includ datafram index label tf accept clean csv float valu label"
    },
    {
        "Question_title":"Cant install imbalanced-learn on an Azure ML Environment",
        "Question_body":"<p>I have an Azure ML Workspace which comes by default with some pre-installed packages.<\/p>\n<p>I tried to install<\/p>\n<pre><code>!pip install -U imbalanced-learn\n<\/code><\/pre>\n<p>But I got this error<\/p>\n<pre><code>Requirement already up-to-date: scikit-learn in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (0.24.2)\nRequirement already satisfied, skipping upgrade: scipy&gt;=0.19.1 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.4.1)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: numpy&gt;=1.13.3 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (1.18.5)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from scikit-learn) (2.1.0)\nCollecting imbalanced-learn\n  Using cached imbalanced_learn-0.9.0-py3-none-any.whl (199 kB)\nRequirement already satisfied, skipping upgrade: threadpoolctl&gt;=2.0.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (2.1.0)\nRequirement already satisfied, skipping upgrade: joblib&gt;=0.11 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (0.14.1)\nRequirement already satisfied, skipping upgrade: scipy&gt;=1.1.0 in \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages (from imbalanced-learn) (1.4.1)\nERROR: Could not find a version that satisfies the requirement scikit-learn&gt;=1.0.1 (from imbalanced-learn) (from versions: 0.9, 0.10, 0.11, 0.12, 0.12.1, 0.13, 0.13.1, 0.14, 0.14.1, 0.15.0b1, 0.15.0b2, 0.15.0, 0.15.1, 0.15.2, 0.16b1, 0.16.0, 0.16.1, 0.17b1, 0.17, 0.17.1, 0.18, 0.18.1, 0.18.2, 0.19b2, 0.19.0, 0.19.1, 0.19.2, 0.20rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.20.4, 0.21rc2, 0.21.0, 0.21.1, 0.21.2, 0.21.3, 0.22rc2.post1, 0.22rc3, 0.22, 0.22.1, 0.22.2, 0.22.2.post1, 0.23.0rc1, 0.23.0, 0.23.1, 0.23.2, 0.24.dev0, 0.24.0rc1, 0.24.0, 0.24.1, 0.24.2)\nERROR: No matching distribution found for scikit-learn&gt;=1.0.1 (from imbalanced-\n<\/code><\/pre>\n<p>learn)<\/p>\n<p>Not sure how to solve this, I have read in other posts to use conda, but that didnt work either.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1644933997077,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1644960360047,
        "Question_score":1,
        "Question_view_count":219.0,
        "Poster_id":1014217.0,
        "Poster_created_time":1302030303092,
        "Poster_last_access_time":1663332147472,
        "Poster_age":null,
        "Poster_location":"Brussels, B\u00e9lgica",
        "Poster_reputation":30340.0,
        "Poster_views":2937.0,
        "Answer_body":"<p><a href=\"https:\/\/pypi.org\/project\/scikit-learn\/1.0.1\/\" rel=\"nofollow noreferrer\"><code>scikit-learn<\/code> 1.0.1<\/a> and up require Python &gt;= 3.7; you use Python 3.6. You need to upgrade Python or downgrade <code>imbalanced-learn<\/code>. <a href=\"https:\/\/pypi.org\/project\/imbalanced-learn\/0.8.1\/\" rel=\"nofollow noreferrer\"><code>imbalanced-learn<\/code> 0.8.1<\/a> allows Python 3.6 so<\/p>\n<pre><code>!pip install -U &quot;imbalanced-learn &lt; 0.9&quot;\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":7976758.0,
        "Answerer_created_time":1494171603136,
        "Answerer_last_access_time":1664058701196,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":73187.0,
        "Answerer_views":8473.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71127858",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1644935036627,
        "Question_original_content":"instal imbalanc learn environ workspac come default pre instal packag tri instal pip instal imbalanc learn got error requir date scikit learn anaconda env lib python site packag requir satisfi skip upgrad scipi anaconda env lib python site packag scikit learn requir satisfi skip upgrad joblib anaconda env lib python site packag scikit learn requir satisfi skip upgrad numpi anaconda env lib python site packag scikit learn requir satisfi skip upgrad threadpoolctl anaconda env lib python site packag scikit learn collect imbalanc learn cach imbalanc learn whl requir satisfi skip upgrad threadpoolctl anaconda env lib python site packag imbalanc learn requir satisfi skip upgrad joblib anaconda env lib python site packag imbalanc learn requir satisfi skip upgrad scipi anaconda env lib python site packag imbalanc learn error version satisfi requir scikit learn imbalanc learn version post post dev error match distribut scikit learn imbalanc learn sure solv read post us conda didnt work",
        "Question_preprocessed_content":"instal environ workspac come default packag tri instal got error learn sure solv read post us conda didnt work",
        "Question_gpt_summary_original":"The user is facing challenges installing the imbalanced-learn package on an Azure ML Workspace. The user encountered an error message indicating that the required version of scikit-learn could not be found. The user has attempted to use conda to solve the issue but was unsuccessful.",
        "Question_gpt_summary":"user face challeng instal imbalanc learn packag workspac user encount error messag indic requir version scikit learn user attempt us conda solv issu unsuccess",
        "Answer_original_content":"scikit learn requir python us python need upgrad python downgrad imbalanc learn imbalanc learn allow python pip instal imbalanc learn",
        "Answer_preprocessed_content":"requir python us python need upgrad python downgrad allow python",
        "Answer_gpt_summary_original":"Solution: Downgrading the imbalanced-learn package to version 0.8.1 or lower can solve the issue. The user can use the following command to install the compatible version of the package: \n\n```\n!pip install -U \"imbalanced-learn < 0.9\"\n```",
        "Answer_gpt_summary":"solut downgrad imbalanc learn packag version lower solv issu user us follow command instal compat version packag pip instal imbalanc learn"
    },
    {
        "Question_title":"Deep learning training on Azure steps and tutorial",
        "Question_body":"Hi MSFT Community,\n\nI followed this guide to set up a GPU: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-ubuntu-intro\n\nVM: Standard NC12_Promo, 12 vCPUs, 112 Gib RAM\nOperating System: Linux\nOffer: Ubuntu-1804\n\nI am ready to start deep learning training but I am confused about what to do next. I am doing a medical image classification project. I have 1 millions images store in Azure blob now. Do I need to download them to my VM in order to train? Or is it a better way to access image efficiently?\n\nWhat are some good tutorials to set up the experiments? I've read a lot of documentation but still confused.\n\nThank you very much!\nBest Regards,\nClaire",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1605040090287,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@gecheng-2063\ncheck on the below AI training modules.\nhttps:\/\/aischool.microsoft.com\/en-us\/services\/learning-paths\n\nAI Lab\nhttps:\/\/www.microsoft.com\/en-us\/ai\/ai-lab-projects\n\nAI module gallery\nhttps:\/\/gallery.azure.ai\/browse\n\n\n\n\nPlease don\u2019t forget to \"Accept the answer\" and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/158168\/deep-learning-training-on-azure-steps-and-tutorial.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1605057040680,
        "Question_original_content":"deep learn train azur step tutori msft commun follow guid set gpu http doc microsoft com azur machin learn data scienc virtual machin dsvm ubuntu intro standard promo vcpu gib ram oper linux offer ubuntu readi start deep learn train confus medic imag classif project million imag store azur blob need download order train better wai access imag effici good tutori set experi read lot document confus thank best regard clair",
        "Question_preprocessed_content":"deep learn train azur step tutori msft commun follow guid set gpu standard vcpu gib ram oper linux offer readi start deep learn train confus medic imag classif project million imag store azur blob need download order train better wai access imag effici good tutori set experi read lot document confus thank best regard clair",
        "Question_gpt_summary_original":"The user is facing challenges in setting up deep learning training on Azure for a medical image classification project. They are unsure about whether they need to download 1 million images stored in Azure blob to their VM for training and are looking for tutorials to help them set up the experiments.",
        "Question_gpt_summary":"user face challeng set deep learn train azur medic imag classif project unsur need download million imag store azur blob train look tutori help set experi",
        "Answer_original_content":"gecheng check train modul http aischool microsoft com servic learn path lab http microsoft com lab project modul galleri http galleri azur brows dont forget accept answer vote inform provid help benefici commun member",
        "Answer_preprocessed_content":"check train modul lab modul galleri dont forget accept answer inform provid help benefici commun member",
        "Answer_gpt_summary_original":"Solution: The discussion provides links to AI training modules, AI Lab projects, and AI module gallery on Microsoft Azure. These resources can help the user set up deep learning training for their medical image classification project. No specific solution is mentioned regarding whether the user needs to download 1 million images stored in Azure blob to their VM for training.",
        "Answer_gpt_summary":"solut discuss provid link train modul lab project modul galleri microsoft azur resourc help user set deep learn train medic imag classif project specif solut mention user need download million imag store azur blob train"
    },
    {
        "Question_title":"matplotlib does not upgrade beyound 3.2.1",
        "Question_body":"<p>I am using Notebook on Azure Machine Learning Studio and I am trying to install latest version of matplotlib, but it continues using version 3.2.1 I tried conda, pip, and also tried to uninstall and install again, nothing works.\nsample commands tried:<\/p>\n<pre><code>!pip install matplotlib\n!conda install matplotlib\n<\/code><\/pre>\n<p>Am I missing sth? thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_created_time":1658878043067,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":68.0,
        "Poster_id":9723370.0,
        "Poster_created_time":1525124885907,
        "Poster_last_access_time":1663975416143,
        "Poster_age":null,
        "Poster_location":"Los Angeles, CA, USA",
        "Poster_reputation":65.0,
        "Poster_views":13.0,
        "Answer_body":"<p>The version of <strong>Matplotlib<\/strong> will depend on the version of python. There is no support for the latest version of matplotlib library for <strong>python version 3.8<\/strong>. The latest version of matplotlib can be available only up to <strong>3.7 version<\/strong> of python. Even though we tried to upgrade it with <strong>pip, conda<\/strong> , by default <strong>version of python program<\/strong>, it will take the supporting version of matplotlib.<\/p>\n<p>For the current version of matplotlib in the current case of the situation, it is <strong>3.2<\/strong>. It is suggestable to check the version of python, whether it is 3.6 or not. <strong>Python version 3.6 supports 3.2 and 3.1 versions of matplotlib<\/strong>.<\/p>\n<p>If still the python version is showing the latest version of python, try to <strong>degrade<\/strong> the version and check.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":18428148.0,
        "Answerer_created_time":1646907459852,
        "Answerer_last_access_time":1664066617772,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":1624.0,
        "Answerer_views":1376.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73130723",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659334721720,
        "Question_original_content":"matplotlib upgrad beyound notebook studio try instal latest version matplotlib continu version tri conda pip tri uninstal instal work sampl command tri pip instal matplotlib conda instal matplotlib miss sth thank",
        "Question_preprocessed_content":"matplotlib upgrad beyound notebook studio try instal latest version matplotlib continu version tri conda pip tri uninstal instal work sampl command tri miss sth thank",
        "Question_gpt_summary_original":"The user is facing challenges upgrading matplotlib beyond version 3.2.1 on Azure Machine Learning Studio Notebook. The user has tried various methods such as conda, pip, and uninstalling and reinstalling, but none of them have worked.",
        "Question_gpt_summary":"user face challeng upgrad matplotlib version studio notebook user tri method conda pip uninstal reinstal work",
        "Answer_original_content":"version matplotlib depend version python support latest version matplotlib librari python version latest version matplotlib avail version python tri upgrad pip conda default version python program support version matplotlib current version matplotlib current case situat suggest check version python python version support version matplotlib python version show latest version python try degrad version check",
        "Answer_preprocessed_content":"version matplotlib depend version python support latest version matplotlib librari python version latest version matplotlib avail version python tri upgrad pip conda default version python program support version matplotlib current version matplotlib current case situat suggest check version python python version support version matplotlib python version show latest version python try degrad version check",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n- The latest version of Matplotlib library is only available up to version 3.7 of Python, so upgrading beyond version 3.2.1 may not be possible.\n- Matplotlib version 3.2 is currently available in the situation, and it is suggested to check if the Python version is 3.6, which supports versions 3.2 and 3.1 of Matplotlib.\n- If the Python version is still showing the latest version, try downgrading the version and check.",
        "Answer_gpt_summary":"possibl solut mention discuss latest version matplotlib librari avail version python upgrad version possibl matplotlib version current avail situat suggest check python version support version matplotlib python version show latest version try downgrad version check"
    },
    {
        "Question_title":"No such file or directory: 'docker': 'docker' when running sagemaker studio in local mode",
        "Question_body":"<p>I try to train a pytorch model on amazon sagemaker studio.<\/p>\n\n<p>It's working when I use an EC2 for training with:<\/p>\n\n<pre><code>estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                sagemaker_session = sess,\n                train_instance_count=1,\n                train_instance_type='ml.c5.xlarge',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>and it's work on local mode in classic sagemaker notebook (non studio) with:<\/p>\n\n<pre><code> estimator = PyTorch(entry_point='train_script.py',\n                role=role,\n                train_instance_count=1,\n                train_instance_type='local',\n                framework_version='1.4.0', \n                source_dir='.',\n                git_config=git_config, \n               )\nestimator.fit({'stockdata': data_path})\n<\/code><\/pre>\n\n<p>But when I use it the same code (with train_instance_type='local') on sagemaker studio it doesn't work and I have the following error: No such file or directory: 'docker': 'docker'<\/p>\n\n<p>I tried to install docker with pip install but the docker command is not found if use it in terminal<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1588239469173,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":5,
        "Question_view_count":1884.0,
        "Poster_id":12523234.0,
        "Poster_created_time":1576136255052,
        "Poster_last_access_time":1663858972456,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":795.0,
        "Poster_views":37.0,
        "Answer_body":"<p>This indicates that there is a problem finding the Docker service.<\/p>\n<p>By default, the Docker is not installed in the SageMaker Studio  (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/656#issuecomment-632170943\" rel=\"nofollow noreferrer\">confirming github ticket response<\/a>).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1620410809823,
        "Answer_score":7.0,
        "Answerer_id":2687601.0,
        "Answerer_created_time":1376606307612,
        "Answerer_last_access_time":1664042118956,
        "Answerer_age":null,
        "Answerer_location":"Vancouver, BC, Canada",
        "Answerer_reputation":1567.0,
        "Answerer_views":159.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61520346",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1596561017227,
        "Question_original_content":"file directori docker docker run studio local mode try train pytorch model studio work us train estim pytorch entri point train script role role session sess train instanc count train instanc type xlarg framework version sourc dir git config git config estim fit stockdata data path work local mode classic notebook non studio estim pytorch entri point train script role role train instanc count train instanc type local framework version sourc dir git config git config estim fit stockdata data path us code train instanc type local studio work follow error file directori docker docker tri instal docker pip instal docker command us termin",
        "Question_preprocessed_content":"file directori docker docker run studio local mode try train pytorch model studio work us train work local mode classic notebook us code studio work follow error file directori docker docker tri instal docker pip instal docker command us termin",
        "Question_gpt_summary_original":"The user is encountering an error when trying to train a PyTorch model on Amazon SageMaker Studio in local mode. The same code works on an EC2 instance and in classic SageMaker notebook in local mode, but not in SageMaker Studio. The error message states \"No such file or directory: 'docker': 'docker'\" and the user has attempted to install docker with pip but the command is not found in the terminal.",
        "Question_gpt_summary":"user encount error try train pytorch model studio local mode code work instanc classic notebook local mode studio error messag state file directori docker docker user attempt instal docker pip command termin",
        "Answer_original_content":"indic problem find docker servic default docker instal studio confirm github ticket respons",
        "Answer_preprocessed_content":"indic problem find docker servic default docker instal studio",
        "Answer_gpt_summary_original":"Solution: The user needs to install Docker in SageMaker Studio to resolve the error. By default, Docker is not installed in SageMaker Studio, as confirmed by a response to a Github ticket.",
        "Answer_gpt_summary":"solut user need instal docker studio resolv error default docker instal studio confirm respons github ticket"
    },
    {
        "Question_title":"What are SageMaker pipelines actually?",
        "Question_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1638395443060,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3,
        "Question_view_count":716.0,
        "Poster_id":13155217.0,
        "Poster_created_time":1578250359256,
        "Poster_last_access_time":1663527351312,
        "Poster_age":null,
        "Poster_location":"Amsterdam",
        "Poster_reputation":197.0,
        "Poster_views":49.0,
        "Answer_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1654197850167,
        "Answer_score":2.0,
        "Answerer_id":223478.0,
        "Answerer_created_time":1259808393296,
        "Answerer_last_access_time":1664074603568,
        "Answerer_age":null,
        "Answerer_location":"Vancouver, Canada",
        "Answerer_reputation":44706.0,
        "Answerer_views":4356.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1638396070903,
        "Question_original_content":"pipelin actual pipelin unclear experienc field work figur pipelin definit question pipelin stand servic featur option creat consol cloudform cdk resourc pipelin essenti codepipelin integr differ python sdk differ cdk cloudform exampl python sdk usag come doc workshop properli python sdk usag help clear",
        "Question_preprocessed_content":"pipelin actual pipelin unclear experienc field work figur pipelin definit question pipelin option creat consol cloudform cdk resourc pipelin essenti codepipelin integr differ python sdk differ cdk cloudform exampl python sdk usag come doc workshop properli python sdk usag help clear",
        "Question_gpt_summary_original":"The user is facing challenges in understanding SageMaker pipelines and has several questions regarding its features and integration with other services such as CloudFormation and CDK. They are also confused about the differences between SageMaker pipelines and codepipeline, as well as the Python SDK and CloudFormation. The user is having difficulty finding examples of SageMaker pipelines and is seeking clarification on its usage.",
        "Question_gpt_summary":"user face challeng understand pipelin question featur integr servic cloudform cdk confus differ pipelin codepipelin python sdk cloudform user have difficulti find exampl pipelin seek clarif usag",
        "Answer_original_content":"thing call pipelin model build pipelin serial infer pipelin believ refer model build pipelin defin step machin learn workflow pre process hyperparamet tune batch transform set endpoint serial infer pipelin model run model build pipelin defin json host run sort proprietari serverless fashion pipelin stand servic featur option creat consol cloudform cdk resourc creat modifi api call cli python sdk cloudform us aw api hood start stop view studio left navig bar resourc drop menu pipelin pipelin essenti codepipelin integr differ unlik codepipelin build deploi code specif direct integr far tell start pipelin python sdk differ cdk cloudform python sdk stand librari interact develop friendli fashion dynam cloudform let build pipelin code cloudform take static json string simpl exampl python sdk usag processor sklearnprocessor framework version instanc count instanc type larg role role arn process step processingstep process processor processor code preprocessor pipelin pipelin foo step process step pipelin upsert role arn pipelin start pipelin definit produc verbos json like version metadata paramet pipelineexperimentconfig experimentnam execut pipelinenam trialnam execut pipelineexecutionid step process type process argument processingresourc clusterconfig instancetyp larg instancecount volumesizeingb appspecif imageuri dkr ecr west amazonaw com scikit learn cpu containerentrypoint python opt process input code preprocessor rolearn arn aw iam role foo processinginput inputnam code appmanag fals sinput suri bucket preprocessor localpath opt process input code sdatatyp sprefix sinputmod file sdatadistributiontyp fullyrepl scompressiontyp us json cloudform cdk build json sdk defin model build workflow step function state machin data scienc sdk airflow",
        "Answer_preprocessed_content":"thing call pipelin model build pipelin serial infer pipelin believ refer model build pipelin defin step machin learn workflow hyperparamet tune batch transform set endpoint serial infer pipelin model run model build pipelin defin json sort proprietari serverless fashion pipelin option creat consol cloudform cdk resourc api call cli python sdk cloudform us aw api hood studio pipelin essenti codepipelin integr differ unlik codepipelin build deploi code specif direct integr far tell start pipelin python sdk differ cdk cloudform python sdk librari interact fashion dynam cloudform let build pipelin code cloudform take static json string simpl exampl python sdk usag produc verbos json like us json build json sdk defin model build workflow step function state machin data scienc sdk airflow",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- SageMaker has two types of pipelines: Model Building Pipelines and Serial Inference Pipelines.\n- A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints.\n- A model building pipeline is defined in JSON and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker.\n- You can create\/modify SageMaker pipelines using the API, which can also be called via the CLI, Python SDK, or CloudFormation.\n- You can start\/stop\/view SageMaker pipelines in SageMaker Studio.\n- CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration",
        "Answer_gpt_summary":"possibl solut mention discuss type pipelin model build pipelin serial infer pipelin model build pipelin defin step machin learn workflow pre process hyperparamet tune batch transform set endpoint model build pipelin defin json host run sort proprietari serverless fashion creat modifi pipelin api call cli python sdk cloudform start stop view pipelin studio codepipelin build deploi code specif direct integr"
    },
    {
        "Question_title":"When should I use Azure ML Notebooks VS Azure Databricks? Both are competitor products in my opinion",
        "Question_body":"<p>Pretty self-explanatory question. When should I use Azure ML Notebooks VS Azure Databricks? I feel there\u2019s a great overlap between the two products and one is definitely better marketed than the other.. <\/p>\n\n<p>I\u2019m mainly looking for information concerning datasets sizes and typical workflow. Why should I use Databricks over AzureML if I don\u2019t have a Spark oriented workflow ?<\/p>\n\n<p>Thanks !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1585769120130,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1625712808263,
        "Question_score":7,
        "Question_view_count":3755.0,
        "Poster_id":5554046.0,
        "Poster_created_time":1447320137140,
        "Poster_last_access_time":1663776204940,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":313.0,
        "Poster_views":40.0,
        "Answer_body":"<p>@Nethim, from my pov these are the main difference:<br><\/p>\n\n<ol>\n<li><p>Data Distribution:<\/p>\n\n<ul>\n<li>Azure ML Notebooks are good when you are training with a limited data on single machine. Though Azure ML provides training clusters, the data distribution among the nodes is to be handled in the code.<\/li>\n<li>Azure Databricks with its RDDs are designed to handle data distributed on multiple nodes.This is advantageous when your data size is huge.When your data size is small and can fit in a scaled up single machine\/ you are using a pandas dataframe, then use of Azure databricks is a overkill<\/li>\n<\/ul><\/li>\n<li><p>Data Cleaning:\nDatabricks can support a lot of file formats natively and querying and cleaning huge datasets are easy where as this has to be handled custom in AzureML notebooks. This can be done with a aml notebooks but cleaning and writing to stores has to be handled.<\/p><\/li>\n<li>Training\nBoth has the capabilities if distributing the training, Databricks provides inbuilt ML algorithms that can act on chunk of data on that node and coordinate with other nodes. Though this can be done on both AzureMachineLearning and Databricks with tf,horovod etc.,<\/li>\n<\/ol>\n\n<p>In general(just my opinion), if the dataset is small, aml notebooks is good.If the data size is huge, then Azure databricks is easy for datacleanup and format conversions.Then the training can happen on AML or databricks.Though databricks has a learning curve whereas Azure ML can be easy with the python and pandas.<\/p>\n\n<p>Thanks.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Answerer_id":12451380.0,
        "Answerer_created_time":1574939238203,
        "Answerer_last_access_time":1663968375932,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":350.0,
        "Answerer_views":34.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60978808",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1585823280848,
        "Question_original_content":"us notebook azur databrick competitor product opinion pretti self explanatori question us notebook azur databrick feel there great overlap product definit better market mainli look inform concern dataset size typic workflow us databrick dont spark orient workflow thank",
        "Question_preprocessed_content":"us notebook azur databrick competitor product opinion pretti question us notebook azur databrick feel there great overlap product definit better market mainli look inform concern dataset size typic workflow us databrick dont spark orient workflow thank",
        "Question_gpt_summary_original":"The user is seeking information on when to use Azure ML Notebooks versus Azure Databricks, as they feel there is significant overlap between the two products. They are specifically looking for guidance on dataset sizes and typical workflows, and are curious about why they should use Databricks over Azure ML if they do not have a Spark-oriented workflow.",
        "Question_gpt_summary":"user seek inform us notebook versu azur databrick feel signific overlap product specif look guidanc dataset size typic workflow curiou us databrick spark orient workflow",
        "Answer_original_content":"nethim pov main differ data distribut notebook good train limit data singl machin provid train cluster data distribut node handl code azur databrick rdd design handl data distribut multipl node advantag data size huge data size small fit scale singl machin panda datafram us azur databrick overkil data clean databrick support lot file format nativ queri clean huge dataset easi handl custom notebook aml notebook clean write store handl train capabl distribut train databrick provid inbuilt algorithm act chunk data node coordin node azuremachinelearn databrick horovod gener opinion dataset small aml notebook good data size huge azur databrick easi datacleanup format convers train happen aml databrick databrick learn curv easi python panda thank",
        "Answer_preprocessed_content":"pov main differ data distribut notebook good train limit data singl machin provid train cluster data distribut node handl code azur databrick rdd design handl data distribut multipl advantag data size data size small fit scale singl machin panda datafram us azur databrick overkil data clean databrick support lot file format nativ queri clean huge dataset easi handl custom notebook aml notebook clean write store handl train capabl distribut train databrick provid inbuilt algorithm act chunk data node coordin node azuremachinelearn databrick horovod gener dataset small aml notebook data size huge azur databrick easi datacleanup format train happen aml databrick learn curv easi python panda thank",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are:\n\n- Azure ML Notebooks are good when training with limited data on a single machine, while Azure Databricks is designed to handle data distributed on multiple nodes, which is advantageous when the data size is huge.\n- Databricks can support a lot of file formats natively and querying and cleaning huge datasets are easy, whereas this has to be handled custom in Azure ML Notebooks.\n- Both Azure ML and Databricks have the capabilities of distributing the training, but Databricks provides inbuilt ML algorithms that can act on a chunk of data on that node and coordinate with other nodes.\n- In general, if the dataset is small, Azure ML Notebooks are good. If the data size is huge",
        "Answer_gpt_summary":"solut provid discuss notebook good train limit data singl machin azur databrick design handl data distribut multipl node advantag data size huge databrick support lot file format nativ queri clean huge dataset easi handl custom notebook databrick capabl distribut train databrick provid inbuilt algorithm act chunk data node coordin node gener dataset small notebook good data size huge"
    },
    {
        "Question_title":"C# async\/await to F# using Azure ML example",
        "Question_body":"<p>I am working with Azure ML and I have the code sample to invoke my web  service (alas it is only in C#).  Can someone help me translate this to F#?  I have everything but the async and await done.<\/p>\n\n<pre><code> static async Task InvokeRequestResponseService()\n        {\n            using (var client = new HttpClient())\n            {\n                ScoreData scoreData = new ScoreData()\n                {\n                    FeatureVector = new Dictionary&lt;string, string&gt;() \n                    {\n                        { \"Zip Code\", \"0\" },\n                        { \"Race\", \"0\" },\n                        { \"Party\", \"0\" },\n                        { \"Gender\", \"0\" },\n                        { \"Age\", \"0\" },\n                        { \"Voted Ind\", \"0\" },\n                    },\n                    GlobalParameters = new Dictionary&lt;string, string&gt;() \n                    {\n                    }\n                };\n\n                ScoreRequest scoreRequest = new ScoreRequest()\n                {\n                    Id = \"score00001\",\n                    Instance = scoreData\n                };\n\n                const string apiKey = \"abc123\"; \/\/ Replace this with the API key for the web service\n                client.DefaultRequestHeaders.Authorization = new AuthenticationHeaderValue( \"Bearer\", apiKey);\n\n                client.BaseAddress = new Uri(\"https:\/\/ussouthcentral.services.azureml.net\/workspaces\/19a2e623b6a944a3a7f07c74b31c3b6d\/services\/f51945a42efa42a49f563a59561f5014\/score\");\n                HttpResponseMessage response = await client.PostAsJsonAsync(\"\", scoreRequest);\n                if (response.IsSuccessStatusCode)\n                {\n                    string result = await response.Content.ReadAsStringAsync();\n                    Console.WriteLine(\"Result: {0}\", result);\n                }\n                else\n                {\n                    Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode);\n                }\n            }\n<\/code><\/pre>\n\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1410732744493,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1446025307110,
        "Question_score":2,
        "Question_view_count":607.0,
        "Poster_id":1728410.0,
        "Poster_created_time":1349689794400,
        "Poster_last_access_time":1648997159983,
        "Poster_age":null,
        "Poster_location":"Denver, CO, USA",
        "Poster_reputation":4174.0,
        "Poster_views":396.0,
        "Answer_body":"<p>I was not able to compile and run the code, but you probably need something like this:<\/p>\n\n<pre><code>let invokeRequestResponseService() = async {\n    use client = new HttpClient()\n    let scoreData = (...)\n    let apiKey = \"abc123\"\n    client.DefaultRequestHeaders.Authorization &lt;- \n        new AuthenticationHeaderValue(\"Bearer\", apiKey)\n    client.BaseAddress &lt;- Uri(\"https:\/\/ussouthcentral....\/score\");\n    let! response = client.PostAsJsonAsync(\"\", scoreRequest) |&gt; Async.AwaitTask\n    if response.IsSuccessStatusCode then\n        let! result = response.Content.ReadAsStringAsync() |&gt; Async.AwaitTask\n        Console.WriteLine(\"Result: {0}\", result);\n    else\n        Console.WriteLine(\"Failed with status code: {0}\", response.StatusCode) }\n<\/code><\/pre>\n\n<ul>\n<li><p>Wrapping the code in the <code>async { .. }<\/code> block makes it asynchronous and lets you use <code>let!<\/code> inside the block to perform asynchronous waiting (i.e. in places where you'd use <code>await<\/code> in C#)<\/p><\/li>\n<li><p>F# uses type <code>Async&lt;T&gt;<\/code> instead of .NET Task, so when you're awaiting a task, you need to insert <code>Async.AwaitTask<\/code> (or you can write wrappers for the most frequently used operations)<\/p><\/li>\n<li><p>The <code>invokeRequestResponseService()<\/code> function returns F# async, so if you need to pass it to some other library function (or if it needs to return a task), you can use <code>Async.StartAsTask<\/code><\/p><\/li>\n<\/ul>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Answerer_id":33518.0,
        "Answerer_created_time":1225669466307,
        "Answerer_last_access_time":1663936487652,
        "Answerer_age":null,
        "Answerer_location":"Cambridge, United Kingdom",
        "Answerer_reputation":236107.0,
        "Answerer_views":18730.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/25838512",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1410733358503,
        "Question_original_content":"async await exampl work code sampl invok web servic ala help translat async await static async task invokerequestresponseservic var client new httpclient scoredata scoredata new scoredata featurevector new dictionari zip code race parti gender ag vote ind globalparamet new dictionari scorerequest scorerequest new scorerequest score instanc scoredata const string apikei abc replac api kei web servic client defaultrequesthead author new authenticationheadervalu bearer apikei client baseaddress new uri http ussouthcentr servic net workspac aebaaafcbcbd servic faefaafaf score httpresponsemessag respons await client postasjsonasync scorerequest respons issuccetatuscod string result await respons content readasstringasync consol writelin result result consol writelin fail statu code respons statuscod thank",
        "Question_preprocessed_content":"exampl work code sampl invok web servic help translat async await thank",
        "Question_gpt_summary_original":"The user is facing a challenge in translating a C# code sample to F# for invoking a web service in Azure ML. The user has everything except for the async and await functions.",
        "Question_gpt_summary":"user face challeng translat code sampl invok web servic user async await function",
        "Answer_original_content":"abl compil run code probabl need like let invokerequestresponseservic async us client new httpclient let scoredata let apikei abc client defaultrequesthead author async awaittask respons issuccetatuscod let result respons content readasstringasync async awaittask consol writelin result result consol writelin fail statu code respons statuscod wrap code async block make asynchron let us let insid block perform asynchron wait place us await us type async instead net task await task need insert async awaittask write wrapper frequent oper invokerequestresponseservic function return async need pass librari function need return task us async startastask",
        "Answer_preprocessed_content":"abl compil run code probabl need like wrap code block make asynchron let us insid block perform asynchron wait us type instead net task await task need insert function return async need pass librari function us",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Wrap the code in the `async { .. }` block to make it asynchronous and use `let!` inside the block to perform asynchronous waiting.\n- Use type `Async<T>` instead of .NET Task, and insert `Async.AwaitTask` when awaiting a task.\n- The `invokeRequestResponseService()` function returns F# async, so use `Async.StartAsTask` if it needs to return a task.\n\nThese solutions were provided to help the user translate a C# code sample to F# for invoking a web service in Azure ML.",
        "Answer_gpt_summary":"possibl solut mention discuss wrap code async block asynchron us let insid block perform asynchron wait us type async instead net task insert async awaittask await task invokerequestresponseservic function return async us async startastask need return task solut provid help user translat code sampl invok web servic"
    },
    {
        "Question_title":"Google Cloud Vertex AI with Golang: rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found)",
        "Question_body":"<p>I have a Vertex AI model deployed on an endpoint and want to do some prediction from my app in Golang.<\/p>\n<p>To do this I create code inspired by this example : <a href=\"https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en\" rel=\"nofollow noreferrer\">https:\/\/cloud.google.com\/go\/docs\/reference\/cloud.google.com\/go\/aiplatform\/latest\/apiv1?hl=en<\/a><\/p>\n<pre><code>const file = &quot;MY_BASE64_IMAGE&quot;\n\nfunc main() {\n\n    ctx := context.Background()\n\n    c, err := aiplatform.NewPredictionClient(cox)\n    if err != nil {\n        log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n    }\n    defer c.Close()\n\n    parameters, err := structpb.NewValue(map[string]interface{}{\n        &quot;confidenceThreshold&quot;: 0.2,\n        &quot;maxPredictions&quot;:      5,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue parameters - Err:%s&quot;, err)\n    }\n\n    instance, err := structpb.NewValue(map[string]interface{}{\n        &quot;content&quot;: file,\n    })\n    if err != nil {\n        log.Printf(&quot;QueryVertex structpb.NewValue instance - Err:%s&quot;, err)\n    }\n\n    reqP := &amp;aiplatformpb.PredictRequest{\n        Endpoint:   &quot;projects\/PROJECT_ID\/locations\/LOCATION_ID\/endpoints\/ENDPOINT_ID&quot;,\n        Instances:  []*structpb.Value{instance},\n        Parameters: parameters,\n    }\n\n    resp, err := c.Predict(cox, reqP)\n    if err != nil {\n        log.Printf(&quot;QueryVertex Predict - Err:%s&quot;, err)\n    }\n\n    log.Printf(&quot;QueryVertex Res:%+v&quot;, resp)\n}\n<\/code><\/pre>\n<p>I put the path to my service account JSON file on GOOGLE_APPLICATION_CREDENTIALS environment variable.\nBut when I run my test app I obtain this error message:<\/p>\n<pre><code>QueryVertex Predict - Err:rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found); transport: received unexpected content-type &quot;text\/html; charset=UTF-8&quot;\nQueryVertex Res:&lt;nil&gt;\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_created_time":1651731616723,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1654669807780,
        "Question_score":3,
        "Question_view_count":455.0,
        "Poster_id":19040027.0,
        "Poster_created_time":1651730962056,
        "Poster_last_access_time":1663929720412,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":31.0,
        "Poster_views":1.0,
        "Answer_body":"<p>As @DazWilkin suggested, configure the client option to specify the specific <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest#service-endpoint\" rel=\"noreferrer\">regional endpoint<\/a> with a port 443:<\/p>\n<pre><code>option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;)\n<\/code><\/pre>\n<p>Try like below:<\/p>\n<pre><code>func main() {\n \n   ctx := context.Background()\n   c, err := aiplatform.NewPredictionClient(\n       ctx,\n       option.WithEndpoint(&quot;&lt;region&gt;-aiplatform.googleapis.com:443&quot;),\n   )\n   if err != nil {\n       log.Printf(&quot;QueryVertex NewPredictionClient - Err:%s&quot;, err)\n   }\n   defer c.Close()\n       .\n       .\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1652712010567,
        "Answer_score":5.0,
        "Answerer_id":15747106.0,
        "Answerer_created_time":1619174589310,
        "Answerer_last_access_time":1664047335000,
        "Answerer_age":null,
        "Answerer_location":"Bangalore, Karnataka, India",
        "Answerer_reputation":1200.0,
        "Answerer_views":804.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72122744",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1652647881720,
        "Question_original_content":"googl cloud golang rpc error code unimpl desc unexpect http statu code receiv server model deploi endpoint want predict app golang creat code inspir exampl http cloud googl com doc refer cloud googl com aiplatform latest apiv const file base imag func main ctx context background err aiplatform newpredictioncli cox err nil log printf queryvertex newpredictioncli err err defer close paramet err structpb newvalu map string interfac confidencethreshold maxpredict err nil log printf queryvertex structpb newvalu paramet err err instanc err structpb newvalu map string interfac content file err nil log printf queryvertex structpb newvalu instanc err err reqp aiplatformpb predictrequest endpoint project project locat locat endpoint endpoint instanc structpb valu instanc paramet paramet resp err predict cox reqp err nil log printf queryvertex predict err err log printf queryvertex re resp path servic account json file googl applic credenti environ variabl run test app obtain error messag queryvertex predict err rpc error code unimpl desc unexpect http statu code receiv server transport receiv unexpect content type text html charset utf queryvertex re",
        "Question_preprocessed_content":"googl cloud golang rpc error code unimpl desc unexpect http statu code receiv server model deploi endpoint want predict app golang creat code inspir exampl path servic account json file environ variabl run test app obtain error messag",
        "Question_gpt_summary_original":"The user is encountering an error while trying to do prediction from their Golang app using a Vertex AI model deployed on an endpoint. They have created code inspired by an example, but when they run their test app, they receive an error message stating \"rpc error: code = Unimplemented desc = unexpected HTTP status code received from server: 404 (Not Found).\" The error occurs despite setting the path to the service account JSON file on the GOOGLE_APPLICATION_CREDENTIALS environment variable.",
        "Question_gpt_summary":"user encount error try predict golang app model deploi endpoint creat code inspir exampl run test app receiv error messag state rpc error code unimpl desc unexpect http statu code receiv server error occur despit set path servic account json file googl applic credenti environ variabl",
        "Answer_original_content":"dazwilkin suggest configur client option specifi specif region endpoint port option withendpoint aiplatform googleapi com try like func main ctx context background err aiplatform newpredictioncli ctx option withendpoint aiplatform googleapi com err nil log printf queryvertex newpredictioncli err err defer close",
        "Answer_preprocessed_content":"suggest configur client option specifi specif region endpoint port try like",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to configure the client option to specify the specific regional endpoint with a port 443. The user can try setting the endpoint like this: \n\n```\noption.WithEndpoint(\"<region>-aiplatform.googleapis.com:443\")\n```\n\nThen, they can create a new prediction client and pass the context and endpoint option to it.",
        "Answer_gpt_summary":"solut solut suggest discuss configur client option specifi specif region endpoint port user try set endpoint like option withendpoint aiplatform googleapi com creat new predict client pass context endpoint option"
    },
    {
        "Question_title":"Sagemaker Notebook keeps hanging\/freezing",
        "Question_body":"I have been using Sagemaker Studio Notebook and suddenly it started hanging. When this happens, the notebook freezes completely. Than I have to wait some seconds (the delay duration is not constant and is common to reach about 30 seconds) and then it just freezes again, making its usage impossible. I was using a temporary account provided by Udacity and after trying different approaches to find and solve the problem, I switched to a personal account but the problem persists. Approaches I have tried so far:\n\nShutdow and start kernel\nRestart kernel\nRestart kernel and clear outputs\nLog out and Login (from Sagemaker)\nLog out and Login (from AWS)\nChange region\nTrying a different browser (I tried Chrome and Firefox)\nTrying using other account (personal)\n\nI also checked CloudWatch logs but didn't find anything that seemed unusual.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1657750167416,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":50.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"The most likely cause of this from my experience is a (very) large number of active git changes.\n\nGiven your \"current\" working folder (the one you're navigated to in the folder sidebar menu), the jupyterlab-git integration regularly checks if you're inside a git repository and polls for changes in that repository if so.\n\nWhen this list is very large, I've sometimes seen it cause significant slowdowns in the overall UI because of the way the underlying (open-source) extension works. This has been discussed before for example in this GitHub issue - which is now marked closed but I've still seen it happening.\n\nFor example, maybe you (like me \ud83d\ude05) forgot to gitignore a data folder or node_modules and generated thousands of untracked files there: You might see a significant slowdown whenever you're navigated to a folder within the scope of that git repo.\n\nSuggested solution would be:\n\nUse the folder sidebar to navigate anywhere other than the affected git repository (e.g. to your root folder?), and you should see the slowdown resolve pretty much immediately if this is the underlying cause\nNow the tricky task of finding and clearing up the problemmatic folder(s) without navigating to them in the folder GUI:\nYou could use a System Terminal, cd to the affected folder and run git status to see where the many changes are hiding, if you're not sure already\nAdd a .gitignore file (or modify your existing one) to make git ignore those changes. Because it starts with a dot, .gitignore is hidden by default in the JupyterLab file browser anyway. I usually use a system terminal to e.g. cp myrepo\/.gitignore gitignore.txt to create a visible copy (somewhere other than the repository folder which you're trying to avoid navigating to!) and then mv gitignore.txt myrepo\/.gitignore to overwrite with my edited version\n\nAlternatively (if e.g. it's a folder full of new files that you no longer care about like node_modules) you could just slog through the slowness to delete the problemmatic folder in the UI - but of course the problem would return if you re-created them later without .gitignore.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbUkR0L2-Q1CcAHtTbLYJmg\/sagemaker-notebook-keeps-hanging-freezing",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1657857148680,
        "Question_original_content":"notebook keep hang freez studio notebook suddenli start hang happen notebook freez complet wait second delai durat constant common reach second freez make usag imposs temporari account provid udac try differ approach solv problem switch person account problem persist approach tri far shutdow start kernel restart kernel restart kernel clear output log login log login aw chang region try differ browser tri chrome firefox try account person check cloudwatch log unusu",
        "Question_preprocessed_content":"notebook keep studio notebook suddenli start hang happen notebook freez complet wait second freez make usag imposs temporari account provid udac try differ approach solv problem switch person account problem persist approach tri far shutdow start kernel restart kernel restart kernel clear output log login log login chang region try differ browser try account check cloudwatch log unusu",
        "Question_gpt_summary_original":"The user is facing challenges with Sagemaker Studio Notebook as it keeps hanging and freezing, making it impossible to use. The delay duration is not constant and can reach up to 30 seconds. The user has tried different approaches such as shutting down and starting the kernel, restarting the kernel, clearing outputs, logging out and logging in from Sagemaker and AWS, changing the region, trying a different browser, and using a different account, but the problem persists. The user also checked CloudWatch logs but didn't find anything unusual.",
        "Question_gpt_summary":"user face challeng studio notebook keep hang freez make imposs us delai durat constant reach second user tri differ approach shut start kernel restart kernel clear output log log aw chang region try differ browser differ account problem persist user check cloudwatch log unusu",
        "Answer_original_content":"like caus experi larg number activ git chang given current work folder navig folder sidebar menu jupyterlab git integr regularli check insid git repositori poll chang repositori list larg seen caus signific slowdown overal wai underli open sourc extens work discuss exampl github issu mark close seen happen exampl mayb like forgot gitignor data folder node modul gener thousand untrack file signific slowdown navig folder scope git repo suggest solut us folder sidebar navig affect git repositori root folder slowdown resolv pretti immedi underli caus tricki task find clear problemmat folder navig folder gui us termin affect folder run git statu chang hide sure add gitignor file modifi exist git ignor chang start dot gitignor hidden default jupyterlab file browser usual us termin myrepo gitignor gitignor txt creat visibl copi repositori folder try avoid navig gitignor txt myrepo gitignor overwrit edit version altern folder new file longer care like node modul slog slow delet problemmat folder cours problem return creat later gitignor",
        "Answer_preprocessed_content":"like caus experi larg number activ git chang given current work folder integr regularli check insid git repositori poll chang repositori list larg seen caus signific slowdown overal wai underli extens work discuss exampl github issu mark close seen happen exampl mayb forgot gitignor data folder gener thousand untrack file signific slowdown navig folder scope git repo suggest solut us folder sidebar navig affect git repositori slowdown resolv pretti immedi underli caus tricki task find clear problemmat folder navig folder gui us termin affect folder run git statu chang hide sure add gitignor file git ignor chang start dot gitignor hidden default jupyterlab file browser usual us termin creat visibl copi overwrit edit version altern slog slow delet problemmat folder cours problem return later gitignor",
        "Answer_gpt_summary_original":"The suggested solution to the challenge is to check if the issue is caused by a large number of active git changes. If this is the case, the user should use the folder sidebar to navigate anywhere other than the affected git repository and add a .gitignore file to make git ignore those changes. If the user is not sure where the changes are hiding, they can use a System Terminal to cd to the affected folder and run git status. Alternatively, the user could slog through the slowness to delete the problematic folder in the UI, but the problem would return if they re-created them later without .gitignore.",
        "Answer_gpt_summary":"suggest solut challeng check issu caus larg number activ git chang case user us folder sidebar navig affect git repositori add gitignor file git ignor chang user sure chang hide us termin affect folder run git statu altern user slog slow delet problemat folder problem return creat later gitignor"
    },
    {
        "Question_title":"Azure Machine Learning (and cognitive services) is not supported in Region \"Germany\"?",
        "Question_body":"I'm working for a campany located in Germany. We want to use Azure Machine Learning (and other stuff like that).\nWe are only allowed to use Azure in the Region \"Germany\", because the data of our customers cannot left germany.\n\nNow I saw, that a lot of stuff in Azure Machine Learning is not available in Germany?\n\nQuestions:\n1. Is that true?\n2. Does some one now, at what time Microsoft plans to make the stuff available in Germany?\n\nThank you for a answer!\n\nPatrick",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1612860215177,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi @PatrickHuber-5684\nYes, Azure Machine Learning is not available in Germany region.\n\nPlease check in Azure feedback\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/265151\/azure-machine-learning-and-cognitive-services-is-n.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1612862196647,
        "Question_original_content":"cognit servic support region germani work campani locat germani want us stuff like allow us azur region germani data custom left germani saw lot stuff avail germani question true time microsoft plan stuff avail germani thank answer patrick",
        "Question_preprocessed_content":"support region germani work campani locat germani want us allow us azur region germani data custom left germani saw lot stuff avail germani question true time microsoft plan stuff avail germani thank answer patrick",
        "Question_gpt_summary_original":"The user is facing challenges in using Azure Machine Learning and other related services in Germany due to restrictions on customer data leaving the country. They have noticed that many features of Azure Machine Learning are not available in Germany and are seeking information on when these features will be made available.",
        "Question_gpt_summary":"user face challeng relat servic germani restrict custom data leav countri notic featur avail germani seek inform featur avail",
        "Answer_original_content":"patrickhub ye avail germani region check azur feedback answer help click accept answer vote benefici commun member",
        "Answer_preprocessed_content":"ye avail germani region check azur feedback answer help click accept answer benefici commun member",
        "Answer_gpt_summary_original":"Solution: No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"What is wrong with my experiment (Trying to predict car sales)?",
        "Question_body":"<p>I have the dataset like this (just a sample of it):<\/p>\n\n<pre><code>DATE_REF,MONTH,YEAR,DAY_OF_YEAR,DAY_OF_MONTH,WEEK_DAY,WEEK_DAY_1,WEEK_DAY_2,WEEK_DAY_3,WEEK_DAY_4,WEEK_DAY_5,WEEK_DAY_6,WEEK_DAY_7,WEEK_NUMBER_IN_MONTH,WEEKEND,WORK_DAY,AMOUNT_SOLD\n20100101,1,2010,1,1,6,0,0,0,0,0,1,0,1,0,0,0\n20100102,1,2010,2,2,7,0,0,0,0,0,0,1,1,1,0,2\n20100103,1,2010,3,3,1,1,0,0,0,0,0,0,2,1,0,0\n20100104,1,2010,4,4,2,0,1,0,0,0,0,0,2,0,1,12830\n20100105,1,2010,5,5,3,0,0,1,0,0,0,0,2,0,1,19200\n20100106,1,2010,6,6,4,0,0,0,1,0,0,0,2,0,1,22930\n20100107,1,2010,7,7,5,0,0,0,0,1,0,0,2,0,1,23495\n20100108,1,2010,8,8,6,0,0,0,0,0,1,0,2,0,1,23215\n20100109,1,2010,9,9,7,0,0,0,0,0,0,1,2,1,0,172\n20100110,1,2010,10,10,1,1,0,0,0,0,0,0,3,1,0,0\n20100111,1,2010,11,11,2,0,1,0,0,0,0,0,3,0,1,18815\n20100112,1,2010,12,12,3,0,0,1,0,0,0,0,3,0,1,25415\n20100113,1,2010,13,13,4,0,0,0,1,0,0,0,3,0,1,25262\n20100114,1,2010,14,14,5,0,0,0,0,1,0,0,3,0,1,27967\n20100115,1,2010,15,15,6,0,0,0,0,0,1,0,3,0,1,26352\n20100116,1,2010,16,16,7,0,0,0,0,0,0,1,3,1,0,202\n20100117,1,2010,17,17,1,1,0,0,0,0,0,0,4,1,0,10\n20100118,1,2010,18,18,2,0,1,0,0,0,0,0,4,0,1,20295\n20100119,1,2010,19,19,3,0,0,1,0,0,0,0,4,0,1,25982\n20100120,1,2010,20,20,4,0,0,0,1,0,0,0,4,0,1,24745\n20100121,1,2010,21,21,5,0,0,0,0,1,0,0,4,0,1,28087\n20100122,1,2010,22,22,6,0,0,0,0,0,1,0,4,0,1,28417\n20100123,1,2010,23,23,7,0,0,0,0,0,0,1,4,1,0,115\n20100124,1,2010,24,24,1,1,0,0,0,0,0,0,5,1,0,5\n20100125,1,2010,25,25,2,0,1,0,0,0,0,0,5,0,1,20185\n20100126,1,2010,26,26,3,0,0,1,0,0,0,0,5,0,1,25932\n20100127,1,2010,27,27,4,0,0,0,1,0,0,0,5,0,1,31710\n20100128,1,2010,28,28,5,0,0,0,0,1,0,0,5,0,1,21020\n20100129,1,2010,29,29,6,0,0,0,0,0,1,0,5,0,1,51460\n20100130,1,2010,30,30,7,0,0,0,0,0,0,1,5,1,0,670\n20100131,1,2010,31,31,1,1,0,0,0,0,0,0,6,1,0,17\n<\/code><\/pre>\n\n<p>I'm trying to predict the <code>AMOUNT_SOLD<\/code> for new dates (<code>DATE_REF<\/code>) using the following experiment on Azure ML:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/7Mfhs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/7Mfhs.png\" alt=\"Azure ML Experiment\"><\/a><\/p>\n\n<p>Then I deployed the Web Service and tested the prediction, but all I got was zero for the <code>AMOUNT_SOLD<\/code> column.<\/p>\n\n<p>What may I be missing? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1502159015170,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":178.0,
        "Poster_id":1184708.0,
        "Poster_created_time":1328174321836,
        "Poster_last_access_time":1659728310848,
        "Poster_age":null,
        "Poster_location":"Belo Horizonte - MG, Brasil",
        "Poster_reputation":7753.0,
        "Poster_views":743.0,
        "Answer_body":"<p>As much as I want to replicate your Azure ML experiment, I do not have enough data. But what I've done are as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XNaeg.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XNaeg.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I copied your sample data, and then multiplied it by 4 times (<strong>Add Rows x 2<\/strong>).\nThen <strong>Split Data<\/strong> (70%\/30%), random seed 7 (for reproducible results).\nThe <strong>Boosted Decision Tree Regression<\/strong> has default parameters.\nOn <strong>Tune Model Hyperparameters<\/strong>, I selected <strong><em>AMOUNT_SOLD<\/em><\/strong> as the label column.\nThen <strong>Score Model<\/strong> and <strong>Evaluate Model<\/strong>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/aIJlk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aIJlk.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Accuracy \/ Coefficient of Determination was pretty good.<\/p>\n\n<p>After that, to deploy this as a web service, you must setup first a Predictive Experiment from your Training Experiment. <code>Setup Web Service &gt; Predictive Experiment<\/code> You experiment will move like magic.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/gTEOl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/gTEOl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The <strong>Web Service Input<\/strong> module will be placed by default at the top of the experiment. I <strong>moved it and connected at the right side of Score Model<\/strong>, so that when you are inputting the parameters of your web service, it <em>will be predicted using your Trained Model<\/em>.<\/p>\n\n<p>After the Score Model module, I placed a <strong>Select Columns in Dataset<\/strong> module and selected only the column named <strong>Scored Labels<\/strong>. This column contains the model's predictions. Then I used <strong>Edit Metadata<\/strong> module to rename the Scored Labels column, before passing it to the <strong>Web Service Output<\/strong> module.<\/p>\n\n<p>Your experiment is now ready to deploy as a web service.<\/p>\n\n<p>To predict new values, I tested the web service using the current date details as input. (<strong>Although the DATE_REF input must be 20170818<\/strong> :D )<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fPm65.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fPm65.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And then the output looks like this:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/R6N4B.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/R6N4B.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Your web service can now predict new values.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":7777617.0,
        "Answerer_created_time":1490674180056,
        "Answerer_last_access_time":1655211268107,
        "Answerer_age":null,
        "Answerer_location":"%Temp%",
        "Answerer_reputation":302.0,
        "Answerer_views":39.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45558337",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1503036396008,
        "Question_original_content":"wrong experi try predict car sale dataset like sampl date ref month year dai year dai month week dai week dai week dai week dai week dai week dai week dai week dai week number month weekend work dai sold try predict sold new date date ref follow experi deploi web servic test predict got zero sold column miss",
        "Question_preprocessed_content":"wrong experi dataset like try predict new date follow experi deploi web servic test predict got zero column miss",
        "Question_gpt_summary_original":"The user is trying to predict car sales using a dataset and an experiment on Azure ML. However, when they deployed the web service and tested the prediction, they received zero for the AMOUNT_SOLD column. The user is seeking help to identify what they may be missing.",
        "Question_gpt_summary":"user try predict car sale dataset experi deploi web servic test predict receiv zero sold column user seek help identifi miss",
        "Answer_original_content":"want replic experi data follow copi sampl data multipli time add row split data random seed reproduc result boost decis tree regress default paramet tune model hyperparamet select sold label column score model evalu model accuraci coeffici determin pretti good deploi web servic setup predict experi train experi setup web servic predict experi experi like magic web servic input modul place default experi move connect right score model input paramet web servic predict train model score model modul place select column dataset modul select column name score label column contain model predict edit metadata modul renam score label column pass web servic output modul experi readi deploi web servic predict new valu test web servic current date detail input date ref input output look like web servic predict new valu",
        "Answer_preprocessed_content":"want replic experi data follow copi sampl data multipli time split data random seed boost decis tree regress default paramet tune model hyperparamet select label column score model evalu model accuraci coeffici determin pretti good deploi web servic setup predict experi train experi experi like magic web servic input modul place default experi move connect right score model input paramet web servic predict train model score model modul place select column dataset modul select column name score label column contain model predict edit metadata modul renam score label column pass web servic output modul experi readi deploi web servic predict new valu test web servic current date detail input output look like web servic predict new valu",
        "Answer_gpt_summary_original":"The discussion provides a step-by-step guide on how to deploy a web service using Azure ML. The user who posted the discussion replicated the experiment, split the data, used the Boosted Decision Tree Regression with default parameters, tuned the model hyperparameters, scored and evaluated the model, and then deployed the experiment as a web service. The user also moved the Web Service Input module and connected it to the right side of the Score Model module, selected only the Scored Labels column using the Select Columns in Dataset module, renamed the Scored Labels column using the Edit Metadata module, and passed it to the Web Service Output module. Finally, the user tested the web service using the current date details as input and received an output. No solution is provided for",
        "Answer_gpt_summary":"discuss provid step step guid deploi web servic user post discuss replic experi split data boost decis tree regress default paramet tune model hyperparamet score evalu model deploi experi web servic user move web servic input modul connect right score model modul select score label column select column dataset modul renam score label column edit metadata modul pass web servic output modul final user test web servic current date detail input receiv output solut provid"
    },
    {
        "Question_title":"Sagemaker taking an unexpectedly long time to download training data",
        "Question_body":"My customer's 220 Gb of training data took 54 minutes for Sagemaker to download. This is a rate of only 70 MB\/s, which is unexpectedly slow. He is accessing the data in S3 from his p3.8xlarge instance through a private VPC endpoint, so the theoretical maximum bandwidth is 25 Gbps. Is there anything that can be done to speed up the download?\n\nHe started the Sagemaker training with the following function:\n\nestimator = Estimator(image_name, role=role, output_path=output_location, train_instance_count=1, train_instance_type='ml.p3.8xlarge', train_volume_size=300, train_max_run = 52460*60 , security_group_ids='sg-00f1529adc4076841')\n\nThe output was: 2018-10-18 23:27:15 Starting - Starting the training job... Launching requested ML instances...... Preparing the instances for training... 2018-10-18 23:29:15 Downloading - Downloading input data............ .................................................................... .................................................................... .................................................................... 2018-10-19 00:23:50 Training - Downloading the training image..\n\nDataset download took ~54mins",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1540384039000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":654.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"How are they connect to S3? are they using a VPC endpoint \/ NAT? If they are using a VPC endpoint, My recommendation will be the open a support ticket, it's possible that support will be able to look at the network logs.\n\nAnother option for the customer is to use pipe input, pipe mode is recommended for large datasets, and it'll shorter their startup time because the data is being streamed instead of being downloaded to your training instances.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUPpqUS0ckRXCHW0BXgxV5wQ\/sagemaker-taking-an-unexpectedly-long-time-to-download-training-data",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1540622900000,
        "Question_original_content":"take unexpectedli long time download train data custom train data took minut download rate unexpectedli slow access data xlarg instanc privat vpc endpoint theoret maximum bandwidth gbp speed download start train follow function estim estim imag role role output path output locat train instanc count train instanc type xlarg train volum size train max run secur group id fadc output start start train job launch request instanc prepar instanc train download download input data train download train imag dataset download took min",
        "Question_preprocessed_content":"take unexpectedli long time download train data custom train data took minut download rate unexpectedli slow access data instanc privat vpc endpoint theoret maximum bandwidth gbp speed download start train follow function estim role role output start start train launch request prepar instanc download download input train download train dataset download took min",
        "Question_gpt_summary_original":"The user encountered a challenge with Sagemaker taking an unexpectedly long time to download 220 GB of training data, which took 54 minutes at a rate of only 70 MB\/s. Despite accessing the data in S3 from a p3.8xlarge instance through a private VPC endpoint with a theoretical maximum bandwidth of 25 Gbps, the user is seeking ways to speed up the download.",
        "Question_gpt_summary":"user encount challeng take unexpectedli long time download train data took minut rate despit access data xlarg instanc privat vpc endpoint theoret maximum bandwidth gbp user seek wai speed download",
        "Answer_original_content":"connect vpc endpoint nat vpc endpoint recommend open support ticket possibl support abl look network log option custom us pipe input pipe mode recommend larg dataset shorter startup time data stream instead download train instanc",
        "Answer_preprocessed_content":"connect vpc endpoint nat vpc endpoint recommend open support ticket possibl support abl look network log option custom us pipe input pipe mode recommend larg dataset shorter startup time data stream instead download train instanc",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include opening a support ticket to investigate the network logs if the user is using a VPC endpoint to connect to S3. Another solution is to use pipe input, which is recommended for large datasets and can shorten the startup time by streaming the data instead of downloading it to the training instances.",
        "Answer_gpt_summary":"possibl solut mention discuss includ open support ticket investig network log user vpc endpoint connect solut us pipe input recommend larg dataset shorten startup time stream data instead download train instanc"
    },
    {
        "Question_title":"How to match input\/output with sagemaker batch transform?",
        "Question_body":"<p>I'm using sagemaker batch transform, with json input files. see below for sample input\/output files. i have custom inference code below, and i'm using json.dumps to return prediction, but it's not returning json. I tried to use =&gt;    &quot;DataProcessing&quot;: {&quot;JoinSource&quot;: &quot;string&quot;,  }, to match input and output. but i'm getting error that &quot;unable to marshall ...&quot; . I think because , the output_fn is returning array of list or just list and not json , that is why it is unable to match input with output.any suggestions on how should i return the data?<\/p>\n<p>infernce code<\/p>\n<pre><code>def model_fn(model_dir):\n...\ndef input_fn(data, content_type):\n...\ndef predict_fn(data, model):\n...\ndef output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n        return json.dumps(prediction), mimetype=accept)\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p>input file<\/p>\n<pre><code>{&quot;data&quot; : &quot;input line  one&quot; }\n{&quot;data&quot; : &quot;input line  two&quot; }\n....\n<\/code><\/pre>\n<p>output file<\/p>\n<pre><code>[&quot;output line  one&quot; ]\n[&quot;output line  two&quot; ]\n<\/code><\/pre>\n<pre><code>{\n   &quot;BatchStrategy&quot;: SingleRecord,\n   &quot;DataProcessing&quot;: { \n      &quot;JoinSource&quot;: &quot;string&quot;,\n   },\n   &quot;MaxConcurrentTransforms&quot;: 3,\n   &quot;MaxPayloadInMB&quot;: 6,\n   &quot;ModelClientConfig&quot;: { \n      &quot;InvocationsMaxRetries&quot;: 1,\n      &quot;InvocationsTimeoutInSeconds&quot;: 3600\n   },\n   &quot;ModelName&quot;: &quot;some-model&quot;,\n   &quot;TransformInput&quot;: { \n      &quot;ContentType&quot;: &quot;string&quot;,\n      &quot;DataSource&quot;: { \n         &quot;S3DataSource&quot;: { \n            &quot;S3DataType&quot;: &quot;string&quot;,\n            &quot;S3Uri&quot;: &quot;s3:\/\/bucket-sample&quot;\n         }\n      },\n      &quot;SplitType&quot;: &quot;Line&quot;\n   },\n   &quot;TransformJobName&quot;: &quot;transform-job&quot;\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1653780407103,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":358.0,
        "Poster_id":13645093.0,
        "Poster_created_time":1590797441983,
        "Poster_last_access_time":1664049080543,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":525.0,
        "Poster_views":98.0,
        "Answer_body":"<p><code>json.dumps<\/code> will not convert your array to a dict structure and serialize it to a JSON String.<\/p>\n<p>What data type is <code>prediction<\/code> ? Have you tested making sure <code>prediction<\/code> is a dict?<\/p>\n<p>You can confirm the data type by adding <code>print(type(prediction))<\/code> to see the data type in the CloudWatch Logs.<\/p>\n<p>If prediction is a <code>list<\/code> you can test the following:<\/p>\n<pre><code>def output_fn(prediction, accept):\n    if accept == &quot;application\/json&quot;:\n\n        my_dict = {'output': prediction}\n        return json.dumps(my_dict), mimetype=accept)\n\n    raise RuntimeException(&quot;{} accept type is not supported by this script.&quot;.format(accept))\n<\/code><\/pre>\n<p><code>DataProcessing<\/code> and <code>JoinSource<\/code> are used to associate the data that is relevant to the prediction results in the output. It is not meant to be used to match the input and output format.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":18272224.0,
        "Answerer_created_time":1645475560783,
        "Answerer_last_access_time":1663885040952,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":466.0,
        "Answerer_views":32.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72419908",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1654129068707,
        "Question_original_content":"match input output batch transform batch transform json input file sampl input output file custom infer code json dump return predict return json tri us dataprocess joinsourc string match input output get error unabl marshal think output return arrai list list json unabl match input output suggest return data infernc code def model model dir def input data content type def predict data model def output predict accept accept applic json return json dump predict mimetyp accept rais runtimeexcept accept type support script format accept input file data input line data input line output file output line output line batchstrategi singlerecord dataprocess joinsourc string maxconcurrenttransform maxpayloadinmb modelclientconfig invocationsmaxretri invocationstimeoutinsecond modelnam model transforminput contenttyp string datasourc sdatasourc sdatatyp string suri bucket sampl splittyp line transformjobnam transform job",
        "Question_preprocessed_content":"match batch transform batch transform json input file sampl file custom infer code return predict return json tri us dataprocess match input output get error unabl marshal think return arrai list list json unabl match input suggest return data infernc code input file output file",
        "Question_gpt_summary_original":"The user is facing challenges in matching input\/output with Sagemaker batch transform. The custom inference code is using json.dumps to return predictions, but it's not returning json. The user tried to use \"DataProcessing\" to match input and output, but it resulted in an error. The output_fn is returning an array of lists or just a list, which is not in json format, causing the issue. The user is seeking suggestions on how to return the data.",
        "Question_gpt_summary":"user face challeng match input output batch transform custom infer code json dump return predict return json user tri us dataprocess match input output result error output return arrai list list json format caus issu user seek suggest return data",
        "Answer_original_content":"json dump convert arrai dict structur serial json string data type predict test make sure predict dict confirm data type ad print type predict data type cloudwatch log predict list test follow def output predict accept accept applic json dict output predict return json dump dict mimetyp accept rais runtimeexcept accept type support script format accept dataprocess joinsourc associ data relev predict result output meant match input output format",
        "Answer_preprocessed_content":"convert arrai dict structur serial json string data type test make sure dict confirm data type ad data type cloudwatch log predict test follow associ data relev predict result output meant match input output format",
        "Answer_gpt_summary_original":"Solution:\n- Check the data type of the prediction variable and ensure that it is a dictionary.\n- If the prediction variable is a list, create a dictionary with the list as a value and return it using json.dumps.\n- Do not use DataProcessing or JoinSource to match input and output format.",
        "Answer_gpt_summary":"solut check data type predict variabl ensur dictionari predict variabl list creat dictionari list valu return json dump us dataprocess joinsourc match input output format"
    },
    {
        "Question_title":"'[Errno 13] Permission denied' - Jupyter Labs on AWS SageMaker",
        "Question_body":"<p>I'm using a <strong>Jupyter Lab<\/strong> instance on <strong>AWS SageMaker<\/strong>.<\/p>\n<p>Kernel: <code>conda_mxnet_latest_p37<\/code>.<\/p>\n<p><code>url_lib<\/code> contains some false urls, that I exception handle.<\/p>\n<pre><code>['15', '259', '26', '58', 'https:\/\/imagepool.1und1-drillisch.de\/v2\/download\/nachhaltigkeitsbericht\/1&amp;1Drillisch_Sustainability_Report_EN_2018.pdf', 'https:\/\/imagepool.1und1-drillisch.de\/\/v2\/download\/nachhaltigkeitsbericht\/2018-04-06_1und1-Drillisch_Sustainability_Report_eng.pdf', '6', 'http:\/\/youxin.37.com\/uploads\/file\/1556248045.pdf', '80', 'https:\/\/multimedia.3m.com\/mws\/media\/1691941O\/2019-sustainability-report.PDF', 'https:\/\/s3-us-west-2.amazonaws.com\/ungc-production\/attachments\/cop_2020\/483648\/original\/GPIC_Sustainability_Report_2020__-_40_Years_of_Sustainable_Success.pdf?1583154650', 'https:\/\/drive.google.com\/open?id=1_dnBcfXWjexy9QoWRhOk_3gnOkWfYRCw', 'http:\/\/aepsustainability.com\/performance\/docs\/2020AEPGRIReport.pdf']  # sample\n<\/code><\/pre>\n<p>However, ones that are working URLs, throw this error:<\/p>\n<pre><code>[Errno 13] Permission denied: '\/data'\n<\/code><\/pre>\n<p>I don't have the directory opened, nor files since I they're not downloaded.<\/p>\n<p>I ran in <strong>Terminal<\/strong> without luck:<\/p>\n<pre><code>sh-4.2$ chmod 777 data\nsh-4.2$ chmod 777 data\/\nsh-4.2$ chmod 777 data\/gri\nsh-4.2$ chmod 777 data\/gri\/\n<\/code><\/pre>\n<p><strong>Code:<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport opendatasets as od\nimport urllib\nimport zipfile\nimport os\n\ncsr_df = pd.read_excel('data\/Company Sustainability Reports.xlsx', index_col=None)\nurl_list = csr_df['Report PDF Address'].tolist()\n\nfor url in url_list:\n    try:\n        download = od.download(url, '\/data\/gri\/')\n        filename = url.rsplit('\/', 1)[1]\n\n        path_extract = 'data\/gri\/' + filename\n        with zipfile.ZipFile('data\/gri\/' + filename + '.zip', 'r') as zip_ref:\n            zip_ref.extractall(path_extract)\n\n        os.remove(path_extract + 'readme.txt')\n\n        filenames = os.listdir(path_extract)\n        scans = []\n        for f in filenames:\n            with Image.open(path_extract + f) as img:\n                matrix = np.array(img)\n                scans.append(matrix)\n\n        # shutil.rmtree(path_extract)\n        os.remove(path_extract[:-1] + '.zip')\n\n    except (urllib.error.URLError, IOError, RuntimeError) as e:\n        print('Download PDFs', e)\n<\/code><\/pre>\n<p><strong>Output:<\/strong><\/p>\n<pre><code>Download PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs list index out of range\nDownload PDFs &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'imagepool.1und1-drillisch.de'. (_ssl.c:1091)&gt;\nDownload PDFs &lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: Hostname mismatch, certificate is not valid for 'imagepool.1und1-drillisch.de'. (_ssl.c:1091)&gt;\nDownload PDFs list index out of range\nDownload PDFs [Errno 13] Permission denied: '\/data'\n...\n<\/code><\/pre>\n<p>Please let me know if there is anything else I should clarify.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1639402213260,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1639406637992,
        "Question_score":0,
        "Question_view_count":684.0,
        "Poster_id":16105404.0,
        "Poster_created_time":1622632545867,
        "Poster_last_access_time":1646839538183,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1.0,
        "Poster_views":111.0,
        "Answer_body":"<p><code>download<\/code> has a forward-slash <code>\/<\/code> as first character of save directory (second parameter). I removed this:<\/p>\n<pre><code>download = od.download(url, 'data\/gri\/')\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>...\nDownloading http:\/\/youxin.37.com\/uploads\/file\/1556248045.pdf to data\/gri\/1556248045.pdf\n450560it [00:02, 207848.59it\/s]\n...\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":16105404.0,
        "Answerer_created_time":1622632545867,
        "Answerer_last_access_time":1646839538183,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1.0,
        "Answerer_views":111.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70335470",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1639406624843,
        "Question_original_content":"errno permiss deni jupyt lab jupyt lab instanc kernel conda mxnet latest url lib contain fals url except handl http imagepool und drillisch download nachhaltigkeitsbericht drillisch sustain report pdf http imagepool und drillisch download nachhaltigkeitsbericht und drillisch sustain report eng pdf http youxin com upload file pdf http multimedia com mw media sustain report pdf http west amazonaw com ungc product attach cop origin gpic sustain report year sustain success pdf http drive googl com open dnbcfxwjexyqowrhok gnokwfyrcw http aepsustain com perform doc aepgrireport pdf sampl on work url throw error errno permiss deni data directori open file download ran termin luck chmod data chmod data chmod data gri chmod data gri code import panda import opendataset import urllib import zipfil import csr read excel data compani sustain report xlsx index col url list csr report pdf address tolist url url list try download download url data gri filenam url rsplit path extract data gri filenam zipfil zipfil data gri filenam zip zip ref zip ref extractal path extract remov path extract readm txt filenam listdir path extract scan filenam imag open path extract img matrix arrai img scan append matrix shutil rmtree path extract remov path extract zip urllib error urlerror ioerror runtimeerror print download pdf output download pdf list index rang download pdf list index rang download pdf list index rang download pdf list index rang download pdf download pdf download pdf list index rang download pdf errno permiss deni data let know clarifi",
        "Question_preprocessed_content":"permiss deni jupyt lab jupyt lab instanc kernel contain fals url except handl on work url throw error directori open file download ran termin luck code output let know clarifi",
        "Question_gpt_summary_original":"The user is encountering a \"Permission denied\" error when trying to download files from working URLs using a Jupyter Lab instance on AWS SageMaker. The user has tried to change the permissions of the directory and files but without success. The user's code is attempting to download files from a list of URLs, and the error is occurring during the download process.",
        "Question_gpt_summary":"user encount permiss deni error try download file work url jupyt lab instanc user tri chang permiss directori file success user code attempt download file list url error occur download process",
        "Answer_original_content":"download forward slash charact save directori second paramet remov download download url data gri output download http youxin com upload file pdf data gri pdf",
        "Answer_preprocessed_content":"charact save directori remov output",
        "Answer_gpt_summary_original":"Solution: The user was able to solve the \"Permission denied\" error by removing the forward-slash \"\/\" as the first character of the save directory in the download function.",
        "Answer_gpt_summary":"solut user abl solv permiss deni error remov forward slash charact save directori download function"
    },
    {
        "Question_title":"What is the cluster manager in SageMaker Spark Processing?",
        "Question_body":"SageMaker Processing can launch multi-instance jobs. What is the underlying cluster manager? Yarn? Mesos? Something custom?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1602770746000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":81.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"The Spark container uses YARN - for ref the bootstrap script on github: https:\/\/github.com\/aws\/sagemaker-spark-container\/blob\/master\/src\/smspark\/bootstrapper.py and the Dockerfile with hadoop-yarn dependencies",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUShPm0t4vR4S8XBKMiAcA6g\/what-is-the-cluster-manager-in-sage-maker-spark-processing",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1602771143000,
        "Question_original_content":"cluster manag spark process process launch multi instanc job underli cluster manag yarn meso custom",
        "Question_preprocessed_content":"cluster manag spark process process launch job underli cluster manag yarn meso custom",
        "Question_gpt_summary_original":"The user is facing a challenge in determining the underlying cluster manager used by SageMaker Processing for launching multi-instance jobs. They are unsure if it is Yarn, Mesos, or a custom solution.",
        "Question_gpt_summary":"user face challeng determin underli cluster manag process launch multi instanc job unsur yarn meso custom solut",
        "Answer_original_content":"spark contain us yarn ref bootstrap script github http github com aw spark contain blob master src smspark bootstrapp dockerfil hadoop yarn depend",
        "Answer_preprocessed_content":"spark contain us yarn ref bootstrap script github dockerfil depend",
        "Answer_gpt_summary_original":"Solution: The user can determine that the underlying cluster manager used by SageMaker Processing for launching multi-instance jobs is YARN. This is based on the fact that the Spark container uses YARN, as evidenced by the bootstrap script on Github and the Dockerfile with Hadoop-YARN dependencies.",
        "Answer_gpt_summary":"solut user determin underli cluster manag process launch multi instanc job yarn base fact spark contain us yarn evidenc bootstrap script github dockerfil hadoop yarn depend"
    },
    {
        "Question_title":"Sagemaker to use processed pickled ndarray instead of csv files from S3",
        "Question_body":"<p>I understand that you can pass a CSV file from S3 into a Sagemaker XGBoost container using the following code<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>train_channel = sagemaker.session.s3_input(train_data, content_type='text\/csv')\nvalid_channel = sagemaker.session.s3_input(validation_data, content_type='text\/csv')\n\ndata_channels = {'train': train_channel, 'validation': valid_channel}\nxgb_model.fit(inputs=data_channels,  logs=True)\n<\/code><\/pre>\n\n<p>But I have an ndArray stored in S3 bucket. These are processed, label encoded, feature engineered arrays. I would want to pass this into the container instead of the csv. I do understand I can always convert my ndarray into csv files before saving it in S3. Just checking if there is an array option.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1566996073187,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":92.0,
        "Poster_id":2177003.0,
        "Poster_created_time":1363437641347,
        "Poster_last_access_time":1606461663280,
        "Poster_age":null,
        "Poster_location":"Chennai, India",
        "Poster_reputation":468.0,
        "Poster_views":97.0,
        "Answer_body":"<p>There are multiple options for algorithms in SageMaker:<\/p>\n\n<ol>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">Built-in algorithms<\/a>, like the SageMaker XGBoost you mention<\/li>\n<li>Custom, user-created algorithm code, which can be:\n\n<ul>\n<li>Written for a pre-built docker image, available for Sklearn, TensorFlow, Pytorch, MXNet<\/li>\n<li>Written in your own container<\/li>\n<\/ul><\/li>\n<\/ol>\n\n<p>When you use built-ins (option 1), your choice of data format options is limited to what the built-ins support, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#InputOutput-XGBoost\" rel=\"nofollow noreferrer\">which is only csv and libsvm in the case of the built-in XGBoost<\/a>. If you want to use custom data formats and pre-processing logic before XGBoost, it is absolutely possible if you use your own script leveraging the open-source XGBoost. You can get inspiration from the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">Random Forest demo<\/a> to see how to create custom models in pre-built containers<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":5331834.0,
        "Answerer_created_time":1442180190107,
        "Answerer_last_access_time":1664048608876,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57692681",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1568417822248,
        "Question_original_content":"us process pickl ndarrai instead csv file understand pass csv file xgboost contain follow code train channel session input train data content type text csv valid channel session input valid data content type text csv data channel train train channel valid valid channel xgb model fit input data channel log true ndarrai store bucket process label encod featur engin arrai want pass contain instead csv understand convert ndarrai csv file save check arrai option",
        "Question_preprocessed_content":"us process pickl ndarrai instead csv file understand pass csv file xgboost contain follow code ndarrai store bucket process label encod featur engin arrai want pass contain instead csv understand convert ndarrai csv file save check arrai option",
        "Question_gpt_summary_original":"The user wants to pass a processed pickled ndarray from an S3 bucket into a Sagemaker XGBoost container instead of using CSV files. They are looking for an array option and are aware that they can convert their ndarray into CSV files before saving it in S3.",
        "Question_gpt_summary":"user want pass process pickl ndarrai bucket xgboost contain instead csv file look arrai option awar convert ndarrai csv file save",
        "Answer_original_content":"multipl option algorithm built algorithm like xgboost mention custom user creat algorithm code written pre built docker imag avail sklearn tensorflow pytorch mxnet written contain us built in option choic data format option limit built in support csv libsvm case built xgboost want us custom data format pre process logic xgboost absolut possibl us script leverag open sourc xgboost inspir random forest demo creat custom model pre built contain",
        "Answer_preprocessed_content":"multipl option algorithm algorithm like xgboost mention custom algorithm code written docker imag avail sklearn tensorflow pytorch mxnet written contain us choic data format option limit support csv libsvm case xgboost want us custom data format logic xgboost absolut possibl us script leverag xgboost inspir random forest demo creat custom model contain",
        "Answer_gpt_summary_original":"There are two options for algorithms in SageMaker: built-in algorithms and custom, user-created algorithm code. When using built-in algorithms, the choice of data format options is limited to what the built-ins support, which is only csv and libsvm in the case of the built-in XGBoost. However, it is possible to use custom data formats and pre-processing logic before XGBoost by using a custom script leveraging the open-source XGBoost. The user can get inspiration from the Random Forest demo to see how to create custom models in pre-built containers. No solution is provided for passing a processed pickled ndarray from an S3 bucket into a Sagemaker XGBoost container instead of using CSV files.",
        "Answer_gpt_summary":"option algorithm built algorithm custom user creat algorithm code built algorithm choic data format option limit built in support csv libsvm case built xgboost possibl us custom data format pre process logic xgboost custom script leverag open sourc xgboost user inspir random forest demo creat custom model pre built contain solut provid pass process pickl ndarrai bucket xgboost contain instead csv file"
    },
    {
        "Question_title":"How to run tensorboard for tensorflow in AWS Sagemaker?",
        "Question_body":"<p>I need to visualize real-time losses and metrics for a tensorflow model on AWS Sagemaker instance.\nIn a Jupyter notebook, I tried running<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &lt;path&gt;\n<\/code><\/pre>\n<p>But nothing really happened. How can I get this working?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1610628627223,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1610628935100,
        "Question_score":1,
        "Question_view_count":715.0,
        "Poster_id":9031512.0,
        "Poster_created_time":1512023194592,
        "Poster_last_access_time":1663919015323,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":547.0,
        "Poster_views":61.0,
        "Answer_body":"<p>You need to use the conda_pytorch_36 kernel (this is the one I used) and tensorboard is not installed by default so you need to run<\/p>\n<pre><code>!pip install tensorboard\n<\/code><\/pre>\n<p>Then you will get a blank screen when you run.<\/p>\n<pre><code>%load_ext tensorboard\n%tensorboard --logdir &quot;.\/runs&quot;\n<\/code><\/pre>\n<p>You can connect to tensorboard using your URL with notebook or lab replaced with proxy\/6006<\/p>\n<pre><code>https:\/\/YOUR_NOTEBOOK_INSTANCE_NAME.notebook.ap-northeast-1.sagemaker.aws\/proxy\/6006\/\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":4453351.0,
        "Answerer_created_time":1421238326280,
        "Answerer_last_access_time":1664055207523,
        "Answerer_age":null,
        "Answerer_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Answerer_reputation":1951.0,
        "Answerer_views":217.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65719292",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1610631020420,
        "Question_original_content":"run tensorboard tensorflow need visual real time loss metric tensorflow model instanc jupyt notebook tri run load ext tensorboard tensorboard logdir happen work",
        "Question_preprocessed_content":"run tensorboard tensorflow need visual loss metric tensorflow model instanc jupyt notebook tri run happen work",
        "Question_gpt_summary_original":"The user is facing challenges in visualizing real-time losses and metrics for a TensorFlow model on an AWS Sagemaker instance. They have attempted to run tensorboard in a Jupyter notebook using specific commands, but have not been successful. The user is seeking guidance on how to resolve this issue.",
        "Question_gpt_summary":"user face challeng visual real time loss metric tensorflow model instanc attempt run tensorboard jupyt notebook specif command success user seek guidanc resolv issu",
        "Answer_original_content":"need us conda pytorch kernel tensorboard instal default need run pip instal tensorboard blank screen run load ext tensorboard tensorboard logdir run connect tensorboard url notebook lab replac proxi http notebook instanc notebook northeast aw proxi",
        "Answer_preprocessed_content":"need us kernel tensorboard instal default need run blank screen run connect tensorboard url notebook lab replac",
        "Answer_gpt_summary_original":"Solution:\n- Use the conda_pytorch_36 kernel.\n- Install tensorboard by running \"!pip install tensorboard\".\n- Load the tensorboard extension and run \"%tensorboard --logdir '.\/runs'\".\n- Connect to tensorboard using the URL with \"proxy\/6006\" instead of \"notebook\" or \"lab\".",
        "Answer_gpt_summary":"solut us conda pytorch kernel instal tensorboard run pip instal tensorboard load tensorboard extens run tensorboard logdir run connect tensorboard url proxi instead notebook lab"
    },
    {
        "Question_title":"How should Trains be used with hyper-param optimization tools like RayTune?",
        "Question_body":"<p>What could be a reasonable setup for this? Can I call Task.init() multiple times in the same execution?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1598478428030,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1609427499190,
        "Question_score":2,
        "Question_view_count":107.0,
        "Poster_id":857731.0,
        "Poster_created_time":1311330349880,
        "Poster_last_access_time":1663773102327,
        "Poster_age":null,
        "Poster_location":"Tel Aviv",
        "Poster_reputation":3784.0,
        "Poster_views":342.0,
        "Answer_body":"<p>Disclaimer: I'm part of the allegro.ai Trains team<\/p>\n<p>One solution is to inherit from <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/838c9cb0d2a5df5c193dfc85286abe59a80217c2\/trains\/automation\/optimization.py#L226\" rel=\"nofollow noreferrer\">trains.automation.optimization.SearchStrategy<\/a> and extend the functionality. This is similar to the <a href=\"https:\/\/github.com\/allegroai\/trains\/blob\/master\/trains\/automation\/optuna\/optuna.py\" rel=\"nofollow noreferrer\">Optuna<\/a> integration, where Optuna is used for the Bayesian optimization and Trains does the hyper-parameter setting, launching experiments, and retrieving performance metrics.<\/p>\n<p>Another option (not scalable but probably easier to start with), is to use have the RayTuner run your code (obviously setting the environment \/ git repo \/ docker etc is on the user), and have your training code look something like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># create new experimnt\ntask = Task.init('hp optimization', 'ray-tuner experiment', reuse_last_task_id=False)\n# store the hyperparams (assuming hparam is a dict) \ntask.connect(hparam) \n# training loop here\n# ...\n# shutdown experimnt\ntask.close()\n<\/code><\/pre>\n<p>This means every time the RayTuner executes the script a new experiment will be created, with new set of hyper parameters (assuming <code>haparm<\/code> is a dictionary, it will be registered on the experiment as hyper-parameters)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":11682840.0,
        "Answerer_created_time":1561143508792,
        "Answerer_last_access_time":1659992319200,
        "Answerer_age":null,
        "Answerer_location":"TRAINS Station",
        "Answerer_reputation":489.0,
        "Answerer_views":60.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63606182",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1598536233436,
        "Question_original_content":"train hyper param optim tool like raytun reason setup task init multipl time execut",
        "Question_preprocessed_content":"train optim tool like raytun reason setup multipl time execut",
        "Question_gpt_summary_original":"The user is seeking advice on how to use trains with hyper-parameter optimization tools like RayTune and is unsure if it is possible to call Task.init() multiple times in the same execution.",
        "Question_gpt_summary":"user seek advic us train hyper paramet optim tool like raytun unsur possibl task init multipl time execut",
        "Answer_original_content":"disclaim allegro train team solut inherit train autom optim searchstrategi extend function similar integr bayesian optim train hyper paramet set launch experi retriev perform metric option scalabl probabl easier start us raytun run code obvious set environ git repo docker user train code look like creat new experimnt task task init optim rai tuner experi reus task fals store hyperparam assum hparam dict task connect hparam train loop shutdown experimnt task close mean time raytun execut script new experi creat new set hyper paramet assum haparm dictionari regist experi hyper paramet",
        "Answer_preprocessed_content":"disclaim train team solut inherit extend function similar integr bayesian optim train set launch experi retriev perform metric option us raytun run code train code look like mean time raytun execut script new experi creat new set hyper paramet",
        "Answer_gpt_summary_original":"Two possible solutions were discussed. The first solution is to inherit from trains.automation.optimization.SearchStrategy and extend its functionality, similar to the Optuna integration. The second solution is to have the RayTuner run the code and create a new experiment every time with a new set of hyperparameters.",
        "Answer_gpt_summary":"possibl solut discuss solut inherit train autom optim searchstrategi extend function similar integr second solut raytun run code creat new experi time new set hyperparamet"
    },
    {
        "Question_title":"Are S3 Kedro datasets thread-safe?",
        "Question_body":"<p><code>CSVS3DataSet<\/code>\/<code>HDFS3DataSet<\/code>\/<code>HDFS3DataSet<\/code> use <code>boto3<\/code>, which is known to be not thread-safe <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/resources.html?highlight=multithreading#multithreading-multiprocessing\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/guide\/resources.html?highlight=multithreading#multithreading-multiprocessing<\/a><\/p>\n\n<p>Is it OK to use these datasets with the ParallelRunner?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1574069164940,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":146.0,
        "Poster_id":3437494.0,
        "Poster_created_time":1395230906503,
        "Poster_last_access_time":1652366319603,
        "Poster_age":null,
        "Poster_location":"London, United Kingdom",
        "Poster_reputation":129.0,
        "Poster_views":51.0,
        "Answer_body":"<p><code>Kedro<\/code> uses <code>s3fs<\/code>, which uses <code>boto3<\/code> library to access S3. <code>Boto3<\/code> is not thread-safe indeed, but only if you are trying to reuse the same Session object.<\/p>\n\n<p>All <code>Kedro<\/code> S3 datasets maintain separate instances of <code>S3FileSystem<\/code>, which means separate boto sessions, so it's safe.<\/p>\n\n<p>It's probably not great in terms of performance, and if you work with hundreds of S3 data sets in parallel, or thousands of small S3 datasets sequentially - the pipeline might run quite long and even fail on connection errors, but you are totally safe with a few dozens of them.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":3437494.0,
        "Answerer_created_time":1395230906503,
        "Answerer_last_access_time":1652366319603,
        "Answerer_age":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation":129.0,
        "Answerer_views":51.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58911398",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1574069164940,
        "Question_original_content":"dataset thread safe csvsdataset hdfsdataset hdfsdataset us boto known thread safe http boto amazonaw com document api latest guid resourc html highlight multithread multithread multiprocess us dataset parallelrunn",
        "Question_preprocessed_content":"dataset us known us dataset parallelrunn",
        "Question_gpt_summary_original":"The user is questioning whether S3 Kedro datasets, specifically CSVS3DataSet, HDFS3DataSet, and S3DataSet, are thread-safe due to their use of boto3, which is known to be not thread-safe. The user is asking if it is safe to use these datasets with the ParallelRunner.",
        "Question_gpt_summary":"user question dataset specif csvsdataset hdfsdataset sdataset thread safe us boto known thread safe user ask safe us dataset parallelrunn",
        "Answer_original_content":"us sf us boto librari access boto thread safe try reus session object dataset maintain separ instanc sfilesystem mean separ boto session safe probabl great term perform work hundr data set parallel thousand small dataset sequenti pipelin run long fail connect error total safe dozen",
        "Answer_preprocessed_content":"us us librari access try reus session object dataset maintain separ instanc mean separ boto session safe probabl great term perform work hundr data set parallel thousand small dataset sequenti pipelin run long fail connect error total safe dozen",
        "Answer_gpt_summary_original":"Solution: All Kedro S3 datasets maintain separate instances of S3FileSystem, which means separate boto sessions, so it's safe to use them with the ParallelRunner. However, it may not be great in terms of performance if working with hundreds of S3 datasets in parallel or thousands of small S3 datasets sequentially.",
        "Answer_gpt_summary":"solut dataset maintain separ instanc sfilesystem mean separ boto session safe us parallelrunn great term perform work hundr dataset parallel thousand small dataset sequenti"
    },
    {
        "Question_title":"how to delete Azure ML real-time endpoints which is in transition state",
        "Question_body":"Hi,\n\nI have made deployment of the model from the AutoML experiment, due to the issue in the resources associated. Deployment has failed.\n\nBut the real-time endpoint has been in the transition state for few hours, I can't delete it and the model registered along with it due to this. How can I force delete in this case. Please provide a solution.\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1628858328673,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Thank you for the response @romungi-MSFT. I have left the feedback to the team.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/513012\/how-to-delete-azure-ml-real-time-endpoints-which-i.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1629112596936,
        "Question_original_content":"delet real time endpoint transit state deploy model automl experi issu resourc associ deploy fail real time endpoint transit state hour delet model regist forc delet case provid solut thank",
        "Question_preprocessed_content":"delet endpoint transit state deploy model automl experi issu resourc associ deploy fail endpoint transit state hour delet model regist forc delet case provid solut thank",
        "Question_gpt_summary_original":"The user is facing challenges in deleting an Azure ML real-time endpoint that is in a transition state for several hours, which is preventing them from deleting the associated model due to deployment failure. They are seeking a solution to force delete the endpoint and model.",
        "Question_gpt_summary":"user face challeng delet real time endpoint transit state hour prevent delet associ model deploy failur seek solut forc delet endpoint model",
        "Answer_original_content":"thank respons romungi msft left feedback team",
        "Answer_preprocessed_content":"thank respons left feedback team",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"How to process huge datasets in kedro",
        "Question_body":"<p>I have pretty big (~200Gb, ~20M lines) raw jsonl dataset. I need to extract important properties from there and store the intermediate dataset in csv for further conversion into something like HDF5, parquet, etc. Obviously, I can't use <code>JSONDataSet<\/code> for loading raw dataset, because it utilizes <code>pandas.read_json<\/code> under the hood, and using pandas for the dataset of such size sounds like a bad idea. So I'm thinking about reading the raw dataset line by line, process and append processed data line by line to the intermediate dataset.<\/p>\n\n<p>What I can't understand is how to make this compatible with <code>AbstractDataSet<\/code> with its <code>_load<\/code> and <code>_save<\/code> methods.<\/p>\n\n<p>P.S. I understand I can move this out of kedro's context, and introduce preprocessed dataset as a raw one, but that kinda breaks the whole idea of complete pipelines. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1582237049943,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1583417458307,
        "Question_score":6,
        "Question_view_count":656.0,
        "Poster_id":1110044.0,
        "Poster_created_time":1324477592580,
        "Poster_last_access_time":1663840044463,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1315.0,
        "Poster_views":91.0,
        "Answer_body":"<p>Try to use pyspark to leverage lazy evaluation and batch execution. \nSparkDataSet is implemented in kedro.contib.io.spark_data_set<\/p>\n\n<p>Sample catalog config for jsonl:<\/p>\n\n<pre><code>your_dataset_name:   \n  type: kedro.contrib.io.pyspark.SparkDataSet\n  filepath: \"\\file_path\"\n  file_format: json\n  load_args:\n    multiline: True\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1582292099020,
        "Answer_score":4.0,
        "Answerer_id":8754467.0,
        "Answerer_created_time":1507660761310,
        "Answerer_last_access_time":1663528599203,
        "Answerer_age":null,
        "Answerer_location":"Madrid, Espa\u00f1a",
        "Answerer_reputation":492.0,
        "Answerer_views":22.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60329363",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1582275656936,
        "Question_original_content":"process huge dataset pretti big line raw jsonl dataset need extract import properti store intermedi dataset csv convers like hdf parquet obvious us jsondataset load raw dataset util panda read json hood panda dataset size sound like bad idea think read raw dataset line line process append process data line line intermedi dataset understand compat abstractdataset load save method understand context introduc preprocess dataset raw kinda break idea complet pipelin",
        "Question_preprocessed_content":"process huge dataset pretti big raw jsonl dataset need extract import properti store intermedi dataset csv convers like hdf parquet obvious us load raw dataset util hood panda dataset size sound like bad idea think read raw dataset line line process append process data line line intermedi dataset understand compat method understand context introduc preprocess dataset raw kinda break idea complet pipelin",
        "Question_gpt_summary_original":"The user has a large raw jsonl dataset of around 200GB and 20 million lines that needs to be processed to extract important properties. However, using pandas for such a large dataset is not feasible. The user is considering reading the dataset line by line and appending processed data to an intermediate dataset, but is unsure how to make it compatible with AbstractDataSet's load and save methods. The user is also hesitant to move the dataset out of Kedro's context as it would break the idea of complete pipelines.",
        "Question_gpt_summary":"user larg raw jsonl dataset million line need process extract import properti panda larg dataset feasibl user consid read dataset line line append process data intermedi dataset unsur compat abstractdataset load save method user hesit dataset context break idea complet pipelin",
        "Answer_original_content":"try us pyspark leverag lazi evalu batch execut sparkdataset implement contib spark data set sampl catalog config jsonl dataset type contrib pyspark sparkdataset filepath file path file format json load arg multilin true",
        "Answer_preprocessed_content":"try us pyspark leverag lazi evalu batch execut sparkdataset implement sampl catalog config jsonl",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use PySpark to leverage lazy evaluation and batch execution. The user can use SparkDataSet, which is implemented in kedro.contrib.io.spark_data_set. A sample catalog config for jsonl is also provided in the discussion.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us pyspark leverag lazi evalu batch execut user us sparkdataset implement contrib spark data set sampl catalog config jsonl provid discuss"
    },
    {
        "Question_title":"Voice\/Speech to Text Train Model",
        "Question_body":"Hi.\n\nSo I would like to create a model that 'listens' to audio from movies\/podcasts (with subtitles) then returns the text transcript from it. Problem is, it's in a language not supported by Azure (or most of the big cloud providers). How would I go about and, from scratch, build a model that is trained on the audio from a new language? The input audio all will have subtitles or captions.\n\nI tried Azure ML studio but I couldn't create datasets with audio files. Not sure if I missed something there. Also tried Speech studio but it only supports a select number of languages. Would that be possible at all?\n\nAny suggestions would be appreciated. Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1627330507383,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@NathanCarns-0092 Yes, you are correct, to develop a model for speech to text we need a deep learning model here. This is out of the scope of Azure Machine Learning Studio(classic). But I think Azure Machine Learning service should support it, please refer to this: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-deep-learning-vs-machine-learning#machine-translation\n\nI have found one post which may help: https:\/\/towardsdatascience.com\/audio-deep-learning-made-simple-automatic-speech-recognition-asr-how-it-works-716cfce4c706\n\nMoreover, I have forwarded your feedback to see any plan here for Nigerian in Azure.\n\nThanks.\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/490113\/voicespeech-to-text-train-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1628120380252,
        "Question_original_content":"voic speech text train model like creat model listen audio movi podcast subtitl return text transcript problem languag support azur big cloud provid scratch build model train audio new languag input audio subtitl caption tri studio couldn creat dataset audio file sure miss tri speech studio support select number languag possibl suggest appreci thank",
        "Question_preprocessed_content":"text train model like creat model listen audio return text transcript problem languag support azur scratch build model train audio new languag input audio subtitl caption tri studio couldn creat dataset audio file sure miss tri speech studio support select number languag possibl suggest appreci thank",
        "Question_gpt_summary_original":"The user wants to create a model that can transcribe audio from movies\/podcasts with subtitles in a language that is not supported by Azure or other big cloud providers. They have tried using Azure ML studio and Speech studio but have encountered difficulties in creating datasets with audio files and limited language support. The user is seeking suggestions on how to build a model from scratch that can be trained on audio from a new language.",
        "Question_gpt_summary":"user want creat model transcrib audio movi podcast subtitl languag support azur big cloud provid tri studio speech studio encount difficulti creat dataset audio file limit languag support user seek suggest build model scratch train audio new languag",
        "Answer_original_content":"nathancarn ye correct develop model speech text need deep learn model scope studio classic think servic support refer http doc microsoft com azur machin learn concept deep learn machin learn machin translat post help http towardsdatasci com audio deep learn simpl automat speech recognit asr work cfcec forward feedback plan nigerian azur thank yutong",
        "Answer_preprocessed_content":"ye correct develop model speech text need deep learn model scope studio think servic support refer post help forward feedback plan nigerian azur thank yutong",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include using a deep learning model to develop a speech-to-text model, exploring Azure Machine Learning service for support, referring to a post on audio deep learning for automatic speech recognition, and forwarding feedback to Azure for potential language support.",
        "Answer_gpt_summary":"possibl solut mention discuss includ deep learn model develop speech text model explor servic support refer post audio deep learn automat speech recognit forward feedback azur potenti languag support"
    },
    {
        "Question_title":"How to restart VM in Azure ML Notebook?",
        "Question_body":"<p>I am writing code in Jupyter Notebook in Azure ML Studio.<\/p>\n\n<p>At current moment every command causes kernel death. Even in new clear notebook I could not execute even <code>print 'hello'<\/code> - kernel died immediately.<\/p>\n\n<p>Also I could not use bash commands like <code>!ls<\/code> - It crashes kernel too.<\/p>\n\n<p>How could I restart my VM or restart session in Azure ML Studio with killing all running VM?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1458551625417,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1458567186852,
        "Question_score":0,
        "Question_view_count":488.0,
        "Poster_id":5769555.0,
        "Poster_created_time":1452426675696,
        "Poster_last_access_time":1615978476292,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":77.0,
        "Poster_views":21.0,
        "Answer_body":"<p>I have found that if I go from notebook menu to File->Open, then I see all my notebooks, their statuses and I could shutdown them.<\/p>\n\n<p>Also I have found that some of my closed notebooks were still alive and have shut them down.<\/p>\n\n<p>After this my working notebook came back to life.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":5769555.0,
        "Answerer_created_time":1452426675696,
        "Answerer_last_access_time":1615978476292,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":77.0,
        "Answerer_views":21.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36126897",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1458573398360,
        "Question_original_content":"restart notebook write code jupyt notebook studio current moment command caus kernel death new clear notebook execut print hello kernel di immedi us bash command like crash kernel restart restart session studio kill run",
        "Question_preprocessed_content":"restart notebook write code jupyt notebook studio current moment command caus kernel death new clear notebook execut kernel di immedi us bash command like crash kernel restart restart session studio kill run",
        "Question_gpt_summary_original":"The user is facing challenges with their Jupyter Notebook in Azure ML Studio, as every command causes kernel death, including simple commands like print and bash commands like ls. The user is seeking a solution to restart their VM or session in Azure ML Studio without killing all running VMs.",
        "Question_gpt_summary":"user face challeng jupyt notebook studio command caus kernel death includ simpl command like print bash command like user seek solut restart session studio kill run vm",
        "Answer_original_content":"notebook menu file open notebook status shutdown close notebook aliv shut work notebook came life",
        "Answer_preprocessed_content":"notebook menu notebook status shutdown close notebook aliv shut work notebook came life",
        "Answer_gpt_summary_original":"Solution: The user can try to go to the notebook menu and select File->Open to see all their notebooks and their statuses. They can then shut down any closed notebooks that are still running, which may help their working notebook come back to life.",
        "Answer_gpt_summary":"solut user try notebook menu select file open notebook status shut close notebook run help work notebook come life"
    },
    {
        "Question_title":"How to get image classification prediction from GCP AIPlatform in ruby?",
        "Question_body":"<p>I'm new with ruby and I want to use GCP AIPlatform but I'm struggeling with the payload.<\/p>\n<p>So far, I have :<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = ::Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  'data:image\/png;base64,' + Base64.strict_encode64(img.read)\nend\n\ninstance = Instance.new(:content =&gt; img)\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>Here is my proto<\/p>\n<pre><code>message Instance {\n  required bytes content = 1;\n};\n<\/code><\/pre>\n<p>But I have the following error : <code>Invalid type Instance to assign to submessage field 'instances'<\/code><\/p>\n<p>I read the documentation but for ruby SDK it's a bit light.\nThe parameters are OK, the JS example here : <a href=\"https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js\" rel=\"nofollow noreferrer\">https:\/\/github.com\/googleapis\/nodejs-ai-platform\/blob\/main\/samples\/predict-image-object-detection.js<\/a> is working with those parameters<\/p>\n<p>What am I doing wrong ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1656671602857,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":54.0,
        "Poster_id":19459566.0,
        "Poster_created_time":1656670919183,
        "Poster_last_access_time":1659105111752,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1.0,
        "Poster_views":2.0,
        "Answer_body":"<p>I managed it<\/p>\n<pre class=\"lang-rb prettyprint-override\"><code>client = Google::Cloud::AIPlatform::V1::PredictionService::Client.new do |config|\n  config.endpoint = &quot;#{location}-aiplatform.googleapis.com&quot;\nend\n\nimg = File.open(imgPath, 'rb') do |img|\n  Base64.strict_encode64(img.read)\nend\n\ninstance = Google::Protobuf::Value.new(:struct_value =&gt; {:fields =&gt; {\n  :content =&gt; {:string_value =&gt; img}\n}})\nendpoint = &quot;projects\/#{project}\/locations\/#{location}\/endpoints\/#{endpoint}&quot;\n\n\nrequest = Google::Cloud::AIPlatform::V1::PredictRequest.new(\n  endpoint: endpoint,\n  instances: [instance]\n)\n\nresult = client.predict request\np result\n<\/code><\/pre>\n<p>The use of the Google::Protobuf::Value looks ugly to me but it works<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":19459566.0,
        "Answerer_created_time":1656670919183,
        "Answerer_last_access_time":1659105111752,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1.0,
        "Answerer_views":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72827960",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1656947266916,
        "Question_original_content":"imag classif predict gcp aiplatform rubi new rubi want us gcp aiplatform struggel payload far client googl cloud aiplatform predictionservic client new config config endpoint locat aiplatform googleapi com end img file open imgpath img data imag png base base strict encod img read end instanc instanc new content img request googl cloud aiplatform predictrequest new endpoint project project locat locat endpoint endpoint instanc instanc result client predict request result proto messag instanc requir byte content follow error invalid type instanc assign submessag field instanc read document rubi sdk bit light paramet exampl http github com googleapi nodej platform blob main sampl predict imag object detect work paramet wrong",
        "Question_preprocessed_content":"imag classif predict gcp aiplatform rubi new rubi want us gcp aiplatform struggel payload far proto follow error read document rubi sdk bit light paramet exampl work paramet wrong",
        "Question_gpt_summary_original":"The user is facing challenges in using GCP AIPlatform in Ruby and is struggling with the payload. They have encountered an error message \"Invalid type Instance to assign to submessage field 'instances'\" while using the Instance message in their proto. The user has read the documentation but found it to be insufficient for the Ruby SDK. They are seeking help to identify the issue.",
        "Question_gpt_summary":"user face challeng gcp aiplatform rubi struggl payload encount error messag invalid type instanc assign submessag field instanc instanc messag proto user read document insuffici rubi sdk seek help identifi issu",
        "Answer_original_content":"manag client googl cloud aiplatform predictionservic client new config config endpoint locat aiplatform googleapi com end img file open imgpath img base strict encod img read end instanc googl protobuf valu new struct valu field content string valu img endpoint project project locat locat endpoint endpoint request googl cloud aiplatform predictrequest new endpoint endpoint instanc instanc result client predict request result us googl protobuf valu look ugli work",
        "Answer_preprocessed_content":"manag us googl protobuf valu look ugli work",
        "Answer_gpt_summary_original":"Solution: The user shared their solution to the challenge by using the Google::Protobuf::Value to encode the image file and pass it as an instance to the predict request. They also mentioned that the solution worked for them, even though they found it to be not aesthetically pleasing.",
        "Answer_gpt_summary":"solut user share solut challeng googl protobuf valu encod imag file pass instanc predict request mention solut work aesthet pleas"
    },
    {
        "Question_title":"Do Training Jobs Run in Parallel? (VERTEX AI)",
        "Question_body":"I am wondering if training jobs on vertex AI run in parallel, based on my tests it seems they do but wondering if anyone can confirm this is true as the number of concurrent jobs grows past say 1000. Thanks! ",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1668498960000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":70.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Yes training jobs run in parallel but the concurrency is subject to quota. See Vertex AI quota document.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Do-Training-Jobs-Run-in-Parallel-VERTEX-AI\/td-p\/489639\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1668513420000,
        "Question_original_content":"train job run parallel wonder train job run parallel base test wonder confirm true number concurr job grow past thank",
        "Question_preprocessed_content":"train job run parallel wonder train job run parallel base test wonder confirm true number concurr job grow past thank",
        "Question_gpt_summary_original":"The user is unsure if training jobs on Vertex AI run in parallel and is seeking confirmation, especially when the number of concurrent jobs exceeds 1000.",
        "Question_gpt_summary":"user unsur train job run parallel seek confirm especi number concurr job exce",
        "Answer_original_content":"ye train job run parallel concurr subject quota quota document view solut origin post",
        "Answer_preprocessed_content":"ye train job run parallel concurr subject quota quota document view solut origin post",
        "Answer_gpt_summary_original":"Solution: The discussion confirms that training jobs on Vertex AI run in parallel, but the number of concurrent jobs is subject to quota. The user can refer to the Vertex AI quota document for more information. No additional solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut discuss confirm train job run parallel number concurr job subject quota user refer quota document inform addit solut mention discuss"
    },
    {
        "Question_title":"AWS Sagemaker + AWS Lambda",
        "Question_body":"<p>I try to use AWS SageMaker following documentation. I successfully loaded data, trained and deployed the model.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4Mjew.png\" rel=\"nofollow noreferrer\">deployed-model<\/a><\/p>\n<p>My next step have to be using AWS Lambda, connect it to this SageMaker endpoint.\nI saw, that I need to give Lambda IAM execution role permission to invoke a model endpoint.\nI add some data to IAM policy JSON and now it has this view<\/p>\n<pre><code>{\n&quot;Version&quot;: &quot;2012-10-17&quot;,\n&quot;Statement&quot;: [\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;logs:CreateLogGroup&quot;,\n        &quot;Resource&quot;: &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:*&quot;\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: [\n            &quot;logs:CreateLogStream&quot;,\n            &quot;logs:PutLogEvents&quot;\n        ],\n        &quot;Resource&quot;: [\n            &quot;arn:aws:logs:us-east-1:&lt;my-account&gt;:log-group:\/aws\/lambda\/test-sagemaker:*&quot;\n        ]\n    },\n    {\n        &quot;Effect&quot;: &quot;Allow&quot;,\n        &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n        &quot;Resource&quot;: &quot;*&quot;\n    }\n]\n<\/code><\/pre>\n<p>}<\/p>\n<p>Problem that even with role that have permission for invoking SageMaker endpoint my Lambda function didn't see it<\/p>\n<pre><code>An error occurred (ValidationError) when calling the InvokeEndpoint operation: Endpoint xgboost-2020-10-02-12-15-36-097 of account &lt;my-account&gt; not found.: ValidationError\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1601650547150,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1601906646463,
        "Question_score":0,
        "Question_view_count":623.0,
        "Poster_id":13376024.0,
        "Poster_created_time":1587507179987,
        "Poster_last_access_time":1655319429432,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1.0,
        "Poster_views":5.0,
        "Answer_body":"<p>I found an error by myself. Problem was in different regions. For training and deploying model I used us-east-2 and for lambda I used us-east-1. Just creating all in same region fixed this issue!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":13376024.0,
        "Answerer_created_time":1587507179987,
        "Answerer_last_access_time":1655319429432,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1.0,
        "Answerer_views":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64173739",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1601906782043,
        "Question_original_content":"aw lambda try us follow document successfulli load data train deploi model deploi model step aw lambda connect endpoint saw need lambda iam execut role permiss invok model endpoint add data iam polici json view version statement effect allow action log createloggroup resourc arn aw log east effect allow action log createlogstream log putlogev resourc arn aw log east log group aw lambda test effect allow action invokeendpoint resourc problem role permiss invok endpoint lambda function error occur validationerror call invokeendpoint oper endpoint xgboost account validationerror",
        "Question_preprocessed_content":"aw lambda try us follow document successfulli load data train deploi model step aw lambda connect endpoint saw need lambda iam execut role permiss invok model endpoint add data iam polici json view problem role permiss invok endpoint lambda function",
        "Question_gpt_summary_original":"The user successfully loaded data, trained and deployed a model using AWS SageMaker, but encountered challenges when trying to connect AWS Lambda to the SageMaker endpoint. Despite adding data to the IAM policy JSON to give Lambda IAM execution role permission to invoke the model endpoint, the Lambda function was unable to see the endpoint, resulting in a validation error.",
        "Question_gpt_summary":"user successfulli load data train deploi model encount challeng try connect aw lambda endpoint despit ad data iam polici json lambda iam execut role permiss invok model endpoint lambda function unabl endpoint result valid error",
        "Answer_original_content":"error problem differ region train deploi model east lambda east creat region fix issu",
        "Answer_preprocessed_content":"error problem differ region train deploi model lambda creat region fix issu",
        "Answer_gpt_summary_original":"Solution: The user found that the issue was caused by using different regions for training and deploying the model and for the Lambda function. Creating all resources in the same region resolved the issue.",
        "Answer_gpt_summary":"solut user issu caus differ region train deploi model lambda function creat resourc region resolv issu"
    },
    {
        "Question_title":"How to get Model ID of the Latest Version registered in Azure Machine Learning Service Model Registry using az ml cli?",
        "Question_body":"<p>Using Azure Machine Learning CLI extension, how do we get the Model ID for the latest version of a Model (with known model name)?<\/p>\n<p>To get the entire list of Model Details with a given name the command is<\/p>\n<pre><code>az ml model list --model-name [Model_Name] --resource-group [RGP_NAME] --subscription-id [SUB_ID] --workspace-name [WS_NAME]\n<\/code><\/pre>\n<p>Running this will give a list of all the models:<\/p>\n<pre><code>[\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T07:02:03.814172+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:2&quot;\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 3\n  },\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T06:46:34.301054+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:2&quot;,\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 2\n  },\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T06:38:56.558385+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:1&quot;,\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 1\n  }\n]\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/model?view=azure-cli-latest#ext_azure_cli_ml_az_ml_model_list\" rel=\"nofollow noreferrer\">Microsoft Documentation<\/a> mentions, we can use a <code>-l<\/code> parameter to get the latest version details:<\/p>\n<pre><code>az ml model list --model-name [Model_Name] --resource-group [RGP_NAME] --subscription-id [SUB_ID] --workspace-name [WS_NAME] -l\n<\/code><\/pre>\n<p>However, running this gives the following error:<\/p>\n<pre><code>ERROR: UnrecognizedArgumentError: unrecognized arguments: -l\n<\/code><\/pre>\n<p>What is the syntax to use this <code>-l<\/code> flag?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_created_time":1616140269540,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":545.0,
        "Poster_id":14384792.0,
        "Poster_created_time":1601729162436,
        "Poster_last_access_time":1663774065772,
        "Poster_age":null,
        "Poster_location":"Bengaluru, Karnataka, India",
        "Poster_reputation":887.0,
        "Poster_views":130.0,
        "Answer_body":"<p>If we wish to obtain the model-id for the latest model, instead of using <code>az ml model<\/code> list with <code>-l<\/code> flag, using <code>az model show<\/code> will return the details for the latest model. The syntax to get a string for model-id will be:<\/p>\n<pre><code>az ml model show --model-id $(TRN_MODEL_ID) --resource-group $(AML_TRN_RG) --subscription-id $(AML_TRN_SUB_ID) --workspace-name $(AML_TRN_WS) --query name -o tsv\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":14384792.0,
        "Answerer_created_time":1601729162436,
        "Answerer_last_access_time":1663774065772,
        "Answerer_age":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation":887.0,
        "Answerer_views":130.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66704314",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1616423749203,
        "Question_original_content":"model latest version regist servic model registri cli cli extens model latest version model known model entir list model detail given command model list model model resourc group rgp subscript sub workspac run list model createdtim framework custom frameworkvers null model model version createdtim framework custom frameworkvers null model model version createdtim framework custom frameworkvers null model model version microsoft document mention us paramet latest version detail model list model model resourc group rgp subscript sub workspac run give follow error error unrecognizedargumenterror unrecogn argument syntax us flag",
        "Question_preprocessed_content":"model latest version regist servic model registri cli cli extens model latest version model entir list model detail given command run list model microsoft document mention us paramet latest version detail run give follow error syntax us flag",
        "Question_gpt_summary_original":"The user is trying to get the Model ID for the latest version of a Model using Azure Machine Learning CLI extension. They have tried using the command \"az ml model list\" with the \"-l\" parameter to get the latest version details, but it resulted in an error. The user is seeking help to understand the correct syntax to use the \"-l\" flag.",
        "Question_gpt_summary":"user try model latest version model cli extens tri command model list paramet latest version detail result error user seek help understand correct syntax us flag",
        "Answer_original_content":"wish obtain model latest model instead model list flag model return detail latest model syntax string model model model trn model resourc group aml trn subscript aml trn sub workspac aml trn queri tsv",
        "Answer_preprocessed_content":"wish obtain latest model instead list flag return detail latest model syntax string",
        "Answer_gpt_summary_original":"Solution: Instead of using \"az ml model list\" with the \"-l\" flag, the user can use \"az model show\" to obtain the model ID for the latest model. The syntax to get a string for model ID will be: \"az ml model show --model-id $(TRN_MODEL_ID) --resource-group $(AML_TRN_RG) --subscription-id $(AML_TRN_SUB_ID) --workspace-name $(AML_TRN_WS) --query name -o tsv\".",
        "Answer_gpt_summary":"solut instead model list flag user us model obtain model latest model syntax string model model model trn model resourc group aml trn subscript aml trn sub workspac aml trn queri tsv"
    },
    {
        "Question_title":"How to run authentication on a mlFlow server?",
        "Question_body":"<p>As I am logging my entire models and params into mlflow I thought it will be a good idea to have  it protected under a user name and password.<\/p>\n\n<p>I use the following code to run the mlflow server<\/p>\n\n<p><code>mlflow server --host 0.0.0.0 --port 11111<\/code>\nworks perfect,in mybrowser i type <code>myip:11111<\/code> and i see everything (which eventually is the problem)<\/p>\n\n<p>If I understood the documentation and the following <a href=\"https:\/\/groups.google.com\/forum\/#!topic\/mlflow-users\/E9QW4HdS8a8\" rel=\"noreferrer\">https:\/\/groups.google.com\/forum\/#!topic\/mlflow-users\/E9QW4HdS8a8<\/a> link here correct, I should use nginx to create the authentication.<\/p>\n\n<p>I installed <code>nginx open sourcre<\/code>  and <code>apache2-utils<\/code><\/p>\n\n<p>created <code>sudo htpasswd -c \/etc\/apache2\/.htpasswd user1<\/code> user and passwords.<\/p>\n\n<p>I edited my <code>\/etc\/nginx\/nginx.conf<\/code> to the following:<\/p>\n\n<pre><code>server {\n        listen 80;\n        listen 443 ssl;\n\n        server_name my_ip;\n        root NOT_SURE_WHICH_PATH_TO_PUT_HERE, THE VENV?;\n        location \/ {\n            proxy_pass                      my_ip:11111\/;\n            auth_basic                      \"Restricted Content\";\n            auth_basic_user_file \/home\/path to the password file\/.htpasswd;\n        }\n    }\n<\/code><\/pre>\n\n<p><strong>but no authentication appears.<\/strong><\/p>\n\n<p>if I change the conf to listen to  <code>listen 11111<\/code>\nI get an error that the port is already in use ( of course, by the mlflow server....)<\/p>\n\n<p>my wish is to have a authentication window before anyone can enter by the mlflow with a browser.<\/p>\n\n<p>would be happy to hear any suggestions.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_created_time":1574259400087,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":null,
        "Question_score":11,
        "Question_view_count":13870.0,
        "Poster_id":11306005.0,
        "Poster_created_time":1554298968016,
        "Poster_last_access_time":1662838530056,
        "Poster_age":null,
        "Poster_location":"wondeland",
        "Poster_reputation":1540.0,
        "Poster_views":118.0,
        "Answer_body":"<p>the problem here is that both <code>mlflow<\/code> and <code>nginx<\/code> are trying to run on the <strong>same port<\/strong>... <\/p>\n\n<ol>\n<li><p>first lets deal with nginx:<\/p>\n\n<p>1.1 in \/etc\/nginx\/sites-enable make a new file <code>sudo nano mlflow<\/code> and delete the exist default.<\/p>\n\n<p>1.2 in mlflow file:<\/p><\/li>\n<\/ol>\n\n<pre><code>server {\n    listen YOUR_PORT;\n    server_name YOUR_IP_OR_DOMAIN;\n    auth_basic           \u201cAdministrator\u2019s Area\u201d;\n    auth_basic_user_file \/etc\/apache2\/.htpasswd; #read the link below how to set username and pwd in nginx\n\n    location \/ {\n        proxy_pass http:\/\/localhost:8000;\n        include \/etc\/nginx\/proxy_params;\n        proxy_redirect off;\n    }\n}\n<\/code><\/pre>\n\n<p>1.3.  restart nginx <code>sudo systemctl restart nginx<\/code><\/p>\n\n<ol start=\"2\">\n<li>on your server run mlflow  <code>mlflow server --host localhost --port 8000<\/code><\/li>\n<\/ol>\n\n<p>Now if you try access the YOUR_IP_OR_DOMAIN:YOUR_PORT within your browser an auth popup should appear, enter your host and pass and now you in mlflow<\/p>\n\n<ol start=\"3\">\n<li><p>now there are 2 options to tell the mlflow server about it:<\/p>\n\n<p>3.1 set username and pwd as environment variable \n<code>export MLFLOW_TRACKING_USERNAME=user export MLFLOW_TRACKING_PASSWORD=pwd<\/code><\/p>\n\n<p>3.2 edit in your <code>\/venv\/lib\/python3.6\/site-packages\/mlflowpackages\/mlflow\/tracking\/_tracking_service\/utils.py<\/code> the function <\/p><\/li>\n<\/ol>\n\n<pre><code>def _get_rest_store(store_uri, **_):\n    def get_default_host_creds():\n        return rest_utils.MlflowHostCreds(\n            host=store_uri,\n            username=replace with nginx user\n            password=replace with nginx pwd\n            token=os.environ.get(_TRACKING_TOKEN_ENV_VAR),\n            ignore_tls_verification=os.environ.get(_TRACKING_INSECURE_TLS_ENV_VAR) == 'true',\n        )\n<\/code><\/pre>\n\n<p>in your .py file where you work with mlflow:<\/p>\n\n<pre><code>import mlflow\nremote_server_uri = \"YOUR_IP_OR_DOMAIN:YOUR_PORT\" # set to your server URI\nmlflow.set_tracking_uri(remote_server_uri)\nmlflow.set_experiment(\"\/my-experiment\")\nwith mlflow.start_run():\n    mlflow.log_param(\"a\", 1)\n    mlflow.log_metric(\"b\", 2)\n<\/code><\/pre>\n\n<p>A link to nginx authentication doc <a href=\"https:\/\/docs.nginx.com\/nginx\/admin-guide\/security-controls\/configuring-http-basic-authentication\/\" rel=\"noreferrer\">https:\/\/docs.nginx.com\/nginx\/admin-guide\/security-controls\/configuring-http-basic-authentication\/<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Answerer_id":11306005.0,
        "Answerer_created_time":1554298968016,
        "Answerer_last_access_time":1662838530056,
        "Answerer_age":null,
        "Answerer_location":"wondeland",
        "Answerer_reputation":1540.0,
        "Answerer_views":118.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58956459",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1576255052616,
        "Question_original_content":"run authent server log entir model param thought good idea protect user password us follow code run server server host port work perfect mybrows type myip eventu problem understood document follow http group googl com forum topic user eqwhdsa link correct us nginx creat authent instal nginx open sourcr apach util creat sudo htpasswd apach htpasswd user user password edit nginx nginx conf follow server listen listen ssl server root sure path venv locat proxi pass auth basic restrict content auth basic user file home path password file htpasswd authent appear chang conf listen listen error port us cours server wish authent window enter browser happi hear suggest",
        "Question_preprocessed_content":"run authent server log entir model param thought good idea protect user password us follow code run server work perfect mybrows type understood document follow link correct us nginx creat authent instal creat user password edit follow authent appear chang conf listen error port us wish authent window enter browser happi hear suggest",
        "Question_gpt_summary_original":"The user is facing challenges in implementing authentication for their mlFlow server. They have installed nginx and apache2-utils, created a user and password, and edited their nginx.conf file, but authentication is not appearing. They are seeking suggestions to implement an authentication window before anyone can enter the mlFlow server with a browser.",
        "Question_gpt_summary":"user face challeng implement authent server instal nginx apach util creat user password edit nginx conf file authent appear seek suggest implement authent window enter server browser",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"AWS SageMaker - Extending Pre-built Container, Deploy Endpoint Failed. No such file or directory: 'serve'\"",
        "Question_body":"I am trying to deploy the SageMaker Inference Endpoint by extending the Pre-built image. However, it failed with \"FileNotFoundError: [Errno 2] No such file or directory: 'serve'\"\n\nMy Dockerfile\n\nARG REGION=us-west-2\n\n# SageMaker PyTorch image\nFROM 763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2\n\nRUN apt-get update\n\nENV PATH=\"\/opt\/ml\/code:${PATH}\"\n\n# this environment variable is used by the SageMaker PyTorch container to determine our user code directory.\nENV SAGEMAKER_SUBMIT_DIRECTORY \/opt\/ml\/code\n\n# \/opt\/ml and all subdirectories are utilized by SageMaker, use the \/code subdirectory to store your user code.\nCOPY inference.py \/opt\/ml\/code\/inference.py\n\n# Defines inference.py as script entrypoint \nENV SAGEMAKER_PROGRAM inference.py\n\n\nCloudWatch Log From \/aws\/sagemaker\/Endpoints\/mytestEndpoint\n\n2022-09-30T04:47:09.178-07:00\nTraceback (most recent call last):\n  File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module>\n    subprocess.check_call(shlex.split(' '.join(sys.argv[1:])))\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call\n    retcode = call(*popenargs, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call\n    with Popen(*popenargs, **kwargs) as p:\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child\n    raise child_exception_type(errno_num, err_msg, err_filename)\nTraceback (most recent call last): File \"\/usr\/local\/bin\/dockerd-entrypoint.py\", line 20, in <module> subprocess.check_call(shlex.split(' '.join(sys.argv[1:]))) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 359, in check_call retcode = call(*popenargs, **kwargs) File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 340, in call with Popen(*popenargs, **kwargs) as p: File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 858, in __init__ self._execute_child(args, executable, preexec_fn, close_fds, File \"\/opt\/conda\/lib\/python3.8\/subprocess.py\", line 1704, in _execute_child raise child_exception_type(errno_num, err_msg, err_filename)\n\n2022-09-30T04:47:13.409-07:00\nFileNotFoundError: [Errno 2] No such file or directory: 'serve'",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1664542013756,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":97.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Should use the Sagemaker image\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-sagemaker\n\n\ninstead of ec2\n\n763104351884.dkr.ecr.$REGION.amazonaws.com\/pytorch-inference:1.12.1-gpu-py38-cu116-ubuntu20.04-ec2",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUR-uTDaDsQBGjMoAUcsi2sQ\/aws-sage-maker-extending-pre-built-container-deploy-endpoint-failed-no-such-file-or-directory-serve",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1664550898846,
        "Question_original_content":"extend pre built contain deploi endpoint fail file directori serv try deploi infer endpoint extend pre built imag fail filenotfounderror errno file directori serv dockerfil arg region west pytorch imag dkr ecr region amazonaw com pytorch infer gpu ubuntu run apt updat env path opt code path environ variabl pytorch contain determin user code directori env submit directori opt code opt subdirectori util us code subdirectori store user code copi infer opt code infer defin infer script entrypoint env program infer cloudwatch log aw endpoint mytestendpoint traceback recent file usr local bin dockerd entrypoint line subprocess check shlex split join sy argv file opt conda lib python subprocess line check retcod popenarg kwarg file opt conda lib python subprocess line popen popenarg kwarg file opt conda lib python subprocess line init self execut child arg execut preexec close fd file opt conda lib python subprocess line execut child rais child except type errno num err msg err filenam traceback recent file usr local bin dockerd entrypoint line subprocess check shlex split join sy argv file opt conda lib python subprocess line check retcod popenarg kwarg file opt conda lib python subprocess line popen popenarg kwarg file opt conda lib python subprocess line init self execut child arg execut preexec close fd file opt conda lib python subprocess line execut child rais child except type errno num err msg err filenam filenotfounderror errno file directori serv",
        "Question_preprocessed_content":"extend contain deploi endpoint fail file directori serv try deploi infer endpoint extend imag fail filenotfounderror file directori serv dockerfil arg pytorch imag run updat env environ variabl pytorch contain determin user code directori env subdirectori util us code subdirectori store user code copi defin script entrypoint env cloudwatch log traceback file line file line retcod file line popen file line execut file line rais traceback file line file line retcod file line popen file line execut file line rais filenotfounderror file directori serv",
        "Question_gpt_summary_original":"The user encountered a challenge while trying to deploy the SageMaker Inference Endpoint by extending the pre-built image. The deployment failed with a \"FileNotFoundError\" indicating that the file 'serve' could not be found. The user provided a Dockerfile and a CloudWatch log that shows the error message.",
        "Question_gpt_summary":"user encount challeng try deploi infer endpoint extend pre built imag deploy fail filenotfounderror indic file serv user provid dockerfil cloudwatch log show error messag",
        "Answer_original_content":"us imag dkr ecr region amazonaw com pytorch infer gpu ubuntu instead dkr ecr region amazonaw com pytorch infer gpu ubuntu",
        "Answer_preprocessed_content":"us imag instead",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use the Sagemaker image instead of the ec2 image. The Sagemaker image is suggested as it contains the necessary files and dependencies required for deploying the SageMaker Inference Endpoint.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us imag instead imag imag suggest contain necessari file depend requir deploi infer endpoint"
    },
    {
        "Question_title":"aws sagemaker for detecting text in an image",
        "Question_body":"<p>I am aware that it is better to use aws Rekognition for this. However, it does not seem to work well when I tried it out with the images I have (which are sort of like small containers with labels on them). The text comes out misspelled and fragmented.<\/p>\n\n<p>I am new to ML and sagemaker. From what I have seen, the use cases seem to be for prediction and image classification. I could not find one on training a model for detecting text in an image. Is it possible to to do it with Sagemaker? I would appreciate it if someone pointed me in the right direction.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1549300262230,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":4,
        "Question_view_count":2255.0,
        "Poster_id":4104481.0,
        "Poster_created_time":1412300825550,
        "Poster_last_access_time":1663958318848,
        "Poster_age":null,
        "Poster_location":"West Lafayette, IN, United States",
        "Poster_reputation":463.0,
        "Poster_views":111.0,
        "Answer_body":"<p>The different services will all provide different levels of abstraction for Optical Character Recognition (OCR) depending on what parts of the pipeline you are most comfortable with working with, and what you prefer to have abstracted.<\/p>\n\n<p>Here are a few options:<\/p>\n\n<ul>\n<li><p><strong>Rekognition<\/strong> will provide out of the box OCR with the <a href=\"https:\/\/docs.aws.amazon.com\/rekognition\/latest\/dg\/text-detecting-text-procedure.html\" rel=\"nofollow noreferrer\">DetectText<\/a> feature. However, it seems you will need to perform some sort of pre-processing on your images in your current case in order to get better results. This can be done through any method of your choice (Lambda, EC2, etc).<\/p><\/li>\n<li><p><strong>SageMaker<\/strong> is a tool that will enable you to easily train and deploy your own models (of any type). You have two primary options with SageMaker:<\/p>\n\n<ol>\n<li><p><em>Do-it-yourself option<\/em>: If you're looking to go the route of labeling your own data, gathering a sizable training set, and training your own OCR model, this is possible by training and deploying your own model via SageMaker.<\/p><\/li>\n<li><p><em>Existing OCR algorithm<\/em>: There are many algorithms out there that all have different potential tradeoffs for OCR. One example would be <a href=\"https:\/\/github.com\/tesseract-ocr\/tesseract\" rel=\"nofollow noreferrer\">Tesseract<\/a>. Using this, you can more closely couple your pre-processing step to the text detection.<\/p><\/li>\n<\/ol><\/li>\n<li><p><a href=\"https:\/\/aws.amazon.com\/textract\/\" rel=\"nofollow noreferrer\"><strong>Amazon Textract<\/strong><\/a> (In preview) is a purpose-built dedicated OCR service that may offer better performance depending on what your images look like and the settings you choose. <\/p><\/li>\n<\/ul>\n\n<p>I would personally recommend looking into <a href=\"https:\/\/docparser.com\/blog\/improve-ocr-accuracy\/\" rel=\"nofollow noreferrer\">pre-processing for OCR<\/a> to see if it improves Rekognition accuracy before moving onto the other options. Even if it doesn't improve Rekognition's accuracy, it will still be valuable for most of the other options!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Answerer_id":10885720.0,
        "Answerer_created_time":1546969667040,
        "Answerer_last_access_time":1649085825632,
        "Answerer_age":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation":1689.0,
        "Answerer_views":170.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54521080",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1549313202943,
        "Question_original_content":"detect text imag awar better us aw rekognit work tri imag sort like small contain label text come misspel fragment new seen us case predict imag classif train model detect text imag possibl appreci point right direct",
        "Question_preprocessed_content":"detect text imag awar better us aw rekognit work tri imag text come misspel fragment new seen us case predict imag classif train model detect text imag possibl appreci point right direct",
        "Question_gpt_summary_original":"The user is facing challenges with using aws Rekognition to detect text in images, as the text comes out misspelled and fragmented. They are new to ML and sagemaker and are unsure if it is possible to train a model for detecting text in an image using Sagemaker. They are seeking guidance on how to proceed.",
        "Question_gpt_summary":"user face challeng aw rekognit detect text imag text come misspel fragment new unsur possibl train model detect text imag seek guidanc proce",
        "Answer_original_content":"differ servic provid differ level abstract optic charact recognit ocr depend part pipelin comfort work prefer abstract option rekognit provid box ocr detecttext featur need perform sort pre process imag current case order better result method choic lambda tool enabl easili train deploi model type primari option option look rout label data gather sizabl train set train ocr model possibl train deploi model exist ocr algorithm algorithm differ potenti tradeoff ocr exampl tesseract close coupl pre process step text detect amazon textract preview purpos built dedic ocr servic offer better perform depend imag look like set choos person recommend look pre process ocr improv rekognit accuraci move option improv rekognit accuraci valuabl option",
        "Answer_preprocessed_content":"differ servic provid differ level abstract optic charact recognit depend part pipelin comfort work prefer abstract option rekognit provid box ocr detecttext featur need perform sort imag current case order better result method choic tool enabl easili train deploi model primari option option look rout label data gather sizabl train set train ocr model possibl train deploi model exist ocr algorithm algorithm differ potenti tradeoff ocr exampl tesseract close coupl step text detect amazon textract dedic ocr servic offer better perform depend imag look like set choos person recommend look ocr improv rekognit accuraci move option improv rekognit accuraci valuabl option",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Pre-processing the images before using Rekognition's DetectText feature to improve accuracy.\n2. Using SageMaker to train and deploy a custom OCR model or using an existing OCR algorithm like Tesseract.\n3. Trying Amazon Textract, a dedicated OCR service that may offer better performance.\n\nThe user is advised to look into pre-processing for OCR to see if it improves Rekognition accuracy before moving onto the other options.",
        "Answer_gpt_summary":"possibl solut mention discuss pre process imag rekognit detecttext featur improv accuraci train deploi custom ocr model exist ocr algorithm like tesseract try amazon textract dedic ocr servic offer better perform user advis look pre process ocr improv rekognit accuraci move option"
    },
    {
        "Question_title":"Machine Learning\u306b\u3064\u3044\u3066\u306e\u8cea\u554f",
        "Question_body":"\u63b2\u984c\u306e\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u73fe\u5728Machine Learning\u3092\u4f7f\u7528\u3057\u3066\u6a5f\u68b0\u5b66\u7fd2\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u305d\u3053\u3067\u8cea\u554f\u306b\u306a\u308b\u306e\u3067\u3059\u304c\u3001\u30c7\u30b6\u30a4\u30ca\u30fc\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u5b66\u7fd2\u7d50\u679c\u3092CSV\u3067\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304c\u3001\nExport Data\u30e2\u30c7\u30eb\u3067CSV\u5f62\u5f0f\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u3066\u3082CSV\u3067\u306f\u306a\u3044\u5f62\u5f0f\u3067\u5171\u6709\u305b\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3059\u304c\u3001\u539f\u56e0\u304c\u308f\u304b\u3089\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002\n\u3054\u6559\u793a\u306e\u307b\u3069\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1631251067517,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@63862379 Are you referring to the export data module of the designer from ml.azure.com?\nI think I understand the issue, Are you seeing that the .csv format of file is not listed on the blob storage?\n\nSince the input is a dataframe directory to export module the output format selected should still be the format you selected, in this case CSV. The file name extension only might be missing. You can still open the csv file in excel and it will recognize the delimiters and headers so you can convert it into excel files.\n\nYou can also avoid this by providing the .csv extension in the path itself in export settings and file will be exported as a csv file directly.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/546760\/machine-learning%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E8%B3%AA%E5%95%8F.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1631268910236,
        "Question_original_content":"machin learn machin learn csv export datacsvcsv",
        "Question_preprocessed_content":"machin learn machin learn csv export datacsvcsv",
        "Question_gpt_summary_original":"The user is facing challenges with exporting machine learning results in CSV format using the designer function. Despite setting the Export Data model to CSV format, the shared file is not in CSV format, and the user is unsure of the cause of the issue.",
        "Question_gpt_summary":"user face challeng export machin learn result csv format design function despit set export data model csv format share file csv format user unsur caus issu",
        "Answer_original_content":"refer export data modul design azur com think understand issu see csv format file list blob storag input datafram directori export modul output format select format select case csv file extens miss open csv file excel recogn delimit header convert excel file avoid provid csv extens path export set file export csv file directli forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"refer export data modul design think understand issu see csv format file list blob storag input datafram directori export modul output format select format select case csv file extens miss open csv file excel recogn delimit header convert excel file avoid provid csv extens path export set file export csv file directli forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_gpt_summary_original":"Solutions provided:\n- The file may still be in CSV format even if the file extension is missing. The user can try opening the file in Excel and it should recognize the delimiters and headers, allowing the user to convert it into an Excel file.\n- The user can provide the .csv extension in the path itself in export settings to ensure that the file is exported as a CSV file directly.",
        "Answer_gpt_summary":"solut provid file csv format file extens miss user try open file excel recogn delimit header allow user convert excel file user provid csv extens path export set ensur file export csv file directli"
    },
    {
        "Question_title":"Kubeflow Pipeline Training Component Failing | Unknown return type: <class 'inspect._empty'>",
        "Question_body":"<p>I am running an ML pipeline and the training component\/step (see code below) continues to fail with the following error: &quot;RuntimeError: Unknown return type: &lt;class 'inspect._empty'&gt;. Must be one of <code>str<\/code>, <code>int<\/code>, <code>float<\/code>, a subclass of <code>Artifact<\/code>, or a NamedTuple collection of these types.&quot;<\/p>\n<p>Any ideas on what might be causing the issue\/error and how to resolve it?<\/p>\n<p>Thank you!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n@component(\n    # this component builds an xgboost classifier with xgboost\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;\n)\n\ndef build_xgb_xgboost(project: str, \n                            bq_dataset: str, \n                            test_view_name: str,\n                            bq_location: str,\n                            metrics: Output[Metrics],\n                            model: Output[Model]\n\n):\n    from google.cloud import bigquery\n    import xgboost as xgb\n    import pandas as pd\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import mean_squared_error as MSE\n    from sklearn.metrics import mean_absolute_error\n    import joblib\n    import pyarrow\n    import db_dtypes\n     \n\n    client = bigquery.Client(project=project) \n\n    view_uri = f&quot;{project}.{bq_dataset}.{test_view_name}&quot; #replace view_name with test_view_name\n    \n    build_df_for_xgboost = '''\n    SELECT * FROM `{view_uri}`\n    '''.format(view_uri = view_uri)\n\n    job_config = bigquery.QueryJobConfig()\n    df_1 = client.query(build_df_for_xgboost).to_dataframe()\n    \n    #client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  \n    \n    df = df_1.drop(['int64_field_0'], axis=1)\n    \n    def onehot_encode(df, column):\n        df = df.copy()\n        dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n    \n    # Binary encoding\n    df['preferred_foot'] = df['preferred_foot'].replace({'left': 0, 'right': 1})\n    \n    # One-hot encoding\n    for column in ['attacking_work_rate', 'defensive_work_rate']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['overall_rating']\n    X = df.drop('overall_rating', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n\n    #specify parameters\n    \n    #define your model \n    bst = XGBRegressor(\n    objective='reg:linear',\n    learning_rate = '.1',\n    alpha = '0.001'\n    )\n    \n    #fit your model\n    bst.fit(X_train, y_train)\n    \n    # Predict the model \n    y_pred = bst.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    metrics.log_metric(&quot;RMSE&quot;, rmse)\n    metrics.log_metric(&quot;framework&quot;, &quot;xgboost&quot;)\n    metrics.log_metric(&quot;dataset_size&quot;, len(df))\n    metrics.log_metric(&quot;MAE&quot;, mae)\n    \n    dump(bst, model.path + &quot;.joblib&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1655350561587,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1659256989790,
        "Question_score":0,
        "Question_view_count":107.0,
        "Poster_id":15994848.0,
        "Poster_created_time":1621620820567,
        "Poster_last_access_time":1660057756750,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":29.0,
        "Poster_views":16.0,
        "Answer_body":"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.<\/p>\n<p>I mostly use the stable KFPv1 methods to avoid problems.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\ndef train_xgboost_model(\n    project: str, \n    bq_dataset: str, \n    test_view_name: str,\n    bq_location: str,\n    metrics_path: OutputPath(Metrics),\n    model_path: OutputPath(Model),\n):\n    import json\n    from pathlib import Path\n\n    metrics = {\n       ...\n    }\n    Path(metrics_path).write_text(json.dumps(metrics))\n\n    dump(bst, model_path)\n\ntrain_xgboost_model_op = create_component_from_func(\n    func=train_xgboost_model,\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,\n)\n\n<\/code><\/pre>\n<p>You can also find many examples of real-world components in this repo: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components<\/a><\/p>\n<p>including an XGBoost trainer <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py<\/a><\/p>\n<p>and a full XGBoost pipeline: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":1497385.0,
        "Answerer_created_time":1341273154903,
        "Answerer_last_access_time":1664077379792,
        "Answerer_age":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation":5996.0,
        "Answerer_views":666.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72640182",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659257407332,
        "Question_original_content":"kubeflow pipelin train compon fail unknown return type run pipelin train compon step code continu fail follow error runtimeerror unknown return type str int float subclass artifact namedtupl collect type idea caus issu error resolv thank compon compon build xgboost classifi xgboost packag instal googl cloud bigqueri xgboost panda sklearn joblib pyarrow dtype base imag python output compon file creat xgb model xgboost yaml def build xgb xgboost project str dataset str test view str locat str metric output metric model output model googl cloud import bigqueri import xgboost xgb import panda xgboost import xgbregressor sklearn model select import train test split sklearn preprocess import standardscal sklearn metric import mean squar error mse sklearn metric import mean absolut error import joblib import pyarrow import dtype client bigqueri client project project view uri project dataset test view replac view test view build xgboost select view uri format view uri view uri job config bigqueri queryjobconfig client queri build xgboost datafram client queri build xgboost job config job config datafram drop int field axi def onehot encod column copi dummi dummi column prefix column concat dummi axi drop column axi return binari encod prefer foot prefer foot replac left right hot encod column attack work rate defens work rate onehot encod column column split overal rate drop overal rate axi train test split train test train test train test split train size shuffl true random state scale scaler standardscal scaler fit train train datafram scaler transform train index train index column train column test datafram scaler transform test index test index column test column specifi paramet defin model bst xgbregressor object reg linear learn rate alpha fit model bst fit train train predict model pred bst predict test rmse sqrt mean test pred mae mean absolut error test pred metric log metric rmse rmse metric log metric framework xgboost metric log metric dataset size len metric log metric mae mae dump bst model path joblib",
        "Question_preprocessed_content":"kubeflow pipelin train compon fail unknown return type run pipelin train continu fail follow error runtimeerror unknown return type subclass namedtupl collect idea caus resolv thank",
        "Question_gpt_summary_original":"The user is encountering an error while running an ML pipeline's training component\/step. The error message states that the return type is unknown and must be one of the specified types. The user is seeking help to identify the cause of the issue and resolve it. The code for the training component is also provided.",
        "Question_gpt_summary":"user encount error run pipelin train compon step error messag state return type unknown specifi type user seek help identifi caus issu resolv code train compon provid",
        "Answer_original_content":"think bug version kfp sdk code us stabl kfpv method avoid problem kfp compon import inputpath outputpath creat compon func def train xgboost model project str dataset str test view str locat str metric path outputpath metric model path outputpath model import json pathlib import path metric path metric path write text json dump metric dump bst model path train xgboost model creat compon func func train xgboost model packag instal googl cloud bigqueri xgboost panda sklearn joblib pyarrow dtype base imag python output compon file creat xgb model xgboost yaml exampl real world compon repo http github com ark kun pipelin compon tree master compon includ xgboost trainer http github com ark kun pipelin compon blob dccf compon xgboost train compon xgboost pipelin http github com ark kun pipelin compon blob fbefeaafbaddbea sampl googl cloud vertex train tabular regress model xgboost import vertex pipelin",
        "Answer_preprocessed_content":"think bug version kfp sdk code us stabl kfpv method avoid problem exampl compon repo includ xgboost trainer xgboost pipelin",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Question_body":"I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.\n\nFeatures in https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python are neat, but that of having API docs\/swagger for the webservice seems missing.\n\nHaving some documentation is useful especially if the model takes in input several features of different type.\n\nTo overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.\n\nCan I do something equivalent to this with AKS in AzureML instead? If so, how?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1600897231890,
        "Question_favorite_count":7.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@DavideFiocco-7346 The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available here. You can print your swagger_uri of the web service and check if it confirms with the specifications you are creating currently.\n\nIf the above response helps, please accept the response as answer. Thanks!!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1600930445547,
        "Question_original_content":"add openapi specif webservic deploi ak like deploi machin learn servic ak need add openapi specif featur http doc microsoft com azur machin learn deploi azur kubernet servic tab python neat have api doc swagger webservic miss have document us especi model take input featur differ type overcom current model train includ docker contain us python fastapi librari build api openapi swagger spec deploi host equival ak instead",
        "Question_preprocessed_content":"add openapi specif webservic deploi ak like deploi machin learn servic ak need add openapi specif featur neat have api webservic miss have document us especi model take input featur differ type overcom current model train includ docker contain us python fastapi librari build api spec deploi host equival ak instead",
        "Question_gpt_summary_original":"The user is facing a challenge in adding OpenAPI specification to a machine learning service deployed using AzureML on AKS. The user finds the API documentation missing in the features provided by AzureML and is currently using Docker containers with FastAPI library to build the API and OpenAPI\/Swagger specs. The user is seeking guidance on how to add OpenAPI specification to the webservice deployed with AzureML in AKS.",
        "Question_gpt_summary":"user face challeng ad openapi specif machin learn servic deploi ak user find api document miss featur provid current docker contain fastapi librari build api openapi swagger spec user seek guidanc add openapi specif webservic deploi ak",
        "Answer_original_content":"davidefiocco deploy provid swagger specif uri directli document avail print swagger uri web servic check confirm specif creat current respons help accept respons answer thank",
        "Answer_preprocessed_content":"deploy provid swagger specif uri directli document avail print web servic check confirm specif creat current respons help accept respons answer thank",
        "Answer_gpt_summary_original":"Solution: The user can use the swagger specification URI provided by Azure ML deployments to add OpenAPI specification to the webservice deployed with AzureML in AKS. The documentation for this is available on the Azure ML website. The user can print the swagger_uri of the web service and check if it confirms with the specifications they are creating currently.",
        "Answer_gpt_summary":"solut user us swagger specif uri provid deploy add openapi specif webservic deploi ak document avail websit user print swagger uri web servic check confirm specif creat current"
    },
    {
        "Question_title":"How to setup AWS sagemaker - Resource limit Error",
        "Question_body":"<p>I'm trying to set up my first SageMaker Studio so my team and myself can run some post processing scripts in a shared environment but I'm having issues.<\/p>\n<p>I've followed the steps in this video(<a href=\"https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=wiDHCWVrjCU&amp;ab_channel=AmazonWebServices<\/a>) which are:<\/p>\n<ol>\n<li>Select Standard setup<\/li>\n<li>Select AWS Identity and Access Management (IAM)<\/li>\n<li>Under permissions - Create and select new execution role<\/li>\n<li>Under Network and storage - Select VPC, Subnet and Security group<\/li>\n<li>Hit the submit button at the bottom of the page.<\/li>\n<\/ol>\n<p>In the video, he clicks submit and is taken to the control panel where he starts the next phase of adding users, however I'm greeted with this error.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/g4k2g.png\" rel=\"nofollow noreferrer\"> Resource limit Error<\/a><\/p>\n<p>I've checked my Registered domains under route 53 and it says No domains to display, I've also checked my S2 and I have no instances so I have no idea where the 2 domains being utilized are.<\/p>\n<p>My dashboard, image and Notebooks are all empty so as far as I know there's nothing setup on this Sage Maker account.<\/p>\n<p>Could anyone tell me how to resolve this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1615398273417,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":237.0,
        "Poster_id":10018352.0,
        "Poster_created_time":1530468231707,
        "Poster_last_access_time":1663982618347,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":357.0,
        "Poster_views":72.0,
        "Answer_body":"<p>You can have maximum 1 studio domain per region, by the default limits. Though, it seems like you have two domains already provisioned. Try to delete all the domains through the AWS cli and recreate with the AWS Management Console.<\/p>\n<p>Unfortunately, AWS Management Console cannot visualize more than one Studio domain.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":3891961.0,
        "Answerer_created_time":1406731060412,
        "Answerer_last_access_time":1661378385252,
        "Answerer_age":null,
        "Answerer_location":"Washington, USA",
        "Answerer_reputation":139.0,
        "Answerer_views":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66570138",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1626975605176,
        "Question_original_content":"setup resourc limit error try set studio team run post process script share environ have issu follow step video http youtub com watch widhcwvrjcu channel amazonwebservic select standard setup select aw ident access manag iam permiss creat select new execut role network storag select vpc subnet secur group hit submit button page video click submit taken control panel start phase ad user greet error resourc limit error check regist domain rout sai domain displai check instanc idea domain util dashboard imag notebook far know setup sage maker account tell resolv error",
        "Question_preprocessed_content":"setup resourc limit error try set studio team run post process script share environ have issu follow step select standard setup select aw ident access manag permiss creat select new execut role network storag select vpc subnet secur group hit submit button page video click submit taken control panel start phase ad user greet error resourc limit error check regist domain rout sai domain displai check instanc idea domain util dashboard imag notebook far know setup sage maker account tell resolv error",
        "Question_gpt_summary_original":"The user is encountering a \"Resource limit Error\" while setting up their first SageMaker Studio to run post-processing scripts in a shared environment. They followed the steps in a video tutorial but are unable to proceed due to the error. The user has checked their Registered domains under route 53 and S2 but found no instances. They are seeking help to resolve the error.",
        "Question_gpt_summary":"user encount resourc limit error set studio run post process script share environ follow step video tutori unabl proce error user check regist domain rout instanc seek help resolv error",
        "Answer_original_content":"maximum studio domain region default limit like domain provis try delet domain aw cli recreat aw manag consol unfortun aw manag consol visual studio domain",
        "Answer_preprocessed_content":"maximum studio domain region default limit like domain provis try delet domain aw cli recreat aw manag consol unfortun aw manag consol visual studio domain",
        "Answer_gpt_summary_original":"Solution: The user can try to delete all the domains through the AWS cli and recreate with the AWS Management Console. However, it should be noted that the AWS Management Console cannot visualize more than one Studio domain.",
        "Answer_gpt_summary":"solut user try delet domain aw cli recreat aw manag consol note aw manag consol visual studio domain"
    },
    {
        "Question_title":"Models registered in workspace do not show up in Designer (preview)",
        "Question_body":"<p>I registered a model in my AML workspace, and I can see it in the Model List:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rtL5Q.png\" rel=\"nofollow noreferrer\">Model List view<\/a><\/p>\n<p>But I cannot see it in Designer (preview), which prevents me from using the new model there.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WpvIb.png\" rel=\"nofollow noreferrer\">Designer view<\/a><\/p>\n<p>Looks like a bug to me. Datasets work fine.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_created_time":1596056234913,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":81.0,
        "Poster_id":10095494.0,
        "Poster_created_time":1531852372996,
        "Poster_last_access_time":1599552407772,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":23.0,
        "Poster_views":4.0,
        "Answer_body":"<p>This is known issue as the models registered in workspace cannot be consumed in Designer without the new custom module capability (in private preview) available.<\/p>\n<p>The models showing up in Designer today are these generated from Designer training -&gt; inference pipeline conversion and can only be used in Designer (not registered in the workspace).\nWe have an effort ongoing to reduce the confusion.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1596073668927,
        "Answer_score":1.0,
        "Answerer_id":11297406.0,
        "Answerer_created_time":1554186784008,
        "Answerer_last_access_time":1662988357176,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":2175.0,
        "Answerer_views":434.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63162310",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1596073257467,
        "Question_original_content":"model regist workspac design preview regist model aml workspac model list model list view design preview prevent new model design view look like bug dataset work fine",
        "Question_preprocessed_content":"model regist workspac design regist model aml workspac model list model list view design prevent new model design view look like bug dataset work fine",
        "Question_gpt_summary_original":"The user has encountered a challenge where a model registered in their AML workspace is not showing up in Designer (preview), preventing them from using the new model. The user suspects it to be a bug as datasets work fine.",
        "Question_gpt_summary":"user encount challeng model regist aml workspac show design preview prevent new model user suspect bug dataset work fine",
        "Answer_original_content":"known issu model regist workspac consum design new custom modul capabl privat preview avail model show design todai gener design train infer pipelin convers design regist workspac effort ongo reduc confus",
        "Answer_preprocessed_content":"known issu model regist workspac consum design new custom modul capabl avail model show design todai gener design train infer pipelin convers design effort ongo reduc confus",
        "Answer_gpt_summary_original":"Solutions provided: There is currently no solution provided for the challenge. The discussion mentions that the issue is a known bug and that there is an ongoing effort to reduce confusion. However, it is also mentioned that models registered in the workspace cannot be consumed in Designer without the new custom module capability, which is currently in private preview.",
        "Answer_gpt_summary":"solut provid current solut provid challeng discuss mention issu known bug ongo effort reduc confus mention model regist workspac consum design new custom modul capabl current privat preview"
    },
    {
        "Question_title":"ModuleNotFound Error - Azure ML with prebuilt docker image",
        "Question_body":"<p>I have developed a module which works perfectly when executed locally.<\/p>\n<p>I have created an environment on azure using a prebuilt docker image found here:\n<strong>&quot;azureml\/minimal-ubuntu18.04-py37-cpu-inference&quot;<\/strong> <a href=\"https:\/\/mcr.microsoft.com\/v2\/_catalog\" rel=\"nofollow noreferrer\">https:\/\/mcr.microsoft.com\/v2\/_catalog<\/a>\n. Also, Using pythonScriptStep, to run a pipeline. Here is how the step looks<\/p>\n<pre><code>StepPreprocessing = PythonScriptStep(\n    name=&quot;Preprocessing&quot;,\n    script_name=e.preprocess_script_path,\n    arguments=[\n        &quot;--config_path&quot;, e.preprocess_config_path,\n        &quot;--task&quot;, e.preprocess_task,\n    ],\n    inputs=None,\n    compute_target=aml_compute,\n    runconfig=run_config,\n    source_directory=e.sources_directory,\n    allow_reuse=False\n)\nprint(&quot;Step Preprocessing created&quot;)\n<\/code><\/pre>\n<p>This results in error:<\/p>\n<pre><code>Traceback (most recent call last):\n[stderr]  File &quot;Pipeline\/custom_pipeline.py&quot;, line 4, in &lt;module&gt;\n[stderr]    from Preprocess.logger import logger\n[stderr]ModuleNotFoundError: No module named 'Preprocess'\n<\/code><\/pre>\n<p>in the 1st line of entry script (<strong>custom_pipeline.py<\/strong>):<\/p>\n<pre><code>import sys\nsys.path.append(&quot;.&quot;) \nfrom Preprocess.logger import logger\n<\/code><\/pre>\n<p>The folder structure is as:<\/p>\n<pre><code>-Preprocess\n  -__init__.py\n  - Module1\n    -__init__.py\n    -somefile.py\n  - Module2\n    -__init__.py\n    -someOtherfile.py\n  - Pipeline\n    -__init__.py\n    -custom_pipeline.py\n  - logger\n    -__init__.py\n    -logger.py\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1644260100200,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":109.0,
        "Poster_id":5368122.0,
        "Poster_created_time":1443017464707,
        "Poster_last_access_time":1663923275743,
        "Poster_age":null,
        "Poster_location":"Sweden",
        "Poster_reputation":644.0,
        "Poster_views":126.0,
        "Answer_body":"<p>I found out that the python script step copies everything inside the source_dir and therefore in my case it was copying the modules and not the root folder. So I had to put the dir Preprocess inside another dir and mention the new dir as source_dir.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":5368122.0,
        "Answerer_created_time":1443017464707,
        "Answerer_last_access_time":1663923275743,
        "Answerer_age":null,
        "Answerer_location":"Sweden",
        "Answerer_reputation":644.0,
        "Answerer_views":126.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71023918",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1644604906183,
        "Question_original_content":"modulenotfound error prebuilt docker imag develop modul work perfectli execut local creat environ azur prebuilt docker imag minim ubuntu cpu infer http mcr microsoft com catalog pythonscriptstep run pipelin step look steppreprocess pythonscriptstep preprocess script preprocess script path argument config path preprocess config path task preprocess task input comput target aml comput runconfig run config sourc directori sourc directori allow reus fals print step preprocess creat result error traceback recent stderr file pipelin custom pipelin line stderr preprocess logger import logger stderr modulenotfounderror modul name preprocess line entri script custom pipelin import sy sy path append preprocess logger import logger folder structur preprocess init modul init somefil modul init someotherfil pipelin init custom pipelin logger init logger",
        "Question_preprocessed_content":"modulenotfound error prebuilt docker imag develop modul work perfectli execut local creat environ azur prebuilt docker imag pythonscriptstep run pipelin step look result error line entri script folder structur",
        "Question_gpt_summary_original":"The user encountered a ModuleNotFound error when running a pipeline on Azure ML using a prebuilt docker image. The error occurred in the entry script, custom_pipeline.py, when importing a module named 'Preprocess'. The folder structure includes the 'Preprocess' folder containing subfolders and files, including the 'logger' module.",
        "Question_gpt_summary":"user encount modulenotfound error run pipelin prebuilt docker imag error occur entri script custom pipelin import modul name preprocess folder structur includ preprocess folder contain subfold file includ logger modul",
        "Answer_original_content":"python script step copi insid sourc dir case copi modul root folder dir preprocess insid dir mention new dir sourc dir",
        "Answer_preprocessed_content":"python script step copi insid case copi modul root folder dir preprocess insid dir mention new dir",
        "Answer_gpt_summary_original":"Solution: The user resolved the ModuleNotFound error by putting the 'Preprocess' folder inside another directory and specifying the new directory as the source directory in the python script step.",
        "Answer_gpt_summary":"solut user resolv modulenotfound error put preprocess folder insid directori specifi new directori sourc directori python script step"
    },
    {
        "Question_title":"Change data type using Azure Machine Learning Studio",
        "Question_body":"<p>Allow me to ask again this question, as answers found on the forum did not help me so far.<\/p>\n\n<p>I am trying to convert a column from 'string' into 'numerical' data type. <\/p>\n\n<p>The column has no missing values and no errors, it comes from a CSV file. For the record, I tried modifing the format type of the column on the CSV file and saving it as a number, but later when importing the CSV file on Azure ML it was coded as string.<\/p>\n\n<p>So far, I have tried the following options:<\/p>\n\n<ul>\n<li><p>'<strong>Execute Python scrip<\/strong>t'. Unfortunately it does not work . It returns an error when I run the experiment. The Code I entered is:<\/p>\n\n<pre><code>import pandas as df\n\ndef azureml_main (df):\n  df.age=pd.to_numeric(df.age,errors=\u2019coerce\u2019)\n\nreturn df\n<\/code><\/pre><\/li>\n<li><p>Use '<strong>Edit Metadata<\/strong>' module. Select as Datatype: 'Integer' or 'Floating point' but I keep on getting an error when running the experiment.<\/p><\/li>\n<\/ul>\n\n<p>Please kindly let me know what your thoughts are.<\/p>\n\n<p>Thanks for your help.<\/p>\n\n<p>Josep Maria<\/p>\n\n<p>P.S: It's the second time I write in this forum. I hope this time it is well formulated.\n<a href=\"https:\/\/i.stack.imgur.com\/glLFo.jpg\" rel=\"nofollow noreferrer\">screenshot of 'Execute Python Script' error<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":7,
        "Question_created_time":1533283734597,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1533293784207,
        "Question_score":2,
        "Question_view_count":636.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>It looks like the Python script just needs a little updating. :)<\/p>\n\n<p>This should work since you get <code>dataframe1<\/code> automatically as a <code>pandas<\/code> data frame.<\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n  dataframe1.age = pd.to_numeric(dataframe1.age, errors=\"coerce\")\n\n  return dataframe1\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1533488730467,
        "Answer_score":3.0,
        "Answerer_id":186013.0,
        "Answerer_created_time":1254957460063,
        "Answerer_last_access_time":1659941040212,
        "Answerer_age":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation":2484.0,
        "Answerer_views":362.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51668053",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1533375326832,
        "Question_original_content":"chang data type studio allow ask question answer forum help far try convert column string numer data type column miss valu error come csv file record tri modif format type column csv file save number later import csv file code string far tri follow option execut python script unfortun work return error run experi code enter import panda def main ag numer ag error coerc return us edit metadata modul select datatyp integ float point get error run experi kindli let know thought thank help josep maria second time write forum hope time formul screenshot execut python script error",
        "Question_preprocessed_content":"chang data type studio allow ask question answer forum help far try convert column string numer data type column miss valu error come csv file record tri modif format type column csv file save number later import csv file code string far tri follow option execut python script unfortun work return error run experi code enter us edit metadata modul select datatyp integ float point get error run experi kindli let know thought thank help josep maria second time write forum hope time formul screenshot execut python script error",
        "Question_gpt_summary_original":"The user is facing challenges in converting a column from 'string' to 'numerical' data type in Azure Machine Learning Studio. The user has tried using the 'Execute Python Script' and 'Edit Metadata' modules, but both have resulted in errors when running the experiment. The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user face challeng convert column string numer data type studio user tri execut python script edit metadata modul result error run experi user seek assist resolv issu",
        "Answer_original_content":"look like python script need littl updat work datafram automat panda data frame import panda def main datafram datafram datafram ag numer datafram ag error coerc return datafram",
        "Answer_preprocessed_content":"look like python script need littl updat work automat data frame",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to update the Python script by using the 'pandas' library to convert the column from 'string' to 'numerical' data type. The code snippet provided in the discussion uses the 'to_numeric' function from the 'pandas' library to convert the 'age' column to numerical data type.",
        "Answer_gpt_summary":"solut possibl solut mention discuss updat python script panda librari convert column string numer data type code snippet provid discuss us numer function panda librari convert ag column numer data type"
    },
    {
        "Question_title":"Upgrading Python version for running and creating custom container for Sagemaker Endpoint",
        "Question_body":"<p>[UPDATED] We are currently working on creating a Multi-Arm Bandit model for sign up optimization using the Build Your Own workflow that can be found here (basically substituting the model for our own):<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own<\/a><\/p>\n<p>Our project directory is set up as:\n<a href=\"https:\/\/i.stack.imgur.com\/QwaIQ.png\" rel=\"nofollow noreferrer\">Project Directory<\/a><\/p>\n<p>The issue is that I added some code including the dataclasses library that is only available since Python 3.7, and our project seems to keep using 3.6, causing a failure when running the Cloud Formation set up. The error in our Cloudwatch Logs is:<\/p>\n<pre><code>2021-03-31T11:04:11.077-05:00 Copy\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt;\n    import predictor as myapp\n  File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt;\n    from model_contents.model import MultiArmBandit, BanditParameters\n  File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt;\n    from dataclasses import dataclass, field, asdict\nTraceback (most recent call last): File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker worker.init_process() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process self.load_wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi self.wsgi = self.app.wsgi() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi self.callable = self.load() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load return self.load_wsgiapp() File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp return util.import_app(self.app_uri) File &quot;\/usr\/local\/lib\/python3.6\/dist-packages\/gunicorn\/util.py&quot;, line 359, in import_app mod = importlib.import_module(module) File &quot;\/usr\/lib\/python3.6\/importlib\/__init__.py&quot;, line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 994, in _gcd_import File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 971, in _find_and_load File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 955, in _find_and_load_unlocked File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 665, in _load_unlocked File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 678, in exec_module File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed File &quot;\/opt\/program\/wsgi.py&quot;, line 1, in &lt;module&gt; import predictor as myapp File &quot;\/opt\/program\/predictor.py&quot;, line 9, in &lt;module&gt; from model_contents.model import MultiArmBandit, BanditParameters File &quot;\/opt\/program\/model_contents\/model.py&quot;, line 7, in &lt;module&gt; from dataclasses import dataclass, field, asdict\n\n    2021-03-31T11:04:11.077-05:00\n\nCopy\nModuleNotFoundError: No module named 'dataclasses'\nModuleNotFoundError: No module named 'dataclasses'\n<\/code><\/pre>\n<p>Our updated Dockerfile is:<\/p>\n<pre><code># This is a Python 3 image that uses the nginx, gunicorn, flask stack\n# for serving inferences in a stable way.\n\nFROM ubuntu:18.04\n\n# Retrieves information about what packages can be installed\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python3-pip \\\n         python3.8 \\\n         python3-setuptools \\\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\n# Set python 3.8 as default\nRUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n\n# Here we get all python packages.\nRUN pip --no-cache-dir install numpy boto3 flask gunicorn\n\n# Set some environment variables. PYTHONUNBUFFERED keeps Python from buffering our standard\n# model_output stream, which means that logs can be delivered to the user quickly. PYTHONDONTWRITEBYTECODE\n# keeps Python from writing the .pyc files which are unnecessary in this case. We also update\n# PATH so that the train and serve programs are found when the container is invoked.\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\nENV PYTHONPATH \/model_contents\n\n# Set up the program in the image\nCOPY bandit\/ \/opt\/program\/\nWORKDIR \/opt\/program\/\n\nRUN chmod +x \/opt\/program\/serve &amp;&amp; chmod +x \/opt\/program\/train\nLABEL git_tag=$GIT_TAG\n<\/code><\/pre>\n<p>I'm not sure if the nginx.conf file defaults to Py 3.6 so I want to make sure that it's not a big hassle to upgrade to Py 3.7 or 3.8 without many changes.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1617309858560,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1617383727407,
        "Question_score":1,
        "Question_view_count":1398.0,
        "Poster_id":9787221.0,
        "Poster_created_time":1526288719836,
        "Poster_last_access_time":1656688730603,
        "Poster_age":null,
        "Poster_location":"Madrid, Spain",
        "Poster_reputation":33.0,
        "Poster_views":11.0,
        "Answer_body":"<p>You can update the Dockerfile after it install Python3.8 using <code>apt-get<\/code> with the following <code>RUN<\/code> commands<\/p>\n<pre><code>RUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.8 1\nRUN update-alternatives --install \/usr\/bin\/python3 python3 \/usr\/bin\/python3.8 1\n<\/code><\/pre>\n<p>The first <code>RUN<\/code> command will link <code>\/usr\/bin\/python<\/code> to <code>\/usr\/bin\/python3.8<\/code> and the second one will link <code>\/usr\/bin\/python3<\/code> to <code>\/usr\/bin\/python3.8<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1617312029390,
        "Answer_score":1.0,
        "Answerer_id":7375347.0,
        "Answerer_created_time":1483548930012,
        "Answerer_last_access_time":1663627767283,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1875.0,
        "Answerer_views":146.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66911321",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1617311610732,
        "Question_original_content":"upgrad python version run creat custom contain endpoint updat current work creat multi arm bandit model sign optim build workflow basic substitut model http github com aw amazon exampl tree master advanc function scikit bring project directori set project directori issu ad code includ dataclass librari avail python project caus failur run cloud format set error cloudwatch log copi traceback recent file usr local lib python dist packag gunicorn arbit line spawn worker worker init process file usr local lib python dist packag gunicorn worker base line init process self load wsgi file usr local lib python dist packag gunicorn worker base line load wsgi self wsgi self app wsgi file usr local lib python dist packag gunicorn app base line wsgi self callabl self load file usr local lib python dist packag gunicorn app wsgiapp line load return self load wsgiapp file usr local lib python dist packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file usr local lib python dist packag gunicorn util line import app mod importlib import modul modul file usr lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file opt program wsgi line import predictor myapp file opt program predictor line model content model import multiarmbandit banditparamet file opt program model content model line dataclass import dataclass field asdict traceback recent file usr local lib python dist packag gunicorn arbit line spawn worker worker init process file usr local lib python dist packag gunicorn worker base line init process self load wsgi file usr local lib python dist packag gunicorn worker base line load wsgi self wsgi self app wsgi file usr local lib python dist packag gunicorn app base line wsgi self callabl self load file usr local lib python dist packag gunicorn app wsgiapp line load return self load wsgiapp file usr local lib python dist packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file usr local lib python dist packag gunicorn util line import app mod importlib import modul modul file usr lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file opt program wsgi line import predictor myapp file opt program predictor line model content model import multiarmbandit banditparamet file opt program model content model line dataclass import dataclass field asdict copi modulenotfounderror modul name dataclass modulenotfounderror modul name dataclass updat dockerfil python imag us nginx gunicorn flask stack serv infer stabl wai ubuntu retriev inform packag instal run apt updat apt instal instal recommend wget python pip python python setuptool nginx certif var lib apt list set python default run updat altern instal usr bin python python usr bin python run updat altern instal usr bin python python usr bin python python packag run pip cach dir instal numpi boto flask gunicorn set environ variabl pythonunbuff keep python buffer standard model output stream mean log deliv user quickli pythondontwritebytecod keep python write pyc file unnecessari case updat path train serv program contain invok env pythonunbuff true env pythondontwritebytecod true env path opt program path env pythonpath model content set program imag copi bandit opt program workdir opt program run chmod opt program serv chmod opt program train label git tag git tag sure nginx conf file default want sure big hassl upgrad chang",
        "Question_preprocessed_content":"upgrad python version run creat custom contain endpoint current work creat bandit model sign optim build workflow project directori set project directori issu ad code includ dataclass librari avail python project caus failur run cloud format set error cloudwatch log updat dockerfil sure file default want sure big hassl upgrad chang",
        "Question_gpt_summary_original":"The user is facing a challenge in upgrading the Python version for running and creating a custom container for Sagemaker Endpoint. The user added some code that requires the dataclasses library, which is only available since Python 3.7, but the project is still using Python 3.6, causing a failure when running the Cloud Formation set up. The user has updated the Dockerfile to include Python 3.8 and wants to ensure that upgrading to Python 3.7 or 3.8 will not require many changes.",
        "Question_gpt_summary":"user face challeng upgrad python version run creat custom contain endpoint user ad code requir dataclass librari avail python project python caus failur run cloud format set user updat dockerfil includ python want ensur upgrad python requir chang",
        "Answer_original_content":"updat dockerfil instal python apt follow run command run updat altern instal usr bin python python usr bin python run updat altern instal usr bin python python usr bin python run command link usr bin python usr bin python second link usr bin python usr bin python",
        "Answer_preprocessed_content":"updat dockerfil instal follow command command link second link",
        "Answer_gpt_summary_original":"Solution: The user can update the Dockerfile to install Python 3.8 using \"apt-get\" and then use the \"RUN\" commands to link \"\/usr\/bin\/python\" and \"\/usr\/bin\/python3\" to \"\/usr\/bin\/python3.8\". This will ensure that upgrading to Python 3.7 or 3.8 will not require many changes.",
        "Answer_gpt_summary":"solut user updat dockerfil instal python apt us run command link usr bin python usr bin python usr bin python ensur upgrad python requir chang"
    },
    {
        "Question_title":"Is it possible to check that the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file?",
        "Question_body":"<p>My team has a set up wherein we track datasets and models in DVC, and have a GitLab repository for tracking our code and DVC metadata files. We have a job in our dev GitLab pipeline (run on each push to a merge request) that has the goal of checking to be sure that the developer remembered to run <code>dvc push<\/code> to keep DVC remote storage up-to-date. Right now, the way we do this is by running <code>dvc pull<\/code> on the GitLab runner, which will fail with errors telling you which files (new files or latest versions of existing files) were not found.<\/p>\n<p>The downside to this approach is that we are loading the entirety of our data stored in DVC onto a GitLab runner, and we've run into out-of-memory issues, not to mention lengthy run time to download all that data. Since the path and md5 hash of the objects are stored in the DVC metadata files, I would think that's all the information that DVC would need to be able to answer the question &quot;is the remote storage system up-to-date&quot;.<\/p>\n<p>It seems like <code>dvc status<\/code> is similar to what I'm asking for, but compares the cache or workspace and remote storage. In other words, it requires the files to actually be present on whatever filesystem is making the call.<\/p>\n<p>Is there some way to achieve the goal I laid out above (&quot;inform the developer that they need to run <code>dvc push<\/code>&quot;) without pulling everything from DVC?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1622232629793,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1622257491983,
        "Question_score":5,
        "Question_view_count":488.0,
        "Poster_id":15614436.0,
        "Poster_created_time":1618255062696,
        "Poster_last_access_time":1645555346683,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":75.0,
        "Poster_views":2.0,
        "Answer_body":"<blockquote>\n<p>It seems like dvc status is similar to what I'm asking for<\/p>\n<\/blockquote>\n<p><code>dvc status --cloud<\/code> will give you a list of &quot;new&quot; files if they that haven't been pushed to the (default) remote. It won't error out though, so your CI script should fail depending on the stdout message.<\/p>\n<p>More info: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/status#options\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/status#options<\/a><\/p>\n<p>I'd also ask everyone to run <code>dvc install<\/code>, which will setup some Git hooks, including automatic <code>dvc push<\/code> with <code>git push<\/code>.<\/p>\n<p>See <a href=\"https:\/\/dvc.org\/doc\/command-reference\/install\" rel=\"nofollow noreferrer\">https:\/\/dvc.org\/doc\/command-reference\/install<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1622503453296,
        "Answer_score":3.0,
        "Answerer_id":761963.0,
        "Answerer_created_time":1305851487736,
        "Answerer_last_access_time":1663864920647,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":5993.0,
        "Answerer_views":457.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67744934",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_closed_time":1622257759208,
        "Question_original_content":"possibl check version file track metadata file exist remot storag pull file team set track dataset model gitlab repositori track code metadata file job dev gitlab pipelin run push merg request goal check sure develop rememb run push remot storag date right wai run pull gitlab runner fail error tell file new file latest version exist file downsid approach load entireti data store gitlab runner run memori issu mention lengthi run time download data path hash object store metadata file think inform need abl answer question remot storag date like statu similar ask compar cach workspac remot storag word requir file actual present filesystem make wai achiev goal laid inform develop need run push pull",
        "Question_preprocessed_content":"possibl check version file track metadata file exist remot storag pull file team set track dataset model gitlab repositori track code metadata file job dev gitlab pipelin goal check sure develop rememb run remot storag right wai run gitlab runner fail error tell file downsid approach load entireti data store gitlab runner run issu mention lengthi run time download data path hash object store metadata file think inform need abl answer question remot storag like similar ask compar cach workspac remot storag word requir file actual present filesystem make wai achiev goal laid pull",
        "Question_gpt_summary_original":"The user's team tracks datasets and models in DVC and has a GitLab repository for tracking code and DVC metadata files. They have a job in their dev GitLab pipeline that checks if the developer remembered to run \"dvc push\" to keep DVC remote storage up-to-date. Currently, they run \"dvc pull\" on the GitLab runner, which loads the entirety of their data stored in DVC onto the runner, causing out-of-memory issues and lengthy run time. The user is looking for a way to check if the version of a file tracked by a DVC metadata file exists in remote storage without pulling the file.",
        "Question_gpt_summary":"user team track dataset model gitlab repositori track code metadata file job dev gitlab pipelin check develop rememb run push remot storag date current run pull gitlab runner load entireti data store runner caus memori issu lengthi run time user look wai check version file track metadata file exist remot storag pull file",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"Sweep from existing runs not showing up in parallel coordinates, is this intended or a bug?",
        "Question_body":"<p>Hi, I created a sweep from existing runs, but the panel Parallel Coordinates are empty, is this an intended behaviour or a bug?<\/p>\n<p>Here is what I did:<\/p>\n<ul>\n<li>populate projects with many runs (using ray\u2019s wandb_mixin)<\/li>\n<li>create a sweep following <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs\">https:\/\/docs.wandb.ai\/guides\/sweeps\/existing-project#seed-a-new-sweep-with-existing-runs<\/a>\n<\/li>\n<li>the panel at \u201cSweeps &gt; [2]\u201d contains only 1 run, should contains all 42 runs.<\/li>\n<\/ul>\n<p>The sweep is at <a href=\"https:\/\/wandb.ai\/inc\/try_ray_tune\/sweeps\/smh3d0wg\" class=\"inline-onebox\">Weights &amp; Biases<\/a>, if any one is interested.<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_created_time":1640245639181,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":201.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/inc\">@inc<\/a>,<\/p>\n<p>You should be able to see all 42 runs on your parallel coordinates plot by ungrouping the runs. Grouping runs groups them for charts on your workspace as well.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/sweep-from-existing-runs-not-showing-up-in-parallel-coordinates-is-this-intended-or-a-bug\/1601",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1640309961152,
        "Question_original_content":"sweep exist run show parallel coordin intend bug creat sweep exist run panel parallel coordin intend behaviour bug popul project run rai mixin creat sweep follow http doc guid sweep exist project seed new sweep exist run panel sweep contain run contain run sweep interest",
        "Question_preprocessed_content":"sweep exist run show parallel coordin intend bug creat sweep exist run panel parallel coordin intend behaviour bug popul project run creat sweep follow panel sweep contain run contain run sweep interest",
        "Question_gpt_summary_original":"The user created a sweep from existing runs using ray's wandb_mixin, but the panel Parallel Coordinates is empty. The user wonders if this is intended or a bug. The panel at \"Sweeps > [2]\" contains only 1 run instead of all 42 runs.",
        "Question_gpt_summary":"user creat sweep exist run rai mixin panel parallel coordin user wonder intend bug panel sweep contain run instead run",
        "Answer_original_content":"abl run parallel coordin plot ungroup run group run group chart workspac thank ramit",
        "Answer_preprocessed_content":"abl run parallel coordin plot ungroup run group run group chart workspac thank ramit",
        "Answer_gpt_summary_original":"Solution: The user can ungroup the runs to see all 42 runs on the parallel coordinates plot. Grouping runs groups them for charts on the workspace as well.",
        "Answer_gpt_summary":"solut user ungroup run run parallel coordin plot group run group chart workspac"
    },
    {
        "Question_title":"SageMaker with multiple models",
        "Question_body":"Customer wants to host multiple DNN models on same SageMaker container due to latency concerns. Customer does not want to spin-up different containers for each model due to network adding additional latency. Thus, my customer asked me a question below -\n\nCan one SageMaker host more than one model? Each model then share the same input and produce different outputs concatenated together?\n\nI answered as below -\n\nYes. Amazon SageMaker supports you hosting multiple models in several different ways \u2013\n\nUsing Multi-model Inference endpoints: Amazon SageMaker supports serving multiple models from same Inference endpoint. Details can be found here. The sample code can be found here. Currently, this feature do not support Elastic Inference or serial inference pipelines. Multi-model endpoints also enable time-sharing of memory resources across your models. This works best when the models are fairly similar in size and invocation latency. When this is the case, multi-model endpoints can effectively use instances across all models. If you have models that have significantly higher transactions per second (TPS) or latency requirements, we recommend hosting them on dedicated endpoints. Multi-model endpoints are also well suited to scenarios that can tolerate occasional cold-start-related latency penalties that occur when invoking infrequently used models\n\nUsing Bring your own algorithm on SageMaker You can also bring your own container with your own libs and runtime\/programming language for serving and training. See the example notebook on how you can bring your own algorithm\/container image on sagemaker here\n\nUsing Multi-model serving container by using multi-model archive file You can find a sample example here [4] for tensorflow serving\n\nIf models are called sequentially, the SageMaker inference pipeline allows you to chain up to 5 models called one after the other on the same endpoint Sagemaker endpoints include optimizations that will save costs, such as (1) 1-click deploy to pre-configured environments for popular ML frameworks with a managed serving stack, (2) autoscaling, (3) model compilation, (4) cost-effective hardware acceleration via Elastic Inference, (5) multi-variant model deployment for testing and overlapped model replacement, (6) multi-AZ backend. It is not necessarily a good idea to have multiple models on same endpoint (unless you have the reasons and requirements I mentioned in Option A above). Having one model per endpoint creates an isolation which has positive benefits on fault tolerance, security and scalability. Please keep in mind that SageMaker works on containers that runs on top of EC2.\n\n[1]https:\/\/aws.amazon.com\/blogs\/machine-learning\/save-on-inference-costs-by-using-amazon-sagemaker-multi-model-endpoints\/\n\n[2]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\n\n[3]https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\n\n[4]https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/deploying_tensorflow_serving.rst#deploying-more-than-one-model-to-your-endpoint\n\n[5]https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\n\nAm I missing anything? Any other suggestions in terms of other approaches?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1587366119000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":254.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Customer does not want to spin-up different containers for each model due to network adding additional latency.\n\nI am assuming this is a pipeline scenario where different models need to be chained. If so, it's important to keep in mind that all containers in pipeline run on the same EC2 instance so that \"inferences run with low latency because the containers are co-located on the same EC2 instances.\"[1]\n\nHope this is useful.\n[1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfmnWJIIZQs6_2K1uIH9stQ\/sage-maker-with-multiple-models",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1593677528000,
        "Question_original_content":"multipl model custom want host multipl dnn model contain latenc concern custom want spin differ contain model network ad addit latenc custom ask question host model model share input produc differ output concaten answer ye support host multipl model differ wai multi model infer endpoint support serv multipl model infer endpoint detail sampl code current featur support elast infer serial infer pipelin multi model endpoint enabl time share memori resourc model work best model fairli similar size invoc latenc case multi model endpoint effect us instanc model model significantli higher transact second tp latenc requir recommend host dedic endpoint multi model endpoint suit scenario toler occasion cold start relat latenc penalti occur invok infrequ model bring algorithm bring contain lib runtim program languag serv train exampl notebook bring algorithm contain imag multi model serv contain multi model archiv file sampl exampl tensorflow serv model call sequenti infer pipelin allow chain model call endpoint endpoint includ optim save cost click deploi pre configur environ popular framework manag serv stack autosc model compil cost effect hardwar acceler elast infer multi variant model deploy test overlap model replac multi backend necessarili good idea multipl model endpoint reason requir mention option have model endpoint creat isol posit benefit fault toler secur scalabl mind work contain run http aw amazon com blog machin learn save infer cost amazon multi model endpoint http github com awslab amazon exampl blob master advanc function multi model bring multi model endpoint bring ipynb http github com awslab amazon exampl blob master advanc function scikit bring scikit bring ipynb http github com aw python sdk blob master src tensorflow deploi tensorflow serv rst deploi model endpoint http doc aw amazon com latest infer pipelin html miss suggest term approach",
        "Question_preprocessed_content":"multipl model custom want host multipl dnn model contain latenc concern custom want differ contain model network ad addit latenc custom ask question host model model share input produc differ output concaten answer ye support host multipl model differ wai infer endpoint support serv multipl model infer endpoint detail sampl code current featur support elast infer serial infer pipelin endpoint enabl memori resourc model work best model fairli similar size invoc latenc case endpoint effect us instanc model model significantli higher transact second latenc requir recommend host dedic endpoint endpoint suit scenario toler occasion latenc penalti occur invok infrequ model bring algorithm bring contain lib languag serv train exampl notebook bring imag serv contain archiv file sampl exampl tensorflow serv model call sequenti infer pipelin allow chain model call endpoint endpoint includ optim save cost deploi environ popular framework manag serv stack autosc model compil hardwar acceler elast infer model deploy test overlap model replac backend necessarili good idea multipl model endpoint have model endpoint creat isol posit benefit fault toler secur scalabl mind work contain run miss suggest term approach",
        "Question_gpt_summary_original":"The user wants to host multiple DNN models on the same SageMaker container to avoid additional network latency. They asked if it is possible for one SageMaker to host more than one model, with each model sharing the same input and producing different outputs concatenated together. The response suggests that Amazon SageMaker supports hosting multiple models using multi-model inference endpoints, bring your own algorithm on SageMaker, multi-model serving container by using multi-model archive file, and SageMaker inference pipeline. However, it is not necessarily a good idea to have multiple models on the same endpoint as it may affect fault tolerance, security, and scalability.",
        "Question_gpt_summary":"user want host multipl dnn model contain avoid addit network latenc ask possibl host model model share input produc differ output concaten respons suggest support host multipl model multi model infer endpoint bring algorithm multi model serv contain multi model archiv file infer pipelin necessarili good idea multipl model endpoint affect fault toler secur scalabl",
        "Answer_original_content":"custom want spin differ contain model network ad addit latenc assum pipelin scenario differ model need chain import mind contain pipelin run instanc infer run low latenc contain locat instanc hope us http doc aw amazon com latest infer pipelin html",
        "Answer_preprocessed_content":"custom want differ contain model network ad addit latenc assum pipelin scenario differ model need chain import mind contain pipelin run instanc infer run low latenc contain hope us",
        "Answer_gpt_summary_original":"The response suggests that Amazon SageMaker supports hosting multiple models using multi-model inference endpoints, bring your own algorithm on SageMaker, multi-model serving container by using multi-model archive file, and SageMaker inference pipeline. However, it is not necessarily a good idea to have multiple models on the same endpoint as it may affect fault tolerance, security, and scalability. The discussion mentions that the customer does not want to spin-up different containers for each model due to network adding additional latency. The response suggests that if this is a pipeline scenario where different models need to be chained, it's important to keep in mind that all containers in the pipeline run on the same EC2 instance so that \"inferences run with low latency because the containers are co-located on",
        "Answer_gpt_summary":"respons suggest support host multipl model multi model infer endpoint bring algorithm multi model serv contain multi model archiv file infer pipelin necessarili good idea multipl model endpoint affect fault toler secur scalabl discuss mention custom want spin differ contain model network ad addit latenc respons suggest pipelin scenario differ model need chain import mind contain pipelin run instanc infer run low latenc contain locat"
    },
    {
        "Question_title":"Mount a datalake storage in azure ML studio",
        "Question_body":"<p>I created a file dataset from a data lake folder on Azure ML Studio,  at the moment I\u00b4m able to download the data from the dataset to the compute instance with this code:<\/p>\n<pre><code>subscription_id = 'xxx'\nresource_group = 'luisdatapipelinetest'\nworkspace_name = 'ml-pipelines'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='files_test')\npath = &quot;\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/demo1231\/code\/Users\/luis.rramirez\/test\/&quot;\ndataset.download(target_path=path, overwrite=True)\n<\/code><\/pre>\n<p>With that I'm able to access the files from the notebook.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But copying the data from the data lake to the compute instance is not efficient, how can I mount the data lake directory in the vm instead of copying the data each time?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1630008530263,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1630044624416,
        "Question_score":1,
        "Question_view_count":258.0,
        "Poster_id":4544413.0,
        "Poster_created_time":1423439611840,
        "Poster_last_access_time":1663978112523,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":8349.0,
        "Poster_views":949.0,
        "Answer_body":"<p>MOUNTING ADLS2 to AML so you can save files into your mountPoint directly. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"nofollow noreferrer\">Here<\/a> is the example of registering the storage and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.file_dataset.filedataset?view=azure-ml-py#mount-mount-point-none----kwargs-\" rel=\"nofollow noreferrer\">here<\/a> shows how to mount your registered datastore.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":11297406.0,
        "Answerer_created_time":1554186784008,
        "Answerer_last_access_time":1662988357176,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":2175.0,
        "Answerer_views":434.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68944750",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1630298993132,
        "Question_original_content":"mount datalak storag studio creat file dataset data lake folder studio moment abl download data dataset comput instanc code subscript resourc group luisdatapipelinetest workspac pipelin workspac workspac subscript resourc group workspac dataset dataset workspac file test path mnt batch task share root mount cluster demo code user lui rramirez test dataset download target path path overwrit true abl access file notebook copi data data lake comput instanc effici mount data lake directori instead copi data time",
        "Question_preprocessed_content":"mount datalak storag studio creat file dataset data lake folder studio moment abl download data dataset comput instanc code abl access file notebook copi data data lake comput instanc effici mount data lake directori instead copi data time",
        "Question_gpt_summary_original":"The user has created a file dataset from a data lake folder on Azure ML Studio and is able to download the data from the dataset to the compute instance. However, the user is facing the challenge of copying the data from the data lake to the compute instance each time, which is not efficient. The user is seeking a solution to mount the data lake directory in the VM instead of copying the data each time.",
        "Question_gpt_summary":"user creat file dataset data lake folder studio abl download data dataset comput instanc user face challeng copi data data lake comput instanc time effici user seek solut mount data lake directori instead copi data time",
        "Answer_original_content":"mount adl aml save file mountpoint directli exampl regist storag show mount regist datastor",
        "Answer_preprocessed_content":"mount adl aml save file mountpoint directli exampl regist storag show mount regist datastor",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to mount the data lake directory in the VM instead of copying the data each time. The user can register the storage and mount the registered datastore using the Azure ML Studio. The example of registering the storage and how to mount the registered datastore is provided in the discussion.",
        "Answer_gpt_summary":"solut solut suggest discuss mount data lake directori instead copi data time user regist storag mount regist datastor studio exampl regist storag mount regist datastor provid discuss"
    },
    {
        "Question_title":"Azure ML endpoint 404 error",
        "Question_body":"<p>An Azure Data Factory pipeline for updating a trained ML model returns this error:<\/p>\n\n<pre><code>HTTP 404. The resource you are looking for (or one of its dependencies) could have been removed, had its name changed, or is temporarily unavailable. Please review the following URL and make sure that it is spelled correctly.\nRequested URL: \/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update\n\nDiagnostic details: job ID xxxx. Endpoint https:\/\/services.azureml.net\/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update.\n<\/code><\/pre>\n\n<p>I don't even want to think about why it returned a HTML document...\nI am 100% sure that the endpoint exists and the key provided is correct.\nSo what is my mistake?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1503048195263,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1503233060163,
        "Question_score":0,
        "Question_view_count":494.0,
        "Poster_id":7037695.0,
        "Poster_created_time":1476806455803,
        "Poster_last_access_time":1661945674687,
        "Poster_age":null,
        "Poster_location":"Holzkirchen, Deutschland",
        "Poster_reputation":3068.0,
        "Poster_views":386.0,
        "Answer_body":"<p>Deleting and creating the endpoint again fixed it.<\/p>\n\n<p>Microsoft...<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":7037695.0,
        "Answerer_created_time":1476806455803,
        "Answerer_last_access_time":1661945674687,
        "Answerer_age":null,
        "Answerer_location":"Holzkirchen, Deutschland",
        "Answerer_reputation":3068.0,
        "Answerer_views":386.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45753090",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1508942544572,
        "Question_original_content":"endpoint error azur data factori pipelin updat train model return error http resourc look depend remov chang temporarili unavail review follow url sure spell correctli request url workspac webservic endpoint updat diagnost detail job endpoint http servic net workspac webservic endpoint updat want think return html document sure endpoint exist kei provid correct mistak",
        "Question_preprocessed_content":"endpoint error azur data factori pipelin updat train model return error want think return html sure endpoint exist kei provid correct mistak",
        "Question_gpt_summary_original":"The user encountered a 404 error when trying to update a trained ML model using an Azure Data Factory pipeline. The error message suggests that the resource may have been removed, had its name changed, or is temporarily unavailable. The user is certain that the endpoint exists and the key provided is correct, but is unsure of what mistake they may have made.",
        "Question_gpt_summary":"user encount error try updat train model azur data factori pipelin error messag suggest resourc remov chang temporarili unavail user certain endpoint exist kei provid correct unsur mistak",
        "Answer_original_content":"delet creat endpoint fix microsoft",
        "Answer_preprocessed_content":"delet creat endpoint fix",
        "Answer_gpt_summary_original":"Solution: The user resolved the issue by deleting and creating the endpoint again. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user resolv issu delet creat endpoint solut mention discuss"
    },
    {
        "Question_title":"Slow ResNet50 training time using AWS Sagemaker GPU instance",
        "Question_body":"<p>I am trying to train a ResNet50 model using keras with tensorflow backend. I'm using a sagemaker GPU instance <strong>ml.p3.2xlarge<\/strong> but my training time is extremely long. I am using conda_tensorflow_p36 kernel and I have verified that I have tensorflow-gpu installed.<\/p>\n<p>When inspecting the output of nvidia-smi I see the process is on the GPU, but the utilization is never above <strong>0%<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CDSkC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CDSkC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Tensorflow also recognizes the GPU.\n<a href=\"https:\/\/i.stack.imgur.com\/wRHBC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wRHBC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Screenshot of training time.\n<a href=\"https:\/\/i.stack.imgur.com\/Yh73K.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Yh73K.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Is sagemaker in fact using the GPU even though the usage is <strong>0%?<\/strong>\nCould the long epoch training time be caused by another issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1649867231580,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1649898908327,
        "Question_score":0,
        "Question_view_count":120.0,
        "Poster_id":10572702.0,
        "Poster_created_time":1540782143880,
        "Poster_last_access_time":1663637054396,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":31.0,
        "Poster_views":4.0,
        "Answer_body":"<p>Looks like you've completed 8 steps and it just takes very long. What's your step time?<br \/>\nIt might be due to data loading. Where ia data stored? Try to take data loading out of the picture by caching and feeding a single image to the DNN repeatedly and see if that helps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":121956.0,
        "Answerer_created_time":1244808478036,
        "Answerer_last_access_time":1663981477123,
        "Answerer_age":null,
        "Answerer_location":"Israel",
        "Answerer_reputation":4932.0,
        "Answerer_views":405.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71860839",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1650008050343,
        "Question_original_content":"slow resnet train time gpu instanc try train resnet model kera tensorflow backend gpu instanc xlarg train time extrem long conda tensorflow kernel verifi tensorflow gpu instal inspect output nvidia smi process gpu util tensorflow recogn gpu screenshot train time fact gpu usag long epoch train time caus issu",
        "Question_preprocessed_content":"slow resnet train time gpu instanc try train resnet model kera tensorflow backend gpu instanc train time extrem long kernel verifi instal inspect output process gpu util tensorflow recogn gpu screenshot train time fact gpu usag long epoch train time caus issu",
        "Question_gpt_summary_original":"The user is facing challenges with slow training time while using a ResNet50 model with keras and tensorflow backend on a sagemaker GPU instance. The user has verified that tensorflow-gpu is installed and the GPU is recognized by tensorflow, but the GPU utilization is never above 0%. The user is unsure if sagemaker is actually using the GPU and is also considering if there could be another issue causing the long epoch training time.",
        "Question_gpt_summary":"user face challeng slow train time resnet model kera tensorflow backend gpu instanc user verifi tensorflow gpu instal gpu recogn tensorflow gpu util user unsur actual gpu consid issu caus long epoch train time",
        "Answer_original_content":"look like complet step take long step time data load data store try data load pictur cach feed singl imag dnn repeatedli help",
        "Answer_preprocessed_content":"look like complet step take long step time data load data store try data load pictur cach feed singl imag dnn repeatedli help",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are: \n- Check the step time and verify if it is taking too long.\n- Verify if the data loading is causing the issue and try to take data loading out of the picture by caching and feeding a single image to the DNN repeatedly.",
        "Answer_gpt_summary":"possibl solut mention discuss check step time verifi take long verifi data load caus issu try data load pictur cach feed singl imag dnn repeatedli"
    },
    {
        "Question_title":"How to optimize for multiple metrics in Optuna",
        "Question_body":"<p>How do I optimize for multiple metrics simultaneously inside the <code>objective<\/code> function of Optuna. For example, I am training an LGBM classifier and want to find the best hyperparameter set for all common classification metrics like F1, precision, recall, accuracy, AUC, etc.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    # Train\n    gbm = lgb.train(param, dtrain)\n\n    preds = gbm.predict(X_test)\n    pred_labels = np.rint(preds)\n    # Calculate metrics\n    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n    recall = metrics.recall_score(pred_labels, y_test)\n    precision = metrics.precision_score(pred_labels, y_test)\n    f1 = metrics.f1_score(pred_labels, y_test, pos_label=1)\n\n    ...\n<\/code><\/pre>\n<p>How do I do it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1630917852487,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1630917952870,
        "Question_score":2,
        "Question_view_count":1887.0,
        "Poster_id":11922237.0,
        "Poster_created_time":1565697423932,
        "Poster_last_access_time":1664031418056,
        "Poster_age":null,
        "Poster_location":"Uzbekistan",
        "Poster_reputation":602.0,
        "Poster_views":117.0,
        "Answer_body":"<p>After defining the grid and fitting the model with these params and generate predictions, calculate all metrics you want to optimize for:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    param_grid = {&quot;n_estimators&quot;: trial.suggest_int(&quot;n_estimators&quot;, 2000, 10000, step=200)}\n    clf = lgbm.LGBMClassifier(objective='binary', **param_grid)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_valid)\n    probs = clf.predict_proba(X_valid)\n \n    # Metrics\n    f1 = sklearn.metrics.f1_score(y_valid, press)\n    accuracy = ...\n    precision = ...\n    recall = ...\n    logloss = ...\n<\/code><\/pre>\n<p>and return them in the order you want:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    ...\n\n    return f1, logloss, accuracy, precision, recall\n<\/code><\/pre>\n<p>Then, in the study object, specify whether you want to minimize or maximize each metric to <code>directions<\/code> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>study = optuna.create_study(directions=['maximize', 'minimize', 'maximize', 'maximize', 'maximize'])\n\nstudy.optimize(objective, n_trials=100)\n<\/code><\/pre>\n<p>For more details, see <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\" rel=\"nofollow noreferrer\">Multi-objective Optimization with Optuna<\/a> in the documentation.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1659853446112,
        "Answer_score":6.0,
        "Answerer_id":11922237.0,
        "Answerer_created_time":1565697423932,
        "Answerer_last_access_time":1664031418056,
        "Answerer_age":null,
        "Answerer_location":"Uzbekistan",
        "Answerer_reputation":602.0,
        "Answerer_views":117.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69071684",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Question_closed_time":1630917852487,
        "Question_original_content":"optim multipl metric optim multipl metric simultan insid object function exampl train lgbm classifi want best hyperparamet set common classif metric like precis recal accuraci auc def object trial train gbm lgb train param dtrain pred gbm predict test pred label rint pred calcul metric accuraci sklearn metric accuraci score test pred label recal metric recal score pred label test precis metric precis score pred label test metric score pred label test po label",
        "Question_preprocessed_content":"optim multipl metric optim multipl metric simultan insid function exampl train lgbm classifi want best hyperparamet set common classif metric like precis recal accuraci auc",
        "Question_gpt_summary_original":"The user is facing a challenge of optimizing for multiple metrics simultaneously inside the objective function of Optuna while training an LGBM classifier. The user wants to find the best hyperparameter set for all common classification metrics like F1, precision, recall, accuracy, AUC, etc.",
        "Question_gpt_summary":"user face challeng optim multipl metric simultan insid object function train lgbm classifi user want best hyperparamet set common classif metric like precis recal accuraci auc",
        "Answer_original_content":"defin grid fit model param gener predict calcul metric want optim def object trial param grid estim trial suggest int estim step clf lgbm lgbmclassifi object binari param grid clf fit train train pred clf predict valid prob clf predict proba valid metric sklearn metric score valid press accuraci precis recal logloss return order want def object trial return logloss accuraci precis recal studi object specifi want minim maxim metric direct like studi creat studi direct maxim minim maxim maxim maxim studi optim object trial detail multi object optim document",
        "Answer_preprocessed_content":"defin grid fit model param gener predict calcul metric want optim return order want studi object specifi want minim maxim metric like detail optim document",
        "Answer_gpt_summary_original":"The solution suggested in the discussion is to define the grid and fit the model with the parameters, generate predictions, and calculate all the metrics that need to be optimized for. Then, return the metrics in the order they need to be optimized for and specify whether to minimize or maximize each metric in the study object using the directions parameter. The documentation link for multi-objective optimization with Optuna is also provided for more details.",
        "Answer_gpt_summary":"solut suggest discuss defin grid fit model paramet gener predict calcul metric need optim return metric order need optim specifi minim maxim metric studi object direct paramet document link multi object optim provid detail"
    },
    {
        "Question_title":"Azure container instances deployment failed",
        "Question_body":"<p>I am deploying a machine learning image to Azure Container Instances from Azure Machine Learning services according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml\" rel=\"nofollow noreferrer\">this article<\/a>, but am always stuck with the error message:<\/p>\n\n<blockquote>\n  <p>Aci Deployment failed with exception: Your container application crashed. This may be caused by errors in your scoring file's init() function.<br>\n  Please check the logs for your container instance xxxxxxx'.<\/p>\n<\/blockquote>\n\n<p>I tried:<\/p>\n\n<ol>\n<li>increasing memory_gb=4 in aci_config.<\/li>\n<li>I did\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-troubleshoot-deployment#debug-the-docker-image-locally\" rel=\"nofollow noreferrer\">troubleshooting<\/a> locally, but I could not have found any.<\/li>\n<\/ol>\n\n<p>Below is my score.py<\/p>\n\n<pre><code>def init():\n    global model\n    model_path = Model.get_model_path('pofc_fc_model')\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)['data'])\n    y_hat = model.predict(data)\n    return y_hat.tolist()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1553593173310,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1562618473092,
        "Question_score":2,
        "Question_view_count":3020.0,
        "Poster_id":8960386.0,
        "Poster_created_time":1510960409296,
        "Poster_last_access_time":1640215803972,
        "Poster_age":null,
        "Poster_location":"Bangkok Thailand",
        "Poster_reputation":306.0,
        "Poster_views":32.0,
        "Answer_body":"<p>Have you registered the model <code>'pofc_fc_model'<\/code> in your workspace using the <code>register()<\/code> function on the model object? If not, there will be no model path and that can cause failure.<\/p>\n\n<p>See this section on model registration: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#registermodel\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#registermodel<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":6610036.0,
        "Answerer_created_time":1468951834403,
        "Answerer_last_access_time":1663711534752,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation":645.0,
        "Answerer_views":101.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55353889",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1553715289510,
        "Question_original_content":"azur contain instanc deploy fail deploi machin learn imag azur contain instanc servic accord articl stuck error messag aci deploy fail except contain applic crash caus error score file init function check log contain instanc tri increas memori aci config troubleshoot local score def init global model model path model model path pofc model model joblib load model path def run raw data data arrai json load raw data data hat model predict data return hat tolist",
        "Question_preprocessed_content":"azur contain instanc deploy fail deploi machin learn imag azur contain instanc servic accord articl stuck error messag aci deploy fail except contain applic crash caus error score file init function check log contain instanc tri increas troubleshoot local",
        "Question_gpt_summary_original":"The user is encountering challenges while deploying a machine learning image to Azure Container Instances from Azure Machine Learning services. The deployment is failing with an error message indicating that the container application crashed, possibly due to errors in the scoring file's init() function. The user has tried increasing memory_gb and troubleshooting locally but has not found any solutions. The score.py file is also provided.",
        "Question_gpt_summary":"user encount challeng deploi machin learn imag azur contain instanc servic deploy fail error messag indic contain applic crash possibl error score file init function user tri increas memori troubleshoot local solut score file provid",
        "Answer_original_content":"regist model pofc model workspac regist function model object model path caus failur section model registr http doc microsoft com azur machin learn servic deploi registermodel",
        "Answer_preprocessed_content":"regist model workspac function model object model path caus failur section model registr",
        "Answer_gpt_summary_original":"Solution: The user is advised to register the model 'pofc_fc_model' in their workspace using the register() function on the model object. This can be done by following the instructions provided in the section on model registration in the Azure Machine Learning documentation.",
        "Answer_gpt_summary":"solut user advis regist model pofc model workspac regist function model object follow instruct provid section model registr document"
    },
    {
        "Question_title":"Making a Prediction Sagemaker Pytorch",
        "Question_body":"<p>I have trained and deployed a model in Pytorch with Sagemaker. I am able to call the endpoint and get a prediction. I am using the default input_fn() function (i.e. not defined in my serve.py).<\/p>\n\n<pre><code>model = PyTorchModel(model_data=trained_model_location,\n                     role=role,\n                     framework_version='1.0.0',\n                     entry_point='serve.py',\n                     source_dir='source')\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>A prediction can be made as follows:<\/p>\n\n<pre><code>input =\"0.12787057,  1.0612601,  -1.1081504\"\npredictor.predict(np.genfromtxt(StringIO(input), delimiter=\",\").reshape(1,3) )\n<\/code><\/pre>\n\n<p>I want to be able to serve the model with REST API and am HTTP POST using lambda and API gateway. I was able to use invoke_endpoint() for this with an XGBOOST model in Sagemaker this way. I am not sure what to send into the body for Pytorch.<\/p>\n\n<pre><code>client = boto3.client('sagemaker-runtime')\nresponse = client.invoke_endpoint(EndpointName=ENDPOINT  ,\nContentType='text\/csv',\nBody=???)\n<\/code><\/pre>\n\n<p>I believe I need to understand how to write the customer input_fn to accept and process the type of data I am able to send through invoke_client. Am I on the right track and if so, how could the input_fn be written to accept a csv from invoke_endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1559765963530,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":2346.0,
        "Poster_id":569313.0,
        "Poster_created_time":1294628108596,
        "Poster_last_access_time":1663884676412,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1748.0,
        "Poster_views":393.0,
        "Answer_body":"<p>Yes you are on the right track. You can send csv-serialized input to the endpoint without using the <code>predictor<\/code> from the SageMaker SDK, and using other SDKs such as <code>boto3<\/code> which is installed in lambda:<\/p>\n\n<pre><code>import boto3\nruntime = boto3.client('sagemaker-runtime')\n\npayload = '0.12787057,  1.0612601,  -1.1081504'\n\nresponse = runtime.invoke_endpoint(\n    EndpointName=ENDPOINT_NAME,\n    ContentType='text\/csv',\n    Body=payload.encode('utf-8'))\n\nresult = json.loads(response['Body'].read().decode()) \n<\/code><\/pre>\n\n<p>This will pass to the endpoint a csv-formatted input, that you may need to reshape back in the <code>input_fn<\/code> to put in the appropriate dimension expected by the model.<\/p>\n\n<p>for example:<\/p>\n\n<pre><code>def input_fn(request_body, request_content_type):\n    if request_content_type == 'text\/csv':\n        return torch.from_numpy(\n            np.genfromtxt(StringIO(request_body), delimiter=',').reshape(1,3))\n<\/code><\/pre>\n\n<p><strong>Note<\/strong>: I wasn't able to test the specific <code>input_fn<\/code> above with your input content and shape but I used the approach on Sklearn RandomForest couple times, and looking at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_pytorch.html#model-serving\" rel=\"nofollow noreferrer\">Pytorch SageMaker serving doc<\/a> the above rationale should work.<\/p>\n\n<p>Don't hesitate to use endpoint logs in Cloudwatch to diagnose any inference error (available from the endpoint UI in the console), those logs are usually <strong>much more verbose<\/strong> that the high-level logs returned by the inference SDKs<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":5331834.0,
        "Answerer_created_time":1442180190107,
        "Answerer_last_access_time":1664048608876,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56467434",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1559781766688,
        "Question_original_content":"make predict pytorch train deploi model pytorch abl endpoint predict default input function defin serv model pytorchmodel model data train model locat role role framework version entri point serv sourc dir sourc predictor model deploi initi instanc count instanc type xlarg predict follow input predictor predict genfromtxt stringio input delimit reshap want abl serv model rest api http post lambda api gatewai abl us invok endpoint xgboost model wai sure send bodi pytorch client boto client runtim respons client invok endpoint endpointnam endpoint contenttyp text csv bodi believ need understand write custom input accept process type data abl send invok client right track input written accept csv invok endpoint",
        "Question_preprocessed_content":"make predict pytorch train deploi model pytorch abl endpoint predict default function predict follow want abl serv model rest api http post lambda api gatewai abl us xgboost model wai sure send bodi pytorch believ need understand write custom accept process type data abl send right track written accept csv",
        "Question_gpt_summary_original":"The user has trained and deployed a Pytorch model in Sagemaker and is able to get a prediction using the default input_fn() function. However, they are facing challenges in serving the model with REST API and HTTP POST using lambda and API gateway. They are not sure what to send into the body for Pytorch and need to understand how to write a custom input_fn to accept and process the type of data they are able to send through invoke_client.",
        "Question_gpt_summary":"user train deploi pytorch model abl predict default input function face challeng serv model rest api http post lambda api gatewai sure send bodi pytorch need understand write custom input accept process type data abl send invok client",
        "Answer_original_content":"ye right track send csv serial input endpoint predictor sdk sdk boto instal lambda import boto runtim boto client runtim payload respons runtim invok endpoint endpointnam endpoint contenttyp text csv bodi payload encod utf result json load respons bodi read decod pass endpoint csv format input need reshap input appropri dimens expect model exampl def input request bodi request content type request content type text csv return torch numpi genfromtxt stringio request bodi delimit reshap note wasn abl test specif input input content shape approach sklearn randomforest coupl time look pytorch serv doc rational work hesit us endpoint log cloudwatch diagnos infer error avail endpoint consol log usual verbos high level log return infer sdk",
        "Answer_preprocessed_content":"ye right track send input endpoint sdk sdk instal lambda pass endpoint input need reshap appropri dimens expect model exampl note wasn abl test specif input content shape approach sklearn randomforest coupl time look pytorch serv doc rational work hesit us endpoint log cloudwatch diagnos infer error log usual verbos log return infer sdk",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Send csv-serialized input to the endpoint without using the predictor from the SageMaker SDK, and using other SDKs such as boto3 which is installed in lambda.\n- Pass to the endpoint a csv-formatted input, that may need to be reshaped back in the input_fn to put in the appropriate dimension expected by the model.\n- Use endpoint logs in Cloudwatch to diagnose any inference error, as those logs are usually much more verbose than the high-level logs returned by the inference SDKs.",
        "Answer_gpt_summary":"possibl solut mention discuss send csv serial input endpoint predictor sdk sdk boto instal lambda pass endpoint csv format input need reshap input appropri dimens expect model us endpoint log cloudwatch diagnos infer error log usual verbos high level log return infer sdk"
    },
    {
        "Question_title":"Rest api to create or update azure ML workspace doesn't create dependant resources",
        "Question_body":"Problem with https:\/\/docs.microsoft.com\/en-gb\/rest\/api\/azureml\/workspacesandcomputes\/workspaces\/createorupdate API... In the request body, Is it mandatory to create storage account, app insights, key vault, registration resources before? Ideally since these are dependent resources, shouldn\u2019t it be created as part of workflow creation?\nI get below response when dependent resources are not created prior.\n\n `{\n   \u201cerror\u201d: {\n     \u201ccode\u201d: \u201cValidationError\u201d,\n     \u201cmessage\u201d: \u201cOne or more validation errors occured.\u201c,\n     \u201cmessageFormat\u201d: null,\n     \u201cmessageParameters\u201d: null,\n     \u201creferenceCode\u201d: null,\n     \u201cdetailsUri\u201d: null,\n     \u201ctarget\u201d: \u201cCan not perform requested operation on nested resource. Parent resource \u2018&amp;lt;resourceid&amp;gt;\u2019 not found.\u201c,\n     \u201cdetails\u201d: [],\n     \u201cinnerError\u201d: null,\n     \u201cdebugInfo\u201d: null\n   },\n   \u201ccorrelation\u201d: {\n     \u201coperation\u201d: \u201c&amp;lt;opid&amp;gt;\u201c,\n     \u201crequest\u201d: \u201c&amp;lt;reqid&amp;gt;\u201d\n   },\n   \u201cenvironment\u201d: \u201cwestus\u201d,\n   \u201clocation\u201d: \u201cwestus\u201d,\n   \u201ctime\u201d: \u201c2020-06-03T07:13:14.6463577+00:00&amp;#34;\n }`\n\n\n\n\nI need an API which works similar to https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/workspace?view=azure-cli-latest",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1591181902550,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi @HarshiniKS-4497,\n\nYes, the REST API needs the other resource ids to be mentioned in the request body or they need to be created prior to this call unlike azure cli which provides the option to create them in a single request with input parameters. You could also try to use ARM template to create all the resources by calling this action from PS or cli.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/31569\/rest-api-to-create-or-update-workspace-doesnt-crea.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1591266732952,
        "Question_original_content":"rest api creat updat workspac creat depend resourc problem http doc microsoft com rest api workspacesandcomput workspac createorupd api request bodi mandatori creat storag account app insight kei vault registr resourc ideal depend resourc shouldnt creat workflow creation respons depend resourc creat prior error code validationerror messag valid error occur messageformat null messageparamet null referencecod null detailsuri null target perform request oper nest resourc parent resourc resourceid detail innererror null debuginfo null correl oper opid request reqid environ westu locat westu time need api work similar http doc microsoft com cli azur ext azur cli workspac view azur cli latest",
        "Question_preprocessed_content":"rest api creat updat workspac creat depend resourc problem request bodi mandatori creat storag account app insight kei vault registr resourc ideal depend resourc shouldnt creat workflow creation respons depend resourc creat prior need api work similar",
        "Question_gpt_summary_original":"The user is facing challenges with the Azure ML workspace API, specifically with the creation of dependent resources such as storage account, app insights, key vault, and registration resources. The API requires these resources to be created prior to the workflow creation, which is causing validation errors when they are not present. The user is seeking an API that works similarly to the Azure CLI for ML workspace creation.",
        "Question_gpt_summary":"user face challeng workspac api specif creation depend resourc storag account app insight kei vault registr resourc api requir resourc creat prior workflow creation caus valid error present user seek api work similarli azur cli workspac creation",
        "Answer_original_content":"harshinik ye rest api need resourc id mention request bodi need creat prior unlik azur cli provid option creat singl request input paramet try us arm templat creat resourc call action cli",
        "Answer_preprocessed_content":"ye rest api need resourc id mention request bodi need creat prior unlik azur cli provid option creat singl request input paramet try us arm templat creat resourc call action cli",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use an ARM template to create all the required resources prior to calling the Azure ML workspace API. This can be done by calling the ARM template action from PowerShell or CLI. However, no API similar to Azure CLI for ML workspace creation was mentioned in the discussion.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us arm templat creat requir resourc prior call workspac api call arm templat action powershel cli api similar azur cli workspac creation mention discuss"
    },
    {
        "Question_title":"AWS Sagemaker ClientError: Unable to initialize the algorithm",
        "Question_body":"<p>Cannot run hyper-parameter auto tuning jobs using the image classification algorithm. <\/p>\n\n<p>Getting this from Sagemaker job info:<\/p>\n\n<blockquote>\n  <p>Failure reason\n  ClientError: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError) Caused by: Additional properties are not allowed (u'val' was unexpected) Failed validating u'additionalProperties' in schema: {u'$schema': u'<a href=\"http:\/\/json-schema.org\/draft-04\/schema#\" rel=\"noreferrer\">http:\/\/json-schema.org\/draft-04\/schema#<\/a>', u'additionalProperties': False, u'anyOf': [{u'required': [u'train']}, {u'required': [u'validation']}, {u'optional': [u'train_lst']}, {u'optional': [u'validation_lst']}, {u'optional': [u'model']}], u'definitions': {u'data_channel': {u'properties': {u'ContentType': {u'type': u'string'}}, u'type': u'object'}}, u'properties': {u'model': {u'$ref': u'#\/definitions\/data_channel'}, u'train': {u'$ref': u'#\/definitions\/data_channel'}, u'train_lst': {u'$ref': u'#\/definitions\/data_channel'}, u'validation': {u'$ref': u'#\/definitio<\/p>\n<\/blockquote>\n\n<p>CloudWatch is giving me this reason:<\/p>\n\n<blockquote>\n  <p>00:42:35\n  2018-12-09 22:42:35 Customer Error: Unable to initialize the algorithm. Failed to validate input data configuration. (caused by ValidationError)<\/p>\n  \n  <p>Caused by: Additional properties are not allowed (u'val' was\n  unexpected)<\/p>\n<\/blockquote>\n\n<p>Any help please thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1544396689430,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1546636801950,
        "Question_score":6,
        "Question_view_count":1697.0,
        "Poster_id":4453351.0,
        "Poster_created_time":1421238326280,
        "Poster_last_access_time":1664055207523,
        "Poster_age":null,
        "Poster_location":"Melrose, Johannesburg, Gauteng, South Africa",
        "Poster_reputation":1951.0,
        "Poster_views":217.0,
        "Answer_body":"<p>as showed in your log, one of input channels was named as <code>val<\/code>. The correct channel name for validation data should be <code>validation<\/code>. More details on input configuration can be found here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":10130097.0,
        "Answerer_created_time":1532464254552,
        "Answerer_last_access_time":1565390460652,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":81.0,
        "Answerer_views":16.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53697587",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1547752819392,
        "Question_original_content":"clienterror unabl initi algorithm run hyper paramet auto tune job imag classif algorithm get job info failur reason clienterror unabl initi algorithm fail valid input data configur caus validationerror caus addit properti allow val unexpect fail valid additionalproperti schema schema http json schema org draft schema additionalproperti fals anyof requir train requir valid option train lst option valid lst option model definit data channel properti contenttyp type string type object properti model ref definit data channel train ref definit data channel train lst ref definit data channel valid ref definitio cloudwatch give reason custom error unabl initi algorithm fail valid input data configur caus validationerror caus addit properti allow val unexpect help thank",
        "Question_preprocessed_content":"clienterror unabl initi algorithm run auto tune job imag classif algorithm get job info failur reason clienterror unabl initi algorithm fail valid input data configur caus addit properti allow fail valid additionalproperti schema definit type object properti train valid ref cloudwatch give reason custom error unabl initi algorithm fail valid input data configur caus addit properti allow help thank",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to run hyper-parameter auto tuning jobs using the image classification algorithm on AWS Sagemaker. The error message indicates that there is a failure to validate input data configuration, caused by additional properties that are not allowed. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user encount challeng try run hyper paramet auto tune job imag classif algorithm error messag indic failur valid input data configur caus addit properti allow user seek help resolv issu",
        "Answer_original_content":"show log input channel name val correct channel valid data valid detail input configur http doc aw amazon com latest imag classif html",
        "Answer_preprocessed_content":"show log input channel name correct channel valid data detail input configur",
        "Answer_gpt_summary_original":"Solution: The error message is caused by an incorrect input channel name. The correct channel name for validation data should be \"validation\" instead of \"val\". More details on input configuration can be found in the AWS Sagemaker documentation.",
        "Answer_gpt_summary":"solut error messag caus incorrect input channel correct channel valid data valid instead val detail input configur document"
    },
    {
        "Question_title":"Visualizations, metrics, etc. keep randomly appearing and disappearing",
        "Question_body":"<p>This morning we were looking at the visualizations and charts on an active training run and everything was fine. After about 11am PDT, all of the visualizations started randomly disappearing. Sometimes only the loss charts would be visible, other times the losses and metrics and statistics would all be visible.<\/p>\n<p>The best I can tell is that the site is only showing charts for whatever things were in the most recent step. If you send some things less frequently, then their charts\/visualizations disappear until they\u2019re in the step data again.<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_created_time":1650483251598,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4,
        "Question_view_count":147.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/cogwheel\">@cogwheel<\/a> Could you possibly send me a link to your workspace?  If you don\u2019t want to share here you can also email <a href=\"mailto:support@wandb.com\">support@wandb.com<\/a> and explain the issue and I can respond via email.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/visualizations-metrics-etc-keep-randomly-appearing-and-disappearing\/2283",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1650494038572,
        "Question_original_content":"visual metric randomli appear disappear morn look visual chart activ train run fine pdt visual start randomli disappear loss chart visibl time loss metric statist visibl best tell site show chart thing recent step send thing frequent chart visual disappear theyr step data",
        "Question_preprocessed_content":"visual metric randomli appear disappear morn look visual chart activ train run fine pdt visual start randomli disappear loss chart visibl time loss metric statist visibl best tell site show chart thing recent step send thing frequent disappear theyr step data",
        "Question_gpt_summary_original":"The user is experiencing challenges with visualizations and charts disappearing randomly during an active training run. The issue seems to be related to the site only showing charts for the most recent step, causing charts for less frequently sent data to disappear until they are in the step data again.",
        "Question_gpt_summary":"user experienc challeng visual chart disappear randomli activ train run issu relat site show chart recent step caus chart frequent sent data disappear step data",
        "Answer_original_content":"cogwheel possibl send link workspac dont want share email support com explain issu respond email",
        "Answer_preprocessed_content":"possibl send link workspac dont want share email explain issu respond email",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut mention discuss"
    },
    {
        "Question_title":"SQL - How to join two tables using values from the other table for missing or null values in either table",
        "Question_body":"<p><strong>Alright StackOverflow, I have a problem:<\/strong><\/p>\n\n<p>I am doing some work with <a href=\"http:\/\/azureml.com\" rel=\"nofollow\" title=\"Azure Machine Learning\">Azure Machine Learning<\/a> and I have reached an impasse. I have two tables, and I need to join them. The tables look like this:<\/p>\n\n<pre><code>   TABLE A          TABLE B   \n+-----------+    +-----------+\n| a | b | c |    | a | b | c |\n+-----------+    +-----------+\n| 1 | 2 |   |    |   | 2 | 3 |\n+-----------+    +-----------+\n<\/code><\/pre>\n\n<p>(those are just examples.)<\/p>\n\n<p>I need to join these tables when columns they share (in this case only b, but could be multiple) are equivalent. I also, however, need to populate missing values. If TABLE A is missing a value for one of its columns, and TABLE B has it for a matching row, they should combine values in the result table. I know that there is a way to do this one way, but it also needs to work in reverse, so that if TABLE B is missing a value, and TABLE A has one, it is populated.<\/p>\n\n<p><strong>EDIT: Desired Result:<\/strong><\/p>\n\n<pre><code>   TABLE C\n+-----------+\n| a | b | c |\n+-----------+\n| 1 | 2 | 3 |\n+-----------+\n<\/code><\/pre>\n\n<p>Some background information:<\/p>\n\n<ul>\n<li>AzureML uses a form of SQLite for their SQL interpretation, so please try and keep your answers in as basic SQL as possible. Thanks! :)<\/li>\n<li>AzureML has a built in join module, for those familiar with AzureML, but I don't think it'll be able to accomplish what is necessary. I'll use the SQL interpretation module.<\/li>\n<\/ul>\n\n<p>Your assistance is appreciated! Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1430669511030,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1430676924820,
        "Question_score":0,
        "Question_view_count":90.0,
        "Poster_id":4196127.0,
        "Poster_created_time":1414621306620,
        "Poster_last_access_time":1663883459472,
        "Poster_age":null,
        "Poster_location":"Raleigh, NC, USA",
        "Poster_reputation":1000.0,
        "Poster_views":124.0,
        "Answer_body":"<p><strong>Answering my own question:<\/strong><\/p>\n\n<p>It turned out the join type I needed was a <em>Full Outer Join.<\/em><\/p>\n\n<p>Background information:<\/p>\n\n<ul>\n<li>For those pursuing AzureML related to this question in the future, I had to enable the functionality to save columns from the 'Right' table input.<\/li>\n<li>I then ran this through a 'Project Columns' module and a 'Metadata' module to rename them to the form I desired.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4196127.0,
        "Answerer_created_time":1414621306620,
        "Answerer_last_access_time":1663883459472,
        "Answerer_age":null,
        "Answerer_location":"Raleigh, NC, USA",
        "Answerer_reputation":1000.0,
        "Answerer_views":124.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30016116",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1430677708887,
        "Question_original_content":"sql join tabl valu tabl miss null valu tabl alright stackoverflow problem work reach impass tabl need join tabl look like tabl tabl exampl need join tabl column share case multipl equival need popul miss valu tabl miss valu column tabl match row combin valu result tabl know wai wai need work revers tabl miss valu tabl popul edit desir result tabl background inform us form sqlite sql interpret try answer basic sql possibl thank built join modul familiar think abl accomplish necessari us sql interpret modul assist appreci thank",
        "Question_preprocessed_content":"sql join tabl valu tabl miss null valu tabl alright stackoverflow problem work reach impass tabl need join tabl look like need join tabl column share equival need popul miss valu tabl miss valu column tabl match row combin valu result tabl know wai wai need work revers tabl miss valu tabl popul edit desir result background inform us form sqlite sql interpret try answer basic sql possibl thank built join modul familiar think abl accomplish necessari us sql interpret modul assist appreci thank",
        "Question_gpt_summary_original":"The user is facing a challenge in joining two tables in Azure Machine Learning using values from the other table for missing or null values in either table. The user needs to join the tables when columns they share are equivalent and populate missing values. The desired result is a new table that combines the values from both tables. The user is using a form of SQLite for their SQL interpretation and needs basic SQL solutions.",
        "Question_gpt_summary":"user face challeng join tabl valu tabl miss null valu tabl user need join tabl column share equival popul miss valu desir result new tabl combin valu tabl user form sqlite sql interpret need basic sql solut",
        "Answer_original_content":"answer question turn join type need outer join background inform pursu relat question futur enabl function save column right tabl input ran project column modul metadata modul renam form desir",
        "Answer_preprocessed_content":"answer question turn join type need outer join background inform pursu relat question futur enabl function save column right tabl input ran project column modul metadata modul renam form desir",
        "Answer_gpt_summary_original":"Solution: The user found that a Full Outer Join was the join type needed to join the two tables in Azure Machine Learning. They also enabled the functionality to save columns from the 'Right' table input, ran it through a 'Project Columns' module, and a 'Metadata' module to rename them to the desired form.",
        "Answer_gpt_summary":"solut user outer join join type need join tabl enabl function save column right tabl input ran project column modul metadata modul renam desir form"
    },
    {
        "Question_title":"How to pass a bigger .csv files to amazon sagemaker for predictions using batch transform jobs",
        "Question_body":"<p>I created a custom model and deployed it on sagemaker. I am invoking the endpoint using batch transform jobs. It works if the input file is small, i.e, number of rows in the csv file is less. If I upload a file with around 200000 rows, I am getting this error in the cloudwatch logs.<\/p>\n\n<pre><code>2018-11-21 09:11:52.666476: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113]\nAllocation of 2878368000 exceeds 10% of system memory.\n2018-11-21 09:11:53.166493: W external\/org_tensorflow\/tensorflow\/core\/framework\/allocator.cc:113] \nAllocation of 2878368000 exceeds 10% of system memory.\n[2018-11-21 09:12:02,544] ERROR in serving: &lt;_Rendezvous of RPC that \nterminated with:\n#011status = StatusCode.DEADLINE_EXCEEDED\n#011details = \"Deadline Exceeded\"\n#011debug_error_string = \"\n{\n\"created\": \"@1542791522.543282048\",\n\"description\": \"Error received from peer\",\n\"file\": \"src\/core\/lib\/surface\/call.cc\",\n\"file_line\": 1017,\n\"grpc_message\": \"Deadline Exceeded\",\n\"grpc_status\": 4\n}\n\"\n<\/code><\/pre>\n\n<p>Any ideas what might be going wrong. This is the transform function which I am using to create the transform job.<\/p>\n\n<pre><code>transformer =sagemaker.transformer.Transformer(\nbase_transform_job_name='Batch-Transform',\nmodel_name='sagemaker-tensorflow-2018-11-21-07-58-15-887',\ninstance_count=1,\ninstance_type='ml.m4.xlarge',\noutput_path='s3:\/\/2-n2m-sagemaker-json-output\/out_files\/'\n\n)\ninput_location = 's3:\/\/1-n2m-n2g-csv-input\/smal_sagemaker_sample.csv'\ntransformer.transform(input_location, content_type='text\/csv', split_type='Line')\n<\/code><\/pre>\n\n<p>The .csv file contains 2 columns for first and last name of customer, which I am then preprocessing it in the sagemaker itself using input_fn().<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1542792620897,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1542799396316,
        "Question_score":1,
        "Question_view_count":1941.0,
        "Poster_id":5430085.0,
        "Poster_created_time":1444454434270,
        "Poster_last_access_time":1652071645660,
        "Poster_age":null,
        "Poster_location":"Pune, Maharashtra, India",
        "Poster_reputation":140.0,
        "Poster_views":23.0,
        "Answer_body":"<p>The error looks to be coming from a GRPC client closing the connection before the server is able to respond. (There looks to be an existing feature request for the sagemaker tensorflow container on <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container\/issues\/46<\/a> to make this timeout configurable)<\/p>\n\n<p>You could try out a few things with the sagemaker Transformer to limit the size of each individual request so that it fits within the timeout:<\/p>\n\n<ul>\n<li>Set a <code>max_payload<\/code> to a smaller value, say 2-3 MB (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">the default is 6 MB<\/a>)<\/li>\n<li>If your instance metrics indicate it has compute \/ memory resources to spare, try <code>max_concurrent_transforms<\/code> > 1 to make use of multiple workers<\/li>\n<li>Split up your csv file into multiple input files. With a bigger dataset, you could also increase the instance count to fan out processing<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":10392672.0,
        "Answerer_created_time":1537462795807,
        "Answerer_last_access_time":1632172213510,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":99.0,
        "Answerer_views":14.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53408927",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1544503147007,
        "Question_original_content":"pass bigger csv file predict batch transform job creat custom model deploi invok endpoint batch transform job work input file small number row csv file upload file row get error cloudwatch log extern org tensorflow tensorflow core framework alloc alloc exce memori extern org tensorflow tensorflow core framework alloc alloc exce memori error serv rendezv rpc termin statu statuscod deadlin exceed detail deadlin exceed debug error string creat descript error receiv peer file src core lib surfac file line grpc messag deadlin exceed grpc statu idea go wrong transform function creat transform job transform transform transform base transform job batch transform model tensorflow instanc count instanc type xlarg output path json output file input locat csv input smal sampl csv transform transform input locat content type text csv split type line csv file contain column custom preprocess input",
        "Question_preprocessed_content":"pass bigger csv file predict batch transform job creat custom model deploi invok endpoint batch transform job work input file small number row csv file upload file row get error cloudwatch log idea go wrong transform function creat transform job csv file contain column custom preprocess",
        "Question_gpt_summary_original":"The user is encountering an error when trying to use batch transform jobs in Amazon Sagemaker to make predictions on a large .csv file with around 200,000 rows. The error message indicates that the allocation of memory exceeds the system limit, and the job terminates with a \"Deadline Exceeded\" error. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user encount error try us batch transform job predict larg csv file row error messag indic alloc memori exce limit job termin deadlin exceed error user seek advic resolv issu",
        "Answer_original_content":"error look come grpc client close connect server abl respond look exist featur request tensorflow contain http github com aw tensorflow contain issu timeout configur try thing transform limit size individu request fit timeout set max payload smaller valu default instanc metric indic comput memori resourc spare try max concurr transform us multipl worker split csv file multipl input file bigger dataset increas instanc count fan process",
        "Answer_preprocessed_content":"error look come grpc client close connect server abl respond try thing transform limit size individu request fit timeout set smaller valu instanc metric indic comput memori resourc spare try us multipl worker split csv file multipl input file bigger dataset increas instanc count fan process",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Set a smaller value for the `max_payload` parameter in the Sagemaker Transformer.\n- Increase the `max_concurrent_transforms` parameter if the instance has enough compute\/memory resources.\n- Split the CSV file into multiple input files or increase the instance count to fan out processing.\n\nThere is also an existing feature request to make the timeout configurable in the Sagemaker TensorFlow container.",
        "Answer_gpt_summary":"possibl solut mention discuss set smaller valu max payload paramet transform increas max concurr transform paramet instanc comput memori resourc split csv file multipl input file increas instanc count fan process exist featur request timeout configur tensorflow contain"
    },
    {
        "Question_title":"Sagemaker Studio is not opening after deleting lifecycle configuration",
        "Question_body":"I was working on a sagemaker studio for ML work, I attached Lifecycle Configuration with it, which was creating problem. Then I deleted the lifecycle configuration without detaching it, and this problem is happening. Can't start sagemaker studio notebook and this is shown.\n\nAny suggestion to fix this ?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1667756083758,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":23.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"You can try detaching the LCC script using the CLI. You can use the CloudShell from console, since your console role is able to perform updates on the domain.\n\nUse the update-domain CLI call, and provide an empty configuration for the default user settings, something like-\n\naws sagemaker update-domain --domain-id d-abc123 \\\n--default-user-settings '{\n\"JupyterServerAppSettings\": {\n  \"DefaultResourceSpec\": {\n    \"InstanceType\": \"system\"\n   },\n}}'",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU91ywEwTsRRqmHKZJ1yVrrA\/sagemaker-studio-is-not-opening-after-deleting-lifecycle-configuration",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1667785787444,
        "Question_original_content":"studio open delet lifecycl configur work studio work attach lifecycl configur creat problem delet lifecycl configur detach problem happen start studio notebook shown suggest fix",
        "Question_preprocessed_content":"studio open delet lifecycl configur work studio work attach lifecycl configur creat problem delet lifecycl configur detach problem happen start studio notebook shown suggest fix",
        "Question_gpt_summary_original":"The user is facing challenges with opening Sagemaker Studio after deleting a Lifecycle Configuration without detaching it. The notebook is not starting and the user is seeking suggestions to fix the issue.",
        "Question_gpt_summary":"user face challeng open studio delet lifecycl configur detach notebook start user seek suggest fix issu",
        "Answer_original_content":"try detach lcc script cli us cloudshel consol consol role abl perform updat domain us updat domain cli provid configur default user set like updat domain domain abc default user set jupyterserverappset defaultresourcespec instancetyp",
        "Answer_preprocessed_content":"try detach lcc script cli us cloudshel consol consol role abl perform updat domain us cli provid configur default user set like",
        "Answer_gpt_summary_original":"Solution: The user can try detaching the LCC script using the CLI. They can use the CloudShell from console and perform updates on the domain using the update-domain CLI call, and provide an empty configuration for the default user settings.",
        "Answer_gpt_summary":"solut user try detach lcc script cli us cloudshel consol perform updat domain updat domain cli provid configur default user set"
    },
    {
        "Question_title":"Azure ML: Getting Error 503: NoMoreResources to any web service API even when I only make 1 request",
        "Question_body":"<p>Getting the following response even when I make one request (concurrency set to 200) to a web service. <\/p>\n\n<p>{ status: 503, headers: '{\"content-length\":\"174\",\"content-type\":\"application\/json; charset=utf-8\",\"etag\":\"\\\"8ce068bf420a485c8096065ea3e4f436\\\"\",\"server\":\"Microsoft-HTTPAPI\/2.0\",\"x-ms-request-id\":\"d5c56cdd-644f-48ba-ba2b-6eb444975e4c\",\"date\":\"Mon, 15 Feb 2016 04:54:01 GMT\",\"connection\":\"close\"}',  body: '{\"error\":{\"code\":\"ServiceUnavailable\",\"message\":\"Service is temporarily unavailable.\",\"details\":[{\"code\":\"NoMoreResources\",\"message\":\"No resources available for request.\"}]}}' }<\/p>\n\n<p>The request-response web service is a recommender retraining web service with the training set containing close to 200k records. The training set is already present in my ML studio dataset, only 10-15 extra records are passed in the request. The same experiment was working flawlessly till 13th Feb 2016. I have already tried increasing the concurrency but still the same issue. I even reduced the size of the training set to 20 records, still didn't work.<\/p>\n\n<p>I have two web service both doing something similar and both aren't working since 13th Feb 2016. <\/p>\n\n<p>Finally, I created a really small experiment ( skill.csv --> split row ---> web output )   which doesn't take any input. It just has to return some part of the dataset. Did not work, response code 503.<\/p>\n\n<p>The logs I got are as follows<\/p>\n\n<p>{\n  \"version\": \"2014-10-01\",\n  \"diagnostics\": [{\n    .....\n    {\n      \"type\": \"GetResourceEndEvent\",\n      \"timestamp\": 13.1362,\n      \"resourceId\": \"5e2d653c2b214e4dad2927210af4a436.865467b9e7c5410e9ebe829abd0050cd.v1-default-111\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    },\n    {\n      \"type\": \"InitializationSummary\",\n      \"time\": \"2016-02-15T04:46:18.3651714Z\",\n      \"status\": \"Failure\",\n      \"error\": \"The Uri for the target storage location is not specified. Please consider changing the request's location mode.\"\n    }\n  ]\n}<\/p>\n\n<p>What am I missing? Or am I doing it completely wrong?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>PS: Data is stored in mongoDB and then imported as CSV<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1455545889500,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1456850010663,
        "Question_score":1,
        "Question_view_count":283.0,
        "Poster_id":4646197.0,
        "Poster_created_time":1425802890212,
        "Poster_last_access_time":1662088405512,
        "Poster_age":null,
        "Poster_location":"Boston, MA, USA",
        "Poster_reputation":41.0,
        "Poster_views":19.0,
        "Answer_body":"<p>This was an Azure problem. I quote the Microsoft guy, <\/p>\n\n<blockquote>\n  <p>We believe we have isolated the issue impacting tour service and we are currently working on a fix. We will be able to deploy this in the next couple of days. The problem is impacting only the ASIA AzureML region at this time, so if this is an option for you, might I suggest using a workspace in either the US or EU region until the fix gets rolled out here.<\/p>\n<\/blockquote>\n\n<p>To view the complete discussion, click <a href=\"https:\/\/social.msdn.microsoft.com\/Forums\/en-US\/985e253e-5e54-45a5-a359-5c501152c445\/getting-error-503-nomoreresources-to-any-web-service-api-even-when-i-only-make-1-request?forum=MachineLearning&amp;prof=required\" rel=\"nofollow\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":4646197.0,
        "Answerer_created_time":1425802890212,
        "Answerer_last_access_time":1662088405512,
        "Answerer_age":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation":41.0,
        "Answerer_views":19.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35411741",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1455597422632,
        "Question_original_content":"get error nomoreresourc web servic api request get follow respons request concurr set web servic statu header content length content type applic json charset utf etag cebfaceaef server microsoft httpapi request dccdd bab ebec date mon feb gmt connect close bodi error code serviceunavail messag servic temporarili unavail detail code nomoreresourc messag resourc avail request request respons web servic recommend retrain web servic train set contain close record train set present studio dataset extra record pass request experi work flawlessli till feb tri increas concurr issu reduc size train set record work web servic similar aren work feb final creat small experi skill csv split row web output input return dataset work respons code log got follow version diagnost type getresourceendev timestamp resourceid edcbedadafa beceebeabdcd default statu failur error uri target storag locat specifi consid chang request locat mode type initializationsummari time statu failur error uri target storag locat specifi consid chang request locat mode miss complet wrong thank advanc data store mongodb import csv",
        "Question_preprocessed_content":"get error nomoreresourc web servic api request get follow respons request web servic statu header feb gmt connect close bodi web servic recommend retrain web servic train set contain close record train set present studio dataset extra record pass request experi work flawlessli till feb tri increas concurr issu reduc size train set record work web servic similar aren work feb final creat small experi input return dataset work respons code log got follow version diagnost miss complet wrong thank advanc data store mongodb import csv",
        "Question_gpt_summary_original":"The user is encountering an error 503 when making requests to a web service API, even when only making one request with a concurrency set to 200. The web service is a recommender retraining web service with a training set containing close to 200k records. The training set is already present in the user's ML studio dataset, and only 10-15 extra records are passed in the request. The same experiment was working flawlessly until February 13th, 2016. The user has tried increasing the concurrency and reducing the size of the training set, but the issue persists. The user has also created a small experiment that doesn't take any input, but it still didn't work. The logs show an error related to the target storage location.",
        "Question_gpt_summary":"user encount error make request web servic api make request concurr set web servic recommend retrain web servic train set contain close record train set present user studio dataset extra record pass request experi work flawlessli februari user tri increas concurr reduc size train set issu persist user creat small experi input work log error relat target storag locat",
        "Answer_original_content":"azur problem quot microsoft gui believ isol issu impact tour servic current work fix abl deploi coupl dai problem impact asia region time option suggest workspac region fix get roll view complet discuss click",
        "Answer_preprocessed_content":"azur problem quot microsoft gui believ isol issu impact tour servic current work fix abl deploi coupl dai problem impact asia region time option suggest workspac region fix get roll view complet discuss click",
        "Answer_gpt_summary_original":"Solution: The issue was identified as an Azure problem, and the Microsoft team was working on a fix. The user was advised to use a workspace in either the US or EU region until the fix was rolled out in the ASIA AzureML region. No other solutions were mentioned.",
        "Answer_gpt_summary":"solut issu identifi azur problem microsoft team work fix user advis us workspac region fix roll asia region solut mention"
    },
    {
        "Question_title":"How can I mark an Azure Dataset as a time series dataset reading from a parquet folder with date partitions?",
        "Question_body":"<p>I would like to create a Time series dataset from a folder that contains parquet files this way:<\/p>\n<ul>\n<li>timestamp=2018-01-06<\/li>\n<li>timestamp=2018-01-07<\/li>\n<\/ul>\n<p>How can I make Azure Dataset, through the GUI, recognises the timestamp partition as a date and mark my dataset as a time series dataset?<\/p>\n<p>It is supposed to be automatic, but it doesn't work.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1601474658457,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":109.0,
        "Poster_id":4553735.0,
        "Poster_created_time":1423640080283,
        "Poster_last_access_time":1663943557963,
        "Poster_age":null,
        "Poster_location":"Lyon, France",
        "Poster_reputation":457.0,
        "Poster_views":125.0,
        "Answer_body":"<p>Thanks for reaching out to us.<\/p>\n<p>In Azure Machine Learning Studio, you would need to setup partition format similar to python SDK, as follows, assuming your data path is &quot;timeseries\/timestamp=2020-01-01\/data.parquet&quot;:\n<a href=\"https:\/\/i.stack.imgur.com\/HwYfF.png\" rel=\"nofollow noreferrer\">Set up partition format when creating time series dataset<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":14396244.0,
        "Answerer_created_time":1601920258310,
        "Answerer_last_access_time":1663615935520,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":56.0,
        "Answerer_views":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64139290",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1601920685456,
        "Question_original_content":"mark azur dataset time seri dataset read parquet folder date partit like creat time seri dataset folder contain parquet file wai timestamp timestamp azur dataset gui recognis timestamp partit date mark dataset time seri dataset suppos automat work",
        "Question_preprocessed_content":"mark azur dataset time seri dataset read parquet folder date partit like creat time seri dataset folder contain parquet file wai azur dataset gui recognis timestamp partit date mark dataset time seri dataset suppos automat work",
        "Question_gpt_summary_original":"The user is facing challenges in marking an Azure Dataset as a time series dataset while reading from a parquet folder with date partitions. The user is seeking guidance on how to make Azure Dataset recognize the timestamp partition as a date and mark the dataset as a time series dataset through the GUI. The automatic process is not working as expected.",
        "Question_gpt_summary":"user face challeng mark azur dataset time seri dataset read parquet folder date partit user seek guidanc azur dataset recogn timestamp partit date mark dataset time seri dataset gui automat process work expect",
        "Answer_original_content":"thank reach studio need setup partit format similar python sdk follow assum data path timeseri timestamp data parquet set partit format creat time seri dataset",
        "Answer_preprocessed_content":"thank reach studio need setup partit format similar python sdk follow assum data path set partit format creat time seri dataset",
        "Answer_gpt_summary_original":"Solution: The user can set up partition format similar to python SDK in Azure Machine Learning Studio to make Azure Dataset recognize the timestamp partition as a date and mark the dataset as a time series dataset through the GUI. The partition format should be set up assuming the data path is \"timeseries\/timestamp=2020-01-01\/data.parquet\".",
        "Answer_gpt_summary":"solut user set partit format similar python sdk studio azur dataset recogn timestamp partit date mark dataset time seri dataset gui partit format set assum data path timeseri timestamp data parquet"
    },
    {
        "Question_title":"How to disable logs in Kedro",
        "Question_body":"<p>I have been unsuccessful in disabling kedro logs.  I have tried adding <code>disable_existing_loggers: True<\/code> to the logging.yml file as well as <code>disable:True<\/code> to all of the existing logs and it still appears to be saving log files.  Any suggestions?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1573137628147,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":410.0,
        "Poster_id":7159026.0,
        "Poster_created_time":1479159384132,
        "Poster_last_access_time":1663361584220,
        "Poster_age":null,
        "Poster_location":"Illinois, United States",
        "Poster_reputation":513.0,
        "Poster_views":113.0,
        "Answer_body":"<p>If you want <code>kedro<\/code> to stop logging you can override the <code>_setup_logging<\/code> in <code>ProjectContext<\/code> in <code>src\/&lt;package-name&gt;\/run.py<\/code> as per the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/07_logging.html#configure-logging\" rel=\"nofollow noreferrer\">documentation<\/a>. For example:<\/p>\n\n<pre><code>class ProjectContext(KedroContext):\n    \"\"\"Users can override the remaining methods from the parent class here, or create new ones\n    (e.g. as required by plugins)\n\n    \"\"\"\n\n    project_name = \"&lt;PACKGE-NAME&gt;\"\n    project_version = \"0.15.4\"\n\n    def _get_pipelines(self) -&gt; Dict[str, Pipeline]:\n        return create_pipelines()\n\n    def _setup_logging(self) -&gt; None:\n        import logging\n        logging.disable()\n<\/code><\/pre>\n\n<p>If you want it to still log to the console, but not save to <code>logs\/info.log<\/code> then you can do <code>def _setup_logging(self) -&gt; None: pass<\/code>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1573142581883,
        "Answer_score":3.0,
        "Answerer_id":9999106.0,
        "Answerer_created_time":1530092504712,
        "Answerer_last_access_time":1652101318888,
        "Answerer_age":null,
        "Answerer_location":"London, United Kingdom",
        "Answerer_reputation":915.0,
        "Answerer_views":288.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58751122",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1573141908796,
        "Question_original_content":"disabl log unsuccess disabl log tri ad disabl exist logger true log yml file disabl true exist log appear save log file suggest",
        "Question_preprocessed_content":"disabl log unsuccess disabl log tri ad file exist log appear save log file suggest",
        "Question_gpt_summary_original":"The user is facing challenges in disabling logs in Kedro despite trying to add \"disable_existing_loggers: True\" to the logging.yml file and \"disable:True\" to all existing logs. The user is seeking suggestions to resolve the issue.",
        "Question_gpt_summary":"user face challeng disabl log despit try add disabl exist logger true log yml file disabl true exist log user seek suggest resolv issu",
        "Answer_original_content":"want stop log overrid setup log projectcontext src run document exampl class projectcontext context user overrid remain method parent class creat new on requir plugin project project version def pipelin self dict str pipelin return creat pipelin def setup log self import log log disabl want log consol save log info log def setup log self pass",
        "Answer_preprocessed_content":"want stop log overrid document exampl want log consol save",
        "Answer_gpt_summary_original":"Solution:\n- Override the `_setup_logging` method in `ProjectContext` in `src\/<package-name>\/run.py` and add `logging.disable()` to disable logging in Kedro. \n- If you want to still log to the console, but not save to `logs\/info.log`, then add `def _setup_logging(self) -> None: pass`.",
        "Answer_gpt_summary":"solut overrid setup log method projectcontext src run add log disabl disabl log want log consol save log info log add def setup log self pass"
    },
    {
        "Question_title":"Can I limit the type of instances that data scientists can launch for training jobs in SageMaker?",
        "Question_body":"We want to limit the types of instances that our data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. Is it possible to limit the instance size options available through SageMaker by using IAM policies, or another method? For example: Could we remove the ability to launch ml.p3.16xlarge instances?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1603454458000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":426.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Yes, you can limit the types of instances that are available for your data scientists to launch in SageMaker by using an IAM policy similar to the following one:\n\nNote: This example IAM policy allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.\n\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Sid\": \"EnforceInstanceType\",\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"sagemaker:CreateTrainingJob\",\n                \"sagemaker:CreateHyperParameterTuningJob\"\n            ],\n            \"Resource\": \"*\",\n            \"Condition\": {\n                \"ForAllValues:StringLike\": {\n                    \"sagemaker:InstanceTypes\": [\"ml.c5.*\"]\n                }\n            }\n        }\n\n     ]\n}",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUd77APmdHTx-2FZCvZfS6Qg\/can-i-limit-the-type-of-instances-that-data-scientists-can-launch-for-training-jobs-in-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1603455348000,
        "Question_original_content":"limit type instanc data scientist launch train job want limit type instanc data scientist launch run train job hyperparamet tune job possibl limit instanc size option avail iam polici method exampl remov abil launch xlarg instanc",
        "Question_preprocessed_content":"limit type instanc data scientist launch train job want limit type instanc data scientist launch run train job hyperparamet tune job possibl limit instanc size option avail iam polici method exampl remov abil launch instanc",
        "Question_gpt_summary_original":"The user wants to limit the types of instances that data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker. They are seeking a way to restrict the instance size options available through SageMaker, such as removing the ability to launch ml.p3.16xlarge instances, using IAM policies or another method.",
        "Question_gpt_summary":"user want limit type instanc data scientist launch run train job hyperparamet tune job seek wai restrict instanc size option avail remov abil launch xlarg instanc iam polici method",
        "Answer_original_content":"ye limit type instanc avail data scientist launch iam polici similar follow note exampl iam polici allow user launch comput optim type train job hyperparamet tune job version statement sid enforceinstancetyp effect allow action createtrainingjob createhyperparametertuningjob resourc condit forallvalu stringlik instancetyp",
        "Answer_preprocessed_content":"ye limit type instanc avail data scientist launch iam polici similar follow note exampl iam polici allow user launch comput optim type train job hyperparamet tune job version statement resourc condit",
        "Answer_gpt_summary_original":"Solution: The user can limit the types of instances that data scientists can launch for running training jobs and hyperparameter tuning jobs in SageMaker by using an IAM policy. The example IAM policy provided in the discussion allows SageMaker users to launch only Compute Optimized (ml.c5)-type training jobs and hyperparameter tuning jobs.",
        "Answer_gpt_summary":"solut user limit type instanc data scientist launch run train job hyperparamet tune job iam polici exampl iam polici provid discuss allow user launch comput optim type train job hyperparamet tune job"
    },
    {
        "Question_title":"Local controller seems block",
        "Question_body":"<p>I make the following sweep (yaml) file:<\/p>\n<pre><code class=\"lang-auto\">program: train_mnist.py\nmethod: grid\nparameters:\n  lr_schedule:\n    values: [ step, cyclic ]\n  epoch_total:\n    values: [ 2, 4 ]\nmetric:\n  goal: maximize\n  name: test-result\/accuracy\nproject: my-mnist-test-project\nname: MNIST-Sweep-Test\ndescription: test sweep demo\n<\/code><\/pre>\n<p>and I use <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/advanced-sweeps\/local-controller#running-the-local-controller-from-the-command-line\">local controller<\/a> to perform sweep locally. However, it seems block here:<\/p>\n<pre><code class=\"lang-auto\">(pytorch) geyao@geyaodeMacBook-Air wandb_test % wandb sweep --controller sweep_config.yaml\nwandb: Creating sweep from: sweep_config.yaml\nwandb: Created sweep with ID: o2mzl569\nwandb: View sweep at: https:\/\/wandb.ai\/geyao\/my-mnist-test-project\/sweeps\/o2mzl569\nwandb: Run sweep agent with: wandb agent geyao\/my-mnist-test-project\/o2mzl569\nwandb: Starting wandb controller...\nSweep: o2mzl569 (grid) | Runs: 0\n\n# ------blocked here!------\n<\/code><\/pre>\n<p>When I turn off the network, it will be:<\/p>\n<pre data-code-wrap=\"shell\"><code class=\"lang-nohighlight\">(pytorch) geyao@geyaodeMacBook-Air wandb_test % wandb sweep --controller sweep_config.yaml\nwandb: Creating sweep from: sweep_config.yaml\nwandb: Network error (ConnectionError), entering retry loop.\n<\/code><\/pre>\n<p>Why local controller tries to connect the network? How can I perform local sweep with\/without network in the right way?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1660803433186,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":287.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> , the local controller doesn\u2019t have the full functionality of W&amp;B cloud, and is not intended for actual hyperparameter optimization workloads. It\u2019s intended for development and debugging of new algorithms for the Sweeps tool. You don\u2019t need to connect to W&amp;B cloud service to use the controller.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/local-controller-seems-block\/2955",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1661992753334,
        "Question_original_content":"local control block follow sweep yaml file program train mnist method grid paramet schedul valu step cyclic epoch total valu metric goal maxim test result accuraci project mnist test project mnist sweep test descript test sweep demo us local control perform sweep local block pytorch geyao geyaodemacbook air test sweep control sweep config yaml creat sweep sweep config yaml creat sweep omzl view sweep http geyao mnist test project sweep omzl run sweep agent agent geyao mnist test project omzl start control sweep omzl grid run block turn network pytorch geyao geyaodemacbook air test sweep control sweep config yaml creat sweep sweep config yaml network error connectionerror enter retri loop local control tri connect network perform local sweep network right wai",
        "Question_preprocessed_content":"local control block follow sweep file us local control perform sweep local block turn network local control tri connect network perform local sweep network right wai",
        "Question_gpt_summary_original":"The user is facing challenges while performing a local sweep using the local controller. The local controller seems to be blocked and the user is unable to perform the sweep. When the network is turned off, the user encounters a network error. The user is unsure why the local controller is trying to connect to the network and is seeking guidance on how to perform a local sweep with or without a network connection.",
        "Question_gpt_summary":"user face challeng perform local sweep local control local control block user unabl perform sweep network turn user encount network error user unsur local control try connect network seek guidanc perform local sweep network connect",
        "Answer_original_content":"geyao local control doesnt function cloud intend actual hyperparamet optim workload intend develop debug new algorithm sweep tool dont need connect cloud servic us control",
        "Answer_preprocessed_content":"local control doesnt function cloud intend actual hyperparamet optim workload intend develop debug new algorithm sweep tool dont need connect cloud servic us control",
        "Answer_gpt_summary_original":"Solutions provided: The discussion suggests that the local controller is not intended for hyperparameter optimization workloads and is only meant for development and debugging of new algorithms for the Sweeps tool. It is also mentioned that the user does not need to connect to the W&B cloud service to use the controller.",
        "Answer_gpt_summary":"solut provid discuss suggest local control intend hyperparamet optim workload meant develop debug new algorithm sweep tool mention user need connect cloud servic us control"
    },
    {
        "Question_title":"AWS S3 and Sagemaker: No such file or directory",
        "Question_body":"<p>I have created an S3 bucket 'testshivaproject' and uploaded an image in it. When I try to access it in sagemaker notebook, it throws an error 'No such file or directory'.<\/p>\n\n<pre><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                   \n\n# Define IAM role\nrole = get_execution_role()\n\nmy_region = boto3.session.Session().region_name # set the region of the instance\n\nprint(\"success :\"+my_region)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> success :us-east-2<\/p>\n\n<pre><code>role\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> 'arn:aws:iam::847047967498:role\/service-role\/AmazonSageMaker-ExecutionRole-20190825T121483'<\/p>\n\n<pre><code>bucket = 'testprojectshiva2' \ndata_key = 'ext_image6.jpg' \ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key) \nprint(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> s3:\/\/testprojectshiva2\/ext_image6.jpg<\/p>\n\n<pre><code>test = load_img(data_location)\n<\/code><\/pre>\n\n<p><strong>Output:<\/strong> No such file or directory<\/p>\n\n<p>There are similar questions raised (<a href=\"https:\/\/stackoverflow.com\/questions\/48264656\/load-s3-data-into-aws-sagemaker-notebook\">Load S3 Data into AWS SageMaker Notebook<\/a>) but did not find any solution?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1566834582570,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":4622.0,
        "Poster_id":5355429.0,
        "Poster_created_time":1442731325232,
        "Poster_last_access_time":1649309665536,
        "Poster_age":null,
        "Poster_location":"Hyderabad",
        "Poster_reputation":187.0,
        "Poster_views":25.0,
        "Answer_body":"<p>Thanks for using Amazon SageMaker!<\/p>\n\n<p>I sort of guessed from your description, but are you trying to use the Keras load_img function to load images directly from your S3 bucket?<\/p>\n\n<p>Unfortunately, <a href=\"https:\/\/github.com\/keras-team\/keras\/issues\/11684\" rel=\"nofollow noreferrer\">the load_img function is designed to only load files from disk<\/a>, so passing an s3:\/\/ URL to that function will always return a <code>FileNotFoundError<\/code>.<\/p>\n\n<p>It's common to first download images from S3 before using them, so you can use boto3 or the AWS CLI to download the file before calling load_img.<\/p>\n\n<p><strong>Alternatively<\/strong>, since the load_img function simply creates a <a href=\"https:\/\/en.wikipedia.org\/wiki\/Python_Imaging_Library\" rel=\"nofollow noreferrer\">PIL Image<\/a> object, you can create the PIL object directly from the data in S3 using boto3, and not use the load_img function at all.<\/p>\n\n<p>In other words, you could do something like this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from PIL import Image\n\ns3 = boto3.client('s3')\ntest = Image.open(BytesIO(\n    s3.get_object(Bucket=bucket, Key=data_key)['Body'].read()\n    ))\n<\/code><\/pre>\n\n<p>Hope this helps you out in your project!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":977443.0,
        "Answerer_created_time":1317676233236,
        "Answerer_last_access_time":1650916041003,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, United States",
        "Answerer_reputation":2348.0,
        "Answerer_views":206.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57661142",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1567829268830,
        "Question_original_content":"aw file directori creat bucket testshivaproject upload imag try access notebook throw error file directori import librari import boto sy math json urllib request import execut role import numpi defin iam role role execut role region boto session session region set region instanc print success region output success east role output arn aw iam role servic role amazon executionrol bucket testprojectshiva data kei ext imag jpg data locat format bucket data kei print data locat output testprojectshiva ext imag jpg test load img data locat output file directori similar question rais load data notebook solut",
        "Question_preprocessed_content":"aw file directori creat bucket testshivaproject upload imag try access notebook throw error file directori output success output output output file directori similar question rais solut",
        "Question_gpt_summary_original":"The user encountered a challenge when trying to access an image file uploaded in an S3 bucket named 'testshivaproject' from a Sagemaker notebook. The notebook threw an error message \"No such file or directory\" when the user tried to load the image file. The user has searched for solutions to similar questions but has not found any solution yet.",
        "Question_gpt_summary":"user encount challeng try access imag file upload bucket name testshivaproject notebook notebook threw error messag file directori user tri load imag file user search solut similar question solut",
        "Answer_original_content":"thank sort guess descript try us kera load img function load imag directli bucket unfortun load img function design load file disk pass url function return filenotfounderror common download imag us boto aw cli download file call load img altern load img function simpli creat pil imag object creat pil object directli data boto us load img function word like pil import imag boto client test imag open bytesio object bucket bucket kei data kei bodi read hope help project",
        "Answer_preprocessed_content":"thank sort guess descript try us kera function load imag directli bucket unfortun function design load file disk pass url function return common download imag us boto aw cli download file call altern function simpli creat pil imag object creat pil object directli data boto us function word like hope help project",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Download the image file from S3 using boto3 or AWS CLI before calling the load_img function.\n2. Create a PIL Image object directly from the data in S3 using boto3 and not use the load_img function at all.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss download imag file boto aw cli call load img function creat pil imag object directli data boto us load img function person opinion bias includ summari"
    },
    {
        "Question_title":"How to upgrade the sklearn library in sagemaker",
        "Question_body":"<p>I noticed my Sagemaker (Amazon aws) jupyter notebook has an outdated version of the sklearn library.<\/p>\n<p>when I run <code>! pip freeze<\/code> I get:<\/p>\n<pre><code>sklearn==0.0\n<\/code><\/pre>\n<p>and when I run (with python) <code>print(sklearn.__version__)<\/code> I get<\/p>\n<pre><code>0.24.1\n<\/code><\/pre>\n<p>I'm not sure which one is my real version but I need 1.0.0 in order to use the <code>from_predictions()<\/code> method.<\/p>\n<p>But when I am trying to run <code>! \/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/bin\/python -m pip install --upgrade sklearn<\/code> I am getting the following output:<\/p>\n<blockquote>\n<p>Requirement already satisfied: sklearn in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(0.0) Requirement already satisfied: scikit-learn in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from sklearn) (0.24.1) Requirement already satisfied: scipy&gt;=0.19.1\nin\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.5.3) Requirement already satisfied:\njoblib&gt;=0.11 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.0.1) Requirement already satisfied:\nthreadpoolctl&gt;=2.0.0 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (2.1.0) Requirement already satisfied:\nnumpy&gt;=1.13.3 in\n\/home\/ec2-user\/anaconda3\/envs\/amazonei_mxnet_p36\/lib\/python3.6\/site-packages\n(from scikit-learn-&gt;sklearn) (1.19.5)<\/p>\n<\/blockquote>\n<p>This is a very pupular library so it's weird if sagemaker cant upgrade it. Anyone has an idea what am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1637426688750,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1637429547623,
        "Question_score":2,
        "Question_view_count":634.0,
        "Poster_id":2867356.0,
        "Poster_created_time":1381413304940,
        "Poster_last_access_time":1663945567600,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":593.0,
        "Poster_views":94.0,
        "Answer_body":"<p>I managed to update sklearn to version 0.24.2 via the following command:<\/p>\n<pre><code>!conda update scikit-learn --yes\n<\/code><\/pre>\n<p>To further update it, you probably have to also update Python, which is version 3.6 in the current conda_python3 kernel on Sagemaker.<\/p>\n<p>It also looks promising to create your custom conda environment, as explained here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":8971938.0,
        "Answerer_created_time":1511190340208,
        "Answerer_last_access_time":1663335968476,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":385.0,
        "Answerer_views":39.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70047920",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1641036273827,
        "Question_original_content":"upgrad sklearn librari notic amazon aw jupyt notebook outdat version sklearn librari run pip freez sklearn run python print sklearn version sure real version need order us predict method try run home user anaconda env amazonei mxnet bin python pip instal upgrad sklearn get follow output requir satisfi sklearn home user anaconda env amazonei mxnet lib python site packag requir satisfi scikit learn home user anaconda env amazonei mxnet lib python site packag sklearn requir satisfi scipi home user anaconda env amazonei mxnet lib python site packag scikit learn sklearn requir satisfi joblib home user anaconda env amazonei mxnet lib python site packag scikit learn sklearn requir satisfi threadpoolctl home user anaconda env amazonei mxnet lib python site packag scikit learn sklearn requir satisfi numpi home user anaconda env amazonei mxnet lib python site packag scikit learn sklearn pupular librari weird upgrad idea wrong",
        "Question_preprocessed_content":"upgrad sklearn librari notic jupyt notebook outdat version sklearn librari run run sure real version need order us method try run get follow output requir satisfi sklearn requir satisfi sklearn requir satisfi requir satisfi requir satisfi requir satisfi pupular librari weird upgrad idea wrong",
        "Question_gpt_summary_original":"The user is facing a challenge in upgrading the sklearn library in their Sagemaker Jupyter notebook. They have an outdated version of the library and need version 1.0.0 to use a specific method. However, when they try to upgrade using pip, they receive a message indicating that the requirement is already satisfied. The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user face challeng upgrad sklearn librari jupyt notebook outdat version librari need version us specif method try upgrad pip receiv messag indic requir satisfi user seek assist resolv issu",
        "Answer_original_content":"manag updat sklearn version follow command conda updat scikit learn ye updat probabl updat python version current conda python kernel look promis creat custom conda environ explain http doc aw amazon com latest nbi add extern html",
        "Answer_preprocessed_content":"manag updat sklearn version follow command updat probabl updat python version current kernel look promis creat custom conda environ explain",
        "Answer_gpt_summary_original":"Solutions provided:\n- Use the command \"!conda update scikit-learn --yes\" to update sklearn to version 0.24.2.\n- To further update sklearn, update Python as well, which is version 3.6 in the current conda_python3 kernel on Sagemaker.\n- Create a custom conda environment as explained in the documentation provided in the link: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid us command conda updat scikit learn ye updat sklearn version updat sklearn updat python version current conda python kernel creat custom conda environ explain document provid link http doc aw amazon com latest nbi add extern html person opinion bias includ respons"
    },
    {
        "Question_title":"Anaconda commercial use on Azure Data Science Virtual Machine",
        "Question_body":"I would like to know if there is any problem in terms of license if enterprise companies use Anaconda that is preinstalled in Azure Data Science Virtual Machine. In another inquiry, I saw an answer that Anaconda included in Azure Machine Learning service has no problem in terms of the license but I would like to confirm whether DSVM also has a problem or not. https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/165312\/anaconda-commercial-use-on-azure-machine-learning.html",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1614757334137,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@kenta-takahashi The thread referenced by a user was in a different context who wanted to check if they had to subscribe to commercial license to use Azure ML. In the case of DSVM where anaconda packages are installed they are still configured to use open source packages irrespective of the subscription that spins them up. So, you can definitely use the DSVM for your purposes and configure any license's that were acquired to enhance your usage experience with the tools that have been pre-installed. Thanks!!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/296502\/anaconda-commercial-use-on-azure-data-science-virt.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1614766234916,
        "Question_original_content":"anaconda commerci us azur data scienc virtual machin like know problem term licens enterpris compani us anaconda preinstal azur data scienc virtual machin inquiri saw answer anaconda includ servic problem term licens like confirm dsvm problem http doc microsoft com answer question anaconda commerci us azur machin learn html",
        "Question_preprocessed_content":"anaconda commerci us azur data scienc virtual machin like know problem term licens enterpris compani us anaconda preinstal azur data scienc virtual machin inquiri saw answer anaconda includ servic problem term licens like confirm dsvm problem",
        "Question_gpt_summary_original":"The user is seeking clarification on whether there are any licensing issues for enterprise companies using Anaconda, which is preinstalled in Azure Data Science Virtual Machine. They have seen an answer stating that Anaconda included in Azure Machine Learning service has no licensing issues, but they want to confirm if the same applies to DSVM.",
        "Question_gpt_summary":"user seek clarif licens issu enterpris compani anaconda preinstal azur data scienc virtual machin seen answer state anaconda includ servic licens issu want confirm appli dsvm",
        "Answer_original_content":"kenta takahashi thread referenc user differ context want check subscrib commerci licens us case dsvm anaconda packag instal configur us open sourc packag irrespect subscript spin definit us dsvm purpos configur licens acquir enhanc usag experi tool pre instal thank",
        "Answer_preprocessed_content":"thread referenc user differ context want check subscrib commerci licens us case dsvm anaconda packag instal configur us open sourc packag irrespect subscript spin definit us dsvm purpos configur licens acquir enhanc usag experi tool thank",
        "Answer_gpt_summary_original":"Solution: The discussion confirms that there are no licensing issues for enterprise companies using Anaconda preinstalled in Azure Data Science Virtual Machine. The Anaconda packages are configured to use open source packages, and any licenses acquired can be used to enhance the usage experience with the pre-installed tools.",
        "Answer_gpt_summary":"solut discuss confirm licens issu enterpris compani anaconda preinstal azur data scienc virtual machin anaconda packag configur us open sourc packag licens acquir enhanc usag experi pre instal tool"
    },
    {
        "Question_title":"Hi. I am very new to MLFlow, and want to implement MLFlow project on my own ML model. However I am getting \"\"Could not find main among entry points\"\"",
        "Question_body":"<p>The full error message is as below:<\/p>\n<pre><code>ERROR mlflow.cli: === Could not find main among entry points [] or interpret main as a runnable script. Supported script file extensions: ['.py', '.sh'] ===\n\n<\/code><\/pre>\n<p>I also try the solutions suggested here <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code>, but the result is the same.<\/p>\n<p>Below I provide all the required files to run <code>MLflow<\/code> project.<\/p>\n<p>The <code>conda.yaml<\/code> file<\/p>\n<pre><code>name: lightgbm-example\nchannels:\n  - conda-forge\ndependencies:\n  - python=3.6\n  - pip\n  - pip:\n      - mlflow&gt;=1.6.0\n      - lightgbm\n      - pandas\n      - numpy\n<\/code><\/pre>\n<p>The MLProject file<\/p>\n<pre><code>name: lightgbm-example\nconda_env: ~\/Desktop\/MLflow\/conda.yaml\nentry-points:\n    main:\n      parameters:\n        learning_rate: {type: float, default: 0.1}\n        colsample_bytree: {type: float, default: 1.0}\n        subsample: {type: float, default: 1.0} \n      command: |\n          python3 ~\/Desktop\/MLflow\/Test.py \\\n            --learning-rate={learning_rate} \\\n            --colsample-bytree={colsample_bytree} \\\n            --subsample={subsample}\n<\/code><\/pre>\n<p>My Test.py file<\/p>\n<pre><code>import pandas as pd\nimport lightgbm as lgb\nimport numpy as np\nimport mlflow\nimport mlflow.lightgbm\nimport argparse\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n\ndef parse_args():\n    parser = argparse.ArgumentParser(description=&quot;LightGBM example&quot;)\n    parser.add_argument(\n        &quot;--learning-rate&quot;,\n        type=float,\n        default=0.1,\n        help=&quot;learning rate to update step size at each boosting step (default: 0.3)&quot;,\n    )\n    parser.add_argument(\n        &quot;--colsample-bytree&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of columns when constructing each tree (default: 1.0)&quot;,\n    )\n    parser.add_argument(\n        &quot;--subsample&quot;,\n        type=float,\n        default=1.0,\n        help=&quot;subsample ratio of the training instances (default: 1.0)&quot;,\n    )\n    return parser.parse_args()\n\ndef find_specificity(c_matrix):\n    specificity = c_matrix[1][1]\/(c_matrix[1][1]+c_matrix[0][1])\n    return specificity\n    \n    \ndef main():\n\n    args = parse_args()\n\n    df = pd.read_csv('~\/Desktop\/MLflow\/Churn_demo.csv')\n    train_df = df.sample(frac=0.8, random_state=25)\n    test_df = df.drop(train_df.index)\n\n\n        \n    train_df.drop(['subscriberid'], axis = 1, inplace = True)\n    test_df.drop(['subscriberid'], axis = 1, inplace = True)\n\n    TrainX = train_df.iloc[:,:-1]\n    TrainY = train_df.iloc[:,-1]\n\n    TestX = test_df.iloc[:,:-1]\n    TestY = test_df.iloc[:,-1]\n    \n    mlflow.lightgbm.autolog()\n    \n    dtrain = lgb.Dataset(TrainX, label=TrainY)\n    dtest = lgb.Dataset(TestX, label=TestY)\n    \n    with mlflow.start_run():\n\n        parameters = {\n            'objective': 'binary',\n            'device':'cpu',\n            'num_threads': 6,\n            'num_leaves': 127,\n            'metric' : 'binary',\n            'lambda_l2':5,\n            'max_bin': 63,\n            'bin_construct_sample_cnt' :2*1000*1000,\n            'learning_rate': args.learning_rate,\n            'colsample_bytree': args.colsample_bytree,\n            'subsample': args.subsample,\n            'verbose': 1\n        }\n\n\n\n        model = lgb.train(parameters,\n                       dtrain,\n                       valid_sets=dtest,\n                       num_boost_round=10000,\n                       early_stopping_rounds=10)\n                       \n               \n        y_proba=model.predict(TestX)\n        pred=np.where(y_proba&gt;0.25,1,0) \n        conf_matrix = confusion_matrix(TestY,pred)\n        \n        specificity = find_specificity(conf_matrix)\n        acc = accuracy_score(TestY,pred)\n        \n        mlflow.log_metric({&quot;specificity&quot; : specificity, &quot;accuracy&quot; : acc})\n\n\nif __name__ == &quot;__main__&quot;:\n    main()\n        \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1633602279323,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":418.0,
        "Poster_id":13018302.0,
        "Poster_created_time":1583491811220,
        "Poster_last_access_time":1663774319043,
        "Poster_age":null,
        "Poster_location":"Baku, Azerbaijan",
        "Poster_reputation":23.0,
        "Poster_views":4.0,
        "Answer_body":"<p>Fortunately, I have been resolved my problem. I list some solutions for the same error which can help you in the future if you face the same problem.<\/p>\n<ol>\n<li>File names. The file names should be the same suggested in MLFlow docs <code>https:\/\/mlflow.org\/ <\/code>. For example not <code>conda.yamp<\/code>, but <code>conda.yaml<\/code>, as there was such problem in <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/3856<\/code><\/li>\n<li>The <code>conda.yaml<\/code> file does not support Tab, please consider using spaces instead<\/li>\n<li>In the MLProject file name 'P' should be the upper case before MLFlow 1.4. But the later versions it does not matter as explained there <code>https:\/\/github.com\/mlflow\/mlflow\/issues\/1094<\/code><\/li>\n<li>(In my case) MLProject file is space sensitive. Let the <code> https:\/\/github.com\/mlflow\/mlflow\/tree\/master\/examples<\/code> GitHub examples guide you.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":13018302.0,
        "Answerer_created_time":1583491811220,
        "Answerer_last_access_time":1663774319043,
        "Answerer_age":null,
        "Answerer_location":"Baku, Azerbaijan",
        "Answerer_reputation":23.0,
        "Answerer_views":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69479488",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1633946464143,
        "Question_original_content":"new want implement project model get main entri point error messag error cli main entri point interpret main runnabl script support script file extens try solut suggest http github com issu result provid requir file run project conda yaml file lightgbm exampl channel conda forg depend python pip pip lightgbm panda numpi mlproject file lightgbm exampl conda env desktop conda yaml entri point main paramet learn rate type float default colsampl bytre type float default subsampl type float default command python desktop test learn rate learn rate colsampl bytre colsampl bytre subsampl subsampl test file import panda import lightgbm lgb import numpi import import lightgbm import argpars sklearn metric import accuraci score confus matrix def pars arg parser argpars argumentpars descript lightgbm exampl parser add argument learn rate type float default help learn rate updat step size boost step default parser add argument colsampl bytre type float default help subsampl ratio column construct tree default parser add argument subsampl type float default help subsampl ratio train instanc default return parser pars arg def specif matrix specif matrix matrix matrix return specif def main arg pars arg read csv desktop churn demo csv train sampl frac random state test drop train index train drop subscriberid axi inplac true test drop subscriberid axi inplac true trainx train iloc traini train iloc testx test iloc testi test iloc lightgbm autolog dtrain lgb dataset trainx label traini dtest lgb dataset testx label testi start run paramet object binari devic cpu num thread num leav metric binari lambda max bin bin construct sampl cnt learn rate arg learn rate colsampl bytre arg colsampl bytre subsampl arg subsampl verbos model lgb train paramet dtrain valid set dtest num boost round earli stop round proba model predict testx pred proba conf matrix confus matrix testi pred specif specif conf matrix acc accuraci score testi pred log metric specif specif accuraci acc main main",
        "Question_preprocessed_content":"new want implement project model get main entri point error messag try solut suggest result provid requir file run project file mlproject file file",
        "Question_gpt_summary_original":"The user is encountering an error message \"Could not find main among entry points\" while trying to implement an MLFlow project on their own ML model. They have provided all the required files including the conda.yaml file, MLProject file, and Test.py file. The user has also tried the solutions suggested in a GitHub issue but the error persists.",
        "Question_gpt_summary":"user encount error messag main entri point try implement project model provid requir file includ conda yaml file mlproject file test file user tri solut suggest github issu error persist",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"incremental training on custom code in amazon sagemaker",
        "Question_body":"<p>I'm moving my first steps in <code>amazon sagemaker<\/code>. I'm using script mode to train a classification algorithm. Training is fine, however I'm not able to do incremental training. I want to train again the same model with new data. Here what I did. This is my script:<\/p>\n\n<pre><code>import sagemaker\nfrom sagemaker.tensorflow import TensorFlow\nfrom sagemaker import get_execution_role\n\nbucket = 'sagemaker-blablabla'\ntrain_data = 's3:\/\/{}\/{}'.format(bucket,'train')\nvalidation_data = 's3:\/\/{}\/{}'.format(bucket,'test')\n\ns3_output_location = 's3:\/\/{}'.format(bucket)\n\ntf_estimator = TensorFlow(entry_point='main.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          framework_version='1.12', \n                          py_version='py3',\n                          output_path=s3_output_location)\n\ninputs = {'train': train_data, 'test': validation_data}\ntf_estimator.fit(inputs)\n<\/code><\/pre>\n\n<p>The entry point is my custom keras code, which I adapted to receive arguments from the script.\nNow the training is successfully completed and I have in my s3 bucket the model.tar.gz. I want to train again, but it's not clear to me how to do it.. I tried this<\/p>\n\n<pre><code>trained_model = 's3:\/\/sagemaker-blablabla\/sagemaker-tensorflow-scriptmode-2019-11-27-12-01-42-300\/output\/model.tar.gz'\n\ntf_estimator = sagemaker.estimator.Estimator(image_name='blablabla-west-1.amazonaws.com\/sagemaker-tensorflow-scriptmode:1.12-gpu-py3', \n                                              role=get_execution_role(),\n                                              train_instance_count=1, \n                                              train_instance_type='ml.p2.xlarge',\n                                              output_path=s3_output_location,\n                                              model_uri = trained_model)\n\ninputs = {'train': train_data, 'test': validation_data}\n\ntf_estimator.fit(inputs)\n<\/code><\/pre>\n\n<p>Doesn't work. Firstly, I don't know how to retrieve the training image name (for this I looked for it in the <code>aws<\/code> console, but I guess there should be a smarter solution), second this code throws an exception about the entry point but it is my understanding that I shouldn't need it when I do incremental learning with a ready image.\nI'm surely missing something important, any help? Thank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1574941643827,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1574944370140,
        "Question_score":1,
        "Question_view_count":621.0,
        "Poster_id":4267439.0,
        "Poster_created_time":1416346350292,
        "Poster_last_access_time":1664039219436,
        "Poster_age":null,
        "Poster_location":"Jesi, Italy",
        "Poster_reputation":2302.0,
        "Poster_views":227.0,
        "Answer_body":"<p>Incremental training is a native feature for the built-in <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/now-easily-perform-incremental-learning-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">Image Classifier and Object Detector<\/a>. For custom code, it is the developer responsibility to write the incremental training logic and to verify its validity. Here is a possible path:<\/p>\n\n<ol>\n<li>use one of the data channels passed in the <code>fit<\/code> to load a model state (artifact to fine-tune)<\/li>\n<li>in your code, check if the model state channel is filled\nwith artifacts. If it is, instantiate a model from that state\nand continue training. This is framework specific and you may to take\nnecessary precautions to avoid forgetting previous learnings.<\/li>\n<\/ol>\n\n<p>Some frameworks provide better support for incremental learning that others. For example some sklearn models provide an <a href=\"https:\/\/scikit-learn.org\/0.15\/modules\/scaling_strategies.html#incremental-learning\" rel=\"nofollow noreferrer\">incremental_fit<\/a> method. For DL frameworks it is technically very easy to continue training from a checkpoint, but if new data is very different from previously-seen data this may lead your model to forget previous learnings.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":5331834.0,
        "Answerer_created_time":1442180190107,
        "Answerer_last_access_time":1664048608876,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59088199",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1574981388936,
        "Question_original_content":"increment train custom code move step script mode train classif algorithm train fine abl increment train want train model new data script import tensorflow import tensorflow import execut role bucket blablabla train data format bucket train valid data format bucket test output locat format bucket estim tensorflow entri point main role execut role train instanc count train instanc type xlarg framework version version output path output locat input train train data test valid data estim fit input entri point custom kera code adapt receiv argument script train successfulli complet bucket model tar want train clear tri train model blablabla tensorflow scriptmod output model tar estim estim estim imag blablabla west amazonaw com tensorflow scriptmod gpu role execut role train instanc count train instanc type xlarg output path output locat model uri train model input train train data test valid data estim fit input work firstli know retriev train imag look aw consol guess smarter solut second code throw except entri point understand shouldn need increment learn readi imag sure miss import help thank",
        "Question_preprocessed_content":"increment train custom code move step script mode train classif algorithm train fine abl increment train want train model new data script entri point custom kera code adapt receiv argument script train successfulli complet bucket want train clear tri work firstli know retriev train imag second code throw except entri point understand shouldn need increment learn readi imag sure miss import help thank",
        "Question_gpt_summary_original":"The user is facing challenges with incremental training on custom code in Amazon SageMaker. They have successfully trained a classification algorithm using script mode but are unable to train the same model with new data. The user attempted to use the trained model and a new estimator but encountered an exception about the entry point. They are seeking guidance on how to retrieve the training image name and how to perform incremental learning with a ready image.",
        "Question_gpt_summary":"user face challeng increment train custom code successfulli train classif algorithm script mode unabl train model new data user attempt us train model new estim encount except entri point seek guidanc retriev train imag perform increment learn readi imag",
        "Answer_original_content":"increment train nativ featur built imag classifi object detector custom code develop respons write increment train logic verifi valid possibl path us data channel pass fit load model state artifact fine tune code check model state channel fill artifact instanti model state continu train framework specif necessari precaut avoid forget previou learn framework provid better support increment learn exampl sklearn model provid increment fit method framework technic easi continu train checkpoint new data differ previous seen data lead model forget previou learn",
        "Answer_preprocessed_content":"increment train nativ featur imag classifi object detector custom code develop respons write increment train logic verifi valid possibl path us data channel pass load model state code check model state channel fill artifact instanti model state continu train framework specif necessari precaut avoid forget previou learn framework provid better support increment learn exampl sklearn model provid method framework technic easi continu train checkpoint new data differ data lead model forget previou learn",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include:\n\n- The developer needs to write the incremental training logic and verify its validity for custom code.\n- To perform incremental learning, the developer can use one of the data channels passed in the `fit` to load a model state and check if the model state channel is filled with artifacts. If it is, instantiate a model from that state and continue training.\n- Some frameworks provide better support for incremental learning than others. For example, some sklearn models provide an `incremental_fit` method. For DL frameworks, it is technically easy to continue training from a checkpoint, but if new data is very different from previously-seen data, this may lead the model to forget previous learnings.",
        "Answer_gpt_summary":"possibl solut mention discuss includ develop need write increment train logic verifi valid custom code perform increment learn develop us data channel pass fit load model state check model state channel fill artifact instanti model state continu train framework provid better support increment learn exampl sklearn model provid increment fit method framework technic easi continu train checkpoint new data differ previous seen data lead model forget previou learn"
    },
    {
        "Question_title":"Comment s\u00e9lectionner Standard_DS11_v2",
        "Question_body":"Bonjour\nJe suis le cours en ligne concernant l'impl\u00e9mentation d'algorithmes de machine learning\nA l'\u00e9tape Create compute resources\nhttps:\/\/docs.microsoft.com\/en-us\/learn\/modules\/use-automated-machine-learning\/create-compute\n\nOn me demande Search for and select Standard_DS11_v2\n\nHors, l'interface me dit que je n'ai pas les quotas disponibles.\nJ'utilise l'offre d'essai \u00e0 200 USD.\nComment faire pour que cela fonctionne ?\nCordialement\nThibaut",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_created_time":1638368598000,
        "Question_favorite_count":12.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@ThibautJacquin-3972 For a free account only 200$ credit is available and not all compute can be created or selected because of this limitation. You can choose a lower priced VM and proceed with the creation of compute or upgrade to a pay-as-you-go account for your subscription and select the required compute type. I hope this helps.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/647767\/comment-selectionner-standard-ds11-v2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1638432598510,
        "Question_original_content":"comment slectionn standard bonjour sui cour lign concern implment algorithm machin learn tape creat comput resourc http doc microsoft com learn modul us autom machin learn creat comput demand search select standard hor interfac dit que pa le quota dispon utilis offr essai usd comment fair pour que cela fonctionn cordial thibaut",
        "Question_preprocessed_content":"comment slectionn bonjour sui cour lign concern implment algorithm machin learn tape creat comput resourc demand search select hor interfac dit que pa le quota dispon utilis offr essai usd comment fair pour que cela fonctionn cordial thibaut",
        "Question_gpt_summary_original":"The user is facing challenges in selecting Standard_DS11_v2 while creating compute resources for an online course on machine learning implementation. The interface is indicating that the user does not have the required quotas available, despite using the $200 trial offer. The user is seeking guidance on how to resolve this issue.",
        "Question_gpt_summary":"user face challeng select standard creat comput resourc onlin cours machin learn implement interfac indic user requir quota avail despit trial offer user seek guidanc resolv issu",
        "Answer_original_content":"thibautjacquin free account credit avail comput creat select limit choos lower price proce creation comput upgrad pai account subscript select requir comput type hope help answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"free account credit avail comput creat select limit choos lower price proce creation comput upgrad account subscript select requir comput type hope help answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Choose a lower priced VM and proceed with the creation of compute.\n2. Upgrade to a pay-as-you-go account for the subscription and select the required compute type.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss choos lower price proce creation comput upgrad pai account subscript select requir comput type person opinion bias includ summari"
    },
    {
        "Question_title":"Treatment of Existing Models in a Stage When Transitioning Models",
        "Question_body":"<p>With the MLflow client library, it is possible to transition models through stages as shown below,<\/p>\n<pre><code>client = MlflowClient()\nclient.transition_model_version_stage(\n    name=&quot;sk-learn-random-forest-reg-model&quot;,\n    version=3,\n    stage=&quot;Production&quot;\n)\n<\/code><\/pre>\n<p>Upon doing some testing, I noticed that this does not in any way affect the model(s) that are currently in the stage that the model was transitioned to.<\/p>\n<p>For example, let's say version 2 of a given model is in Production. This will remain tagged as a Production model, even if I were to move version 3 to Production as well.<\/p>\n<p>Is there any way that I can control what happens to models that exist in a stage when making transitions using the above code?<\/p>\n<p>Basically, I only want one version of a model to be in a given stage at a time.<\/p>\n<p>This functionality is available when transitioning models through the Databricks UI,\n<a href=\"https:\/\/i.stack.imgur.com\/aycvC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/aycvC.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1656383133240,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":26.0,
        "Poster_id":9542989.0,
        "Poster_created_time":1521856385820,
        "Poster_last_access_time":1664037995903,
        "Poster_age":null,
        "Poster_location":"Sri Lanka",
        "Poster_reputation":820.0,
        "Poster_views":165.0,
        "Answer_body":"<p>I have missed the <code>archive_existing_versions=True<\/code> that comes with the <code>transition_model_version_stage<\/code> function.<\/p>\n<p>This flag defaults to <code>False<\/code>.<\/p>\n<p>The documentation is available here,\n<br>\n<a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.transition_model_version_stage\" rel=\"nofollow noreferrer\">https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.tracking.html#mlflow.tracking.MlflowClient.transition_model_version_stage<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":9542989.0,
        "Answerer_created_time":1521856385820,
        "Answerer_last_access_time":1664037995903,
        "Answerer_age":null,
        "Answerer_location":"Sri Lanka",
        "Answerer_reputation":820.0,
        "Answerer_views":165.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72780102",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1656613905027,
        "Question_original_content":"treatment exist model stage transit model client librari possibl transit model stage shown client client client transit model version stage learn random forest reg model version stage product test notic wai affect model current stage model transit exampl let version given model product remain tag product model version product wai control happen model exist stage make transit code basic want version model given stage time function avail transit model databrick",
        "Question_preprocessed_content":"treatment exist model stage transit model client librari possibl transit model stage shown test notic wai affect model current stage model transit exampl let version given model product remain tag product model version product wai control happen model exist stage make transit code basic want version model given stage time function avail transit model databrick",
        "Question_gpt_summary_original":"The user is facing a challenge with the MLflow client library when transitioning models through stages. They have noticed that transitioning a model to a stage does not affect the existing models in that stage. The user wants to know if there is a way to control what happens to models that exist in a stage when making transitions using the code provided. They want only one version of a model to be in a given stage at a time.",
        "Question_gpt_summary":"user face challeng client librari transit model stage notic transit model stage affect exist model stage user want know wai control happen model exist stage make transit code provid want version model given stage time",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error",
        "Question_body":"<p>I\u2019ve recently started working with azure for ML and am trying to use machine learning service workspace.\nI\u2019ve set up a workspace with the compute set to NC6s-V2 machines since I need train a NN using images on GPU. <\/p>\n\n<p>The issue is that the training still happens on the CPU \u2013 the logs say it\u2019s not able to find CUDA. Here\u2019s the warning log when running my script.\nAny clues how to solve this issue?<\/p>\n\n<p>I\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. <\/p>\n\n<p>Here's my code for the estimator,<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>script_params = {\n         '--input_data_folder': ds.path('dataset').as_mount(),\n         '--zip_file_name': 'train.zip',\n         '--run_mode': 'train'\n    }\n\n\nest = Estimator(source_directory='.\/scripts',\n                     script_params=script_params,\n                     compute_target=compute_target,\n                     entry_script='main.py',\n                     conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu']\n                     )\n\nrun = exp.submit(config=est)\n\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>The compute target was made as per the sample code on github:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>compute_name = \"P100-NC6s-V2\"\ncompute_min_nodes = 0\ncompute_max_nodes = 4\n\nvm_size = \"STANDARD_NC6S_V2\"\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout.\n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\n\n<\/code><\/pre>\n\n<p>This is the warning with which it fails to use the GPU:<\/p>\n\n<pre><code>2019-08-12 14:50:16.961247: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x55a7ce570830 executing computations on platform Host. Devices:\n2019-08-12 14:50:16.961278: I tensorflow\/compiler\/xla\/service\/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2019-08-12 14:50:16.971025: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/azureml-envs\/azureml_5fdf05c5671519f307e0f43128b8610e\/lib:\n2019-08-12 14:50:16.971054: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2019-08-12 14:50:16.971081: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971089: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971164: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n2019-08-12 14:50:16.971202: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\nDevice mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n2019-08-12 14:50:16.973301: I tensorflow\/core\/common_runtime\/direct_session.cc:296] Device mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n\n<\/code><\/pre>\n\n<p>It's currently using the CPU as per the logs. Any clues how to resolve the issue here?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1565670597500,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1565670701316,
        "Question_score":5,
        "Question_view_count":1402.0,
        "Poster_id":3946083.0,
        "Poster_created_time":1408145271463,
        "Poster_last_access_time":1620887445852,
        "Poster_age":null,
        "Poster_location":"India",
        "Poster_reputation":65.0,
        "Poster_views":4.0,
        "Answer_body":"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. <\/p>\n\n<p>See here for documentation:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.tensorflow?view=azure-ml-py\" rel=\"nofollow noreferrer\">API Reference<\/a> You can use <code>conda_packages<\/code> argument to specify additional libraries. Also set argument <code>use_gpu = True<\/code>.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-keras\/train-hyperparameter-tune-deploy-with-keras.ipynb\" rel=\"nofollow noreferrer\">Example Notebook<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":5784983.0,
        "Answerer_created_time":1452696930640,
        "Answerer_last_access_time":1603992113372,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":746.0,
        "Answerer_views":112.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57471129",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1565705412680,
        "Question_original_content":"unabl us gpu train model servic nc comput fail wth cuda error iv recent start work azur try us machin learn servic workspac iv set workspac comput set nc machin need train imag gpu issu train happen cpu log abl cuda here warn log run script clue solv issu iv mention explicitli tensorflow gpu packag conda packag option estim code estim script param input data folder path dataset mount zip file train zip run mode train est estim sourc directori script script param script param comput target comput target entri script main conda packag scikit imag kera tqdm pillow matplotlib scipi tensorflow gpu run exp submit config est run wait complet output true comput target sampl code github comput nc comput min node comput max node size standard nc comput comput target comput target comput target comput comput target type comput target amlcomput print comput target us comput print creat new comput target provis config amlcomput provis configur size size min node comput min node max node comput max node creat cluster comput target computetarget creat comput provis config poll minimum number node specif timeout min node count provid us scale set cluster comput target wait complet output true min node count timeout minut detail view current amlcomput statu us statu print comput target statu serial warn fail us gpu tensorflow compil xla servic servic xla servic xace execut comput platform host devic tensorflow compil xla servic servic streamexecutor devic tensorflow stream executor platform default dso loader dlopen librari libcuda dlerror libcuda open share object file file directori librari path opt intel compil librari linux mpi intel lib opt intel compil librari linux mpi mic lib opt intel compil librari linux mpi intel lib opt intel compil librari linux mpi mic lib env fdfcfefb lib tensorflow stream executor cuda cuda driver fail cuinit unknown error tensorflow stream executor cuda cuda diagnost retriev cuda diagnost inform host bddfbeedaaf tensorflow stream executor cuda cuda diagnost hostnam bddfbeedaaf tensorflow stream executor cuda cuda diagnost libcuda report version unabl libcuda dso load program tensorflow stream executor cuda cuda diagnost kernel report version devic map job localhost replica task devic xla cpu devic xla cpu devic tensorflow core common runtim direct session devic map job localhost replica task devic xla cpu devic xla cpu devic current cpu log clue resolv issu",
        "Question_preprocessed_content":"unabl us gpu train model servic comput fail wth cuda error iv recent start work azur try us machin learn servic workspac iv set workspac comput set machin need train imag gpu issu train happen cpu log abl cuda here warn log run script clue solv issu iv mention explicitli packag conda packag option estim code estim comput target sampl code github warn fail us gpu current cpu log clue resolv issu",
        "Question_gpt_summary_original":"The user is facing challenges in using GPU to train a neural network model in Azure Machine Learning Service using P100-NC6s-V2 compute. The training is happening on the CPU and the logs show that it's not able to find CUDA. The user has mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. The warning log shows that it fails to use the GPU and is currently using the CPU.",
        "Question_gpt_summary":"user face challeng gpu train neural network model servic nc comput train happen cpu log abl cuda user mention explicitli tensorflow gpu packag conda packag option estim warn log show fail us gpu current cpu",
        "Answer_original_content":"instead base estim us tensorflow estim kera librari layer wai worri set configur gpu librari tensorflow estim us docker imag gpu librari pre configur document api refer us conda packag argument specifi addit librari set argument us gpu true exampl notebook",
        "Answer_preprocessed_content":"instead base estim us tensorflow estim kera librari layer wai worri set configur gpu librari tensorflow estim us docker imag gpu librari document api refer us argument specifi addit librari set argument exampl notebook",
        "Answer_gpt_summary_original":"Solution: The user can use the Tensorflow Estimator with pre-configured GPU libraries instead of the base Estimator. They can specify additional libraries using the `conda_packages` argument and set `use_gpu=True`. Documentation and an example notebook are provided for reference.",
        "Answer_gpt_summary":"solut user us tensorflow estim pre configur gpu librari instead base estim specifi addit librari conda packag argument set us gpu true document exampl notebook provid refer"
    },
    {
        "Question_title":"Redshift ML \/ SageMaker - Deploy an existing model artifact to a Redshift Cluster",
        "Question_body":"Is it possible to deploy an existing model artifact from SageMaker to Redshift ML?\n\nFor example, with an Aurora ML you can reference a SageMaker endpoint and then use it as a UDF in a SELECT statement. Redshift ML works a bit differently - when you call CREATE MODEL - the model is trained with SageMaker Autopilot and then deployed to the Redshift Cluster.\n\nWhat if I already have a trained model, can i deploy it to a Redshift Cluster and then use a UDF for Inference?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1609954586000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":68.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"As of January 30 2021, you can't deploy an existing model artifact from SageMaker to Redshift ML directly with currently announced Redshift ML preview features. But you can reference sagemaker endpoint through a lambda function and use that lambda function as an user defined function in Redshift.\n\nBelow would be the steps:\n\nTrain and deploy your SageMaker model in a SageMaker Endpoint.\nUse Lambda function to reference sagemaker endpoint.\nCreate a Redshift Lambda UDF referring above lambda function to run predictions.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCMYCx28qRe-MOCIfj91Y2g\/redshift-ml-sage-maker-deploy-an-existing-model-artifact-to-a-redshift-cluster",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1609955532000,
        "Question_original_content":"redshift deploi exist model artifact redshift cluster possibl deploi exist model artifact redshift exampl aurora refer endpoint us udf select statement redshift work bit differ creat model model train autopilot deploi redshift cluster train model deploi redshift cluster us udf infer",
        "Question_preprocessed_content":"redshift deploi exist model artifact redshift cluster possibl deploi exist model artifact redshift exampl aurora refer endpoint us udf select statement redshift work bit differ creat model model train autopilot deploi redshift cluster train model deploi redshift cluster us udf infer",
        "Question_gpt_summary_original":"The user is facing a challenge of deploying an existing model artifact from SageMaker to Redshift ML. They are unsure if it is possible to deploy a trained model to a Redshift Cluster and use a UDF for inference. The user notes that Redshift ML works differently from Aurora ML, where a SageMaker endpoint can be referenced and used as a UDF in a SELECT statement.",
        "Question_gpt_summary":"user face challeng deploi exist model artifact redshift unsur possibl deploi train model redshift cluster us udf infer user note redshift work differ aurora endpoint referenc udf select statement",
        "Answer_original_content":"januari deploi exist model artifact redshift directli current announc redshift preview featur refer endpoint lambda function us lambda function user defin function redshift step train deploi model endpoint us lambda function refer endpoint creat redshift lambda udf refer lambda function run predict",
        "Answer_preprocessed_content":"januari deploi exist model artifact redshift directli current announc redshift preview featur refer endpoint lambda function us lambda function user defin function redshift step train deploi model endpoint us lambda function refer endpoint creat redshift lambda udf refer lambda function run predict",
        "Answer_gpt_summary_original":"Solution: As of January 30, 2021, it is not possible to deploy an existing model artifact from SageMaker to Redshift ML directly. However, a possible solution is to reference the SageMaker endpoint through a lambda function and use that lambda function as a user-defined function in Redshift. The steps involved in this solution are to train and deploy the SageMaker model in a SageMaker endpoint, use a lambda function to reference the SageMaker endpoint, and create a Redshift Lambda UDF referring to the above lambda function to run predictions.",
        "Answer_gpt_summary":"solut januari possibl deploi exist model artifact redshift directli possibl solut refer endpoint lambda function us lambda function user defin function redshift step involv solut train deploi model endpoint us lambda function refer endpoint creat redshift lambda udf refer lambda function run predict"
    },
    {
        "Question_title":"how to create a serverless endpoint in sagemaker?",
        "Question_body":"<p>I followed the aws documentation ( <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/serverless-endpoints-create.html#serverless-endpoints-create-config<\/a>) to create a model and to use that model, i coded for a serverless endpoint config (sample code below) ,I have all the required values  but this throws an error below and i'm not sure why<\/p>\n<p>parameter validation failed unknown parameter inProductVariants [ 0 ]: &quot;ServerlessConfig&quot;, must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...<\/p>\n<pre><code>response = client.create_endpoint_config(\n   EndpointConfigName=&quot;abc&quot;,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: &quot;foo&quot;,\n            &quot;VariantName&quot;: &quot;variant-1&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 1024,\n                &quot;MaxConcurrency&quot;: 2\n            }\n        } \n    ]\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1645067620433,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1645073711768,
        "Question_score":2,
        "Question_view_count":161.0,
        "Poster_id":13645093.0,
        "Poster_created_time":1590797441983,
        "Poster_last_access_time":1664049080543,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":525.0,
        "Poster_views":98.0,
        "Answer_body":"<p>You are probably using <strong>old boto3<\/strong> version. <code>ServerlessConfig<\/code> is a very new configuration option. You need to upgrade to the latest version (1.21.1) if possible.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":248823.0,
        "Answerer_created_time":1263294862568,
        "Answerer_last_access_time":1664084973947,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":183045.0,
        "Answerer_views":13691.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71152047",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1645069539320,
        "Question_original_content":"creat serverless endpoint follow aw document http doc aw amazon com latest serverless endpoint creat html serverless endpoint creat config creat model us model code serverless endpoint config sampl code requir valu throw error sure paramet valid fail unknown paramet inproductvari serverlessconfig vairantnam modelnam initialinstancecount instancetyp respons client creat endpoint config endpointconfignam abc productionvari modelnam foo variantnam variant serverlessconfig memorysizeinmb maxconcurr",
        "Question_preprocessed_content":"creat serverless endpoint follow aw document creat model us model code serverless endpoint config requir valu throw error sure paramet valid fail unknown paramet inproductvari serverlessconfig vairantnam modelnam initialinstancecount",
        "Question_gpt_summary_original":"The user encountered a challenge while trying to create a serverless endpoint in Sagemaker by following the AWS documentation. The error message \"parameter validation failed unknown parameter inProductVariants [ 0 ]: &quot;ServerlessConfig&quot;, must be one of : VairantName, ModelName, InitialInstanceCount , Instancetype...\" appeared while trying to execute the sample code provided in the documentation.",
        "Question_gpt_summary":"user encount challeng try creat serverless endpoint follow aw document error messag paramet valid fail unknown paramet inproductvari serverlessconfig vairantnam modelnam initialinstancecount instancetyp appear try execut sampl code provid document",
        "Answer_original_content":"probabl old boto version serverlessconfig new configur option need upgrad latest version possibl",
        "Answer_preprocessed_content":"probabl old boto version new configur option need upgrad latest version possibl",
        "Answer_gpt_summary_original":"Solution: The user needs to upgrade to the latest version (1.21.1) of boto3 to resolve the error message.",
        "Answer_gpt_summary":"solut user need upgrad latest version boto resolv error messag"
    },
    {
        "Question_title":"NVIDIA Triton vs TorchServe for SageMaker Inference",
        "Question_body":"<p><a href=\"https:\/\/developer.nvidia.com\/nvidia-triton-inference-server\" rel=\"nofollow noreferrer\">NVIDIA Triton<\/a>\u00a0vs\u00a0<a href=\"https:\/\/pytorch.org\/serve\/\" rel=\"nofollow noreferrer\">TorchServe<\/a>\u00a0for SageMaker inference? When to recommend each?<\/p>\n<p>Both are modern, production grade inference servers. TorchServe is the DLC default inference server for PyTorch models. Triton is also supported for PyTorch inference on SageMaker.<\/p>\n<p>Anyone has a good comparison matrix for both?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1663943338403,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":12.0,
        "Poster_id":3203213.0,
        "Poster_created_time":1389887039672,
        "Poster_last_access_time":1664076128463,
        "Poster_age":null,
        "Poster_location":"Singapore",
        "Poster_reputation":5854.0,
        "Poster_views":794.0,
        "Answer_body":"<p>Important notes to add here where both serving stacks differ:<\/p>\n<p>TorchServe does not provide the Instance Groups feature that Triton does (that is, stacking many copies of the same model or even different models onto the same GPU). This is a major advantage for both realtime and batch use-cases, as the performance increase is almost proportional to the model replication count (i.e. 2 copies of the model get you almost twice the throughput and half the latency; check out a BERT benchmark of this here). Hard to match a feature that is almost like having 2+ GPU's for the price of one.\nif you are deploying PyTorch DL models, odds are you often want to accelerate them with GPU's. TensorRT (TRT) is a compiler developed by NVIDIA that automatically quantizes and optimizes your model graph, which represents another huge speed up, depending on GPU architecture and model. It is understandably so probably the best way of automatically optimizing your model to run efficiently on GPU's and make good use of TensorCores. Triton has native integration to run TensorRT engines as they're called (even automatically converting your model to a TRT engine via config file), while TorchServe does not (even though you can use TRT engines with it).\nThere is more parity between both when it comes to other important serving features: both have dynamic batching support, you can define inference DAG's with both (not sure if the latter works with TorchServe on SageMaker without a big hassle), and both support custom code\/handlers instead of just being able to serve a model's forward function.<\/p>\n<p>Finally, MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new feature for cost-optimization.<\/p>\n<p>Bottom line I think that Triton is just as easy (if not easier) ot use, a lot more optimized\/integrated for taking full advantage of the underlying hardware (and will be updated to keep being that way as newer GPU architectures are released, enabling an easy move to them), and in general blows TorchServe out of the water performance-wise when its optimization features are used in combination.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":6411548.0,
        "Answerer_created_time":1464811778510,
        "Answerer_last_access_time":1663971018792,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":196.0,
        "Answerer_views":34.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73829280",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1663971240670,
        "Question_original_content":"nvidia triton torchserv infer nvidia tritonvstorchservefor infer recommend modern product grade infer server torchserv dlc default infer server pytorch model triton support pytorch infer good comparison matrix",
        "Question_preprocessed_content":"nvidia triton torchserv infer nvidia tritonvstorchservefor infer recommend modern product grade infer server torchserv dlc default infer server pytorch model triton support pytorch infer good comparison matrix",
        "Question_gpt_summary_original":"The user is seeking a comparison between NVIDIA Triton and TorchServe for SageMaker inference, as both are modern, production grade inference servers. The user is specifically looking for a comparison matrix to determine when to recommend each.",
        "Question_gpt_summary":"user seek comparison nvidia triton torchserv infer modern product grade infer server user specif look comparison matrix determin recommend",
        "Answer_original_content":"import note add serv stack differ torchserv provid instanc group featur triton stack copi model differ model gpu major advantag realtim batch us case perform increas proport model replic count copi model twice throughput half latenc check bert benchmark hard match featur like have gpu price deploi pytorch model odd want acceler gpu tensorrt trt compil develop nvidia automat quantiz optim model graph repres huge speed depend gpu architectur model understand probabl best wai automat optim model run effici gpu good us tensorcor triton nativ integr run tensorrt engin call automat convert model trt engin config file torchserv us trt engin pariti come import serv featur dynam batch support defin infer dag sure work torchserv big hassl support custom code handler instead abl serv model forward function final mme gpu come shortli base triton valid argument custom familiar quickli leverag new featur cost optim line think triton easi easier us lot optim integr take advantag underli hardwar updat wai newer gpu architectur releas enabl easi gener blow torchserv water perform wise optim featur combin",
        "Answer_preprocessed_content":"import note add serv stack differ torchserv provid instanc group featur triton major advantag realtim batch perform increas proport model replic count hard match featur like have gpu price deploi pytorch model odd want acceler gpu tensorrt compil develop nvidia automat quantiz optim model graph repres huge speed depend gpu architectur model understand probabl best wai automat optim model run effici gpu good us tensorcor triton nativ integr run tensorrt engin call torchserv pariti come import serv featur dynam batch support defin infer dag support custom instead abl serv model forward function final mme gpu base triton valid argument custom familiar quickli leverag new featur line think triton easi us lot take advantag underli hardwar gener blow torchserv water optim featur combin",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- NVIDIA Triton provides the Instance Groups feature that TorchServe does not, which is a major advantage for both real-time and batch use-cases.\n- Triton has native integration to run TensorRT engines, while TorchServe does not.\n- Both Triton and TorchServe have dynamic batching support and can define inference DAG's, but it is unclear if the latter works with TorchServe on SageMaker without a big hassle.\n- Both support custom code\/handlers instead of just being able to serve a model's forward function.\n- MME on GPU (coming shortly) will be based on Triton, which is a valid argument for customers to get familiar with it so that they can quickly leverage this new",
        "Answer_gpt_summary":"possibl solut mention discuss nvidia triton provid instanc group featur torchserv major advantag real time batch us case triton nativ integr run tensorrt engin torchserv triton torchserv dynam batch support defin infer dag unclear work torchserv big hassl support custom code handler instead abl serv model forward function mme gpu come shortli base triton valid argument custom familiar quickli leverag new"
    },
    {
        "Question_title":"How to find XGBoost containers for different regions in AWS Sagemaker",
        "Question_body":"<p>I'm trying to follow AWS Sagemaker tutorial to train a machine learning model with a Jupyter notebook environment. <\/p>\n\n<p>According to the tutorial, I'm supposed to copy the following code and run it to import required libraries and set environment variables. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># import libraries\nimport boto3, re, sys, math, json, os, sagemaker, urllib.request\nfrom sagemaker import get_execution_role\nimport numpy as np                                \nimport pandas as pd                               \nimport matplotlib.pyplot as plt                   \nfrom IPython.display import Image                 \nfrom IPython.display import display               \nfrom time import gmtime, strftime                 \nfrom sagemaker.predictor import csv_serializer   \n\n# Define IAM role\nrole = get_execution_role()\nprefix = 'sagemaker\/DEMO-xgboost-dm'\ncontainers = {'us-west-2': '433757028032.dkr.ecr.us-west-2.amazonaws.com\/xgboost:latest',\n              'us-east-1': '811284229777.dkr.ecr.us-east-1.amazonaws.com\/xgboost:latest',\n              'us-east-2': '825641698319.dkr.ecr.us-east-2.amazonaws.com\/xgboost:latest',\n              'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\nmy_region = boto3.session.Session().region_name # set the region of the instance\nprint(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")\n<\/code><\/pre>\n\n<p>And the expected outcome is below.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wLJ2j.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>However, I am getting this error.<\/p>\n\n<blockquote>\n  <p>KeyError                                  Traceback (most recent call last)\n   in ()\n       18               'eu-west-1': '685385470294.dkr.ecr.eu-west-1.amazonaws.com\/xgboost:latest'} # each region has its XGBoost container\n       19 my_region = boto3.session.Session().region_name # set the region of the instance\n  ---> 20 print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + containers[my_region] + \" container for your SageMaker endpoint.\")<\/p>\n  \n  <p>KeyError: 'ap-northeast-2'<\/p>\n<\/blockquote>\n\n<p>I assume that this is happening because my region is <strong>\"ap-northeast-2\"<\/strong>. \nI have a feeling that I need to change the containers for my region.  <\/p>\n\n<p><strong>If my guess is correct, how can I find containers for my region?<\/strong><br>\n<strong>Also, am I overlooking anything else?<\/strong> <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1576462664100,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":2197.0,
        "Poster_id":10504469.0,
        "Poster_created_time":1539556112483,
        "Poster_last_access_time":1664004360847,
        "Poster_age":null,
        "Poster_location":"Seoul, South Korea",
        "Poster_reputation":2954.0,
        "Poster_views":1143.0,
        "Answer_body":"<p>I expect your rational is correct. There isn't an entry for your region in the code. I don't know if there's a list of these containers per region. That being said, you find them in ECR (Elastic Container Registry). <\/p>\n\n<p>Keep in mind, that you can probably fix this quickly by switching to one of the supported regions. Otherwise:<\/p>\n\n<p>If AWS doesn't have a publicly listed container in your region, you can register the container yourself in AWS with ECR. You'll need to login to ECR using the AWS CLI and docker login.<\/p>\n\n<p>You can use the command <code>aws ecr get-login --region ap-northeast-2<\/code> in order to get the token you'll need for docker login.<\/p>\n\n<p>Then, clone this repo: <a href=\"https:\/\/github.com\/aws\/sagemaker-xgboost-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-xgboost-container<\/a><\/p>\n\n<p>You can build this image locally and push it up to ECR. After that, login to the AWS console (or use the AWS CLI) and find the ARN of the image. It should match the format of the others in your code. <\/p>\n\n<p>After that, just add another key\/value entry into the code for your <code>containers<\/code> variable and use <code>'ap-northeast-2': '&lt;ARN of the docker image&gt;'<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1576465909367,
        "Answer_score":1.0,
        "Answerer_id":5310188.0,
        "Answerer_created_time":1441651557140,
        "Answerer_last_access_time":1587328887043,
        "Answerer_age":null,
        "Answerer_location":"Parker, CO, USA",
        "Answerer_reputation":851.0,
        "Answerer_views":71.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59349805",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1576464384063,
        "Question_original_content":"xgboost contain differ region try follow tutori train machin learn model jupyt notebook environ accord tutori suppos copi follow code run import requir librari set environ variabl import librari import boto sy math json urllib request import execut role import numpi import panda import matplotlib pyplot plt ipython displai import imag ipython displai import displai time import gmtime strftime predictor import csv serial defin iam role role execut role prefix demo xgboost contain west dkr ecr west amazonaw com xgboost latest east dkr ecr east amazonaw com xgboost latest east dkr ecr east amazonaw com xgboost latest west dkr ecr west amazonaw com xgboost latest region xgboost contain region boto session session region set region instanc print success myinstanc region region us contain region contain endpoint expect outcom get error keyerror traceback recent west dkr ecr west amazonaw com xgboost latest region xgboost contain region boto session session region set region instanc print success myinstanc region region us contain region contain endpoint keyerror northeast assum happen region northeast feel need chang contain region guess correct contain region overlook",
        "Question_preprocessed_content":"xgboost contain differ region try follow tutori train machin learn model jupyt notebook environ accord tutori suppos copi follow code run import requir librari set environ variabl expect outcom get error keyerror traceback region xgboost contain set region instanc print keyerror assum happen region feel need chang contain region guess correct contain region overlook",
        "Question_gpt_summary_original":"The user is encountering an error while trying to follow an AWS Sagemaker tutorial to train a machine learning model with a Jupyter notebook environment. The error is caused by the absence of an XGBoost container for the user's region, which is \"ap-northeast-2\". The user needs to find the appropriate container for their region and update the code accordingly. The user is seeking guidance on how to find the containers for their region and if they are overlooking anything else.",
        "Question_gpt_summary":"user encount error try follow tutori train machin learn model jupyt notebook environ error caus absenc xgboost contain user region northeast user need appropri contain region updat code accordingli user seek guidanc contain region overlook",
        "Answer_original_content":"expect ration correct isn entri region code know list contain region said ecr elast contain registri mind probabl fix quickli switch support region aw publicli list contain region regist contain aw ecr need login ecr aw cli docker login us command aw ecr login region northeast order token need docker login clone repo http github com aw xgboost contain build imag local push ecr login aw consol us aw cli arn imag match format code add kei valu entri code contain variabl us northeast",
        "Answer_preprocessed_content":"expect ration correct isn entri region code know list contain region said ecr mind probabl fix quickli switch support region aw publicli list contain region regist contain aw ecr need login ecr aw cli docker login us command order token need docker login clone repo build imag local push ecr login aw consol arn imag match format code add entri code variabl us",
        "Answer_gpt_summary_original":"Solution:\n- The user can find the appropriate container for their region in ECR (Elastic Container Registry).\n- If AWS doesn't have a publicly listed container in the user's region, they can register the container themselves in AWS with ECR.\n- The user can use the command \"aws ecr get-login --region ap-northeast-2\" in order to get the token they'll need for docker login.\n- Then, the user can clone the repo \"https:\/\/github.com\/aws\/sagemaker-xgboost-container\", build the image locally, and push it up to ECR.\n- After that, the user can login to the AWS console (or use the AWS CLI) and find the ARN of the image. It",
        "Answer_gpt_summary":"solut user appropri contain region ecr elast contain registri aw publicli list contain user region regist contain aw ecr user us command aw ecr login region northeast order token need docker login user clone repo http github com aw xgboost contain build imag local push ecr user login aw consol us aw cli arn imag"
    },
    {
        "Question_title":"Deploying custom model on Azure ML Studio",
        "Question_body":"<p>In Azure ML Studio, we have the option of choosing a number of inbuilt ML models like Classification, Regression, etc. , which we can drag and drop to our workflow.<\/p>\n\n<p>My question is, can I upload a custom ML model that I have built locally on my system in Python, and add it to the workflow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1565761282107,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1565767277470,
        "Question_score":1,
        "Question_view_count":1187.0,
        "Poster_id":11925607.0,
        "Poster_created_time":1565761178208,
        "Poster_last_access_time":1613393617227,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":33.0,
        "Poster_views":10.0,
        "Answer_body":"<ol>\n<li>Take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio. Click the \u201cNew\u201d icon in the bottom left:\n<a href=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" alt=\"\"><\/a><\/li>\n<li>In the pane that comes up, click on dataset, and then \u201cFrom Local File\u201d:\n<a href=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" alt=\"\"><\/a><\/li>\n<li>Select the zip file where you stored your serialized model and click the tick. You expirement should look like this:\n<a href=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" alt=\"\"><\/a><\/li>\n<li>Put the following code to run your classification experiment:<\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]])\n<\/code><\/pre>\n\n<p><strong>Update<\/strong> \nIf you want to declare this experiment as an API you need to add web input and output to the Python script module.\n<a href=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":1566565696980,
        "Answer_score":0.0,
        "Answerer_id":9929041.0,
        "Answerer_created_time":1528790837107,
        "Answerer_last_access_time":1660146049396,
        "Answerer_age":null,
        "Answerer_location":"Paris, France",
        "Answerer_reputation":610.0,
        "Answerer_views":203.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57488706",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1566202471556,
        "Question_original_content":"deploi custom model studio studio option choos number inbuilt model like classif regress drag drop workflow question upload custom model built local python add workflow",
        "Question_preprocessed_content":"deploi custom model studio studio option choos number inbuilt model like classif regress drag drop workflow question upload custom model built local python add workflow",
        "Question_gpt_summary_original":"The user is facing a challenge of deploying a custom machine learning model on Azure ML Studio and wants to know if it is possible to upload a locally built model in Python to the workflow.",
        "Question_gpt_summary":"user face challeng deploi custom machin learn model studio want know possibl upload local built model python workflow",
        "Answer_original_content":"model pkl file zip upload studio click new icon left pane come click dataset local file select zip file store serial model click tick expir look like follow code run classif experi import panda import sy import pickl def main datafram datafram sy path insert script bundl model pickl load open script bundl model pkl pred model predict datafram return datafram pred updat want declar experi api need add web input output python script modul",
        "Answer_preprocessed_content":"file zip upload studio click new icon left pane come click dataset local file select zip file store serial model click tick expir look like follow code run classif experi updat want declar experi api need add web input output python script modul",
        "Answer_gpt_summary_original":"Solution: The user can upload the locally built model in Python to Azure ML Studio by zipping the model.pkl file and uploading it as a dataset in the workflow. Then, the user can run the classification experiment by adding the provided code to the Python script module. If the user wants to declare this experiment as an API, they need to add web input and output to the Python script module.",
        "Answer_gpt_summary":"solut user upload local built model python studio zip model pkl file upload dataset workflow user run classif experi ad provid code python script modul user want declar experi api need add web input output python script modul"
    },
    {
        "Question_title":"How to verify all the dependencies are installed in sagemaker?",
        "Question_body":"<p>I am creating an sagemaker endpoint and loading a pretrained model from an s3 bucket. the model -&gt; model.tar.gz file has directory structure as documented here, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure<\/a><\/p>\n<pre><code>model.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  # only for versions 1.3.1 and higher\n<\/code><\/pre>\n<p>I have put few dependencies in requirements.txt, is there a way to verify that all the dependencies were installed correctly?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1645671696107,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":107.0,
        "Poster_id":13645093.0,
        "Poster_created_time":1590797441983,
        "Poster_last_access_time":1664049080543,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":525.0,
        "Poster_views":98.0,
        "Answer_body":"<p>It is not possible to get access or SSH into the machine that's running your deployment. So, one way is to assert the versions of your dependencies in the <code>model_fn<\/code> inside &quot;inference.py&quot; something like below.<\/p>\n<p>if your requirements.txt looks like this:<\/p>\n<pre><code>numpy==1.20.3\npandas==1.3.4\n<\/code><\/pre>\n<p>get the versions and assert them in `model_fn like below:<\/p>\n<pre><code>import os\n\n### your other code ###\n\ndef model_fn(model_dir):\n    # assuming you have numpy and pandas\n    assert os.popen(&quot;python3 -m pip freeze | grep -E 'numpy|pandas'&quot;).read() == 'numpy==1.20.3\\npandas==1.3.4\\n'\n    ### your other code ###\n    return xxxx\n\n### your other code ###\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1645695474836,
        "Answer_score":1.0,
        "Answerer_id":11814996.0,
        "Answerer_created_time":1559910246180,
        "Answerer_last_access_time":1664039951323,
        "Answerer_age":null,
        "Answerer_location":"Bengaluru, Karnataka, India",
        "Answerer_reputation":2046.0,
        "Answerer_views":369.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71246559",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1645694718683,
        "Question_original_content":"verifi depend instal creat endpoint load pretrain model bucket model model tar file directori structur document http readthedoc stabl framework pytorch pytorch html model directori structur model tar model pth code infer requir txt version higher depend requir txt wai verifi depend instal correctli",
        "Question_preprocessed_content":"verifi depend instal creat endpoint load pretrain model bucket model file directori structur document depend wai verifi depend instal correctli",
        "Question_gpt_summary_original":"The user is facing a challenge in verifying whether all the dependencies in the requirements.txt file were installed correctly while creating an sagemaker endpoint and loading a pretrained model from an s3 bucket.",
        "Question_gpt_summary":"user face challeng verifi depend requir txt file instal correctli creat endpoint load pretrain model bucket",
        "Answer_original_content":"possibl access ssh machin run deploy wai assert version depend model insid infer like requir txt look like numpi panda version assert model like import code def model model dir assum numpi panda assert popen python pip freez grep numpi panda read numpi npanda code return code",
        "Answer_preprocessed_content":"possibl access ssh machin run deploy wai assert version depend insid like look like version assert like",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to assert the versions of the dependencies in the `model_fn` inside \"inference.py\" by getting the versions from the requirements.txt file and comparing them with the installed versions using the `os.popen()` command.",
        "Answer_gpt_summary":"solut possibl solut mention discuss assert version depend model insid infer get version requir txt file compar instal version popen command"
    },
    {
        "Question_title":"how connect to Azure Machine Learning Web service using PowerShell?",
        "Question_body":"<p>To use Azure Machine Learning Web service <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/consume-web-services\" rel=\"nofollow noreferrer\">here<\/a> you can find some sample code in C#, R, Python and JavaScript. I want to use it in PowerShell.\nI found <a href=\"https:\/\/www.sepago.com\/blog\/2015\/11\/30\/zugriff-mit-powershell-auf-azure-machine-learning-api-azureml\" rel=\"nofollow noreferrer\">this<\/a> tutorial, but when I am running bellow line of code, it will return error that it is not recognized:<\/p>\n\n<pre><code>Set-AzureMLWebServiceConnection -URI $Url -APIKey $API_key\n\nOutput:\nSet-AzureMLWebServiceConnection : The term 'Set-AzureMLWebServiceConnection' is not recognized as the name of a cmdlet, function, script file, or operable \nprogram. Check the spelling of the name, or if a path was included, verify that the path is correct and try again.\nAt C:\\Users\\Reza\\Desktop\\ndbench\\Azure\\Automation\\01_get_metrics\\add_target_to_tables - runbook_01.ps1:33 char:1\n+ Set-AzureMLWebServiceConnection -URI $Url -APIKey $API_key\n+ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    + CategoryInfo          : ObjectNotFound: (Set-AzureMLWebServiceConnection:String) [], CommandNotFoundException\n    + FullyQualifiedErrorId : CommandNotFoundException\n<\/code><\/pre>\n\n<p>I can't found <code>Set-AzureMLWebServiceConnection<\/code> in my PowerShell command-list and I don't know how I can enable\/install it.\n<a href=\"https:\/\/i.stack.imgur.com\/YMso7.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YMso7.jpg\" alt=\"enter image description here\"><\/a>\nCan you please guide me, how I can connect to Azure Machine Learning Web service using PowerShell?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1523625908693,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":303.0,
        "Poster_id":4991669.0,
        "Poster_created_time":1433870950220,
        "Poster_last_access_time":1663930681916,
        "Poster_age":null,
        "Poster_location":"Tehran, Iran",
        "Poster_reputation":1316.0,
        "Poster_views":201.0,
        "Answer_body":"<p>The comment @gvee mentioned may be the best to use going forward though it is in beta.<\/p>\n\n<p>However, to answer your question, use the <code>Install-Module -Name AzureML<\/code> <a href=\"https:\/\/www.powershellgallery.com\/packages\/AzureML\/1.0.1\" rel=\"nofollow noreferrer\">command<\/a> to get access to the Azure ML commands.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZerMp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZerMp.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":186013.0,
        "Answerer_created_time":1254957460063,
        "Answerer_last_access_time":1659941040212,
        "Answerer_age":null,
        "Answerer_location":"North Carolina, USA",
        "Answerer_reputation":2484.0,
        "Answerer_views":362.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49818134",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1523724686212,
        "Question_original_content":"connect web servic powershel us web servic sampl code python javascript want us powershel tutori run bellow line code return error recogn set webserviceconnect uri url apikei api kei output set webserviceconnect term set webserviceconnect recogn cmdlet function script file oper program check spell path includ verifi path correct try user reza desktop ndbench azur autom metric add target tabl runbook char set webserviceconnect uri url apikei api kei categoryinfo objectnotfound set webserviceconnect string commandnotfoundexcept fullyqualifiederrorid commandnotfoundexcept set webserviceconnect powershel command list know enabl instal guid connect web servic powershel",
        "Question_preprocessed_content":"connect web servic powershel us web servic sampl code python javascript want us powershel tutori run bellow line code return error recogn powershel know guid connect web servic powershel",
        "Question_gpt_summary_original":"The user is facing challenges in connecting to Azure Machine Learning Web service using PowerShell. They have tried to follow a tutorial but encountered an error message stating that the command 'Set-AzureMLWebServiceConnection' is not recognized. The user is seeking guidance on how to enable\/install the command and connect to Azure Machine Learning Web service using PowerShell.",
        "Question_gpt_summary":"user face challeng connect web servic powershel tri follow tutori encount error messag state command set webserviceconnect recogn user seek guidanc enabl instal command connect web servic powershel",
        "Answer_original_content":"comment gvee mention best us go forward beta answer question us instal modul command access command",
        "Answer_preprocessed_content":"comment mention best us go forward beta answer question us command access command",
        "Answer_gpt_summary_original":"Solution: The user can use the command 'Install-Module -Name AzureML' to get access to the Azure ML commands and connect to Azure Machine Learning Web service using PowerShell.",
        "Answer_gpt_summary":"solut user us command instal modul access command connect web servic powershel"
    },
    {
        "Question_title":"How can I clean memory or use SageMaker instead to avoid MemoryError: Unable to allocate for an array with shape (25000, 2000) and data type float64",
        "Question_body":"<p>I'm using keras to train a model on SageMaker, here's the code I'm using but I hit the error:<\/p>\n<pre><code>MemoryError: Unable to allocate 381. MiB for an array with shape (25000, 2000) \n    and data type float64\n<\/code><\/pre>\n<p>Here's the code:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom keras.datasets import imdb\nfrom keras import models, layers, optimizers, losses, metrics\nimport matplotlib.pyplot as plt\n\n# load imbd preprocessed dataset\n(train_data, train_labels), (test_data, test_labels) = imdb.load_data(\n    num_words=2000)\n\n# one-hot encoding all the integer into a binary matrix\ndef vectorize_sequences(sequences, dimension=2000):\n    results = np.zeros((len(sequences), dimension))        \n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.                          \n    return results\n\nx_train = vectorize_sequences(train_data)                  \nx_test = vectorize_sequences(test_data)\n<\/code><\/pre>\n<p>Then I get the error.<\/p>\n<p>The first time when I run this code it works but it failed when I tried to re-run it, how I can fix it by cleaning the memory or is there a way that I can use the memory on SageMaker?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1594553641273,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1594581031423,
        "Question_score":1,
        "Question_view_count":1210.0,
        "Poster_id":10581944.0,
        "Poster_created_time":1540920956270,
        "Poster_last_access_time":1663875036883,
        "Poster_age":null,
        "Poster_location":"United Kingdom",
        "Poster_reputation":2385.0,
        "Poster_views":585.0,
        "Answer_body":"<p>I wouldn't know about SageMaker or AWS specifically, but something you can do is cast your input to <code>float32<\/code>, which takes less memory space. You can cast it like this:<\/p>\n<pre><code>train_data = tf.cast(train_data, tf.float32)\n<\/code><\/pre>\n<p><code>float32<\/code> is the default value of Tensorflow weights so you don't need <code>float64<\/code> anyway. Proof:<\/p>\n<pre><code>import tensorflow as tf\nlayer = tf.keras.layers.Dense(8)\nprint(layer(tf.random.uniform((10, 100), 0, 1)).dtype)\n<\/code><\/pre>\n<pre><code>&lt;dtype: 'float32'&gt;\n<\/code><\/pre>\n<p>My other suggestions are to get less words from your dataset, or to not one-hot encode them. If you're planning on training a recurrent model with an embedding layer, you won't need to anyway.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Answerer_id":10908375.0,
        "Answerer_created_time":1547395160296,
        "Answerer_last_access_time":1663766440512,
        "Answerer_age":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation":30365.0,
        "Answerer_views":5514.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62860539",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1594554603092,
        "Question_original_content":"clean memori us instead avoid memoryerror unabl alloc arrai shape data type float kera train model code hit error memoryerror unabl alloc mib arrai shape data type float code import panda import numpi kera dataset import imdb kera import model layer optim loss metric import matplotlib pyplot plt load imbd preprocess dataset train data train label test data test label imdb load data num word hot encod integ binari matrix def vector sequenc sequenc dimens result zero len sequenc dimens sequenc enumer sequenc result sequenc return result train vector sequenc train data test vector sequenc test data error time run code work fail tri run fix clean memori wai us memori",
        "Question_preprocessed_content":"clean memori us instead avoid memoryerror unabl alloc arrai shape data type float kera train model code hit error code error time run code work fail tri fix clean memori wai us memori",
        "Question_gpt_summary_original":"The user is encountering a MemoryError while using Keras to train a model on SageMaker. The error occurs when trying to allocate memory for an array with shape (25000, 2000) and data type float64. The user is seeking advice on how to clean the memory or use SageMaker's memory to avoid the error.",
        "Question_gpt_summary":"user encount memoryerror kera train model error occur try alloc memori arrai shape data type float user seek advic clean memori us memori avoid error",
        "Answer_original_content":"wouldn know aw specif cast input float take memori space cast like train data cast train data float float default valu tensorflow weight need float proof import tensorflow layer kera layer dens print layer random uniform dtype suggest word dataset hot encod plan train recurr model embed layer won need",
        "Answer_preprocessed_content":"wouldn know aw specif cast input take memori space cast like default valu tensorflow weight need proof suggest word dataset encod plan train recurr model embed layer won need",
        "Answer_gpt_summary_original":"Solution:\n- Cast the input to float32, which takes less memory space.\n- Use float32 instead of float64 as it is the default value of Tensorflow weights.\n- Get less words from the dataset or not one-hot encode them.",
        "Answer_gpt_summary":"solut cast input float take memori space us float instead float default valu tensorflow weight word dataset hot encod"
    },
    {
        "Question_title":"Multi-file source_dir bundle with SM Training Compiler (distributed)",
        "Question_body":"I'm hoping to use SageMaker Training Compiler with a (Hugging Face Trainer API, PyTorch) program split across multiple .py files for maintainability. The job needs to run on multiple GPUs (although at the current scale, multi-device single-node would be acceptable).\n\nFollowing the docs, I added the distributed_training_launcher.py launcher script to my source_dir bundle, and passed in the true training script via a training_script hyperparameter.\n\n...But when the job tries to start, I get:\n\nTraceback (most recent call last):\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 194, in _run_module_as_main\nreturn _run_code(code, main_globals, None,\n  File \"\/opt\/conda\/lib\/python3.8\/runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 90, in <module>\nmain()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/distributed\/xla_spawn.py\", line 86, in main\nxmp.spawn(mod._mp_fn, args=(), nprocs=args.num_gpus)\nAttributeError: module 'train' has no attribute '_mp_fn'\n\n\nAny ideas what might be causing this? Is there some particular limitation or additional requirement for training scripts that are written over multiple files?\n\nI also tried running in single-GPU mode (p3.2xlarge) instead - directly calling the train script instead of the distributed launcher - and saw the below error which seems to originate within TrainingArguments itself? Not sure why it's trying to call a 'tensorflow\/compiler' compiler when running in PT..?\n\nEDIT: Turns out the below error can be solved by explicitly setting n_gpus as mentioned on the troubleshooting doc - but that takes me back to the error message above\n\nFile \"\/opt\/ml\/code\/code\/config.py\", line 124, in __post_init__\nsuper().__post_init__()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 761, in __post_init__\nif is_torch_available() and self.device.type != \"cuda\" and (self.fp16 or self.fp16_full_eval):\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 975, in device\nreturn self._setup_devices\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1754, in __get__\ncached = self.fget(obj)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/file_utils.py\", line 1764, in wrapper\nreturn func(*args, **kwargs)\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/transformers\/training_args.py\", line 918, in _setup_devices\ndevice = xm.xla_device()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 231, in xla_device\ndevices = get_xla_supported_devices(\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 137, in get_xla_supported_devices\nxla_devices = _DEVICES.value\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/utils\/utils.py\", line 32, in value\nself._value = self._gen_fn()\n  File \"\/opt\/conda\/lib\/python3.8\/site-packages\/torch_xla\/core\/xla_model.py\", line 19, in <lambda>\n_DEVICES = xu.LazyProperty(lambda: torch_xla._XLAC._xla_get_devices())\nRuntimeError: tensorflow\/compiler\/xla\/xla_client\/computation_client.cc:273 : Missing XLA configuration",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1639669045329,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":29.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Ahh I solved this a while ago and forgot to update -\n\nYes, the training script needs to define a _mp_fn (which can just execute the same code as gets run if __name__ == \"__main__\") and number of GPUs (at least the last time I checked - hopefully this could change in future) needs to be explicitly configured.\n\nFor my particular project the fix to enable SMTC on the existing job is available online here. For others would also suggest referring to the official SMTC example notebooks & scripts!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwcM0XER5TcOggtQ_5cfVPw\/multi-file-source-dir-bundle-with-sm-training-compiler-distributed",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1657872107440,
        "Question_original_content":"multi file sourc dir bundl train compil distribut hope us train compil hug face trainer api pytorch program split multipl file maintain job need run multipl gpu current scale multi devic singl node accept follow doc ad distribut train launcher launcher script sourc dir bundl pass true train script train script hyperparamet job tri start traceback recent file opt conda lib python runpi line run modul main return run code code main global file opt conda lib python runpi line run code exec code run global file opt conda lib python site packag torch xla distribut xla spawn line main file opt conda lib python site packag torch xla distribut xla spawn line main xmp spawn mod arg nproc arg num gpu attributeerror modul train attribut idea caus particular limit addit requir train script written multipl file tri run singl gpu mode xlarg instead directli call train script instead distribut launcher saw error origin trainingargu sure try tensorflow compil compil run edit turn error solv explicitli set gpu mention troubleshoot doc take error messag file opt code code config line post init super post init file opt conda lib python site packag transform train arg line post init torch avail self devic type cuda self self eval file opt conda lib python site packag transform file util line wrapper return func arg kwarg file opt conda lib python site packag transform train arg line devic return self setup devic file opt conda lib python site packag transform file util line cach self fget obj file opt conda lib python site packag transform file util line wrapper return func arg kwarg file opt conda lib python site packag transform train arg line setup devic devic xla devic file opt conda lib python site packag torch xla core xla model line xla devic devic xla support devic file opt conda lib python site packag torch xla core xla model line xla support devic xla devic devic valu file opt conda lib python site packag torch xla util util line valu self valu self gen file opt conda lib python site packag torch xla core xla model line devic lazyproperti lambda torch xla xlac xla devic runtimeerror tensorflow compil xla xla client comput client miss xla configur",
        "Question_preprocessed_content":"bundl train compil hope us train compil program split multipl file maintain job need run multipl gpu follow doc ad launcher script bundl pass true train script hyperparamet job tri start traceback file line return file line exec file line main file line main arg attributeerror modul train attribut idea caus particular limit addit requir train script written multipl file tri run mode instead directli call train script instead distribut launcher saw error origin trainingargu sure try compil run edit turn error solv explicitli set mention troubleshoot doc take error messag file line file line cuda file line wrapper return func file line devic return file line cach file line wrapper return func file line devic file line devic file line file line valu file line runtimeerror miss xla configur",
        "Question_gpt_summary_original":"The user is facing challenges while using SageMaker Training Compiler with a program split across multiple .py files for maintainability. The job needs to run on multiple GPUs, but when the job tries to start, the user gets an error message. The error message suggests that there might be some particular limitation or additional requirement for training scripts that are written over multiple files. The user also tried running in single-GPU mode but encountered another error message that seems to originate within TrainingArguments itself.",
        "Question_gpt_summary":"user face challeng train compil program split multipl file maintain job need run multipl gpu job tri start user get error messag error messag suggest particular limit addit requir train script written multipl file user tri run singl gpu mode encount error messag origin trainingargu",
        "Answer_original_content":"ahh solv ago forgot updat ye train script need defin execut code get run main number gpu time check hopefulli chang futur need explicitli configur particular project fix enabl smtc exist job avail onlin suggest refer offici smtc exampl notebook script",
        "Answer_preprocessed_content":"ahh solv ago forgot updat ye train script need defin number gpu need explicitli configur particular project fix enabl smtc exist job avail onlin suggest refer offici smtc exampl notebook script",
        "Answer_gpt_summary_original":"Solution: The user needs to define a _mp_fn in the training script and explicitly configure the number of GPUs. Referring to the official SageMaker Training Compiler example notebooks and scripts is also suggested. The fix to enable SMTC on the existing job is available online.",
        "Answer_gpt_summary":"solut user need defin train script explicitli configur number gpu refer offici train compil exampl notebook script suggest fix enabl smtc exist job avail onlin"
    },
    {
        "Question_title":"Vertex AI Custom Container Training Job python SDK - google.api_core.exceptions.FailedPrecondition: 400 '",
        "Question_body":"<p>I have built a custom container which use my managed dataset on vertex to run my training code, it worked successfully when I create the training job on the Vertex AI website interface.<\/p>\n<p>But now I'm trying to create the training job from a python script using<\/p>\n<pre><code>class google.cloud.aiplatform.CustomContainerTrainingJob\n<\/code><\/pre>\n<p>I load a managed dataset that I have on vertex AI with<\/p>\n<pre><code>dataset = aiplatform.ImageDataset(dataset_id) if dataset_id else None\n<\/code><\/pre>\n<p>But when I try to run the following code:<\/p>\n<pre><code>model = job.run(\n        dataset=dataset,\n        model_display_name=model_display_name,\n        args=args,\n        replica_count=replica_count,\n        machine_type=machine_type,\n        accelerator_type=accelerator_type,\n        accelerator_count=accelerator_count,\n        training_fraction_split=training_fraction_split,\n        validation_fraction_split=validation_fraction_split,\n        test_fraction_split=test_fraction_split,\n        sync=sync,\n    )\n\n    model.wait()\n\n    print(model.display_name)\n    print(model.resource_name)\n    print(model.uri)\n    return model\n<\/code><\/pre>\n<p>I got the following error:<\/p>\n<pre><code>google.api_core.exceptions.FailedPrecondition: 400 'annotation_schema_uri' should be set in the TrainingPipeline.input_data_config for custom training or hyperparameter tuning with managed dataset.\n<\/code><\/pre>\n<p>I feel like something is wrong because when I create the job on the website I specify an export directory for the managed dataset, but I have not found where to do it here.<\/p>\n<p>Any ideas?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1656062302777,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":170.0,
        "Poster_id":19406402.0,
        "Poster_created_time":1656061360900,
        "Poster_last_access_time":1659951704647,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":11.0,
        "Poster_views":0.0,
        "Answer_body":"<p>Well I found the answer in the <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/aiplatform?hl=fr#class-googlecloudaiplatformcustomcontainertrainingjobdisplayname-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-containeruri-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-command-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerimageuri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerpredictroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerhealthroute-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainercommand-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerargs-optionalsequencestrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerenvironmentvariables-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelservingcontainerports-optionalsequenceinthttpspythonreadthedocsioenlatestlibraryfunctionshtmlint--none-modeldescription-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelinstanceschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelparametersschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelpredictionschemauri-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-project-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-location-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-credentials-optionalgoogleauthcredentialscredentialshttpsgoogleapisdevpythongoogle-authlatestreferencegoogleauthcredentialshtmlgoogleauthcredentialscredentials--none-labels-optionaldictstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr-strhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-trainingencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-modelencryptionspeckeyname-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none-stagingbucket-optionalstrhttpspythonreadthedocsioenlatestlibrarystdtypeshtmlstr--none\" rel=\"nofollow noreferrer\">documentation<\/a>, data are automatically exported to the provided bucket thus it was not the issue. The issue was in the error ( obviously ).\nTo provide a good annotation URI, it is enough to just add a parameter to run():<\/p>\n<pre><code>annotation_schema_uri=aiplatform.schema.dataset.annotation.image.classification\n<\/code><\/pre>\n<p>image.classification was what I needed here but can be replaced by text.extraction if you do text extraction for example.<\/p>\n<p>This will pass as string value the following value which is the asked gs uri:<\/p>\n<pre><code>gs:\/\/google-cloud-aiplatform\/schema\/dataset\/annotation\/image_classification_1.0.0.yaml\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1656343337227,
        "Answer_score":1.0,
        "Answerer_id":19406402.0,
        "Answerer_created_time":1656061360900,
        "Answerer_last_access_time":1659951704647,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":11.0,
        "Answerer_views":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72741757",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1656071944492,
        "Question_original_content":"custom contain train job python sdk googl api core except failedprecondit built custom contain us manag dataset vertex run train code work successfulli creat train job websit interfac try creat train job python script class googl cloud aiplatform customcontainertrainingjob load manag dataset dataset aiplatform imagedataset dataset dataset try run follow code model job run dataset dataset model displai model displai arg arg replica count replica count machin type machin type acceler type acceler type acceler count acceler count train fraction split train fraction split valid fraction split valid fraction split test fraction split test fraction split sync sync model wait print model displai print model resourc print model uri return model got follow error googl api core except failedprecondit annot schema uri set trainingpipelin input data config custom train hyperparamet tune manag dataset feel like wrong creat job websit specifi export directori manag dataset idea thank",
        "Question_preprocessed_content":"custom contain train job python sdk built custom contain us manag dataset vertex run train code work successfulli creat train job websit interfac try creat train job python script load manag dataset try run follow code got follow error feel like wrong creat job websit specifi export directori manag dataset idea thank",
        "Question_gpt_summary_original":"The user is encountering an error while trying to create a training job from a python script using the google.cloud.aiplatform.CustomContainerTrainingJob class. The error message indicates that the 'annotation_schema_uri' should be set in the TrainingPipeline.input_data_config for custom training or hyperparameter tuning with managed dataset. The user is unsure of how to specify an export directory for the managed dataset in the python script.",
        "Question_gpt_summary":"user encount error try creat train job python script googl cloud aiplatform customcontainertrainingjob class error messag indic annot schema uri set trainingpipelin input data config custom train hyperparamet tune manag dataset user unsur specifi export directori manag dataset python script",
        "Answer_original_content":"answer document data automat export provid bucket issu issu error obvious provid good annot uri add paramet run annot schema uri aiplatform schema dataset annot imag classif imag classif need replac text extract text extract exampl pass string valu follow valu ask uri googl cloud aiplatform schema dataset annot imag classif yaml",
        "Answer_preprocessed_content":"answer document data automat export provid bucket issu issu error provid good annot uri add paramet run need replac text extract exampl pass string valu follow valu ask uri",
        "Answer_gpt_summary_original":"Solution:\n- The error message indicates that the 'annotation_schema_uri' should be set in the TrainingPipeline.input_data_config for custom training or hyperparameter tuning with managed dataset.\n- To specify an export directory for the managed dataset in the python script, it is enough to add a parameter to run(): annotation_schema_uri=aiplatform.schema.dataset.annotation.image.classification. This will pass as string value the following value which is the asked gs uri: gs:\/\/google-cloud-aiplatform\/schema\/dataset\/annotation\/image_classification_1.0.0.yaml.",
        "Answer_gpt_summary":"solut error messag indic annot schema uri set trainingpipelin input data config custom train hyperparamet tune manag dataset specifi export directori manag dataset python script add paramet run annot schema uri aiplatform schema dataset annot imag classif pass string valu follow valu ask uri googl cloud aiplatform schema dataset annot imag classif yaml"
    },
    {
        "Question_title":"How to have multiple MLFlow runs in parallel?",
        "Question_body":"<p>I'm not very familiar with parallelization in Python and I'm getting an error when trying to train a model on multiple training folds in parallel. Here's a simplified version of my code:<\/p>\n<pre><code>def train_test_model(fold):\n    # here I train the model etc...\n    \n    # now I want to save the parameters and metrics\n    with mlflow.start_run():\n        mlflow.log_param(&quot;run_name&quot;, run_name)\n        mlflow.log_param(&quot;modeltype&quot;, modeltype)\n        # and so on...\n\nif __name__==&quot;__main__&quot;:\n    pool = ThreadPool(processes = num_trials)\n    # run folds in parallel\n    pool.map(lambda fold:train_test_model(fold), folds)\n<\/code><\/pre>\n<p>I'm getting the following error:<\/p>\n<pre><code>Exception: Run with UUID 23e9bb6d22674a518e48af9c51252860 is already active. To start a new run, first end the current run with mlflow.end_run(). To start a nested run, call start_run with nested=True\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/mlflow.org\/docs\/latest\/python_api\/mlflow.html#mlflow.start_run\" rel=\"nofollow noreferrer\">documentation<\/a> says that <code>mlflow.start_run()<\/code> starts a new run and makes it active which is the root of my problem. Every thread starts a MLFlow run for its corresponding fold and makes it active while I need the runs to run in parallel i.e. all be active(?) and save parameters\/metrics of the corresponding fold. How can I solve that issue?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1603544320573,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":2275.0,
        "Poster_id":9215579.0,
        "Poster_created_time":1515931819516,
        "Poster_last_access_time":1664047231110,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":177.0,
        "Poster_views":7.0,
        "Answer_body":"<p>I found a solution, maybe it will be useful for someone else. You can see details with code examples here: <a href=\"https:\/\/github.com\/mlflow\/mlflow\/issues\/3592\" rel=\"nofollow noreferrer\">https:\/\/github.com\/mlflow\/mlflow\/issues\/3592<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":9215579.0,
        "Answerer_created_time":1515931819516,
        "Answerer_last_access_time":1664047231110,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":177.0,
        "Answerer_views":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64513552",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1603877162672,
        "Question_original_content":"multipl run parallel familiar parallel python get error try train model multipl train fold parallel simplifi version code def train test model fold train model want save paramet metric start run log param run run log param modeltyp modeltyp main pool threadpool process num trial run fold parallel pool map lambda fold train test model fold fold get follow error except run uuid ebbdaeafc activ start new run end current run end run start nest run start run nest true document sai start run start new run make activ root problem thread start run correspond fold make activ need run run parallel activ save paramet metric correspond fold solv issu",
        "Question_preprocessed_content":"multipl run parallel familiar parallel python get error try train model multipl train fold parallel simplifi version code get follow error document sai start new run make activ root problem thread start run correspond fold make activ need run run parallel activ save correspond fold solv issu",
        "Question_gpt_summary_original":"The user is encountering an error when attempting to train a model on multiple training folds in parallel using Python's ThreadPool. The error message indicates that the MLFlow runs are not running in parallel, as each thread starts a new run and makes it active. The user is seeking a solution to allow the runs to run in parallel and save parameters\/metrics of the corresponding fold.",
        "Question_gpt_summary":"user encount error attempt train model multipl train fold parallel python threadpool error messag indic run run parallel thread start new run make activ user seek solut allow run run parallel save paramet metric correspond fold",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"Is there a way to pass arguments to our own docker container in sagemaker?",
        "Question_body":"<p>I am trying to train my model using Bring your own container technique in sagemaker. My model training runs correctly without any issues locally. But my docker image takes env-file as an input that could change at different runs. But in sagemaker when passing the ECR image, I don't know how to pass this env-file. So instead, inside the <code>train<\/code> script, which is called by the sagemaker, I added <code>export KEY=value<\/code> statements to create my variables. Even that did not expose my variables. Another way I tried it was by executing <code>RUN source file.env<\/code> while building my image. Even this approach did not work out as I got an error <code>\/bin\/sh: 1: source: not found<\/code>.<\/p>\n<p>I could try <code>ENV<\/code> while building my image and that would probably work but this approach won't be flexible as my variables could change at different runs. Is there any way to pass docker run arguments from a sagemaker estimator or notebook? I checked out the documentation but I couldn't find anything.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1634032422110,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":292.0,
        "Poster_id":17079471.0,
        "Poster_created_time":1633433905427,
        "Poster_last_access_time":1663334017808,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":43.0,
        "Poster_views":5.0,
        "Answer_body":"<p>I've been passing environment variables along with the Docker image URL when creating the Training job using the SageMaker Python SDK. Documentation of the <code>train<\/code> method states that:<\/p>\n<pre><code>environment (dict[str, str]) : Environment variables to be set for\n            use during training job (default: ``None``): \n<\/code><\/pre>\n<p>For reference, the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/5bc3ccf\/src\/sagemaker\/session.py#L569\" rel=\"nofollow noreferrer\">SDK source<\/a>.<\/p>\n<p>Because the SDK is a wrapper on top of <a href=\"https:\/\/pypi.org\/project\/boto3\/\" rel=\"nofollow noreferrer\">Boto3<\/a>, I'm pretty sure that the same can be implemented with Boto3 alone, and that there is an equivalent for every other <a href=\"https:\/\/aws.amazon.com\/developer\/tools\/#SDKs\" rel=\"nofollow noreferrer\">Amazon Services SDK<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":368544.0,
        "Answerer_created_time":1276712500692,
        "Answerer_last_access_time":1663960754063,
        "Answerer_age":null,
        "Answerer_location":"Tandil, Buenos Aires Province, Argentina",
        "Answerer_reputation":7226.0,
        "Answerer_views":637.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69538469",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1662661497360,
        "Question_original_content":"wai pass argument docker contain try train model bring contain techniqu model train run correctli issu local docker imag take env file input chang differ run pass ecr imag know pass env file instead insid train script call ad export kei valu statement creat variabl expos variabl wai tri execut run sourc file env build imag approach work got error bin sourc try env build imag probabl work approach won flexibl variabl chang differ run wai pass docker run argument estim notebook check document couldn",
        "Question_preprocessed_content":"wai pass argument docker contain try train model bring contain techniqu model train run correctli issu local docker imag take input chang differ run pass ecr imag know pass instead insid script call ad statement creat variabl expos variabl wai tri execut build imag approach work got error try build imag probabl work approach won flexibl variabl chang differ run wai pass docker run argument estim notebook check document couldn",
        "Question_gpt_summary_original":"The user is facing challenges in passing arguments to their own docker container in Sagemaker while using the Bring your own container technique. The docker image takes an env-file as an input that could change at different runs, but the user does not know how to pass this env-file while passing the ECR image. The user tried adding export KEY=value statements to create variables, but it did not expose the variables. The user also tried executing RUN source file.env while building the image, but it resulted in an error. The user is looking for a way to pass docker run arguments from a Sagemaker estimator or notebook, but could not find any documentation on it.",
        "Question_gpt_summary":"user face challeng pass argument docker contain bring contain techniqu docker imag take env file input chang differ run user know pass env file pass ecr imag user tri ad export kei valu statement creat variabl expos variabl user tri execut run sourc file env build imag result error user look wai pass docker run argument estim notebook document",
        "Answer_original_content":"pass environ variabl docker imag url creat train job python sdk document train method state environ dict str str environ variabl set us train job default refer sdk sourc sdk wrapper boto pretti sure implement boto equival amazon servic sdk",
        "Answer_preprocessed_content":"pass environ variabl docker imag url creat train job python sdk document method state refer sdk sourc sdk wrapper boto pretti sure implement boto equival amazon servic sdk",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to pass environment variables along with the Docker image URL when creating the training job using the SageMaker Python SDK. The SDK provides an option to set environment variables for use during the training job. It is also suggested that the same can be implemented with Boto3 alone, and that there is an equivalent for every other Amazon Services SDK.",
        "Answer_gpt_summary":"solut possibl solut mention discuss pass environ variabl docker imag url creat train job python sdk sdk provid option set environ variabl us train job suggest implement boto equival amazon servic sdk"
    },
    {
        "Question_title":"Use dataset registed in on pipelines in AML",
        "Question_body":"<p>I was following the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-python-sdk\" rel=\"nofollow noreferrer\">SDK v2 Python tutorial<\/a> in order to create a pipeline job with my own assets. I notice that in this tutorial they let you use a csv file that can be downloaded but Im trying to use a registered dataset that I already registered by my own. The problem that I facing is that I dont know where I need to specify the dataset.<\/p>\n<p>The funny part is that at the beginning they create this dataset like this:<\/p>\n<pre><code>credit_data = ml_client.data.create_or_update(credit_data)\nprint(\n    f&quot;Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}&quot;\n)\n<\/code><\/pre>\n<p>But the only part where they refer to this dataset is on the last part where they # the line:<\/p>\n<pre><code>registered_model_name = &quot;credit_defaults_model&quot;\n\n# Let's instantiate the pipeline with the parameters of our choice\npipeline = credit_defaults_pipeline(\n    # pipeline_job_data_input=credit_data,\n    pipeline_job_data_input=Input(type=&quot;uri_file&quot;, path=web_path),\n    pipeline_job_test_train_ratio=0.2,\n    pipeline_job_learning_rate=0.25,\n    pipeline_job_registered_model_name=registered_model_name,\n)\n<\/code><\/pre>\n<p>For me this means that I can use this data like this (a already registered dataset), the problem is that I don't know where I need to do the changes (I know that in the data_prep.py and in the code below but I don\u00b4t know where else) and I don't know how to set this:<\/p>\n<pre><code>%%writefile {data_prep_src_dir}\/data_prep.py\n...\n\ndef main():\n    &quot;&quot;&quot;Main function of the script.&quot;&quot;&quot;\n\n    # input and output arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don\u00b4t know how\n    parser.add_argument(&quot;--test_train_ratio&quot;, type=float, required=False, default=0.25)\n    parser.add_argument(&quot;--train_data&quot;, type=str, help=&quot;path to train data&quot;)\n    parser.add_argument(&quot;--test_data&quot;, type=str, help=&quot;path to test data&quot;)\n    args = parser.parse_args()\n\n...\n<\/code><\/pre>\n<p>Does anyone have experience working as registered datasets?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1656688618547,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1657482591390,
        "Question_score":0,
        "Question_view_count":85.0,
        "Poster_id":17379894.0,
        "Poster_created_time":1636569000947,
        "Poster_last_access_time":1661898017243,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":9.0,
        "Poster_views":2.0,
        "Answer_body":"<blockquote>\n<p>parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don\u00b4t know how<\/p>\n<\/blockquote>\n<p>To get the path to input data, according to <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/how-to-train-with-datasets.md\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<ul>\n<li><p>You can get <code>--input-data<\/code> by ID which you can access in your training script.<\/p>\n<\/li>\n<li><p>Use it as <code>argument<\/code> on <code>mounted_input_path<\/code><\/p>\n<\/li>\n<\/ul>\n<p>For example, try the following three code snippets taken from the <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/how-to-train-with-datasets.md\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<p><strong>Access dataset in training script:<\/strong><\/p>\n<pre><code>parser = argparse.ArgumentParser()\nparser.add_argument(&quot;--input-data&quot;, type=str)\nargs = parser.parse_args()\n\nrun = Run.get_context()\nws = run.experiment.workspace\n\n# get the input dataset by ID\ndataset = Dataset.get_by_id(ws, id=args.input_data)\n<\/code><\/pre>\n<p><strong>Configure the training run:<\/strong><\/p>\n<pre><code>src = ScriptRunConfig(source_directory=script_folder,\n                      script='train_titanic.py',\n                      # pass dataset as an input with friendly name 'titanic'\n                      arguments=['--input-data', titanic_ds.as_named_input('titanic')],\n                      compute_target=compute_target,\n                      environment=myenv)\n<\/code><\/pre>\n<p><strong>Pass <code>mounted_input_path<\/code> as argument:<\/strong><\/p>\n<pre><code>mounted_input_path = sys.argv[1]\nmounted_output_path = sys.argv[2]\n\nprint(&quot;Argument 1: %s&quot; % mounted_input_path)\nprint(&quot;Argument 2: %s&quot; % mounted_output_path)\n<\/code><\/pre>\n<p>References: <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/v1\/how-to-create-register-datasets.md\" rel=\"nofollow noreferrer\">How to create register dataset<\/a> and <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/scriptrun-with-data-input-output\/how-to-use-scriptrun.ipynb\" rel=\"nofollow noreferrer\">How to use configure a training run with data input and output<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":9303470.0,
        "Answerer_created_time":1517548787092,
        "Answerer_last_access_time":1664029451396,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1925.0,
        "Answerer_views":3530.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72831360",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1657082550892,
        "Question_original_content":"us dataset regist pipelin aml follow sdk python tutori order creat pipelin job asset notic tutori let us csv file download try us regist dataset regist problem face dont know need specifi dataset funni begin creat dataset like credit data client data creat updat credit data print dataset credit data regist workspac dataset version credit data version refer dataset line regist model credit default model let instanti pipelin paramet choic pipelin credit default pipelin pipelin job data input credit data pipelin job data input input type uri file path web path pipelin job test train ratio pipelin job learn rate pipelin job regist model regist model mean us data like regist dataset problem know need chang know data prep code dont know know set writefil data prep src dir data prep def main main function script input output argument parser argpars argumentpars parser add argument data type str help path input data dont know parser add argument test train ratio type float requir fals default parser add argument train data type str help path train data parser add argument test data type str help path test data arg parser pars arg experi work regist dataset",
        "Question_preprocessed_content":"us dataset regist pipelin aml follow sdk python tutori order creat pipelin job asset notic tutori let us csv file download try us regist dataset regist problem face dont know need specifi dataset funni begin creat dataset like refer dataset line mean us data like problem know need chang know set experi work regist dataset",
        "Question_gpt_summary_original":"The user is facing challenges in using a registered dataset in a pipeline job while following the SDK v2 Python tutorial. The user is unsure where to specify the dataset and how to make changes in the data_prep.py file to use the registered dataset. The user is seeking help from someone with experience in working with registered datasets.",
        "Question_gpt_summary":"user face challeng regist dataset pipelin job follow sdk python tutori user unsur specifi dataset chang data prep file us regist dataset user seek help experi work regist dataset",
        "Answer_original_content":"parser add argument data type str help path input data dont know path input data accord document input data access train script us argument mount input path exampl try follow code snippet taken document access dataset train script parser argpars argumentpars parser add argument input data type str arg parser pars arg run run context run experi workspac input dataset dataset dataset arg input data configur train run src scriptrunconfig sourc directori script folder script train titan pass dataset input friendli titan argument input data titan name input titan comput target comput target environ myenv pass mount input path argument mount input path sy argv mount output path sy argv print argument mount input path print argument mount output path refer creat regist dataset us configur train run data input output",
        "Answer_preprocessed_content":"type str help path input data dont know path input data accord document access train script us exampl try follow code snippet taken document access dataset train script configur train run pass argument refer creat regist dataset us configur train run data input output",
        "Answer_gpt_summary_original":"The discussion provides solutions to the challenge of using a registered dataset in a pipeline job while following the SDK v2 Python tutorial. The solutions include accessing the dataset in the training script using the dataset ID, configuring the training run by passing the dataset as an input with a friendly name, and passing the mounted_input_path as an argument. The references to the documentation on creating and registering datasets and configuring a training run with data input and output are also provided.",
        "Answer_gpt_summary":"discuss provid solut challeng regist dataset pipelin job follow sdk python tutori solut includ access dataset train script dataset configur train run pass dataset input friendli pass mount input path argument refer document creat regist dataset configur train run data input output provid"
    },
    {
        "Question_title":"Deploying multiple models to same endpoint in Vertex AI",
        "Question_body":"<p>Our use case is as follows:\nWe have multiple custom trained models (in the hundreds, and the number increases as we allow the user of our application to create models through the UI, which we then train and deploy on the fly) and so deploying each model to a separate endpoint is expensive as Vertex AI <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/deploy-model-console#custom-trained\" rel=\"nofollow noreferrer\">charges per node used<\/a>. Based on the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> it seems that we can deploy models of different types to the same endpoint but I am not sure how that would work. Let's say I have 2 different custom trained models deployed using custom containers for prediction to the same endpoint. Also, say I specify the traffic split to be 50% for the two models. Now, how do I send a request to a specific model? Using the python SDK, we make calls to the endpoint, like so:<\/p>\n<pre><code>from google.cloud import aiplatform\nendpoint = aiplatform.Endpoint(endpoint_id)\nprediction = endpoint.predict(instances=instances)\n\n# where endpoint_id is the id of the endpoint and instances are the observations for which a prediction is required\n<\/code><\/pre>\n<p>My understanding is that in this scenario, vertex AI will route some calls to one model and some to the other based on the traffic split. I could use the parameters field, as specified in the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#prediction\" rel=\"nofollow noreferrer\">docs<\/a>, to specify the model and then process the request accordingly in the custom prediction container, but still some calls will end up going to a model which it will not be able to process (because Vertex AI is not going to be sending all requests to all models, otherwise the traffic split wouldn't make sense). How do I then deploy multiple models to the same endpoint and make sure that every prediction request is guaranteed to be served?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_created_time":1636348555970,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":1271.0,
        "Poster_id":6719286.0,
        "Poster_created_time":1471292986790,
        "Poster_last_access_time":1664058170412,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":700.0,
        "Poster_views":90.0,
        "Answer_body":"<p>This <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/general\/deployment#models-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> talks about a use case where 2 models are trained on the same feature set and are sharing the ingress prediction traffic. As you have understood correctly, this does not apply to models that have been trained on different feature sets, that is, different models.<\/p>\n<p>Unfortunately, deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI at the moment. There is an ongoing feature request that is being worked on. However, we cannot provide an exact ETA on when that feature will be available.<\/p>\n<p>I reproduced the multi-model setup and noticed the below points.<\/p>\n<p><strong>Traffic Splitting<\/strong><\/p>\n<blockquote>\n<p>I deployed 2 different models to the same endpoint and sent predictions to it. I set a 50-50 traffic splitting rule and saw errors that implied requests being sent to the wrong model.<\/p>\n<\/blockquote>\n<p><strong>Cost Optimization<\/strong><\/p>\n<blockquote>\n<p>When multiple models are deployed to the same endpoint, they are deployed to separate, independent nodes. So, you will still be charged for each node used. Also, node autoscaling happens at the model level, not at the endpoint level.<\/p>\n<\/blockquote>\n<p>A plausible workaround would be to pack all your models into a single container and use a custom HTTP server logic to send prediction requests to the appropriate model. This could be achieved using the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/predictions\/custom-container-requirements#request_requirements\" rel=\"nofollow noreferrer\"><code>parameters<\/code><\/a> field of the prediction request body. The custom logic would look something like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@app.post(os.environ['AIP_PREDICT_ROUTE'])\nasync def predict(request: Request):\n    body = await request.json()\n    parameters = body[&quot;parameters&quot;]\n    instances = body[&quot;instances&quot;]\n    inputs = np.asarray(instances)\n    preprocessed_inputs = _preprocessor.preprocess(inputs)\n\n    if(parameters[&quot;model_name&quot;]==&quot;random_forest&quot;):\n        print(parameters[&quot;model_name&quot;])\n        outputs = _random_forest_model.predict(preprocessed_inputs)\n    else:\n        print(parameters[&quot;model_name&quot;])\n        outputs = _decision_tree_model.predict(inputs)\n\n    return {&quot;predictions&quot;: [_class_names[class_num] for class_num in outputs]}\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":1636405249843,
        "Answer_score":1.0,
        "Answerer_id":15745884.0,
        "Answerer_created_time":1619163566860,
        "Answerer_last_access_time":1664066627280,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1730.0,
        "Answerer_views":555.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69878915",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1636404890008,
        "Question_original_content":"deploi multipl model endpoint us case follow multipl custom train model hundr number increas allow user applic creat model train deploi fly deploi model separ endpoint expens charg node base document deploi model differ type endpoint sure work let differ custom train model deploi custom contain predict endpoint specifi traffic split model send request specif model python sdk call endpoint like googl cloud import aiplatform endpoint aiplatform endpoint endpoint predict endpoint predict instanc instanc endpoint endpoint instanc observ predict requir understand scenario rout call model base traffic split us paramet field specifi doc specifi model process request accordingli custom predict contain call end go model abl process go send request model traffic split wouldn sens deploi multipl model endpoint sure predict request guarante serv",
        "Question_preprocessed_content":"deploi multipl model endpoint us case follow multipl custom train model deploi model separ endpoint expens charg node base document deploi model differ type endpoint sure work let differ custom train model deploi custom contain predict endpoint specifi traffic split model send request specif model python sdk call endpoint like understand scenario rout call model base traffic split us paramet field specifi doc specifi model process request accordingli custom predict contain call end go model abl process deploi multipl model endpoint sure predict request guarante serv",
        "Question_gpt_summary_original":"The user is facing the challenge of deploying multiple custom trained models to the same endpoint in Vertex AI. Deploying each model to a separate endpoint is expensive, and the user is unsure how to send a request to a specific model when multiple models are deployed to the same endpoint. The user is concerned that some calls will end up going to a model that cannot process the request, and is seeking a solution to ensure that every prediction request is guaranteed to be served.",
        "Question_gpt_summary":"user face challeng deploi multipl custom train model endpoint deploi model separ endpoint expens user unsur send request specif model multipl model deploi endpoint user concern call end go model process request seek solut ensur predict request guarante serv",
        "Answer_original_content":"document talk us case model train featur set share ingress predict traffic understood correctli appli model train differ featur set differ model unfortun deploi differ model endpoint util node possibl moment ongo featur request work provid exact eta featur avail reproduc multi model setup notic point traffic split deploi differ model endpoint sent predict set traffic split rule saw error impli request sent wrong model cost optim multipl model deploi endpoint deploi separ independ node charg node node autosc happen model level endpoint level plausibl workaround pack model singl contain us custom http server logic send predict request appropri model achiev paramet field predict request bodi custom logic look like app post environ aip predict rout async def predict request request bodi await request json paramet bodi paramet instanc bodi instanc input asarrai instanc preprocess input preprocessor preprocess input paramet model random forest print paramet model output random forest model predict preprocess input print paramet model output decis tree model predict input return predict class name class num class num output",
        "Answer_preprocessed_content":"document talk us case model train featur set share ingress predict traffic understood correctli appli model train differ featur set differ model unfortun deploi differ model endpoint util node possibl moment ongo featur request work provid exact eta featur avail reproduc setup notic point traffic split deploi differ model endpoint sent predict set traffic split rule saw error impli request sent wrong model cost optim multipl model deploi endpoint deploi separ independ node charg node node autosc happen model level endpoint level plausibl workaround pack model singl contain us custom http server logic send predict request appropri model achiev field predict request bodi custom logic look like",
        "Answer_gpt_summary_original":"There is no direct solution to deploy multiple custom trained models to the same endpoint in Vertex AI. However, a possible workaround is to pack all the models into a single container and use a custom HTTP server logic to send prediction requests to the appropriate model using the parameters field of the prediction request body. It is important to note that deploying different models to the same endpoint utilizing only one node is not possible in Vertex AI at the moment, and there is an ongoing feature request that is being worked on. Additionally, when multiple models are deployed to the same endpoint, they are deployed to separate, independent nodes, and node autoscaling happens at the model level, not at the endpoint level.",
        "Answer_gpt_summary":"direct solut deploi multipl custom train model endpoint possibl workaround pack model singl contain us custom http server logic send predict request appropri model paramet field predict request bodi import note deploi differ model endpoint util node possibl moment ongo featur request work addition multipl model deploi endpoint deploi separ independ node node autosc happen model level endpoint level"
    },
    {
        "Question_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Question_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1607681930000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":65.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sage-maker-manifest-files-enable-dataset-versioning",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1607684202000,
        "Question_original_content":"manifest file enabl dataset version algorithm train manifest json file store map imag amazon arn metadata label great option manifest file smaller dataset manifest file small easili version tool save model artifact appear best construct enabl exact dataset version exclud creation uniqu train set hard copi train job scale larg dataset understand accur",
        "Question_preprocessed_content":"manifest file enabl dataset version algorithm train manifest json file store map imag amazon arn metadata label great option manifest file smaller dataset manifest file small easili version tool save model artifact appear best construct enabl exact dataset version exclud creation uniqu train set hard copi train job scale larg dataset understand accur",
        "Question_gpt_summary_original":"The user is trying to determine if Amazon SageMaker manifest files enable dataset versioning. They have found that manifest files are a good option because they are smaller than the dataset itself and can be easily used in versioning tools or saved as part of the model artifact. The user is questioning if this is the best way to enable exact dataset versioning within SageMaker, as creating a unique training set hard copy per training job is not scalable for large datasets.",
        "Question_gpt_summary":"user try determin manifest file enabl dataset version manifest file good option smaller dataset easili version tool save model artifact user question best wai enabl exact dataset version creat uniqu train set hard copi train job scalabl larg dataset",
        "Answer_original_content":"creat condit immut asset manifest point manifest enabl exact dataset version data store amazon version data asset us manifest file creat version dataset specif usag guarante immut asset manifest point manifest invalid",
        "Answer_preprocessed_content":"creat condit immut asset manifest point manifest enabl exact dataset version data store amazon version data asset us manifest file creat version dataset specif usag guarante immut asset manifest point manifest invalid",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that Amazon SageMaker manifest files can enable exact dataset versioning if the assets that the manifest points to are immutable. To ensure immutability, a data store in Amazon S3 can be used to store all versions of the data assets, and the manifest files can be used for creating and versioning datasets for specific usage. However, if the assets are not immutable, the manifest becomes invalid.",
        "Answer_gpt_summary":"solut discuss suggest manifest file enabl exact dataset version asset manifest point immut ensur immut data store amazon store version data asset manifest file creat version dataset specif usag asset immut manifest invalid"
    },
    {
        "Question_title":"Google translate API gave a mixed language translation result?",
        "Question_body":"I am using Google Translate API for translating a Japanese sentence to Portuguese.On July 6 to 8, the translation result was a sentence of mixed English and Portuguese words, but on July 9 the result seems to be a correct Portuguese sentence.Was there any event on July 6 to 8 such that Google Translate API gave a mixed language translation result?Thank you for your time. ",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1657498980000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":105.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"The issue was generated due our service update and multiple service languages including Portuguese were affected. Our Translate API engineer team detected the root cause of the issue, and it was mitigated by a roll back recently performed.\n\nOur engineers confirmed that the issue is officially mitigated and you should not be experiencing any service misbehavior at this point.\n\nWe apologize for any inconvenience this may have caused on your operation.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-translate-API-gave-a-mixed-language-translation-result\/td-p\/440841\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1657726380000,
        "Question_original_content":"googl translat api gave mix languag translat result googl translat api translat japanes sentenc portugues juli translat result sentenc mix english portugues word juli result correct portugues sentenc event juli googl translat api gave mix languag translat result thank time",
        "Question_preprocessed_content":"googl translat api gave mix languag translat result googl translat api translat japanes sentenc juli translat result sentenc mix english portugues word juli result correct portugues event juli googl translat api gave mix languag translat result thank time",
        "Question_gpt_summary_original":"The user encountered a challenge with the Google Translate API when translating a Japanese sentence to Portuguese. The translation result on July 6 to 8 was a sentence with mixed English and Portuguese words, but on July 9, the result was a correct Portuguese sentence. The user is inquiring if there was any event on July 6 to 8 that caused the mixed language translation result.",
        "Question_gpt_summary":"user encount challeng googl translat api translat japanes sentenc portugues translat result juli sentenc mix english portugues word juli result correct portugues sentenc user inquir event juli caus mix languag translat result",
        "Answer_original_content":"issu gener servic updat multipl servic languag includ portugues affect translat api engin team detect root caus issu mitig roll recent perform engin confirm issu offici mitig experienc servic misbehavior point apolog inconveni caus oper view solut origin post",
        "Answer_preprocessed_content":"issu gener servic updat multipl servic languag includ portugues affect translat api engin team detect root caus issu mitig roll recent perform engin confirm issu offici mitig experienc servic misbehavior point apolog inconveni caus oper view solut origin post",
        "Answer_gpt_summary_original":"Solution: The issue was caused by a service update that affected multiple service languages including Portuguese. The Translate API engineer team detected the root cause of the issue and mitigated it by performing a roll back. The issue has been officially mitigated and users should not be experiencing any service misbehavior at this point.",
        "Answer_gpt_summary":"solut issu caus servic updat affect multipl servic languag includ portugues translat api engin team detect root caus issu mitig perform roll issu offici mitig user experienc servic misbehavior point"
    },
    {
        "Question_title":"Amazon Sagemaker Notebook instance stop execution as I logout",
        "Question_body":"<p>Currently I am exploring AWS sagemaker and I am facing a problem e.g. If I want to train my network on 1000s of epochs I cant stay active all the time. But as I logout my the notebook instance also stop execution. Is there any way to keep the instance active even after you logout ? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1541090908080,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":2598.0,
        "Poster_id":7896479.0,
        "Poster_created_time":1492699347027,
        "Poster_last_access_time":1664084212390,
        "Poster_age":null,
        "Poster_location":"Germany",
        "Poster_reputation":37.0,
        "Poster_views":35.0,
        "Answer_body":"<p>Do you mean logging out of AWS console or your laptop? Your training job should still be running on the notebook instance whether you have notebook open or not. Notebook instance will always be active until you manually stop it[1]. You can always access the notebook instance again by opening the notebook through console.<\/p>\n\n<p>[1]<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_StopNotebookInstance.html<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4842651.0,
        "Answerer_created_time":1430233500800,
        "Answerer_last_access_time":1578442340096,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":212.0,
        "Answerer_views":25.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53105741",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1541803691470,
        "Question_original_content":"notebook instanc stop execut logout current explor face problem want train network epoch stai activ time logout notebook instanc stop execut wai instanc activ logout",
        "Question_preprocessed_content":"notebook instanc stop execut logout current explor face problem want train network epoch stai activ time logout notebook instanc stop execut wai instanc activ logout",
        "Question_gpt_summary_original":"The user is facing a challenge with AWS Sagemaker as the notebook instance stops execution when they log out, which is problematic when training a network for a long time. They are seeking a solution to keep the instance active even after logging out.",
        "Question_gpt_summary":"user face challeng notebook instanc stop execut log problemat train network long time seek solut instanc activ log",
        "Answer_original_content":"mean log aw consol laptop train job run notebook instanc notebook open notebook instanc activ manual stop access notebook instanc open notebook consol http doc aw amazon com latest api stopnotebookinst html",
        "Answer_preprocessed_content":"mean log aw consol laptop train job run notebook instanc notebook open notebook instanc activ manual stop access notebook instanc open notebook consol",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the notebook instance should still be active even after logging out of the AWS console. The user can access the notebook instance again by opening the notebook through the console. Therefore, there is no need to keep the instance active after logging out.",
        "Answer_gpt_summary":"solut discuss suggest notebook instanc activ log aw consol user access notebook instanc open notebook consol need instanc activ log"
    },
    {
        "Question_title":"How do i schedule Azure Machine learning web service in Azure Scheduler?",
        "Question_body":"<p>I have published the web service from Azure Machine Learning experiment and now i want this web service to be scheduled using Azure Scheduler<\/p>\n\n<p>Can somebody please state the procedure?<\/p>\n\n<p>I got the API KEY, REQUEST\/RESPONSE and Batch Execution URI from the web service homepage.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1430975121057,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1431617131740,
        "Question_score":2,
        "Question_view_count":1301.0,
        "Poster_id":3768302.0,
        "Poster_created_time":1403541426412,
        "Poster_last_access_time":1592469887883,
        "Poster_age":null,
        "Poster_location":"Bengaluru, India",
        "Poster_reputation":191.0,
        "Poster_views":22.0,
        "Answer_body":"<p>You will need to first create a new job in the Azure management portal (<a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn495651.aspx\" rel=\"nofollow\">https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn495651.aspx<\/a>), where you can configure the URL and the HTTP method to POST, and specify the body. However, the initial configuration screens don't let you add any headers, so once you have created the job, go in and edit it to add the following headers:<\/p>\n\n<p>Content-Type: application\/json<br>\nAccept: application\/json<br>\nAuthorization: Bearer <\/p>\n\n<p>This will work, but am wondering if this actually serves your purpose. If you're calling the synchronous (request response) endpoint of the AzureML service, you need to specify the inputs in the request payload, which is statically configured with the Azure Scheduler job. So you will effectively be repeating the same call over and over again. You may also want to explore <a href=\"http:\/\/azure.microsoft.com\/en-us\/services\/data-factory\/\" rel=\"nofollow\">Azure Data Factory<\/a> if your needs are served by calling the asynchronous (batch) endpoint of the AzureML service.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":1153282.0,
        "Answerer_created_time":1326780552283,
        "Answerer_last_access_time":1509114686287,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":71.0,
        "Answerer_views":13.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30092360",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1431016140103,
        "Question_original_content":"schedul web servic azur schedul publish web servic experi want web servic schedul azur schedul somebodi state procedur got api kei request respons batch execut uri web servic homepag",
        "Question_preprocessed_content":"schedul web servic azur schedul publish web servic experi want web servic schedul azur schedul somebodi state procedur got api kei batch execut uri web servic homepag",
        "Question_gpt_summary_original":"The user is facing a challenge in scheduling an Azure Machine Learning web service using Azure Scheduler. They have already published the web service and obtained the necessary API key, request\/response, and batch execution URI. The user is seeking guidance on the procedure for scheduling the web service.",
        "Question_gpt_summary":"user face challeng schedul web servic azur schedul publish web servic obtain necessari api kei request respons batch execut uri user seek guidanc procedur schedul web servic",
        "Answer_original_content":"need creat new job azur manag portal http msdn microsoft com librari azur aspx configur url http method post specifi bodi initi configur screen let add header creat job edit add follow header content type applic json accept applic json author bearer work wonder actual serv purpos call synchron request respons endpoint servic need specifi input request payload static configur azur schedul job effect repeat want explor azur data factori need serv call asynchron batch endpoint servic",
        "Answer_preprocessed_content":"need creat new job azur manag portal configur url http method post specifi bodi initi configur screen let add header creat job edit add follow header accept author bearer work wonder actual serv purpos call synchron endpoint servic need specifi input request payload static configur azur schedul job effect repeat want explor azur data factori need serv call asynchron endpoint servic",
        "Answer_gpt_summary_original":"Solution:\n- Create a new job in the Azure management portal and configure the URL and HTTP method to POST, and specify the body.\n- Edit the job to add the necessary headers: Content-Type, Accept, and Authorization.\n- If calling the synchronous endpoint, specify the inputs in the request payload, which is statically configured with the Azure Scheduler job.\n- Consider using Azure Data Factory if the needs are served by calling the asynchronous endpoint.",
        "Answer_gpt_summary":"solut creat new job azur manag portal configur url http method post specifi bodi edit job add necessari header content type accept author call synchron endpoint specifi input request payload static configur azur schedul job consid azur data factori need serv call asynchron endpoint"
    },
    {
        "Question_title":"What is SageMaker Autopilot doing when in state \"InProgress - AnalyzingData\" ?",
        "Question_body":"Hi,\n\nI'm trying this nice SageMaker Autopilot demo https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/autopilot\/autopilot_customer_churn_high_level_with_evaluation.ipynb\n\nAt the beginning of the job, the status is \"InProgress - AnalyzingData\" for several minutes. This is long enough that I'd like to know more about it: what is Autopilot doing when at that status?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1596036787000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":24.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"There are some metrics begin collected in this stage. To understanding what is doing is the same as what happens when you're using tensorflow autoML. There's a deep explanation what is does in our Science page https:\/\/www.amazon.science\/publications\/amazon-sagemaker-autopilot-a-white-box-automl-solution-at-scale",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU8QUiTTSMQ2W2uOgHXC7lqA\/what-is-sage-maker-autopilot-doing-when-in-state-in-progress-analyzing-data",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1597886708000,
        "Question_original_content":"autopilot state inprogress analyzingdata try nice autopilot demo http github com awslab amazon exampl blob master autopilot autopilot custom churn high level evalu ipynb begin job statu inprogress analyzingdata minut long like know autopilot statu",
        "Question_preprocessed_content":"autopilot state inprogress analyzingdata try nice autopilot demo begin job statu inprogress analyzingdata minut long like know autopilot statu",
        "Question_gpt_summary_original":"The user is encountering a challenge with the SageMaker Autopilot demo where the status remains \"InProgress - AnalyzingData\" for several minutes at the beginning of the job. The user is seeking information on what Autopilot is doing during this stage.",
        "Question_gpt_summary":"user encount challeng autopilot demo statu remain inprogress analyzingdata minut begin job user seek inform autopilot stage",
        "Answer_original_content":"metric begin collect stage understand happen tensorflow automl deep explan scienc page http amazon scienc public amazon autopilot white box automl solut scale",
        "Answer_preprocessed_content":"metric begin collect stage understand happen tensorflow automl deep explan scienc page",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to Amazon's Science page that explains what Autopilot is doing during the \"InProgress - AnalyzingData\" stage. The page provides a deep explanation of what Autopilot does and how it works.",
        "Answer_gpt_summary":"solut discuss provid link amazon scienc page explain autopilot inprogress analyzingdata stage page provid deep explan autopilot work"
    },
    {
        "Question_title":"Cannot read \".parquet\" files in Azure Jupyter Notebook (Python 2 and 3)",
        "Question_body":"<p>I am currently trying to open parquet files using Azure Jupyter Notebooks. I have tried both Python kernels (2 and 3).\nAfter the installation of <em>pyarrow<\/em> I can import the module only if the Python kernel is 2 (not working with Python 3)<\/p>\n\n<p>Here is what I've done so far (for clarity, I am not mentioning all my various attempts, such as using <em>conda<\/em> instead of <em>pip<\/em>, as it also failed):<\/p>\n\n<pre><code>!pip install --upgrade pip\n!pip install -I Cython==0.28.5\n!pip install pyarrow\n\nimport pandas  \nimport pyarrow\nimport pyarrow.parquet\n\n#so far, so good\n\nfilePath_parquet = \"foo.parquet\"\ntable_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n<\/code><\/pre>\n\n<p>This works well if I'm doing that off-line (using Spyder, Python v.3.7.0). But it fails using an Azure Notebook.<\/p>\n\n<pre><code> AttributeErrorTraceback (most recent call last)\n&lt;ipython-input-54-2739da3f2d20&gt; in &lt;module&gt;()\n      6 \n      7 #table_parquet_raw = pd.read_parquet(filePath_parquet, engine='pyarrow')\n----&gt; 8 table_parquet_raw = pandas.read_parquet(filePath_parquet, engine='pyarrow')\n\nAttributeError: 'module' object has no attribute 'read_parquet'\n<\/code><\/pre>\n\n<p>Any idea please?<\/p>\n\n<p>Thank you in advance !<\/p>\n\n<p>EDIT:<\/p>\n\n<p>Thank you very much for your reply Peter Pan !\nI have typed these  statements, here is what I got:<\/p>\n\n<p>1.<\/p>\n\n<pre><code>    print(pandas.__dict__)\n<\/code><\/pre>\n\n<p>=> read_parquet does not appear<\/p>\n\n<p>2.<\/p>\n\n<pre><code>    print(pandas.__file__)\n<\/code><\/pre>\n\n<p>=> I get:<\/p>\n\n<pre><code>    \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/pandas\/__init__.py\n<\/code><\/pre>\n\n<ol start=\"3\">\n<li><p>import sys; print(sys.path) => I get:<\/p>\n\n<pre><code>['', '\/home\/nbuser\/anaconda3_23\/lib\/python34.zip',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/plat-linux',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/lib-dynload',\n'\/home\/nbuser\/.local\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/Sphinx-1.3.1-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg',\n'\/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\/IPython\/extensions',\n'\/home\/nbuser\/.ipython']\n<\/code><\/pre><\/li>\n<\/ol>\n\n<p>Do you have any idea please ?<\/p>\n\n<p>EDIT 2:<\/p>\n\n<p>Dear @PeterPan, I have typed both <code>!conda update conda<\/code> and  <code>!conda update pandas<\/code> : when checking the Pandas version (<code>pandas.__version__<\/code>), it is still <code>0.19.2<\/code>.<\/p>\n\n<p>I have also tried with <code>!conda update pandas -y -f<\/code>, it returns:\n`Fetching package metadata ...........\nSolving package specifications: .<\/p>\n\n<p>Package plan for installation in environment \/home\/nbuser\/anaconda3_23:<\/p>\n\n<p>The following NEW packages will be INSTALLED:<\/p>\n\n<pre><code>pandas: 0.19.2-np111py34_1`\n<\/code><\/pre>\n\n<p>When typing:\n<code>!pip install --upgrade pandas<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Requirement already up-to-date: pandas in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages\nRequirement already up-to-date: pytz&gt;=2011k in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: numpy&gt;=1.9.0 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: python-dateutil&gt;=2 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from pandas)\nRequirement already up-to-date: six&gt;=1.5 in \/home\/nbuser\/anaconda3_23\/lib\/python3.4\/site-packages (from python-dateutil&gt;=2-&gt;pandas)<\/code><\/p>\n\n<p>Finally, when typing:<\/p>\n\n<p><code>!pip install --upgrade pandas==0.24.0<\/code><\/p>\n\n<p>I get:<\/p>\n\n<p><code>Collecting pandas==0.24.0\n  Could not find a version that satisfies the requirement pandas==0.24.0 (from versions: 0.1, 0.2b0, 0.2b1, 0.2, 0.3.0b0, 0.3.0b2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0rc1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0rc1, 0.8.0rc2, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0rc1, 0.19.0, 0.19.1, 0.19.2, 0.20.0rc1, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0rc1, 0.21.0, 0.21.1, 0.22.0)\nNo matching distribution found for pandas==0.24.0<\/code><\/p>\n\n<p>Therefore, my guess is that the problem comes from the way the packages are managed in Azure. Updating a package (here Pandas), should lead to an update to the latest version available, shouldn't it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1545322944093,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1547188516027,
        "Question_score":2,
        "Question_view_count":1879.0,
        "Poster_id":10816666.0,
        "Poster_created_time":1545322329030,
        "Poster_last_access_time":1556103635952,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":25.0,
        "Poster_views":5.0,
        "Answer_body":"<p>I tried to reproduce your issue on my Azure Jupyter Notebook, but failed. There was no any issue for me without doing your two steps <code>!pip install --upgrade pip<\/code> &amp; <code>!pip install -I Cython==0.28.5<\/code> which I think not matter.<\/p>\n\n<p>Please run some codes below to check your import package <code>pandas<\/code> whether be correct.<\/p>\n\n<ol>\n<li>Run <code>print(pandas.__dict__)<\/code> to check whether has the description of <code>read_parquet<\/code> function in the output.<\/li>\n<li>Run <code>print(pandas.__file__)<\/code> to check whether you imported a different <code>pandas<\/code> package.<\/li>\n<li>Run <code>import sys; print(sys.path)<\/code> to check the order of paths whether there is a same named file or directory under these paths.<\/li>\n<\/ol>\n\n<p>If there is a same file or directory named <code>pandas<\/code>, you just need to rename it and restart your <code>ipynb<\/code> to re-run. It's a common issue which you can refer to these SO threads <a href=\"https:\/\/stackoverflow.com\/questions\/35341363\/attributeerror-module-object-has-no-attribute-reader\">AttributeError: &#39;module&#39; object has no attribute &#39;reader&#39;<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/36250353\/importing-installed-package-from-script-raises-attributeerror-module-has-no-at\">Importing installed package from script raises &quot;AttributeError: module has no attribute&quot; or &quot;ImportError: cannot import name&quot;<\/a>.<\/p>\n\n<p>In Other cases, please update your post for more details to let me know.<\/p>\n\n<hr>\n\n<p>The latest <code>pandas<\/code> version should be <code>0.23.4<\/code>, not <code>0.24.0<\/code>.<\/p>\n\n<p>I tried to find out the earliest version of <code>pandas<\/code> which support the <code>read_parquet<\/code> feature via search the function name <code>read_parquet<\/code> in the documents of different version from <code>0.19.2<\/code> to <code>0.23.3<\/code>. Then, I found <code>pandas<\/code> supports <code>read_parquet<\/code> feature after the version <code>0.21.1<\/code>, as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a6Jl9.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The new features shown in the <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/whatsnew.html\" rel=\"nofollow noreferrer\"><code>What's New<\/code><\/a> of version <code>0.21.1<\/code>\n<a href=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/cuSOe.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>According to your <code>EDIT 2<\/code> description, it seems that you are using Python 3.4 in Azure Jupyter Notebook. Not all <code>pandas<\/code> versions support Python 3.4 version.<\/p>\n\n<p>The versions <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.21\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.21.1<\/code><\/a> &amp; <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/version\/0.22\/install.html#python-version-support\" rel=\"nofollow noreferrer\"><code>0.22.0<\/code><\/a> offically support Python 2.7,3.5, and 3.6, as below.\n<a href=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fM9RT.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>And the <a href=\"https:\/\/pypi.org\/project\/pandas\/\" rel=\"nofollow noreferrer\">PyPI page for <code>pandas<\/code><\/a> also requires the Python version as below.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/6613J.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6613J.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>So you can try to install the <code>pandas<\/code> versions <code>0.21.1<\/code> &amp; <code>0.22.0<\/code> in the current notebook of Python 3.4. if failed, please create a new notebook in Python <code>2.7<\/code> or <code>&gt;=3.5<\/code> to install <code>pandas<\/code> version <code>&gt;= 0.21.1<\/code> to use the function <code>read_parquet<\/code>.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":1547190062312,
        "Answer_score":1.0,
        "Answerer_id":4989676.0,
        "Answerer_created_time":1433841188323,
        "Answerer_last_access_time":1662775294983,
        "Answerer_age":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation":22467.0,
        "Answerer_views":2692.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53872444",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1545730275287,
        "Question_original_content":"read parquet file azur jupyt notebook python current try open parquet file azur jupyt notebook tri python kernel instal pyarrow import modul python kernel work python far clariti mention attempt conda instead pip fail pip instal upgrad pip pip instal cython pip instal pyarrow import panda import pyarrow import pyarrow parquet far good filepath parquet foo parquet tabl parquet raw panda read parquet filepath parquet engin pyarrow work line spyder python fail azur notebook attributeerrortraceback recent tabl parquet raw read parquet filepath parquet engin pyarrow tabl parquet raw panda read parquet filepath parquet engin pyarrow attributeerror modul object attribut read parquet idea thank advanc edit thank repli peter pan type statement got print panda dict read parquet appear print panda file home nbuser anaconda lib python site packag panda init import sy print sy path home nbuser anaconda lib python zip home nbuser anaconda lib python home nbuser anaconda lib python plat linux home nbuser anaconda lib python lib dynload home nbuser local lib python site packag home nbuser anaconda lib python site packag home nbuser anaconda lib python site packag sphinx egg home nbuser anaconda lib python site packag setuptool egg home nbuser anaconda lib python site packag ipython extens home nbuser ipython idea edit dear peterpan type conda updat conda conda updat panda check panda version panda version tri conda updat panda return fetch packag metadata solv packag specif packag plan instal environ home nbuser anaconda follow new packag instal panda nppy type pip instal upgrad panda requir date panda home nbuser anaconda lib python site packag requir date pytz home nbuser anaconda lib python site packag panda requir date numpi home nbuser anaconda lib python site packag panda requir date python dateutil home nbuser anaconda lib python site packag panda requir date home nbuser anaconda lib python site packag python dateutil panda final type pip instal upgrad panda collect panda version satisfi requir panda version match distribut panda guess problem come wai packag manag azur updat packag panda lead updat latest version avail shouldn",
        "Question_preprocessed_content":"read file azur jupyt notebook current try open parquet file azur jupyt notebook tri python kernel instal pyarrow import modul python kernel far work fail azur notebook idea thank advanc edit thank repli peter pan type statement got appear import sy idea edit dear type check panda version tri return fetch packag metadata solv packag specif packag plan instal environ follow new packag instal type final type guess problem come wai packag manag azur updat packag lead updat latest version avail shouldn",
        "Question_gpt_summary_original":"The user is facing challenges in opening \".parquet\" files using Azure Jupyter Notebooks with Python kernels 2 and 3. After installing \"pyarrow\", the module can only be imported with Python kernel 2 and not with Python kernel 3. The user has tried various attempts, including using \"conda\" instead of \"pip\", but it failed. The user has also tried updating the \"pandas\" package, but it did not solve the issue. The user suspects that the problem may be due to the way packages are managed in Azure.",
        "Question_gpt_summary":"user face challeng open parquet file azur jupyt notebook python kernel instal pyarrow modul import python kernel python kernel user tri attempt includ conda instead pip fail user tri updat panda packag solv issu user suspect problem wai packag manag azur",
        "Answer_original_content":"tri reproduc issu azur jupyt notebook fail issu step pip instal upgrad pip pip instal cython think matter run code check import packag panda correct run print panda dict check descript read parquet function output run print panda file check import differ panda packag run import sy print sy path check order path name file directori path file directori name panda need renam restart ipynb run common issu refer thread attributeerror modul object attribut reader import instal packag script rais attributeerror modul attribut importerror import case updat post detail let know latest panda version tri earliest version panda support read parquet featur search function read parquet document differ version panda support read parquet featur version new featur shown new version accord edit descript python azur jupyt notebook panda version support python version version offic support python pypi page panda requir python version try instal panda version current notebook python fail creat new notebook python instal panda version us function read parquet",
        "Answer_preprocessed_content":"tri reproduc issu azur jupyt notebook fail issu step think matter run code check import packag correct run check descript function output run check import differ packag run check order path name file directori path file directori name need renam restart common issu refer thread attributeerror modul object attribut reader import instal packag script rais attributeerror modul attribut importerror import case updat post detail let know latest version tri earliest version support featur search function document differ version support featur version new featur shown version accord descript python azur jupyt notebook version support python version version offic support python pypi page requir python version try instal version current notebook python fail creat new notebook python instal version us function",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Check the import package \"pandas\" using the following codes:\n  - `print(pandas.__dict__)` to check whether the description of \"read_parquet\" function is in the output.\n  - `print(pandas.__file__)` to check whether a different \"pandas\" package is imported.\n  - `import sys; print(sys.path)` to check the order of paths whether there is a same named file or directory under these paths.\n- Rename the same file or directory named \"pandas\" and restart the \"ipynb\" to re-run.\n- Install the \"pandas\" versions \"0.21.1\" & \"0.22.0\" in the",
        "Answer_gpt_summary":"possibl solut mention discuss check import packag panda follow code print panda dict check descript read parquet function output print panda file check differ panda packag import import sy print sy path check order path name file directori path renam file directori name panda restart ipynb run instal panda version"
    },
    {
        "Question_title":"AWS Sagemaker | how to train text data | For ticket classification",
        "Question_body":"<p>I am new to Sagemaker and not sure how to classify the text input in AWS sagemaker, <\/p>\n\n<p>Suppose I have a Dataframe having two fields like 'Ticket' and 'Category', Both are text input, Now I want to split it test and training set and upload in Sagemaker training model. <\/p>\n\n<pre><code>X_train, X_test, y_train, y_test = model_selection.train_test_split(fewRecords['Ticket'],fewRecords['Category'])\n<\/code><\/pre>\n\n<p>Now as I want to perform TD-IDF feature extraction and then convert it to numeric value, so performing this operation<\/p>\n\n<pre><code>tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\ntfidf_vect.fit(fewRecords['Category'])\nxtrain_tfidf =  tfidf_vect.transform(X_train)\nxvalid_tfidf =  tfidf_vect.transform(X_test)\n<\/code><\/pre>\n\n<p>When I want to upload the model in Sagemaker so I can perform next operation like <\/p>\n\n<pre><code>buf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\nbuf.seek(0)\n<\/code><\/pre>\n\n<p>I am getting this error <\/p>\n\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-36-8055e6cdbf34&gt; in &lt;module&gt;()\n      1 buf = io.BytesIO()\n----&gt; 2 smac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n      3 buf.seek(0)\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in write_numpy_to_dense_tensor(file, array, labels)\n     98             raise ValueError(\"Label shape {} not compatible with array shape {}\".format(\n     99                              labels.shape, array.shape))\n--&gt; 100         resolved_label_type = _resolve_type(labels.dtype)\n    101     resolved_type = _resolve_type(array.dtype)\n    102 \n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker\/amazon\/common.py in _resolve_type(dtype)\n    205     elif dtype == np.dtype('float32'):\n    206         return 'Float32'\n--&gt; 207     raise ValueError('Unsupported dtype {} on array'.format(dtype))\n\nValueError: Unsupported dtype object on array\n<\/code><\/pre>\n\n<p>Other than this exception, I am not clear if this is right way as TfidfVectorizer convert the series to Matrix.<\/p>\n\n<p>The code is predicting fine on my local machine but not sure how to do the same on Sagemaker, All the example mentioned there are too lengthy and not for the person who still reached to SciKit Learn <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1535524538620,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1535540817292,
        "Question_score":0,
        "Question_view_count":1519.0,
        "Poster_id":8388707.0,
        "Poster_created_time":1501403168107,
        "Poster_last_access_time":1663685130870,
        "Poster_age":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation":1370.0,
        "Poster_views":125.0,
        "Answer_body":"<p>The output of <code>TfidfVectorizer<\/code> is a scipy sparse matrix, not a simple numpy array.<\/p>\n<p>So either use a different function like:<\/p>\n<blockquote>\n<p><a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/amazon\/common.py#L113\" rel=\"nofollow noreferrer\">write_spmatrix_to_sparse_tensor<\/a><\/p>\n<p>&quot;&quot;&quot;Writes a scipy sparse matrix to a sparse tensor&quot;&quot;&quot;<\/p>\n<\/blockquote>\n<p>See <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/27\" rel=\"nofollow noreferrer\">this issue<\/a> for more details.<\/p>\n<p><strong>OR<\/strong> first convert the output of <code>TfidfVectorizer<\/code> to a dense numpy array and then use your above code<\/p>\n<pre><code>xtrain_tfidf =  tfidf_vect.transform(X_train).toarray()   \nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, xtrain_tfidf, y_train)\n...\n...\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1592644375060,
        "Answer_score":1.0,
        "Answerer_id":3374996.0,
        "Answerer_created_time":1393852033260,
        "Answerer_last_access_time":1663951895503,
        "Answerer_age":null,
        "Answerer_location":"India",
        "Answerer_reputation":33599.0,
        "Answerer_views":6250.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52070950",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1535540801190,
        "Question_original_content":"train text data ticket classif new sure classifi text input suppos datafram have field like ticket categori text input want split test train set upload train model train test train test model select train test split fewrecord ticket fewrecord categori want perform idf featur extract convert numer valu perform oper tfidf vect tfidfvector analyz word token pattern max featur tfidf vect fit fewrecord categori xtrain tfidf tfidf vect transform train xvalid tfidf tfidf vect transform test want upload model perform oper like buf bytesio smac write numpi dens tensor buf xtrain tfidf train buf seek get error valueerror traceback recent buf bytesio smac write numpi dens tensor buf xtrain tfidf train buf seek anaconda env python lib python site packag amazon common write numpi dens tensor file arrai label rais valueerror label shape compat arrai shape format label shape arrai shape resolv label type resolv type label dtype resolv type resolv type arrai dtype anaconda env python lib python site packag amazon common resolv type dtype elif dtype dtype float return float rais valueerror unsupport dtype arrai format dtype valueerror unsupport dtype object arrai except clear right wai tfidfvector convert seri matrix code predict fine local machin sure exampl mention lengthi person reach scikit learn",
        "Question_preprocessed_content":"train text data ticket classif new sure classifi text input suppos datafram have field like ticket categori text input want split test train set upload train model want perform featur extract convert numer valu perform oper want upload model perform oper like get error except clear right wai tfidfvector convert seri matrix code predict fine local machin sure exampl mention lengthi person reach scikit learn",
        "Question_gpt_summary_original":"The user is facing challenges in classifying text input in AWS Sagemaker. They are trying to split a Dataframe into test and training sets, perform TD-IDF feature extraction, and convert it to numeric value. However, they are encountering an error when trying to upload the model in Sagemaker. The error message indicates an unsupported dtype object on the array. The user is also unsure if their approach is correct as TfidfVectorizer converts the series to a matrix. They are finding the examples provided by Sagemaker too lengthy and not suitable for someone new to SciKit Learn.",
        "Question_gpt_summary":"user face challeng classifi text input try split datafram test train set perform idf featur extract convert numer valu encount error try upload model error messag indic unsupport dtype object arrai user unsur approach correct tfidfvector convert seri matrix find exampl provid lengthi suitabl new scikit learn",
        "Answer_original_content":"output tfidfvector scipi spars matrix simpl numpi arrai us differ function like write spmatrix spars tensor write scipi spars matrix spars tensor issu detail convert output tfidfvector dens numpi arrai us code xtrain tfidf tfidf vect transform train toarrai buf bytesio smac write numpi dens tensor buf xtrain tfidf train",
        "Answer_preprocessed_content":"output scipi spars matrix simpl numpi arrai us differ function like write scipi spars matrix spars tensor issu detail convert output dens numpi arrai us code",
        "Answer_gpt_summary_original":"Two possible solutions were provided in the discussion. The first solution is to use a different function called \"write_spmatrix_to_sparse_tensor\" to write a scipy sparse matrix to a sparse tensor. The second solution is to first convert the output of TfidfVectorizer to a dense numpy array and then use the above code. The user was also informed that the output of TfidfVectorizer is a scipy sparse matrix, not a simple numpy array.",
        "Answer_gpt_summary":"possibl solut provid discuss solut us differ function call write spmatrix spars tensor write scipi spars matrix spars tensor second solut convert output tfidfvector dens numpi arrai us code user inform output tfidfvector scipi spars matrix simpl numpi arrai"
    },
    {
        "Question_title":"Can I import data from On-Premises SQL Server Database to Azure Machine Learning virtual machine?",
        "Question_body":"<p>On the limited Azure Machine Learning Studio, one can import data from an On-Premises SQL Server Database.\nWhat about the ability to do the exact same thing on a python jupyter notebook on a virtual machine from the Azure Machine Learning Services workspace ?<\/p>\n\n<p>It does not seem possible from what I've found in the documentation.\nData sources would be limited in Azure ML Services : \"Currently, the list of supported Azure storage services that can be registered as datastores are Azure Blob Container, Azure File Share, Azure Data Lake, Azure Data Lake Gen2, Azure SQL Database, Azure PostgreSQL, and Databricks File System\"<\/p>\n\n<p>Thank you in advance for your assistance<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1558448804447,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":1147.0,
        "Poster_id":11534310.0,
        "Poster_created_time":1558447987352,
        "Poster_last_access_time":1650446213527,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":3.0,
        "Poster_views":8.0,
        "Answer_body":"<p>As of today, <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-load-data#load-sql-data\" rel=\"nofollow noreferrer\">you can load SQL data, but only a MS SQL Server source (also on-premise) is supported<\/a>.<\/p>\n\n<p>Using <code>azureml.dataprep<\/code>, code would read along the lines of<\/p>\n\n<pre><code>import azureml.dataprep as dprep\n\nsecret = dprep.register_secret(value=\"[SECRET-PASSWORD]\", id=\"[SECRET-ID]\")\n\nds = dprep.MSSQLDataSource(server_name=\"[SERVER-NAME]\",\n                           database_name=\"[DATABASE-NAME]\",\n                           user_name=\"[DATABASE-USERNAME]\",\n                           password=secret)\n\ndflow = dprep.read_sql(ds, \"SELECT top 100 * FROM [YourDB].[ATable]\")\n# print first records\ndflow.head(5)\n<\/code><\/pre>\n\n<p>As far as I understand the APIs are under heavy development and <code>azureml.dataprep<\/code> may be soon superseded by functionality provided by the <a href=\"https:\/\/aka.ms\/azureml\/concepts\/datasets\" rel=\"nofollow noreferrer\">Dataset class<\/a>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4240413.0,
        "Answerer_created_time":1415722650716,
        "Answerer_last_access_time":1664051478172,
        "Answerer_age":null,
        "Answerer_location":"Verona, VR, Italy",
        "Answerer_reputation":4811.0,
        "Answerer_views":713.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56240481",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1558479194140,
        "Question_original_content":"import data premis sql server databas virtual machin limit studio import data premis sql server databas abil exact thing python jupyt notebook virtual machin servic workspac possibl document data sourc limit servic current list support azur storag servic regist datastor azur blob contain azur file share azur data lake azur data lake gen azur sql databas azur postgresql databrick file thank advanc assist",
        "Question_preprocessed_content":"import data sql server databas virtual machin limit studio import data sql server databas abil exact thing python jupyt notebook virtual machin servic workspac possibl document data sourc limit servic current list support azur storag servic regist datastor azur blob contain azur file share azur data lake azur data lake gen azur sql databas azur postgresql databrick file thank advanc assist",
        "Question_gpt_summary_original":"The user is facing a challenge in importing data from an On-Premises SQL Server Database to a python jupyter notebook on a virtual machine from the Azure Machine Learning Services workspace. The user has found that it is not possible according to the documentation, as data sources are limited in Azure ML Services.",
        "Question_gpt_summary":"user face challeng import data premis sql server databas python jupyt notebook virtual machin servic workspac user possibl accord document data sourc limit servic",
        "Answer_original_content":"todai load sql data sql server sourc premis support dataprep code read line import dataprep dprep secret dprep regist secret valu secret password secret dprep mssqldatasourc server server databas databas user databas usernam password secret dflow dprep read sql select yourdb atabl print record dflow head far understand api heavi develop dataprep soon supersed function provid dataset class",
        "Answer_preprocessed_content":"todai load sql data sql server sourc support code read line far understand api heavi develop soon supersed function provid dataset class",
        "Answer_gpt_summary_original":"Solution: The user can use the <code>azureml.dataprep<\/code> library to load SQL data from an On-Premises SQL Server source. The code to read the data would involve importing the library, registering the secret password, defining the data source, and reading the SQL data using the <code>read_sql<\/code> function. However, it is important to note that the APIs are under heavy development and <code>azureml.dataprep<\/code> may be soon superseded by functionality provided by the <a href=\"https:\/\/aka.ms\/azureml\/concepts\/datasets\" rel=\"nofollow noreferrer\">Dataset class<\/a>.",
        "Answer_gpt_summary":"solut user us dataprep librari load sql data premis sql server sourc code read data involv import librari regist secret password defin data sourc read sql data read sql function import note api heavi develop dataprep soon supersed function provid dataset class"
    },
    {
        "Question_title":"SageMaker Experiments Deletion Help Needed",
        "Question_body":"Hi Friends,\n\nI have deleted everything in Sagemaker - but support is asking me to delete the experiments that are still in my account : they sent me a link to follow\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/experiments-cleanup.html\n\nbut I have no idea how to complete this task -- does anyone know in terms that someone who has no idea what this means - can follow and achieve this task\n\nyou have no idea how much it would mean to me for any assistance",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1668037196721,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":44.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"You need to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. If you are already using SageMaker Studio, that should work.\n\nUse the second method. Create a file (Menu File -> New -> Python File). Rename it as cleanup_experiments.py(right click on the file on top and select Rename Python File), then paste the code in the documentation (those three sections, one after another). Save the file and open a terminal (Menu File -> New -> Terminal). Navigate to the directory where you saved the file and execute the command python cleanup_experiments.py",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFMzl26gfQna8sAZcCDJw_Q\/sage-maker-experiments-deletion-help-needed",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1668070837516,
        "Question_original_content":"experi delet help need friend delet support ask delet experi account sent link follow http doc aw amazon com latest experi cleanup html idea complet task know term idea mean follow achiev task idea mean assist",
        "Question_preprocessed_content":"experi delet help need friend delet support ask delet experi account sent link follow idea complet task know term idea mean follow achiev task idea mean assist",
        "Question_gpt_summary_original":"The user has deleted everything in SageMaker but is now facing challenges in deleting experiments that are still in their account. They have been provided with a link by support to follow for guidance, but they are unsure of how to complete the task and are seeking assistance.",
        "Question_gpt_summary":"user delet face challeng delet experi account provid link support follow guidanc unsur complet task seek assist",
        "Answer_original_content":"need us python sdk instal aw credenti permiss account configur studio work us second method creat file menu file new python file renam cleanup experi right click file select renam python file past code document section save file open termin menu file new termin navig directori save file execut command python cleanup experi",
        "Answer_preprocessed_content":"need us python sdk instal aw credenti permiss account configur studio work us second method creat file renam click file select renam python file past code document save file open termin navig directori save file execut command python",
        "Answer_gpt_summary_original":"Solution: The user needs to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. They should create a file, rename it as cleanup_experiments.py, paste the code in the documentation, save the file, and open a terminal. Then, they should navigate to the directory where they saved the file and execute the command python cleanup_experiments.py.",
        "Answer_gpt_summary":"solut user need us python sdk instal aw credenti permiss account configur creat file renam cleanup experi past code document save file open termin navig directori save file execut command python cleanup experi"
    },
    {
        "Question_title":"Having a problem calling the function AudioConfig.FromWavFileInput through python library",
        "Question_body":"<p>I am trying to process a .wav file with the Azure Cognitive Speech Service. I am using the script below. I get an exception that says \"type object 'AudioConfig' has no attribute 'FromWavFileInput'\" when I try to setup the wav file by calling <a href=\"https:\/\/docs.microsoft.com\/en-us\/dotnet\/api\/microsoft.cognitiveservices.speech.audio.audioconfig.fromwavfileinput\" rel=\"nofollow noreferrer\">AudioConfig.FromWavFileInput()<\/a>. The documentation says the function exists, at least in the .net library. Does FromWaveFileInput exist for the <a href=\"https:\/\/pypi.org\/project\/azure-cognitiveservices-speech\/\" rel=\"nofollow noreferrer\">cognitiveservices-speech python library<\/a>? How can I process an audio file with python?<\/p>\n\n<pre><code>import azure.cognitiveservices.speech as speechsdk\n\nspeechKey = 'xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx'\nservice_region = 'eastus2'\n\n#### # Creates an instance of a speech config with specified subscription key and service region.\n#### # Replace with your own subscription key and service region (e.g., \"westus\").\nspeech_config = speechsdk.SpeechConfig(subscription=speechKey, region=service_region)\n\naudioInput = speechsdk.AudioConfig.FromWavFileInput('RainSpain.wav')\n\n#### # Creates a recognizer with the given settings\nspeech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_input=audioInput)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1545954446910,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1546237174287,
        "Question_score":0,
        "Question_view_count":1136.0,
        "Poster_id":4954801.0,
        "Poster_created_time":1432934372580,
        "Poster_last_access_time":1635961997472,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":5.0,
        "Poster_views":5.0,
        "Answer_body":"<p>Indeed as you said. I searched for the keywords <code>AudioConfig<\/code> &amp; <code>FromWavFileInput<\/code> on GitHub repo <a href=\"https:\/\/github.com\/Azure-Samples\/cognitive-services-speech-sdk\" rel=\"nofollow noreferrer\"><code>Azure-Samples\/cognitive-services-speech-sdk<\/code><\/a>, there is not any Python codes about it except for Java, C#, and <a href=\"https:\/\/github.com\/Azure-Samples\/cognitive-services-speech-sdk\/blob\/3131ab75577116fe9359242d6d86321808601e19\/samples\/cpp\/windows\/console\/samples\/speech_recognition_samples.cpp#L116\" rel=\"nofollow noreferrer\">C++<\/a>.<\/p>\n\n<p>So per my experience, there are two workaround ways to do it.<\/p>\n\n<ol>\n<li>Wrap the C++ codes as a <a href=\"https:\/\/docs.python.org\/3\/c-api\/index.html\" rel=\"nofollow noreferrer\">Python extension module<\/a>, or communicate with C++\/Java codes.<\/li>\n<li>Directly using <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/speech-service\/rest-apis\" rel=\"nofollow noreferrer\">Speech service REST APIs<\/a> with <a href=\"http:\/\/docs.python-requests.org\/en\/master\/\" rel=\"nofollow noreferrer\"><code>requests<\/code><\/a>, it's simple for Python and Azure Speech Service.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":4989676.0,
        "Answerer_created_time":1433841188323,
        "Answerer_last_access_time":1662775294983,
        "Answerer_age":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation":22467.0,
        "Answerer_views":2692.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53952110",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1546249441007,
        "Question_original_content":"have problem call function audioconfig fromwavfileinput python librari try process wav file azur cognit speech servic script except sai type object audioconfig attribut fromwavfileinput try setup wav file call audioconfig fromwavfileinput document sai function exist net librari fromwavefileinput exist cognitiveservic speech python librari process audio file python import azur cognitiveservic speech speechsdk speechkei servic region eastu creat instanc speech config specifi subscript kei servic region replac subscript kei servic region westu speech config speechsdk speechconfig subscript speechkei region servic region audioinput speechsdk audioconfig fromwavfileinput rainspain wav creat recogn given set speech recogn speechsdk speechrecogn speech config speech config audio input audioinput",
        "Question_preprocessed_content":"have problem call function python librari try process wav file azur cognit speech servic script except sai type object audioconfig attribut fromwavfileinput try setup wav file call document sai function exist net librari fromwavefileinput exist python librari process audio file python",
        "Question_gpt_summary_original":"The user is facing a challenge while trying to process a .wav file with the Azure Cognitive Speech Service using the cognitiveservices-speech python library. The user is unable to call the function AudioConfig.FromWavFileInput() and is receiving an exception that says \"type object 'AudioConfig' has no attribute 'FromWavFileInput'\". The user is seeking help to know if the function exists for the cognitiveservices-speech python library and how to process an audio file with python.",
        "Question_gpt_summary":"user face challeng try process wav file azur cognit speech servic cognitiveservic speech python librari user unabl function audioconfig fromwavfileinput receiv except sai type object audioconfig attribut fromwavfileinput user seek help know function exist cognitiveservic speech python librari process audio file python",
        "Answer_original_content":"said search keyword audioconfig fromwavfileinput github repo azur sampl cognit servic speech sdk python code java experi workaround wai wrap code python extens modul commun java code directli speech servic rest api request simpl python azur speech servic",
        "Answer_preprocessed_content":"said search keyword github repo python code java experi workaround wai wrap code python extens modul commun code directli speech servic rest api simpl python azur speech servic",
        "Answer_gpt_summary_original":"Solutions provided:\n- Wrap the C++ codes as a Python extension module, or communicate with C++\/Java codes.\n- Directly use Speech service REST APIs with requests, which is simple for Python and Azure Speech Service.",
        "Answer_gpt_summary":"solut provid wrap code python extens modul commun java code directli us speech servic rest api request simpl python azur speech servic"
    },
    {
        "Question_title":"Out of disk space",
        "Question_body":"<p>when running an AML pipeline on AML compute, I get this kind of error : <\/p>\n\n<p>I can try rebooting the cluster, but that may not fix the problem (if storage gets accumulated no the nodes, that should be cleaned.<\/p>\n\n<pre><code>Session ID: 933fc468-7a22-425d-aa1b-94eba5784faa\n{\"error\":{\"code\":\"ServiceError\",\"message\":\"Job preparation failed: [Errno 28] No space left on device\",\"detailsUri\":null,\"target\":null,\"details\":[],\"innerError\":null,\"debugInfo\":{\"type\":\"OSError\",\"message\":\"[Errno 28] No space left on device\",\"stackTrace\":\" File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 126, in &lt;module&gt;\\n invoke()\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 97, in invoke\\n extract_project(project_dir, options.project_zip, options.snapshots)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1-setup\/job_prep.py\\\", line 60, in extract_project\\n project_fetcher.fetch_project_snapshot(snapshot[\\\"Id\\\"], snapshot[\\\"PathStack\\\"])\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 72, in fetch_project_snapshot\\n _download_tree(sas_tree, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 106, in _download_tree\\n _download_tree(child, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 106, in _download_tree\\n _download_tree(child, path_stack)\\n File \\\"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/jj2\/azureml\/piperun-20190911_1568231788841835_1\/mounts\/workspacefilestore\/azureml\/PipeRun-20190911_1568231788841835_1\/azureml-setup\/project_fetcher.py\\\", line 98, in _download_tree\\n fh.write(response.read())\\n\",\"innerException\":null,\"data\":null,\"errorResponse\":null}},\"correlation\":null,\"environment\":null,\"location\":null,\"time\":\"0001-01-01T00:00:00+00:00\"}\n<\/code><\/pre>\n\n<p>I would expect the job to run as it should. And in fact, I've checked on the node and the node do have lots of available harddrive space :<\/p>\n\n<pre><code>root@4f57957ac829466a86bad4d4dc51fadd000001:~# df -kh                                                                                               Filesystem      Size  Used Avail Use% Mounted on\nudev             28G     0   28G   0% \/dev\ntmpfs           5.6G  9.0M  5.5G   1% \/run\n\/dev\/sda1       125G  2.8G  122G   3% \/\ntmpfs            28G     0   28G   0% \/dev\/shm\ntmpfs           5.0M     0  5.0M   0% \/run\/lock\ntmpfs            28G     0   28G   0% \/sys\/fs\/cgroup\n\/dev\/sdb1       335G  6.7G  311G   3% \/mnt\ntmpfs           5.6G     0  5.6G   0% \/run\/user\/1002\n<\/code><\/pre>\n\n<p>Suggestions on what I should check?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1568232866207,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":738.0,
        "Poster_id":10435556.0,
        "Poster_created_time":1538275960603,
        "Poster_last_access_time":1658458641830,
        "Poster_age":null,
        "Poster_location":"Montreal, QC, Canada",
        "Poster_reputation":381.0,
        "Poster_views":50.0,
        "Answer_body":"<p>Seems like you've run into Azure file share constraints. You can use the following sample code to change your runs to use blob storage which can scale to large number of jobs running in parallel:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#accessing-source-code-during-training\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-access-data#accessing-source-code-during-training<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":8807958.0,
        "Answerer_created_time":1508520702036,
        "Answerer_last_access_time":1631844492368,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":36.0,
        "Answerer_views":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57896195",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1568309206067,
        "Question_original_content":"disk space run aml pipelin aml comput kind error try reboot cluster fix problem storag get accumul node clean session aab ebafaa error code serviceerror messag job prepar fail errno space left devic detailsuri null target null detail innererror null debuginfo type oserror messag errno space left devic stacktrac file mnt batch task share root job piperun mount workspacefilestor piperun setup job prep line invok file mnt batch task share root job piperun mount workspacefilestor piperun setup job prep line invok extract project project dir option project zip option snapshot file mnt batch task share root job piperun mount workspacefilestor piperun setup job prep line extract project project fetcher fetch project snapshot snapshot snapshot pathstack file mnt batch task share root job piperun mount workspacefilestor piperun setup project fetcher line fetch project snapshot download tree sa tree path stack file mnt batch task share root job piperun mount workspacefilestor piperun setup project fetcher line download tree download tree child path stack file mnt batch task share root job piperun mount workspacefilestor piperun setup project fetcher line download tree download tree child path stack file mnt batch task share root job piperun mount workspacefilestor piperun setup project fetcher line download tree write respons read innerexcept null data null errorrespons null correl null environ null locat null time expect job run fact check node node lot avail harddriv space root facabadddcfadd filesystem size avail us mount udev dev tmpf run dev sda tmpf dev shm tmpf run lock tmpf sy cgroup dev sdb mnt tmpf run user suggest check",
        "Question_preprocessed_content":"disk space run aml pipelin aml comput kind error try reboot cluster fix problem storag get accumul node clean expect job run fact check node node lot avail harddriv space suggest check",
        "Question_gpt_summary_original":"The user is encountering an error message indicating that there is no space left on the device when running an AML pipeline on AML compute. The user has checked the node and found that there is available hard drive space. The user is unsure of what to check next.",
        "Question_gpt_summary":"user encount error messag indic space left devic run aml pipelin aml comput user check node avail hard drive space user unsur check",
        "Answer_original_content":"like run azur file share constraint us follow sampl code chang run us blob storag scale larg number job run parallel http doc microsoft com azur machin learn servic access data access sourc code train",
        "Answer_preprocessed_content":"like run azur file share constraint us follow sampl code chang run us blob storag scale larg number job run parallel",
        "Answer_gpt_summary_original":"Solution: The user can use the sample code provided in the discussion to change their runs to use blob storage instead of Azure file share, which can scale to a large number of jobs running in parallel.",
        "Answer_gpt_summary":"solut user us sampl code provid discuss chang run us blob storag instead azur file share scale larg number job run parallel"
    },
    {
        "Question_title":"AWS NoCredentials in training",
        "Question_body":"<p>I am attempting to run the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mxnet_mnist_with_gluon_local_mode.ipynb\" rel=\"nofollow noreferrer\">example code<\/a> for Amazon Sagemaker on a local GPU.  I have copied the code from the Jupyter notebook to the following Python script:<\/p>\n\n<pre><code>import boto3\nimport subprocess\nimport sagemaker\nfrom sagemaker.mxnet import MXNet\nfrom mxnet import gluon\nfrom sagemaker import get_execution_role\nimport os\n\nsagemaker_session = sagemaker.Session()\ninstance_type = 'local'\nif subprocess.call('nvidia-smi') == 0:\n    # Set type to GPU if one is present\n    instance_type = 'local_gpu'\n# role = get_execution_role()\n\ngluon.data.vision.MNIST('.\/data\/train', train=True)\ngluon.data.vision.MNIST('.\/data\/test', train=False)\n\n# successfully connects and uploads data\ninputs = sagemaker_session.upload_data(path='data', key_prefix='data\/mnist')\n\nhyperparameters = {\n    'batch_size': 100,\n    'epochs': 20,\n    'learning_rate': 0.1,\n    'momentum': 0.9,\n    'log_interval': 100\n}\n\nm = MXNet(\"mnist.py\",\n          role=role,\n          train_instance_count=1,\n          train_instance_type=instance_type,\n          framework_version=\"1.1.0\",\n          hyperparameters=hyperparameters)\n\n# fails in Docker container\nm.fit(inputs)\npredictor = m.deploy(initial_instance_count=1, instance_type=instance_type)\nm.delete_endpoint()\n<\/code><\/pre>\n\n<p>where the referenced <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_mnist\/mnist.py\" rel=\"nofollow noreferrer\">mnist.py<\/a> file is exactly as specified on Github. The script fails on <code>m.fit<\/code> in Docker container with the following error: <\/p>\n\n<pre><code>algo-1-1DUU4_1  | Downloading s3:\/\/&lt;S3-BUCKET&gt;\/sagemaker-mxnet-2018-10-07-00-47-10-435\/source\/sourcedir.tar.gz to \/tmp\/script.tar.gz\nalgo-1-1DUU4_1  | 2018-10-07 00:47:29,219 ERROR - container_support.training - uncaught exception during training: Unable to locate credentials\nalgo-1-1DUU4_1  | Traceback (most recent call last):\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/training.py\", line 36, in start\nalgo-1-1DUU4_1  |     fw.train()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/mxnet_container\/train.py\", line 169, in train\nalgo-1-1DUU4_1  |     mxnet_env.download_user_module()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/environment.py\", line 89, in download_user_module\nalgo-1-1DUU4_1  |     cs.download_s3_resource(self.user_script_archive, tmp)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/container_support\/utils.py\", line 37, in download_s3_resource\nalgo-1-1DUU4_1  |     script_bucket.download_file(script_key_name, target)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 246, in bucket_download_file\nalgo-1-1DUU4_1  |     ExtraArgs=ExtraArgs, Callback=Callback, Config=Config)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/inject.py\", line 172, in download_file\nalgo-1-1DUU4_1  |     extra_args=ExtraArgs, callback=Callback)\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/boto3\/s3\/transfer.py\", line 307, in download_file\nalgo-1-1DUU4_1  |     future.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 73, in result\nalgo-1-1DUU4_1  |     return self._coordinator.result()\nalgo-1-1DUU4_1  |   File \"\/usr\/local\/lib\/python2.7\/dist-packages\/s3transfer\/futures.py\", line 233, in result\nalgo-1-1DUU4_1  |     raise self._exception\nalgo-1-1DUU4_1  | NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>\n\n<p>I am confused that I can authenticate to S3 outside of the container (to pload the training\/test data) but I cannot within the Docker container.  So I am guessing the issues has to do with passing the AWS credentials to the Docker container.  Here is the generated Docker-compose file:<\/p>\n\n<pre><code>networks:\n  sagemaker-local:\n    name: sagemaker-local\nservices:\n  algo-1-1DUU4:\n    command: train\n    environment:\n    - AWS_REGION=us-west-2\n    - TRAINING_JOB_NAME=sagemaker-mxnet-2018-10-07-00-47-10-435\n    image: 123456789012.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-mxnet:1.1.0-gpu-py2\n    networks:\n      sagemaker-local:\n        aliases:\n        - algo-1-1DUU4\n    stdin_open: true\n    tty: true\n    volumes:\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/input:\/opt\/ml\/input\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output:\/opt\/ml\/output\n    - \/tmp\/tmpSkaR3x\/algo-1-1DUU4\/output\/data:\/opt\/ml\/output\/data\n    - \/tmp\/tmpSkaR3x\/model:\/opt\/ml\/model\nversion: '2.1'\n<\/code><\/pre>\n\n<p>Should the AWS credentials be passed in as enviromental variables?<\/p>\n\n<p>I upgraded my <code>sagemaker<\/code> install to after reading <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/403\" rel=\"nofollow noreferrer\">Using boto3 in install local mode?<\/a>, but that had no effect.  I checked the credentials that are being fetched in the Sagemaker session (outside the container) and they appear to be blank, even though I have an <code>~\/.aws\/config<\/code> and <code>~\/.aws\/credentials<\/code> file:<\/p>\n\n<pre><code>{'_token': None, '_time_fetcher': &lt;function _local_now at 0x7f4dbbe75230&gt;, '_access_key': None, '_frozen_credentials': None, '_refresh_using': &lt;bound method AssumeRoleCredentialFetcher.fetch_credentials of &lt;botocore.credentials.AssumeRoleCredentialFetcher object at 0x7f4d2de48bd0&gt;&gt;, '_secret_key': None, '_expiry_time': None, 'method': 'assume-role', '_refresh_lock': &lt;thread.lock object at 0x7f4d9f2aafd0&gt;}\n<\/code><\/pre>\n\n<p>I am new to AWS so I do not know how to diagnose the issue regarding AWS credentials.  My <code>.aws\/config<\/code> file has the following information (with placeholder values):<\/p>\n\n<pre><code>[default]\noutput = json\nregion = us-west-2\nrole_arn = arn:aws:iam::123456789012:role\/SageMakers\nsource_profile = sagemaker-test\n\n[profile sagemaker-test]\noutput = json\nregion = us-west-2\n<\/code><\/pre>\n\n<p>Where the <code>sagemaker-test<\/code> profile has <code>AmazonSageMakerFullAccess<\/code> in the IAM Management Console.<\/p>\n\n<p>The <code>.aws\/credentials<\/code> file has the following information (represented by placeholder values):<\/p>\n\n<pre><code>[default]\naws_access_key_id = 1234567890\naws_secret_access_key = zyxwvutsrqponmlkjihgfedcba\n[sagemaker-test]\naws_access_key_id = 0987654321\naws_secret_access_key = abcdefghijklmopqrstuvwxyz\n<\/code><\/pre>\n\n<p>Lastly, these are versions of the applicable libraries from a <code>pip freeze<\/code>:<\/p>\n\n<pre><code>awscli==1.16.19\nboto==2.48.0\nboto3==1.9.18\nbotocore==1.12.18\ndocker==3.5.0\ndocker-compose==1.22.0\nmxnet-cu91==1.1.0.post0\nsagemaker==1.11.1\n<\/code><\/pre>\n\n<p>Please let me know if I left out any relevant information and thanks for any help\/feedback that you can provide.<\/p>\n\n<p><strong>UPDATE<\/strong>: Thanks for your help, everyone! While attempting some of your suggested fixes, I noticed that <code>boto3<\/code> was out of date, and update it (to <code>boto3-1.9.26<\/code> and <code>botocore-1.12.26<\/code>) which appeared to resolve the issue.  I was not able to find any documentation on that being an issue with <code>boto3==1.9.18<\/code>.  If someone could help me understand what the issue was with <code>boto3<\/code>, I would happy to make mark their answer as correct.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":3,
        "Question_created_time":1538879533133,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1539893383687,
        "Question_score":1,
        "Question_view_count":1374.0,
        "Poster_id":2829487.0,
        "Poster_created_time":1380496984176,
        "Poster_last_access_time":1663610467728,
        "Poster_age":null,
        "Poster_location":"Gloucester, VA, USA",
        "Poster_reputation":544.0,
        "Poster_views":85.0,
        "Answer_body":"<p>SageMaker local mode is designed to pick up whatever credentials are available in your boto3 session, and pass them into the docker container as environment variables. <\/p>\n\n<p>However, the version of the sagemaker sdk that you are using (1.11.1 and earlier) will ignore the credentials if they include a token, because that usually indicates short-lived credentials that won't remain valid long enough for a training job to complete or endpoint to be useful.<\/p>\n\n<p>If you are using temporary credentials, try replacing them with permanent ones, or running from an ec2 instance (or SageMaker notebook!) that has an appropriate instance role assigned.<\/p>\n\n<p>Also, the sagemaker sdk's handling of credentials changed in v1.11.2 and later -- temporary credentials will be passed to local mode containers, but with a warning message. So you could just upgrade to a newer version and try again (<code>pip install -U sagemaker<\/code>). <\/p>\n\n<p>Also, try upgrading <code>boto3<\/code> can change, so try using the latest version.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1542529234207,
        "Answer_score":1.0,
        "Answerer_id":3920238.0,
        "Answerer_created_time":1407449881432,
        "Answerer_last_access_time":1636486313420,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":228.0,
        "Answerer_views":14.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52684987",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1539818808056,
        "Question_original_content":"aw nocredenti train attempt run exampl code local gpu copi code jupyt notebook follow python script import boto import subprocess import mxnet import mxnet mxnet import gluon import execut role import session session instanc type local subprocess nvidia smi set type gpu present instanc type local gpu role execut role gluon data vision mnist data train train true gluon data vision mnist data test train fals successfulli connect upload data input session upload data path data kei prefix data mnist hyperparamet batch size epoch learn rate momentum log interv mxnet mnist role role train instanc count train instanc type instanc type framework version hyperparamet hyperparamet fail docker contain fit input predictor deploi initi instanc count instanc type instanc type delet endpoint referenc mnist file exactli specifi github script fail fit docker contain follow error algo duu download mxnet sourc sourcedir tar tmp script tar algo duu error contain support train uncaught except train unabl locat credenti algo duu traceback recent algo duu file usr local lib python dist packag contain support train line start algo duu train algo duu file usr local lib python dist packag mxnet contain train line train algo duu mxnet env download user modul algo duu file usr local lib python dist packag contain support environ line download user modul algo duu download resourc self user script archiv tmp algo duu file usr local lib python dist packag contain support util line download resourc algo duu script bucket download file script kei target algo duu file usr local lib python dist packag boto inject line bucket download file algo duu extraarg extraarg callback callback config config algo duu file usr local lib python dist packag boto inject line download file algo duu extra arg extraarg callback callback algo duu file usr local lib python dist packag boto transfer line download file algo duu futur result algo duu file usr local lib python dist packag stransfer futur line result algo duu return self coordin result algo duu file usr local lib python dist packag stransfer futur line result algo duu rais self except algo duu nocredentialserror unabl locat credenti confus authent outsid contain pload train test data docker contain guess issu pass aw credenti docker contain gener docker compos file network local local servic algo duu command train environ aw region west train job mxnet imag dkr ecr west amazonaw com mxnet gpu network local alias algo duu stdin open true tty true volum tmp tmpskarx algo duu input opt input tmp tmpskarx algo duu output opt output tmp tmpskarx algo duu output data opt output data tmp tmpskarx model opt model version aw credenti pass enviroment variabl upgrad instal read boto instal local mode effect check credenti fetch session outsid contain appear blank aw config aw credenti file token time fetcher access kei frozen credenti refresh secret kei expiri time method assum role refresh lock new aw know diagnos issu aw credenti aw config file follow inform placehold valu default output json region west role arn arn aw iam role sourc profil test profil test output json region west test profil amazonfullaccess iam manag consol aw credenti file follow inform repres placehold valu default aw access kei aw secret access kei zyxwvutsrqponmlkjihgfedcba test aw access kei aw secret access kei abcdefghijklmopqrstuvwxyz lastli version applic librari pip freez awscli boto boto botocor docker docker compos mxnet post let know left relev inform thank help feedback provid updat thank help attempt suggest fix notic boto date updat boto botocor appear resolv issu abl document issu boto help understand issu boto happi mark answer correct",
        "Question_preprocessed_content":"aw nocredenti train attempt run exampl code local gpu copi code jupyt notebook follow python script referenc file exactli specifi github script fail docker contain follow error confus authent outsid contain docker contain guess issu pass aw credenti docker contain gener file aw credenti pass enviroment variabl upgrad instal read boto instal local mode effect check credenti fetch session appear blank file new aw know diagnos issu aw credenti file follow inform profil iam manag consol file follow inform lastli version applic librari let know left relev inform thank provid updat thank help attempt suggest fix notic date updat appear resolv issu abl document issu help understand issu happi mark answer correct",
        "Question_gpt_summary_original":"The user is encountering an error when attempting to run example code for Amazon Sagemaker on a local GPU. The error occurs during the Docker container training process and is related to the inability to locate AWS credentials. The user has checked their AWS credentials and found that they appear to be blank, despite having a .aws\/config and .aws\/credentials file. The user is unsure how to diagnose the issue and is seeking help to understand the problem with their boto3 library.",
        "Question_gpt_summary":"user encount error attempt run exampl code local gpu error occur docker contain train process relat inabl locat aw credenti user check aw credenti appear blank despit have aw config aw credenti file user unsur diagnos issu seek help understand problem boto librari",
        "Answer_original_content":"local mode design pick credenti avail boto session pass docker contain environ variabl version sdk earlier ignor credenti includ token usual indic short live credenti won remain valid long train job complet endpoint us temporari credenti try replac perman on run instanc notebook appropri instanc role assign sdk handl credenti chang later temporari credenti pass local mode contain warn messag upgrad newer version try pip instal try upgrad boto chang try latest version",
        "Answer_preprocessed_content":"local mode design pick credenti avail boto session pass docker contain environ variabl version sdk ignor credenti includ token usual indic credenti won remain valid long train job complet endpoint us temporari credenti try replac perman on run instanc appropri instanc role assign sdk handl credenti chang later temporari credenti pass local mode contain warn messag upgrad newer version try try upgrad chang try latest version",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Replace temporary credentials with permanent ones or run from an EC2 instance or SageMaker notebook that has an appropriate instance role assigned.\n- Upgrade to a newer version of the sagemaker sdk (v1.11.2 or later) that passes temporary credentials to local mode containers with a warning message.\n- Upgrade boto3 to the latest version.",
        "Answer_gpt_summary":"possibl solut mention discuss replac temporari credenti perman on run instanc notebook appropri instanc role assign upgrad newer version sdk later pass temporari credenti local mode contain warn messag upgrad boto latest version"
    },
    {
        "Question_title":"Sagemaker API online feature store creation not using given kms key",
        "Question_body":"<p>I am calling <code>Sagemaker API<\/code> from python script inside <code>EC2<\/code> instance to create online feature store. I gave required permission and its creating feature group.\nHowever I observed that key I'm passing in below program (<code>online_store_kms_key_id = 'arn:aws:kms:us-east-1:1234:key\/1111'<\/code>) is not being used to write objects to s3 bucket instead it's using default bucket key.\nI'm not sure what is causing this to happen? Why its not using key given in create feature group config? Any idea?<\/p>\n<p>code snippet:<\/p>\n<pre><code>customer_data = pd.read_csv(&quot;data.csv&quot;,dtype={'customer_id': int,'city_code': int, 'state_code': int, 'country_code': int, 'eventtime': float })\n\n    customers_feature_group_name = &quot;customers-fg-01&quot;\n    customers_feature_group = FeatureGroup(name=customers_feature_group_name, sagemaker_session=sagemaker_session\n                                           )\n\n    current_time_sec = int(round(time.time()))\n\n    record_identifier_feature_name = &quot;customer_id&quot;\n\n    customers_feature_group.load_feature_definitions(data_frame=customer_data)\n\n    customers_feature_group.create(\n        s3_uri=&quot;s3:\/\/xxxx\/sagemaker-featurestore\/&quot;,\n        record_identifier_name=record_identifier_feature_name,\n        event_time_feature_name=&quot;eventtime&quot;,\n        role_arn='arn:aws:iam::1234:role\/role-1234',\n        enable_online_store=True,\n        online_store_kms_key_id = 'arn:aws:kms:us-east-1:1234:key\/1111'\n    )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1659040058783,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1659040501740,
        "Question_score":1,
        "Question_view_count":47.0,
        "Poster_id":2264559.0,
        "Poster_created_time":1365570541220,
        "Poster_last_access_time":1663194847096,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":519.0,
        "Poster_views":576.0,
        "Answer_body":"<p>For encryption of data stored in s3 ( offline store ) you need to add a field\n'offline_store_kms_key_id ' to the create() method call, please refer the document below<\/p>\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/prep_data\/feature_store.html#sagemaker.feature_store.feature_group.FeatureGroup.create\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/prep_data\/feature_store.html#sagemaker.feature_store.feature_group.FeatureGroup.create<\/a><\/p>\n<p>Also please go through the below document to check the policies and also to verify if you have a symmetric customer managed keys or asymmetric customer managed keys as feature store only supports symmetric keys.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/feature-store-security.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/feature-store-security.html<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":19490330.0,
        "Answerer_created_time":1657058369727,
        "Answerer_last_access_time":1663974347336,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":116.0,
        "Answerer_views":9.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73158818",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659050391287,
        "Question_original_content":"api onlin featur store creation given km kei call api python script insid instanc creat onlin featur store gave requir permiss creat featur group observ kei pass program onlin store km kei arn aw km east kei write object bucket instead default bucket kei sure caus happen kei given creat featur group config idea code snippet custom data read csv data csv dtype custom int citi code int state code int countri code int eventtim float custom featur group custom custom featur group featuregroup custom featur group session session current time sec int round time time record identifi featur custom custom featur group load featur definit data frame custom data custom featur group creat uri featurestor record identifi record identifi featur event time featur eventtim role arn arn aw iam role role enabl onlin store true onlin store km kei arn aw km east kei",
        "Question_preprocessed_content":"api onlin featur store creation given km kei call python script insid instanc creat onlin featur store gave requir permiss creat featur group observ kei pass program write object bucket instead default bucket kei sure caus happen kei given creat featur group config idea code snippet",
        "Question_gpt_summary_original":"The user is encountering a challenge where the Sagemaker API online feature store creation is not using the given KMS key. The user has provided the required permissions and created the feature group, but the key specified in the create feature group configuration is not being used to write objects to the S3 bucket. The user is unsure of the cause of this issue and is seeking assistance.",
        "Question_gpt_summary":"user encount challeng api onlin featur store creation given km kei user provid requir permiss creat featur group kei specifi creat featur group configur write object bucket user unsur caus issu seek assist",
        "Answer_original_content":"encrypt data store offlin store need add field offlin store km kei creat method refer document http readthedoc stabl api prep data featur store html featur store featur group featuregroup creat document check polici verifi symmetr custom manag kei asymmetr custom manag kei featur store support symmetr kei http doc aw amazon com latest featur store secur html",
        "Answer_preprocessed_content":"encrypt data store need add field creat method refer document document check polici verifi symmetr custom manag kei asymmetr custom manag kei featur store support symmetr kei",
        "Answer_gpt_summary_original":"Solution: The user needs to add the 'offline_store_kms_key_id' field to the create() method call for encryption of data stored in S3. The user should also check the policies and verify if they have a symmetric customer managed key as feature store only supports symmetric keys. The user can refer to the provided documentation for more information.",
        "Answer_gpt_summary":"solut user need add offlin store km kei field creat method encrypt data store user check polici verifi symmetr custom manag kei featur store support symmetr kei user refer provid document inform"
    },
    {
        "Question_title":"SageMaker framework processor compatibility with sagemaker pipelines",
        "Question_body":"Hi all,\n\nI am asking if it's possible to use framework processor inside a sagemaker pipeline.\n\nI am asking because the to submit the source_dir for the framework processor, we have to do so when calling the .run() method, when wrapping the processor inside a sagemaker.workflow.steps.ProcessingStep, there isn't an available argument to specify the source_dir.\n\nThank you! Best, Ruoy",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1652344291417,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":201.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"You can do this with the latest version of the sagemaker sdk 2.89.0\n\nfrom sagemaker.workflow.pipeline_context import PipelineSession\n\nsession = PipelineSession()\n\ninputs = [\n    ProcessingInput(\n    source=\"s3:\/\/my-bucket\/sourcefile\", \n    destination=\"\/opt\/ml\/processing\/inputs\/\",),\n]\n\nprocessor = FrameworkProcessor(...)\n\nstep_args = processor.run(inputs=inputs, source_dir=\"...\")\n\nstep_sklearn = ProcessingStep(\n    name=\"MyProcessingStep\",\n    step_args=step_args,\n)",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbY_u2lSORnmomHzZsGOZAA\/sage-maker-framework-processor-compatibility-with-sagemaker-pipelines",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1652383066859,
        "Question_original_content":"framework processor compat pipelin ask possibl us framework processor insid pipelin ask submit sourc dir framework processor call run method wrap processor insid workflow step processingstep isn avail argument specifi sourc dir thank best ruoi",
        "Question_preprocessed_content":"framework processor compat pipelin ask possibl us framework processor insid pipelin ask submit framework processor call run method wrap processor insid isn avail argument specifi thank best ruoi",
        "Question_gpt_summary_original":"The user is facing a challenge with using a framework processor inside a SageMaker pipeline. They are unable to specify the source_dir when wrapping the processor inside a sagemaker.workflow.steps.ProcessingStep.",
        "Question_gpt_summary":"user face challeng framework processor insid pipelin unabl specifi sourc dir wrap processor insid workflow step processingstep",
        "Answer_original_content":"latest version sdk workflow pipelin context import pipelinesess session pipelinesess input processinginput sourc bucket sourcefil destin opt process input processor frameworkprocessor step arg processor run input input sourc dir step sklearn processingstep myprocessingstep step arg step arg",
        "Answer_preprocessed_content":"latest version sdk import pipelinesess session pipelinesess input processor processingstep",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use the latest version of the sagemaker sdk 2.89.0. With this version, you can specify the source_dir when wrapping the processor inside a sagemaker.workflow.steps.ProcessingStep.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us latest version sdk version specifi sourc dir wrap processor insid workflow step processingstep"
    },
    {
        "Question_title":"After installing scrubadub_spacy package, spacy.load(\"en_core_web_sm\") not working OSError: [E053] Could not read config.cfg",
        "Question_body":"<p>I am getting the below error when I'm trying to run the following line of code to load en_core_web_sm in the Azure Machine Learning instance.<\/p>\n<p>I debugged the issue and found out that once I install scrubadub_spacy, that seems is the issue causing the error.<\/p>\n<pre><code>spacy.load(&quot;en_core_web_sm&quot;)\n<\/code><\/pre>\n<pre><code>OSError                                   Traceback (most recent call last)\n&lt;ipython-input-2-c6e652d70518&gt; in &lt;module&gt;\n     1 # Load English tokenizer, tagger, parser and NER\n----&gt; 2 nlp = spacy.load(&quot;en_core_web_sm&quot;)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/__init__.py in load(name, vocab, disable, exclude, config)\n    50     &quot;&quot;&quot;\n    51     return util.load_model(\n---&gt; 52         name, vocab=vocab, disable=disable, exclude=exclude, config=config\n    53     )\n    54 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model(name, vocab, disable, exclude, config)\n   418             return get_lang_class(name.replace(&quot;blank:&quot;, &quot;&quot;))()\n   419         if is_package(name):  # installed as package\n--&gt; 420             return load_model_from_package(name, **kwargs)  # type: ignore[arg-type]\n   421         if Path(name).exists():  # path to model data directory\n   422             return load_model_from_path(Path(name), **kwargs)  # type: ignore[arg-type]\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model_from_package(name, vocab, disable, exclude, config)\n   451     &quot;&quot;&quot;\n   452     cls = importlib.import_module(name)\n--&gt; 453     return cls.load(vocab=vocab, disable=disable, exclude=exclude, config=config)  # type: ignore[attr-defined]\n   454 \n   455 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/en_core_web_sm\/__init__.py in load(**overrides)\n    10 \n    11 def load(**overrides):\n---&gt; 12     return load_model_from_init_py(__file__, **overrides)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model_from_init_py(init_file, vocab, disable, exclude, config)\n   619         disable=disable,\n   620         exclude=exclude,\n--&gt; 621         config=config,\n   622     )\n   623 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_model_from_path(model_path, meta, vocab, disable, exclude, config)\n   485     config_path = model_path \/ &quot;config.cfg&quot;\n   486     overrides = dict_to_dot(config)\n--&gt; 487     config = load_config(config_path, overrides=overrides)\n   488     nlp = load_model_from_config(config, vocab=vocab, disable=disable, exclude=exclude)\n   489     return nlp.from_disk(model_path, exclude=exclude, overrides=overrides)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/spacy\/util.py in load_config(path, overrides, interpolate)\n   644     else:\n   645         if not config_path or not config_path.exists() or not config_path.is_file():\n--&gt; 646             raise IOError(Errors.E053.format(path=config_path, name=&quot;config.cfg&quot;))\n   647         return config.from_disk(\n   648             config_path, overrides=overrides, interpolate=interpolate\n\nOSError: [E053] Could not read config.cfg from \/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/en_core_web_sm\/en_core_web_sm-2.3.1\/config.cfg\n<\/code><\/pre>\n<p>I installed the packages using the below three lines codes from <a href=\"https:\/\/spacy.io\/usage\" rel=\"nofollow noreferrer\">Spacy<\/a><\/p>\n<pre><code>pip install -U pip setuptools wheel\npip install -U spacy\npython -m spacy download en_core_web_sm\n<\/code><\/pre>\n<p>How should I fix this issue? thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1643912383590,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1643998339103,
        "Question_score":2,
        "Question_view_count":201.0,
        "Poster_id":12363966.0,
        "Poster_created_time":1573605534107,
        "Poster_last_access_time":1656003381830,
        "Poster_age":null,
        "Poster_location":"Saint Louis, MO, USA",
        "Poster_reputation":25.0,
        "Poster_views":2.0,
        "Answer_body":"<p>Taking the path from your error message:<\/p>\n<pre><code>en_core_web_sm-2.3.1\/config.cfg\n<\/code><\/pre>\n<p>You have a model for v2.3, but it's looking for a <code>config.cfg<\/code>, which is only a thing in v3 of spaCy. It looks like you upgraded spaCy without realizing it.<\/p>\n<p>There are two ways to fix this. One is to reinstall the model with <code>spacy download<\/code>, which will get a version that matches your current spaCy version. If you are just starting something that is probably the best idea. Based on the release date of scrubadub, it seems to be intended for use with spaCy v3.<\/p>\n<p>However, note that v2 and v3 are pretty different - if you have a project with v2 of spaCy you might want to downgrade instead.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":355715.0,
        "Answerer_created_time":1275416023832,
        "Answerer_last_access_time":1664001679808,
        "Answerer_age":null,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation":12639.0,
        "Answerer_views":932.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70976353",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1644122814723,
        "Question_original_content":"instal scrubadub spaci packag spaci load core web work oserror read config cfg get error try run follow line code load core web instanc debug issu instal scrubadub spaci issu caus error spaci load core web oserror traceback recent load english token tagger parser ner nlp spaci load core web anaconda env lib python site packag spaci init load vocab disabl exclud config return util load model vocab vocab disabl disabl exclud exclud config config anaconda env lib python site packag spaci util load model vocab disabl exclud config return lang class replac blank packag instal packag return load model packag kwarg type ignor arg type path exist path model data directori return load model path path kwarg type ignor arg type anaconda env lib python site packag spaci util load model packag vocab disabl exclud config cl importlib import modul return cl load vocab vocab disabl disabl exclud exclud config config type ignor attr defin anaconda env lib python site packag core web init load overrid def load overrid return load model init file overrid anaconda env lib python site packag spaci util load model init init file vocab disabl exclud config disabl disabl exclud exclud config config anaconda env lib python site packag spaci util load model path model path meta vocab disabl exclud config config path model path config cfg overrid dict dot config config load config config path overrid overrid nlp load model config config vocab vocab disabl disabl exclud exclud return nlp disk model path exclud exclud overrid overrid anaconda env lib python site packag spaci util load config path overrid interpol config path config path exist config path file rais ioerror error format path config path config cfg return config disk config path overrid overrid interpol interpol oserror read config cfg anaconda env lib python site packag core web core web config cfg instal packag line code spaci pip instal pip setuptool wheel pip instal spaci python spaci download core web fix issu thank advanc",
        "Question_preprocessed_content":"instal packag work oserror read get error try run follow line code load instanc debug issu instal issu caus error instal packag line code spaci fix issu thank advanc",
        "Question_gpt_summary_original":"The user is encountering an OSError when trying to load en_core_web_sm in Azure Machine Learning instance, and has found that the issue is caused by installing scrubadub_spacy package. The user has installed the packages using the recommended Spacy commands, but is still facing the issue and is seeking a solution.",
        "Question_gpt_summary":"user encount oserror try load core web instanc issu caus instal scrubadub spaci packag user instal packag recommend spaci command face issu seek solut",
        "Answer_original_content":"take path error messag core web config cfg model look config cfg thing spaci look like upgrad spaci realiz wai fix reinstal model spaci download version match current spaci version start probabl best idea base releas date scrubadub intend us spaci note pretti differ project spaci want downgrad instead",
        "Answer_preprocessed_content":"take path error messag model look thing spaci look like upgrad spaci realiz wai fix reinstal model version match current spaci version start probabl best idea base releas date scrubadub intend us spaci note pretti differ project spaci want downgrad instead",
        "Answer_gpt_summary_original":"Solution: The user can either reinstall the en_core_web_sm model using the \"spacy download\" command to get a version that matches the current spaCy version, or downgrade to spaCy v2 if they have a project with v2 of spaCy. The scrubadub_spacy package seems to be intended for use with spaCy v3.",
        "Answer_gpt_summary":"solut user reinstal core web model spaci download command version match current spaci version downgrad spaci project spaci scrubadub spaci packag intend us spaci"
    },
    {
        "Question_title":"Is it possible to \"apt install\" in SageMaker Studio Lab?",
        "Question_body":"<p>I have started using SageMaker Studio Lab.<\/p>\n<p>When I run &quot;apt install xvfb&quot; in SageMaker Studio Lab Notebook, I get the following error.<\/p>\n<pre><code>!apt install xvfb\n\nE: Could not open lock file \/var\/lib\/dpkg\/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the dpkg frontend lock (\/var\/lib\/dpkg\/lock-frontend), are you root?\n<\/code><\/pre>\n<p>Then I tried with sudo, but the sudo command was not installed.<\/p>\n<pre><code>!sudo apt install xvfb\n\n\/usr\/bin\/sh: 1: sudo: not found\n<\/code><\/pre>\n<p>Can you please tell me how to solve this problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1638461318997,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":1174.0,
        "Poster_id":7304987.0,
        "Poster_created_time":1481861289276,
        "Poster_last_access_time":1643180283947,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":13.0,
        "Poster_views":1.0,
        "Answer_body":"<p>To date Studio Lab doesn't support package installs that require root access. It does support packages installable via pip and conda. You can do that either in your notebook with the %, rather than the !, or you can do that via opening a terminal.<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-manage.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-lab-use-manage.html<\/a><\/li>\n<\/ul>\n<p>If you'd like to open an issue you're welcome to do that on our repository right here:<\/p>\n<ul>\n<li><a href=\"https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/ISSUE_TEMPLATE\/bug-report-for-sagemaker-studio-lab.md\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/studio-lab-examples\/blob\/main\/.github\/ISSUE_TEMPLATE\/bug-report-for-sagemaker-studio-lab.md<\/a><\/li>\n<\/ul>\n<p>Thanks for trying out Studio Lab!<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4348604.0,
        "Answerer_created_time":1418276189383,
        "Answerer_last_access_time":1663514884063,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":272.0,
        "Answerer_views":142.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70202848",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1638468298390,
        "Question_original_content":"possibl apt instal studio lab start studio lab run apt instal xvfb studio lab notebook follow error apt instal xvfb open lock file var lib dpkg lock frontend open permiss deni unabl acquir dpkg frontend lock var lib dpkg lock frontend root tri sudo sudo command instal sudo apt instal xvfb usr bin sudo tell solv problem",
        "Question_preprocessed_content":"possibl apt instal studio lab start studio lab run apt instal xvfb studio lab notebook follow error tri sudo sudo command instal tell solv problem",
        "Question_gpt_summary_original":"The user is facing challenges while trying to install xvfb in SageMaker Studio Lab Notebook. They are unable to install it using the \"apt install\" command due to permission denied error and also cannot use the sudo command as it is not installed. The user is seeking help to solve this problem.",
        "Question_gpt_summary":"user face challeng try instal xvfb studio lab notebook unabl instal apt instal command permiss deni error us sudo command instal user seek help solv problem",
        "Answer_original_content":"date studio lab support packag instal requir root access support packag instal pip conda notebook open termin http doc aw amazon com latest studio lab us manag html like open issu welcom repositori right http github com aw studio lab exampl blob main github issu templat bug report studio lab thank try studio lab",
        "Answer_preprocessed_content":"date studio lab support packag instal requir root access support packag instal pip conda notebook open termin like open issu welcom repositori right thank try studio lab",
        "Answer_gpt_summary_original":"Solution:\n- SageMaker Studio Lab Notebook does not support package installs that require root access.\n- Packages installable via pip and conda are supported in the notebook with the % command or via opening a terminal.\n- The user can open an issue on the AWS repository if they encounter any problems.",
        "Answer_gpt_summary":"solut studio lab notebook support packag instal requir root access packag instal pip conda support notebook command open termin user open issu aw repositori encount problem"
    },
    {
        "Question_title":"How do I use Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#?",
        "Question_body":"<p>I'm trying to call an Azure Machine Learning Pipeline Endpoint I've set up using C# &amp; the Machine Learning REST api.<\/p>\n<p>I am certain that I have the Service Principal configured correctly, as I can successfully authenticate &amp; hit the endpoint using the <code>azureml-core<\/code> python sdk:<\/p>\n<pre><code>sp = ServicePrincipalAuthentication(\n    tenant_id=tenant_id,\n    service_principal_id=service_principal_id,\n    service_principal_password=service_principal_password)\nws =Workspace.get(\n    name=workspace_name, \n    resource_group=resource_group, \n    subscription_id=subscription_id, \n    auth=sp)\n\nendpoint = PipelineEndpoint.get(ws, name='MyEndpoint')\nendpoint.submit('Test_Experiment')\n<\/code><\/pre>\n<p>I'm using the following example in C# to attempt to run my endpoint: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-pipelines#run-a-published-pipeline-using-c<\/a><\/p>\n<p>I'm attempting to fill <code>auth_key<\/code> with the following code:<\/p>\n<pre><code>var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\nvar clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\nvar tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\nvar cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\nvar auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] {&quot;.default&quot; }));\n<\/code><\/pre>\n<p>I receive a 401 (unauthorized).<\/p>\n<p>What am I am doing wrong?<\/p>\n<ul>\n<li>UPDATE *<\/li>\n<\/ul>\n<p>I changed the 'scopes' param in the <code>TokenRequestContext<\/code> to look like:<\/p>\n<pre><code>var auth_key = cred.GetToken(new Azure.Core.TokenRequestContext(new string[] { &quot;http:\/\/DataTriggerApp\/.default&quot; }));\n<\/code><\/pre>\n<p><code>http:\/\/DataTriggerApp<\/code> is one of the <code>servicePrincipalNames<\/code> that shows up when i query my Service Principal from the azure CLI.<\/p>\n<p>Now, when I attempt to use the returned token to call the Machine Learning Pipeline Endpoint, I receive a 403 instead of a 401.  Maybe some progress?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1634153827710,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1634156473112,
        "Question_score":1,
        "Question_view_count":752.0,
        "Poster_id":1486998.0,
        "Poster_created_time":1340833876128,
        "Poster_last_access_time":1663795160110,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":751.0,
        "Poster_views":73.0,
        "Answer_body":"<p>Ok, through a lot of trial-and-error I was able to come up with two ways of acquiring a token that allows me to hit my Azure Machine Learning Pipeline Endpoint through the REST api.  One uses Microsoft.Identity.Client &amp; one uses Azure.Identity.<\/p>\n<pre><code>using Microsoft.Identity.Client;\n\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n   \n      var app = ConfidentialClientApplicationBuilder.Create(clientId)\n                                                .WithClientSecret(clientSecret)                                                \n                                                .WithAuthority(AzureCloudInstance.AzurePublic, tenantId)\n                                                .Build();\n      var result = await app.AcquireTokenForClient(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }).ExecuteAsync();\n      return result.AccessToken;\n}\n<\/code><\/pre>\n<p>Or:<\/p>\n<pre><code>using Azure.Identity;\n...\n\npublic static async Task&lt;string&gt; GetAccessToken()\n{\n      var clientId = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_ID&quot;);\n      var clientSecret = Environment.GetEnvironmentVariable(&quot;AZURE_CLIENT_SECRET&quot;);\n      var tenantId = Environment.GetEnvironmentVariable(&quot;AZURE_TENANT_ID&quot;);\n\n\n      var cred = new ClientSecretCredential(tenantId, clientId, clientSecret);\n      var token =  await cred.GetTokenAsync(new Azure.Core.TokenRequestContext(new string[] { &quot;https:\/\/ml.azure.com\/.default&quot; }));\n      return token.Token;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1634160459928,
        "Answer_score":1.0,
        "Answerer_id":1486998.0,
        "Answerer_created_time":1340833876128,
        "Answerer_last_access_time":1663795160110,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":751.0,
        "Answerer_views":73.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69561386",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1634160031172,
        "Question_original_content":"us servic princip authent pipelin endpoint try pipelin endpoint set machin learn rest api certain servic princip configur correctli successfulli authent hit endpoint core python sdk serviceprincipalauthent tenant tenant servic princip servic princip servic princip password servic princip password workspac workspac resourc group resourc group subscript subscript auth endpoint pipelineendpoint myendpoint endpoint submit test experi follow exampl attempt run endpoint http doc microsoft com azur machin learn deploi pipelin run publish pipelin attempt auth kei follow code var clientid environ getenvironmentvari azur client var clientsecret environ getenvironmentvari azur client secret var tenantid environ getenvironmentvari azur tenant var cred new clientsecretcredenti tenantid clientid clientsecret var auth kei cred gettoken new azur core tokenrequestcontext new string default receiv unauthor wrong updat chang scope param tokenrequestcontext look like var auth kei cred gettoken new azur core tokenrequestcontext new string http datatriggerapp default http datatriggerapp serviceprincipalnam show queri servic princip azur cli attempt us return token machin learn pipelin endpoint receiv instead mayb progress",
        "Question_preprocessed_content":"us servic princip authent pipelin endpoint try pipelin endpoint set machin learn rest api certain servic princip configur correctli successfulli authent hit endpoint python sdk follow exampl attempt run endpoint attempt follow code receiv wrong updat chang scope param look like show queri servic princip azur cli attempt us return token machin learn pipelin endpoint receiv instead mayb progress",
        "Question_gpt_summary_original":"The user is facing challenges in using Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#. They have configured the Service Principal correctly and can successfully authenticate and hit the endpoint using the azureml-core python sdk. However, when attempting to run the endpoint in C#, they receive a 401 (unauthorized) error. They have tried changing the 'scopes' param in the TokenRequestContext to include one of the servicePrincipalNames that shows up when they query their Service Principal from the azure CLI, but now receive a 403 error instead of a 401.",
        "Question_gpt_summary":"user face challeng servic princip authent pipelin endpoint configur servic princip correctli successfulli authent hit endpoint core python sdk attempt run endpoint receiv unauthor error tri chang scope param tokenrequestcontext includ serviceprincipalnam show queri servic princip azur cli receiv error instead",
        "Answer_original_content":"lot trial error abl come wai acquir token allow hit pipelin endpoint rest api us microsoft ident client us azur ident microsoft ident client public static async task getaccesstoken var clientid environ getenvironmentvari azur client var clientsecret environ getenvironmentvari azur client secret var tenantid environ getenvironmentvari azur tenant var app confidentialclientapplicationbuild creat clientid withclientsecret clientsecret withauthor azurecloudinst azurepubl tenantid build var result await app acquiretokenforcli new string http azur com default executeasync return result accesstoken azur ident public static async task getaccesstoken var clientid environ getenvironmentvari azur client var clientsecret environ getenvironmentvari azur client secret var tenantid environ getenvironmentvari azur tenant var cred new clientsecretcredenti tenantid clientid clientsecret var token await cred gettokenasync new azur core tokenrequestcontext new string http azur com default return token token",
        "Answer_preprocessed_content":"lot abl come wai acquir token allow hit pipelin endpoint rest api us us",
        "Answer_gpt_summary_original":"The discussion provides two possible solutions to the challenge of using Service Principal authentication with an Azure Machine Learning Pipeline Endpoint in C#. The first solution uses Microsoft.Identity.Client and the second solution uses Azure.Identity. Both solutions involve acquiring a token that allows the user to hit the Azure Machine Learning Pipeline Endpoint through the REST API.",
        "Answer_gpt_summary":"discuss provid possibl solut challeng servic princip authent pipelin endpoint solut us microsoft ident client second solut us azur ident solut involv acquir token allow user hit pipelin endpoint rest api"
    },
    {
        "Question_title":"Sagemaker - Random Cut Forest - Feature Normalization? Pre-Processing?",
        "Question_body":"<p>I am having trouble understanding the RCF algorithm, particularly how it expects \/ anticipates data or the pre-processing that should be completed?  For example, I have the following data\/features (with example values) for about 500K records):  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/emD7M.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/emD7M.png\" alt=\"valuesandheader\"><\/a><\/p>\n\n<p>The results of my RCF model (trained on 500K records for 57 features - amount, 30 countries dummied, and 26 categories dummied) is extremely focused on the amount feature (e.g., all anomalies are above approx. 1000.00 which absolutely no variation based on the country or type).  <\/p>\n\n<p>Also, I also normalized the amount field and the results for that are also not really that strong. In fact, its safe to say I the results are terrible and I am clearly missing something with this.   <\/p>\n\n<p>Overall, I am looking for some guidance on getting the features right (again - 1 amount field and 2 fields that are categorical and dummied 1 and 0 - resulting in about 57 fields).  Im wondering if I am better off with something like kmeans.<\/p>\n\n<p>EDIT: Some context here...  I am wondering:<br>\n1) Weighting - Is there is a way to give weight to certain variables (i.e., one of the categorical variables is more important than the other). For example, I am using Country and Category as key attributes and want to give more weight to Category over Country.<br>\n2) Context - How can I ensure outliers are considered in context of its peers (the categorical data)?  For example, a transaction of $5000 for an \"airfare\" expense is not an outlier for that category but would be for any other. I could create N numbers of models, but that would get messy and cumbersome, right?<\/p>\n\n<p>I looked through most of the available documentation (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rcf_how-it-works.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/rcf_how-it-works.html<\/a>) and cannot find anything that describes this! <\/p>\n\n<p>Thank you so much for your help in advance!<\/p>\n\n<p>EDIT: Not sure its critical at this point where I dont even have semi-reasonable results, but I have used the following hyperparameters:<br>\nnum_samples_per_tree=256,<br>\nnum_trees=100  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1575572661943,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1575816519720,
        "Question_score":3,
        "Question_view_count":475.0,
        "Poster_id":3802473.0,
        "Poster_created_time":1404403532672,
        "Poster_last_access_time":1663877601683,
        "Poster_age":null,
        "Poster_location":"Pennsylvania",
        "Poster_reputation":596.0,
        "Poster_views":65.0,
        "Answer_body":"<p><p><\/p>\n\n<p>I have never used Amazon RCF, but in general tree based models do not perform particularly well when using One Hot Encoding (or dummy encoding). In that sense, I would rather use a Numeric Encoding (giving numbers from 1 to len(category)) or a Binary Encoder (same thing, but with binary variables). This should allow the trees to have more meaningful splits on those variables. <p><\/p>\n\n<p>In terms of Hyperparameters is hard to say, num_samples_per_trees depends on the ratio of outliers you expect to have, while num_trees will impact the amount of data in each partition, and therefore the size of the single trees, so it depends on the size of your dataset. <p><\/p>\n\n<p>Try changing these things, and if you see no improvement you can try different stuff. but I suggest DBSCAN over Kmeans honestly, but to my knowledge they all need the definition of some distance or measure between your points, which is not trivial since you are using a mix of categorical an numeric variables <br><\/p>\n\n<p>EDIT: <br>\n1 - No, I dont think there's a way to weight features in RCF, like usually there's no way to do it in any tree based algorithm as long as I know. However, if you use distance based methods (hierarchical clustering, Kmeans, etc.) you define your own distance metric that weights differently on your features <br>\n2 - Well, that's what the algorithm is for. It is supposed to find outliers based on the distribution of all features, not just one. <\/p>\n\n<p><p>\nYou can also try Isolation Forest if you want. It does not require any metric and it is easier to understand than RCF in my opinion.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1575883221236,
        "Answer_score":1.0,
        "Answerer_id":8830950.0,
        "Answerer_created_time":1508927839110,
        "Answerer_last_access_time":1663840004688,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":766.0,
        "Answerer_views":38.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59201775",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1575636502700,
        "Question_original_content":"random cut forest featur normal pre process have troubl understand rcf algorithm particularli expect anticip data pre process complet exampl follow data featur exampl valu record result rcf model train record featur countri dummi categori dummi extrem focus featur anomali approx absolut variat base countri type normal field result strong fact safe result terribl clearli miss overal look guidanc get featur right field field categor dummi result field wonder better like kmean edit context wonder weight wai weight certain variabl categor variabl import exampl countri categori kei attribut want weight categori countri context ensur outlier consid context peer categor data exampl transact airfar expens outlier categori creat number model messi cumbersom right look avail document http doc aw amazon com latest rcf work html describ thank help advanc edit sure critic point dont semi reason result follow hyperparamet num sampl tree num tree",
        "Question_preprocessed_content":"random cut forest featur normal have troubl understand rcf algorithm particularli expect anticip data complet exampl follow record result rcf model extrem focus featur normal field result strong fact safe result terribl clearli miss overal look guidanc get featur right wonder better like kmean edit context wonder weight wai weight certain variabl exampl countri categori kei attribut want weight categori countri context ensur outlier consid context peer exampl transact airfar expens outlier categori creat number model messi cumbersom right look avail document describ thank help advanc edit sure critic point dont result follow hyperparamet",
        "Question_gpt_summary_original":"The user is having difficulty understanding the Random Cut Forest (RCF) algorithm and how it expects data or pre-processing to be completed. The RCF model trained on 500K records for 57 features is extremely focused on the amount feature, and the normalized amount field results are also not strong. The user is looking for guidance on getting the features right and is wondering if they are better off with something like kmeans. The user is also wondering about weighting and context and how to ensure outliers are considered in the context of its peers. The user has looked through most of the available documentation but cannot find anything that describes this.",
        "Question_gpt_summary":"user have difficulti understand random cut forest rcf algorithm expect data pre process complet rcf model train record featur extrem focus featur normal field result strong user look guidanc get featur right wonder better like kmean user wonder weight context ensur outlier consid context peer user look avail document describ",
        "Answer_original_content":"amazon rcf gener tree base model perform particularli hot encod dummi encod sens us numer encod give number len categori binari encod thing binari variabl allow tree meaning split variabl term hyperparamet hard num sampl tree depend ratio outlier expect num tree impact data partit size singl tree depend size dataset try chang thing improv try differ stuff suggest dbscan kmean honestli knowledg need definit distanc measur point trivial mix categor numer variabl edit dont think wai weight featur rcf like usual wai tree base algorithm long know us distanc base method hierarch cluster kmean defin distanc metric weight differ featur algorithm suppos outlier base distribut featur try isol forest want requir metric easier understand rcf opinion",
        "Answer_preprocessed_content":"amazon rcf gener tree base model perform particularli hot encod sens us numer encod binari encod allow tree meaning split variabl term hyperparamet hard depend ratio outlier expect impact data partit size singl tree depend size dataset try chang thing improv try differ stuff suggest dbscan kmean honestli knowledg need definit distanc measur point trivial mix categor numer variabl edit dont think wai weight featur rcf like usual wai tree base algorithm long know us distanc base method defin distanc metric weight differ featur algorithm suppos outlier base distribut featur try isol forest want requir metric easier understand rcf opinion",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Use Numeric Encoding or Binary Encoder instead of One Hot Encoding for better performance of tree-based models like RCF.\n- Adjust Hyperparameters like num_samples_per_trees and num_trees based on the dataset size and expected ratio of outliers.\n- Try DBSCAN instead of Kmeans for clustering, as it can handle a mix of categorical and numeric variables.\n- Use Isolation Forest as an alternative to RCF, as it does not require any metric and is easier to understand.\n- There is no way to weight features in RCF, but distance-based methods like hierarchical clustering and Kmeans allow defining a custom distance metric that can weight features differently.\n- RCF is designed to find outliers based on the distribution of all",
        "Answer_gpt_summary":"possibl solut mention discuss us numer encod binari encod instead hot encod better perform tree base model like rcf adjust hyperparamet like num sampl tree num tree base dataset size expect ratio outlier try dbscan instead kmean cluster handl mix categor numer variabl us isol forest altern rcf requir metric easier understand wai weight featur rcf distanc base method like hierarch cluster kmean allow defin custom distanc metric weight featur differ rcf design outlier base distribut"
    },
    {
        "Question_title":"Is the default DVC behavior to store connection data in git?",
        "Question_body":"<p>I've recently started to play with <a href=\"https:\/\/dvc.org\" rel=\"nofollow noreferrer\">DVC<\/a>, and I was a bit surprised to see the <a href=\"https:\/\/dvc.org\/doc\/start\/data-and-model-versioning#storing-and-sharing\" rel=\"nofollow noreferrer\">getting started docs<\/a> are suggesting to store <code>.dvc\/config<\/code> in git.<\/p>\n<p>This seemed like a fine idea at first, but then I noticed that my Azure Blob Storage account (i.e. my Azure username) is also stored in .dvc\/config, which means it would end up in git. Making it not ideal for team collaboration scenarios.<\/p>\n<p>What's even less ideal (read: really scary) is that connection strings entered using <code>dvc remote modify blah connection_string ...<\/code> also end up in <code>.dvc\/config<\/code>, making them end up in git and, in the case of open source projects, making them end up in <strong>very<\/strong> interesting places.<\/p>\n<p>Am I doing something obviously wrong? I wouldn't expect the getting started docs to go very deep into security issues, but I wouldn't expect them to store connection strings in source control either.<\/p>\n<p>My base assumption is that I'm misunderstanding\/misconfiguring something, I'd be curious to know what.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1635260868803,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":77.0,
        "Poster_id":155697.0,
        "Poster_created_time":1250158552416,
        "Poster_last_access_time":1663847198323,
        "Poster_age":null,
        "Poster_location":"Romania",
        "Poster_reputation":7916.0,
        "Poster_views":801.0,
        "Answer_body":"<p>DVC has few &quot;levels&quot; of config, that can be controlled with proper flag:<\/p>\n<ul>\n<li><code>--local<\/code> - repository level, ignored by git by default - designated for project-scope, sensitive data<\/li>\n<li>project - same as above, not ignored - designated to specify non-sensitive data (it is the default)<\/li>\n<li><code>--global<\/code> \/ <code>--system<\/code> - for common config for more repositories.<\/li>\n<\/ul>\n<p>More information can be found in the <a href=\"https:\/\/dvc.org\/doc\/command-reference\/config#description\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1635332764020,
        "Answer_score":1.0,
        "Answerer_id":3406563.0,
        "Answerer_created_time":1394547235287,
        "Answerer_last_access_time":1664049273327,
        "Answerer_age":null,
        "Answerer_location":"Warsaw, Poland",
        "Answerer_reputation":352.0,
        "Answerer_views":22.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69725612",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_closed_time":1635332461183,
        "Question_original_content":"default behavior store connect data git recent start plai bit surpris get start doc suggest store config git like fine idea notic azur blob storag account azur usernam store config mean end git make ideal team collabor scenario ideal read scari connect string enter remot modifi blah connect string end config make end git case open sourc project make end interest place obvious wrong wouldn expect get start doc deep secur issu wouldn expect store connect string sourc control base assumpt misunderstand misconfigur curiou know",
        "Question_preprocessed_content":"default behavior store connect data git recent start plai bit surpris get start doc suggest store git like fine idea notic azur blob storag account store mean end git make ideal team collabor scenario ideal connect string enter end make end git case open sourc project make end interest place obvious wrong wouldn expect get start doc deep secur issu wouldn expect store connect string sourc control base assumpt curiou know",
        "Question_gpt_summary_original":"The user is concerned about the default behavior of DVC to store connection data in git, as it includes sensitive information such as their Azure Blob Storage account and connection strings. This makes it unsuitable for team collaboration and open source projects. The user is seeking clarification on whether they are misunderstanding or misconfiguring something.",
        "Question_gpt_summary":"user concern default behavior store connect data git includ sensit inform azur blob storag account connect string make unsuit team collabor open sourc project user seek clarif misunderstand misconfigur",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"Endpoints cost on Azure Machine Learning",
        "Question_body":"<p>I have been following the learning path for <a href=\"https:\/\/docs.microsoft.com\/en-us\/learn\/certifications\/exams\/ai-900\" rel=\"nofollow noreferrer\">Microsoft Azure AI 900<\/a>. In the second module, I have deployed my model as an endpoint. It says Container instances for compute type. How much will this cost me. Azure doesn't seem to show any pricing for this. Is this endpoint always active? If yes how much does it cost?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1635485800157,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3,
        "Question_view_count":633.0,
        "Poster_id":11941095.0,
        "Poster_created_time":1566078293736,
        "Poster_last_access_time":1663949466116,
        "Poster_age":null,
        "Poster_location":"Mumbai, Maharashtra, India",
        "Poster_reputation":449.0,
        "Poster_views":77.0,
        "Answer_body":"<p>The price depends on the number of <strong>vCPU<\/strong> and <strong>GBs<\/strong> of memory requested for the container group. You are charged based on the <strong>vCPU request<\/strong> for your container group rounded up to the nearest whole number for the duration (measured in seconds) <strong>your instance is running<\/strong>. You are also charged for the <strong>GB request<\/strong> for your container group rounded up to the nearest tenths place for the duration (measured in seconds) your <strong>container group is running<\/strong>. There is an additional charge of $0.000012 per vCPU second for Windows software duration on Windows container groups. Check here <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-instances\/\" rel=\"nofollow noreferrer\">Pricing - Container Instances | Microsoft Azure<\/a> for details<\/p>\n<ul>\n<li>After Deployed the Azure Machine Learning managed online endpoint (preview).<\/li>\n<li>Have at least <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/role-based-access-control\/role-assignments-portal.md\" rel=\"nofollow noreferrer\">Billing Reader<\/a> access on the subscription where the endpoint is deployed<\/li>\n<\/ul>\n<p>To know the costs estimation<\/p>\n<ol>\n<li><p>In the <a href=\"https:\/\/portal.azure.com\/\" rel=\"nofollow noreferrer\">Azure portal<\/a>, Go to your subscription<\/p>\n<\/li>\n<li><p>Select <strong>Cost Analysis<\/strong> for your subscription.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/W2eaRIO.png\" alt=\"enter image description here\" \/><\/p>\n<p>Create a filter to scope data to your Azure Machine learning workspace resource:<\/p>\n<ol>\n<li><p>At the top navigation bar, select <strong>Add filter<\/strong>.<\/p>\n<\/li>\n<li><p>In the first filter dropdown, select <strong>Resource<\/strong> for the filter type.<\/p>\n<\/li>\n<li><p>In the second filter dropdown, select your Azure Machine Learning workspace.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/HEvprph.png\" alt=\"enter image description here\" \/><\/p>\n<p>Create a tag filter to show your managed online endpoint and\/or managed online deployment:<\/p>\n<ol>\n<li><p>Select <strong>Add filter<\/strong> &gt; <strong>Tag<\/strong> &gt; <strong>azuremlendpoint<\/strong>: &quot;&lt; your endpoint name&gt;&quot;<\/p>\n<\/li>\n<li><p>Select <strong>Add filter<\/strong> &gt; <strong>Tag<\/strong> &gt; <strong>azuremldeployment<\/strong>: &quot;&lt; your deployment name&gt;&quot;.<\/p>\n<\/li>\n<\/ol>\n<p><img src=\"https:\/\/i.imgur.com\/1aapYGB.png\" alt=\"enter image description here\" \/><\/p>\n<p>Refer  <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-view-online-endpoints-costs.md\" rel=\"nofollow noreferrer\">here <\/a> for more detailed steps<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":15997690.0,
        "Answerer_created_time":1621658973823,
        "Answerer_last_access_time":1664015452383,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":3213.0,
        "Answerer_views":1896.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69764100",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1635505501916,
        "Question_original_content":"endpoint cost follow learn path microsoft azur second modul deploi model endpoint sai contain instanc comput type cost azur price endpoint activ ye cost",
        "Question_preprocessed_content":"endpoint cost follow learn path microsoft azur second modul deploi model endpoint sai contain instanc comput type cost azur price endpoint activ ye cost",
        "Question_gpt_summary_original":"The user is facing challenges related to the cost of deploying a model as an endpoint on Azure Machine Learning. They are unsure about the pricing for container instances as a compute type and whether the endpoint is always active, which affects the cost.",
        "Question_gpt_summary":"user face challeng relat cost deploi model endpoint unsur price contain instanc comput type endpoint activ affect cost",
        "Answer_original_content":"price depend number vcpu gb memori request contain group charg base vcpu request contain group round nearest number durat measur second instanc run charg request contain group round nearest tenth place durat measur second contain group run addit charg vcpu second window softwar durat window contain group check price contain instanc microsoft azur detail deploi manag onlin endpoint preview bill reader access subscript endpoint deploi know cost estim azur portal subscript select cost analysi subscript creat filter scope data workspac resourc navig bar select add filter filter dropdown select resourc filter type second filter dropdown select workspac creat tag filter manag onlin endpoint manag onlin deploy select add filter tag endpoint select add filter tag deploy refer detail step",
        "Answer_preprocessed_content":"price depend number vcpu gb memori request contain group charg base vcpu request contain group round nearest number durat instanc run charg request contain group round nearest tenth place durat contain group run addit charg vcpu second window softwar durat window contain group check price contain instanc microsoft azur detail deploi manag onlin endpoint bill reader access subscript endpoint deploi know cost estim azur portal subscript select cost analysi subscript creat filter scope data workspac resourc navig bar select add filter filter dropdown select resourc filter type second filter dropdown select workspac creat tag filter manag onlin endpoint manag onlin deploy select add filter tag endpoint endpoint select add filter tag deploy deploy refer detail step",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- The cost of deploying a model as an endpoint on Azure Machine Learning depends on the number of vCPU and GBs of memory requested for the container group. The user is charged based on the vCPU request and GB request for the duration the container group is running.\n- There is an additional charge of $0.000012 per vCPU second for Windows software duration on Windows container groups.\n- To estimate the costs, the user can go to the Azure portal, select Cost Analysis for their subscription, create a filter to scope data to their Azure Machine Learning workspace resource, and create a tag filter to show their managed online endpoint and\/or managed online deployment. The user needs to have Billing Reader access on the",
        "Answer_gpt_summary":"possibl solut mention discuss cost deploi model endpoint depend number vcpu gb memori request contain group user charg base vcpu request request durat contain group run addit charg vcpu second window softwar durat window contain group estim cost user azur portal select cost analysi subscript creat filter scope data workspac resourc creat tag filter manag onlin endpoint manag onlin deploy user need bill reader access"
    },
    {
        "Question_title":"how to write to Azure PipelineData properly?",
        "Question_body":"<p>Im trying to learn Azure, with little luck (yet). All the tutorials show using PipelineData just as a file, when configured in &quot;upload&quot; mode. However, im getting &quot;FileNotFoundError: [Errno 2] No such file or directory: ''&quot; error. I would love to ask a more specific question, but i just can't see what im doing wrong.<\/p>\n<pre><code>from azureml.core import Workspace, Datastore,Dataset,Environment\nfrom azureml.core.compute import ComputeTarget, AmlCompute\nfrom azureml.core.compute_target import ComputeTargetException\nfrom azureml.core.runconfig import RunConfiguration\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.pipeline.steps import PythonScriptStep\nfrom azureml.pipeline.core import Pipeline, PipelineData\nimport os\n\nws = Workspace.from_config()\ndatastore = ws.get_default_datastore()\n\ncompute_name = &quot;cpucluster&quot;\ncompute_target = ComputeTarget(workspace=ws, name=compute_name)\naml_run_config = RunConfiguration()\naml_run_config.target = compute_target\naml_run_config.environment.python.user_managed_dependencies = False\naml_run_config.environment.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['pandas','scikit-learn'], \n    pip_packages=['azureml-sdk', 'azureml-dataprep[fuse,pandas]'], \n    pin_sdk_version=False)\n\noutput1 = PipelineData(&quot;processed_data1&quot;,datastore=datastore, output_mode=&quot;upload&quot;)\nprep_step = PythonScriptStep(\n    name=&quot;dataprep&quot;,\n    script_name=&quot;dataprep.py&quot;,\n    source_directory=os.path.join(os.getcwd(),'dataprep'),\n    arguments=[&quot;--output&quot;, output1],\n    outputs = [output1],\n    compute_target=compute_target,\n    runconfig=aml_run_config,\n    allow_reuse=True\n)\n<\/code><\/pre>\n<p>In the dataprep.py i hve the following:<\/p>\n<pre><code>import numpy, argparse, pandas\nfrom azureml.core import Run\nrun = Run.get_context()\nparser = argparse.ArgumentParser()\nparser.add_argument('--output', dest='output', required=True)\nargs = parser.parse_args()\ndf = pandas.DataFrame(numpy.random.rand(100,3))\ndf.iloc[:, 2] = df.iloc[:,0] + df.iloc[:,1]\nprint(df.iloc[:5,:])\ndf.to_csv(args.output)\n\n<\/code><\/pre>\n<p>So, yeah. pd is supposed to write to the output, but my compute cluster says the following:<\/p>\n<pre><code>&quot;User program failed with FileNotFoundError: [Errno 2] No such file or directory: ''\\&quot;.\n<\/code><\/pre>\n<p>When i dont include the to_csv() function, the cluster does not complain<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1626541888290,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1626559748576,
        "Question_score":4,
        "Question_view_count":404.0,
        "Poster_id":13454552.0,
        "Poster_created_time":1588424911652,
        "Poster_last_access_time":1664069253107,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":59.0,
        "Poster_views":8.0,
        "Answer_body":"<p>Here is an <a href=\"https:\/\/github.com\/james-tn\/highperformance_python_in_azure\/blob\/master\/parallel_python_processing\/pipeline_definition.ipynb\" rel=\"nofollow noreferrer\">example<\/a> for PRS.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-pipeline-core\/azureml.pipeline.core.pipelinedata?view=azure-ml-py\" rel=\"nofollow noreferrer\">PipelineData<\/a> was intended to represent &quot;transient&quot; data from one step to the next one, while OutputDatasetConfig was intended for capturing the final state of a dataset (and hence why you see features like lineage, ADLS support, etc). PipelineData always outputs data in a folder structure like {run_id}{output_name}. OutputDatasetConfig allows to decouple the data from the run and hence it allows you to control where to land the data (although by default it will produce similar folder structure). The OutputDatasetConfig allows even to register the output as a Dataset, where getting rid of such folder structure makes sense. From the docs itself: &quot;Represent how to copy the output of a run and be promoted as a FileDataset. The OutputFileDatasetConfig allows you to specify how you want a particular local path on the compute target to be uploaded to the specified destination&quot;.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-batch-scoring-classification#create-dataset-objects\" rel=\"nofollow noreferrer\">OutFileDatasetConfig<\/a> is a control plane concept to pass data between pipeline steps.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1626668067887,
        "Answer_score":2.0,
        "Answerer_id":11297406.0,
        "Answerer_created_time":1554186784008,
        "Answerer_last_access_time":1662988357176,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":2175.0,
        "Answerer_views":434.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68422680",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1626667519372,
        "Question_original_content":"write azur pipelinedata properli try learn azur littl luck tutori pipelinedata file configur upload mode get filenotfounderror errno file directori error love ask specif question wrong core import workspac datastor dataset environ core comput import computetarget amlcomput core comput target import computetargetexcept core runconfig import runconfigur core conda depend import condadepend pipelin step import pythonscriptstep pipelin core import pipelin pipelinedata import workspac config datastor default datastor comput cpuclust comput target computetarget workspac comput aml run config runconfigur aml run config target comput target aml run config environ python user manag depend fals aml run config environ python conda depend condadepend creat conda packag panda scikit learn pip packag sdk dataprep fuse panda pin sdk version fals output pipelinedata process data datastor datastor output mode upload prep step pythonscriptstep dataprep script dataprep sourc directori path join getcwd dataprep argument output output output output comput target comput target runconfig aml run config allow reus true dataprep hve follow import numpi argpars panda core import run run run context parser argpars argumentpars parser add argument output dest output requir true arg parser pars arg panda datafram numpi random rand iloc iloc iloc print iloc csv arg output yeah suppos write output comput cluster sai follow user program fail filenotfounderror errno file directori dont includ csv function cluster complain",
        "Question_preprocessed_content":"write azur pipelinedata properli try learn azur littl luck tutori pipelinedata file configur upload mode get filenotfounderror file directori error love ask specif question wrong hve follow yeah suppos write output comput cluster sai follow dont includ function cluster complain",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to learn Azure and use PipelineData. They are getting a \"FileNotFoundError\" error when trying to write to the output using pd.to_csv() function. The tutorials they have followed show using PipelineData as a file in \"upload\" mode, but they are unable to identify what they are doing wrong.",
        "Question_gpt_summary":"user encount challeng try learn azur us pipelinedata get filenotfounderror error try write output csv function tutori follow pipelinedata file upload mode unabl identifi wrong",
        "Answer_original_content":"exampl pr pipelinedata intend repres transient data step outputdatasetconfig intend captur final state dataset featur like lineag adl support pipelinedata output data folder structur like run output outputdatasetconfig allow decoupl data run allow control land data default produc similar folder structur outputdatasetconfig allow regist output dataset get rid folder structur make sens doc repres copi output run promot filedataset outputfiledatasetconfig allow specifi want particular local path comput target upload specifi destin outfiledatasetconfig control plane concept pass data pipelin step",
        "Answer_preprocessed_content":"exampl pr pipelinedata intend repres transient data step outputdatasetconfig intend captur final state dataset pipelinedata output data folder structur like outputdatasetconfig allow decoupl data run allow control land data outputdatasetconfig allow regist output dataset get rid folder structur make sens doc repres copi output run promot filedataset outputfiledatasetconfig allow specifi want particular local path comput target upload specifi destin outfiledatasetconfig control plane concept pass data pipelin step",
        "Answer_gpt_summary_original":"Solution: The discussion provides some possible solutions to the challenge. One solution is to use OutputDatasetConfig instead of PipelineData to capture the final state of a dataset. OutputDatasetConfig allows the user to control where to land the data and even register the output as a Dataset. Another solution is to use OutFileDatasetConfig, which is a control plane concept to pass data between pipeline steps.",
        "Answer_gpt_summary":"solut discuss provid possibl solut challeng solut us outputdatasetconfig instead pipelinedata captur final state dataset outputdatasetconfig allow user control land data regist output dataset solut us outfiledatasetconfig control plane concept pass data pipelin step"
    },
    {
        "Question_title":"How to specify source directory and entry point for a SageMaker training job using Boto3 SDK? The use case is start training via Lambda call",
        "Question_body":"<p>I've been running training jobs using SageMaker Python SDK on SageMaker notebook instances and locally using IAM credentials. They are working fine but I want to be able to start a training job via AWS Lambda + Gateway.<\/p>\n<p>Lambda does not support SageMaker SDK (High-level SDK) so I am forced to use the SageMaker client from <code>boto3<\/code> in my Lambda handler, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\n<\/code><\/pre>\n<p>Supposedly this boto3 service-level SDK would give me 100% control, but I can't find the argument or config name to specify a source directory and an entry point. I am running a custom training job that requires some data generation (using Keras generator) on the flight.<\/p>\n<p>Here's an example of my SageMaker SDK call<\/p>\n<pre><code>tf_estimator = TensorFlow(base_job_name='tensorflow-nn-training',\n                          role=sagemaker.get_execution_role(),\n                          source_dir=training_src_path,\n                          code_location=training_code_path,\n                          output_path=training_output_path,\n                          dependencies=['requirements.txt'],\n                          entry_point='main.py',\n                          script_mode=True,\n                          instance_count=1,\n                          instance_type='ml.g4dn.2xlarge',\n                          framework_version='2.3',\n                          py_version='py37',\n                          hyperparameters={\n                              'model-name': 'my-model-name',\n                              'epochs': 1000,\n                              'batch-size': 64,\n                              'learning-rate': 0.01,\n                              'training-split': 0.80,\n                              'patience': 50,\n                          })\n<\/code><\/pre>\n<p>The input path is injected via calling <code>fit()<\/code><\/p>\n<pre class=\"lang-py prettyprint-override\"><code>input_channels = {\n    'train': training_input_path,\n}\ntf_estimator.fit(inputs=input_channels)\n<\/code><\/pre>\n<ul>\n<li><code>source_dir<\/code> is a S3 URI to find my <code>src.zip.gz<\/code> which contains the model and script to\nperform a training.<\/li>\n<li><code>entry_point<\/code> is where the training begins. TensorFlow container simply runs <code>python main.py<\/code><\/li>\n<li><code>code_location<\/code> is a S3 prefix where training source code can be uploaded to if I were to run\nthis training locally using local model and script.<\/li>\n<li><code>output_path<\/code> is a S3 URI where the training job will upload model artifacts to.<\/li>\n<\/ul>\n<p>However, I went through the documentation for <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_training_job\" rel=\"nofollow noreferrer\">SageMaker.Client.create_training_job<\/a>, I couldn't find any field that allows me to set a source directory and entry point.<\/p>\n<p>Here's an example,<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>sagemaker = boto3.client('sagemaker')\nsagemaker.create_training_job(\n    TrainingJobName='tf-training-job-from-lambda',\n    Hyperparameters={} # Same dictionary as above,\n    AlgorithmSpecification={\n        'TrainingImage': '763104351884.dkr.ecr.us-west-1.amazonaws.com\/tensorflow-training:2.3.1-gpu-py37-cu110-ubuntu18.04',\n        'TrainingInputMode': 'File',\n        'EnableSageMakerMetricsTimeSeries': True\n    },\n    RoleArn='My execution role goes here',\n    InputDataConfig=[\n        {\n            'ChannelName': 'train',\n            'DataSource': {\n                'S3DataSource': {\n                    'S3DataType': 'S3Prefix',\n                    'S3Uri': training_input_path,\n                    'S3DataDistributionType': 'FullyReplicated'\n                }\n            },\n            'CompressionType': 'None',\n            'RecordWrapperType': 'None',\n            'InputMode': 'File',\n        }  \n    ],\n    OutputDataConfig={\n        'S3OutputPath': training_output_path,\n    }\n    ResourceConfig={\n        'InstanceType': 'ml.g4dn.2xlarge',\n        'InstanceCount': 1,\n        'VolumeSizeInGB': 16\n    }\n    StoppingCondition={\n        'MaxRuntimeInSeconds': 600 # 10 minutes for testing\n    }\n)\n<\/code><\/pre>\n<p>From the config above, the SDK accepts training input and output location, but which config field allows user to specify the source code directory and entry point?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1614044459343,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":1369.0,
        "Poster_id":7359470.0,
        "Poster_created_time":1483140008952,
        "Poster_last_access_time":1661373883536,
        "Poster_age":null,
        "Poster_location":"San Francisco Bay Area, CA, United States",
        "Poster_reputation":604.0,
        "Poster_views":35.0,
        "Answer_body":"<p>You can pass the source_dir to Hyperparameters like this:<\/p>\n<pre><code>    response = sm_boto3.create_training_job(\n        TrainingJobName=f&quot;{your job name}&quot;),\n        HyperParameters={\n            'model-name': 'my-model-name',\n            'epochs': 1000,\n            'batch-size': 64,\n            'learning-rate': 0.01,\n            'training-split': 0.80,\n            'patience': 50,\n            &quot;sagemaker_program&quot;: &quot;script.py&quot;, # this is where you specify your train script\n            &quot;sagemaker_submit_directory&quot;: &quot;s3:\/\/&quot; + bucket + &quot;\/&quot; + project + &quot;\/&quot; + source, # your s3 URI like s3:\/\/sm\/tensorflow\/source\/sourcedir.tar.gz\n        },\n        AlgorithmSpecification={\n            &quot;TrainingImage&quot;: training_image,\n            ...\n        }, \n<\/code><\/pre>\n<p>Note: make sure it's xxx.tar.gz otherwise. Otherwise Sagemaker will throw errors.<\/p>\n<p>Refer to <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":15967118.0,
        "Answerer_created_time":1621392911643,
        "Answerer_last_access_time":1663763912447,
        "Answerer_age":null,
        "Answerer_location":"London, UK",
        "Answerer_reputation":56.0,
        "Answerer_views":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66325857",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1621508759232,
        "Question_original_content":"specifi sourc directori entri point train job boto sdk us case start train lambda run train job python sdk notebook instanc local iam credenti work fine want abl start train job aw lambda gatewai lambda support sdk high level sdk forc us client boto lambda handler boto client supposedli boto servic level sdk control argument config specifi sourc directori entri point run custom train job requir data gener kera gener flight exampl sdk estim tensorflow base job tensorflow train role execut role sourc dir train src path code locat train code path output path train output path depend requir txt entri point main script mode true instanc count instanc type gdn xlarg framework version version hyperparamet model model epoch batch size learn rate train split patienc input path inject call fit input channel train train input path estim fit input input channel sourc dir uri src zip contain model script perform train entri point train begin tensorflow contain simpli run python main code locat prefix train sourc code upload run train local local model script output path uri train job upload model artifact went document client creat train job couldn field allow set sourc directori entri point exampl boto client creat train job trainingjobnam train job lambda hyperparamet dictionari algorithmspecif trainingimag dkr ecr west amazonaw com tensorflow train gpu ubuntu traininginputmod file enablemetricstimeseri true rolearn execut role goe inputdataconfig channelnam train datasourc sdatasourc sdatatyp sprefix suri train input path sdatadistributiontyp fullyrepl compressiontyp recordwrappertyp inputmod file outputdataconfig soutputpath train output path resourceconfig instancetyp gdn xlarg instancecount volumesizeingb stoppingcondit maxruntimeinsecond minut test config sdk accept train input output locat config field allow user specifi sourc code directori entri point",
        "Question_preprocessed_content":"specifi sourc directori entri point train job boto sdk us case start train lambda run train job python sdk notebook instanc local iam credenti work fine want abl start train job aw lambda gatewai lambda support sdk forc us client lambda handler supposedli boto sdk control argument config specifi sourc directori entri point run custom train job requir data gener flight exampl sdk input path inject call uri contain model script perform train train begin tensorflow contain simpli run prefix train sourc code upload run train local local model script uri train job upload model artifact went document couldn field allow set sourc directori entri point exampl config sdk accept train input output locat config field allow user specifi sourc code directori entri point",
        "Question_gpt_summary_original":"The user is facing a challenge in specifying the source directory and entry point for a SageMaker training job using the Boto3 SDK. They are trying to start a training job via AWS Lambda + Gateway, but Lambda does not support the SageMaker SDK, so they are forced to use the SageMaker client from Boto3 in their Lambda handler. However, they cannot find the argument or config name to specify a source directory and an entry point, which is required for their custom training job that requires data generation on the fly. The user has provided an example of their SageMaker SDK call and their Boto3 SDK call, but they cannot find any field that allows them to set a source directory and entry point in the Boto3 SDK.",
        "Question_gpt_summary":"user face challeng specifi sourc directori entri point train job boto sdk try start train job aw lambda gatewai lambda support sdk forc us client boto lambda handler argument config specifi sourc directori entri point requir custom train job requir data gener fly user provid exampl sdk boto sdk field allow set sourc directori entri point boto sdk",
        "Answer_original_content":"pass sourc dir hyperparamet like respons boto creat train job trainingjobnam job hyperparamet model model epoch batch size learn rate train split patienc program script specifi train script submit directori bucket project sourc uri like tensorflow sourc sourcedir tar algorithmspecif trainingimag train imag note sure tar throw error refer http github com aw amazon exampl blob master python sdk scikit learn randomforest sklearn endend ipynb",
        "Answer_preprocessed_content":"pass hyperparamet like note sure throw error refer",
        "Answer_gpt_summary_original":"Solution: The user can pass the source directory to Hyperparameters in the Boto3 SDK call. They need to specify the train script using the \"sagemaker_program\" field and the S3 URI of the source directory using the \"sagemaker_submit_directory\" field. The user should make sure that the source directory is in xxx.tar.gz format, otherwise, SageMaker will throw errors. The user can refer to the provided link for more information.",
        "Answer_gpt_summary":"solut user pass sourc directori hyperparamet boto sdk need specifi train script program field uri sourc directori submit directori field user sure sourc directori tar format throw error user refer provid link inform"
    },
    {
        "Question_title":"Train machine learning model using reserved instance",
        "Question_body":"Hi.\n\nIs it possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance every time which is somewhat time consuming? I'm familiar with local mode, but I understand this is not supported when using AWS SageMaker machine learning estimators.\n\nAppreciate any suggestions for how to make the model training process in SageMaker go faster when using AWS SageMaker machine learning estimators.\n\nThanks, Stefan",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1641871148701,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":151.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"As of today, it's not possible to train a machine learning model with SageMaker using a reserved instance that is already up and running instead of provisioning a new instance. The service team is currently working on it, unfortunately I don't have an ETA as to when the feature will be released.\n\nLocal Mode is supported for frameworks images (TensorFlow, MXNet, Chainer, PyTorch, and Scikit-Learn) and images you supply yourself.\n\nUsing the SageMaker Python SDK \u2014 sagemaker 2.72.3 documentation\n\nIf you want to train Built-in algorithm models simply faster, you should check the recommendation in the SageMaker document.\n\nExample Blazingtext-instances, Deepar-instances\n\nIf the algorithm supports it, one can also try using Pipe mode or FastFile mode. These offer some fast training job startup time. Accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUsy3vkTMkSA2ojA1bmafDSA\/train-machine-learning-model-using-reserved-instance",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1642148033228,
        "Question_original_content":"train machin learn model reserv instanc possibl train machin learn model reserv instanc run instead provis new instanc time somewhat time consum familiar local mode understand support machin learn estim appreci suggest model train process faster machin learn estim thank stefan",
        "Question_preprocessed_content":"train machin learn model reserv instanc possibl train machin learn model reserv instanc run instead provis new instanc time somewhat time consum familiar local mode understand support machin learn estim appreci suggest model train process faster machin learn estim thank stefan",
        "Question_gpt_summary_original":"The user is facing challenges with the time-consuming process of provisioning a new instance every time to train a machine learning model using SageMaker. They are seeking suggestions to make the model training process faster while using AWS SageMaker machine learning estimators. The user is also wondering if it is possible to use a reserved instance that is already up and running for this purpose.",
        "Question_gpt_summary":"user face challeng time consum process provis new instanc time train machin learn model seek suggest model train process faster machin learn estim user wonder possibl us reserv instanc run purpos",
        "Answer_original_content":"todai possibl train machin learn model reserv instanc run instead provis new instanc servic team current work unfortun eta featur releas local mode support framework imag tensorflow mxnet chainer pytorch scikit learn imag suppli python sdk document want train built algorithm model simpli faster check recommend document exampl blazingtext instanc deepar instanc algorithm support try pipe mode fastfil mode offer fast train job startup time acceler model train faster pipe mode amazon",
        "Answer_preprocessed_content":"todai possibl train machin learn model reserv instanc run instead provis new instanc servic team current work unfortun eta featur releas local mode support framework imag imag suppli python sdk document want train algorithm model simpli faster check recommend document exampl algorithm support try pipe mode fastfil mode offer fast train job startup time",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- It is not currently possible to use a reserved instance that is already up and running for training a machine learning model with SageMaker.\n- Local Mode can be used for frameworks images and images supplied by the user.\n- Check the recommendations in the SageMaker document for faster training of Built-in algorithm models, such as using Blazingtext-instances or Deepar-instances.\n- If the algorithm supports it, one can try using Pipe mode or FastFile mode for faster training job startup time.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss current possibl us reserv instanc run train machin learn model local mode framework imag imag suppli user check recommend document faster train built algorithm model blazingtext instanc deepar instanc algorithm support try pipe mode fastfil mode faster train job startup time person opinion bias includ summari"
    },
    {
        "Question_title":"Executing pipeline in AML from Logic Apps stopped working",
        "Question_body":"Hello! I have a few logic apps for my company that trigger ML pipelines at specific time intervals. I followed the documentation on how to set up a logic app and trigger pipeline to the letter and for the past 2 months everything was working fine and my logic apps were able to trigger the ML pipelines with no issues. However, on 12\/08\/2021 at exactly in between 1:30PM - 2:30PM CST, every single pipeline starting failing and they continue to do so up until now. I noticed that we are now receiving this error on every execution:\n\n\"UserError: Response status code does not indicate success: 400 (User starting the run is not an owner or assigned user to the Compute Instance). User starting the run is not an owner or assigned user to the Compute Instance\"\n\nMy Logic apps are setup with \"Managed Identities\" of Owners (like the documentation explains). My last successful run for all the logic apps was on 12\/08 before 1:30PM CST. Did something change on both Azure Logic Apps and Azure ML that is now causing this issue? Any help is greatly appreciated as this is impacting my company's business.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1639583614587,
        "Question_favorite_count":31.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"I ran into this same issue in a slightly different context. I didn't manage to figure out the root cause but managed to resolve it in practice by standing up a Compute Cluster instead of a Compute Instance (see https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-cluster?tabs=python)",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/664994\/executing-pipeline-in-aml-from-logic-apps-stopped.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1639690591763,
        "Question_original_content":"execut pipelin aml logic app stop work hello logic app compani trigger pipelin specif time interv follow document set logic app trigger pipelin letter past month work fine logic app abl trigger pipelin issu exactli cst singl pipelin start fail continu notic receiv error execut usererror respons statu code indic success user start run owner assign user comput instanc user start run owner assign user comput instanc logic app setup manag ident owner like document explain success run logic app cst chang azur logic app caus issu help greatli appreci impact compani busi",
        "Question_preprocessed_content":"execut pipelin aml logic app stop work hello logic app compani trigger pipelin specif time interv follow document set logic app trigger pipelin letter past month work fine logic app abl trigger pipelin issu exactli cst singl pipelin start fail continu notic receiv error execut usererror respons statu code indic success user start run owner assign user comput instanc logic app setup manag ident owner success run logic app cst chang azur logic app caus issu help greatli appreci impact compani busi",
        "Question_gpt_summary_original":"The user has encountered a challenge with executing pipeline in AML from Logic Apps. The pipelines were working fine for the past 2 months, but on 12\/08\/2021, every single pipeline started failing with an error message indicating that the user starting the run is not an owner or assigned user to the Compute Instance. The Logic apps are set up with \"Managed Identities\" of Owners, as per the documentation. The user is seeking help to resolve the issue as it is impacting their company's business.",
        "Question_gpt_summary":"user encount challeng execut pipelin aml logic app pipelin work fine past month singl pipelin start fail error messag indic user start run owner assign user comput instanc logic app set manag ident owner document user seek help resolv issu impact compani busi",
        "Answer_original_content":"ran issu slightli differ context manag figur root caus manag resolv practic stand comput cluster instead comput instanc http doc microsoft com azur machin learn creat attach comput cluster tab python",
        "Answer_preprocessed_content":"ran issu slightli differ context manag figur root caus manag resolv practic stand comput cluster instead comput instanc",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to stand up a Compute Cluster instead of a Compute Instance. Although the root cause of the issue is not identified, this solution has worked for the person who faced a similar issue. The documentation link for creating and attaching a Compute Cluster is provided for reference.",
        "Answer_gpt_summary":"solut possibl solut mention discuss stand comput cluster instead comput instanc root caus issu identifi solut work person face similar issu document link creat attach comput cluster provid refer"
    },
    {
        "Question_title":"Training with TensorFlow on Sagemaker No module named 'tf_container'",
        "Question_body":"<p>I'm trying to training TensorFlow model on AWS Sagemaker.\nI created container with external lib for that (Use Your Own Algorithms or Models with Amazon SageMaker).<\/p>\n\n<p>we run a training job with TensorFlow API<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\nestimator = TensorFlow(\n  entry_point=\"entry.py\",             # entry script\n  role=role,\n  framework_version=\"1.13.0\",   \n  py_version='py3',\n  hyperparameters=hyperparameters,\n  train_instance_count=1,                   # \"The number of GPUs instances to use\"\n  train_instance_type=train_instance_type,\n  image_name=my_image\n\n)\nestimator.fit({'train': train_s3, 'eval': eval_s3})\n<\/code><\/pre>\n\n<p>and got an error:<\/p>\n\n<pre><code>09:06:46\n2019-07-23 09:06:45,463 INFO - root - running container entrypoint \uf141\n09:06:46\n2019-07-23 09:06:45,463 INFO - root - starting train task \uf141\n09:06:46\n2019-07-23 09:06:45,476 INFO - container_support.training - Training starting \uf141\n09:06:46\n2019-07-23 09:06:45,479 ERROR - container_support.training - uncaught exception during training: No module named 'tf_container'\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 136, in load_framework return importlib.import_module('mxnet_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"&lt;frozen importlib._bootstrap&gt;\", line 994, in _gcd_i \uf141\n09:06:46\nModuleNotFoundError: No module named 'mxnet_container'\n\uf141\n09:06:46\nDuring handling of the above exception, another exception occurred:\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/training.py\", line 35, in start fw = TrainingEnvironment.load_framework() File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 138, in load_framework return importlib.import_module('tf_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, \uf141\n09:06:46\nModuleNotFoundError: No module named 'tf_container'\n<\/code><\/pre>\n\n<p>What can I do to solve this issue? how can I debug this case?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1563885123080,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":1191.0,
        "Poster_id":6747729.0,
        "Poster_created_time":1471952551663,
        "Poster_last_access_time":1662451190670,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":53.0,
        "Poster_views":2.0,
        "Answer_body":"<p>I'm guessing that you used your own TF container, not the SageMaker one at <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>\n\n<p>If that's the case, your container is missing the support code needed to use the TensorFlow estimator ('tf_container' package).<\/p>\n\n<p>The solution is to start from the SageMaker container, customize it, push it back to ECR and pass the image name to the SageMaker estimator with the 'image_name' parameter.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":4686192.0,
        "Answerer_created_time":1426694564423,
        "Answerer_last_access_time":1659633906700,
        "Answerer_age":null,
        "Answerer_location":"Paris",
        "Answerer_reputation":2425.0,
        "Answerer_views":459.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57164290",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1563901268600,
        "Question_original_content":"train tensorflow modul name contain try train tensorflow model creat contain extern lib us algorithm model run train job tensorflow api tensorflow import tensorflow estim tensorflow entri point entri entri script role role framework version version hyperparamet hyperparamet train instanc count number gpu instanc us train instanc type train instanc type imag imag estim fit train train eval eval got error info root run contain entrypoint info root start train task info contain support train train start error contain support train uncaught except train modul name contain traceback recent file usr local lib python dist packag contain support environ line load framework return importlib import modul mxnet contain file usr lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd modulenotfounderror modul name mxnet contain handl except except occur traceback recent file usr local lib python dist packag contain support train line start trainingenviron load framework file usr local lib python dist packag contain support environ line load framework return importlib import modul contain file usr lib python importlib init line modulenotfounderror modul name contain solv issu debug case",
        "Question_preprocessed_content":"train tensorflow modul name try train tensorflow model creat contain extern lib run train job tensorflow api got error solv issu debug case",
        "Question_gpt_summary_original":"The user is encountering an error while trying to train a TensorFlow model on AWS Sagemaker. The error message indicates that there is no module named 'tf_container'. The user is seeking advice on how to solve this issue and how to debug the problem.",
        "Question_gpt_summary":"user encount error try train tensorflow model error messag indic modul name contain user seek advic solv issu debug problem",
        "Answer_original_content":"guess contain http github com aw tensorflow contain case contain miss support code need us tensorflow estim contain packag solut start contain custom push ecr pass imag estim imag paramet",
        "Answer_preprocessed_content":"guess contain case contain miss support code need us tensorflow estim solut start contain custom push ecr pass imag estim paramet",
        "Answer_gpt_summary_original":"Solution: The user needs to start from the SageMaker container, customize it, push it back to ECR, and pass the image name to the SageMaker estimator with the 'image_name' parameter.",
        "Answer_gpt_summary":"solut user need start contain custom push ecr pass imag estim imag paramet"
    },
    {
        "Question_title":"SageMaker: ClientError: .lst file missing in the train_lst channel. (customized image classification)",
        "Question_body":"<p><strong><em>Problem:<\/em><\/strong><\/p>\n\n<p>Jobs repeatedly fail after 5 minutes with the error<\/p>\n\n<blockquote>\n  <p>ClientError: .lst file missing in the train_lst channel.<\/p>\n<\/blockquote>\n\n<p><strong><em>Context:<\/em><\/strong><\/p>\n\n<p>Working within the AWS console, I have a binary classification task of images. I have labeled the classes in their filenames, per a guide.<\/p>\n\n<p>Eventually I started hitting errors that revealed that for this particular algorithm a <code>.lst<\/code> file is required for gathering the labels, since \"Content Type\" is specified as image, which apparently requires a lst file.<\/p>\n\n<p><strong><em>Example Data:<\/em><\/strong><\/p>\n\n<p>I am trying to match the examples I see on <a href=\"https:\/\/stackoverflow.com\/questions\/51670563\/invalid-lst-file-in-sagemaker\">StackOverflow<\/a> and elsewhere online. The current iteration of <code>trn_list.lst<\/code> looks like this:<\/p>\n\n<pre><code>292 \\t 1 \\t dog-292.jpeg\n214 \\t 1 \\t dog-214.jpeg\n290 \\t 0 \\t cat-290.jpeg\n288 \\t 1 \\t dog-288.jpeg\n160 \\t 1 \\t dog-160.jpeg\n18 \\t 0 \\t cat-18.jpeg\n215 \\t 1 \\t dog-215.jpeg\n254 \\t 1 \\t dog-254.jpeg\n53 \\t 1 \\t dog-53.jpeg\n337 \\t 0 \\t cat-337.jpeg\n284 \\t 0 \\t cat-284.jpeg\n177 \\t 1 \\t dog-177.jpeg\n192 \\t 1 \\t dog-192.jpeg\n228 \\t 0 \\t cat-228.jpeg\n305 \\t 0 \\t cat-305.jpeg\n258 \\t 1 \\t dog-258.jpeg\n75 \\t 0 \\t cat-75.jpeg\n148 \\t 0 \\t cat-148.jpeg\n268 \\t 1 \\t dog-268.jpeg\n281 \\t 1 \\t dog-281.jpeg\n24 \\t 1 \\t dog-24.jpeg\n328 \\t 1 \\t dog-328.jpeg\n99 \\t 1 \\t dog-99.jpeg\n<\/code><\/pre>\n\n<p>The bucket has no sub-folders, so I just put the .lst on the <\/p>\n\n<p>In one iteration I allowed my R program that creates the .lst to replace the <code>\\t<\/code> with actual tabs when it writes it out. In other iterations I left the actual delimiters (<code>\\t<\/code>) in there. Didn't seem to affect it (?).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1559761618273,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":608.0,
        "Poster_id":3604745.0,
        "Poster_created_time":1399301338467,
        "Poster_last_access_time":1664044371256,
        "Poster_age":null,
        "Poster_location":"Columbia, MD, USA",
        "Poster_reputation":21413.0,
        "Poster_views":6465.0,
        "Answer_body":"<p>When you are using SageMaker training jobs you are actually deploying a Docker image to a cluster of EC2 instances. The Docker has a python file that is running the training code in a similar way that you train it on your machine. In the training code you are referring to local folders when it expects to find the data such as the images to train on and the meta-data to use for that training. <\/p>\n\n<p>The \"magic\" is how to get the data from S3 to be available locally for the training instances. This is done using the definition of the channels in your training job configuration. Each channel definition creates a local folder on the training instance and copies the data from S3 to that local folder. You need to match the names and the S3 location and file formats.<\/p>\n\n<p>Here is the documentation of the definition of a channel in SageMaker: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_Channel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_Channel.html<\/a><\/p>\n\n<p>For the specific example of the built-in algorithm for image classification and if you use the Image format for training, specify <code>train<\/code>, <code>validation<\/code>, <code>train_lst<\/code>, and <code>validation_lst<\/code> channels as values for the <code>InputDataConfig<\/code> parameter of the <code>CreateTrainingJob<\/code> request. Specify the individual image data (.jpg or .png files) for the train and validation channels. Specify one .lst file in each of the train_lst and validation_lst channels. Set the content type for all four channels to <code>application\/x-image<\/code>.<\/p>\n\n<p>See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html#IC-inputoutput\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html#IC-inputoutput<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1559985734928,
        "Answer_score":3.0,
        "Answerer_id":179529.0,
        "Answerer_created_time":1253986272627,
        "Answerer_last_access_time":1663931086832,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":11930.0,
        "Answerer_views":2649.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56466592",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1559907535367,
        "Question_original_content":"clienterror lst file miss train lst channel custom imag classif problem job repeatedli fail minut error clienterror lst file miss train lst channel context work aw consol binari classif task imag label class filenam guid eventu start hit error reveal particular algorithm lst file requir gather label content type specifi imag appar requir lst file exampl data try match exampl stackoverflow onlin current iter trn list lst look like dog jpeg dog jpeg cat jpeg dog jpeg dog jpeg cat jpeg dog jpeg dog jpeg dog jpeg cat jpeg cat jpeg dog jpeg dog jpeg cat jpeg cat jpeg dog jpeg cat jpeg cat jpeg dog jpeg dog jpeg dog jpeg dog jpeg dog jpeg bucket sub folder lst iter allow program creat lst replac actual tab write iter left actual delimit affect",
        "Question_preprocessed_content":"clienterror lst file miss channel problem job repeatedli fail minut error clienterror lst file miss channel context work aw consol binari classif task imag label class filenam guid eventu start hit error reveal particular algorithm file requir gather label content type specifi imag appar requir lst file exampl data try match exampl stackoverflow onlin current iter look like bucket lst iter allow program creat lst replac actual tab write iter left actual delimit affect",
        "Question_gpt_summary_original":"The user is encountering a problem with AWS SageMaker where jobs repeatedly fail after 5 minutes with the error \"ClientError: .lst file missing in the train_lst channel.\" The user is working on a binary classification task of images and has labeled the classes in their filenames. However, the algorithm requires a .lst file for gathering the labels, which the user did not initially provide. The user has tried creating a .lst file and placing it in the bucket, but the error persists. The user has also tried different delimiters in the .lst file, but it did not seem to affect the error.",
        "Question_gpt_summary":"user encount problem job repeatedli fail minut error clienterror lst file miss train lst channel user work binari classif task imag label class filenam algorithm requir lst file gather label user initi provid user tri creat lst file place bucket error persist user tri differ delimit lst file affect error",
        "Answer_original_content":"train job actual deploi docker imag cluster instanc docker python file run train code similar wai train machin train code refer local folder expect data imag train meta data us train magic data avail local train instanc definit channel train job configur channel definit creat local folder train instanc copi data local folder need match name locat file format document definit channel http doc aw amazon com latest api channel html specif exampl built algorithm imag classif us imag format train specifi train valid train lst valid lst channel valu inputdataconfig paramet createtrainingjob request specifi individu imag data jpg png file train valid channel specifi lst file train lst valid lst channel set content type channel applic imag detail http doc aw amazon com latest imag classif html inputoutput",
        "Answer_preprocessed_content":"train job actual deploi docker imag cluster instanc docker python file run train code similar wai train machin train code refer local folder expect data imag train us train magic data avail local train instanc definit channel train job configur channel definit creat local folder train instanc copi data local folder need match name locat file format document definit channel specif exampl algorithm imag classif us imag format train specifi channel valu paramet request specifi individu imag data train valid channel specifi lst file channel set content type channel detail",
        "Answer_gpt_summary_original":"Solution: The user needs to define the channels in their training job configuration to match the names and S3 location and file formats of the data. For the specific example of the built-in algorithm for image classification, the user needs to specify train, validation, train_lst, and validation_lst channels as values for the InputDataConfig parameter of the CreateTrainingJob request. The user also needs to specify one .lst file in each of the train_lst and validation_lst channels and set the content type for all four channels to application\/x-image. The user can find more details on this in the documentation provided.",
        "Answer_gpt_summary":"solut user need defin channel train job configur match name locat file format data specif exampl built algorithm imag classif user need specifi train valid train lst valid lst channel valu inputdataconfig paramet createtrainingjob request user need specifi lst file train lst valid lst channel set content type channel applic imag user detail document provid"
    },
    {
        "Question_title":"Update live AWS Sagemaker auto scaled endpoint instance type without putting it down",
        "Question_body":"<p>I have one live AWS Sagemaker endpoint where we have auto scaled enabled. \nNow I want to updated it from 'ml.t2.xlarge' to 'ml.t2.2xlarge' but it is showing this error <\/p>\n\n<pre><code>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the \nUpdateEndpoint operation: The variant(s) \"[config1]\" must be deregistered as scalable targets with \nApplication Auto Scaling before they can be removed or have their instance type updated.\n<\/code><\/pre>\n\n<p>I believe we need to first de-register auto-scaling using this link \n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html<\/a><\/p>\n\n<p>but I doubt if will take our application down and the new model with training will take multiple hours. We can't afford this so please let me know if there are any better way to do it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1582793187027,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":869.0,
        "Poster_id":8388707.0,
        "Poster_created_time":1501403168107,
        "Poster_last_access_time":1663685130870,
        "Poster_age":null,
        "Poster_location":"Delhi, India",
        "Poster_reputation":1370.0,
        "Poster_views":125.0,
        "Answer_body":"<p>You should have no problem updating your Endpoint instance type without taking the availability hit. The basic method looks like this when you have an active autoscaling policy:<\/p>\n\n<ol>\n<li>Create a new EndpointConfig that uses the new instance type, <code>ml.t2.2xlarge<\/code>\n\n<ol>\n<li>Do this by calling <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_CreateEndpointConfig.html\" rel=\"nofollow noreferrer\"><code>CreateEndpointConfig<\/code><\/a>.<\/li>\n<li>Pass in the same values you used for your previous Endpoint config. You can point to the same <code>ModelName<\/code> that you did as well. By reusing the same model, you don't have to retrain it or anything<\/li>\n<\/ol><\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-delete.html\" rel=\"nofollow noreferrer\">Delete the existing autoscaling policy<\/a>\n\n<ol>\n<li>Depending on your autoscaling, you might want to increase the desired count of your Endpoint in the event it needs to scale while you are doing this.<\/li>\n<li>If you are experience a spike in traffic while you are making these API calls, you risk an outage of your model if it can't keep up with traffic. Just keep this in mind and possibly scale in advance for this possibility.<\/li>\n<\/ol><\/li>\n<li>Call <code>UpdateEndpoint<\/code> like you did previously and specify this new <code>EndpointConfigName<\/code><\/li>\n<li>Wait for your Endpoint status to be <code>InService<\/code>. This should take 10-20 mins.<\/li>\n<li><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/endpoint-auto-scaling-add-policy.html\" rel=\"nofollow noreferrer\">Create a new autoscaling policy<\/a> for this new Endpoint and production variant<\/li>\n<\/ol>\n\n<p>You should be good to go without sacrificing availability.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":2337627.0,
        "Answerer_created_time":1367358004727,
        "Answerer_last_access_time":1660515323110,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA",
        "Answerer_reputation":6421.0,
        "Answerer_views":642.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60429339",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1582835892307,
        "Question_original_content":"updat live auto scale endpoint instanc type put live endpoint auto scale enabl want updat xlarg xlarg show error botocor except clienterror error occur validationexcept call updateendpoint oper variant config deregist scalabl target applic auto scale remov instanc type updat believ need regist auto scale link http doc aw amazon com latest endpoint auto scale delet html doubt applic new model train multipl hour afford let know better wai",
        "Question_preprocessed_content":"updat live auto scale endpoint instanc type put live endpoint auto scale enabl want updat show error believ need link doubt applic new model train multipl hour afford let know better wai",
        "Question_gpt_summary_original":"The user is facing a challenge in updating the instance type of a live AWS Sagemaker endpoint with auto scaling enabled. They received an error message stating that the variant(s) must be deregistered as scalable targets with Application Auto Scaling before updating the instance type. The user is concerned that de-registering auto scaling may take their application down and the new model with training will take multiple hours, which they cannot afford. They are seeking advice on a better way to update the instance type without affecting their application.",
        "Question_gpt_summary":"user face challeng updat instanc type live endpoint auto scale enabl receiv error messag state variant deregist scalabl target applic auto scale updat instanc type user concern regist auto scale applic new model train multipl hour afford seek advic better wai updat instanc type affect applic",
        "Answer_original_content":"problem updat endpoint instanc type take avail hit basic method look like activ autosc polici creat new endpointconfig us new instanc type xlarg call createendpointconfig pass valu previou endpoint config point modelnam reus model retrain delet exist autosc polici depend autosc want increas desir count endpoint event need scale experi spike traffic make api call risk outag model traffic mind possibl scale advanc possibl updateendpoint like previous specifi new endpointconfignam wait endpoint statu inservic min creat new autosc polici new endpoint product variant good sacrif avail",
        "Answer_preprocessed_content":"problem updat endpoint instanc type take avail hit basic method look like activ autosc polici creat new endpointconfig us new instanc type call pass valu previou endpoint config point reus model retrain delet exist autosc polici depend autosc want increas desir count endpoint event need scale experi spike traffic make api call risk outag model traffic mind possibl scale advanc possibl like previous specifi new wait endpoint statu min creat new autosc polici new endpoint product variant good sacrif avail",
        "Answer_gpt_summary_original":"Solution:\n- Create a new EndpointConfig that uses the new instance type, ml.t2.2xlarge.\n- Delete the existing autoscaling policy.\n- Call UpdateEndpoint like you did previously and specify this new EndpointConfigName.\n- Wait for your Endpoint status to be InService. This should take 10-20 mins.\n- Create a new autoscaling policy for this new Endpoint and production variant.",
        "Answer_gpt_summary":"solut creat new endpointconfig us new instanc type xlarg delet exist autosc polici updateendpoint like previous specifi new endpointconfignam wait endpoint statu inservic min creat new autosc polici new endpoint product variant"
    },
    {
        "Question_title":"mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='input_rows' was already logged with value='32205'",
        "Question_body":"<p>I am using Mlflow as a work orchestration tool. I have a Machine Learning pipeline. In this pipeline, I have real-time data. I'm listening this data with Apache Kafka. Also, I'm doing this: Whenever 250 message comes to this topic, I'm gathering them, and I'm appending this message my previous data. After that, my training function is triggered. Thus, I am able to making new training in every 250 new data. With Mlflow, I can show the results, metrics and any other parameters of trained models. But After training occurred one time, the second one doesn't occurs, and It throws me this error which I have shown in title. Here it is my consumer:<\/p>\n<pre><code>topic_name = 'twitterdata'\ntrain_every = 250\n\n\ndef consume_tweets():\n    consumer = KafkaConsumer(\n        topic_name,\n        bootstrap_servers=['localhost:9093'],\n        auto_offset_reset='latest',\n        enable_auto_commit=True,\n        auto_commit_interval_ms=5000,\n        fetch_max_bytes=128,\n        max_poll_records=100,\n        value_deserializer=lambda x: json.loads(x.decode('utf-8')))\n\n    tweet_counter = 0\n    for message in consumer:\n        tweets = json.loads(json.dumps(message.value))\n        # print(tweets['text'])\n        tweet_sentiment = make_prediction(tweets['text'])\n\n        if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n\n        else:\n            tweet_counter += 1\n\n        publish_prediction(tweet_sentiment, tweets['text'])\n\n<\/code><\/pre>\n<p>And here it is my train.py:<\/p>\n<pre><code>train_tweets = pd.read_csv(DATA_PATH)\n    # train_tweets = train_tweets[:20000]\n\n    tweets = train_tweets.tweet.values\n    labels = train_tweets.label.values\n\n    # Log data params\n    mlflow.log_param('input_rows', train_tweets.shape[0])\n\n    # Do preprocessing and return vectorizer with it\n    vectorizer, processed_features = embedding(tweets)\n\n    # Saving vectorizer\n    save_vectorizer(vectorizer)\n\n    # Split data\n    X_train, X_test, y_train, y_test = train_test_split(processed_features, labels, test_size=0.2, random_state=0)\n\n    # Handle imbalanced data by using 'Smote' and log to Mlflow\n    smote = SMOTE('minority')\n    mlflow.log_param(&quot;over-sampling&quot;, smote)\n\n    X_train, y_train = smote.fit_sample(X_train, y_train)\n\n    # text_classifier = MultinomialNB()\n    text_classifier = LogisticRegression(max_iter=10000)\n    text_classifier.fit(X_train, y_train)\n    predictions = text_classifier.predict(X_test)\n\n    # Model metrics\n    (rmse, mae, r2) = eval_metrics(y_test, predictions)\n\n    mlflow.log_param('os-row-Xtrain', X_train.shape[0])\n    mlflow.log_param('os-row-ytrain', y_train.shape[0])\n    mlflow.log_param(&quot;model_name&quot;, text_classifier)\n    mlflow.log_metric(&quot;rmse&quot;, rmse)\n    mlflow.log_metric(&quot;r2&quot;, r2)\n    mlflow.log_metric(&quot;mae&quot;, mae)\n    mlflow.log_metric('acc_score', accuracy_score(y_test, predictions))\n\n    mlflow.sklearn.log_model(text_classifier, &quot;model&quot;)\n<\/code><\/pre>\n<p>I couldn't solve the problem. MLflow is one of the newest tool, so issues and examples of Mlflow are very few.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1614014592930,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1614015098323,
        "Question_score":1,
        "Question_view_count":2716.0,
        "Poster_id":15197291.0,
        "Poster_created_time":1613130263950,
        "Poster_last_access_time":1663846837707,
        "Poster_age":null,
        "Poster_location":"Turkey",
        "Poster_reputation":44.0,
        "Poster_views":12.0,
        "Answer_body":"<p>I think you need an MLflow &quot;run&quot; for every new batch of data, so that your parameters are logged independently for each new training.<\/p>\n<p>So, try the following in your consumer:<\/p>\n<pre><code>if tweet_counter == train_every:\n            update_df()\n            data_path = 'data\/updated_tweets.csv'\n            with mlflow.start_run() as mlrun:\n               train(data_path)\n            print(&quot;\\nTraining with new data is completed!\\n&quot;)\n            tweet_counter = 0\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":8921612.0,
        "Answerer_created_time":1510344022336,
        "Answerer_last_access_time":1662019097927,
        "Answerer_age":null,
        "Answerer_location":"Moscow, Russia",
        "Answerer_reputation":1182.0,
        "Answerer_views":117.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66320435",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1614350710747,
        "Question_original_content":"except except chang param valu allow param kei input row log valu work orchestr tool machin learn pipelin pipelin real time data listen data apach kafka messag come topic gather append messag previou data train function trigger abl make new train new data result metric paramet train model train occur time second occur throw error shown titl consum topic twitterdata train def consum tweet consum kafkaconsum topic bootstrap server localhost auto offset reset latest enabl auto commit true auto commit interv fetch max byte max poll record valu deseri lambda json load decod utf tweet counter messag consum tweet json load json dump messag valu print tweet text tweet sentiment predict tweet text tweet counter train updat data path data updat tweet csv train data path print ntrain new data complet tweet counter tweet counter publish predict tweet sentiment tweet text train train tweet read csv data path train tweet train tweet tweet train tweet tweet valu label train tweet label valu log data param log param input row train tweet shape preprocess return vector vector process featur embed tweet save vector save vector vector split data train test train test train test split process featur label test size random state handl imbalanc data smote log smote smote minor log param sampl smote train train smote fit sampl train train text classifi multinomialnb text classifi logisticregress max iter text classifi fit train train predict text classifi predict test model metric rmse mae eval metric test predict log param row xtrain train shape log param row ytrain train shape log param model text classifi log metric rmse rmse log metric log metric mae mae log metric acc score accuraci score test predict sklearn log model text classifi model couldn solv problem newest tool issu exampl",
        "Question_preprocessed_content":"chang param valu allow param log valu work orchestr tool machin learn pipelin pipelin data listen data apach kafka messag come topic gather append messag previou data train function trigger abl make new train new data result metric paramet train model train occur time second occur throw error shown titl consum couldn solv problem newest tool issu exampl",
        "Question_gpt_summary_original":"The user is encountering an error while using Mlflow as a work orchestration tool for a machine learning pipeline. The pipeline involves real-time data gathered from Apache Kafka, and the training function is triggered every time 250 new data is received. However, after the first training, the second one does not occur, and the error \"mlflow.exceptions.MlflowException: Changing param values is not allowed. Param with key='input_rows' was already logged with value='32205'\" is thrown. The user has provided the code for the consumer and train.py but has been unable to solve the problem.",
        "Question_gpt_summary":"user encount error work orchestr tool machin learn pipelin pipelin involv real time data gather apach kafka train function trigger time new data receiv train second occur error except except chang param valu allow param kei input row log valu thrown user provid code consum train unabl solv problem",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"Version 2 model in natural language API",
        "Question_body":"Hi, could anyone share the python code on how to get  natural language API to use version 2 classify text  categories?I can get it working well with the default (version 1) categories but can't figure out where to adapt the standard code (as here: https:\/\/cloud.google.com\/natural-language\/docs\/samples\/language-classify-text-tutorial-classify?hl=e...)  to  use model version 2.Many thanks ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1667301900000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":63.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"From the Classifying Content guide, you can include classification_model_options\u00a0within the request\u00a0dictionary argument to the classify_text()\u00a0function. In these options, you can define the model and version to use for content categories.\n\n\/\/ ...\ncontent_categories_version = (\n        language_v1.ClassificationModelOptions.V2Model.ContentCategoriesVersion.V2) \/\/ Assigning the v2 model type\n    response = client.classify_text(request = {\n        \"document\": document,\n        \"classification_model_options\": {\n            \"v2_model\": {\n                \"content_categories_version\": content_categories_version\n            }\n        }\n    })\n\/\/ ...\n\n\nYou can also check ClassificationModelOptions\u00a0reference for available options.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Version-2-model-in-natural-language-API\/td-p\/484641\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1667406420000,
        "Question_original_content":"version model natur languag api share python code natur languag api us version classifi text categori work default version categori figur adapt standard code http cloud googl com natur languag doc sampl languag classifi text tutori classifi us model version thank",
        "Question_preprocessed_content":"version model natur languag api share python code natur languag api us version classifi text categori work default categori figur adapt standard code us model version thank",
        "Question_gpt_summary_original":"The user is facing challenges in adapting the standard python code for the natural language API to use version 2 model for classifying text categories. They are able to use the default version 1 categories but are unable to figure out how to modify the code for version 2.",
        "Question_gpt_summary":"user face challeng adapt standard python code natur languag api us version model classifi text categori abl us default version categori unabl figur modifi code version",
        "Answer_original_content":"classifi content guid includ classif model optionswithin requestdictionari argument classifi text function option defin model version us content categori content categori version languag classificationmodelopt vmodel contentcategoriesvers assign model type respons client classifi text request document document classif model option model content categori version content categori version check classificationmodeloptionsrefer avail option view solut origin post",
        "Answer_preprocessed_content":"classifi content guid includ requestdictionari argument option defin model version us content categori assign model type respons check classificationmodeloptionsrefer avail option view solut origin post",
        "Answer_gpt_summary_original":"Solution: The user can modify the code for version 2 by including classification_model_options within the request dictionary argument to the classify_text() function. In these options, they can define the model and version to use for content categories. They can assign the v2 model type to content_categories_version and include it in the request dictionary. The user can also check the ClassificationModelOptions reference for available options.",
        "Answer_gpt_summary":"solut user modifi code version includ classif model option request dictionari argument classifi text function option defin model version us content categori assign model type content categori version includ request dictionari user check classificationmodelopt refer avail option"
    },
    {
        "Question_title":"AWS Sagemaker - Install External Library and Make it Persist",
        "Question_body":"<p>I have a sagemaker instance up and running and I have a few libraries that I frequently use with it but each time I restart the instance they get wiped and I have to reinstall them. Is it possible to install my libraries to one of the anaconda environments and have the change remain?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1530380054783,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":13,
        "Question_view_count":10578.0,
        "Poster_id":7754184.0,
        "Poster_created_time":1490229156263,
        "Poster_last_access_time":1616449120832,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":401.0,
        "Poster_views":13.0,
        "Answer_body":"<p>The supported way to do this for Sagemaker notebook instances is with <strong>Lifecycle Configurations<\/strong>.<\/p>\n\n<p>You can create an <strong>onStart<\/strong> lifecycle hook that can install the required packages into the respective Conda environments each time your notebook instance starts.<\/p>\n\n<p>Please see the following blog post for more details<\/p>\n\n<p><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":12.0,
        "Answerer_id":8899112.0,
        "Answerer_created_time":1510046220943,
        "Answerer_last_access_time":1613678255310,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":292.0,
        "Answerer_views":43.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51117133",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1530555566567,
        "Question_original_content":"instal extern librari persist instanc run librari frequent us time restart instanc wipe reinstal possibl instal librari anaconda environ chang remain",
        "Question_preprocessed_content":"instal extern librari persist instanc run librari frequent us time restart instanc wipe reinstal possibl instal librari anaconda environ chang remain",
        "Question_gpt_summary_original":"The user is facing a challenge with AWS Sagemaker where the libraries they frequently use get wiped out each time they restart the instance, requiring them to reinstall the libraries. They are seeking a solution to install the libraries to an anaconda environment and make the change persist.",
        "Question_gpt_summary":"user face challeng librari frequent us wipe time restart instanc requir reinstal librari seek solut instal librari anaconda environ chang persist",
        "Answer_original_content":"support wai notebook instanc lifecycl configur creat onstart lifecycl hook instal requir packag respect conda environ time notebook instanc start follow blog post detail http aw amazon com blog machin learn custom amazon notebook instanc lifecycl configur option disabl internet access",
        "Answer_preprocessed_content":"support wai notebook instanc lifecycl configur creat onstart lifecycl hook instal requir packag respect conda environ time notebook instanc start follow blog post detail",
        "Answer_gpt_summary_original":"Solution: The user can use Lifecycle Configurations to create an onStart lifecycle hook that installs the required packages into the respective Conda environments each time the notebook instance starts. The following blog post provides more details: https:\/\/aws.amazon.com\/blogs\/machine-learning\/customize-your-amazon-sagemaker-notebook-instances-with-lifecycle-configurations-and-the-option-to-disable-internet-access\/",
        "Answer_gpt_summary":"solut user us lifecycl configur creat onstart lifecycl hook instal requir packag respect conda environ time notebook instanc start follow blog post provid detail http aw amazon com blog machin learn custom amazon notebook instanc lifecycl configur option disabl internet access"
    },
    {
        "Question_title":"Start Sagemaker Notebook Instances - Question",
        "Question_body":"<p>Quick question regarding the start\/boot up on a existing sagemaker notebook instance. I previously created a notebook instance, and shut it down when I stop using it. I was giving access to this shared account by a co-worker who manages our AWS infrastructure, and this account is shared to multiple members from other departments, but then he gave me a separate sandbox account, where I'm the only one using.<\/p>\n<p>After one month, I went back to that same account (shared one) and found out my notebook instance, along side others, were running. Not sure how, and why. I'm pretty sure I turned them off (at least mine), and no other person uses them, or uses Sagemaker.<\/p>\n<p>Is there any possible way that these notebook instances started by chance, or other possible explanation to it?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1653053318850,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1653053849907,
        "Question_score":0,
        "Question_view_count":34.0,
        "Poster_id":12282834.0,
        "Poster_created_time":1572208191120,
        "Poster_last_access_time":1663847604676,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":139.0,
        "Poster_views":45.0,
        "Answer_body":"<p>A quick way to diagnose this would be using <a href=\"https:\/\/aws.amazon.com\/cloudtrail\/\" rel=\"nofollow noreferrer\">CloudTrail<\/a>. Go to the CloudTrail console in the shared account, choose &quot;Event History&quot; section on the left. You can then filter by &quot;Event name&quot;: &quot;StartNotebookInstance&quot; and you'll be able to see which identity made the call.<\/p>\n<p>Afaik, no AWS service would automatically start a notebook instance.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":2458691.0,
        "Answerer_created_time":1370505440848,
        "Answerer_last_access_time":1663973511163,
        "Answerer_age":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation":333.0,
        "Answerer_views":32.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72319750",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1653063317112,
        "Question_original_content":"start notebook instanc question quick question start boot exist notebook instanc previous creat notebook instanc shut stop give access share account worker manag aw infrastructur account share multipl member depart gave separ sandbox account month went account share notebook instanc run sure pretti sure turn person us us possibl wai notebook instanc start chanc possibl explan thank",
        "Question_preprocessed_content":"start notebook instanc question quick question exist notebook instanc previous creat notebook instanc shut stop give access share account manag aw infrastructur account share multipl member depart gave separ sandbox account month went account notebook instanc run sure pretti sure turn person us us possibl wai notebook instanc start chanc possibl explan thank",
        "Question_gpt_summary_original":"The user is facing a challenge with their Sagemaker notebook instance as they found it running after a month despite shutting it down. The instance is shared with multiple members from other departments, but the user was given a separate sandbox account. The user is unsure how and why the instance started running again and is seeking possible explanations.",
        "Question_gpt_summary":"user face challeng notebook instanc run month despit shut instanc share multipl member depart user given separ sandbox account user unsur instanc start run seek possibl explan",
        "Answer_original_content":"quick wai diagnos cloudtrail cloudtrail consol share account choos event histori section left filter event startnotebookinst abl ident afaik aw servic automat start notebook instanc",
        "Answer_preprocessed_content":"quick wai diagnos cloudtrail cloudtrail consol share account choos event histori section left filter event startnotebookinst abl ident afaik aw servic automat start notebook instanc",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use CloudTrail to diagnose the issue. By filtering the \"Event name\" to \"StartNotebookInstance\" in the CloudTrail console, the user can identify which identity made the call to start the instance. It was also mentioned that no AWS service would automatically start a notebook instance.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us cloudtrail diagnos issu filter event startnotebookinst cloudtrail consol user identifi ident start instanc mention aw servic automat start notebook instanc"
    },
    {
        "Question_title":"[Bug] azureml-train Python package deprecated but did receive an update which is not in line with azureml-train-core",
        "Question_body":"The Python package azureml-train is deprecated and seems basically a wrapper around azureml-train-core. When installing azureml-train, if I'm correct it tries to install the azureml-train-core package with the same version number. However, on the 24th of August 2021 the azureml-train package was updated to version 1.33.1 whereas azureml-train-core wasn't updated. This causes the installation of azureml-train to fail.\n\nI would suggest to remove version 1.33.1 of azureml-train such that it still can be installed.\nOtherwise, I'm curious why this situation is the case.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1629973035930,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@SjoerdGn-2530 Yes, this is a bug in the release cycle that was pushed to pypi, This also caused other packages to get updated.\n\nhttps:\/\/pypi.org\/project\/azureml-train-automl-runtime\/1.33.1.post1\/\nhttps:\/\/pypi.org\/project\/azureml-train-automl\/1.33.1\/\n\nazureml-sdk 1.33.0.post1 is released now to ensure the correct versions are installed with the SDK. As you mentioned above azureml-train 1.33.1 is not required and can be removed.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/528959\/bug-azureml-train-python-package-deprecated-but-di.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1629981340732,
        "Question_original_content":"bug train python packag deprec receiv updat line train core python packag train deprec basic wrapper train core instal train correct tri instal train core packag version number august train packag updat version train core wasn updat caus instal train fail suggest remov version train instal curiou situat case",
        "Question_preprocessed_content":"train python packag deprec receiv updat line python packag train deprec basic wrapper instal train correct tri instal packag version number august train packag updat version wasn updat caus instal train fail suggest remov version train instal curiou situat case",
        "Question_gpt_summary_original":"The user is facing challenges with the deprecated Python package azureml-train, which is a wrapper around azureml-train-core. The package tries to install azureml-train-core with the same version number, but the recent update of azureml-train to version 1.33.1 is not in line with azureml-train-core, causing installation failure. The user suggests removing version 1.33.1 of azureml-train or seeking clarification on the situation.",
        "Question_gpt_summary":"user face challeng deprec python packag train wrapper train core packag tri instal train core version number recent updat train version line train core caus instal failur user suggest remov version train seek clarif situat",
        "Answer_original_content":"sjoerdgn ye bug releas cycl push pypi caus packag updat http pypi org project train automl runtim post http pypi org project train automl sdk post releas ensur correct version instal sdk mention train requir remov",
        "Answer_preprocessed_content":"ye bug releas cycl push pypi caus packag updat sdk releas ensur correct version instal sdk mention train requir remov",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the issue is a bug in the release cycle that was pushed to pypi, causing installation failure. The solution provided is to remove version 1.33.1 of azureml-train or install azureml-sdk 1.33.0.post1, which ensures the correct versions are installed with the SDK.",
        "Answer_gpt_summary":"solut discuss suggest issu bug releas cycl push pypi caus instal failur solut provid remov version train instal sdk post ensur correct version instal sdk"
    },
    {
        "Question_title":"How to draw many images with a bar",
        "Question_body":"<p>I can only draw image with [wandb.Image(Numpy.array()),] like this<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b48aa9bfc94603e638f56ff8452ed88b900f00db.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\" target=\"_blank\" rel=\"noopener nofollow ugc\" title=\"11:54AM - 20 November 2020\">AIcrowd Forum \u2013 20 Nov 20<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:600\/325;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/e05a631c976b165047261523c356b3fa7e5eab41.gif\" class=\"thumbnail animated\" width=\"600\" height=\"325\"><\/div>\n\n<h3><a href=\"https:\/\/discourse.aicrowd.com\/t\/maskrcnn-integrated-with-wandb-and-direct-submit-from-colab\/3961\" target=\"_blank\" rel=\"noopener nofollow ugc\">MaskRCNN integrated with WandB and DIRECT SUBMIT FROM COLAB!<\/a><\/h3>\n\n  <p>Hi everyone!    @rohitmidha23 and me have been following this challenge for quite a while. We have written a starter notebook using MaskRCNN. We further integrate MaskRCNN with WandB which really helps to keep track of the various experiments that...<\/p>\n\n  <p>\n    <span class=\"label1\">Reading time: 1 mins \ud83d\udd51<\/span>\n      <span class=\"label2\">Likes: 17 \u2764<\/span>\n  <\/p>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<p>\nBut how can I draw many images with a bar like this<br>\n<a href=\"https:\/\/sooftware.io\/static\/fd6ffa741fe53de299a57e6a8852f68d\/f312c\/wandb_image.webp\" class=\"onebox\" target=\"_blank\" rel=\"noopener nofollow ugc\">https:\/\/sooftware.io\/static\/fd6ffa741fe53de299a57e6a8852f68d\/f312c\/wandb_image.webp<\/a><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1652175740664,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":153.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"<p>Oh\uff0cyes! I got it ,the step slider.<br>\nIt\u2019s on the left top of my panel. Thanks!<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7477d027660f227b355c1b7090095a0ca0e72264.png\" alt=\"FireShot Capture 043 - warm-sea-50 - deepfillv2_512x512_dv5_0pv8_1 \u2013 Weights &amp; Biases_ - 192.168.23.40\" data-base62-sha1=\"gCk5gzKgcCCqUAxrDVgTMn9CtF2\" width=\"274\" height=\"249\"><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-draw-many-images-with-a-bar\/2391",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1652432634120,
        "Question_original_content":"draw imag bar draw imag imag numpi arrai like aicrowd forum nov maskrcnn integr direct submit colab rohitmidha follow challeng written starter notebook maskrcnn integr maskrcnn help track experi read time min like draw imag bar like http sooftwar static fdffafedeaeafd imag webp",
        "Question_preprocessed_content":"draw imag bar draw imag like aicrowd forum nov maskrcnn integr direct submit colab follow challeng written starter notebook maskrcnn integr maskrcnn help track experi read time min like draw imag bar like",
        "Question_gpt_summary_original":"The user is facing a challenge of drawing multiple images with a bar using wandb.Image(Numpy.array()), but is unsure of how to achieve this. They have provided an example of the desired output and are seeking guidance on how to accomplish this task.",
        "Question_gpt_summary":"user face challeng draw multipl imag bar imag numpi arrai unsur achiev provid exampl desir output seek guidanc accomplish task",
        "Answer_original_content":"ohy got step slider left panel thank",
        "Answer_preprocessed_content":"ohy got step slider left panel thank",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Which IAM roles and policies should I delete to not being charged by AWS?",
        "Question_body":"<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsLaunchRole<\/li>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>AWSServiceRoleForAmazonSageMakerNotebooks<\/li>\n<\/ol>\n<p>Are these roles to be deleted?<\/p>\n<ol>\n<li>AmazonSageMakerServiceCatalogProductsUseRole<\/li>\n<li>Plus some execution policies<\/li>\n<\/ol>\n<p>Is Jupyter server within sagemaker studio also be stopped for not being charged?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1633708664963,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1634116759467,
        "Question_score":0,
        "Question_view_count":105.0,
        "Poster_id":16187297.0,
        "Poster_created_time":1623330365063,
        "Poster_last_access_time":1644405074280,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":75.0,
        "Poster_views":6.0,
        "Answer_body":"<p><strong>AWS IAM is a free service<\/strong> - you do not get charged for roles, policies or any other aspect of IAM.<\/p>\n<p>From <a href=\"https:\/\/aws.amazon.com\/iam\/#:%7E:text=IAM%20is%20a%20feature%20of,AWS%20services%20by%20your%20users.\" rel=\"nofollow noreferrer\">the documentation<\/a>:<\/p>\n<blockquote>\n<p>IAM is a feature of your AWS account offered at no additional charge. You will be charged only for use of other AWS services by your users.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4800344.0,
        "Answerer_created_time":1429262032907,
        "Answerer_last_access_time":1664057525590,
        "Answerer_age":null,
        "Answerer_location":"Manchester, United Kingdom",
        "Answerer_reputation":11490.0,
        "Answerer_views":2150.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69498670",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1633709785048,
        "Question_original_content":"iam role polici delet charg aw role delet amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol awsserviceroleforamazonnotebook role delet amazonservicecatalogproductsuserol plu execut polici jupyt server studio stop charg",
        "Question_preprocessed_content":"iam role polici delet charg aw role delet amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol awsserviceroleforamazonnotebook role delet amazonservicecatalogproductsuserol plu execut polici jupyt server studio stop charg",
        "Question_gpt_summary_original":"The user is seeking guidance on which IAM roles and policies to delete in order to avoid being charged by AWS. They are specifically asking if certain roles, such as AmazonSageMakerServiceCatalogProductsLaunchRole and AmazonSageMakerServiceCatalogProductsUseRole, should be deleted, as well as some execution policies. The user also inquires if stopping the Jupyter server within SageMaker Studio will prevent charges.",
        "Question_gpt_summary":"user seek guidanc iam role polici delet order avoid charg aw specif ask certain role amazonservicecatalogproductslaunchrol amazonservicecatalogproductsuserol delet execut polici user inquir stop jupyt server studio prevent charg",
        "Answer_original_content":"aw iam free servic charg role polici aspect iam document iam featur aw account offer addit charg charg us aw servic user",
        "Answer_preprocessed_content":"aw iam free servic charg role polici aspect iam document iam featur aw account offer addit charg charg us aw servic user",
        "Answer_gpt_summary_original":"Solution: There are no solutions provided as the user's concern about being charged for IAM roles and policies is unfounded. IAM is a free service and users will only be charged for the use of other AWS services.",
        "Answer_gpt_summary":"solut solut provid user concern charg iam role polici unfound iam free servic user charg us aw servic"
    },
    {
        "Question_title":"Amazon Sagemaker JupiterLab Notebook - No matching distribution found for Pandas",
        "Question_body":"<p>If I attempt to Upgrade <strong>Pandas<\/strong> above version <strong>1.1.5<\/strong> on my <strong>AWS Sagemaker<\/strong> provided <strong>JupyterLab<\/strong> notebook I receive the error <strong>No Matching Distribution Found<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import sys\n!{sys.executable} -m pip install --pre --upgrade pandas==1.3.5\n<\/code><\/pre>\n<pre class=\"lang-bash prettyprint-override\"><code>ERROR: Could not find a version that satisfies the requirement pandas==1.3.5 (from versions: 0.1, 0.2, 0.3.0, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.5.0, 0.6.0, 0.6.1, 0.7.0, 0.7.1, 0.7.2, 0.7.3, 0.8.0, 0.8.1, 0.9.0, 0.9.1, 0.10.0, 0.10.1, 0.11.0, 0.12.0, 0.13.0, 0.13.1, 0.14.0, 0.14.1, 0.15.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.18.0, 0.18.1, 0.19.0, 0.19.1, 0.19.2, 0.20.0, 0.20.1, 0.20.2, 0.20.3, 0.21.0, 0.21.1, 0.22.0, 0.23.0, 0.23.1, 0.23.2, 0.23.3, 0.23.4, 0.24.0, 0.24.1, 0.24.2, 0.25.0, 0.25.1, 0.25.2, 0.25.3, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5)\nERROR: No matching distribution found for pandas==1.3.5\n<\/code><\/pre>\n<h2>Background<\/h2>\n<p>I created a Notebook instance from the AWS Console via <strong>AWS Sagemaker -&gt; Notebook instances -&gt; Create Notebook instance<\/strong>.<\/p>\n<p>I then selected the Kernel <strong>conda_Python3<\/strong>.<\/p>\n<p>I use <strong>sys.executable<\/strong> to show the Kernel's Python, Pip and Pandas version.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>!{sys.executable} -version\nPython 3.6.13\n\n!{sys.executable} -m pip show pip\nName: pip\nVersion: 21.3.1\nSummary: The PyPA recommended tool for installing Python packages.\nHome-page: https:\/\/pip.pypa.io\/\nAuthor: The pip developers\nAuthor-email: distutils-sig@python.org\nLicense: MIT\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: \nRequired-by: \n\n!{sys.executable} -m pip show pandas\nName: pandas\nVersion: 1.1.5\nSummary: Powerful data structures for data analysis, time series, and statistics\nHome-page: https:\/\/pandas.pydata.org\nAuthor: \nAuthor-email: \nLicense: BSD\nLocation: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\nRequires: numpy, python-dateutil, pytz\nRequired-by: autovizwidget, awswrangler, hdijupyterutils, odo, sagemaker, seaborn, shap, smclarify, sparkmagic, statsmodels\n<\/code><\/pre>\n<p>I cannot upgrade <strong>Pandas<\/strong>.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>!{sys.executable} -m pip install --pre --upgrade pandas\nRequirement already satisfied: pandas in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (1.1.5)\nRequirement already satisfied: python-dateutil&gt;=2.7.3 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (2.8.1)\nRequirement already satisfied: pytz&gt;=2017.2 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (2021.1)\nRequirement already satisfied: numpy&gt;=1.15.4 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from pandas) (1.18.5)\nRequirement already satisfied: six&gt;=1.5 in \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages (from python-dateutil&gt;=2.7.3-&gt;pandas) (1.15.0)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1654183154907,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":184.0,
        "Poster_id":8570881.0,
        "Poster_created_time":1504724308867,
        "Poster_last_access_time":1664029219323,
        "Poster_age":null,
        "Poster_location":"Washington D.C., DC, United States",
        "Poster_reputation":97.0,
        "Poster_views":29.0,
        "Answer_body":"<p>see this - <a href=\"https:\/\/stackoverflow.com\/questions\/68750375\/no-matching-distribution-found-for-pandas-1-3-1\">No matching distribution found for pandas==1.3.1<\/a><\/p>\n<p>The latest version to support python 3.6 is 1.1.5.<\/p>\n<p>You can create a new conda environment with python version &gt;= 3.7 in your existing notebook, or move to notebooks with Amazon Linux 2 (see <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/amazon-sagemaker-notebook-instance-now-supports-amazon-linux-2\/\" rel=\"nofollow noreferrer\">blog post<\/a>). In the AL2 notebooks, <code>conda_python3<\/code> kernels come with Python 3.8.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":2458691.0,
        "Answerer_created_time":1370505440848,
        "Answerer_last_access_time":1663973511163,
        "Answerer_age":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation":333.0,
        "Answerer_views":32.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72478572",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1654275686200,
        "Question_original_content":"jupiterlab notebook match distribut panda attempt upgrad panda version provid jupyterlab notebook receiv error match distribut import sy sy execut pip instal pre upgrad panda error version satisfi requir panda version error match distribut panda background creat notebook instanc aw consol notebook instanc creat notebook instanc select kernel conda python us sy execut kernel python pip panda version sy execut version python sy execut pip pip pip version summari pypa recommend tool instal python packag home page http pip pypa author pip develop author email distutil sig python org licens mit locat home user anaconda env python lib python site packag requir requir sy execut pip panda panda version summari power data structur data analysi time seri statist home page http panda pydata org author author email licens bsd locat home user anaconda env python lib python site packag requir numpi python dateutil pytz requir autovizwidget awswrangl hdijupyterutil odo seaborn shap smclarifi sparkmag statsmodel upgrad panda sy execut pip instal pre upgrad panda requir satisfi panda home user anaconda env python lib python site packag requir satisfi python dateutil home user anaconda env python lib python site packag panda requir satisfi pytz home user anaconda env python lib python site packag panda requir satisfi numpi home user anaconda env python lib python site packag panda requir satisfi home user anaconda env python lib python site packag python dateutil panda",
        "Question_preprocessed_content":"jupiterlab notebook match distribut panda attempt upgrad panda version provid jupyterlab notebook receiv error match distribut background creat notebook instanc aw consol notebook instanc creat notebook instanc select kernel us kernel python pip panda version upgrad panda",
        "Question_gpt_summary_original":"The user is encountering an error when attempting to upgrade Pandas above version 1.1.5 on their AWS Sagemaker provided JupyterLab notebook. The error message states that there is no matching distribution found for Pandas. The user has created a Notebook instance from the AWS Console via AWS Sagemaker and selected the Kernel conda_Python3. The user is unable to upgrade Pandas and has attempted to upgrade it using pip install, but it did not work.",
        "Question_gpt_summary":"user encount error attempt upgrad panda version provid jupyterlab notebook error messag state match distribut panda user creat notebook instanc aw consol select kernel conda python user unabl upgrad panda attempt upgrad pip instal work",
        "Answer_original_content":"match distribut panda latest version support python creat new conda environ python version exist notebook notebook amazon linux blog post notebook conda python kernel come python",
        "Answer_preprocessed_content":"match distribut latest version support python creat new conda environ python version exist notebook notebook amazon linux notebook kernel come python",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- The latest version of Pandas that supports Python 3.6 is 1.1.5, so the user can try installing that version instead of a higher version.\n- The user can create a new conda environment with Python version >= 3.7 in their existing notebook.\n- The user can move to notebooks with Amazon Linux 2, which come with Python 3.8 in the conda_python3 kernels. \n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss latest version panda support python user try instal version instead higher version user creat new conda environ python version exist notebook user notebook amazon linux come python conda python kernel person opinion bias includ summari"
    },
    {
        "Question_title":"SageMaker TensorFlow serving stack comparisons",
        "Question_body":"<p>SageMaker seems to give examples of using two different serving stacks for serving custom docker images:<\/p>\n\n<ol>\n<li>NGINX + Gunicorn + Flask<\/li>\n<li>NGINX + TensorFlow Serving<\/li>\n<\/ol>\n\n<p>Could someone explain to me at a very high level (I have very little knowledge of network engineering) what responsibilities these different components have? And since the second stack has only two components instead of one, can I rightly assume that TensorFlow Serving does the job (whatever that may be) of both Gunicorn and Flask? <\/p>\n\n<p>Lastly, I've read that it's possible to use Flask and TensorFlow serving at the same time. Would this then be NGINX -> Gunicorn -> Flask -> TensorFlow Serving? And what are there advantages of this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1547670814743,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1547802223467,
        "Question_score":2,
        "Question_view_count":741.0,
        "Poster_id":2089899.0,
        "Poster_created_time":1361339272692,
        "Poster_last_access_time":1663965928400,
        "Poster_age":null,
        "Poster_location":"NYC",
        "Poster_reputation":6281.0,
        "Poster_views":958.0,
        "Answer_body":"<p>I'll try to answer your question on a high level. Disclaimer: I'm not at an expert across the full stack of what you describe, and I would welcome corrections or additions from people who are. <\/p>\n\n<p>I'll go over the different components from bottom to top:<\/p>\n\n<p><strong>TensorFlow Serving<\/strong> is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions. The idea is to train models with TensorFlow, export them to the SavedModel format and serve them with TF Serving. You can set up a TF Server to accept requests via HTTP and\/or RPC. One advantage of RPC is that the request message is compressed, which can be useful when sending large payloads, for instance with image data.<\/p>\n\n<p><strong>Flask<\/strong> is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures. <\/p>\n\n<p>Now, the combination of Flask and TensorFlow serving should make sense. You could write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood. The user uses the API to transmit some data (<strong>1<\/strong>), the Flask app perhaps transform the data (for example, wrap it in numpy arrays), calls the TF Server to get a model prediction (<strong>2<\/strong>)(<strong>3<\/strong>), perhaps transforms the prediction (for example convert a predicted probability that is larger than 0.5 to a class label of 1), and returns the prediction to the user (<strong>4<\/strong>). You could visualize this as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/67EXW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/67EXW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Gunicorn<\/strong> is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems. As the name says, it's the interface between a web server and a web application. When you are developing a Flask app, you can run it locally to test it. In production, gunicorn will run the app for you.<\/p>\n\n<p>TF Serving will host your model as a functional application. Therefore, you do not need gunicorn to run the TF Server application for you. <\/p>\n\n<p><strong>Nginx<\/strong> is the actual web server, which will host your application, handle requests from the outside and pass them to the application server (gunicorn). Nginx cannot talk directly to Flask applications, which is why gunicorn is there. <\/p>\n\n<p><a href=\"https:\/\/serverfault.com\/questions\/331256\/why-do-i-need-nginx-and-something-like-gunicorn\">This answer<\/a> might be helpful as well. <\/p>\n\n<p>Finally, if you are working on a cloud platform, the web server part will probably be handled for you, so you will either need to write the Flask app and host it with gunicorn, or setup the TF Serving server. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1547816279183,
        "Answer_score":2.0,
        "Answerer_id":5495381.0,
        "Answerer_created_time":1445975382243,
        "Answerer_last_access_time":1663945410183,
        "Answerer_age":null,
        "Answerer_location":"Belgium",
        "Answerer_reputation":6831.0,
        "Answerer_views":653.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54224934",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1547801639500,
        "Question_original_content":"tensorflow serv stack comparison exampl differ serv stack serv custom docker imag nginx gunicorn flask nginx tensorflow serv explain high level littl knowledg network engin respons differ compon second stack compon instead rightli assum tensorflow serv job gunicorn flask lastli read possibl us flask tensorflow serv time nginx gunicorn flask tensorflow serv advantag",
        "Question_preprocessed_content":"tensorflow serv stack comparison exampl differ serv stack serv custom docker imag nginx gunicorn flask nginx tensorflow serv explain high level respons differ compon second stack compon instead rightli assum tensorflow serv job gunicorn flask lastli read possibl us flask tensorflow serv time nginx gunicorn flask tensorflow serv advantag",
        "Question_gpt_summary_original":"The user is seeking an explanation of the differences between two serving stacks for custom docker images in SageMaker: NGINX + Gunicorn + Flask and NGINX + TensorFlow Serving. They are also asking if TensorFlow Serving does the job of both Gunicorn and Flask, and if it's possible to use Flask and TensorFlow serving at the same time. The user is seeking to understand the advantages of using both components together.",
        "Question_gpt_summary":"user seek explan differ serv stack custom docker imag nginx gunicorn flask nginx tensorflow serv ask tensorflow serv job gunicorn flask possibl us flask tensorflow serv time user seek understand advantag compon",
        "Answer_original_content":"try answer question high level disclaim expert stack welcom correct addit peopl differ compon tensorflow serv librari deploi host tensorflow model model server accept request input data return model predict idea train model tensorflow export savedmodel format serv serv set server accept request http rpc advantag rpc request messag compress us send larg payload instanc imag data flask python framework write web applic gener purpos serv wide build web servic instanc microservic architectur combin flask tensorflow serv sens write flask web applic expos api user call model host serv hood user us api transmit data flask app transform data exampl wrap numpi arrai call server model predict transform predict exampl convert predict probabl larger class label return predict user visual follow gunicorn web server gatewai interfac wsgi commonli host flask applic product system sai interfac web server web applic develop flask app run local test product gunicorn run app serv host model function applic need gunicorn run server applic nginx actual web server host applic handl request outsid pass applic server gunicorn nginx talk directli flask applic gunicorn answer help final work cloud platform web server probabl handl need write flask app host gunicorn setup serv server",
        "Answer_preprocessed_content":"try answer question high level disclaim expert stack welcom correct addit peopl differ compon tensorflow serv librari deploi host tensorflow model model server accept request input data return model predict idea train model tensorflow export savedmodel format serv serv set server accept request http rpc advantag rpc request messag compress us send larg payload instanc imag data flask python framework write web applic serv wide build web servic instanc microservic architectur combin flask tensorflow serv sens write flask web applic expos api user call model host serv hood user us api transmit data flask app transform data call server model predict transform predict return predict user visual follow gunicorn web server gatewai interfac commonli host flask applic product system sai interfac web server web applic develop flask app run local test product gunicorn run app serv host model function applic need gunicorn run server applic nginx actual web server host applic handl request outsid pass applic server nginx talk directli flask applic gunicorn answer help final work cloud platform web server probabl handl need write flask app host gunicorn setup serv server",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- TensorFlow Serving is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions.\n- Flask is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures.\n- Flask and TensorFlow serving can be used together to write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood.\n- Gunicorn is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems.\n- Nginx is the actual web server, which will host your application, handle requests from the",
        "Answer_gpt_summary":"possibl solut mention discuss tensorflow serv librari deploi host tensorflow model model server accept request input data return model predict flask python framework write web applic gener purpos serv wide build web servic instanc microservic architectur flask tensorflow serv write flask web applic expos api user call model host serv hood gunicorn web server gatewai interfac wsgi commonli host flask applic product system nginx actual web server host applic handl request"
    },
    {
        "Question_title":"Azure ML Inference Schema - \"List index out of range\" error",
        "Question_body":"<p>I have an ML model deployed on Azure ML Studio and I was updating it with an inference schema to allow compatibility with Power BI as described <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n<p>When sending data up to the model via REST api (before adding this inference schema), everything works fine and I get results returned. However, once adding the schema as described in the instructions linked above and personalising to my data, the same data sent via REST api only returns the error &quot;list index out of range&quot;. The deployment goes ahead fine and is designated as &quot;healthy&quot; with no error messages.<\/p>\n<p>Any help would be greatly appreciated. Thanks.<\/p>\n<p>EDIT:<\/p>\n<p>Entry script:<\/p>\n<pre><code> import numpy as np\n import pandas as pd\n import joblib\n from azureml.core.model import Model\n    \n from inference_schema.schema_decorators import input_schema, output_schema\n from inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n from inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n from inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n    \n def init():\n     global model\n     #Model name is the name of the model registered under the workspace\n     model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n     model = joblib.load(model_path)\n    \n #Provide 3 sample inputs for schema generation for 2 rows of data\n numpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n pandas_sample_input = PandasParameterType(pd.DataFrame({'1': [2400.0, 368.55], '2': [78.26086956521739, 96.88311688311687], '3': [11100.0, 709681.1600000012], '4': [3.612565445026178, 73.88059701492537], '5': [3.0, 44.0], '6': [0.0, 0.0]}))\n standard_sample_input = StandardPythonParameterType(0.0)\n    \n # This is a nested input sample, any item wrapped by `ParameterType` will be described by schema\n sample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                             'input2': pandas_sample_input, \n                                             'input3': standard_sample_input})\n    \n sample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n sample_output = StandardPythonParameterType([1.0, 1.0])\n    \n @input_schema('inputs', sample_input)\n @input_schema('global_parameters', sample_global_parameters) #this is optional\n @output_schema(sample_output)\n    \n def run(inputs, global_parameters):\n     try:\n         data = inputs['input1']\n         # data will be convert to target format\n         assert isinstance(data, np.ndarray)\n         result = model.predict(data)\n         return result.tolist()\n     except Exception as e:\n         error = str(e)\n         return error\n<\/code><\/pre>\n<p>Prediction script:<\/p>\n<pre><code> import requests\n import json\n from ast import literal_eval\n    \n # URL for the web service\n scoring_uri = ''\n ## If the service is authenticated, set the key or token\n #key = '&lt;your key or token&gt;'\n    \n # Two sets of data to score, so we get two results back\n data = {&quot;data&quot;: [[2400.0, 78.26086956521739, 11100.0, 3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, 73.88059701492537, 44.0, 0.0]]}\n # Convert to JSON string\n input_data = json.dumps(data)\n    \n # Set the content type\n headers = {'Content-Type': 'application\/json'}\n ## If authentication is enabled, set the authorization header\n #headers['Authorization'] = f'Bearer {key}'\n    \n # Make the request and display the response\n resp = requests.post(scoring_uri, input_data, headers=headers)\n print(resp.text)\n    \n result = literal_eval(resp.text)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1602495267847,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1615561798207,
        "Question_score":1,
        "Question_view_count":785.0,
        "Poster_id":14287756.0,
        "Poster_created_time":1600260166047,
        "Poster_last_access_time":1615561616230,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":15.0,
        "Poster_views":4.0,
        "Answer_body":"<p>The Microsoft <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#power-bi-compatible-endpoint\" rel=\"nofollow noreferrer\">documentation<\/a> say's: &quot;In order to generate conforming swagger for automated web service consumption, scoring script run() function must have API shape of:<\/p>\n<blockquote>\n<p>A first parameter of type &quot;StandardPythonParameterType&quot;, named\n<strong>Inputs<\/strong> and nested.<\/p>\n<p>An optional second parameter of type &quot;StandardPythonParameterType&quot;,\nnamed GlobalParameters.<\/p>\n<p>Return a dictionary of type &quot;StandardPythonParameterType&quot; named\n<strong>Results<\/strong> and nested.&quot;<\/p>\n<\/blockquote>\n<p>I've already test this and it is case sensitive\nSo it will be like this:<\/p>\n<pre><code>import numpy as np\nimport pandas as pd\nimport joblib\n\nfrom azureml.core.model import Model\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.standard_py_parameter_type import \n    StandardPythonParameterType\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\n\ndef init():\n    global model\n    # Model name is the name of the model registered under the workspace\n    model_path = Model.get_model_path(model_name = 'databricksmodelpowerbi2')\n    model = joblib.load(model_path)\n\n# Provide 3 sample inputs for schema generation for 2 rows of data\nnumpy_sample_input = NumpyParameterType(np.array([[2400.0, 78.26086956521739, 11100.0, \n3.612565445026178, 3.0, 0.0], [368.55, 96.88311688311687, 709681.1600000012, \n73.88059701492537, 44.0, 0.0]], dtype = 'float64'))\n\npandas_sample_input = PandasParameterType(pd.DataFrame({'value': [2400.0, 368.55], \n'delayed_percent': [78.26086956521739, 96.88311688311687], 'total_value_delayed': \n[11100.0, 709681.1600000012], 'num_invoices_per30_dealing_days': [3.612565445026178, \n73.88059701492537], 'delayed_streak': [3.0, 44.0], 'prompt_streak': [0.0, 0.0]}))\n\nstandard_sample_input = StandardPythonParameterType(0.0)\n\n# This is a nested input sample, any item wrapped by `ParameterType` will be described \nby schema\nsample_input = StandardPythonParameterType({'input1': numpy_sample_input, \n                                         'input2': pandas_sample_input, \n                                         'input3': standard_sample_input})\n\nsample_global_parameters = StandardPythonParameterType(1.0) #this is optional\n\nnumpy_sample_output = NumpyParameterType(np.array([1.0, 2.0]))\n\n# 'Results' is case sensitive\nsample_output = StandardPythonParameterType({'Results': numpy_sample_output})\n\n# 'Inputs' is case sensitive\n@input_schema('Inputs', sample_input)\n@input_schema('global_parameters', sample_global_parameters) #this is optional\n@output_schema(sample_output)\ndef run(Inputs, global_parameters):\n    try:\n        data = inputs['input1']\n        # data will be convert to target format\n        assert isinstance(data, np.ndarray)\n        result = model.predict(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<p>`<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1614801695372,
        "Answer_score":1.0,
        "Answerer_id":15146404.0,
        "Answerer_created_time":1612454694036,
        "Answerer_last_access_time":1643843479060,
        "Answerer_age":null,
        "Answerer_location":"Mexico City, CDMX, Mexico",
        "Answerer_reputation":36.0,
        "Answerer_views":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64315239",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1614800344830,
        "Question_original_content":"infer schema list index rang error model deploi studio updat infer schema allow compat power describ send data model rest api ad infer schema work fine result return ad schema describ instruct link personalis data data sent rest api return error list index rang deploy goe ahead fine design healthi error messag help greatli appreci thank edit entri script import numpi import panda import joblib core model import model infer schema schema decor import input schema output schema infer schema paramet type standard paramet type import standardpythonparametertyp infer schema paramet type numpi paramet type import numpyparametertyp infer schema paramet type panda paramet type import pandasparametertyp def init global model model model regist workspac model path model model path model databricksmodelpowerbi model joblib load model path provid sampl input schema gener row data numpi sampl input numpyparametertyp arrai dtype float panda sampl input pandasparametertyp datafram standard sampl input standardpythonparametertyp nest input sampl item wrap parametertyp describ schema sampl input standardpythonparametertyp input numpi sampl input input panda sampl input input standard sampl input sampl global paramet standardpythonparametertyp option sampl output standardpythonparametertyp input schema input sampl input input schema global paramet sampl global paramet option output schema sampl output def run input global paramet try data input input data convert target format assert isinst data ndarrai result model predict data return result tolist except error str return error predict script import request import json ast import liter eval url web servic score uri servic authent set kei token kei set data score result data data convert json string input data json dump data set content type header content type applic json authent enabl set author header header author bearer kei request displai respons resp request post score uri input data header header print resp text result liter eval resp text",
        "Question_preprocessed_content":"infer schema list index rang error model deploi studio updat infer schema allow compat power describ send data model rest api work fine result return ad schema describ instruct link personalis data data sent rest api return error list index rang deploy goe ahead fine design healthi error messag help greatli appreci thank edit entri script predict script",
        "Question_gpt_summary_original":"The user is encountering an error \"list index out of range\" when sending data to an ML model deployed on Azure ML Studio via REST API after adding an inference schema to allow compatibility with Power BI. The deployment is designated as \"healthy\" with no error messages. The user has provided the entry and prediction scripts for reference.",
        "Question_gpt_summary":"user encount error list index rang send data model deploi studio rest api ad infer schema allow compat power deploy design healthi error messag user provid entri predict script refer",
        "Answer_original_content":"microsoft document order gener conform swagger autom web servic consumpt score script run function api shape paramet type standardpythonparametertyp name input nest option second paramet type standardpythonparametertyp name globalparamet return dictionari type standardpythonparametertyp name result nest test case sensit like import numpi import panda import joblib core model import model infer schema schema decor import input schema output schema infer schema paramet type standard paramet type import standardpythonparametertyp infer schema paramet type numpi paramet type import numpyparametertyp infer schema paramet type panda paramet type import pandasparametertyp def init global model model model regist workspac model path model model path model databricksmodelpowerbi model joblib load model path provid sampl input schema gener row data numpi sampl input numpyparametertyp arrai dtype float panda sampl input pandasparametertyp datafram valu delai percent total valu delai num invoic deal dai delai streak prompt streak standard sampl input standardpythonparametertyp nest input sampl item wrap parametertyp describ schema sampl input standardpythonparametertyp input numpi sampl input input panda sampl input input standard sampl input sampl global paramet standardpythonparametertyp option numpi sampl output numpyparametertyp arrai result case sensit sampl output standardpythonparametertyp result numpi sampl output input case sensit input schema input sampl input input schema global paramet sampl global paramet option output schema sampl output def run input global paramet try data input input data convert target format assert isinst data ndarrai result model predict data return result tolist except error str return error",
        "Answer_preprocessed_content":"microsoft document order gener conform swagger autom web servic consumpt score script run function api shape paramet type standardpythonparametertyp name input nest option second paramet type standardpythonparametertyp name globalparamet return dictionari type standardpythonparametertyp name result test case sensit like",
        "Answer_gpt_summary_original":"Solution:\nThe scoring script run() function must have API shape of a first parameter of type \"StandardPythonParameterType\", named \"Inputs\" and nested, an optional second parameter of type \"StandardPythonParameterType\", named GlobalParameters, and return a dictionary of type \"StandardPythonParameterType\" named \"Results\" and nested. The solution is to modify the scoring script run() function to conform to the API shape. The provided code snippet shows an example of how to modify the scoring script.",
        "Answer_gpt_summary":"solut score script run function api shape paramet type standardpythonparametertyp name input nest option second paramet type standardpythonparametertyp name globalparamet return dictionari type standardpythonparametertyp name result nest solut modifi score script run function conform api shape provid code snippet show exampl modifi score script"
    },
    {
        "Question_title":"How can I use R to get confidence intervals in Azure ML?",
        "Question_body":"<p>I came across <a href=\"https:\/\/stackoverflow.com\/questions\/38077884\/can-azure-calculate-confidence-interval-for-regressions\/38115484#comment66660633_38115484\">this<\/a> question which asks <em>if<\/em> Azure ML can calculate confidence - or probabilities - for row data prediction. However, given that the answer to that question is <code>No<\/code>, and suggests to use R, I am trying to figure out how to use R to do exactly this for a regression model.<\/p>\n\n<p>Does anyone have any suggestions for references on where to look for this?<\/p>\n\n<p>My scenario is that I have used Azure ML to build a boosted decision tree regression model, which outputs a <code>Scored Label<\/code> column. But I don't know regression analysis well enough to write R code to use the outputted model to get confidence intervals.<\/p>\n\n<p>I am looking for any references that can help me understand how to do this in R (in conjuncture with Azure ML).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1474847214167,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1495541962792,
        "Question_score":7,
        "Question_view_count":524.0,
        "Poster_id":304684.0,
        "Poster_created_time":1269906749848,
        "Poster_last_access_time":1663614060847,
        "Poster_age":null,
        "Poster_location":"New York",
        "Poster_reputation":11705.0,
        "Poster_views":794.0,
        "Answer_body":"<p>There isn't a straight forward way to compute the confidence interval from the results of the Boosted Decision Tree model in Azure ML. <\/p>\n\n<p>Here are some alternate suggestions:<\/p>\n\n<ol>\n<li><p>Rebuild the model using the library(gbm) <a href=\"http:\/\/artax.karlin.mff.cuni.cz\/r-help\/library\/gbm\/html\/gbm.html\">http:\/\/artax.karlin.mff.cuni.cz\/r-help\/library\/gbm\/html\/gbm.html<\/a> or the library(glm) <a href=\"https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/stats\/html\/glm.html\">https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/stats\/html\/glm.html<\/a> <\/p><\/li>\n<li><p>Then build the confidence interval using confint function: <a href=\"https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/stats\/html\/confint.html\">https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/stats\/html\/confint.html<\/a><\/p><\/li>\n<li><p>For a linear model, the confidence interval computation is simpler: <a href=\"http:\/\/www.r-tutor.com\/elementary-statistics\/simple-linear-regression\/confidence-interval-linear-regression\">http:\/\/www.r-tutor.com\/elementary-statistics\/simple-linear-regression\/confidence-interval-linear-regression<\/a> <\/p><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1475247878920,
        "Answer_score":5.0,
        "Answerer_id":4589073.0,
        "Answerer_created_time":1424453610300,
        "Answerer_last_access_time":1505365713168,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1237.0,
        "Answerer_views":116.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39692921",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1475247736103,
        "Question_original_content":"us confid interv came question ask calcul confid probabl row data predict given answer question suggest us try figur us exactli regress model suggest refer look scenario build boost decis tree regress model output score label column know regress analysi write code us output model confid interv look refer help understand conjunctur",
        "Question_preprocessed_content":"us confid interv came question ask calcul confid probabl row data predict given answer question suggest us try figur us exactli regress model suggest refer look scenario build boost decis tree regress model output column know regress analysi write code us output model confid interv look refer help understand",
        "Question_gpt_summary_original":"The user is facing a challenge in calculating confidence intervals for row data prediction in Azure ML. As Azure ML cannot perform this task, the user is seeking suggestions on how to use R to calculate confidence intervals for a regression model. The user has built a boosted decision tree regression model in Azure ML but lacks the knowledge to write R code to obtain confidence intervals. The user is looking for references to help understand how to use R in conjunction with Azure ML to solve this problem.",
        "Question_gpt_summary":"user face challeng calcul confid interv row data predict perform task user seek suggest us calcul confid interv regress model user built boost decis tree regress model lack knowledg write code obtain confid interv user look refer help understand us conjunct solv problem",
        "Answer_original_content":"isn straight forward wai comput confid interv result boost decis tree model altern suggest rebuild model librari gbm http artax karlin mff cuni help librari gbm html gbm html librari glm http stat ethz manual devel librari stat html glm html build confid interv confint function http stat ethz manual devel librari stat html confint html linear model confid interv comput simpler http tutor com elementari statist simpl linear regress confid interv linear regress",
        "Answer_preprocessed_content":"isn straight forward wai comput confid interv result boost decis tree model altern suggest rebuild model librari librari build confid interv confint function linear model confid interv comput simpler",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Rebuild the model using the library(gbm) or the library(glm).\n2. Build the confidence interval using the confint function.\n3. For a linear model, the confidence interval computation is simpler.\n\nNo personal opinions or biases are included in the response.",
        "Answer_gpt_summary":"possibl solut mention discuss rebuild model librari gbm librari glm build confid interv confint function linear model confid interv comput simpler person opinion bias includ respons"
    },
    {
        "Question_title":"Should I run forecast predictive model with AWS lambda or sagemaker?",
        "Question_body":"<p>I've been reading some articles regarding this topic and have preliminary thoughts as what I should do with it, but still want to see if anyone can share comments if you have more experience with running machine learning on AWS. I was doing a project for a professor at school, and we decided to use AWS. I need to find a cost-effective and efficient way to deploy a forecasting model on it. <\/p>\n\n<p>What we want to achieve is:<\/p>\n\n<ul>\n<li>read the data from S3 bucket monthly (there will be new data coming in every month), <\/li>\n<li>run a few python files (.py) for custom-built packages and install dependencies (including the files, no more than 30kb), <\/li>\n<li>produce predicted results into a file back in S3 (JSON or CSV works), or push to other endpoints (most likely to be some BI tools - tableau etc.) - but really this step can be flexible (not web for sure) <\/li>\n<\/ul>\n\n<p><strong>First thought I have is AWS sagemaker<\/strong>. However, we'll be using \"fb prophet\" model to predict the results, and we built a customized package to use in the model, therefore, I don't think the notebook instance is gonna help us. (Please correct me if I'm wrong) My understanding is that sagemaker is a environment to build and train the model, but we already built and trained the model. Plus, we won't be using AWS pre-built models anyways.<\/p>\n\n<p>Another thing is if we want to use custom-built package, we will need to create container image, and I've never done that before, not sure about the efforts to do that.<\/p>\n\n<p><strong>2nd option is to create multiple lambda functions<\/strong><\/p>\n\n<ul>\n<li><p>one that triggers to run the python scripts from S3 bucket (2-3 .py files) every time a new file is imported into S3 bucket, which will happen monthly.<\/p><\/li>\n<li><p>one that trigger after the python scripts are done running and produce results and save into S3 bucket.<\/p><\/li>\n<\/ul>\n\n<p>3rd option will combine both options:\n - Use lambda function to trigger the implementation on the python scripts in S3 bucket when the new file comes in.\n - Push the result using sagemaker endpoint, which means we host the model on sagemaker and deploy from there.<\/p>\n\n<p>I am still not entirely sure how to put pre-built model and python scripts onto sagemaker instance and host from there.<\/p>\n\n<p>I'm hoping whoever has more experience with AWS service can help give me some guidance, in terms of more cost-effective and efficient way to run model.<\/p>\n\n<p>Thank you!! <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1586306942937,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":2675.0,
        "Poster_id":12725603.0,
        "Poster_created_time":1579188091243,
        "Poster_last_access_time":1661402697580,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":23.0,
        "Poster_views":7.0,
        "Answer_body":"<p>I would say it all depends on how heavy your model is \/ how much data you're running through it. You're right to identify that Lambda will likely be less work. It's quite easy to get a lambda up and running to do the things that you need, and <a href=\"https:\/\/aws.amazon.com\/lambda\/pricing\/\" rel=\"nofollow noreferrer\">Lambda has a very generous free tier<\/a>. The problem is:<\/p>\n\n<ol>\n<li><p>Lambda functions are fundamentally limited in their processing capacity (they timeout after <em>max<\/em> 15 minutes).<\/p><\/li>\n<li><p>Your model might be expensive to load.<\/p><\/li>\n<\/ol>\n\n<p>If you have a lot of data to run through your model, you will need multiple lambdas. Multiple lambdas means you have to load your model multiple times, and that's wasted work. If you're working with \"big data\" this will get expensive once you get through the free tier.<\/p>\n\n<p>If you don't have much data, Lambda will work just fine. I would eyeball it as follows: assuming your data processing step is dominated by your model step, and if all your model interactions (loading the model + evaluating all your data) take less than 15min, you're definitely fine. If they take more, you'll need to do a back-of-the-envelope calculation to figure out whether you'd leave the Lambda free tier.<\/p>\n\n<p>Regarding Lambda: You can literally copy-paste code in to setup a prototype. If your execution takes more than 15min for all your data, you'll need a method of splitting your data up between multiple Lambdas. Consider <a href=\"https:\/\/aws.amazon.com\/step-functions\/\" rel=\"nofollow noreferrer\">Step Functions<\/a> for this.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":7859515.0,
        "Answerer_created_time":1492048364132,
        "Answerer_last_access_time":1664031142340,
        "Answerer_age":null,
        "Answerer_location":"Cambridge, MA, USA",
        "Answerer_reputation":4436.0,
        "Answerer_views":713.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61091659",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1586317398727,
        "Question_original_content":"run forecast predict model aw lambda read articl topic preliminari thought want share comment experi run machin learn aw project professor school decid us aw need cost effect effici wai deploi forecast model want achiev read data bucket monthli new data come month run python file custom built packag instal depend includ file produc predict result file json csv work push endpoint like tool tableau step flexibl web sure thought prophet model predict result built custom packag us model think notebook instanc gonna help correct wrong understand environ build train model built train model plu won aw pre built model anywai thing want us custom built packag need creat contain imag sure effort option creat multipl lambda function trigger run python script bucket file time new file import bucket happen monthli trigger python script run produc result save bucket option combin option us lambda function trigger implement python script bucket new file come push result endpoint mean host model deploi entir sure pre built model python script instanc host hope experi aw servic help guidanc term cost effect effici wai run model thank",
        "Question_preprocessed_content":"run forecast predict model aw lambda read articl topic preliminari thought want share comment experi run machin learn aw project professor school decid us aw need effici wai deploi forecast model want achiev read data bucket monthli run python file packag instal depend produc predict result file push endpoint step flexibl thought prophet model predict result built custom packag us model think notebook instanc gonna help understand environ build train model built train model plu won aw model anywai thing want us packag need creat contain imag sure effort option creat multipl lambda function trigger run python script bucket time new file import bucket happen monthli trigger python script run produc result save bucket option combin option us lambda function trigger implement python script bucket new file come push result endpoint mean host model deploi entir sure model python script instanc host hope experi aw servic help guidanc term effici wai run model thank",
        "Question_gpt_summary_original":"The user is trying to find a cost-effective and efficient way to deploy a forecasting model on AWS. They are considering using AWS Sagemaker, but they have a customized package and won't be using AWS pre-built models. They are also unsure about creating a container image. Another option they are considering is creating multiple lambda functions to trigger the implementation of python scripts in S3 bucket and push the result using Sagemaker endpoint. The user is seeking guidance on the most cost-effective and efficient way to run the model.",
        "Question_gpt_summary":"user try cost effect effici wai deploi forecast model aw consid custom packag won aw pre built model unsur creat contain imag option consid creat multipl lambda function trigger implement python script bucket push result endpoint user seek guidanc cost effect effici wai run model",
        "Answer_original_content":"depend heavi model data run right identifi lambda like work easi lambda run thing need lambda gener free tier problem lambda function fundament limit process capac timeout max minut model expens load lot data run model need multipl lambda multipl lambda mean load model multipl time wast work work big data expens free tier data lambda work fine eyebal follow assum data process step domin model step model interact load model evalu data min definit fine need envelop calcul figur leav lambda free tier lambda liter copi past code setup prototyp execut take min data need method split data multipl lambda consid step function",
        "Answer_preprocessed_content":"depend heavi model data run right identifi lambda like work easi lambda run thing need lambda gener free tier problem lambda function fundament limit process capac model expens load lot data run model need multipl lambda multipl lambda mean load model multipl time wast work work big data expens free tier data lambda work fine eyebal follow assum data process step domin model step model interact min definit fine need calcul figur leav lambda free tier lambda liter code setup prototyp execut take min data need method split data multipl lambda consid step function",
        "Answer_gpt_summary_original":"The discussion suggests that using AWS Lambda could be a cost-effective solution for deploying the forecasting model, but it has limitations in processing capacity and may require multiple lambdas if there is a lot of data to process. If the model interactions take less than 15 minutes, Lambda will work fine. However, if it takes more than 15 minutes, the user will need to split the data between multiple Lambdas using Step Functions. No solution is provided for creating a container image.",
        "Answer_gpt_summary":"discuss suggest aw lambda cost effect solut deploi forecast model limit process capac requir multipl lambda lot data process model interact minut lambda work fine take minut user need split data multipl lambda step function solut provid creat contain imag"
    },
    {
        "Question_title":"Remote Connection fails in setup of Python data-science client for SQL Server Machine Learning Services",
        "Question_body":"<p>I am trying to test the remote connection of a Python data-science client with SQL Server Machine Learning Services following this guide: <a href=\"https:\/\/docs.microsoft.com\/en-us\/sql\/machine-learning\/python\/setup-python-client-tools-sql\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/sql\/machine-learning\/python\/setup-python-client-tools-sql<\/a> (section 6).\nRunning the following script<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def send_this_func_to_sql():\n    from revoscalepy import RxSqlServerData, rx_import\n    from pandas.tools.plotting import scatter_matrix\n    import matplotlib.pyplot as plt\n    import io\n    \n    # remember the scope of the variables in this func are within our SQL Server Python Runtime\n    connection_string = &quot;Driver=SQL Server;Server=localhost\\instance02;Database=testmlsiris;Trusted_Connection=Yes;&quot;\n    \n    # specify a query and load into pandas dataframe df\n    sql_query = RxSqlServerData(connection_string=connection_string, sql_query = &quot;select * from iris_data&quot;)\n    df = rx_import(sql_query)\n    \n    scatter_matrix(df)\n    \n    # return bytestream of image created by scatter_matrix\n    buf = io.BytesIO()\n    plt.savefig(buf, format=&quot;png&quot;)\n    buf.seek(0)\n    \n    return buf.getvalue()\n\nnew_db_name = &quot;testmlsiris&quot;\nconnection_string = &quot;driver={sql server};server=sqlrzs\\instance02;database=%s;trusted_connection=yes;&quot; \n\nfrom revoscalepy import RxInSqlServer, rx_exec\n\n# create a remote compute context with connection to SQL Server\nsql_compute_context = RxInSqlServer(connection_string=connection_string%new_db_name)\n\n# use rx_exec to send the function execution to SQL Server\nimage = rx_exec(send_this_func_to_sql, compute_context=sql_compute_context)[0]\n<\/code><\/pre>\n<p>yields the following error message returned by rx_exec (stored in the <em>image<\/em> variable)<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>connection_string: &quot;driver={sql server};server=sqlrzs\\instance02;database=testmlsiris;trusted_connection=yes;&quot;\nnum_tasks: 1\nexecution_timeout_seconds: 0\nwait: True\nconsole_output: False\nauto_cleanup: True\npackages_to_load: []\ndescription: &quot;sqlserver&quot;\nversion: &quot;1.0&quot;\nXXX lineno: 2, opcode: 0\nTraceback (most recent call last):\n  File &quot;&lt;string&gt;&quot;, line 3, in &lt;module&gt;\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 664, in rx_sql_satellite_pool_call\n    exec(inputfile.read())\n  File &quot;&lt;string&gt;&quot;, line 34, in &lt;module&gt;\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 886, in rx_remote_call\n    results = rx_resumeexecution(state_file = inputfile, patched_server_name=args[&quot;hostname&quot;])\n  File &quot;E:\\SQL\\MSSQL15.INSTANCE02\\PYTHON_SERVICES\\lib\\site-packages\\revoscalepy\\computecontext\\RxInSqlServer.py&quot;, line 135, in rx_resumeexecution\n    return _state[&quot;function&quot;](**_state[&quot;args&quot;])\n  File &quot;C:\\Users\\username\\sendtosql.py&quot;, line 2, in send_this_func_to_sql\nSystemError: unknown opcode\n====== sqlrzs ( process 0 ) has started run at 2022-06-29 13:47:04 W. Europe Daylight Time ======\n{'local_state': {}, 'args': {}, 'function': &lt;function send_this_func_to_sql at 0x0000020F5810F1E0&gt;}\n<\/code><\/pre>\n<p>What is going wrong here? Line 2 in the script is just an import (which works when testing Python scripts on SQL Server directly). Any help is appreciated - thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1656491540923,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1656503806376,
        "Question_score":0,
        "Question_view_count":54.0,
        "Poster_id":19436129.0,
        "Poster_created_time":1656427742040,
        "Poster_last_access_time":1664010979636,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1.0,
        "Poster_views":6.0,
        "Answer_body":"<p>I just figured out the reason. As of today, the Python versions for the data clients in <a href=\"https:\/\/docs.microsoft.com\/de-de\/sql\/machine-learning\/python\/setup-python-client-tools-sql?view=sql-server-ver15\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/de-de\/sql\/machine-learning\/python\/setup-python-client-tools-sql?view=sql-server-ver15<\/a> are not the newest (revoscalepy Version 9.3), while the version of Machine Learning Services that we have running in our SQL Server is already 9.4.7.\nHowever, the revoscalepy libraries for the client and server must be the same, otherwise the deserialization fails server-sided.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":19436129.0,
        "Answerer_created_time":1656427742040,
        "Answerer_last_access_time":1664010979636,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1.0,
        "Answerer_views":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72798225",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1656513437860,
        "Question_original_content":"remot connect fail setup python data scienc client sql server machin learn servic try test remot connect python data scienc client sql server machin learn servic follow guid http doc microsoft com sql machin learn python setup python client tool sql section run follow script def send func sql revoscalepi import rxsqlserverdata import panda tool plot import scatter matrix import matplotlib pyplot plt import rememb scope variabl func sql server python runtim connect string driver sql server server localhost instanc databas testmlsiri trust connect ye specifi queri load panda datafram sql queri rxsqlserverdata connect string connect string sql queri select iri data import sql queri scatter matrix return bytestream imag creat scatter matrix buf bytesio plt savefig buf format png buf seek return buf getvalu new testmlsiri connect string driver sql server server sqlrz instanc databas trust connect ye revoscalepi import rxinsqlserv exec creat remot comput context connect sql server sql comput context rxinsqlserv connect string connect string new us exec send function execut sql server imag exec send func sql comput context sql comput context yield follow error messag return exec store imag variabl connect string driver sql server server sqlrz instanc databas testmlsiri trust connect ye num task execut timeout second wait true consol output fals auto cleanup true packag load descript sqlserver version lineno opcod traceback recent file line file sql mssql instanc python servic lib site packag revoscalepi computecontext rxinsqlserv line sql satellit pool exec inputfil read file line file sql mssql instanc python servic lib site packag revoscalepi computecontext rxinsqlserv line remot result resumeexecut state file inputfil patch server arg hostnam file sql mssql instanc python servic lib site packag revoscalepi computecontext rxinsqlserv line resumeexecut return state function state arg file user usernam sendtosql line send func sql systemerror unknown opcod sqlrz process start run europ daylight time local state arg function go wrong line script import work test python script sql server directli help appreci thank",
        "Question_preprocessed_content":"remot connect fail setup python client sql server machin learn servic try test remot connect python client sql server machin learn servic follow guid run follow script yield follow error messag return go wrong line script import help appreci thank",
        "Question_gpt_summary_original":"The user is encountering an error while trying to test the remote connection of a Python data-science client with SQL Server Machine Learning Services. The error message returned by rx_exec indicates an unknown opcode, and the user is unsure of what is causing the issue. The user is seeking help to resolve the problem.",
        "Question_gpt_summary":"user encount error try test remot connect python data scienc client sql server machin learn servic error messag return exec indic unknown opcod user unsur caus issu user seek help resolv problem",
        "Answer_original_content":"figur reason todai python version data client http doc microsoft com sql machin learn python setup python client tool sql view sql server ver newest revoscalepi version version machin learn servic run sql server revoscalepi librari client server deseri fail server side",
        "Answer_preprocessed_content":"figur reason todai python version data client newest version machin learn servic run sql server revoscalepi librari client server deseri fail",
        "Answer_gpt_summary_original":"Solution: The issue is caused by a version mismatch between the Python data client and the SQL Server Machine Learning Services. The solution is to ensure that the revoscalepy libraries for the client and server are the same.",
        "Answer_gpt_summary":"solut issu caus version mismatch python data client sql server machin learn servic solut ensur revoscalepi librari client server"
    },
    {
        "Question_title":"Types of Regression Algorithm",
        "Question_body":"What are the types of Regression algorithm? Are there any kinds of regression called \"non-linear regression\"?",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1608719172483,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@SanniddhaChakrabarti-9451 Please follow the below document for Regression.\nhttps:\/\/www.analyticsvidhya.com\/blog\/2015\/08\/comprehensive-guide-regression\/#:~:text=Regression%20analysis%20is%20a%20form,effect%20relationship%20between%20the%20variables.\n\nTypes of Regressions:\nLinear Regression\nLogistic Regression\nPolynomial Regression\nStepwise Regression\nRidge Regression\nLasso Regression\nElasticNet Regression",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/208685\/types-of-regression-algorithm.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1608817848270,
        "Question_original_content":"type regress algorithm type regress algorithm kind regress call non linear regress",
        "Question_preprocessed_content":"type regress algorithm type regress algorithm kind regress call regress",
        "Question_gpt_summary_original":"The user is seeking information about the types of regression algorithms and whether there is a type called \"non-linear regression.\"",
        "Question_gpt_summary":"user seek inform type regress algorithm type call non linear regress",
        "Answer_original_content":"sanniddhachakrabarti follow document regress http analyticsvidhya com blog comprehens guid regress text regress analysi form effect relationship variabl type regress linear regress logist regress polynomi regress stepwis regress ridg regress lasso regress elasticnet regress",
        "Answer_preprocessed_content":"follow document regress type regress linear regress logist regress polynomi regress stepwis regress ridg regress lasso regress elasticnet regress",
        "Answer_gpt_summary_original":"Summary: The discussion provides information about different types of regression algorithms, including linear regression, logistic regression, polynomial regression, stepwise regression, ridge regression, lasso regression, and elastic net regression. The document shared in the discussion provides a comprehensive guide to regression analysis. The user's question about whether there is a type of regression called \"non-linear regression\" is not directly addressed in the discussion.",
        "Answer_gpt_summary":"summari discuss provid inform differ type regress algorithm includ linear regress logist regress polynomi regress stepwis regress ridg regress lasso regress elast net regress document share discuss provid comprehens guid regress analysi user question type regress call non linear regress directli address discuss"
    },
    {
        "Question_title":"UserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.",
        "Question_body":"Hi, I am getting this error when scoring a model.\n\nSeems like it is an out-of-memory issue or a segfault issue (no idea what that means).\n\nI'm using Designer while my compute is Standard Dv2 Family vCPUs. Have made no changes to my storage account key.\n\nAny advice on how to debug this one? Many thanks in advance\n\nAzureMLCompute job failed.\nUserProcessKilledBySystemSignal: Job failed since the user script received system termination signal usually due to out-of-memory or segfault.\nReason: Process Killed with either 6:aborted or 9:killed or 11:segment fault. exit code here is from wrapping bash hence 128 + n\nCause: killed\nTaskIndex:\nNodeIp: 10.0.0.5\nNodeId: tvmps_ee452edcf7395836bdf60c0e0cd5f3a6308fafbb41c860c50a47be1367393df6_d\nReason: Job failed with non-zero exit Code",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1620234016640,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Seems like it was an out-of-memory problem. If I reduce the trainning set, I get no error.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/384152\/userprocesskilledbysystemsignal-job-failed-since-t.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1620235517220,
        "Question_original_content":"userprocesskilledbysystemsign job fail user script receiv termin signal usual memori segfault get error score model like memori issu segfault issu idea mean design comput standard famili vcpu chang storag account kei advic debug thank advanc comput job fail userprocesskilledbysystemsign job fail user script receiv termin signal usual memori segfault reason process kill abort kill segment fault exit code wrap bash caus kill taskindex nodeip nodeid tvmp eeedcfbdfcecdfafafbbccabedf reason job fail non zero exit code",
        "Question_preprocessed_content":"userprocesskilledbysystemsign job fail user script receiv termin signal usual segfault get error score model like issu segfault issu design comput standard famili vcpu chang storag account kei advic debug thank advanc comput job fail userprocesskilledbysystemsign job fail user script receiv termin signal usual segfault reason process kill abort kill segment fault exit code wrap bash caus kill taskindex nodeip nodeid reason job fail exit code",
        "Question_gpt_summary_original":"The user encountered an error while scoring a model in AzureMLCompute, which indicates an out-of-memory or segfault issue. The user is unsure how to debug the issue and is seeking advice. The job failed due to the user script receiving a system termination signal, which is usually caused by out-of-memory or segfault issues. The exit code indicates that the process was killed with either 6:aborted, 9:killed, or 11:segment fault.",
        "Question_gpt_summary":"user encount error score model comput indic memori segfault issu user unsur debug issu seek advic job fail user script receiv termin signal usual caus memori segfault issu exit code indic process kill abort kill segment fault",
        "Answer_original_content":"like memori problem reduc train set error",
        "Answer_preprocessed_content":"like problem reduc train set error",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to reduce the training set to avoid the out-of-memory issue.",
        "Answer_gpt_summary":"solut possibl solut mention discuss reduc train set avoid memori issu"
    },
    {
        "Question_title":"Vertex AI create endpoint error - FAILED_PRECONDITION: Project xxxxxxxx is not active.",
        "Question_body":"Hi, I'm stuck at following error message when I try to create vertex-ai endpoint from workbench notebook.  I have enabled aiplatform.googleapis.com.Command:\ngcloud ai endpoints create \\\n--project=XXXXX\n--region=us-central1 \\\n--display-name=ld-test-resnet-classifierUsing endpoint [https:\/\/us-central1-aiplatform.googleapis.com\/]\nERROR: (gcloud.ai.endpoints.create) FAILED_PRECONDITION: Project XXXXXXXXXX is not active.Please suggest what am I missing.   ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1661561100000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":144.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi,\nThe issue is resolved.\nAt least one model has to be uploaded first to model registry for this command to work.\nThe official documentation titled \"Deploy a model using the Vertex AI API\" - implies deploy a model uploaded to model registry\".\n\nThanks for the views.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-create-endpoint-error-FAILED-PRECONDITION-Project\/td-p\/460565\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1661575080000,
        "Question_original_content":"creat endpoint error fail precondit project activ stuck follow error messag try creat vertex endpoint workbench notebook enabl aiplatform googleapi com command gcloud endpoint creat project region central displai test resnet classifierus endpoint http central aiplatform googleapi com error gcloud endpoint creat fail precondit project activ suggest miss",
        "Question_preprocessed_content":"creat endpoint error project activ stuck follow error messag try creat endpoint workbench notebook enabl gcloud endpoint creat project endpoint error project suggest miss",
        "Question_gpt_summary_original":"The user encountered an error message when trying to create a Vertex AI endpoint from a workbench notebook. The error message stated that the project was not active, resulting in a FAILED_PRECONDITION error. The user is seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error messag try creat endpoint workbench notebook error messag state project activ result fail precondit error user seek assist resolv issu",
        "Answer_original_content":"issu resolv model upload model registri command work offici document titl deploi model api impli deploi model upload model registri thank view view solut origin post",
        "Answer_preprocessed_content":"issu resolv model upload model registri command work offici document titl deploi model api impli deploi model upload model registri thank view view solut origin post",
        "Answer_gpt_summary_original":"Solution: The issue was resolved by uploading at least one model to the model registry before creating a Vertex AI endpoint. The official documentation titled \"Deploy a model using the Vertex AI API\" implies that a model uploaded to the model registry is required for this command to work.",
        "Answer_gpt_summary":"solut issu resolv upload model model registri creat endpoint offici document titl deploi model api impli model upload model registri requir command work"
    },
    {
        "Question_title":"Sagemaker with tensorflow 2 not saving model",
        "Question_body":"<p>I am working with Keras and I am trying to train a model using Sagemaker. I have the following issue:\nWhen I train my model using TensorFlow 1.12 everything works fine:<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='entrypoint-2.py',\n                            base_job_name='mlearning-test',\n                         role=role,\n                         train_instance_count=1,\n                         input_mode='Pipe',\n                         train_instance_type='ml.p2.xlarge',\n                         framework_version='1.12.0')\n<\/code><\/pre>\n\n<p>My model is trained and the model is saved in S3. Not problems.<\/p>\n\n<p>However, if I changed the framework version to be 2.0.0<\/p>\n\n<pre><code>estimator = TensorFlow(entry_point='entrypoint-2.py',\n                                base_job_name='mlearning-test',\n                             role=role,\n                             train_instance_count=1,\n                             input_mode='Pipe',\n                             train_instance_type='ml.p2.xlarge',\n                             framework_version='2.0.0')\n<\/code><\/pre>\n\n<p>I get the following error: <\/p>\n\n<pre><code>2020-02-12 13:54:36,601 sagemaker_tensorflow_container.training WARNING  No model artifact is saved under path \/opt\/ml\/model. Your training job will not save any model files to S3.\nFor details of how to construct your training script see:\nhttps:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#adapting-your-local-tensorflow-script\n<\/code><\/pre>\n\n<p>The training job is marked as successful but there is nothing in the S3 bucket and indeed there was not training.<\/p>\n\n<p>As an alternative, I tried putting the py_version='py3' but this keeps happening. is there a major difference that I am not aware of when using TF2 on sagemaker? <\/p>\n\n<p>I don't think the entry point is needed since it works fine with version 1.12 but in case you are curious or can spot something here it is:<\/p>\n\n<pre><code>import tensorflow as tf\nfrom sagemaker_tensorflow import PipeModeDataset\n#from tensorflow.contrib.data import map_and_batch\n\nINPUT_TENSOR_NAME = 'inputs_input'  \nBATCH_SIZE = 64\nNUM_CLASSES = 5\nBUFFER_SIZE = 50\nPREFETCH_SIZE = 1\nLENGHT = 512\nSEED = 26\nEPOCHS = 1\nWIDTH = 512\n\ndef keras_model_fn(hyperparameters):\n    model = tf.keras.Sequential([\n        tf.keras.layers.Dense(WIDTH, 'relu', input_shape=(None, WIDTH), name = 'inputs'),\n        #tf.keras.layers.InputLayer(input_shape=(None, WIDTH), name=INPUT_TENSOR_NAME),\n        tf.keras.layers.Dense(256, 'relu'),\n        tf.keras.layers.Dense(128, 'relu'),\n        tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')\n    ])\n\n    opt = tf.keras.optimizers.RMSprop()\n\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=[\"accuracy\"])\n    return model\n\ndef serving_input_fn(hyperparameters):\n    # Notice that the input placeholder has the same input shape as the Keras model input\n    tensor = tf.placeholder(tf.float32, shape=[None, WIDTH])\n\n    # The inputs key INPUT_TENSOR_NAME matches the Keras InputLayer name\n    inputs = {INPUT_TENSOR_NAME: tensor}\n    return tf.estimator.export.ServingInputReceiver(inputs, inputs)\n\ndef train_input_fn(training_dir, params):\n    \"\"\"Returns input function that would feed the model during training\"\"\"\n    return _input_fn('train')\n\ndef eval_input_fn(training_dir, params):\n    \"\"\"Returns input function that would feed the model during evaluation\"\"\"\n    return _input_fn('eval')\n\ndef _input_fn(channel):\n    \"\"\"Returns a Dataset for reading from a SageMaker PipeMode channel.\"\"\"\n    print(\"DATA \"+channel)\n    features={\n        'question': tf.FixedLenFeature([WIDTH], tf.float32),\n        'label': tf.FixedLenFeature([1], tf.int64)\n    }\n\n    def parse(record):\n        parsed = tf.parse_single_example(record, features)\n        #print(\"--------&gt;\"+str(tf.cast(parsed['question'], tf.float32))\n        return {\n            INPUT_TENSOR_NAME: tf.cast(parsed['question'], tf.float32)\n        }, parsed['label']\n\n    ds = PipeModeDataset(channel)\n    if EPOCHS &gt; 1:\n        ds = ds.repeat(EPOCHS)\n    ds = ds.prefetch(PREFETCH_SIZE)\n    #ds = ds.apply(map_and_batch(parse, batch_size=BATCH_SIZE,\n    #                            num_parallel_batches=BUFFER_SIZE))\n    ds = ds.map(parse, num_parallel_calls=NUM_PARALLEL_BATCHES)\n    ds = ds.batch(BATCH_SIZE)\n\n    return ds\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1581516927893,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":913.0,
        "Poster_id":6308651.0,
        "Poster_created_time":1462770189772,
        "Poster_last_access_time":1626440092163,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":45.0,
        "Poster_views":15.0,
        "Answer_body":"<p>you're correct, <strong>there has been a major, beneficial change last year in the SageMaker TensorFlow experience named the <em>Script Mode<\/em> formalism<\/strong>. As you can see in the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">SDK Documentation<\/a>:<\/p>\n\n<p><em>\"Warning.\nWe have added a new format of your TensorFlow training script with TensorFlow version 1.11. This new way gives the user script more flexibility. This new format is called Script Mode, as opposed to Legacy Mode, which is what we support with TensorFlow 1.11 and older versions. In addition we are adding Python 3 support with Script Mode. The last supported version of Legacy Mode will be TensorFlow 1.12. Script Mode is available with TensorFlow version 1.11 and newer. Make sure you refer to the correct version of this README when you prepare your script. You can find the Legacy Mode README here.\"<\/em><\/p>\n\n<p>with TensorFlow 2, you need to follow that <em>Script Mode<\/em> formalism and save your model in the <code>opt\/ml\/model<\/code> path, otherwise nothing will be sent to S3. <em>Script Mode<\/em> is quite straightforward to implement and gives better flexibility and portability, and this spec is shared with SageMaker Sklearn container, SageMaker Pytorch container and SageMaker MXNet container so definitely worth adopting<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":5331834.0,
        "Answerer_created_time":1442180190107,
        "Answerer_last_access_time":1664048608876,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60190365",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1581724456863,
        "Question_original_content":"tensorflow save model work kera try train model follow issu train model tensorflow work fine estim tensorflow entri point entrypoint base job mlearn test role role train instanc count input mode pipe train instanc type xlarg framework version model train model save problem chang framework version estim tensorflow entri point entrypoint base job mlearn test role role train instanc count input mode pipe train instanc type xlarg framework version follow error tensorflow contain train warn model artifact save path opt model train job save model file detail construct train script http readthedoc stabl html adapt local tensorflow script train job mark success bucket train altern tri put version keep happen major differ awar think entri point need work fine version case curiou spot import tensorflow tensorflow import pipemodedataset tensorflow contrib data import map batch input tensor input input batch size num class buffer size prefetch size lenght seed epoch width def kera model hyperparamet model kera sequenti kera layer dens width relu input shape width input kera layer inputlay input shape width input tensor kera layer dens relu kera layer dens relu kera layer dens num class activ softmax opt kera optim rmsprop model compil loss categor crossentropi optim opt metric accuraci return model def serv input hyperparamet notic input placehold input shape kera model input tensor placehold float shape width input kei input tensor match kera inputlay input input tensor tensor return estim export servinginputreceiv input input def train input train dir param return input function feed model train return input train def eval input train dir param return input function feed model evalu return input eval def input channel return dataset read pipemod channel print data channel featur question fixedlenfeatur width float label fixedlenfeatur int def pars record pars pars singl exampl record featur print str cast pars question float return input tensor cast pars question float pars label pipemodedataset channel epoch repeat epoch prefetch prefetch size appli map batch pars batch size batch size num parallel batch buffer size map pars num parallel call num parallel batch batch batch size return",
        "Question_preprocessed_content":"tensorflow save model work kera try train model follow issu train model tensorflow work fine model train model save problem chang framework version follow error train job mark success bucket train altern tri put keep happen major differ awar think entri point need work fine version case curiou spot",
        "Question_gpt_summary_original":"The user is facing challenges while trying to save a model in Sagemaker using TensorFlow 2.0.0. The training job is marked as successful but the model is not saved in the S3 bucket. The user has tried changing the py_version to 'py3' but the issue persists. The user has shared the entry point code for reference.",
        "Question_gpt_summary":"user face challeng try save model tensorflow train job mark success model save bucket user tri chang version issu persist user share entri point code refer",
        "Answer_original_content":"correct major benefici chang year tensorflow experi name script mode formal sdk document warn ad new format tensorflow train script tensorflow version new wai give user script flexibl new format call script mode oppos legaci mode support tensorflow older version addit ad python support script mode support version legaci mode tensorflow script mode avail tensorflow version newer sure refer correct version readm prepar script legaci mode readm tensorflow need follow script mode formal save model opt model path sent script mode straightforward implement give better flexibl portabl spec share sklearn contain pytorch contain mxnet contain definit worth adopt",
        "Answer_preprocessed_content":"correct major benefici chang year tensorflow experi name script mode formal sdk document warn ad new format tensorflow train script tensorflow version new wai give user script flexibl new format call script mode oppos legaci mode support tensorflow older version addit ad python support script mode support version legaci mode tensorflow script mode avail tensorflow version newer sure refer correct version readm prepar script legaci mode readm tensorflow need follow script mode formal save model path sent script mode straightforward implement give better flexibl portabl spec share sklearn contain pytorch contain mxnet contain definit worth adopt",
        "Answer_gpt_summary_original":"Solution: The user needs to follow the Script Mode formalism while saving the model in Sagemaker using TensorFlow 2.0.0. The model needs to be saved in the opt\/ml\/model path, otherwise, it will not be sent to S3. The Script Mode is easy to implement and provides better flexibility and portability.",
        "Answer_gpt_summary":"solut user need follow script mode formal save model tensorflow model need save opt model path sent script mode easi implement provid better flexibl portabl"
    },
    {
        "Question_title":"How to find memory leak in Python MXNet?",
        "Question_body":"<p>I am afraid that my Neural Network in MXNet, written in Python, has a memory leak. I have tried the MXNet profiler and the tracemalloc module to get an understanding of memory profiling, but I want to get information on any potential memory leaks, just like I'd do with valgrind in C.<\/p>\n\n<p>I found <a href=\"https:\/\/cwiki.apache.org\/confluence\/display\/MXNET\/Detecting+Memory+Leaks+and+Buffer+Overflows+in+MXNet\" rel=\"nofollow noreferrer\">Detecting Memory Leaks and Buffer Overflows in MXNet<\/a>, and after managing to build like described in section \"Using ASAN builds with MXNet\", by replacing the \"ubuntu_cpu\" part in <code>docker\/Dockerfile.build.ubuntu_cpu -t mxnetci\/build.ubuntu_cpu<\/code> with \"ubuntu_cpu_python\", I tried executing in an AWS Sagemaker Notebook like this:<\/p>\n\n<pre><code>root@33e38e00f825:\/work\/mxnet# nosetests3 --verbose \/home\/ec2-user\/SageMaker\/run_predict.py\n<\/code><\/pre>\n\n<p>and I get this import error:<\/p>\n\n<blockquote>\n  <p>Failure: ImportError (No module named 'run_predict') ... ERROR<\/p>\n<\/blockquote>\n\n<p>My run_predict.py looks like this:<\/p>\n\n<pre><code>#!\/usr\/bin\/env python\ndef run_predict(n):\n  # calling MXNet inference method\n\nrun_predict(-1)  # tried it putting it under 'if __name__ == \"__main__\":'\n<\/code><\/pre>\n\n<p>What I am missing in my script, what should I change?<\/p>\n\n<p>The example script they use in the link is <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/faccd91071cc34ed0b3a192d3c7932441fe7e35e\/tests\/python\/unittest\/test_rnn.py\" rel=\"nofollow noreferrer\">rnn_test.py<\/a>, but even when I run this example, I still get an analogous Import Error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1592493163510,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1592579151163,
        "Question_score":0,
        "Question_view_count":263.0,
        "Poster_id":2411320.0,
        "Poster_created_time":1369257942212,
        "Poster_last_access_time":1663776093950,
        "Poster_age":null,
        "Poster_location":"London, UK",
        "Poster_reputation":70285.0,
        "Poster_views":13121.0,
        "Answer_body":"<p>In MXNet, we automatically test for this through examining the garbage collection records. You can find how it's implemented here: <a href=\"https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79\" rel=\"nofollow noreferrer\">https:\/\/github.com\/apache\/incubator-mxnet\/blob\/c3aff732371d6177e5d522c052fb7258978d8ce4\/tests\/python\/conftest.py#L26-L79<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":3062895.0,
        "Answerer_created_time":1386098048127,
        "Answerer_last_access_time":1649194421867,
        "Answerer_age":null,
        "Answerer_location":"New York, NY, USA",
        "Answerer_reputation":619.0,
        "Answerer_views":25.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62453292",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1598820674152,
        "Question_original_content":"memori leak python mxnet afraid neural network mxnet written python memori leak tri mxnet profil tracemalloc modul understand memori profil want inform potenti memori leak like valgrind detect memori leak buffer overflow mxnet manag build like describ section asan build mxnet replac ubuntu cpu docker dockerfil build ubuntu cpu mxnetci build ubuntu cpu ubuntu cpu python tri execut notebook like root eef work mxnet nosetest verbos home user run predict import error failur importerror modul name run predict error run predict look like usr bin env python def run predict call mxnet infer method run predict tri put main miss script chang exampl script us link rnn test run exampl analog import error",
        "Question_preprocessed_content":"memori leak python mxnet afraid neural network mxnet written python memori leak tri mxnet profil tracemalloc modul understand memori profil want inform potenti memori leak like valgrind detect memori leak buffer overflow mxnet manag build like describ section asan build mxnet replac tri execut notebook like import error failur importerror error look like miss script chang exampl script us link run exampl analog import error",
        "Question_gpt_summary_original":"The user is facing challenges in detecting memory leaks in their Neural Network in MXNet, written in Python. They have tried using the MXNet profiler and the tracemalloc module but want to get information on potential memory leaks. They found a guide on detecting memory leaks in MXNet and tried to execute a script in an AWS Sagemaker Notebook but encountered an import error. They are seeking help to identify what they are missing in their script and what needs to be changed.",
        "Question_gpt_summary":"user face challeng detect memori leak neural network mxnet written python tri mxnet profil tracemalloc modul want inform potenti memori leak guid detect memori leak mxnet tri execut script notebook encount import error seek help identifi miss script need chang",
        "Answer_original_content":"mxnet automat test examin garbag collect record implement http github com apach incub mxnet blob caffdedcfbdc test python conftest",
        "Answer_preprocessed_content":"mxnet automat test examin garbag collect record implement",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to automatically test for memory leaks in MXNet by examining the garbage collection records. The implementation of this solution can be found in the given link.",
        "Answer_gpt_summary":"solut discuss provid solut automat test memori leak mxnet examin garbag collect record implement solut given link"
    },
    {
        "Question_title":"Training keras model in AWS Sagemaker",
        "Question_body":"<p>I have keras training script on my machine. I am experimenting to run my script on AWS sagemaker container. For that I have used below code.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\nest = TensorFlow(\n    entry_point=&quot;caller.py&quot;,\n    source_dir=&quot;.\/&quot;,\n    role='role_arn',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_type='ml.m5.large',\n    instance_count=1,\n    hyperparameters={'batch': 8, 'epochs': 10},\n)\n\nest.fit()\n<\/code><\/pre>\n<p>here <code>caller.py<\/code> is my entry point. After executing the above code I am getting <code>keras is not installed<\/code>. Here is the stacktrace.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;executor.py&quot;, line 14, in &lt;module&gt;\n    est.fit()\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 682, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 1625, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3681, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3240, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-06-09-07-14-01-778: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.7 caller.py --batch 4 --epochs 10\n\nModuleNotFoundError: No module named 'keras'\n\n<\/code><\/pre>\n<ol>\n<li>Which instance has pre-installed keras?<\/li>\n<li>Is there any way I can install the python package to the AWS container? or any workaround for the issue?<\/li>\n<\/ol>\n<p>Note: I have tried with my own container uploading to ECR and successfully run my code. I am looking for AWS's existing container capability.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1623223568407,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1623224188496,
        "Question_score":0,
        "Question_view_count":316.0,
        "Poster_id":4684861.0,
        "Poster_created_time":1426675778223,
        "Poster_last_access_time":1664028138916,
        "Poster_age":null,
        "Poster_location":"Coimbatore, Tamil Nadu, India",
        "Poster_reputation":10189.0,
        "Poster_views":1471.0,
        "Answer_body":"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras<\/code> instead of <code>keras<\/code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.\nYou container is <a href=\"https:\/\/aws.amazon.com\/releasenotes\/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0\/\" rel=\"nofollow noreferrer\">this<\/a>, as you can see from the list of the packages, there is no <code>Keras<\/code>.\nIf you instead want to extend a pre-built container you can take a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">here<\/a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":4267439.0,
        "Answerer_created_time":1416346350292,
        "Answerer_last_access_time":1664039219436,
        "Answerer_age":null,
        "Answerer_location":"Jesi, Italy",
        "Answerer_reputation":2302.0,
        "Answerer_views":227.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67899421",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1623320398007,
        "Question_original_content":"train kera model kera train script machin experi run script contain code tensorflow import tensorflow est tensorflow entri point caller sourc dir role role arn framework version version instanc type larg instanc count hyperparamet batch epoch est fit caller entri point execut code get kera instal stacktrac traceback recent file executor line est fit file home thasin document python venv lib python site packag estim line fit self latest train job wait log log file home thasin document python venv lib python site packag estim line wait self session log job self job wait true log type log file home thasin document python venv lib python site packag session line log job self check job statu job descript trainingjobstatu file home thasin document python venv lib python site packag session line check job statu rais except unexpectedstatusexcept except unexpectedstatusexcept error train job tensorflow train fail reason algorithmerror executeuserscripterror command usr local bin python caller batch epoch modulenotfounderror modul name kera instanc pre instal kera wai instal python packag aw contain workaround issu note tri contain upload ecr successfulli run code look aw exist contain capabl",
        "Question_preprocessed_content":"train kera model kera train script machin experi run script contain code entri point execut code get stacktrac instanc kera wai instal python packag aw contain workaround issu note tri contain upload ecr successfulli run code look aw exist contain capabl",
        "Question_gpt_summary_original":"The user is facing challenges while training a Keras model in AWS Sagemaker. After executing the code, the user is getting an error message stating that Keras is not installed. The user is seeking answers to two questions: which instance has pre-installed Keras, and is there any way to install the Python package to the AWS container or any workaround for the issue? The user has tried running the code on their own container uploaded to ECR and it worked successfully, but they are looking for AWS's existing container capability.",
        "Question_gpt_summary":"user face challeng train kera model execut code user get error messag state kera instal user seek answer question instanc pre instal kera wai instal python packag aw contain workaround issu user tri run code contain upload ecr work successfulli look aw exist contain capabl",
        "Answer_original_content":"kera tensorflow reformat code us kera instead kera version tensorflow sync difficult contain list packag kera instead want extend pre built contain look recommend specif us case futur code maintain kera",
        "Answer_preprocessed_content":"kera tensorflow reformat code us instead version tensorflow sync difficult contain list packag instead want extend contain look recommend specif futur code maintain",
        "Answer_gpt_summary_original":"Solution 1: Reformat the code to use tf.keras instead of Keras since Keras is now part of tensorflow and they are in sync since version 2.3.0.\n\nSolution 2: Extend a pre-built container by following the instructions provided in the AWS Sagemaker documentation, but it is not recommended in this specific use-case as it is better to use tf.keras for future code maintainability. \n\nNote: No solution is provided for installing the Python package to the AWS container or any workaround for the issue.",
        "Answer_gpt_summary":"solut reformat code us kera instead kera kera tensorflow sync version solut extend pre built contain follow instruct provid document recommend specif us case better us kera futur code maintain note solut provid instal python packag aw contain workaround issu"
    },
    {
        "Question_title":"Jupyter notebook kernel dies when creating dummy variables with pandas",
        "Question_body":"<p>I am working on the Walmart Kaggle competition and I'm trying to create a dummy column of of the \"FinelineNumber\" column. For context, <code>df.shape<\/code> returns <code>(647054, 7)<\/code>. I am trying to make a dummy column for <code>df['FinelineNumber']<\/code>, which has 5,196 unique values. The results should be a dataframe of shape <code>(647054, 5196)<\/code>, which I then plan to <code>concat<\/code> to the original dataframe. <\/p>\n\n<p>Nearly every time I run <code>fineline_dummies = pd.get_dummies(df['FinelineNumber'], prefix='fl')<\/code>, I get the following error message <code>The kernel appears to have died. It will restart automatically.<\/code> I am running python 2.7 in jupyter notebook on a MacBookPro with 16GB RAM.<\/p>\n\n<p>Can someone explain why this is happening (and why it happens most of the time but not every time)? Is it a jupyter notebook or pandas bug? Also, I thought it might have to do with not enough RAM but I get the same error on a Microsoft Azure Machine Learning notebook with >100 GB of RAM. On Azure ML, the kernel dies every time - almost immediately.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1449073471520,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1454300528203,
        "Question_score":5,
        "Question_view_count":5063.0,
        "Poster_id":2569531.0,
        "Poster_created_time":1373475615300,
        "Poster_last_access_time":1663780318292,
        "Poster_age":null,
        "Poster_location":"New York, United States",
        "Poster_reputation":2037.0,
        "Poster_views":193.0,
        "Answer_body":"<p>It very much could be memory usage - a 647054, 5196 data frame has 3,362,092,584 elements, which would be 24GB just for the pointers to the objects on a 64-bit system.  On AzureML while the VM has a large amount of memory you're actually limited in how much memory you have available (currently 2GB, soon to be 4GB) - and when you hit the limit the kernel typically dies.  So it seems very likely it is a memory usage issue.<\/p>\n\n<p>You might try doing <a href=\"http:\/\/pandas.pydata.org\/pandas-docs\/stable\/sparse.html\" rel=\"noreferrer\">.to_sparse()<\/a> on the data frame first before doing any additional manipulations.  That should allow Pandas to keep most of the data frame out of memory.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Answerer_id":114000.0,
        "Answerer_created_time":1243547542743,
        "Answerer_last_access_time":1601363112243,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":6438.0,
        "Answerer_views":922.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34047782",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1449768300887,
        "Question_original_content":"jupyt notebook kernel di creat dummi variabl panda work walmart kaggl competit try creat dummi column finelinenumb column context shape return try dummi column finelinenumb uniqu valu result datafram shape plan concat origin datafram nearli time run finelin dummi dummi finelinenumb prefix follow error messag kernel appear di restart automat run python jupyt notebook macbookpro ram explain happen happen time time jupyt notebook panda bug thought ram error notebook ram kernel di time immedi",
        "Question_preprocessed_content":"jupyt notebook kernel di creat dummi variabl panda work walmart kaggl competit try creat dummi column finelinenumb column context return try dummi column uniqu valu result datafram shape plan origin datafram nearli time run follow error messag run python jupyt notebook macbookpro ram explain happen jupyt notebook panda bug thought ram error notebook ram kernel di time immedi",
        "Question_gpt_summary_original":"The user is encountering a problem while creating a dummy column for the \"FinelineNumber\" column in a dataset with 5,196 unique values. The expected result is a dataframe of shape (647054, 5196), which will be concatenated to the original dataframe. However, every time the user runs the code, the Jupyter notebook kernel dies, and the error message \"The kernel appears to have died. It will restart automatically\" is displayed. The user is running Python 2.7 in Jupyter notebook on a MacBookPro with 16GB RAM and has also tried running the code on a Microsoft Azure Machine Learning notebook with >100 GB of RAM, but the kernel dies every time. The user is seeking an explanation for why this is happening and whether it is a Jupyter notebook or pandas bug.",
        "Question_gpt_summary":"user encount problem creat dummi column finelinenumb column dataset uniqu valu expect result datafram shape concaten origin datafram time user run code jupyt notebook kernel di error messag kernel appear di restart automat displai user run python jupyt notebook macbookpro ram tri run code notebook ram kernel di time user seek explan happen jupyt notebook panda bug",
        "Answer_original_content":"memori usag data frame element pointer object bit larg memori actual limit memori avail current soon hit limit kernel typic di like memori usag issu try spars data frame addit manipul allow panda data frame memori",
        "Answer_preprocessed_content":"memori usag data frame element pointer object larg memori actual limit memori avail hit limit kernel typic di like memori usag issu try data frame addit manipul allow panda data frame memori",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use the \".to_sparse()\" method on the data frame before any additional manipulations. This should allow Pandas to keep most of the data frame out of memory.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us spars method data frame addit manipul allow panda data frame memori"
    },
    {
        "Question_title":"SageMaker Ground Truth with TensorFlow",
        "Question_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1548942784467,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":454.0,
        "Poster_id":2089899.0,
        "Poster_created_time":1361339272692,
        "Poster_last_access_time":1663965928400,
        "Poster_age":null,
        "Poster_location":"NYC",
        "Poster_reputation":6281.0,
        "Poster_views":958.0,
        "Answer_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":10768172.0,
        "Answerer_created_time":1544390307847,
        "Answerer_last_access_time":1595578657467,
        "Answerer_age":null,
        "Answerer_location":"Palo Alto, CA, USA",
        "Answerer_reputation":151.0,
        "Answerer_views":32.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1549915976767,
        "Question_original_content":"ground truth tensorflow seen exampl label data ground truth data train shelf model abl us annot format tensorflow script mode specif tensorflow kera model train script mode like data label ground truth convert script file mode pipe mode",
        "Question_preprocessed_content":"ground truth tensorflow seen exampl label data ground truth data train model abl us annot format tensorflow script mode specif model train script mode like data label ground truth convert script file mode pipe mode",
        "Question_gpt_summary_original":"The user is facing a challenge of whether they can use the annotation format of SageMaker Ground Truth with TensorFlow Script Mode. They have a tensorflow.keras model that they want to train using TF Script Mode and want to convert their script from File mode to Pipe mode using data labeled with Ground Truth.",
        "Question_gpt_summary":"user face challeng us annot format ground truth tensorflow script mode tensorflow kera model want train script mode want convert script file mode pipe mode data label ground truth",
        "Answer_original_content":"ground truth team happi assist experi clear understand run model estim contain http github com aw python sdk blob master src tensorflow readm rst",
        "Answer_preprocessed_content":"ground truth team happi assist experi clear understand run model estim contain",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Can I download onnx model from pipeline create by Azure Machine Learning Designer",
        "Question_body":"I don't understand the model format trained by pipeline create by Azure Machine Learning Designer, Like default Linear Regression. Can I download the onnx format model? and How.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658129471700,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@QingShuang-7885 If the pipeline is created using the designer, then the train model module output would be .ilearner file which is a binary format that encapsulates the statistical patterns learned from the data. You cannot directly modify or read this format; however, other components can use this trained model in your pipelines and you can register the model and deploy it as a endpoint. Please refer this document for more details about the training process.\n\nHowever, if you use the SDK and register a different model format you can download the trained model and convert it to ONNX if it is supported from the listed formats on this page.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/930701\/can-i-download-onnx-model-from-pipeline-create-by.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1658147834667,
        "Question_original_content":"download onnx model pipelin creat design understand model format train pipelin creat design like default linear regress download onnx format model",
        "Question_preprocessed_content":"download onnx model pipelin creat design understand model format train pipelin creat design like default linear regress download onnx format model",
        "Question_gpt_summary_original":"The user is facing challenges in understanding the model format trained by pipeline created by Azure Machine Learning Designer, specifically the default Linear Regression. They are seeking to download the model in onnx format but are unsure of how to do so.",
        "Question_gpt_summary":"user face challeng understand model format train pipelin creat design specif default linear regress seek download model onnx format unsur",
        "Answer_original_content":"qingshuang pipelin creat design train model modul output ilearn file binari format encapsul statist pattern learn data directli modifi read format compon us train model pipelin regist model deploi endpoint refer document detail train process us sdk regist differ model format download train model convert onnx support list format page answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"pipelin creat design train model modul output ilearn file binari format encapsul statist pattern learn data directli modifi read format compon us train model pipelin regist model deploi endpoint refer document detail train process us sdk regist differ model format download train model convert onnx support list format page answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Solution: The user can download the trained model in onnx format if they use the SDK and register a different model format. However, if the pipeline is created using the designer, the train model module output would be in .ilearner file format which cannot be directly modified or read. The user can register the model and deploy it as an endpoint.",
        "Answer_gpt_summary":"solut user download train model onnx format us sdk regist differ model format pipelin creat design train model modul output ilearn file format directli modifi read user regist model deploi endpoint"
    },
    {
        "Question_title":"Print SageMaker instance type",
        "Question_body":"<p>Is there a function I can use to get the instance type of my SageMaker instance.<\/p>\n<p>I basically want to do something like this<\/p>\n<pre><code>region = boto3.Session().region_name\n<\/code><\/pre>\n<p>but for the instance type.<\/p>\n<p>I know I can find it manually, but I want to automate it so that my script can work on any instance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658254476363,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":40.0,
        "Poster_id":19489788.0,
        "Poster_created_time":1657050754840,
        "Poster_last_access_time":1663796977727,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":15.0,
        "Poster_views":7.0,
        "Answer_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeNotebookInstance.html\" rel=\"nofollow noreferrer\">DescribeNotebookInstance<\/a> API to get the instance size.<\/p>\n<pre><code>sm_client = boto3.client(&quot;sagemaker&quot;)\nsm.describe_notebook_instance(\n    NotebookInstanceName=&lt;nb-name&gt;\n)['InstanceType']\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":2458691.0,
        "Answerer_created_time":1370505440848,
        "Answerer_last_access_time":1663973511163,
        "Answerer_age":null,
        "Answerer_location":"Calgary, AB",
        "Answerer_reputation":333.0,
        "Answerer_views":32.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73041737",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1658271130892,
        "Question_original_content":"print instanc type function us instanc type instanc basic want like region boto session region instanc type know manual want autom script work instanc",
        "Question_preprocessed_content":"print instanc type function us instanc type instanc basic want like instanc type know manual want autom script work instanc",
        "Question_gpt_summary_original":"The user is looking for a way to automate the process of finding the instance type of their SageMaker instance, as they want their script to work on any instance without having to manually find the instance type.",
        "Question_gpt_summary":"user look wai autom process find instanc type instanc want script work instanc have manual instanc type",
        "Answer_original_content":"us describenotebookinst api instanc size client boto client notebook instanc notebookinstancenam instancetyp",
        "Answer_preprocessed_content":"us describenotebookinst api instanc size",
        "Answer_gpt_summary_original":"Solution: The user can use the DescribeNotebookInstance API to get the instance size. They can use the provided code snippet to get the instance type.",
        "Answer_gpt_summary":"solut user us describenotebookinst api instanc size us provid code snippet instanc type"
    },
    {
        "Question_title":"DVC shows files not tracked in source control in visual studio code",
        "Question_body":"<p>I'm using DVC extension in VScode inside a python project. The problem is that dvc shows files not tracked by dvc in the source control panel! As in the following picture.\nDVC track only data folder and not the src folder. How can I fix it? Have you also encountered these problems?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/sn8YY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sn8YY.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1662017466370,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1662021092696,
        "Question_score":0,
        "Question_view_count":39.0,
        "Poster_id":12829151.0,
        "Poster_created_time":1580668804396,
        "Poster_last_access_time":1664031434116,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":498.0,
        "Poster_views":66.0,
        "Answer_body":"<p>The files shown are completely untracked. They are shown in both SCM trees so you can add them to either Git or DVC using inline actions.\nOnce the files are tracked by one of the tools they should only show up under the appropriate tree.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1662022916688,
        "Answer_score":3.0,
        "Answerer_id":19894323.0,
        "Answerer_created_time":1662022756172,
        "Answerer_last_access_time":1663556599936,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":46.0,
        "Answerer_views":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73565648",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_closed_time":1662022756172,
        "Question_original_content":"show file track sourc control visual studio code extens vscode insid python project problem show file track sourc control panel follow pictur track data folder src folder fix encount problem",
        "Question_preprocessed_content":"show file track sourc control visual studio code extens vscode insid python project problem show file track sourc control panel follow pictur track data folder src folder fix encount problem",
        "Question_gpt_summary_original":"The user is facing a challenge with the DVC extension in VScode where it shows files not tracked by DVC in the source control panel. The user mentions that DVC only tracks the data folder and not the src folder and is seeking a solution to fix this issue.",
        "Question_gpt_summary":"user face challeng extens vscode show file track sourc control panel user mention track data folder src folder seek solut fix issu",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"several dependency errors causing in Azure AutoML while running model",
        "Question_body":"<p>I am trying to work on different models on a small piece of ML project which needs to work on azure platform and get the score.py with all the values. It is getting not a single library issue, but getting multiple <strong>Module errors<\/strong> and <strong>Attribute errors<\/strong>. I am using latest SDK version only, but I am not sure, where I am going side path.<\/p>\n<p>Any previous observations on this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660210046063,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":29.0,
        "Poster_id":18968829.0,
        "Poster_created_time":1651094469216,
        "Poster_last_access_time":1660762207472,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":41.0,
        "Poster_views":30.0,
        "Answer_body":"<p>The compatibility break is there for the newer version of the packages based on the current version of <strong>SDK<\/strong>. If the current SDK version is <strong>1.13.0<\/strong> and above, previous versions of packages are not in working stage. The compatibility issue is raising because of support of packages from SDK for different versions. It differs from version-to-version package support from <strong>SDK<\/strong>.<\/p>\n<p>Because of this we are getting Module not found,  <code>ImportError and AttributeError<\/code>.<\/p>\n<p>This solution depends on the AutoML SDK training version.<\/p>\n<ul>\n<li>If you are using 1.13.0 above version of SDK, update the versions of pandas to 0.25.1 and scikit-learn to 0.22.1<\/li>\n<\/ul>\n<p>Using the following command in  <code>BASH<\/code>  to upgrade the versions.<\/p>\n<pre><code>pip install \u2013upgrade pandas==0.25.1\n\npip install \u2013upgrade sickit-learn==0.22.1\n\n<\/code><\/pre>\n<p>The generic syntax for upgrading is:<\/p>\n<pre><code>pip install \u2013upgrade package_name==version\n\n<\/code><\/pre>\n<ul>\n<li>If the error occurs in AutoML Configuration file, then need to upgrade that also.<\/li>\n<li>But it is suggestable to uninstall and reinstall  <code>AutoMLConfig<\/code>.<\/li>\n<\/ul>\n<pre><code>pip uninstall azureml-train automl\n\n<\/code><\/pre>\n<p>Then reinstall using the below code,<\/p>\n<pre><code>pip install azureml-train automl\n\n<\/code><\/pre>\n<p>If you are using windows operating system, then install  <a href=\"https:\/\/docs.conda.io\/en\/latest\/miniconda.html\" rel=\"nofollow noreferrer\">Miniconda<\/a>.<\/p>\n<p>If you are a linux user, then using sudo or conda syntaxes for the same operation.<\/p>\n<p>Some of the advanced libraries of computer vision supportive like TensorFlow will be installed by default. Then we need to install them from dependencies.<\/p>\n<blockquote>\n<pre><code>azureml.core.runconfig import RunConfiguration from\nazureml.core.conda_dependencies import CondaDependencies run_config =\nRunConfiguration() run_config.environment.python.conda_dependencies =\nCondaDependencies.create(conda_packages=['tensorflow==1.12.0']) \n\n<\/code><\/pre>\n<\/blockquote>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-auto-ml#tensorflow\" rel=\"nofollow noreferrer\">Documentation<\/a>  credit to @Larry Franks.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":18428148.0,
        "Answerer_created_time":1646907459852,
        "Answerer_last_access_time":1664066617772,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":1624.0,
        "Answerer_views":1376.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73318372",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1660213117328,
        "Question_original_content":"depend error caus azur automl run model try work differ model small piec project need work azur platform score valu get singl librari issu get multipl modul error attribut error latest sdk version sure go path previou observ error",
        "Question_preprocessed_content":"depend error caus azur automl run model try work differ model small piec project need work azur platform valu get singl librari issu get multipl modul error attribut error latest sdk version sure go path previou observ error",
        "Question_gpt_summary_original":"The user is encountering multiple module errors and attribute errors while trying to work on different models on a small ML project on the Azure platform. The user is unsure of where they are going wrong despite using the latest SDK version.",
        "Question_gpt_summary":"user encount multipl modul error attribut error try work differ model small project azur platform user unsur go wrong despit latest sdk version",
        "Answer_original_content":"compat break newer version packag base current version sdk current sdk version previou version packag work stage compat issu rais support packag sdk differ version differ version version packag support sdk get modul importerror attributeerror solut depend automl sdk train version version sdk updat version panda scikit learn follow command bash upgrad version pip instal upgrad panda pip instal upgrad sickit learn gener syntax upgrad pip instal upgrad packag version error occur automl configur file need upgrad suggest uninstal reinstal automlconfig pip uninstal train automl reinstal code pip instal train automl window oper instal miniconda linux user sudo conda syntax oper advanc librari vision support like tensorflow instal default need instal depend core runconfig import runconfigur core conda depend import condadepend run config runconfigur run config environ python conda depend condadepend creat conda packag tensorflow document credit larri frank",
        "Answer_preprocessed_content":"compat break newer version packag base current version sdk current sdk version previou version packag work stage compat issu rais support packag sdk differ version differ packag support sdk get modul solut depend automl sdk train version version sdk updat version panda follow command upgrad version gener syntax upgrad error occur automl configur file need upgrad suggest uninstal reinstal reinstal code window oper instal miniconda linux user sudo conda syntax oper advanc librari vision support like tensorflow instal default need instal depend document credit frank",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- If using SDK version 1.13.0 or above, update pandas to version 0.25.1 and scikit-learn to version 0.22.1 using the command \"pip install \u2013upgrade pandas==0.25.1\" and \"pip install \u2013upgrade sickit-learn==0.22.1\" respectively.\n- If the error occurs in AutoML Configuration file, upgrade it or uninstall and reinstall AutoMLConfig using the commands \"pip uninstall azureml-train automl\" and \"pip install azureml-train automl\".\n- Install Miniconda if using Windows operating system or use sudo or conda syntaxes if using Linux.\n- Install advanced libraries",
        "Answer_gpt_summary":"possibl solut mention discuss sdk version updat panda version scikit learn version command pip instal upgrad panda pip instal upgrad sickit learn respect error occur automl configur file upgrad uninstal reinstal automlconfig command pip uninstal train automl pip instal train automl instal miniconda window oper us sudo conda syntax linux instal advanc librari"
    },
    {
        "Question_title":"Beginners guide to Sagemaker",
        "Question_body":"<p>I have followed an Amazon tutorial for using SageMaker and have used it to create the model in the tutorial (<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a>).<\/p>\n\n<p>This is my first time using SageMaker, so my question may be stupid.<\/p>\n\n<p>How do you actually view the model that it has created? I want to be able to see a) the final formula created with the parameters etc. b) graphs of plotted factors etc. as if I was reviewing a GLM for example.<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1557237604320,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":748.0,
        "Poster_id":11313063.0,
        "Poster_created_time":1554397763220,
        "Poster_last_access_time":1602503856660,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":327.0,
        "Poster_views":54.0,
        "Answer_body":"<p>If you followed the SageMaker tutorial you must have trained an XGBoost model. SageMaker places the model artifacts in a bucket that you own, check the output S3 location in the AWS SageMaker console. <\/p>\n\n<p>For more information about XGBoost you can check the AWS SageMaker documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks<\/a> and the example notebooks, e.g. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb<\/a><\/p>\n\n<p>To consume the XGBoost artifact generated by SageMaker, check out the official documentation, which contains the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># SageMaker XGBoost uses the Python pickle module to serialize\/deserialize \n# the model, which can be used for saving\/loading the model.\n# To use a model trained with SageMaker XGBoost in open source XGBoost\n# Use the following Python code:\n\nimport pickle as pkl \nmodel = pkl.load(open(model_file_path, 'rb'))\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4110545.0,
        "Answerer_created_time":1412515367427,
        "Answerer_last_access_time":1610474428687,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":161.0,
        "Answerer_views":24.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56024351",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1559693969323,
        "Question_original_content":"beginn guid follow amazon tutori creat model tutori http aw amazon com get start tutori build train deploi machin learn model time question stupid actual view model creat want abl final formula creat paramet graph plot factor review glm exampl thank advanc",
        "Question_preprocessed_content":"beginn guid follow amazon tutori creat model tutori time question stupid actual view model creat want abl final formula creat paramet graph plot factor review glm exampl thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in viewing the model created using SageMaker. They are unsure of how to view the final formula and graphs of plotted factors.",
        "Question_gpt_summary":"user face challeng view model creat unsur view final formula graph plot factor",
        "Answer_original_content":"follow tutori train xgboost model place model artifact bucket check output locat consol inform xgboost check document http doc aw amazon com latest xgboost html xgboost sampl notebook exampl notebook http github com awslab amazon exampl blob master introduct amazon algorithm xgboost abalon xgboost abalon ipynb consum xgboost artifact gener check offici document contain follow code xgboost us python pickl modul serial deseri model save load model us model train xgboost open sourc xgboost us follow python code import pickl pkl model pkl load open model file path predict test data pred model predict dtest",
        "Answer_preprocessed_content":"follow tutori train xgboost model place model artifact bucket check output locat consol inform xgboost check document exampl notebook consum xgboost artifact gener check offici document contain follow code",
        "Answer_gpt_summary_original":"The discussion provides a solution to the challenge. The user can check the output S3 location in the AWS SageMaker console to find the model artifacts. They can also refer to the AWS SageMaker documentation and example notebooks for more information about XGBoost. To consume the XGBoost artifact generated by SageMaker, the user can use the Python pickle module to serialize\/deserialize the model.",
        "Answer_gpt_summary":"discuss provid solut challeng user check output locat consol model artifact refer document exampl notebook inform xgboost consum xgboost artifact gener user us python pickl modul serial deseri model"
    },
    {
        "Question_title":"AKS an ACI Deployment with blob mount",
        "Question_body":"<p>I have a use case wherein I need to refer to the input dataset in the ACI\/AKS which is in a blob (same used for training model). I'm not able to find related resources in the Microsoft official documentation. If anyone suggests to me how to do it, that will be very helpful.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1626172285177,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":57.0,
        "Poster_id":5627023.0,
        "Poster_created_time":1448994884167,
        "Poster_last_access_time":1663853814027,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":265.0,
        "Poster_views":156.0,
        "Answer_body":"<p>It will be supported in the near future, Running Python scripts on Azure with Azure Container Instances to connect the blob.\n<a href=\"https:\/\/kohera.be\/tutorials-2\/running-python-scripts-on-azure-with-azure-container-instances\/\" rel=\"nofollow noreferrer\">https:\/\/kohera.be\/tutorials-2\/running-python-scripts-on-azure-with-azure-container-instances\/<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":11297406.0,
        "Answerer_created_time":1554186784008,
        "Answerer_last_access_time":1662988357176,
        "Answerer_age":null,
        "Answerer_location":"Hyderabad, Telangana, India",
        "Answerer_reputation":2175.0,
        "Answerer_views":434.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68360738",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1627304144467,
        "Question_original_content":"ak aci deploy blob mount us case need refer input dataset aci ak blob train model abl relat resourc microsoft offici document suggest help",
        "Question_preprocessed_content":"ak aci deploy blob mount us case need refer input dataset blob abl relat resourc microsoft offici document suggest help",
        "Question_gpt_summary_original":"The user is facing challenges in referring to an input dataset in ACI\/AKS that is stored in a blob, and is unable to find relevant resources in Microsoft's official documentation. They are seeking suggestions on how to resolve this issue.",
        "Question_gpt_summary":"user face challeng refer input dataset aci ak store blob unabl relev resourc microsoft offici document seek suggest resolv issu",
        "Answer_original_content":"support near futur run python script azur azur contain instanc connect blob http kohera tutori run python script azur azur contain instanc",
        "Answer_preprocessed_content":"support near futur run python script azur azur contain instanc connect blob",
        "Answer_gpt_summary_original":"Solution: The user can refer to the link provided in the discussion which suggests that running Python scripts on Azure with Azure Container Instances can help connect the blob in the near future.",
        "Answer_gpt_summary":"solut user refer link provid discuss suggest run python script azur azur contain instanc help connect blob near futur"
    },
    {
        "Question_title":"MLflow stores artifacts on GCP buckets but is not able to read them",
        "Question_body":"<p>I've found an almost identical question <a href=\"https:\/\/stackoverflow.com\/questions\/63727235\/mlflow-artifacts-storing-artifactsgoogle-cloud-storage-but-not-displaying-them?newreg=923da08a362547daab64c7d7e2275423\">here<\/a> but don't have enough reputation to add comments so will ask again hoping that someone has found a solution in the mean time.<\/p>\n<p>I am using MLflow (1.13.1) to track model performance and GCP Storage to store model artifacts.\nMLflow is running on a GCP VM instance and my python application uses a service account with Storage Object Creator and Storage Object Viewer roles (and then I've also added storage.buckets.get permissions) to store artifacts in GCP buckets and read from them.\nEverything is working as expected with parameters and metrics correctly displaying in MLflow UI and model artifacts correctly stored in buckets. The problem is that the model artifacts do not show up in MLflow UI because of this error:<\/p>\n<pre><code>Unable to list artifacts stored under gs:\/******\/artifacts for the current run. \nPlease contact your tracking server administrator to notify them of this error, \nwhich can happen when the tracking server lacks permission to list artifacts under the current run's root artifact directory.\n<\/code><\/pre>\n<p>The quoted artifacts location exists and contains the correct model artifacts, and MLflow should be able to read the artifacts because of the Storage Object Viewer role and the storage.buckets.get permissions.<\/p>\n<p>Any suggestion on what could be wrong? Thank you.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1611843895217,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":428.0,
        "Poster_id":15099357.0,
        "Poster_created_time":1611841996688,
        "Poster_last_access_time":1637883583270,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":21.0,
        "Poster_views":1.0,
        "Answer_body":"<p>I've found the problem just after posting the question.\nI had forgotten to install the <code>google-cloud-storage<\/code> library on the GCP VM. Everything works as expected now.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":15099357.0,
        "Answerer_created_time":1611841996688,
        "Answerer_last_access_time":1637883583270,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":21.0,
        "Answerer_views":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65939058",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Question_closed_time":1611845294603,
        "Question_original_content":"store artifact gcp bucket abl read ident question reput add comment ask hope solut mean time track model perform gcp storag store model artifact run gcp instanc python applic us servic account storag object creator storag object viewer role ad storag bucket permiss store artifact gcp bucket read work expect paramet metric correctli displai model artifact correctli store bucket problem model artifact error unabl list artifact store artifact current run contact track server administr notifi error happen track server lack permiss list artifact current run root artifact directori quot artifact locat exist contain correct model artifact abl read artifact storag object viewer role storag bucket permiss suggest wrong thank",
        "Question_preprocessed_content":"store artifact gcp bucket abl read ident question reput add comment ask hope solut mean time track model perform gcp storag store model artifact run gcp instanc python applic us servic account storag object creator storag object viewer role store artifact gcp bucket read work expect paramet metric correctli displai model artifact correctli store bucket problem model artifact error quot artifact locat exist contain correct model artifact abl read artifact storag object viewer role permiss suggest wrong thank",
        "Question_gpt_summary_original":"The user is facing a challenge with MLflow where it is not able to read model artifacts stored on GCP buckets, despite the correct permissions being granted to the service account used. The artifacts are correctly stored in the buckets and parameters and metrics are displaying correctly in the MLflow UI. The error message suggests that the tracking server lacks permission to list artifacts under the current run's root artifact directory. The user is seeking suggestions on what could be wrong.",
        "Question_gpt_summary":"user face challeng abl read model artifact store gcp bucket despit correct permiss grant servic account artifact correctli store bucket paramet metric displai correctli error messag suggest track server lack permiss list artifact current run root artifact directori user seek suggest wrong",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"Azure ML Python with Script Bundle cannot import module",
        "Question_body":"<p>In Azure ML, I'm trying to execute a Python module that needs to import the module pyxdameraulevenshtein (<a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>).<\/p>\n\n<p>I followed the usual way, which is to create a zip file and then import it; however for this specific module, it seems to never be able to find it. The error message is as usual:<\/p>\n\n<p><em>ImportError: No module named 'pyxdameraulevenshtein'<\/em><\/p>\n\n<p>Has anyone included this pyxdameraulevenshtein module in Azure ML with success ?<\/p>\n\n<p>(I took the package from <a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>.)<\/p>\n\n<p>Thanks for any help you can provide,<\/p>\n\n<p>PH<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1496235911927,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1496278915287,
        "Question_score":2,
        "Question_view_count":2395.0,
        "Poster_id":8091965.0,
        "Poster_created_time":1496233557192,
        "Poster_last_access_time":1662040842996,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":43.0,
        "Poster_views":5.0,
        "Answer_body":"<p>I viewed the <code>pyxdameraulevenshtein<\/code> module page, there are two packages you can download which include a wheel file for MacOS and a source code tar file. I don't think you can directly use the both on Azure ML, because the MacOS one is just a share library <code>.so<\/code> file for darwin which is not compatible with Azure ML, and the other you need to first compile it.<\/p>\n\n<p>So my suggestion is as below for using <code>pyxdameraulevenshtein<\/code>.<\/p>\n\n<ol>\n<li>First, compile the source code of <code>pyxdameraulevenshtein<\/code> to a DLL file on Windows, please refer to the document for Python <a href=\"https:\/\/docs.python.org\/2\/extending\/windows.html\" rel=\"nofollow noreferrer\">2<\/a>\/<a href=\"https:\/\/docs.python.org\/3\/extending\/windows.html\" rel=\"nofollow noreferrer\">3<\/a> or search for doing this.<\/li>\n<li>Write a Python script using the DLL you compiled to implement your needs, please refer to the SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/252417\/how-can-i-use-a-dll-file-from-python\">How can I use a DLL file from Python?<\/a> for how to use DLL from Python and refer to the Azure offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts\" rel=\"nofollow noreferrer\">tutorial<\/a> to write your Python script<\/li>\n<li>Package your Python script and DLL file as a zip file, then to upload the zip file to use it in <code>Execute Python script<\/code> model of Azure ML.<\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":4989676.0,
        "Answerer_created_time":1433841188323,
        "Answerer_last_access_time":1662775294983,
        "Answerer_age":null,
        "Answerer_location":"Wuxi, Jiangsu, China",
        "Answerer_reputation":22467.0,
        "Answerer_views":2692.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44285641",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1496307870776,
        "Question_original_content":"python script bundl import modul try execut python modul need import modul pyxdameraulevenshtein http pypi python org pypi pyxdameraulevenshtein follow usual wai creat zip file import specif modul abl error messag usual importerror modul name pyxdameraulevenshtein includ pyxdameraulevenshtein modul success took packag http pypi python org pypi pyxdameraulevenshtein thank help provid",
        "Question_preprocessed_content":"python script bundl import modul try execut python modul need import modul pyxdameraulevenshtein follow usual wai creat zip file import specif modul abl error messag usual importerror modul name pyxdameraulevenshtein includ pyxdameraulevenshtein modul success took packag thank help provid",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to execute a Python module that needs to import the module pyxdameraulevenshtein in Azure ML. Despite following the usual way of creating a zip file and importing it, the module seems to never be found, resulting in an ImportError. The user is seeking help to include this module in Azure ML successfully.",
        "Question_gpt_summary":"user encount challeng try execut python modul need import modul pyxdameraulevenshtein despit follow usual wai creat zip file import modul result importerror user seek help includ modul successfulli",
        "Answer_original_content":"view pyxdameraulevenshtein modul page packag download includ wheel file maco sourc code tar file think directli us maco share librari file darwin compat need compil suggest pyxdameraulevenshtein compil sourc code pyxdameraulevenshtein dll file window refer document python search write python script dll compil implement need refer thread us dll file python us dll python refer azur offic tutori write python script packag python script dll file zip file upload zip file us execut python script model hope help",
        "Answer_preprocessed_content":"view modul page packag download includ wheel file maco sourc code tar file think directli us maco share librari file darwin compat need compil suggest compil sourc code dll file window refer document python search write python script dll compil implement need refer thread us dll file python us dll python refer azur offic tutori write python script packag python script dll file zip file upload zip file us model hope help",
        "Answer_gpt_summary_original":"Solution:\n- Compile the source code of pyxdameraulevenshtein to a DLL file on Windows.\n- Write a Python script using the DLL you compiled to implement your needs.\n- Package your Python script and DLL file as a zip file, then upload the zip file to use it in Execute Python script model of Azure ML.",
        "Answer_gpt_summary":"solut compil sourc code pyxdameraulevenshtein dll file window write python script dll compil implement need packag python script dll file zip file upload zip file us execut python script model"
    },
    {
        "Question_title":"How are date features utilized in Microsoft Azure Machine Studio",
        "Question_body":"<p>I'm using Microsoft Azure Machine Learning and was wondering if anyone had done some experiments on date time features. Doe sit automatically derive additional features like \"day of week\", \"day of month\", \"hour of day\" from them, or do I have to provide these?<\/p>\n\n<p>I could not find any info in the official documentation (and a lack of a Microsoft support forum =)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1434716113167,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1445833326870,
        "Question_score":0,
        "Question_view_count":806.0,
        "Poster_id":19935.0,
        "Poster_created_time":1221999894423,
        "Poster_last_access_time":1664035431203,
        "Poster_age":null,
        "Poster_location":"Delaware",
        "Poster_reputation":2603.0,
        "Poster_views":225.0,
        "Answer_body":"<p>Azure ML supports \"execute-R\" module which can be easily used to accomplish this in R - few examples below<\/p>\n\n<p>x&lt;-as.Date(\"12\/3\/2009\", \"%m\/%d\/%Y\")<\/p>\n\n<blockquote>\n  <p>months.Date(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"December\"<\/p>\n\n<blockquote>\n  <p>weekdays.Date(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"Thursday\"<\/p>\n\n<blockquote>\n  <p>quarters(x)<\/p>\n<\/blockquote>\n\n<p>[1] \"Q4\"<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":4589073.0,
        "Answerer_created_time":1424453610300,
        "Answerer_last_access_time":1505365713168,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":1237.0,
        "Answerer_views":116.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30937903",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1434736380420,
        "Question_original_content":"date featur util microsoft azur machin studio wonder experi date time featur doe sit automat deriv addit featur like dai week dai month hour dai provid info offici document lack microsoft support forum",
        "Question_preprocessed_content":"date featur util microsoft azur machin studio wonder experi date time featur doe sit automat deriv addit featur like dai week dai month hour dai provid info offici document",
        "Question_gpt_summary_original":"The user is facing challenges in utilizing date time features in Microsoft Azure Machine Learning. They are unsure if the platform automatically derives additional features like \"day of week\", \"day of month\", \"hour of day\" from the date or if they have to provide these manually. The user has been unable to find information in the official documentation and is also unable to find support on a Microsoft forum.",
        "Question_gpt_summary":"user face challeng util date time featur unsur platform automat deriv addit featur like dai week dai month hour dai date provid manual user unabl inform offici document unabl support microsoft forum",
        "Answer_original_content":"support execut modul easili accomplish exampl date month date decemb weekdai date thursdai quarter",
        "Answer_preprocessed_content":"support modul easili accomplish exampl decemb thursdai quarter",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user can use the \"execute-R\" module in Azure ML to derive additional features like \"day of week\", \"day of month\", \"hour of day\" from the date. The examples provided in R include using functions like \"months.Date\", \"weekdays.Date\", and \"quarters\" to extract the required features.",
        "Answer_gpt_summary":"solut discuss suggest user us execut modul deriv addit featur like dai week dai month hour dai date exampl provid includ function like month date weekdai date quarter extract requir featur"
    },
    {
        "Question_title":"Azure ML and ML.net which is better",
        "Question_body":"I am newly starting in machine learning field with basic training now.\nI am not sure about the difference and whichever should be used for beginner",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1653905877517,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@Joel-9367 Thanks for the question. Azure ML is a cloud service where you pay for the compute power that you \"burn\" whereas ML.NET is a Toolkit for . net that you can run anywhere. You don't pay anything for using ML.NET itself.\n\nAzure ML Empower data scientists and developers to build, deploy, and manage high-quality models faster and with confidence. Here is the document for Azure ML.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/869589\/azure-ml-and-mlnet-which-is-better.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1653921374096,
        "Question_original_content":"net better newli start machin learn field basic train sure differ whichev beginn",
        "Question_preprocessed_content":"better newli start machin learn field basic train sure differ whichev beginn",
        "Question_gpt_summary_original":"The user is a beginner in the field of machine learning and is unsure about the differences between Azure ML and ML.net. They are seeking guidance on which platform to use as a beginner.",
        "Question_gpt_summary":"user beginn field machin learn unsur differ net seek guidanc platform us beginn",
        "Answer_original_content":"joel thank question cloud servic pai comput power burn net toolkit net run pai net empow data scientist develop build deploi manag high qualiti model faster confid document",
        "Answer_preprocessed_content":"thank question cloud servic pai comput power burn toolkit net run pai empow data scientist develop build deploi manag model faster confid document",
        "Answer_gpt_summary_original":"Summary: The discussion provides a comparison between Azure ML and ML.net. It explains that Azure ML is a cloud service that requires payment for compute power, while ML.net is a free toolkit for .net that can be run anywhere. The discussion also provides a link to the documentation for Azure ML. No specific recommendation is given on which platform to use as a beginner.",
        "Answer_gpt_summary":"summari discuss provid comparison net explain cloud servic requir payment comput power net free toolkit net run discuss provid link document specif recommend given platform us beginn"
    },
    {
        "Question_title":"Pulling data from Stream Analytics to Azure Machine Learning",
        "Question_body":"<p>Working on a IoT telemetry project that receives humidity and weather pollution data from different sites on the field. I will then apply Machine Learning on the collected data. I'm using Event Hubs and Stream Analytics. Is there a way of pulling the data to Azure Machine Learning without the hassle of writing an application to get it from Stream Analytics and push to AML web service?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1467278958850,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":627.0,
        "Poster_id":850713.0,
        "Poster_created_time":1311017514580,
        "Poster_last_access_time":1664027428383,
        "Poster_age":null,
        "Poster_location":"Beirut, Lebanon",
        "Poster_reputation":408.0,
        "Poster_views":32.0,
        "Answer_body":"<p>Stream Analytics has a functionality called the \u201c<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">Functions<\/a>\u201d. You can call any web service you\u2019ve published using AML from within Stream Analytics and apply it within your Stream Analytics query. Check this <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link for a tutorial<\/a>.\nExample workflow in your case would be like the following;<\/p>\n\n<ul>\n<li>Telemetry arrives and reaches Stream Analytics<\/li>\n<li>Streaming Analytics (SA) calls the Machine Learning function to apply it on the data<\/li>\n<li>SA redirects it to the output accordingly, here you can use the PowerBI to create a predictions dashboards.<\/li>\n<\/ul>\n\n<p>Another way would be using R, and here\u2019s a good tutorial showing that <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/<\/a> . \nIt is more work of course but can give you more control as you control the code.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":694697.0,
        "Answerer_created_time":1302088276340,
        "Answerer_last_access_time":1659573127447,
        "Answerer_age":null,
        "Answerer_location":"Dubai, United Arab Emirates",
        "Answerer_reputation":5263.0,
        "Answerer_views":637.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38119062",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1467284456456,
        "Question_original_content":"pull data stream analyt work iot telemetri project receiv humid weather pollut data differ site field appli machin learn collect data event hub stream analyt wai pull data hassl write applic stream analyt push aml web servic",
        "Question_preprocessed_content":"pull data stream analyt work iot telemetri project receiv humid weather pollut data differ site field appli machin learn collect data event hub stream analyt wai pull data hassl write applic stream analyt push aml web servic",
        "Question_gpt_summary_original":"The user is facing a challenge in pulling data from Stream Analytics to Azure Machine Learning for an IoT telemetry project without having to write an application to get the data from Stream Analytics and push it to AML web service.",
        "Question_gpt_summary":"user face challeng pull data stream analyt iot telemetri project have write applic data stream analyt push aml web servic",
        "Answer_original_content":"stream analyt function call function web servic youv publish aml stream analyt appli stream analyt queri check link tutori exampl workflow case like follow telemetri arriv reach stream analyt stream analyt call machin learn function appli data redirect output accordingli us powerbi creat predict dashboard wai here good tutori show http blog technet microsoft com machinelearn azur avail function azur stream analyt work cours control control code",
        "Answer_preprocessed_content":"stream analyt function call function web servic youv publish aml stream analyt appli stream analyt queri check link tutori exampl workflow case like follow telemetri arriv reach stream analyt stream analyt call machin learn function appli data redirect output accordingli us powerbi creat predict dashboard wai here good tutori show work cours control control code",
        "Answer_gpt_summary_original":"Solution: The user can use the \"Functions\" functionality in Stream Analytics to call any web service published using AML and apply it within the Stream Analytics query. Another solution is to use R, which requires more work but provides more control over the code.",
        "Answer_gpt_summary":"solut user us function function stream analyt web servic publish aml appli stream analyt queri solut us requir work provid control code"
    },
    {
        "Question_title":"VBA And Azure Machine Learning Excel Add In",
        "Question_body":"Hi! I wanted to see if VBA and Azure Machine Learning Excel Add In can be connected to each other. Are there any way to code VBA (use VBA) for controlling or altering Azure Machine Learning Excel Add In? I have used Azure Machine Learning to rate candidate feedback as negative or positive, but it has like a 75 -80% success rate - there are still a good chunk of comments that are rated wrong. However, it is still an amazing tool that I want to use v- I was just wondering if I can increase the accuracy of it somehow by creating a VBA code that connects it to Azure Machine Learning where I can add words related to negative responses or vice versa for positive response to increase the accuracy.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1610149745593,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hi, we currently don't support VBA and Azure ML Excel add-in integration. You'll need to apply ML techniques for improving your model and re-deploy your model.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/224491\/vba-and-azure-machine-learning-excel-add-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1610161769647,
        "Question_original_content":"vba excel add want vba excel add connect wai code vba us vba control alter excel add rate candid feedback neg posit like success rate good chunk comment rate wrong amaz tool want us wonder increas accuraci creat vba code connect add word relat neg respons vice versa posit respons increas accuraci",
        "Question_preprocessed_content":"vba excel add want vba excel add connect wai code vba control alter excel add rate candid feedback neg posit like success rate good chunk comment rate wrong amaz tool want us wonder increas accuraci creat vba code connect add word relat neg respons vice versa posit respons increas accuraci",
        "Question_gpt_summary_original":"The user is facing challenges with the accuracy of Azure Machine Learning Excel Add In in rating candidate feedback as negative or positive, with a success rate of 75-80%. They are wondering if VBA can be used to control or alter the Add In to increase its accuracy by adding words related to negative or positive responses.",
        "Question_gpt_summary":"user face challeng accuraci excel add rate candid feedback neg posit success rate wonder vba control alter add increas accuraci ad word relat neg posit respons",
        "Answer_original_content":"current support vba excel add integr need appli techniqu improv model deploi model",
        "Answer_preprocessed_content":"current support vba excel integr need appli techniqu improv model model",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Run Sagemaker notebook instance and be able to close tab",
        "Question_body":"<p>I'm currently using Sagemaker notebook instance (not from Sagemaker Studio), and I want to run a notebook that is expected to take around 8 hours to finish. I want to leave it overnight, and see the output from each cell, the output is a combination of print statements and plots.<\/p>\n<p>Howevever, when I start running the notebook and make sure the initial cells run, I close the Jupyterlab tab in my browser, and some minutes after, I open it again to see how is it going, but the notebook is stopped.<\/p>\n<p>Is there any way where I can still use my notebook as it is, see the output from each cell (prints and plots) and do not have to keep the Jupyterlab tab open (turn my laptop off, etc)?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_created_time":1646922683383,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":1154.0,
        "Poster_id":12282834.0,
        "Poster_created_time":1572208191120,
        "Poster_last_access_time":1663847604676,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":139.0,
        "Poster_views":45.0,
        "Answer_body":"<p>Answering my own question.<\/p>\n<p>I ended up using Sagemaker Processing jobs for this. As initially suggested by the other answer. I found this library developed a few months ago: <a href=\"https:\/\/github.com\/aws-samples\/sagemaker-run-notebook\" rel=\"nofollow noreferrer\">Sagemaker run notebook<\/a>, which helped still keep my notebook structure and cells as I had them, and be able to run it using Sagemaker run notebook using a bigger instance, and modifying the notebook in a smaller one.<\/p>\n<p>The output of each cell was saved, along the plots I had, in S3 as a jupyter notebook.<\/p>\n<p>I see that no constant support is given to the library, but you can fork it and make changes to it, and use it as per your requirements. For example, creating a docker container based on your needs.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":12282834.0,
        "Answerer_created_time":1572208191120,
        "Answerer_last_access_time":1663847604676,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":139.0,
        "Answerer_views":45.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71425842",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1662537228663,
        "Question_original_content":"run notebook instanc abl close tab current notebook instanc studio want run notebook expect hour finish want leav overnight output cell output combin print statement plot howevev start run notebook sure initi cell run close jupyterlab tab browser minut open go notebook stop wai us notebook output cell print plot jupyterlab tab open turn laptop",
        "Question_preprocessed_content":"run notebook instanc abl close tab current notebook instanc want run notebook expect hour finish want leav overnight output cell output combin print statement plot howevev start run notebook sure initi cell run close jupyterlab tab browser minut open go notebook stop wai us notebook output cell jupyterlab tab open",
        "Question_gpt_summary_original":"The user is facing challenges in running a Sagemaker notebook instance and leaving it to run overnight. The user wants to be able to see the output from each cell, which includes print statements and plots, without having to keep the Jupyterlab tab open. However, when the user closes the tab and opens it again later, the notebook is stopped. The user is seeking a solution to be able to use the notebook as intended without having to keep the tab open.",
        "Question_gpt_summary":"user face challeng run notebook instanc leav run overnight user want abl output cell includ print statement plot have jupyterlab tab open user close tab open later notebook stop user seek solut abl us notebook intend have tab open",
        "Answer_original_content":"answer question end process job initi suggest answer librari develop month ago run notebook help notebook structur cell abl run run notebook bigger instanc modifi notebook smaller output cell save plot jupyt notebook constant support given librari fork chang us requir exampl creat docker contain base need",
        "Answer_preprocessed_content":"answer question end process job initi suggest answer librari develop month ago run notebook help notebook structur cell abl run run notebook bigger instanc modifi notebook smaller output cell save plot jupyt notebook constant support given librari fork chang us requir exampl creat docker contain base need",
        "Answer_gpt_summary_original":"Solution: The user found a solution by using Sagemaker Processing jobs and a library called Sagemaker run notebook. The library helped the user to keep the notebook structure and cells as they were and run it using a bigger instance, and modifying the notebook in a smaller one. The output of each cell was saved, along with the plots, in S3 as a Jupyter notebook. The user mentioned that no constant support is given to the library, but it can be forked and modified based on the user's requirements, such as creating a docker container based on their needs.",
        "Answer_gpt_summary":"solut user solut process job librari call run notebook librari help user notebook structur cell run bigger instanc modifi notebook smaller output cell save plot jupyt notebook user mention constant support given librari fork modifi base user requir creat docker contain base need"
    },
    {
        "Question_title":"What is the best practice on folder structure for Azure Machine Learning service (preview) projects",
        "Question_body":"<p>I'm very excited on the newly released Azure Machine Learning service (preview), which is a great step up from the previous (and deprecated) Machine Learning Workbench.<\/p>\n\n<p>However, I am thinking a lot about the best practice on structuring the folders and files in my project(s). I'll try to explain my thoughts.<\/p>\n\n<p>Looking at the documentation for the training of a model (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#create-an-estimator\" rel=\"nofollow noreferrer\">Tutorial #1<\/a>), there seems to be good-practice to put all training scripts and necessary additional scripts inside a subfolder, so that it can be passed into the <code>Estimator<\/code> object without also passing all other files in the project. This is fine.<\/p>\n\n<p>But when working with the deployment of the service, specifically the deployment of the image, the documentation (e.g. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml#deploy-in-aci\" rel=\"nofollow noreferrer\">Tutorial #2<\/a>) seems to indicate that the scoring script need to be located in the root folder. If I try to refer to a script located in a subfolder, I get an error message saying<\/p>\n\n<p><code>WebserviceException: Unable to use a driver file not in current directory. Please navigate to the location of the driver file and try again.<\/code><\/p>\n\n<p>This may not be a big deal. Except, I have some additional scripts that I import both in the training script and in the scoring script, and I don't want to duplicate those additional scripts to be able to import them in both the training and the scoring scripts.<\/p>\n\n<p>I am working mainly in Jupyter Notebooks when executing the training and the deployment, and I could of course use some tricks to read the particular scripts from some other folder, save them to disk as a copy, execute the training or deployment while referring to the copies and finally delete the copies. This would be a decent workaround, but it seems to me that there should be a better way than just decent.<\/p>\n\n<p>What do you think?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1539614031870,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":2,
        "Question_view_count":782.0,
        "Poster_id":6361775.0,
        "Poster_created_time":1463756509236,
        "Poster_last_access_time":1662389533070,
        "Poster_age":null,
        "Poster_location":"Uppsala, Sverige",
        "Poster_reputation":400.0,
        "Poster_views":43.0,
        "Answer_body":"<p>Currently, the score.py needs to be in current working directory, but dependency scripts - the <em>dependencies<\/em> argument to  <em>ContainerImage.image_configuration<\/em> - can be in a subfolder.<\/p>\n\n<p>Therefore, you should be able to use folder structure like this:<\/p>\n\n<pre><code>.\/score.py \n.\/myscripts\/train.py \n.\/myscripts\/common.py\n<\/code><\/pre>\n\n<p>Note that the relative folder structure is preserved during web service deployment; if you reference the common file in subfolder from your score.py, that reference should be valid within deployed image.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":5784983.0,
        "Answerer_created_time":1452696930640,
        "Answerer_last_access_time":1603992113372,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":746.0,
        "Answerer_views":112.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52819122",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1540309291403,
        "Question_original_content":"best practic folder structur servic preview project excit newli releas servic preview great step previou deprec machin learn workbench think lot best practic structur folder file project try explain thought look document train model tutori good practic train script necessari addit script insid subfold pass estim object pass file project fine work deploy servic specif deploy imag document tutori indic score script need locat root folder try refer script locat subfold error messag sai webserviceexcept unabl us driver file current directori navig locat driver file try big deal addit script import train script score script want duplic addit script abl import train score script work mainli jupyt notebook execut train deploy cours us trick read particular script folder save disk copi execut train deploy refer copi final delet copi decent workaround better wai decent think",
        "Question_preprocessed_content":"best practic folder structur servic project excit newli releas servic great step previou machin learn workbench think lot best practic structur folder file project try explain thought look document train model train script necessari addit script insid subfold pass object pass file project fine work deploy servic specif deploy imag document indic score script need locat root folder try refer script locat subfold error messag sai big deal addit script import train script score script want duplic addit script abl import train score script work mainli jupyt notebook execut train deploy cours us trick read particular script folder save disk copi execut train deploy refer copi final delet copi decent workaround better wai decent think",
        "Question_gpt_summary_original":"The user is facing challenges in structuring the folders and files in their Azure Machine Learning service (preview) project. While the documentation suggests putting all training scripts and necessary additional scripts inside a subfolder, the scoring script needs to be located in the root folder for deployment. This creates a problem for the user as they have additional scripts that they import in both the training and scoring scripts and do not want to duplicate them. The user is looking for a better way to handle this issue.",
        "Question_gpt_summary":"user face challeng structur folder file servic preview project document suggest put train script necessari addit script insid subfold score script need locat root folder deploy creat problem user addit script import train score script want duplic user look better wai handl issu",
        "Answer_original_content":"current score need current work directori depend script depend argument containerimag imag configur subfold abl us folder structur like score myscript train myscript common note rel folder structur preserv web servic deploy refer common file subfold score refer valid deploi imag",
        "Answer_preprocessed_content":"current need current work directori depend script depend argument subfold abl us folder structur like note rel folder structur preserv web servic deploy refer common file subfold refer valid deploi imag",
        "Answer_gpt_summary_original":"Solution: The user can use a folder structure where the score.py file is located in the root folder and the training scripts and necessary additional scripts are located in a subfolder. The dependency scripts can also be located in a subfolder. The relative folder structure is preserved during web service deployment, so any references to the common file in the subfolder from the score.py file should be valid within the deployed image.",
        "Answer_gpt_summary":"solut user us folder structur score file locat root folder train script necessari addit script locat subfold depend script locat subfold rel folder structur preserv web servic deploy refer common file subfold score file valid deploi imag"
    },
    {
        "Question_title":"Pipeline can't find nodes in kedro",
        "Question_body":"<p>I was following <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html\" rel=\"nofollow noreferrer\">pipelines tutorial<\/a>, create all needed files, started the kedro with <code>kedro run --node=preprocessing_data<\/code> but got stuck with such error message:<\/p>\n\n<pre><code>ValueError: Pipeline does not contain nodes named ['preprocessing_data'].\n<\/code><\/pre>\n\n<p>If I run kedro without <code>node<\/code> parameter, I receive<\/p>\n\n<pre><code>kedro.context.context.KedroContextError: Pipeline contains no nodes\n<\/code><\/pre>\n\n<p>Contents of the files:<\/p>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/nodes.py\ndef preprocess_data(data: SparkDataSet) -&gt; None:\n    print(data)\n    return\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipelines\/data_engineering\/pipeline.py\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                func=preprocess_data,\n                inputs=\"data\",\n                outputs=\"preprocessed_data\",\n                name=\"preprocessing_data\",\n            ),\n        ]\n    )\n<\/code><\/pre>\n\n<pre><code>src\/project\/pipeline.py\ndef create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": de_pipeline,\n        \"__default__\": Pipeline([])\n    }\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1582395101757,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1583175186660,
        "Question_score":4,
        "Question_view_count":2155.0,
        "Poster_id":1110044.0,
        "Poster_created_time":1324477592580,
        "Poster_last_access_time":1663840044463,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":1315.0,
        "Poster_views":91.0,
        "Answer_body":"<p>I think it looks like you need to have the pipeline in <code>__default__<\/code>.\ne.g.<\/p>\n\n<pre><code>def create_pipelines(**kwargs) -&gt; Dict[str, Pipeline]:\n    de_pipeline = de.create_pipeline()\n    return {\n        \"de\": data_engineering_pipeline,\n        \"__default__\": data_engineering_pipeline\n    }\n<\/code><\/pre>\n\n<p>Then <code>kedro run --node=preprocessing_data<\/code> works for me.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Answerer_id":3240812.0,
        "Answerer_created_time":1390832932187,
        "Answerer_last_access_time":1652905558030,
        "Answerer_age":null,
        "Answerer_location":"Boston, MA, USA",
        "Answerer_reputation":249.0,
        "Answerer_views":45.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60355240",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1582427680590,
        "Question_original_content":"pipelin node follow pipelin tutori creat need file start run node preprocess data got stuck error messag valueerror pipelin contain node name preprocess data run node paramet receiv context context contexterror pipelin contain node content file src project pipelin data engin node def preprocess data data sparkdataset print data return src project pipelin data engin pipelin def creat pipelin kwarg return pipelin node func preprocess data input data output preprocess data preprocess data src project pipelin def creat pipelin kwarg dict str pipelin pipelin creat pipelin return pipelin default pipelin",
        "Question_preprocessed_content":"pipelin node follow pipelin tutori creat need file start got stuck error messag run paramet receiv content file",
        "Question_gpt_summary_original":"The user encountered an error while following the pipelines tutorial in Kedro. They created all the necessary files and started Kedro with the node parameter, but received an error message stating that the pipeline does not contain nodes named 'preprocessing_data'. Running Kedro without the node parameter resulted in an error message stating that the pipeline contains no nodes. The user provided the contents of the relevant files, including the function to preprocess data, the pipeline creation function, and the function to create pipelines.",
        "Question_gpt_summary":"user encount error follow pipelin tutori creat necessari file start node paramet receiv error messag state pipelin contain node name preprocess data run node paramet result error messag state pipelin contain node user provid content relev file includ function preprocess data pipelin creation function function creat pipelin",
        "Answer_original_content":"think look like need pipelin default def creat pipelin kwarg dict str pipelin pipelin creat pipelin return data engin pipelin default data engin pipelin run node preprocess data work",
        "Answer_preprocessed_content":"think look like need pipelin work",
        "Answer_gpt_summary_original":"Solution: The user can try adding the pipeline to the `__default__` dictionary in the `create_pipelines` function. This can be done by modifying the function to include the following line of code: `\"__default__\": data_engineering_pipeline`. After making this change, running Kedro with the node parameter should work.",
        "Answer_gpt_summary":"solut user try ad pipelin default dictionari creat pipelin function modifi function includ follow line code default data engin pipelin make chang run node paramet work"
    },
    {
        "Question_title":"401 Errors Calling the Microsoft Luis.ai Programmatic API",
        "Question_body":"<h2><strong>ASKING THIS HERE AT THE EXPLICIT REQUEST OF THE MICROSOFT AZURE SUPPORT TEAM.<\/strong><\/h2>\n\n<p>I've been attempting to call the MS Luis.ai <em>programmatic<\/em> API (bit.ly\/2iev01n) and have been receiving a 401 unauthorized response to every request. Here's a simple GET example: <code>https:\/\/api.projectoxford.ai\/luis\/v1.0\/prog\/apps\/{appId}\/entities?subscription-key={subscription_key}<\/code>.  <\/p>\n\n<p>I am providing my appId from the Luis.ai GUI (as specified by the API docs), here:<br>\n<img src=\"https:\/\/i.stack.imgur.com\/Cg2Fw.png\" alt=\"Luis.ai App Settings App Id\"><\/p>\n\n<p>I am providing my subscription key from Azure (as specified by the API docs), here:<br>\n<img src=\"https:\/\/i.stack.imgur.com\/GS2Fe.png\" alt=\"Azure Console\"><\/p>\n\n<p>The app ID and subscription key, sourced from above, are the exact same as what I'm using to hit the query API successfully (see note at bottom). My account is pay-as-you-go (not free).<\/p>\n\n<p><strong><em>Am I doing something wrong here? Is this API deprecated, moved, down, or out-of-sync with the docs?<\/em><\/strong><\/p>\n\n<p><strong>NOTE:<\/strong> I can manipulate my model through the online GUI but that approach will be far too manual for our business needs where our model will need to be programmatically updated as new business entities come into existence.  <\/p>\n\n<p><strong>NOTE:<\/strong> The programmatic API is different from the query API which has this request URL, which is working fine for me:<br>\n<code>https:\/\/api.projectoxford.ai\/luis\/v2.0\/apps\/{appId}?subscription-key={subscription_key}&amp;verbose=true&amp;q={utterance}<\/code>  <\/p>\n\n<p><strong>NOTE:<\/strong> There doesn't seem to be a Luis.ai programmatic API for v2.0--which is why the URLs from the query and programmatic APIs have different versions.  <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_created_time":1484180085280,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1484193011887,
        "Question_score":4,
        "Question_view_count":1280.0,
        "Poster_id":1549998.0,
        "Poster_created_time":1343167997556,
        "Poster_last_access_time":1663979648103,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":191.0,
        "Poster_views":27.0,
        "Answer_body":"<p>Answering my own question here:<\/p>\n\n<p>I have found my LUIS.ai programmatic API key. It is found by:\nLUIS.ai dashboard -> username (upper-right) -> settings in dropdown -> Subscription Keys tab -> Programmatic API Key<\/p>\n\n<p>It was not immediately obvious since it's found nowhere else: not alongside any of the other key listings in cognitive services or the LUIS.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Answerer_id":1549998.0,
        "Answerer_created_time":1343167997556,
        "Answerer_last_access_time":1663979648103,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":191.0,
        "Answerer_views":27.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41603082",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1484669845332,
        "Question_original_content":"error call microsoft lui programmat api ask explicit request microsoft azur support team attempt lui programmat api bit ievn receiv unauthor respons request simpl exampl http api projectoxford lui prog app appid entiti subscript kei subscript kei provid appid lui gui specifi api doc provid subscript kei azur specifi api doc app subscript kei sourc exact hit queri api successfulli note account pai free wrong api deprec move sync doc note manipul model onlin gui approach far manual busi need model need programmat updat new busi entiti come exist note programmat api differ queri api request url work fine http api projectoxford lui app appid subscript kei subscript kei verbos true utter note lui programmat api url queri programmat api differ version",
        "Question_preprocessed_content":"error call microsoft programmat api ask explicit request microsoft azur support team attempt programmat api receiv unauthor respons request simpl exampl provid appid gui provid subscript kei azur app subscript kei sourc exact hit queri api successfulli account wrong api deprec move doc note manipul model onlin gui approach far manual busi need model need programmat updat new busi entiti come exist note programmat api differ queri api request url work fine note programmat api url queri programmat api differ version",
        "Question_gpt_summary_original":"The user is encountering a challenge while calling the Microsoft Luis.ai programmatic API, as they are receiving a 401 unauthorized response to every request. They have provided the correct app ID and subscription key, which are the same as what they are using to hit the query API successfully. The user is unsure if they are doing something wrong or if the API is deprecated, moved, down, or out-of-sync with the docs. The user needs to programmatically update their model, but the online GUI approach is too manual for their business needs.",
        "Question_gpt_summary":"user encount challeng call microsoft lui programmat api receiv unauthor respons request provid correct app subscript kei hit queri api successfulli user unsur wrong api deprec move sync doc user need programmat updat model onlin gui approach manual busi need",
        "Answer_original_content":"answer question lui programmat api kei lui dashboard usernam upper right set dropdown subscript kei tab programmat api kei immedi obviou alongsid kei list cognit servic lui",
        "Answer_preprocessed_content":"answer question programmat api kei dashboard usernam set dropdown subscript kei tab programmat api kei immedi obviou alongsid kei list cognit servic lui",
        "Answer_gpt_summary_original":"Solution: The user found the LUIS.ai programmatic API key by going to the LUIS.ai dashboard, clicking on their username in the upper-right corner, selecting \"settings\" from the dropdown menu, and then navigating to the \"Subscription Keys\" tab and selecting the \"Programmatic API Key.\"",
        "Answer_gpt_summary":"solut user lui programmat api kei go lui dashboard click usernam upper right corner select set dropdown menu navig subscript kei tab select programmat api kei"
    },
    {
        "Question_title":"Sagemaker Asynchronous Endpoint Configuration",
        "Question_body":"We deployed a LighGBM Regression model and endpoint using Sagemaker Jumpstart. We have attempted to configure this endpoint as 'asynchronous' via the console. Receiving Error: ValidationException-Network Isolation is not supported when specifying an AsyncInferenceConfig.\n\nLooking at the model's network details the model has Enable Network Isolation set as 'True'. This was default output setting set by JumpStart.\n\nHow can we diasble Network Isolation to in order to make this endpoint asynchronous?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1653000488230,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":80.0,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Vanilla SageMaker \"Models\" (as opposed to versioned ModelPackages) are immutable in the API with no \"UpdateModel\" action... But I think you should be able to create a new Model copying the settings of the current one.\n\nI'd suggest to:\n\nUse DescribeModel (via boto3.client(\"sagemaker\").describe_model(), assuming you're using Python) to fetch all the parameters of the existing JumpStart model such as the S3 artifact location and other settings\nUse CreateModel (create_model()) to create a new model with same configuration but network isolation disabled\nUse your new model to try and deploy an async endpoint\n\nProbably you'd find the low-level boto3 SDK more intuitive for this task than the high-level sagemaker SDK's Model class - because the latter does some magic that makes typical build\/train\/deploy workflows easier but can be less natural for hacking around with existing model definitions. For example, creating an SMSDK Model object doesn't actually create a Model in the SageMaker API, because deployment instance type affects choice of container image so that gets deferred until a .deploy() call or similar later.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUZNbZZQHhSl2RYUtLU8zpSQ\/sagemaker-asynchronous-endpoint-configuration",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1653023938004,
        "Question_original_content":"asynchron endpoint configur deploi lighgbm regress model endpoint jumpstart attempt configur endpoint asynchron consol receiv error validationexcept network isol support specifi asyncinferenceconfig look model network detail model enabl network isol set true default output set set jumpstart diasbl network isol order endpoint asynchron",
        "Question_preprocessed_content":"asynchron endpoint configur deploi lighgbm regress model endpoint jumpstart attempt configur endpoint asynchron consol receiv error isol support specifi asyncinferenceconfig look model network detail model enabl network isol set true default output set set jumpstart diasbl network isol order endpoint asynchron",
        "Question_gpt_summary_original":"The user encountered a challenge in configuring their Sagemaker endpoint as 'asynchronous' via the console due to the error message \"ValidationException-Network Isolation is not supported when specifying an AsyncInferenceConfig.\" The model's default output setting, set by JumpStart, has Enable Network Isolation set as 'True', which prevents the endpoint from being configured as asynchronous. The user is seeking guidance on how to disable Network Isolation to make the endpoint asynchronous.",
        "Question_gpt_summary":"user encount challeng configur endpoint asynchron consol error messag validationexcept network isol support specifi asyncinferenceconfig model default output set set jumpstart enabl network isol set true prevent endpoint configur asynchron user seek guidanc disabl network isol endpoint asynchron",
        "Answer_original_content":"vanilla model oppos version modelpackag immut api updatemodel action think abl creat new model copi set current suggest us describemodel boto client model assum python fetch paramet exist jumpstart model artifact locat set us createmodel creat model creat new model configur network isol disabl us new model try deploi async endpoint probabl low level boto sdk intuit task high level sdk model class magic make typic build train deploi workflow easier natur hack exist model definit exampl creat smsdk model object actual creat model api deploy instanc type affect choic contain imag get defer deploi similar later",
        "Answer_preprocessed_content":"vanilla model immut api updatemodel think abl creat new model copi set current suggest us describemodel assum python fetch paramet exist jumpstart model artifact locat set us createmodel creat new model configur network isol disabl us new model try deploi async endpoint probabl boto sdk intuit task sdk model class magic make typic workflow easier natur hack exist model definit exampl creat smsdk model object actual creat model api deploy instanc type affect choic contain imag get defer deploi similar later",
        "Answer_gpt_summary_original":"Solution:\n\nThe user can create a new model with the same configuration as the existing JumpStart model but with network isolation disabled. This can be done by using the DescribeModel function to fetch all the parameters of the existing model and then using the CreateModel function to create a new model with the desired configuration. The low-level boto3 SDK is recommended for this task as it is more intuitive for hacking around with existing model definitions.",
        "Answer_gpt_summary":"solut user creat new model configur exist jumpstart model network isol disabl describemodel function fetch paramet exist model createmodel function creat new model desir configur low level boto sdk recommend task intuit hack exist model definit"
    },
    {
        "Question_title":"AWS Sagemaker inference endpoint not utilizing all vCPUs",
        "Question_body":"<p>I have deployed a custom model on sagemaker inference endpoint (single instance) and while I was load testing, I have observed that CPU utilization metric is maxing out at 100% but according to <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-cpu-gpu-utilization-100\/\" rel=\"nofollow noreferrer\">this post<\/a> it should max out at #vCPU*100 %. I have confirmed that the inference endpoint is not using all cores in clowdwatch logs.<\/p>\n<p>So if one prediction call requires one second to be processed to give response, the deployed model is only able to handle one API call per second which could have been increased to 8 calls per second if all vCPUs would have been used.<\/p>\n<p>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/p>\n<p>Or could we use multiprocessing python package inside <code>inference.py<\/code> file while deploying such that each call comes to the default core and from there all calculations\/prediction is done in any other core whichever is empty at that instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1624366085347,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1628443120180,
        "Question_score":3,
        "Question_view_count":661.0,
        "Poster_id":6834965.0,
        "Poster_created_time":1473938483227,
        "Poster_last_access_time":1664046020100,
        "Poster_age":null,
        "Poster_location":"https:\/\/dzone.com\/articles\/machine-learning-provides-360-degree-view-of-the-c",
        "Poster_reputation":909.0,
        "Poster_views":81.0,
        "Answer_body":"<p>UPDATE<\/p>\n<ul>\n<li><p>Set three environment variables<\/p>\n<ol>\n<li>ENABLE_MULTI_MODEL as &quot;true&quot; (make sure it is string and not bool) and set <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L74\" rel=\"nofollow noreferrer\">SAGEMAKER_HANDLER<\/a> as custom model handler python module path if custom service else dont define it. Also make sure model name <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L94\" rel=\"nofollow noreferrer\">model.mar<\/a>, before compressing it as tar ball and storing in s3<\/li>\n<li>TS_DEFAULT_WORKERS_PER_MODEL as number of vcpus<\/li>\n<li>First environment variable makes sure torch serve env_vars are enabled and second one uses first setting and loads requested number of workers<\/li>\n<li>Setting can be done by passing env dictionary argument to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">PyTorch function<\/a>. Below is explanation as to why it works<\/li>\n<\/ol>\n<\/li>\n<li><p>From the looks of it, sagemaker deployment for pytorch model as given in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">Sagemaker SDK guide<\/a>, uses <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu\" rel=\"nofollow noreferrer\">this dockerfile<\/a>. In this docker, entrypoint is <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> as in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu#L124\" rel=\"nofollow noreferrer\">Dockerfile line#124<\/a>.<\/p>\n<\/li>\n<li><p>This <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> calls <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">serving.main()<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py\" rel=\"nofollow noreferrer\">serving.py<\/a>. Which ends up calling <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py#L34\" rel=\"nofollow noreferrer\">torchserve.start_torchserve(handler_service=HANDLER_SERVICE)<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py\" rel=\"nofollow noreferrer\">torchserve.py<\/a>.<\/p>\n<\/li>\n<li><p><a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L34\" rel=\"nofollow noreferrer\">At line 34 in torchserve.py<\/a> it defines &quot;\/etc\/default-ts.properties&quot; as DEFAULT_TS_CONFIG_FILE. This file is located <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties\" rel=\"nofollow noreferrer\">here<\/a>. In this file <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties#L2\" rel=\"nofollow noreferrer\">enable_envvars_config=true<\/a> is set. It will use this file setting IFF Environment variable &quot;ENABLE_MULTI_MODEL&quot; is set to &quot;false&quot; as refered <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L167\" rel=\"nofollow noreferrer\">here<\/a>. If it is set to &quot;true&quot; then it will use \/etc\/mme-ts.properties<\/p>\n<\/li>\n<\/ul>\n<hr \/>\n<p>As for the question <code>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/code>\nThere are various settings you can use\nFor models you can set <code>default_workers_per_model<\/code> in config.properties <code>TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all)<\/code> in environment variables. Environment variables take top priority.<\/p>\n<p>Other than that for each model, you can set the number of workers by using management API, but sadly it is not possible to curl to management API in sagemaker. SO TS_DEFAULT_WORKERS_PER_MODEL is the best bet.\nSetting this should make sure all cores are used.<\/p>\n<p>But if you are using docker file then in entrypoint you can setup scripts which wait for model loading and curl to it to set number of workers<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># load the model\ncurl -X POST localhost:8081\/models?url=model_1.mar&amp;batch_size=8&amp;max_batch_delay=50\n# after loading the model it is possible to set min_worker, etc\ncurl -v -X PUT http:\/\/localhost:8081\/models\/model_1?min_worker=1\n<\/code><\/pre>\n<p>About the other issue that logs confirm that not all cores are used, I face the same issue and believe that is a problem in the logging system. Please look at this issue <a href=\"https:\/\/github.com\/pytorch\/serve\/issues\/782\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/serve\/issues\/782<\/a>. The community itself agrees that if threads are not set, then by default then it prints 0, even if by default it uses 2*num_cores.<\/p>\n<p><strong>For an exhaustive set of all configs possible<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Reference: https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\n# Variables that can be configured through config.properties and Environment Variables\n# NOTE: Variables which can be configured through environment variables **SHOULD** have a\n# &quot;TS_&quot; prefix\n# debug\ninference_address=http:\/\/0.0.0.0:8080\nmanagement_address=http:\/\/0.0.0.0:8081\nmetrics_address=http:\/\/0.0.0.0:8082\nmodel_store=\/opt\/ml\/model\nload_models=model_1.mar\n# blacklist_env_vars\n# default_workers_per_model\n# default_response_timeout\n# unregister_model_timeout\n# number_of_netty_threads\n# netty_client_threads\n# job_queue_size\n# number_of_gpu\n# async_logging\n# cors_allowed_origin\n# cors_allowed_methods\n# cors_allowed_headers\n# decode_input_request\n# keystore\n# keystore_pass\n# keystore_type\n# certificate_file\n# private_key_file\n# max_request_size\n# max_response_size\n# default_service_handler\n# service_envelope\n# model_server_home\n# snapshot_store\n# prefer_direct_buffer\n# allowed_urls\n# install_py_dep_per_model\n# metrics_format\n# enable_metrics_api\n# initial_worker_port\n\n# Configuration which are not documented or enabled through environment variables\n\n# When below variable is set true, then the variables set in environment have higher precedence.\n# For example, the value of an environment variable overrides both command line arguments and a property in the configuration file. The value of a command line argument overrides a value in the configuration file.\n# When set to false, environment variables are not used at all\n# use_native_io=\n# io_ratio=\n# metric_time_interval=\nenable_envvars_config=true\n# model_snapshot=\n# version=\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1624470584452,
        "Answer_score":5.0,
        "Answerer_id":7035448.0,
        "Answerer_created_time":1476780098123,
        "Answerer_last_access_time":1664047862552,
        "Answerer_age":null,
        "Answerer_location":"Tokyo, Japan",
        "Answerer_reputation":1672.0,
        "Answerer_views":159.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68083831",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1624416005592,
        "Question_original_content":"infer endpoint util vcpu deploi custom model infer endpoint singl instanc load test observ cpu util metric max accord post max vcpu confirm infer endpoint core clowdwatch log predict requir second process respons deploi model abl handl api second increas call second vcpu set deploy us vcpu increas concurr us multiprocess python packag insid infer file deploi come default core calcul predict core whichev instanc",
        "Question_preprocessed_content":"infer endpoint util vcpu deploi custom model infer endpoint load test observ cpu util metric max accord post max vcpu confirm infer endpoint core clowdwatch log predict requir second process respons deploi model abl handl api second increas call second vcpu set deploy us vcpu increas concurr us multiprocess python packag insid file deploi come default core core whichev instanc",
        "Question_gpt_summary_original":"The user has encountered a challenge with their AWS Sagemaker inference endpoint, as it is not utilizing all vCPUs. Despite load testing, the CPU utilization metric is maxing out at 100%, which is not in line with AWS's guidelines. As a result, the deployed model is only able to handle one API call per second, instead of eight. The user is seeking advice on whether there are any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency or if they could use the multiprocessing python package inside the inference.py file to distribute the workload across multiple cores.",
        "Question_gpt_summary":"user encount challeng infer endpoint util vcpu despit load test cpu util metric max line aw guidelin result deploi model abl handl api second instead user seek advic set deploy us vcpu increas concurr us multiprocess python packag insid infer file distribut workload multipl core",
        "Answer_original_content":"updat set environ variabl enabl multi model true sure string bool set handler custom model handler python modul path custom servic dont defin sure model model mar compress tar ball store default worker model number vcpu environ variabl make sure torch serv env var enabl second us set load request number worker set pass env dictionari argument pytorch function explan work look deploy pytorch model given sdk guid us dockerfil docker entrypoint torchserv entrypoint dockerfil line torchserv entrypoint call serv main serv end call torchserv start torchserv handler servic handler servic torchserv line torchserv defin default properti default config file file locat file enabl envvar config true set us file set iff environ variabl enabl multi model set fals refer set true us mme properti question set deploy us vcpu increas concurr set us model set default worker model config properti default worker model nproc environ variabl environ variabl prioriti model set number worker manag api sadli possibl curl manag api default worker model best bet set sure core docker file entrypoint setup script wait model load curl set number worker load model curl post localhost model url model mar batch size max batch delai load model possibl set min worker curl http localhost model model min worker issu log confirm core face issu believ problem log look issu http github com pytorch serv issu commun agre thread set default print default us num core exhaust set config possibl refer http github com pytorch serv blob master doc configur variabl configur config properti environ variabl note variabl configur environ variabl prefix debug infer address http manag address http metric address http model store opt model load model model mar blacklist env var default worker model default respons timeout unregist model timeout number netti thread netti client thread job queue size number gpu async log cor allow origin cor allow method cor allow header decod input request keystor keystor pass keystor type certif file privat kei file max request size max respons size default servic handler servic envelop model server home snapshot store prefer direct buffer allow url instal dep model metric format enabl metric api initi worker port configur document enabl environ variabl variabl set true variabl set environ higher preced exampl valu environ variabl overrid command line argument properti configur file valu command line argument overrid valu configur file set fals environ variabl us nativ ratio metric time interv enabl envvar config true model snapshot version",
        "Answer_preprocessed_content":"updat set environ variabl true set custom model handler python modul path custom servic dont defin sure model compress tar ball store number vcpu environ variabl make sure torch serv enabl second us set load request number worker set pass env dictionari argument pytorch function explan work look deploy pytorch model given sdk guid us dockerfil docker entrypoint dockerfil line call end call line defin file locat file set us file set iff environ variabl set fals refer set true us question set us model set environ variabl environ variabl prioriti model set number worker manag api sadli possibl curl manag api best bet set sure core docker file entrypoint setup script wait model load curl set number worker issu log confirm core face issu believ problem log look issu commun agre thread set default print default us exhaust set config possibl",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Set three environment variables: ENABLE_MULTI_MODEL as \"true\", TS_DEFAULT_WORKERS_PER_MODEL as the number of vCPUs, and SAGEMAKER_HANDLER as a custom model handler python module path if custom service else don't define it. This setting can be done by passing env dictionary argument to PyTorch function. \n\n2. For models, set default_workers_per_model in config.properties TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all) in environment variables. Other than that, for each model, set the number of workers by using management API, but it is not possible to curl to management API in Sagemaker. \n\n3. If using a docker file, in entrypoint",
        "Answer_gpt_summary":"possibl solut mention discuss set environ variabl enabl multi model true default worker model number vcpu handler custom model handler python modul path custom servic defin set pass env dictionari argument pytorch function model set default worker model config properti default worker model nproc environ variabl model set number worker manag api possibl curl manag api docker file entrypoint"
    },
    {
        "Question_title":"Updating tracked dir in DVC",
        "Question_body":"<p>According to <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">this tutorial<\/a> when I update file I should remove file from under DVC control first (i.e. execute <code>dvc unprotect &lt;myfile&gt;.dvc<\/code> or <code>dvc remove &lt;myfile&gt;.dvc<\/code>) and then add it again via <code>dvc add &lt;mifile&gt;<\/code>. However It's not clear if I should apply the same workflow for the directories.<\/p>\n\n<p>I have the directory under DVC control with the following structure:<\/p>\n\n<pre><code>data\/\n    1.jpg\n    2.jpg\n<\/code><\/pre>\n\n<p>Should I run <code>dvc unprotect data<\/code> every time the directory content is updated?<\/p>\n\n<p>More specifically I'm interested if I should run <code>dvc unprotect data<\/code> in the following use cases:<\/p>\n\n<ul>\n<li><strong>New file is added.<\/strong> For example if I put <code>3.jpg<\/code> image in the data dir<\/li>\n<li><strong>File is deleted.<\/strong> For example if I delete <code>2.jpg<\/code> image in the <code>data<\/code> dir<\/li>\n<li><strong>File is updated.<\/strong> For example if I edit <code>1.jpg<\/code> image via graphic editor.<\/li>\n<li>A combination of the previous use cases (i.e. some files are updated, other deleted and new files are added)<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1558667422490,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1558708772616,
        "Question_score":4,
        "Question_view_count":995.0,
        "Poster_id":9565342.0,
        "Poster_created_time":1522254698710,
        "Poster_last_access_time":1663927713687,
        "Poster_age":null,
        "Poster_location":"Russia",
        "Poster_reputation":784.0,
        "Poster_views":77.0,
        "Answer_body":"<p>Only when file is updated - i.e. edit <code>1.jpg<\/code> with your editor <strong>AND<\/strong> only if hadrlink or symlink cache type is enabled.<\/p>\n\n<p>Please, check this <a href=\"https:\/\/dvc.org\/doc\/user-guide\/update-tracked-file\" rel=\"nofollow noreferrer\">link<\/a>:<\/p>\n\n<blockquote>\n  <p>updating tracked files has to be carried out with caution to avoid data corruption when the DVC config option cache.type is set to hardlink or\/and symlink<\/p>\n<\/blockquote>\n\n<p>I would strongly recommend reading this document: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/cache-file-linking\" rel=\"nofollow noreferrer\">Performance Optimization for Large Files<\/a> it explains benefits of using hardlinks\/symlinks.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":298182.0,
        "Answerer_created_time":1250347954880,
        "Answerer_last_access_time":1664073072270,
        "Answerer_age":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation":5575.0,
        "Answerer_views":358.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56285351",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_closed_time":1558674266680,
        "Question_original_content":"updat track dir accord tutori updat file remov file control execut unprotect remov add add clear appli workflow directori directori control follow structur data jpg jpg run unprotect data time directori content updat specif interest run unprotect data follow us case new file ad exampl jpg imag data dir file delet exampl delet jpg imag data dir file updat exampl edit jpg imag graphic editor combin previou us case file updat delet new file ad",
        "Question_preprocessed_content":"updat track dir accord tutori updat file remov file control add clear appli workflow directori directori control follow structur run time directori content updat specif interest run follow us case new file ad exampl imag data dir file delet exampl delet imag dir file updat exampl edit imag graphic editor combin previou us case",
        "Question_gpt_summary_original":"The user is seeking clarification on whether they need to run \"dvc unprotect\" every time the content of a directory under DVC control is updated. They are specifically asking if they need to run it when a new file is added, a file is deleted, a file is updated, or a combination of these use cases.",
        "Question_gpt_summary":"user seek clarif need run unprotect time content directori control updat specif ask need run new file ad file delet file updat combin us case",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"How to delete web service in AzureML with mystery endpoint",
        "Question_body":"<p>I have an AzureML free account in South Central US. At some point I set up a web service, which I no longer need. I also suspect it's blocking my other web services as I'm getting 503 errors whenever I try to use them.<\/p>\n\n<p>When I try to delete the web service it gives the error message:\n<code>Cannot delete web service \"azuremlweb\" because one or more additional endpoints were created for it. These endpoints must be deleted before you can delete the web service.<\/code><\/p>\n\n<p>I didn't intentionally set up any extra endpoints and when trying to follow the instructions <a href=\"https:\/\/azure.microsoft.com\/en-gb\/documentation\/articles\/machine-learning-manage-workspace\/\" rel=\"nofollow noreferrer\">on this doc page<\/a> I couldn't see any endpoints listed that I can remove.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/n5Kg2.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/n5Kg2.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The only unusual thing I can think of about the service was that it had multiple inputs.<\/p>\n\n<p>I've more or less deleted everything in the workspace now: experiments, other web services, but it still won't go. Any thoughts?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1463389894770,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":1550.0,
        "Poster_id":2610267.0,
        "Poster_created_time":1374576589267,
        "Poster_last_access_time":1622156340507,
        "Poster_age":null,
        "Poster_location":"UK",
        "Poster_reputation":635.0,
        "Poster_views":48.0,
        "Answer_body":"<p>You can try to use Azure ML PowerShell module to discover and delete web service endpoints, and web service.<\/p>\n\n<p><a href=\"http:\/\/aka.ms\/amlps\" rel=\"nofollow\">http:\/\/aka.ms\/amlps<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":6191408.0,
        "Answerer_created_time":1460437080990,
        "Answerer_last_access_time":1566967857732,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":386.0,
        "Answerer_views":42.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37250368",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1463407717728,
        "Question_original_content":"delet web servic mysteri endpoint free account south central point set web servic longer need suspect block web servic get error try us try delet web servic give error messag delet web servic web addit endpoint creat endpoint delet delet web servic intention set extra endpoint try follow instruct doc page couldn endpoint list remov unusu thing think servic multipl input delet workspac experi web servic won thought",
        "Question_preprocessed_content":"delet web servic mysteri endpoint free account south central point set web servic longer need suspect block web servic get error try us try delet web servic give error messag intention set extra endpoint try follow instruct doc page couldn endpoint list remov unusu thing think servic multipl input delet workspac experi web servic won thought",
        "Question_gpt_summary_original":"The user is facing challenges in deleting a web service in AzureML, which is suspected to be blocking other web services and causing 503 errors. The error message \"Cannot delete web service 'azuremlweb' because one or more additional endpoints were created for it\" is displayed when trying to delete the web service. The user is unable to find any endpoints to remove and has already deleted other experiments and web services in the workspace.",
        "Question_gpt_summary":"user face challeng delet web servic suspect block web servic caus error error messag delet web servic web addit endpoint creat displai try delet web servic user unabl endpoint remov delet experi web servic workspac",
        "Answer_original_content":"try us powershel modul discov delet web servic endpoint web servic http aka amlp",
        "Answer_preprocessed_content":"try us powershel modul discov delet web servic endpoint web servic",
        "Answer_gpt_summary_original":"Solution: The user can try using the Azure ML PowerShell module to discover and delete web service endpoints and the web service. The module can be accessed through the link http:\/\/aka.ms\/amlps.",
        "Answer_gpt_summary":"solut user try powershel modul discov delet web servic endpoint web servic modul access link http aka amlp"
    },
    {
        "Question_title":"Notebook files have disaperred",
        "Question_body":"Hello,\n\nIn my MLStudio my notebook files window has disappeared so I can not access any of my data (as seen on the image) and I do not know what to do.\n\nPlease your help to solve this as soon as poosible.\n\nThank you.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1652875560357,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hello,\n\nThanks for reaching out to us. Could you please check the access of Storage? https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/assign-azure-role-data-access?tabs=portal#assign-an-azure-role\n\nTo access these storage services, you must have at least Storage Blob Data Reader access to the storage account. Only storage account owners can change your access level via the Azure portal.\n\nOr, your admin put the data storage behind V-Net and you can not get access to it- https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-identity-based-data-access#work-with-virtual-networks\nIn this situation, you need to ask permission from your admin.\n\nCould you please share which situation you are in?\n\n\n\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/854288\/notebook-files-have-disaperred.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1652928385600,
        "Question_original_content":"notebook file disap hello mlstudio notebook file window disappear access data seen imag know help solv soon poosibl thank",
        "Question_preprocessed_content":"notebook file disap hello mlstudio notebook file window disappear access data know help solv soon poosibl thank",
        "Question_gpt_summary_original":"The user is facing a challenge where the notebook files window has disappeared in their MLStudio, making it impossible to access any data. They are seeking help to resolve the issue as soon as possible.",
        "Question_gpt_summary":"user face challeng notebook file window disappear mlstudio make imposs access data seek help resolv issu soon possibl",
        "Answer_original_content":"hello thank reach check access storag http doc microsoft com azur storag blob assign azur role data access tab portal assign azur role access storag servic storag blob data reader access storag account storag account owner chang access level azur portal admin data storag net access http doc microsoft com azur machin learn ident base data access work virtual network situat need ask permiss admin share situat regard yutong",
        "Answer_preprocessed_content":"hello thank reach check access storag access storag servic storag blob data reader access storag account storag account owner chang access level azur portal admin data storag access situat need ask permiss admin share situat regard yutong",
        "Answer_gpt_summary_original":"Solutions provided:\n- Check the access of Storage and ensure that the user has at least Storage Blob Data Reader access to the storage account.\n- If the data storage is behind V-Net, the user needs to ask permission from their admin.",
        "Answer_gpt_summary":"solut provid check access storag ensur user storag blob data reader access storag account data storag net user need ask permiss admin"
    },
    {
        "Question_title":"Is there any limits of saving result on S3 from sagemaker Processing?",
        "Question_body":"<p>\u203b I used google translation, if you have any question, let me know!<\/p>\n\n<p>I am trying to run python script with huge 4 data, using sagemaker processing. And my current situation are as follows:<\/p>\n\n<ul>\n<li>can run this script with 3 data<\/li>\n<li>can't run the script with only 1 data (the biggest, the same structure with others)<\/li>\n<li>as for all of 4 data, the script has finished (so, I suspected this error in S3, ie. when copying sagemaker result to S3)<\/li>\n<\/ul>\n\n<p>The error I got is this InternalServerError.<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"sagemaker_train_and_predict.py\", line 56, in &lt;module&gt;\n    outputs=outputs\n  File \"{xxx}\/sagemaker_constructor.py\", line 39, in run\n    outputs=outputs\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 408, in run\n    self.latest_job.wait(logs=logs)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/processing.py\", line 723, in wait\n    self.sagemaker_session.logs_for_processing_job(self.job_name, wait=True)\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 3111, in logs_for_processing_job\n    self._check_job_status(job_name, description, \"ProcessingJobStatus\")\n  File \"{masked}\/.pyenv\/versions\/3.6.8\/lib\/python3.6\/site-packages\/sagemaker\/session.py\", line 2615, in _check_job_status\n    actual_status=status,\nsagemaker.exceptions.UnexpectedStatusException: Error for Processing job sagemaker-vm-train-and-predict-2020-04-12-04-15-40-655: Failed. Reason: InternalServerError: We encountered an internal error.  Please try again.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1586755348657,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":95.0,
        "Poster_id":13298693.0,
        "Poster_created_time":1586754432800,
        "Poster_last_access_time":1593512686190,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":13.0,
        "Poster_views":1.0,
        "Answer_body":"<p>There may be some issue transferring the output data to S3 if the output is generated at a high rate and size is too large. <\/p>\n\n<p>You can 1) try to slow down writing the output a bit or 2) call S3 from your algorithm container to upload the output directly using boto client (<a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html\" rel=\"nofollow noreferrer\">https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html<\/a>).<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":4805806.0,
        "Answerer_created_time":1429402428283,
        "Answerer_last_access_time":1599168282423,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":76.0,
        "Answerer_views":13.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61181955",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1586886613192,
        "Question_original_content":"limit save result process googl translat question let know try run python script huge data process current situat follow run script data run script data biggest structur data script finish suspect error copi result error got internalservererror traceback recent file train predict line output output file constructor line run output output file mask pyenv version lib python site packag process line run self latest job wait log log file mask pyenv version lib python site packag process line wait self session log process job self job wait true file mask pyenv version lib python site packag session line log process job self check job statu job descript processingjobstatu file mask pyenv version lib python site packag session line check job statu actual statu statu except unexpectedstatusexcept error process job train predict fail reason internalservererror encount intern error try",
        "Question_preprocessed_content":"limit save result process googl translat question let know try run python script huge data process current situat follow run script data run script data data script finish error got internalservererror",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to run a python script with 4 huge data using sagemaker processing. The script runs successfully with 3 data but fails with the biggest data. The error message received is \"InternalServerError\". The user suspects that the error is in S3 while copying sagemaker result to S3.",
        "Question_gpt_summary":"user encount challeng try run python script huge data process script run successfulli data fail biggest data error messag receiv internalservererror user suspect error copi result",
        "Answer_original_content":"issu transfer output data output gener high rate size larg try slow write output bit algorithm contain upload output directli boto client http boto amazonaw com document api latest refer servic html",
        "Answer_preprocessed_content":"issu transfer output data output gener high rate size larg try slow write output bit algorithm contain upload output directli boto client",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Slow down writing the output a bit.\n2. Call S3 from the algorithm container to upload the output directly using boto client.",
        "Answer_gpt_summary":"possibl solut mention discuss slow write output bit algorithm contain upload output directli boto client"
    },
    {
        "Question_title":"Send new data to Deployed model",
        "Question_body":"Hello,\n\nWe are sending data from a smartwatch -> IoT Central -> Event Hubs -> Data Explorer -> Blob Storage.\n\nWe are then using the blob storage as a datastore in Machine Learning, which we make a dataset of.\n\nWe deployed a model we trained locally to Azure Machine Learning.\n\nWe now want to send new data from the watch to the model to make predictions on.\n\nWe are wondering how we can do this?\n\nDo we just update the dataset the same way we are currently sending the data? and if so, how can we then auto send it to the model?\n\nOr is there another way to send this new data? Can we still send through blob storage? Or should we send the data directly from the watch to the webservice made by the model?\n\nThanks so much!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1619546083177,
        "Question_favorite_count":16.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"Hello @yjay-4307 ,\n\nso you get new data from devices and you want to predict using that data?\n\nCheck out Azure Stream Analytics which can ingest messages from the Event Hub.\n\nThen, you can Azure ML as a function on make decisions based on the incoming data using ML.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/374180\/send-new-data-to-deployed-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1619561766012,
        "Question_original_content":"send new data deploi model hello send data smartwatch iot central event hub data explor blob storag blob storag datastor machin learn dataset deploi model train local want send new data watch model predict wonder updat dataset wai current send data auto send model wai send new data send blob storag send data directli watch webservic model thank",
        "Question_preprocessed_content":"send new data deploi model hello send data smartwatch iot central event hub data explor blob storag blob storag datastor machin learn dataset deploi model train local want send new data watch model predict wonder updat dataset wai current send data auto send model wai send new data send blob storag send data directli watch webservic model thank",
        "Question_gpt_summary_original":"The user is facing challenges in sending new data from a smartwatch to a deployed model in Azure Machine Learning. They are currently sending data through IoT Central, Event Hubs, Data Explorer, and Blob Storage, which they are using as a datastore to create a dataset. The user is unsure if they can update the dataset with new data and auto-send it to the model or if they should send the data directly from the watch to the webservice made by the model.",
        "Question_gpt_summary":"user face challeng send new data smartwatch deploi model current send data iot central event hub data explor blob storag datastor creat dataset user unsur updat dataset new data auto send model send data directli watch webservic model",
        "Answer_original_content":"hello yjai new data devic want predict data check azur stream analyt ingest messag event hub function decis base incom data",
        "Answer_preprocessed_content":"hello new data devic want predict data check azur stream analyt ingest messag event hub function decis base incom data",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use Azure Stream Analytics to ingest messages from the Event Hub and then use Azure ML as a function to make decisions based on the incoming data using ML.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us azur stream analyt ingest messag event hub us function decis base incom data"
    },
    {
        "Question_title":"TSV as input to sagemaker",
        "Question_body":"<p>Is there any way to use a tsv instead of a csv as the input into sagemaker's autopilot ?<\/p>\n\n<p>Currently I'm inputting the data as such:<\/p>\n\n<pre><code>input_data_config = [{\n      'DataSource': {\n        'S3DataSource': {\n          'S3DataType': 'S3Prefix',\n          'S3Uri': 's3:\/\/{}\/{}\/train'.format(bucket,prefix)\n        }\n      },\n      'TargetAttributeName': 'sentiment'\n    }\n  ]\n<\/code><\/pre>\n\n<p>this seems to work file for .csv files but fails for my .tsv files.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1580377511313,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":204.0,
        "Poster_id":7156008.0,
        "Poster_created_time":1479115407580,
        "Poster_last_access_time":1662241113300,
        "Poster_age":null,
        "Poster_location":"Earth",
        "Poster_reputation":2944.0,
        "Poster_views":381.0,
        "Answer_body":"<p>I am a developer at AWS SageMaker. Autopilot currently only supports CSV data. While we are working on extending the support to more file formats: JSON, TSV, etc, this might be something that you can try to convert your .tsv file to .csv:<\/p>\n\n<pre><code>import csv\n\n# read tab-delimited file\nwith open('yourfile.tsv','rb') as fin:\n    cr = csv.reader(fin, delimiter='\\t')\n    filecontents = [line for line in cr]\n\n# write comma-delimited file (comma is the default delimiter)\nwith open('yourfile.csv','wb') as fou:\n    cw = csv.writer(fou, quotechar='', quoting=csv.QUOTE_NONE)\n    cw.writerows(filecontents)\n<\/code><\/pre>\n\n<p>Hope this helps.<\/p>\n\n<p>Ref: <a href=\"https:\/\/stackoverflow.com\/questions\/5590631\/how-to-convert-a-tab-separated-file-to-csv-format\">How to convert a tab separated file to CSV format?<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":7815954.0,
        "Answerer_created_time":1491327759476,
        "Answerer_last_access_time":1638772801672,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":46.0,
        "Answerer_views":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59983062",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1580778531436,
        "Question_original_content":"tsv input wai us tsv instead csv input autopilot current input data input data config datasourc sdatasourc sdatatyp sprefix suri train format bucket prefix targetattributenam sentiment work file csv file fail tsv file",
        "Question_preprocessed_content":"tsv input wai us tsv instead csv input autopilot current input data work file csv file fail tsv file",
        "Question_gpt_summary_original":"The user is facing a challenge in using a TSV file as input for Sagemaker's autopilot. The current method of inputting data using a CSV file works fine, but fails for TSV files.",
        "Question_gpt_summary":"user face challeng tsv file input autopilot current method input data csv file work fine fail tsv file",
        "Answer_original_content":"develop autopilot current support csv data work extend support file format json tsv try convert tsv file csv import csv read tab delimit file open yourfil tsv fin csv reader fin delimit filecont line line write comma delimit file comma default delimit open yourfil csv fou csv writer fou quotechar quot csv quot writerow filecont hope help ref convert tab separ file csv format",
        "Answer_preprocessed_content":"develop autopilot current support csv data work extend support file format json tsv try convert tsv file csv hope help ref convert tab separ file csv format",
        "Answer_gpt_summary_original":"Solution: The user can try to convert the TSV file to a CSV file using Python's CSV module. The code for the conversion is provided in the discussion. Currently, Sagemaker's autopilot only supports CSV data, and the team is working on extending the support to more file formats.",
        "Answer_gpt_summary":"solut user try convert tsv file csv file python csv modul code convers provid discuss current autopilot support csv data team work extend support file format"
    },
    {
        "Question_title":"Failed precondition: Table not initialized. on deployed universal sentence encoder from aws sagemaker",
        "Question_body":"<p>I have deployed a the universal_sentence_encoder_large_3 to an aws sagemaker.  When I am attempting to predict with the deployed model I get <code>Failed precondition: Table not initialized.<\/code> as an error. I have included the part where I save my model below:<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\ndef tfhub_to_savedmodel(model_name, export_path):\n\n    model_path = '{}\/{}\/00000001'.format(export_path, model_name)\n    tfhub_uri = 'http:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3'\n\n    with tf.Session() as sess:\n        module = hub.Module(tfhub_uri)\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n        output = module(inputs['text'])\n        outputs = {\n            'vector': output,\n        }\n\n        # export the model\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n<\/code><\/pre>\n\n<p>I have seen other people ask this problem but no solution has been ever posted.  It seems to be a common problem with tensorflow_hub sentence encoders<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1563993432753,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":365.0,
        "Poster_id":10094585.0,
        "Poster_created_time":1531840489147,
        "Poster_last_access_time":1636067734716,
        "Poster_age":null,
        "Poster_location":"Berkeley, CA, USA",
        "Poster_reputation":425.0,
        "Poster_views":92.0,
        "Answer_body":"<p>I was running into this exact issue earlier this week while trying to modify this example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_container\/tensorflow_serving_container.ipynb\" rel=\"nofollow noreferrer\">Sagemaker notebook<\/a>. Particularly the part where serving the model. That is, running <code>predictor.predict()<\/code> on the Sagemaker Tensorflow Estimator.<\/p>\n\n<p>The solution outlined in the issue worked perfectly for me- <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290<\/a><\/p>\n\n<p>I think it's just because <code>tf.tables_initializer()<\/code> only runs for training but it needs to be specified through the <code>legacy_init_op<\/code> if you want to run it during prediction.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":4073736.0,
        "Answerer_created_time":1411546424280,
        "Answerer_last_access_time":1660817052127,
        "Answerer_age":null,
        "Answerer_location":"Edinburgh",
        "Answerer_reputation":58.0,
        "Answerer_views":16.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57189292",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1580113898207,
        "Question_original_content":"fail precondit tabl initi deploi univers sentenc encod deploi univers sentenc encod larg attempt predict deploi model fail precondit tabl initi error includ save model import tensorflow import tensorflow hub hub import numpi def tfhub savedmodel model export path model path format export path model tfhub uri http tfhub dev googl univers sentenc encod larg session sess modul hub modul tfhub uri sess run global variabl initi tabl initi input param modul input info dict dtype input param text dtype shape input param text shape defin model input input text placehold dtype shape text output modul input text output vector output export model save model simpl save sess model path input input output output return model path seen peopl ask problem solut post common problem tensorflow hub sentenc encod",
        "Question_preprocessed_content":"fail precondit tabl initi deploi univers sentenc encod deploi attempt predict deploi model error includ save model seen peopl ask problem solut post common problem sentenc encod",
        "Question_gpt_summary_original":"The user has encountered an error message \"Failed precondition: Table not initialized\" while attempting to predict with the deployed universal_sentence_encoder_large_3 model on AWS Sagemaker. The error seems to be a common problem with TensorFlow Hub sentence encoders, and the user has not found a solution yet.",
        "Question_gpt_summary":"user encount error messag fail precondit tabl initi attempt predict deploi univers sentenc encod larg model error common problem tensorflow hub sentenc encod user solut",
        "Answer_original_content":"run exact issu earlier week try modifi exampl notebook particularli serv model run predictor predict tensorflow estim solut outlin issu work perfectli http github com awslab amazon exampl issu issuecom think tabl initi run train need specifi legaci init want run predict",
        "Answer_preprocessed_content":"run exact issu earlier week try modifi exampl notebook particularli serv model run tensorflow estim solut outlin issu work perfectli think run train need specifi want run predict",
        "Answer_gpt_summary_original":"Solution: The user can try specifying <code>legacy_init_op=tf.tables_initializer()<\/code> when creating the TensorFlow estimator to run <code>tf.tables_initializer()<\/code> during prediction. This solution was suggested in an issue on the AWS Sagemaker examples GitHub repository and worked for the person who encountered the same error.",
        "Answer_gpt_summary":"solut user try specifi legaci init tabl initi creat tensorflow estim run tabl initi predict solut suggest issu exampl github repositori work person encount error"
    },
    {
        "Question_title":"Azure Machine Learning: I cannot find experiment's user logs located in logs\/user folder",
        "Question_body":"I am running experiments in Azure Machine Learning using ParallelRunStep, and I cannot get the user folder with logs as defined in readme.txt file with the log folder structure.\nI cannot find log\/user folder with \"Logs generated when loading and running user's scripts.\"\n\nreadme.txt file states:\nParallelRunStep has two major parts:\n1. Scheduling, progress tracking and file concatenation for append_row.\n2. Processing mini batch by calling the entry script.\nThe agent manager on each node start agents.\nAn agent gets mini batch and calls the entry script against the mini batch.\n\n The \"logs\" folder has user, sys and perf sub folders.\n The user folder includes messages from the entry script in processing mini batches.\n The sys folder includes messages from #1 and non-entry script log from #2.\n The perf folder includes periodical checking result of resource usage.\n\n\n\nIn majority case, users can find the processing messages from the user folder.\nUsers need to check sys folder for messages beyond processing mini batches.\nlogs\/\nazureml\/: Logs from azureml dependencies. e.g. azureml.dataprep\nuser\/ : Logs generated when loading and running user's scripts.\nerror\/ : Logs of errors encountered while loading and running entry script.\nstderr\/ : stderr output of user's scripts.\nstdout\/ : stdout output of user's scripts.\nentry_script_log\/ : Logs generated by loggers of EntryScript()\n<node seq> :\nprocessNNN.log.txt : Logs generated by loggers of EntryScript() from each process.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1645621539517,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":null,
        "Poster_id":null,
        "Poster_created_time":null,
        "Poster_last_access_time":null,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":null,
        "Poster_views":null,
        "Answer_body":"@CalabriaMonteroSalvadorSGRESEDFPDC-5704 Thanks for the question. Please follow the doc to view and log files for a run. Interactive logging sessions are typically used in notebook environments. The method Experiment.start_logging() starts an interactive logging session. Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics#view-and-download-log-files-for-a-run",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":null,
        "Answerer_created_time":null,
        "Answerer_last_access_time":null,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":null,
        "Answerer_views":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/747549\/azure-machine-learning-i-cannot-find-experiment39s.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1645670223416,
        "Question_original_content":"experi user log locat log user folder run experi parallelrunstep user folder log defin readm txt file log folder structur log user folder log gener load run user script readm txt file state parallelrunstep major part schedul progress track file concaten append row process mini batch call entri script agent manag node start agent agent get mini batch call entri script mini batch log folder user sy perf sub folder user folder includ messag entri script process mini batch sy folder includ messag non entri script log perf folder includ period check result resourc usag major case user process messag user folder user need check sy folder messag process mini batch log log depend dataprep user log gener load run user script error log error encount load run entri script stderr stderr output user script stdout stdout output user script entri script log log gener logger entryscript process log txt log gener logger entryscript process",
        "Question_preprocessed_content":"experi user log locat folder run experi parallelrunstep user folder log defin file log folder structur folder log gener load run user file state parallelrunstep major part schedul progress track file concaten process mini batch call entri script agent manag node start agent agent get mini batch call entri script mini batch log folder user sy perf sub folder user folder includ messag entri script process mini batch sy folder includ messag script log perf folder includ period check result resourc usag major case user process messag user folder user need check sy folder messag process mini batch log log depend dataprep user log gener load run user script error log error encount load run entri script stderr stderr output user script stdout stdout output user script log gener logger entryscript log gener logger entryscript process",
        "Question_gpt_summary_original":"The user is encountering challenges in finding the user logs folder located in logs\/user while running experiments in Azure Machine Learning using ParallelRunStep. The readme.txt file provides information on the log folder structure, which includes user, sys, and perf subfolders. The user folder contains messages from the entry script in processing mini batches, while the sys folder includes messages from scheduling, progress tracking, and non-entry script logs. The user needs to check the sys folder for messages beyond processing mini batches.",
        "Question_gpt_summary":"user encount challeng find user log folder locat log user run experi parallelrunstep readm txt file provid inform log folder structur includ user sy perf subfold user folder contain messag entri script process mini batch sy folder includ messag schedul progress track non entri script log user need check sy folder messag process mini batch",
        "Answer_original_content":"calabriamonterosalvadorsgresedfpdc thank question follow doc view log file run interact log session typic notebook environ method experi start log start interact log session metric log session ad run record experi method run complet end session mark run complet http doc microsoft com azur machin learn log view metric view download log file run",
        "Answer_preprocessed_content":"thank question follow doc view log file run interact log session typic notebook environ method start interact log session metric log session ad run record experi method end session mark run complet",
        "Answer_gpt_summary_original":"Solution: The user can follow the documentation provided by Microsoft to view and log files for a run. The documentation suggests using interactive logging sessions, which can be started using the method Experiment.start_logging(). Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed. However, no solution is provided for finding the user logs folder located in logs\/user while running experiments in Azure Machine Learning using ParallelRunStep.",
        "Answer_gpt_summary":"solut user follow document provid microsoft view log file run document suggest interact log session start method experi start log metric log session ad run record experi method run complet end session mark run complet solut provid find user log folder locat log user run experi parallelrunstep"
    },
    {
        "Question_title":"How to make prediction after model registration in azure?",
        "Question_body":"<p>I created a simply model and then registered in azure. How can I make a prediction?<\/p>\n<pre><code>from sklearn import svm\nimport joblib\nimport numpy as np\n\n# customer ages\nX_train = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nX_train = X_train.reshape(-1, 1)\n# churn y\/n\ny_train = [&quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;yes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;yes&quot;]\n\nclf = svm.SVC(gamma=0.001, C=100.)\nclf.fit(X_train, y_train)\n\njoblib.dump(value=clf, filename=&quot;churn-model.pkl&quot;)\n<\/code><\/pre>\n<p>Registration:<\/p>\n<pre><code>from azureml.core import Workspace\nws = Workspace.get(name=&quot;myworkspace&quot;, subscription_id='My_subscription_id', resource_group='ML_Lingaro')\n\nfrom azureml.core.model import Model\nmodel = Model.register(workspace=ws, model_path=&quot;churn-model.pkl&quot;, model_name=&quot;churn-model-test&quot;)\n<\/code><\/pre>\n<p>Prediction:<\/p>\n<pre><code>from azureml.core.model import Model\nimport os\n\nmodel = Model(workspace=ws, name=&quot;churn-model-test&quot;)\nX_test = np.array([50, 17, 35, 23, 28, 40, 31, 29, 19, 62])\nmodel.predict(X_test) ???? \n<\/code><\/pre>\n<p>Error: <code>AttributeError: 'Model' object has no attribute 'predict'<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1609722575690,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1609723457016,
        "Question_score":3,
        "Question_view_count":540.0,
        "Poster_id":14672614.0,
        "Poster_created_time":1605834001336,
        "Poster_last_access_time":1618085210227,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":73.0,
        "Poster_views":24.0,
        "Answer_body":"<p>great question -- I also had the same misconception starting out. The missing piece is that there's a difference between model 'registration' and model 'deployment'. Registration is simply for tracking and for easy downloading at a later place and time. Deployment is what you're after, making a model available to be scored against.<\/p>\n<p>There's a <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python&amp;WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">whole section in the docs about deployment<\/a>. My suggestion would be to deploy it locally first for testing.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":3842610.0,
        "Answerer_created_time":1405457120427,
        "Answerer_last_access_time":1663947733100,
        "Answerer_age":null,
        "Answerer_location":"Seattle, WA, USA",
        "Answerer_reputation":3359.0,
        "Answerer_views":555.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65556574",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1609737626627,
        "Question_original_content":"predict model registr azur creat simpli model regist azur predict sklearn import svm import joblib import numpi custom ag train arrai train train reshap churn train ye ye ye ye ye clf svm svc gamma clf fit train train joblib dump valu clf filenam churn model pkl registr core import workspac workspac myworkspac subscript subscript resourc group lingaro core model import model model model regist workspac model path churn model pkl model churn model test predict core model import model import model model workspac churn model test test arrai model predict test error attributeerror model object attribut predict",
        "Question_preprocessed_content":"predict model registr azur creat simpli model regist azur predict registr predict error",
        "Question_gpt_summary_original":"The user has created a simple model and registered it in Azure, but is facing challenges in making a prediction. The user is encountering an error message stating that the 'Model' object has no attribute 'predict'.",
        "Question_gpt_summary":"user creat simpl model regist azur face challeng make predict user encount error messag state model object attribut predict",
        "Answer_original_content":"great question misconcept start miss piec differ model registr model deploy registr simpli track easi download later place time deploy make model avail score section doc deploy suggest deploi local test",
        "Answer_preprocessed_content":"great question misconcept start miss piec differ model registr model deploy registr simpli track easi download later place time deploy make model avail score section doc deploy suggest deploi local test",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user needs to deploy the model instead of just registering it. The deployment process is explained in detail in the Azure documentation, and the suggestion is to deploy the model locally first for testing.",
        "Answer_gpt_summary":"solut discuss suggest user need deploi model instead regist deploy process explain azur document suggest deploi model local test"
    },
    {
        "Question_title":"cast xgboost.Booster class to XGBRegressor or load XGBRegressor from xgboost.Booster",
        "Question_body":"<p>I get a model from Sagemaker of type:<\/p>\n<pre><code>&lt;class 'xgboost.core.Booster'&gt;\n<\/code><\/pre>\n<p>I can score this locally which is great but some google searches have shown that it may not be possible to do &quot;standard&quot; things like this taken from <a href=\"https:\/\/mljar.com\/blog\/feature-importance-xgboost\/\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>plt.barh(boston.feature_names, xgb.feature_importances_)\n<\/code><\/pre>\n<p>Is it possible to tranform xgboost.core.Booster to XGBRegressor? Maybe one could use the save_raw method looking at <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html#xgboost.Booster\" rel=\"nofollow noreferrer\">this<\/a>? Thanks!<\/p>\n<p>So far I tried:<\/p>\n<pre><code>xgb_reg = xgb.XGBRegressor() \nxgb_reg._Boster = model\nxgb_reg.feature_importances_\n<\/code><\/pre>\n<p>but this reults in:<\/p>\n<pre><code>NotFittedError: need to call fit or load_model beforehand\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1651569762327,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1651579039263,
        "Question_score":0,
        "Question_view_count":172.0,
        "Poster_id":283538.0,
        "Poster_created_time":1267440784443,
        "Poster_last_access_time":1664045779312,
        "Poster_age":null,
        "Poster_location":"Somewhere",
        "Poster_reputation":15705.0,
        "Poster_views":2150.0,
        "Answer_body":"<p>Something along those lines appears to work fine:<\/p>\n<pre><code>local_model_path = &quot;model.tar.gz&quot;\nwith tarfile.open(local_model_path) as tar:\n    tar.extractall()\n\nmodel = xgb.XGBRegressor() \nmodel.load_model(model_file_name)     \n<\/code><\/pre>\n<p>model can then be used as usual - model.tar.gz is an artifcat coming from sagemaker.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Answerer_id":283538.0,
        "Answerer_created_time":1267440784443,
        "Answerer_last_access_time":1664045779312,
        "Answerer_age":null,
        "Answerer_location":"Somewhere",
        "Answerer_reputation":15705.0,
        "Answerer_views":2150.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72097296",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1651588367703,
        "Question_original_content":"cast xgboost booster class xgbregressor load xgbregressor xgboost booster model type score local great googl search shown possibl standard thing like taken plt barh boston featur name xgb featur import possibl tranform xgboost core booster xgbregressor mayb us save raw method look thank far tri xgb reg xgb xgbregressor xgb reg boster model xgb reg featur import reult notfittederror need fit load model",
        "Question_preprocessed_content":"cast class xgbregressor load xgbregressor model type score local great googl search shown possibl standard thing like taken possibl tranform xgbregressor mayb us method look thank far tri reult",
        "Question_gpt_summary_original":"The user is facing a challenge in transforming an xgboost.core.Booster model to XGBRegressor and using standard methods like feature_importances_. The user has tried assigning the model to XGBRegressor but is encountering a NotFittedError.",
        "Question_gpt_summary":"user face challeng transform xgboost core booster model xgbregressor standard method like featur import user tri assign model xgbregressor encount notfittederror",
        "Answer_original_content":"line appear work fine local model path model tar tarfil open local model path tar tar extractal model xgb xgbregressor model load model model file model usual model tar artifcat come",
        "Answer_preprocessed_content":"line appear work fine model usual artifcat come",
        "Answer_gpt_summary_original":"Solution: The user can try loading the model using the code mentioned in the discussion. They can extract the model from the artifact using tarfile and then load it into XGBRegressor using the load_model method. Once loaded, the model can be used as usual.",
        "Answer_gpt_summary":"solut user try load model code mention discuss extract model artifact tarfil load xgbregressor load model method load model usual"
    },
    {
        "Question_title":"Unable to ignore .DS_Store files in DVC",
        "Question_body":"<p>I use DVC to track my media files. I use MacOS and I want\".DS_Store\" files to be ignored by DVC. According to DVC documentation I can achieve it with  <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">.dvcignore<\/a>. I created <code>.dvcignore<\/code> file with \".DS_Store\" rule. However every time \".DS_Store\" is created <code>dvc status<\/code> still says that content has changed<\/p>\n\n<p>Here is the little test to reproduce my issue:<\/p>\n\n<pre><code>$ git init\n$ dvc init\n\n# create directory to store data\n# and track it's content with DVC\n$ mkdir data\n$ dvc add data\n\n# Ignore .DS_Store files created by MacOS\n$ echo \".DS_Store\" &gt; .dvcignore\n\n# create .DS_Store in data dir\n$ touch \"data\/.DS_Store\"\n<\/code><\/pre>\n\n<p>If I understand DVC documentation correctly then <code>dvc status<\/code> should print something like \"Pipeline is up to date. Nothing to reproduce\". However <code>dvc status<\/code> gives me:<\/p>\n\n<pre><code>data.dvc:\n        changed outs:\n                modified:           data\n<\/code><\/pre>\n\n<p>How I can really ignore \".DS_Store\" files?<\/p>\n\n<p><strong>UPDATE:<\/strong> The .dvcignore support noticeably improved in latest versions and the problem is no more relevant.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_created_time":1559722020690,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1568948355943,
        "Question_score":3,
        "Question_view_count":326.0,
        "Poster_id":9565342.0,
        "Poster_created_time":1522254698710,
        "Poster_last_access_time":1663927713687,
        "Poster_age":null,
        "Poster_location":"Russia",
        "Poster_reputation":784.0,
        "Poster_views":77.0,
        "Answer_body":"<p>The current implementation of <code>.dvcignore<\/code> is very limited. Read more on it <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvcignore\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Please, mention that you are interested in this feature here - <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/1876\" rel=\"nofollow noreferrer\">https:\/\/github.com\/iterative\/dvc\/issues\/1876<\/a>. That would help our team to prioritize issues properly.<\/p>\n\n<p>The possible workaround for now would be to use one of these approaches - <a href=\"https:\/\/stackoverflow.com\/questions\/18015978\/how-to-stop-creating-ds-store-on-mac\">How to stop creating .DS_Store on Mac?<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":298182.0,
        "Answerer_created_time":1250347954880,
        "Answerer_last_access_time":1664073072270,
        "Answerer_age":null,
        "Answerer_location":"San Francisco, CA, USA",
        "Answerer_reputation":5575.0,
        "Answerer_views":358.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56456463",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Question_closed_time":1559758177416,
        "Question_original_content":"unabl ignor store file us track media file us maco want store file ignor accord document achiev ignor creat ignor file store rule time store creat statu sai content chang littl test reproduc issu git init init creat directori store data track content mkdir data add data ignor store file creat maco echo store ignor creat store data dir touch data store understand document correctli statu print like pipelin date reproduc statu give data chang out modifi data ignor store file updat ignor support notic improv latest version problem relev",
        "Question_preprocessed_content":"unabl ignor file us track media file us maco file ignor accord document achiev ignor creat file rule time creat sai content chang littl test reproduc issu understand document correctli print like pipelin date reproduc give ignor file updat ignor support notic improv latest version problem relev",
        "Question_gpt_summary_original":"The user is facing a challenge with DVC, a tool used to track media files, on MacOS. The user wants to ignore \".DS_Store\" files using the \".dvcignore\" file, but every time a \".DS_Store\" file is created, the \"dvc status\" command shows that the content has changed. The user is seeking a solution to ignore \".DS_Store\" files.",
        "Question_gpt_summary":"user face challeng tool track media file maco user want ignor store file ignor file time store file creat statu command show content chang user seek solut ignor store file",
        "Answer_original_content":null,
        "Answer_preprocessed_content":null,
        "Answer_gpt_summary_original":null,
        "Answer_gpt_summary":null
    },
    {
        "Question_title":"AWS Sagemaker KeyError: 'SM_CHANNEL_TRAINING' when tuning hyperparameters",
        "Question_body":"<p>When I try to use hyperparameters tuning on Sagemaker I get this error:<\/p>\n<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job imageclassif-job-10-21-47-43: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details.\n<\/code><\/pre>\n<p>When I look up the logs on CloudWatch all 5 failed training jobs have the same error at the end:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 184, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/opt\/ml\/code\/train.py&quot;, line 117, in &lt;module&gt;\n    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n  File &quot;\/usr\/lib\/python3.5\/os.py&quot;, line 725, in __getitem__\n    raise KeyError(key) from None\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>KeyError: 'SM_CHANNEL_TRAINING'\n<\/code><\/pre>\n<p>The problem is at the Step 4 of the project: <a href=\"https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb<\/a><\/p>\n<p>Would hihgly appreciate any hints on where to look next<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1615420401190,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":1121.0,
        "Poster_id":13454956.0,
        "Poster_created_time":1588429621780,
        "Poster_last_access_time":1626734325667,
        "Poster_age":null,
        "Poster_location":"Miami Beach, \u0424\u043b\u043e\u0440\u0438\u0434\u0430, \u0421\u0428\u0410",
        "Poster_reputation":21.0,
        "Poster_views":10.0,
        "Answer_body":"<p>In your <code>train.py<\/code> file, changing the environment variable from<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])<\/code><\/p>\n<p>to<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])<\/code> should address the issue.<\/p>\n<p>This is the case with Torch's framework_version 1.3.1 but other versions might also be affected. Here is the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1292\" rel=\"nofollow noreferrer\">link<\/a> for your reference.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Answerer_id":8367307.0,
        "Answerer_created_time":1501040260887,
        "Answerer_last_access_time":1663957538380,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":66.0,
        "Answerer_views":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66574569",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1625508026043,
        "Question_original_content":"keyerror channel train tune hyperparamet try us hyperparamet tune error unexpectedstatusexcept error hyperparametertun job imageclassif job fail reason train job succeed attempt look train job failur detail look log cloudwatch fail train job error end traceback recent file usr lib python runpi line run modul main main mod spec file usr lib python runpi line run code exec code run global file opt code train line parser add argument data dir type str default environ channel train file usr lib python line getitem rais keyerror kei keyerror channel train problem step project http github com petrooha deploi lstm blob main project ipynb hihgli appreci hint look",
        "Question_preprocessed_content":"keyerror tune hyperparamet try us hyperparamet tune error look log cloudwatch fail train job error end problem step project hihgli appreci hint look",
        "Question_gpt_summary_original":"The user is encountering an error while trying to use hyperparameter tuning on AWS Sagemaker. All 5 failed training jobs have the same error related to a KeyError: 'SM_CHANNEL_TRAINING'. The problem is occurring at Step 4 of the project and the user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error try us hyperparamet tune fail train job error relat keyerror channel train problem occur step project user seek help resolv issu",
        "Answer_original_content":"train file chang environ variabl parser add argument data dir type str default environ channel train parser add argument data dir type str default environ channel train address issu case torch framework version version affect link refer",
        "Answer_preprocessed_content":"file chang environ variabl address issu case torch version affect link refer",
        "Answer_gpt_summary_original":"Solution: The solution to the error related to KeyError: 'SM_CHANNEL_TRAINING' is to change the environment variable in the train.py file from \"SM_CHANNEL_TRAINING\" to \"SM_CHANNEL_TRAIN\". This solution is specific to Torch's framework_version 1.3.1, but other versions may also be affected. The user can refer to the provided link for more information.",
        "Answer_gpt_summary":"solut solut error relat keyerror channel train chang environ variabl train file channel train channel train solut specif torch framework version version affect user refer provid link inform"
    },
    {
        "Question_title":"Using a custom docker with Azure ML",
        "Question_body":"<p>I'm following the guidelines (<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments<\/a>) to use a custom docker file on Azure. My script to create the environment looks like this:<\/p>\n<pre><code>from azureml.core.environment import Environment\n\nmyenv = Environment(name = &quot;myenv&quot;)\nmyenv.docker.enabled = True\ndockerfile = r&quot;&quot;&quot;\nFROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04\nRUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx\nRUN echo &quot;Hello from custom container!&quot;\n&quot;&quot;&quot;\nmyenv.docker.base_image = None\nmyenv.docker.base_dockerfile = dockerfile\n<\/code><\/pre>\n<p>Upon execution, this is totally ignored and libgl1 is not installed. Any ideas why?<\/p>\n<p>EDIT: Here's the rest of my code:<\/p>\n<pre><code>est = Estimator(\n    source_directory = '.',\n    script_params = script_params,\n    use_gpu = True,\n    compute_target = 'gpu-cluster-1',\n    pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],\n    entry_script = 'AzureEntry.py',\n    )\n\nrun = exp.submit(config = est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments<\/a><\/p>",
        "Question_answer_count":5,
        "Question_comment_count":2,
        "Question_created_time":1594687513407,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1594731646430,
        "Question_score":4,
        "Question_view_count":1819.0,
        "Poster_id":2292788.0,
        "Poster_created_time":1366237908703,
        "Poster_last_access_time":1612802367676,
        "Poster_age":null,
        "Poster_location":null,
        "Poster_reputation":170.0,
        "Poster_views":23.0,
        "Answer_body":"<p>This should work :<\/p>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.environment import Environment\nfrom azureml.train.estimator import Estimator\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core import Experiment\n\nws = Workspace (...)\nexp = Experiment(ws, 'test-so-exp')\n\nmyenv = Environment(name = &quot;myenv&quot;)\nmyenv.docker.enabled = True\ndockerfile = r&quot;&quot;&quot;\nFROM mcr.microsoft.com\/azureml\/base:intelmpi2018.3-ubuntu16.04\nRUN apt-get update &amp;&amp; apt-get install -y libgl1-mesa-glx\nRUN echo &quot;Hello from custom container!&quot;\n&quot;&quot;&quot;\nmyenv.docker.base_image = None\nmyenv.docker.base_dockerfile = dockerfile\n\n## You need to instead put your packages in the Environment definition instead... \n## see below for some changes too\n\nmyenv.python.conda_dependencies = CondaDependencies.create(pip_packages = ['scipy==1.1.0', 'torch==1.5.1'])\n<\/code><\/pre>\n<p>Finally you can build your estimator a bit differently :<\/p>\n<pre><code>est = Estimator(\n    source_directory = '.',\n#     script_params = script_params,\n#     use_gpu = True,\n    compute_target = 'gpu-cluster-1',\n#     pip_packages = ['scipy==1.1.0', 'torch==1.5.1'],\n    entry_script = 'AzureEntry.py',\n    environment_definition=myenv\n    )\n<\/code><\/pre>\n<p>And submit it :<\/p>\n<pre><code>run = exp.submit(config = est)\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>Let us know if that works.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Answerer_id":10435556.0,
        "Answerer_created_time":1538275960603,
        "Answerer_last_access_time":1658458641830,
        "Answerer_age":null,
        "Answerer_location":"Montreal, QC, Canada",
        "Answerer_reputation":381.0,
        "Answerer_views":50.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62886435",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1594750426867,
        "Question_original_content":"custom docker follow guidelin http doc microsoft com azur machin learn us environ us custom docker file azur script creat environ look like core environ import environ myenv environ myenv myenv docker enabl true dockerfil mcr microsoft com base intelmpi ubuntu run apt updat apt instal libgl mesa glx run echo hello custom contain myenv docker base imag myenv docker base dockerfil dockerfil execut total ignor libgl instal idea edit rest code est estim sourc directori script param script param us gpu true comput target gpu cluster pip packag scipi torch entri script azureentri run exp submit config est run wait complet output true http doc microsoft com azur machin learn us environ",
        "Question_preprocessed_content":"custom docker follow guidelin us custom docker file azur script creat environ look like execut total ignor libgl instal idea edit rest code",
        "Question_gpt_summary_original":"The user is encountering a challenge in using a custom docker file on Azure. Despite following the guidelines provided, the installation of libgl1 is being ignored and not installed. The user has provided their code for creating the environment and running the script.",
        "Question_gpt_summary":"user encount challeng custom docker file azur despit follow guidelin provid instal libgl ignor instal user provid code creat environ run script",
        "Answer_original_content":"work core import workspac core environ import environ train estim import estim core conda depend import condadepend core import experi workspac exp experi test exp myenv environ myenv myenv docker enabl true dockerfil mcr microsoft com base intelmpi ubuntu run apt updat apt instal libgl mesa glx run echo hello custom contain myenv docker base imag myenv docker base dockerfil dockerfil need instead packag environ definit instead chang myenv python conda depend condadepend creat pip packag scipi torch final build estim bit differ est estim sourc directori script param script param us gpu true comput target gpu cluster pip packag scipi torch entri script azureentri environ definit myenv submit run exp submit config est run wait complet output true let know work",
        "Answer_preprocessed_content":"work final build estim bit differ submit let know work",
        "Answer_gpt_summary_original":"Solution: The solution provided is to include the installation of libgl1 in the Environment definition instead of the Dockerfile. The user needs to make some changes to the code and build the estimator differently. Finally, they can submit it and check if it works.",
        "Answer_gpt_summary":"solut solut provid includ instal libgl environ definit instead dockerfil user need chang code build estim differ final submit check work"
    },
    {
        "Question_title":"Why do I get an error that Sagemaker Endpoint does not have multiple models when it does?",
        "Question_body":"<p>Invoking a multimodel Sagemaker Endpoint, I get an error that it is not multimodel. I create it like this.<\/p>\n<pre><code>create_endpoint_config_response = client.create_endpoint_config(\n    EndpointConfigName=endpoint_config_name,\n    ProductionVariants=[\n        {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name1,\n            &quot;VariantName&quot;: model_name1,\n        },\n         {\n            &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n            &quot;InitialVariantWeight&quot;: 0.5,\n            &quot;InitialInstanceCount&quot;: 1,\n            &quot;ModelName&quot;: model_name2,\n            &quot;VariantName&quot;: model_name2,\n        }\n    ]\n)\n<\/code><\/pre>\n<p>I confirm in the GUI that it in fact has multiple models. I invoke it like this:<\/p>\n<pre><code>response = client.invoke_endpoint(\n    EndpointName=endpoint_name, \n    TargetModel=model_name1,\n    ContentType=&quot;text\/x-libsvm&quot;, \n    Body=payload\n)\n<\/code><\/pre>\n<p>and get this error:<\/p>\n<blockquote>\n<p>ValidationError: An error occurred (ValidationError) when calling the\nInvokeEndpoint operation: Endpoint\nmy-endpoint1 is not a multi-model endpoint\nand does not support target model header.<\/p>\n<\/blockquote>\n<p>The same problem was discussed <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">here<\/a> with no resolution.<\/p>\n<p>How can I invoke a multimodel endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1629114933853,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1629119102992,
        "Question_score":0,
        "Question_view_count":247.0,
        "Poster_id":39242.0,
        "Poster_created_time":1227171471292,
        "Poster_last_access_time":1664047108080,
        "Poster_age":null,
        "Poster_location":"Israel",
        "Poster_reputation":17500.0,
        "Poster_views":1561.0,
        "Answer_body":"<p>The answer (see <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/issues\/1026\" rel=\"nofollow noreferrer\">GitHub<\/a> discussion) is that this error message is simply false.<\/p>\n<p>To avoid this error, the model's local filename (usually for the form <code>model_filename.tar.gz<\/code>) must be used, not the model name.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/invoke-multi-model-endpoint.html\" rel=\"nofollow noreferrer\">documentation<\/a> does say this, though it lacks essential detail.<\/p>\n<p>I found <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb\" rel=\"nofollow noreferrer\">this to be the best example<\/a>.  See the last part  of that Notebook, in which <code>invoke_endpoint<\/code> is used (rather than a predictor as used earlier in the Notebook).<\/p>\n<p>As to the location of that model file: This <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/multi_model_bring_your_own\/multi_model_endpoint_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">Notebook<\/a> says:<\/p>\n<blockquote>\n<p>When creating the Model entity for multi-model endpoints, the container's ModelDataUrl is the S3 prefix where the model\nartifacts that are invokable by the endpoint are located. The rest of the S3 path will be specified when invoking the model.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1629959666336,
        "Answer_score":0.0,
        "Answerer_id":39242.0,
        "Answerer_created_time":1227171471292,
        "Answerer_last_access_time":1664047108080,
        "Answerer_age":null,
        "Answerer_location":"Israel",
        "Answerer_reputation":17500.0,
        "Answerer_views":1561.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68802388",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1629283623048,
        "Question_original_content":"error endpoint multipl model invok multimodel endpoint error multimodel creat like creat endpoint config respons client creat endpoint config endpointconfignam endpoint config productionvari instancetyp larg initialvariantweight initialinstancecount modelnam model variantnam model instancetyp larg initialvariantweight initialinstancecount modelnam model variantnam model confirm gui fact multipl model invok like respons client invok endpoint endpointnam endpoint targetmodel model contenttyp text libsvm bodi payload error validationerror error occur validationerror call invokeendpoint oper endpoint endpoint multi model endpoint support target model header problem discuss resolut invok multimodel endpoint",
        "Question_preprocessed_content":"error endpoint multipl model invok multimodel endpoint error multimodel creat like confirm gui fact multipl model invok like error validationerror error occur call invokeendpoint oper endpoint endpoint support target model header problem discuss resolut invok multimodel endpoint",
        "Question_gpt_summary_original":"The user is encountering an error when invoking a multimodel Sagemaker Endpoint, despite having created it with multiple models. The error message states that the endpoint is not multimodel and does not support target model header. The user has confirmed in the GUI that the endpoint does have multiple models. The user is seeking a solution to invoke the multimodel endpoint.",
        "Question_gpt_summary":"user encount error invok multimodel endpoint despit have creat multipl model error messag state endpoint multimodel support target model header user confirm gui endpoint multipl model user seek solut invok multimodel endpoint",
        "Answer_original_content":"answer github discuss error messag simpli fals avoid error model local filenam usual form model filenam tar model document lack essenti best exampl notebook invok endpoint predictor earlier notebook locat model file notebook sai creat model entiti multi model endpoint contain modeldataurl prefix model artifact invok endpoint locat rest path specifi invok model",
        "Answer_preprocessed_content":"answer error messag simpli fals avoid error model local filenam model document lack essenti best exampl notebook locat model file notebook sai creat model entiti endpoint contain modeldataurl prefix model artifact invok endpoint locat rest path specifi invok model",
        "Answer_gpt_summary_original":"Solution:\n- The error message is false.\n- To avoid the error, use the model's local filename instead of the model name.\n- The documentation mentions this but lacks essential detail.\n- Use the `invoke_endpoint` function instead of a predictor.\n- The location of the model file is specified in the Model entity's container's ModelDataUrl, which is the S3 prefix where the invokable model artifacts are located.",
        "Answer_gpt_summary":"solut error messag fals avoid error us model local filenam instead model document mention lack essenti us invok endpoint function instead predictor locat model file specifi model entiti contain modeldataurl prefix invok model artifact locat"
    },
    {
        "Question_title":"AzureML: experiment working for a subset and not for the whole dataset",
        "Question_body":"<p>some times ago I had written a code in AzureML meeting \"out of memory\" issues. So I tried to split the code in three different codes and that partially worked. It remains a part that (I think) is affected by memory issues too.<\/p>\n\n<p>I have created an experiment that I have published in this <a href=\"http:\/\/gallery.cortanaintelligence.com\/Experiment\/TextMining-sample-NA-v1-1\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>\n\n<p>There is a module that considers only a sample of my dataset, and it does work. This means that the code is supposed to work correctly. If you remove the sampling code (the second module starting from the top) <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Cbzhj.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>and you connect directly the original dataset you have the following situation<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/XOo8e.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>producing the following error:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mRSSQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Does someone have some way to understand where Azure crashes?<\/p>\n\n<p>Thanks you,<\/p>\n\n<p>Andrea<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1472651915230,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1,
        "Question_view_count":102.0,
        "Poster_id":5097722.0,
        "Poster_created_time":1436432728608,
        "Poster_last_access_time":1663607665487,
        "Poster_age":null,
        "Poster_location":"Colleferro, Italy",
        "Poster_reputation":809.0,
        "Poster_views":361.0,
        "Answer_body":"<p>Thanks so much for publishing the example -- this really helped to understand the issue. I suspect that you want to modify the <code>gsub()<\/code> calls in your script by adding the argument \"<code>fixed=TRUE<\/code>\" to each. (The documentation for this function is <a href=\"https:\/\/stat.ethz.ch\/R-manual\/R-devel\/library\/base\/html\/grep.html\" rel=\"nofollow\">here<\/a>.)<\/p>\n\n<p>What appears to have happened is that somewhere in your full dataset -- but not in the subsampled dataset -- there is some text that winds up being included in <code>df[i, \"names\"]<\/code> as \"<code>(art.<\/code>\".  Your script pads this into \"<code>\\\\b(art.\\\\b<\/code>\". The <code>gsub()<\/code> function tries to interpret this as a regular expression instead of a simple string, then throws an error because it is not a valid regular expression: it contains an opening parenthesis but no closing parenthesis. I believe that you actually did not want <code>gsub()<\/code> to interpret the input as a regular expression in the first place, and specifying <code>gsub(..., fixed=TRUE)<\/code> will correct that.<\/p>\n\n<p>I believe the reason why this error disappears when you add the sample\/partition module is because, by chance, the problematic input value was dropped on subsampling. I do not think it is an issue of available resources on Azure ML. (Caveat: I cannot confirm the fix works yet; I made the suggested update and started running the experiment, but it has not yet completed successfully.)<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":6722050.0,
        "Answerer_created_time":1468179475927,
        "Answerer_last_access_time":1660080085528,
        "Answerer_age":null,
        "Answerer_location":"Pasadena, CA, United States",
        "Answerer_reputation":795.0,
        "Answerer_views":210.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39251701",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1472676812496,
        "Question_original_content":"experi work subset dataset time ago written code meet memori issu tri split code differ code partial work remain think affect memori issu creat experi publish link modul consid sampl dataset work mean code suppos work correctli remov sampl code second modul start connect directli origin dataset follow situat produc follow error wai understand azur crash thank andrea",
        "Question_preprocessed_content":"experi work subset dataset time ago written code meet memori issu tri split code differ code partial work remain affect memori issu creat experi publish link modul consid sampl dataset work mean code suppos work correctli remov sampl code connect directli origin dataset follow situat produc follow error wai understand azur crash thank andrea",
        "Question_gpt_summary_original":"The user has encountered memory issues while working with AzureML. They tried to split the code into three different codes, which partially worked, but there is still a part that is affected by memory issues. The experiment created by the user works for a subset of the dataset, but not for the whole dataset, resulting in an error. The user is seeking help to understand where Azure crashes.",
        "Question_gpt_summary":"user encount memori issu work tri split code differ code partial work affect memori issu experi creat user work subset dataset dataset result error user seek help understand azur crash",
        "Answer_original_content":"thank publish exampl help understand issu suspect want modifi gsub call script ad argument fix true document function appear happen dataset subsampl dataset text wind includ name art script pad art gsub function tri interpret regular express instead simpl string throw error valid regular express contain open parenthesi close parenthesi believ actual want gsub interpret input regular express place specifi gsub fix true correct believ reason error disappear add sampl partit modul chanc problemat input valu drop subsampl think issu avail resourc caveat confirm fix work suggest updat start run experi complet successfulli",
        "Answer_preprocessed_content":"thank publish exampl help understand issu suspect want modifi call script ad argument appear happen dataset subsampl dataset text wind includ script pad function tri interpret regular express instead simpl string throw error valid regular express contain open parenthesi close parenthesi believ actual want interpret input regular express place specifi correct believ reason error disappear add modul chanc problemat input valu drop subsampl think issu avail resourc",
        "Answer_gpt_summary_original":"Solution:\n- Modify the `gsub()` calls in the script by adding the argument \"fixed=TRUE\" to each. This will prevent `gsub()` from interpreting the input as a regular expression and correct the error caused by an invalid regular expression.\n- The error disappears when adding the sample\/partition module because the problematic input value was dropped on subsampling, not because of available resources on Azure ML.",
        "Answer_gpt_summary":"solut modifi gsub call script ad argument fix true prevent gsub interpret input regular express correct error caus invalid regular express error disappear ad sampl partit modul problemat input valu drop subsampl avail resourc"
    },
    {
        "Question_title":"Why are shh keys lost on reboot of AWS ec2 instance (sage maker)?",
        "Question_body":"<p>I have an AWS SageMaker notebook running some ML stuff for work, and I have a private github repo with some of my commonly used functions which is formatted in such a way to be pip install-able, so I set up an SSH key by doing this:<\/p>\n<pre><code>ssh-keygen \n\n-t rsa -b 4096 -C &quot;danielwarfield1@gmail.com&quot;\n<\/code><\/pre>\n<p>enter, enter, enter (default save location no password)<\/p>\n<pre><code>eval $(ssh-agent -s)\nssh-add ~\/.ssh\/id_rs\n<\/code><\/pre>\n<p>then I copy the public key into github, then I run this to install my library<\/p>\n<pre><code>$PWD\/pip install git+ssh:\/\/git@github.com\/...\n<\/code><\/pre>\n<p>where <code>$PWD<\/code> is the directory containing pip for the conda env I'm using (tensorflow2_p36 specifically, the one that AWS provides)<\/p>\n<p>this works fine, until I restart the EC2, then it appears my shh key (along with all my other installs) are lost, and I have to repeat the process. I expect the modules to be lost, I know SageMaker manages the environments, but me loosing my ssh key seems peculiar, is there a place I can save my ssh key wher it wont get lost, but I can still find it when I pip install?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1596230563253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0,
        "Question_view_count":278.0,
        "Poster_id":10818367.0,
        "Poster_created_time":1545360696800,
        "Poster_last_access_time":1664070875743,
        "Poster_age":null,
        "Poster_location":"Earth",
        "Poster_reputation":1011.0,
        "Poster_views":93.0,
        "Answer_body":"<p>The <code>\/home\/ec2-user\/SageMaker<\/code> location is persisted even when you switch down the notebook instance, you can try saving things here to get them persisted. Things saved elsewhere will be lost when you switch off the instance<\/p>\n<p>Regarding private git integration, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html\" rel=\"nofollow noreferrer\">SageMaker git Notebook integration<\/a>, which uses Secrets Manager to safely handle your credentials<\/p>\n<p>You can perform steps automatically when the notebook starts with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>. This is useful for example to standardise and automatise copying of data and environment customization<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Answerer_id":5331834.0,
        "Answerer_created_time":1442180190107,
        "Answerer_last_access_time":1664048608876,
        "Answerer_age":null,
        "Answerer_location":null,
        "Answerer_reputation":3203.0,
        "Answerer_views":400.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63199239",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1596232881236,
        "Question_original_content":"shh kei lost reboot aw instanc sage maker notebook run stuff work privat github repo commonli function format wai pip instal abl set ssh kei ssh keygen rsa danielwarfield gmail com enter enter enter default save locat password eval ssh agent ssh add ssh copi public kei github run instal librari pwd pip instal git ssh git github com pwd directori contain pip conda env tensorflow specif aw provid work fine restart appear shh kei instal lost repeat process expect modul lost know manag environ loos ssh kei peculiar place save ssh kei wher wont lost pip instal",
        "Question_preprocessed_content":"shh kei lost reboot aw instanc notebook run stuff work privat github repo commonli function format wai pip set ssh kei enter enter enter copi public kei github run instal librari directori contain pip conda env work fine restart appear shh kei lost repeat process expect modul lost know manag environ loos ssh kei peculiar place save ssh kei wher wont lost pip instal",
        "Question_gpt_summary_original":"The user is encountering a challenge where their SSH key, along with all their other installs, are lost after restarting their AWS SageMaker notebook. They have to repeat the process of setting up the SSH key and installing their library every time they restart the EC2 instance. The user is looking for a solution to save their SSH key in a place where it won't get lost but can still be found when they pip install.",
        "Question_gpt_summary":"user encount challeng ssh kei instal lost restart notebook repeat process set ssh kei instal librari time restart instanc user look solut save ssh kei place won lost pip instal",
        "Answer_original_content":"home user locat persist switch notebook instanc try save thing persist thing save lost switch instanc privat git integr us git notebook integr us secret manag safe handl credenti perform step automat notebook start lifecycl configur us exampl standardis automatis copi data environ custom",
        "Answer_preprocessed_content":"locat persist switch notebook instanc try save thing persist thing save lost switch instanc privat git integr us git notebook integr us secret manag safe handl credenti perform step automat notebook start lifecycl configur us exampl standardis automatis copi data environ custom",
        "Answer_gpt_summary_original":"Solution:\n- Save the SSH key in the <code>\/home\/ec2-user\/SageMaker<\/code> location to persist it even when the notebook instance is switched off.\n- Use the SageMaker git Notebook integration, which uses Secrets Manager to safely handle private git integration.\n- Use a lifecycle configuration to perform steps automatically when the notebook starts, such as standardizing and automatizing copying of data and environment customization.",
        "Answer_gpt_summary":"solut save ssh kei home user locat persist notebook instanc switch us git notebook integr us secret manag safe handl privat git integr us lifecycl configur perform step automat notebook start standard automat copi data environ custom"
    }
]