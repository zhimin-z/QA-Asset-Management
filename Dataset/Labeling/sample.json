[
    {
        "Question_id":null,
        "Question_title":"I dont understand why my wandb_metadata.json file is showing this",
        "Question_body":"<p>Hello, i am pretty new to this wandb function. I have been trying to run a program with it, but it shows an error of like this:<br>\nTraceback (most recent call last):<br>\nFile \u201cmain.py\u201d, line 105, in <br>\nmain()<br>\nFile \u201cmain.py\u201d, line 99, in main<br>\ntrainer.train(start_iteration=epoch)<br>\nFile \u201c\/home\/cs2212\/Desktop\/voxel2mesh-master\/train.py\u201d, line 58, in train<br>\nloss = self.training_step(data, start_iteration)<br>\nFile \u201c\/home\/cs2212\/Desktop\/voxel2mesh-master\/train.py\u201d, line 22, in training_step<br>\nloss, log = self.net.loss(data, epoch)<br>\nFile \u201c\/home\/cs2212\/Desktop\/voxel2mesh-master\/model\/voxel2mesh.py\u201d, line 214, in loss<br>\npred_points = sample_points_from_meshes(pred_mesh, 3000)<br>\nFile \u201c\/home\/cs2212\/.local\/lib\/python3.8\/site-packages\/pytorch3d\/ops\/sample_points_from_meshes.py\u201d, line 55, in sample_points_from_meshes<br>\nareas, _ = mesh_face_areas_normals(<br>\nFile \u201c\/home\/cs2212\/.local\/lib\/python3.8\/site-packages\/pytorch3d\/ops\/mesh_face_areas_normals.py\u201d, line 44, in forward<br>\nareas, normals = _C.face_areas_normals_forward(verts, faces)<br>\nRuntimeError: Not compiled with GPU support. (FaceAreasNormalsForward at \/root\/project\/pytorch3d\/csrc\/face_areas_normals\/face_areas_normals.h:51)<\/p>\n<p>The preprocessing data job was done fine, but as I try to run the program with the preprocessed data, the upper error happens<\/p>\n<p>I checked the metadata.json file and realized the cuda was set as null even though i checked the cuda was there with nvcc --version. I am guessing the wandb not realizing the cuda is there seems to be an issue. Are there any methods of how i could solve this? Any advice is appreciated Thank you<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1649363599905,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":144.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/i-dont-understand-why-my-wandb-metadata-json-file-is-showing-this\/2199",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-07T21:26:49.968Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/harrykwon97\">@harrykwon97<\/a>,<\/p>\n<p>Looking at the traceback of this error, this issue does not seem to be originating from wandb. The wandb library does not communicate with CUDA directly, nor is CUDA nessecary for wandb to operate, so I would not expect it to break because of wandb.<\/p>\n<p>It seems like the error actually originates from PyTorch3D, which would make sense since it would need to communicate to CUDA directly. I would suggest checking compatibility of your CUDA version with PyTorch 3D as a first step.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-14T19:58:00.804Z",
                "Answer_body":"<p>\u200bHi <a class=\"mention\" href=\"\/u\/harrykwon97\">@harrykwon97<\/a>,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<\/p>\n<p>Weights &amp; Biases<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-19T19:27:44.968Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/harrykwon97\">@harrykwon97<\/a>, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>\n<p>\u200b<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-18T19:28:07.692Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"dont understand metadata json file show hello pretti new function try run program show error like traceback recent file main line main file main line main trainer train start iter epoch file home desktop voxelmesh master train line train loss self train step data start iter file home desktop voxelmesh master train line train step loss log self net loss data epoch file home desktop voxelmesh master model voxelmesh line loss pred point sampl point mesh pred mesh file home local lib python site packag pytorchd op sampl point mesh line sampl point mesh area mesh face area normal file home local lib python site packag pytorchd op mesh face area normal line forward area normal face area normal forward vert face runtimeerror compil gpu support faceareasnormalsforward root project pytorchd csrc face area normal face area normal preprocess data job fine try run program preprocess data upper error happen check metadata json file realiz cuda set null check cuda nvcc version guess realiz cuda issu method solv advic appreci thank",
        "Question_preprocessed_content":"dont understand file show hello pretti new function try run program show error like traceback file line main file line main file line train loss file line loss log epoch file line loss file line area file line forward area normal face runtimeerror compil gpu support preprocess data job fine try run program preprocess data upper error happen check file realiz cuda set null check cuda nvcc guess realiz cuda issu method solv advic appreci thank",
        "Answer_original_content":"harrykwon look traceback error issu origin librari commun cuda directli cuda nessecari oper expect break like error actual origin pytorchd sens need commun cuda directli suggest check compat cuda version pytorch step thank ramit harrykwon want follow support request heard let know assist issu resolv best harrykwon heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"look traceback error issu origin librari commun cuda directli cuda nessecari oper expect break like error actual origin pytorch sens need commun cuda directli suggest check compat cuda version pytorch step thank ramit want follow support request heard let know assist issu resolv best heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering an error with their _metadata.json file, which is preventing them from running their program due to an issue with recognizing the cuda support.",
        "Question_gpt_summary":"user encount error metadata json file prevent run program issu recogn cuda support",
        "Answer_gpt_summary_original":"the answer suggests that the error with the metadata.json file is not originating from the library and that cuda support is not necessary for the library to operate. the error seems to be originating from pytorch3d, so the user should check the compatibility of their cuda version with pytorch3d as a first step. there is no further solution provided in the answer.",
        "Answer_gpt_summary":"answer suggest error metadata json file origin librari cuda support necessari librari oper error origin pytorchd user check compat cuda version pytorchd step solut provid answer"
    },
    {
        "Question_id":null,
        "Question_title":"Best practice for applying pipelines to many datasets?",
        "Question_body":"<p>Hi folks,<\/p>\n<p>I have a data analysis problem which I want to use DVC to help me with. I have a series of many different datasets which each need to have an analysis stage applied to them. Each dataset is a folder with a specific set of (large) files, and the same python analysis code needs to be applied to each one.<\/p>\n<p>Rather than just run one monolithic script that loops over all the datasets, I would like to create a pipeline to help me handle this. I am imagining some sort of pipeline where the script gets applied to each dataset separately (and possibly in parallel).<\/p>\n<p>Is something like this even possible with dvc?<\/p>\n<p>Thanks,<\/p>\n<p>Steve<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1649258414789,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":150.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/best-practice-for-applying-pipelines-to-many-datasets\/1147",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-06T15:48:14.835Z",
                "Answer_body":"<p>Actually, I just found out about \u201cforeach\u201d stages in the documentation:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files?msclkid=6aa0e6cdb5c011ec8141cc26dddd4c38#foreach-stages\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon-32x32.png?v=dfbc4a93a926127fc4495e9d640409f8\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files?msclkid=6aa0e6cdb5c011ec8141cc26dddd4c38#foreach-stages\" target=\"_blank\" rel=\"noopener nofollow ugc\">Data Version Control \u00b7 DVC<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files?msclkid=6aa0e6cdb5c011ec8141cc26dddd4c38#foreach-stages\" target=\"_blank\" rel=\"noopener nofollow ugc\">Pipelines Files (dvc.yaml)<\/a><\/h3>\n\n  <p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>I am pretty sure that I can use this to solve my problem.<\/p>\n<p>Thanks!<\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-06T17:21:52.233Z",
                "Answer_body":"<p>One thing to note is that <code>foreach<\/code> does not currently support parallelization. There is an open issue discussing it <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5440\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">dvc.yaml: future of foreach stages \u00b7 Issue #5440 \u00b7 iterative\/dvc \u00b7 GitHub<\/a><\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-06T17:40:10.359Z",
                "Answer_body":"<p>Thanks for pointing that out. While parallelization would be awesome for my use case, even having the <code>foreach<\/code> loop capability in serial seems like it will work for me.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"best practic appli pipelin dataset folk data analysi problem want us help seri differ dataset need analysi stage appli dataset folder specif set larg file python analysi code need appli run monolith script loop dataset like creat pipelin help handl imagin sort pipelin script get appli dataset separ possibl parallel like possibl thank steve",
        "Question_preprocessed_content":"best practic appli pipelin dataset folk data analysi problem want us help seri differ dataset need analysi stage appli dataset folder specif set file python analysi code need appli run monolith script loop dataset like creat pipelin help handl imagin sort pipelin script get appli dataset separ like possibl thank steve",
        "Answer_original_content":"actual foreach stage document data version control pipelin file yaml open sourc version control data scienc machin learn project git like experi organ data model experi pretti sure us solv problem thank thing note foreach current support parallel open issu discuss yaml futur foreach stage issu iter github thank point parallel awesom us case have foreach loop capabl serial like work",
        "Answer_preprocessed_content":"actual foreach stage document data version control pipelin file version control data scienc machin learn project experi organ data model experi pretti sure us solv problem thank thing note current support parallel open issu discuss yaml futur foreach stage issu iter github thank point parallel awesom us case have loop capabl serial like work",
        "Question_gpt_summary_original":"the user steve is looking for best practices to apply a pipeline to many datasets in order to analyze large files.",
        "Question_gpt_summary":"user steve look best practic appli pipelin dataset order analyz larg file",
        "Answer_gpt_summary_original":"the answer suggests using foreach stages in the data version control pipeline files (.yaml) open-source version control system for data science and machine learning projects. the user acknowledges that foreach does not currently support parallelization, but even having the foreach loop capability in serial seems like it will work for them.",
        "Answer_gpt_summary":"answer suggest foreach stage data version control pipelin file yaml open sourc version control data scienc machin learn project user acknowledg foreach current support parallel have foreach loop capabl serial like work"
    },
    {
        "Question_id":58989610.0,
        "Question_title":"How to custom code an inference pipeline in AWS sagemaker?",
        "Question_body":"<p>I am building a time series usecase to automate the preprocess and retrain tasks.At first the data is preprocessed using numpy, pandas, statsmodels etc &amp; later a machine learning algorithm is applied to make predictions.\nThe reason for using inference pipeline is that it reuses the same preprocess code for training and inference. I have checked the examples given by AWS sagemaker team with spark and sci-kit learn. In both the examples they use a sci-kit learn container to fit &amp; transform their preprocess code. Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code? <\/p>\n\n<p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n\n<p><strong>Sources looked into:<\/strong><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a>\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1574408498230,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1186.0,
        "Answer_body":"<p>Apologies for the late response.<\/p>\n\n<p>Below is some documentation on inference pipelines:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html<\/a>\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html<\/a><\/p>\n\n<blockquote>\n  <p>Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code?<\/p>\n<\/blockquote>\n\n<p>Your container is an encapsulation of the environment needed for your custom code needed to run properly. Based on the requirements listed above, <code>numpy, pandas, statsmodels etc &amp; later a machine learning algorithm<\/code>, I would create a container if you wish to isolate your dependencies or modify an existing predefined SageMaker container, such as the scikit-learn one, and add your dependencies into that.<\/p>\n\n<blockquote>\n  <p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n<\/blockquote>\n\n<p>Unfortunately, the two example notebooks referenced above are the only examples utilizing inference pipelines. The biggest hurdle most likely is creating containers that fulfill the preprocessing and prediction task you are seeking and then combining those two together into the inference pipeline.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1579974118672,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58989610",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1575505485728,
        "Question_original_content":"custom code infer pipelin build time seri usecas autom preprocess retrain task data preprocess numpi panda statsmodel later machin learn algorithm appli predict reason infer pipelin reus preprocess code train infer check exampl given team spark sci kit learn exampl us sci kit learn contain fit transform preprocess code creat contain need us case sci kit learn code custom exampl pipelin help appreci sourc look http github com awslab amazon exampl tree master python sdk scikit learn infer pipelin http github com awslab amazon exampl tree master advanc function infer pipelin sparkml blazingtext dbpedia",
        "Question_preprocessed_content":"custom code infer pipelin build time seri usecas autom preprocess retrain data preprocess numpi panda statsmodel later machin learn algorithm appli predict reason infer pipelin reus preprocess code train infer check exampl given team spark learn exampl us learn contain fit transform preprocess code creat contain need us case code custom exampl pipelin help appreci sourc look",
        "Answer_original_content":"apolog late respons document infer pipelin http doc aw amazon com latest infer pipelin html http doc aw amazon com latest infer pipelin real time html creat contain need us case sci kit learn code contain encapsul environ need custom code need run properli base requir list numpi panda statsmodel later machin learn algorithm creat contain wish isol depend modifi exist predefin contain scikit learn add depend custom exampl pipelin help appreci unfortun exampl notebook referenc exampl util infer pipelin biggest hurdl like creat contain fulfil preprocess predict task seek combin infer pipelin",
        "Answer_preprocessed_content":"apolog late respons document infer pipelin creat contain need us case code contain encapsul environ need custom code need run properli base requir list creat contain wish isol depend modifi exist predefin contain add depend custom exampl pipelin help appreci unfortun exampl notebook referenc exampl util infer pipelin biggest hurdl like creat contain fulfil preprocess predict task seek combin infer pipelin",
        "Question_gpt_summary_original":"the user is encountering challenges in custom coding an inference pipeline for a time series use case, and is looking for a custom example to help them understand how to do this.",
        "Question_gpt_summary":"user encount challeng custom code infer pipelin time seri us case look custom exampl help understand",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- check the provided documentation on inference pipelines to understand how to create one.\n- create a container to encapsulate the environment needed for the custom code to run properly.\n- modify an existing predefined container, such as the scikit-learn one, and add the necessary dependencies.\n- there are no custom examples available for using these pipelines, but the provided documentation and example notebooks can be helpful.\n- the biggest hurdle is creating containers that fulfill the preprocessing and prediction task and then combining them into the inference pipeline.",
        "Answer_gpt_summary":"possibl solut answer check provid document infer pipelin understand creat creat contain encapsul environ need custom code run properli modifi exist predefin contain scikit learn add necessari depend custom exampl avail pipelin provid document exampl notebook help biggest hurdl creat contain fulfil preprocess predict task combin infer pipelin"
    },
    {
        "Question_id":null,
        "Question_title":"Getting OSError: [Errno 30] Read-only file system",
        "Question_body":"I am new to AzureML, I am trying to run the pipeline using parallelRunSteps and pipeline is getting submitted successfully but while running the pipeline it is throwing an above error not sure what would be the root cause of it.\n\nThe step I am following is\n\nCreating the workspace if does not exists\n\n\nFetching the datastore by specifying the storage account and other details\n\n\nUsing the from file dataset\n\n\nRegistering the dataset\n\n\nAfter registering fetching the dataset\n\n\nFetching\/Initialising Experiment\n\n\nFetching\/Initialising Environment\n\n\nAdding Private wheel file to pip package\n\n\nRegistering the packages to conda dependencies\n\n\nRegistering the Environment\n\n\nFetching\/Initialising the Compute Target\n\n\nInitialising the ParallelRunConfig\n\n\nInitialising the PipelineData as output data\n\n\nInitialising the ParallelRunStep\n\n\nFetching\/Initialising the Pipeline\n\n\nSubmitting the Pipeline\n\nThe above same technique I tried with different PythonScriptSteps instead of ParallelRunStep method.\n\nCreating the workspace if does not exists\n\n\nFetching the datastore by specifying the storage account and other details\n\n\nTabular Dataset\n\n\nsetting dataset name input\n\n\nFetching the Experiment\n\n\nFetching\/Initialising the Experiment\n\n\nFetching\/Initialising the Environment\n\n\nAdding Private wheel file to pip package\n\n\nRegistering the packages to conda dependencies\n\n\nRegistering the Environment\n\n\nFetching the ComputeTarget\n\n\nInitialising the PythonStepScript\n\n\nInitialising the Pipeline\n\n\nSubmitting the Pipeline\n\nWith PythonStepScripts it is working fine. Not able to understand what mistake I am doing while running ParallelRunStep method.",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1624894187850,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/454901\/getting-oserror-errno-30-read-only-file-system.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-06T08:12:22.683Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nHope your issue has been solved. We haven\u2019t heard from you on the last response and was just checking back to see if you have a resolution yet.\n\nThe workaround I have seen for the similar issue is to add \"tmp\" to the file path like filepath = '\/tmp\/' + key\n\nIn case if you have any resolution please do share that same with the community as it can be helpful to others . Please do let us know if you still have issue for it.\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-07-13T06:48:36.4Z",
                "Answer_score":0,
                "Answer_body":"Hi Yutong,\n\nSorry for the late reply was on leave.\n\nSharing the error message below\n\nTraceback (most recent call last):\nFile \"driver\/amlbi_main.py\", line 48, in <module>\nmain()\nFile \"driver\/amlbi_main.py\", line 44, in main\nJobStarter().start_job()\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/job_starter.py\", line 50, in start_job\nself.setup(is_master=True)\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/job_starter.py\", line 44, in setup\nLogConfig().config(args.logging_level, is_master=is_master)\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/singleton_meta.py\", line 18, in call\ncls.instances[cls] = super(SingletonMeta, cls).call(args, *kwargs)\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/log_config.py\", line 39, in init_\nself.log_dir = self.get_log_dir()\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/log_config.py\", line 48, in get_log_dir\nworking_dir = RunContextFactory.get_context().working_dir\nFile \"\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/wd\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/driver\/run_context.py\", line 64, in working_dir\npth.mkdir(parents=True, exist_ok=True)\nFile \"\/azureml-envs\/azureml_91e342c44c0de9bc46808411bb1fed8e\/lib\/python3.6\/pathlib.py\", line 1226, in mkdir\nself._accessor.mkdir(self, mode)\nFile \"\/azureml-envs\/azureml_91e342c44c0de9bc46808411bb1fed8e\/lib\/python3.6\/pathlib.py\", line 387, in wrapped\nreturn strfunc(str(pathobj), *args)\nOSError: [Errno 30] Read-only file system: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/gmail\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e\/mounts\/workspaceblobstore\/azureml\/68b3ef53-65a6-4d2f-a3ba-07af48d1081e'\n\nSorry not getting where to add file path like filepath = '\/tmp\/' + key can you some reference or example",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-19T21:21:49.037Z",
                "Answer_score":0,
                "Answer_body":"I am having the same issue\n\nI am creating a compute cluster and then mounting a Jupyter Lab mounted at the Workspace default datastore at this location:\n\n\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/{workspace_name}\/azureml\/{run_id.lower()}\/mounts\/\n\nI also uploaded Jupyter Notebooks to\n\n\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/{workspace_name}\/azureml\/{run_id.lower()}\/mounts\/workspaceblobstore\/\n\nI used to be able to run the Jupyter Notebooks and save the results on the mount, or being able to upload content using the Jupyter Lab, or duplicating or saving changes to the notebooks\n\nBut not anymore I am getting this error:\n\nUnexpected error while saving file: workspaceblobstore\/tao\/bpnet\/bpnet-Copy1.ipynb [Errno 30] Read-only file system: '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/ngc_aml_toolkit_ws_test2\/azureml\/tao-mrg-exp34_1634671669_282ea162\/mounts\/workspaceblobstore\/tao\/bpnet\/bpnet-Copy1.ipynb\n\nWhile trying to duplicate notebook bpnet.ipynb",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get oserror errno read file new try run pipelin parallelrunstep pipelin get submit successfulli run pipelin throw error sure root caus step follow creat workspac exist fetch datastor specifi storag account detail file dataset regist dataset regist fetch dataset fetch initialis experi fetch initialis environ ad privat wheel file pip packag regist packag conda depend regist environ fetch initialis comput target initialis parallelrunconfig initialis pipelinedata output data initialis parallelrunstep fetch initialis pipelin submit pipelin techniqu tri differ pythonscriptstep instead parallelrunstep method creat workspac exist fetch datastor specifi storag account detail tabular dataset set dataset input fetch experi fetch initialis experi fetch initialis environ ad privat wheel file pip packag regist packag conda depend regist environ fetch computetarget initialis pythonstepscript initialis pipelin submit pipelin pythonstepscript work fine abl understand mistak run parallelrunstep method",
        "Question_preprocessed_content":"get oserror file new try run pipelin parallelrunstep pipelin get submit successfulli run pipelin throw error sure root caus step follow creat workspac exist fetch datastor specifi storag account detail file dataset regist dataset regist fetch dataset experi environ ad privat wheel file pip packag regist packag conda depend regist environ comput target initialis parallelrunconfig initialis pipelinedata output data initialis parallelrunstep pipelin submit pipelin techniqu tri differ pythonscriptstep instead parallelrunstep method creat workspac exist fetch datastor specifi storag account detail tabular dataset set dataset input fetch experi experi environ ad privat wheel file pip packag regist packag conda depend regist environ fetch computetarget initialis pythonstepscript initialis pipelin submit pipelin pythonstepscript work fine abl understand mistak run parallelrunstep method",
        "Answer_original_content":"hello hope issu solv havent heard respons check resolut workaround seen similar issu add tmp file path like filepath tmp kei case resolut share commun help let know issu regard yutong yutong sorri late repli leav share error messag traceback recent file driver amlbi main line main file driver amlbi main line main jobstart start job file mnt batch task share root job gmail bef aba afd bef aba afd driver job starter line start job self setup master true file mnt batch task share root job gmail bef aba afd bef aba afd driver job starter line setup logconfig config arg log level master master file mnt batch task share root job gmail bef aba afd bef aba afd driver singleton meta line cl instanc cl super singletonmeta cl arg kwarg file mnt batch task share root job gmail bef aba afd bef aba afd driver log config line init self log dir self log dir file mnt batch task share root job gmail bef aba afd bef aba afd driver log config line log dir work dir runcontextfactori context work dir file mnt batch task share root job gmail bef aba afd bef aba afd driver run context line work dir pth mkdir parent true exist true file env eccdebcbbfed lib python pathlib line mkdir self accessor mkdir self mode file env eccdebcbbfed lib python pathlib line wrap return strfunc str pathobj arg oserror errno read file mnt batch task share root job gmail bef aba afd mount workspaceblobstor bef aba afd sorri get add file path like filepath tmp kei refer exampl have issu creat comput cluster mount jupyt lab mount workspac default datastor locat mnt batch task share root job workspac run lower mount upload jupyt notebook mnt batch task share root job workspac run lower mount workspaceblobstor abl run jupyt notebook save result mount abl upload content jupyt lab duplic save chang notebook anymor get error unexpect error save file workspaceblobstor tao bpnet bpnet copi ipynb errno read file mnt batch task share root job ngc aml toolkit test tao mrg exp mount workspaceblobstor tao bpnet bpnet copi ipynb try duplic notebook bpnet ipynb",
        "Answer_preprocessed_content":"hello hope issu solv havent heard respons check resolut workaround seen similar issu add tmp file path like filepath kei case resolut share commun help let know issu regard yutong yutong sorri late repli leav share error messag traceback file line main file line main file line file line setup file line super file line file line file line file line mkdir mode file line wrap return strfunc arg oserror file sorri get add file path like filepath kei refer exampl have issu creat comput cluster mount jupyt lab mount workspac default datastor locat upload jupyt notebook abl run jupyt notebook save result mount abl upload content jupyt lab duplic save chang notebook anymor get error unexpect error save file file try duplic notebook",
        "Question_gpt_summary_original":"the user is encountering an oserror when attempting to run a pipeline using parallelrunsteps, despite the pipeline being successfully submitted.",
        "Question_gpt_summary":"user encount oserror attempt run pipelin parallelrunstep despit pipelin successfulli submit",
        "Answer_gpt_summary_original":"the answer suggests adding \"tmp\" to the file path like filepath = '\/tmp\/' + key as a workaround for a similar issue. however, the context of the question is not clear, and there is no specific solution provided for the oserror encountered when attempting to run a pipeline using parallelrunsteps.",
        "Answer_gpt_summary":"answer suggest ad tmp file path like filepath tmp kei workaround similar issu context question clear specif solut provid oserror encount attempt run pipelin parallelrunstep"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning Studios Deploy Model - Obtain logs",
        "Question_body":"I am attempting to deploy a model to an Endpoint in MS Azure Machine Learning studio but I get some errror.\n\n2 Questions\n1) How can we get the logs for a deployed model (See my attempts below)\n2) How do models get loaded into the docker images as I suspect for some reason it didn't get copied into the docker image.\n\n\n\n\nStep 1) Upload the model: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-existing-model\n\n\nWhich is successful. I can see and download that my model is correct.\n\nStep 2) Deploy to Azure Container Instances: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-container-instance\n\nI use this code snippet to deploy my code\n\n from azureml.core.model import InferenceConfig\n from azureml.core.webservice import AciWebservice\n from azureml.core.webservice import Webservice\n from azureml.core.model import Model\n from azureml.core.environment import Environment\n    \n script_file_name = 'inference\/score.py'\n    \n inference_config = InferenceConfig(entry_script=script_file_name, environment=tf_env)\n    \n aciconfig = AciWebservice.deploy_configuration(cpu_cores = 1, \n                                                memory_gb = 1, \n                                                tags = {'iris': \"rh1832\", 'type': \"sklearn\"}, \n                                                description = 'sample service for iris')\n    \n aci_service_name = 'rh1832-iris-demo'\n print(aci_service_name)\n aci_service = Model.deploy(ws, aci_service_name, [model], inference_config, aciconfig)\n aci_service.wait_for_deployment(True)\n print(aci_service.state)\n\n\n\nAnd my score.py inference script looks like\n\n # ---------------------------------------------------------\n # Copyright (c) Microsoft Corporation. All rights reserved.\n # ---------------------------------------------------------\n import json\n import logging\n import os\n import pickle\n import numpy as np\n import pandas as pd\n import joblib\n    \n try:\n     logger = logging.getLogger('azureml.automl.core.scoring_script')\n except:\n     pass\n    \n    \n def init():\n     global model\n     model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'sklearn_model')\n     path = os.path.normpath(model_path)\n     path_split = path.split(os.sep)\n     logger.info(\"Loading model from path.\")\n     model = joblib.load(model_path)\n     logger.info(\"Loading successful.\")\n    \n    \n def run(data):\n     try:\n         target_names = ['database', 'network', 'resource']\n         result = model.predict([data])\n         return json.dumps(target_names[result[0]])\n     except Exception as e:\n         print(\"error: \" + str(e))\n         result = str(e)\n         return json.dumps({\"error\": result})\n\n\n\n\nAfter submitting the webservice to be deployed i get an error that just says its in crashloopbackoff state with no details for which part of the script failed.\n\n\nStep 3) I attempted to debug the logs by follow this guide: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment?tabs=azcli#dockerlog\n\n1) Running script locally which I was able to successfully run score.py\n2) Running docker image locally I didn't see the model in the folder azureml-environment-setup in the docker container:\n\nroot@09e7230839e3:\/azureml-environment-setup# ls\nenvironment_context.json log4j.properties mutated_conda_dependencies.yml send_conda_dependencies.py spark_cache.py\n\nand also i didn't see the entrypoint for running the docker container. Is there any guide for running the docker image locally?\n\n3) Finally I attempted to get the logs using Azure CLI and it only return an Null list it appeared",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618204695277,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/353046\/azure-machine-learning-studios-deploy-model-obtain.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-12T08:15:59.617Z",
                "Answer_score":0,
                "Answer_body":"You can try https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-debug-visual-studio-code#debug-and-troubleshoot-deployments",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"studio deploi model obtain log attempt deploi model endpoint studio eor question log deploi model attempt model load docker imag suspect reason copi docker imag step upload model http doc microsoft com azur machin learn deploi exist model success download model correct step deploi azur contain instanc http doc microsoft com azur machin learn deploi azur contain instanc us code snippet deploi code core model import inferenceconfig core webservic import aciwebservic core webservic import webservic core model import model core environ import environ script file infer score infer config inferenceconfig entri script script file environ env aciconfig aciwebservic deploi configur cpu core memori tag iri type sklearn descript sampl servic iri aci servic iri demo print aci servic aci servic model deploi aci servic model infer config aciconfig aci servic wait deploy true print aci servic state score infer script look like copyright microsoft corpor right reserv import json import log import import pickl import numpi import panda import joblib try logger log getlogg automl core score script pass def init global model model path path join getenv model dir sklearn model path path normpath model path path split path split sep logger info load model path model joblib load model path logger info load success def run data try target name databas network resourc result model predict data return json dump target name result except print error str result str return json dump error result submit webservic deploi error sai crashloopbackoff state detail script fail step attempt debug log follow guid http doc microsoft com azur machin learn troubleshoot deploy tab azcli dockerlog run script local abl successfulli run score run docker imag local model folder environ setup docker contain root environ setup environ context json logj properti mutat conda depend yml send conda depend spark cach entrypoint run docker contain guid run docker imag local final attempt log azur cli return null list appear",
        "Question_preprocessed_content":"studio deploi model obtain log attempt deploi model endpoint studio eor question log deploi model model load docker imag suspect reason copi docker imag step upload model success download model correct step deploi azur contain instanc us code snippet deploi code import inferenceconfig import aciwebservic import webservic import model import environ aciconfig tag descript sampl servic iri aciconfig infer script look like copyright microsoft corpor right reserv import json import log import import pickl import numpi import panda import joblib try logger pass def init global model path model model def run try result return except print result str return result submit webservic deploi error sai crashloopbackoff state detail script fail step attempt debug log follow guid run script local abl successfulli run run docker imag local model folder docker contain entrypoint run docker contain guid run docker imag local final attempt log azur cli return null list appear",
        "Answer_original_content":"try http doc microsoft com azur machin learn debug visual studio code debug troubleshoot deploy",
        "Answer_preprocessed_content":"try",
        "Question_gpt_summary_original":"the user is encountering challenges deploying a model to an endpoint in ms studio, including difficulty obtaining logs, loading models into docker images, and running the docker image locally.",
        "Question_gpt_summary":"user encount challeng deploi model endpoint studio includ difficulti obtain log load model docker imag run docker imag local",
        "Answer_gpt_summary_original":"the possible solution to the challenges faced by the user in deploying a model to an endpoint in ms studio, including difficulty obtaining logs, loading models into docker images, and running the docker image locally, is to refer to the microsoft documentation on how to debug and troubleshoot deployments using visual studio code.",
        "Answer_gpt_summary":"possibl solut challeng face user deploi model endpoint studio includ difficulti obtain log load model docker imag run docker imag local refer microsoft document debug troubleshoot deploy visual studio code"
    },
    {
        "Question_id":null,
        "Question_title":"Logging Date Objects",
        "Question_body":"<p>Hey everyone!<\/p>\n<p>I\u2019m looking for a way to log Dates. I want to show the evolution of our labeled data over time over non-uniform time steps. To make it more clear, let\u2019s say I want to display the amount of data on arbitrary days. If I ran my W&amp;B run on those days, I could distinguish them by <code>run:createdAt<\/code>. The plots could display them as dates as one would expect and everything is fine.  Now, this fails as soon as I want to have a starting date AND an end date. Therefore I\u2019m looking to log date data.<\/p>\n<p>The functionality should be there, as it is for <code>createdAt<\/code>,  but I can\u2019t figure out how to log my own. I couldn\u2019t find a suitable object in the docs and neither POSIX timestamp nor iso format\/datetime objects work out of the box.<\/p>\n<p>Is there no way to do this, or did I overlook something?<\/p>\n<p>As a workaround I could just use the POSIX timestamp as scale, but I guess we all agree that\u2019s a little unwieldy.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1650387630533,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":335.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-date-objects\/2263",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-19T22:41:18.450Z",
                "Answer_body":"<p>Hi Thomas,<\/p>\n<p>I looked into this and it looks like there is no good way to upload date data currently. I agree that this is definitely something we want to support, I\u2019ll create a feature request for this and keep you updated on the progress of this feature.<\/p>\n<p>For now, a POSIX timestamp is the only way to upload this data.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":26.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-06T19:39:38.767Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tsteffek\">@tsteffek<\/a>,<\/p>\n<p>Here is one way to do what you want to acheive, through our Tables:<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7204b693004bf73705dcc6bae6452a1dc1a9312f.gif\" alt=\"Timeseries\" data-base62-sha1=\"ggEwPGiUOZrkxn985dV6lRnr4AL\" width=\"690\" height=\"401\" class=\"animated\"><\/p>\n<p>Let me know if this helps with what you are looking for.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":21.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-10T18:37:41.399Z",
                "Answer_body":"<p>Hi Thomas,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-05T19:40:35.412Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"log date object hei look wai log date want evolut label data time non uniform time step clear let want displai data arbitrari dai ran run dai distinguish run createdat plot displai date expect fine fail soon want start date end date look log date data function createdat figur log suitabl object doc posix timestamp iso format datetim object work box wai overlook workaround us posix timestamp scale guess agre that littl unwieldi",
        "Question_preprocessed_content":"log date object hei look wai log date want evolut label data time time step clear let want displai data arbitrari dai ran run dai distinguish plot displai date expect fine fail soon want start date end date look log date data function figur log suitabl object doc posix timestamp iso object work box wai overlook workaround us posix timestamp scale guess agre that littl unwieldi",
        "Answer_original_content":"thoma look look like good wai upload date data current agre definit want support ill creat featur request updat progress featur posix timestamp wai upload data thank ramit tsteffek wai want acheiv tabl let know help look thank ramit thoma want follow support request heard let know assist issu resolv best topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thoma look look like good wai upload date data current agre definit want support ill creat featur request updat progress featur posix timestamp wai upload data thank ramit wai want acheiv tabl let know help look thank ramit thoma want follow support request heard let know assist issu resolv best topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking for a way to log date data in order to display the evolution of labeled data over non-uniform time steps, but is having difficulty finding a suitable object in the documentation and neither posix timestamp nor iso format\/datetime objects are working.",
        "Question_gpt_summary":"user look wai log date data order displai evolut label data non uniform time step have difficulti find suitabl object document posix timestamp iso format datetim object work",
        "Answer_gpt_summary_original":"there is currently no good way to upload date data for displaying the evolution of labeled data over non-uniform time steps. the support team will create a feature request for this and keep the user updated on its progress. for now, a posix timestamp is the only way to upload this data. the support team also suggests using tables as a possible solution, but it is unclear how this would help with the user's specific issue.",
        "Answer_gpt_summary":"current good wai upload date data displai evolut label data non uniform time step support team creat featur request user updat progress posix timestamp wai upload data support team suggest tabl possibl solut unclear help user specif issu"
    },
    {
        "Question_id":null,
        "Question_title":"CloudFormation with SageMaker LifeCycleConfig without leaving the instance running",
        "Question_body":"I have a use case with SageMaker in which I want to create a notebook instance using CloudFormation. I have some initialization to do at creation time (clone a github repo, etc.). That all works fine. The only problem is that I would like to do this ahead of time in a set of accounts, and there doesn't appear to be any way to leave the newly-created instance in a Stopped state. A property in the CFT would be helpful in this regard.\n\nI tried using the aws cli to stop the instance from the lifecycle create script, but that fails as shown in the resulting CloudWatch logs:\n\nAn error occurred (ValidationException) when calling the StopNotebookInstance operation: Status (Pending) not in ([InService]). Unable to transition to (Stopping) for Notebook Instance (arn:aws:sagemaker:us-east-1:147561847539:notebook-instance\/birdclassificationworkshop).\n\n\n\nInterestingly, when I interactively open a notebook instance, open a terminal in the instance, and execute a \"stop-notebook-instance\" command, SageMaker is happy to oblige. I would have thought it would let me do the same in the lifecycle config. Unfortunately, SageMaker still has the notebook in the Pending state at that point, so \"stop\" is not permitted.\n\nAre there other hooks or creative options anyone can provide for me?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539775195000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":306.0,
        "Answer_body":"One solutions will be to create a CFN custom resource backed by lambda. You can configure to run this resource only when the notebook resource completed. and use the lambda function to stop the notebook using one of our SDKs.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU43NLxohAQvmSL3aH-KpPaw\/cloud-formation-with-sage-maker-life-cycle-config-without-leaving-the-instance-running",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-10-17T11:53:11.000Z",
                "Answer_score":0,
                "Answer_body":"One solutions will be to create a CFN custom resource backed by lambda. You can configure to run this resource only when the notebook resource completed. and use the lambda function to stop the notebook using one of our SDKs.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1539777191000,
        "Question_original_content":"cloudform lifecycleconfig leav instanc run us case want creat notebook instanc cloudform initi creation time clone github repo work fine problem like ahead time set account appear wai leav newli creat instanc stop state properti cft help regard tri aw cli stop instanc lifecycl creat script fail shown result cloudwatch log error occur validationexcept call stopnotebookinst oper statu pend inservic unabl transit stop notebook instanc arn aw east notebook instanc birdclassificationworkshop interestingli interact open notebook instanc open termin instanc execut stop notebook instanc command happi oblig thought let lifecycl config unfortun notebook pend state point stop permit hook creativ option provid",
        "Question_preprocessed_content":"cloudform lifecycleconfig leav instanc run us case want creat notebook instanc cloudform initi creation time work fine problem like ahead time set account appear wai leav instanc stop state properti cft help regard tri aw cli stop instanc lifecycl creat script fail shown result cloudwatch log error occur call stopnotebookinst oper statu unabl transit notebook instanc interestingli interact open notebook instanc open termin instanc execut command happi oblig thought let lifecycl config unfortun notebook pend state point stop permit hook creativ option provid",
        "Answer_original_content":"solut creat cfn custom resourc back lambda configur run resourc notebook resourc complet us lambda function stop notebook sdk",
        "Answer_preprocessed_content":"solut creat cfn custom resourc back lambda configur run resourc notebook resourc complet us lambda function stop notebook sdk",
        "Question_gpt_summary_original":"the user is encountering a challenge with cloudformation and lifecycleconfig where they are unable to leave the newly-created instance in a stopped state, and the aws cli fails when attempting to stop the instance from the lifecycle create script.",
        "Question_gpt_summary":"user encount challeng cloudform lifecycleconfig unabl leav newli creat instanc stop state aw cli fail attempt stop instanc lifecycl creat script",
        "Answer_gpt_summary_original":"one possible solution to the challenge of not being able to leave a newly-created instance in a stopped state with cloudformation and lifecycleconfig is to create a custom resource backed by lambda. this resource can be configured to run only when the notebook resource has completed, and the lambda function can be used to stop the notebook using one of aws's sdks.",
        "Answer_gpt_summary":"possibl solut challeng abl leav newli creat instanc stop state cloudform lifecycleconfig creat custom resourc back lambda resourc configur run notebook resourc complet lambda function stop notebook aw sdk"
    },
    {
        "Question_id":null,
        "Question_title":"Can I connect a Sagemaker \"Studio\" instance to a private github repository?",
        "Question_body":"I've successfully connected a Sagemaker \"notebook\" to a private github repository, but wondering if it isn't possible for a studio instance? Failing that is there an easy way to get the remote codecommit git url for an existing \"studio\" instance so that code there can at least be pulled to my local machine?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656504273482,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":74.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUH33ZXpiAQ_aV2TesXBNOBw\/can-i-connect-a-sagemaker-studio-instance-to-a-private-github-repository",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-30T10:17:37.550Z",
                "Answer_score":1,
                "Answer_body":"Hi tansaku!\n\nFor sure, SageMaker Studio is integrated with Git, so you can connect to both your public and private repositories!\n\nWhen you try to connect to a private repository, you will be asked to enter your username and password. Best practices here are to create a personal access token instead of using your password.\n\nIf you want to cache your credentials avoiding typing them each time you interact with the Github server, you can cache them or store them in the home folder with the following command run from the System Terminal:\n\n$ git config --global git credential.helper [cache|store]\n\n\nIf you choose store to store your credentials, it will be written to the ~\/.git-credentials file located in your home folder. The cache parameter stores credentials in memory and never on disk. It also accepts the --timeout <seconds> option, which changes the amount of time the daemon is kept running (default is \"900\" or 15 minutes).\n\nOnce it executes the command, the next time it pushes it will ask for the credentials and store them, after that it shouldn't ask again.\n\nHope this helps!",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"connect studio instanc privat github repositori successfulli connect notebook privat github repositori wonder isn possibl studio instanc fail easi wai remot codecommit git url exist studio instanc code pull local machin",
        "Question_preprocessed_content":"connect studio instanc privat github repositori successfulli connect notebook privat github repositori wonder isn possibl studio instanc fail easi wai remot codecommit git url exist studio instanc code pull local machin",
        "Answer_original_content":"tansaku sure studio integr git connect public privat repositori try connect privat repositori ask enter usernam password best practic creat person access token instead password want cach credenti avoid type time interact github server cach store home folder follow command run termin git config global git credenti helper cach store choos store store credenti written git credenti file locat home folder cach paramet store credenti memori disk accept timeout option chang time daemon kept run default minut execut command time push ask credenti store shouldn ask hope help",
        "Answer_preprocessed_content":"tansaku sure studio integr git connect public privat repositori try connect privat repositori ask enter usernam password best practic creat person access token instead password want cach credenti avoid type time interact github server cach store home folder follow command run termin git config git choos store store credenti written file locat home folder cach paramet store credenti memori disk accept option chang time daemon kept run execut command time push ask credenti store shouldn ask hope help",
        "Question_gpt_summary_original":"the user is trying to connect a \"studio\" instance to a private github repository, and is wondering if there is an easy way to get the remote codecommit git url for an existing \"studio\" instance.",
        "Question_gpt_summary":"user try connect studio instanc privat github repositori wonder easi wai remot codecommit git url exist studio instanc",
        "Answer_gpt_summary_original":"the answer suggests that the user can connect to both public and private repositories using the \"studio\" instance. when connecting to a private repository, the user will be prompted to enter their username and password. it is recommended to create a personal access token instead of using a password. the answer also provides instructions on how to cache or store credentials to avoid typing them each time the user interacts with the github server. the user can use the \"git config\" command to store credentials in the home folder. the answer suggests using the \"cache\" parameter to store credentials in memory or the \"store\" parameter to store them in the ~\/.git-credentials file.",
        "Answer_gpt_summary":"answer suggest user connect public privat repositori studio instanc connect privat repositori user prompt enter usernam password recommend creat person access token instead password answer provid instruct cach store credenti avoid type time user interact github server user us git config command store credenti home folder answer suggest cach paramet store credenti memori store paramet store git credenti file"
    },
    {
        "Question_id":null,
        "Question_title":"Running jobs show as errors on cluster",
        "Question_body":"<p>Not sure if this is a bug or a feature request, but here is the issue: I stage a bunch of jobs on computer 1, part of a cluster with a shared file system. When I call <code>guild runs<\/code> on any computer in the cluster, it will properly show all staged files. If I start a queue on computer 1 it will promptly launch the first staged job:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/48c319be5e838f51786c86a2f2ff3699552b8cdd.png\" data-download-href=\"\/uploads\/short-url\/anGnutFiNKGSAB3mEKYpbp9fAgR.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/48c319be5e838f51786c86a2f2ff3699552b8cdd.png\" alt=\"image\" data-base62-sha1=\"anGnutFiNKGSAB3mEKYpbp9fAgR\" width=\"690\" height=\"15\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/48c319be5e838f51786c86a2f2ff3699552b8cdd_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1095\u00d725 1.94 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>If I then log into computer 2, the running job will show as error:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/9464fe7e0aaa6cccbf30dbc317d508cb5ec6718f.png\" data-download-href=\"\/uploads\/short-url\/laL2L6avIgccXVOeb1x78OHbgIf.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/9464fe7e0aaa6cccbf30dbc317d508cb5ec6718f.png\" alt=\"image\" data-base62-sha1=\"laL2L6avIgccXVOeb1x78OHbgIf\" width=\"690\" height=\"25\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/9464fe7e0aaa6cccbf30dbc317d508cb5ec6718f_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1158\u00d743 3.29 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Why would it show as error instead of just \u201crunning\u201d?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1628278406504,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":262.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/running-jobs-show-as-errors-on-cluster\/746",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-14T16:34:14.284Z",
                "Answer_body":"<p>Guild determines if a run is \u201crunning\u201d by looking at the pid file and checking for a running pid. There are two major problems here:<\/p>\n<ul>\n<li>A run terminated in a way that leaves its pid file (e.g. power loss, SIGKILL, etc) can later show up as \u201crunning\u201d when that pid is recycled<\/li>\n<li>When checking status on a shared file system, the local process sees a different list of pids<\/li>\n<\/ul>\n<p>Guild needs to differentiate remote runs from local when checking this status. To get actual bona fide status, Guild needs an interface to the remote system, which it has, but this requires a <code>guild pull<\/code> or <code>guild sync<\/code> op before running <code>guild runs<\/code>.<\/p>\n<p>We\u2019ll take a look at this for either 0.7.5 or 0.8. Thanks for the report and sorry for the super long response time!<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-15T18:05:50.123Z",
                "Answer_body":"<p>No worries, and thanks for the explanation!<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run job error cluster sure bug featur request issu stage bunch job cluster share file run cluster properli stage file start queue promptli launch stage job imag log run job error imag error instead run",
        "Question_preprocessed_content":"run job error cluster sure bug featur request issu stage bunch job cluster share file cluster properli stage file start queue promptli launch stage job imag log run job error imag error instead run",
        "Answer_original_content":"determin run run look pid file check run pid major problem run termin wai leav pid file power loss sigkil later run pid recycl check statu share file local process see differ list pid need differenti remot run local check statu actual bona fide statu need interfac remot requir pull sync run run look thank report sorri super long respons time worri thank explan",
        "Answer_preprocessed_content":"determin run run look pid file check run pid major problem run termin wai leav pid file later run pid recycl check statu share file local process see differ list pid need differenti remot run local check statu actual bona fide statu need interfac remot requir run look thank report sorri super long respons time worri thank explan",
        "Question_gpt_summary_original":"the user is encountering an issue where jobs that are running on one computer in a cluster are showing as errors on another computer in the same cluster.",
        "Question_gpt_summary":"user encount issu job run cluster show error cluster",
        "Answer_gpt_summary_original":"possible solutions to the issue of jobs showing as errors on another computer in the same cluster are: 1) guild needs to differentiate remote runs from local when checking the status of a run, and 2) guild needs an interface to the remote system to get actual status, which requires a guild pull or guild sync operation before running guild runs. the guild team will look into this for either version 0.7.5 or 0.8.",
        "Answer_gpt_summary":"possibl solut issu job show error cluster need differenti remot run local check statu run need interfac remot actual statu requir pull sync oper run run team look version"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Studio is bugged out and can not create a Microsoft ticket under MSDN. Need a few suggestions",
        "Question_body":"ML studio is, by default picking up Python 3.6 kernel, even when I'm specifying use Python 3.8 AzureML kernel. In UI, it's changed but not actually.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647349970220,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. It looks like the command you ran isn't supported. A better command to test kernel changes is shown below:\n\n from platform import python_version\n print(python_version())\n\nHope this helps!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/772790\/azure-ml-studio-is-bugged-out-and-can-not-create-a.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-16T17:18:38.727Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. It looks like the command you ran isn't supported. A better command to test kernel changes is shown below:\n\n from platform import python_version\n print(python_version())\n\nHope this helps!",
                "Answer_comment_count":6,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1647451118727,
        "Question_original_content":"studio bug creat microsoft ticket msdn need suggest studio default pick python kernel specifi us python kernel chang actual",
        "Question_preprocessed_content":"studio bug creat microsoft ticket msdn need suggest studio default pick python kernel specifi us python kernel chang actual",
        "Answer_original_content":"thank reach look like command ran isn support better command test kernel chang shown platform import python version print python version hope help",
        "Answer_preprocessed_content":"thank reach look like command ran isn support better command test kernel chang shown platform import hope help",
        "Question_gpt_summary_original":"the user is encountering an issue with microsoft ml studio where it is defaulting to python 3.6 kernel even when they are specifying to use python 3.8 kernel.",
        "Question_gpt_summary":"user encount issu microsoft studio default python kernel specifi us python kernel",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer to the issue of microsoft ml studio defaulting to python 3.6 kernel instead of python 3.8 kernel. however, the answer suggests a better command to test kernel changes, which is \"from platform import python_version print(python_version())\".",
        "Answer_gpt_summary":"specif solut provid answer issu microsoft studio default python kernel instead python kernel answer suggest better command test kernel chang platform import python version print python version"
    },
    {
        "Question_id":null,
        "Question_title":"Equation from linear regression model",
        "Question_body":"Hello,\n\nI have run a linear regression model considering several variables applying tune model hyperparameters in design. I am now interested in obtaining the equation of this regression model that considers the coefficients of the inputs of the model, but I cannot find how to get this result.\nPlease let me know how I might accomplish this.\n\nThank you!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1634735454220,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/597388\/equation-from-linear-regression-model.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-21T10:09:05.433Z",
                "Answer_score":0,
                "Answer_body":"@CASTANOSANCHEZMariaJoseA-3582 Thanks for the question. Can you please add more details about the sample that you are trying. Currently do not have coefficients for regression\/forecasting models, but we will raise this with our data science team as we have seen this ask before. We are working on an interface to surface models that compose ensembles, model weights and more. some of this information is available today within the model details tags.\n\nLink to find the best model: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters#find-the-best-model",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"equat linear regress model hello run linear regress model consid variabl appli tune model hyperparamet design interest obtain equat regress model consid coeffici input model result let know accomplish thank",
        "Question_preprocessed_content":"equat linear regress model hello run linear regress model consid variabl appli tune model hyperparamet design interest obtain equat regress model consid coeffici input model result let know accomplish thank",
        "Answer_original_content":"castanosanchezmariajosea thank question add detail sampl try current coeffici regress forecast model rais data scienc team seen ask work interfac surfac model compos ensembl model weight inform avail todai model detail tag link best model http doc microsoft com azur machin learn tune hyperparamet best model",
        "Answer_preprocessed_content":"thank question add detail sampl try current coeffici model rais data scienc team seen ask work interfac surfac model compos ensembl model weight inform avail todai model detail tag link best model",
        "Question_gpt_summary_original":"the user is interested in obtaining the equation of a linear regression model that considers the coefficients of the inputs of the model, but is unsure of how to do so.",
        "Question_gpt_summary":"user interest obtain equat linear regress model consid coeffici input model unsur",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer. however, the answer suggests that the user can find the best model by using the link provided. additionally, the answer mentions that the data science team will be informed about the user's request for coefficients for regression\/forecasting models.",
        "Answer_gpt_summary":"specif solut provid answer answer suggest user best model link provid addition answer mention data scienc team inform user request coeffici regress forecast model"
    },
    {
        "Question_id":64164367.0,
        "Question_title":"Nginx authentication issues when building mlflow through docker-compose",
        "Question_body":"<p>I'm trying to dockerize <a href=\"https:\/\/mlflow.org\/\" rel=\"nofollow noreferrer\">mlflow<\/a> with PostgreSQL and nginx configurations for Google Cloud Run (GCR) on the Google Cloud Platform (GCP).<\/p>\n<p>Before deploying anything to GCP however, I wanted to get a local deployment working. I found <a href=\"https:\/\/towardsdatascience.com\/deploy-mlflow-with-docker-compose-8059f16b6039\" rel=\"nofollow noreferrer\">this<\/a> guide that details the process of setting up the environment. Having followed the guide (excluding the SQL part), I can see the  mlflow UI on <code>localhost:80<\/code> as nginx redirects traffic on port 80 to 5000. To add authentication, I found <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-set-up-password-authentication-with-nginx-on-ubuntu-14-04\" rel=\"nofollow noreferrer\">here<\/a> that I can do it using <code>sudo htpasswd -c .htpasswd &lt;username&gt;<\/code> in the <code>etc\/nginx\/<\/code> directory and then adding<\/p>\n<pre><code>location \\ {\n   auth_basic &quot;Private Property&quot;;\n   auth_basic_user_file .htpasswd;\n}\n<\/code><\/pre>\n<p>to the <code>nginx.conf<\/code> (or <code>mlflow.conf<\/code> in this case) to make it appear online. Trouble is, when I go to <code>localhost:80<\/code> <em>now<\/em> and enter in my username\/password, I continue to see<\/p>\n<pre><code>[error] 6#6: *1 open() &quot;\/etc\/nginx\/.htpasswd&quot; failed (2: No such file or directory)\n<\/code><\/pre>\n<p>in the <code>docker-compose up<\/code> logs as they are printed to the terminal, and as such <em>I'm not able to see the mlflow UI<\/em> on <code>localhost:80<\/code> (either a blank screen or nginx 403 error).<\/p>\n<p>Now, I've looked at several other posts (such as <a href=\"https:\/\/stackoverflow.com\/questions\/2010677\/nginx-and-auth-basic\">this one<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/16510374\/403-forbidden-nginx-using-correct-credentials\">this one<\/a>) and it seems to me that nginx doesn't have the right permissions to read the <code>.htpasswd<\/code> in the <code>etc\/nginx\/<\/code> directory file or that the path of the file isn't correct, i.e. the path has to be in reference to the <code>nginx.conf<\/code> file.<\/p>\n<p>Even though I made these corrections to the above towards-data-science files, the problem still persists.  I've been stuck for a while on this. Any particular reasons why this may be happening?<\/p>\n<p>Edit:\nHere is my directory structure in case it may help:<\/p>\n<pre><code>mlflow-docker\/:\n  mlflow\/:\n    Dockerfile\n  nginx\/:\n    Dockerfile\n    mlflow.conf\n    nginx.conf\n  docker-compose.yml\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1601594496277,
        "Question_favorite_count":null,
        "Question_last_edit_time":1601616621328,
        "Question_score":0.0,
        "Question_view_count":841.0,
        "Answer_body":"<p>You need to add the .htpasswd file inside your container's file system.<\/p>\n<p>Generate the password file in your project's nginx folder.<\/p>\n<pre><code>sudo htpasswd -c .htpasswd sammy\n<\/code><\/pre>\n<p>Copy the password file to the nginx container's directory. Add following line in nginx dockerfile.<\/p>\n<pre><code>COPY .htpasswd \/etc\/nginx\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1601619366396,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64164367",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1601618748112,
        "Question_original_content":"nginx authent issu build docker compos try docker postgresql nginx configur googl cloud run gcr googl cloud platform gcp deploi gcp want local deploy work guid detail process set environ have follow guid exclud sql localhost nginx redirect traffic port add authent sudo htpasswd htpasswd nginx directori ad locat auth basic privat properti auth basic user file htpasswd nginx conf conf case appear onlin troubl localhost enter usernam password continu error open nginx htpasswd fail file directori docker compos log print termin abl localhost blank screen nginx error look post nginx right permiss read htpasswd nginx directori file path file isn correct path refer nginx conf file correct data scienc file problem persist stuck particular reason happen edit directori structur case help docker dockerfil nginx dockerfil conf nginx conf docker compos yml",
        "Question_preprocessed_content":"nginx authent issu build try docker postgresql nginx configur googl cloud run googl cloud platform deploi gcp want local deploy work guid detail process set environ have follow guid nginx redirect traffic port add authent directori ad appear onlin troubl enter continu log print termin abl look post nginx right permiss read directori file path file isn correct path refer file correct file problem persist stuck particular reason happen edit directori structur case help",
        "Answer_original_content":"need add htpasswd file insid contain file gener password file project nginx folder sudo htpasswd htpasswd sammi copi password file nginx contain directori add follow line nginx dockerfil copi htpasswd nginx",
        "Answer_preprocessed_content":"need add htpasswd file insid contain file gener password file project nginx folder copi password file nginx contain directori add follow line nginx dockerfil",
        "Question_gpt_summary_original":"the user is encountering authentication issues when attempting to build a docker-compose environment with nginx configurations for google cloud run on the google cloud platform, despite making corrections to the files.",
        "Question_gpt_summary":"user encount authent issu attempt build docker compos environ nginx configur googl cloud run googl cloud platform despit make correct file",
        "Answer_gpt_summary_original":"solution: to resolve authentication issues when building a docker-compose environment with nginx configurations for google cloud run on the google cloud platform, the user needs to generate a .htpasswd file in the project's nginx folder using the command \"sudo htpasswd -c .htpasswd sammy\", add the password file to the nginx container's directory, and add the line \"copy .htpasswd \/etc\/nginx\" in the nginx dockerfile.",
        "Answer_gpt_summary":"solut resolv authent issu build docker compos environ nginx configur googl cloud run googl cloud platform user need gener htpasswd file project nginx folder command sudo htpasswd htpasswd sammi add password file nginx contain directori add line copi htpasswd nginx nginx dockerfil"
    },
    {
        "Question_id":71694816.0,
        "Question_title":"Unable to get image details : Environment version Autosave_(date)T(time)Z_******** provided in request doesn't match environ",
        "Question_body":"<p>On AzureML Batchendpoint, I'm recently hitting the following error:<\/p>\n<pre><code>Unable to get image details : Environment version Autosave_(date)T(time)Z_******** provided in request doesn't match environ.\n<\/code><\/pre>\n<p>when I setup the batch-endpoint with a <code>yml<\/code> config:<\/p>\n<p><code>environment: azureml:env-name:env-version<\/code><\/p>\n<p>So, AzureML creates and builds the environment with the version I specify <code>env-version<\/code>, which is just a number (in my case = 3).<\/p>\n<p>and then for some weird reason, AzureML creates an extra environment version called <code>Autosave_(date)T(time)Z_********<\/code>, which is not built, but based on the previous one just created, and then it becomes the <code>latest<\/code> version of that environment.<\/p>\n<p>In summary, AzureML instead of looking for the version that I specified as <code>env-name:3<\/code> it seems to be looking for <code>env-name:Autosave_(date)T(time)Z_********<\/code> and then throws the error message mentioned above.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648738692993,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Answer_body":"<p>I found the problem was that when creating an environment from a YAML specification file, one of my <strong>conda dependencies<\/strong> was <code>cmake<\/code>, which I needed to allow installation of another python module. The docker image is exactly the same as a previously created environment.<\/p>\n<p>Removing the <code>cmake<\/code> dependency from the YAML file, eliminated the issue. So the workaround is to install it using a Dockerfile.<\/p>\n<p>The error message was very misleading to start with, but got there in the end after understanding that AzureML reuses a cached image, based on the hash value, from the environment definition accordingly to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#image-caching-and-reuse\" rel=\"nofollow noreferrer\">this<\/a><\/p>\n<p>So for that reason, the automatically created <code>Autosave<\/code> docker image  references to that same build, which only happens once when the first job is sent.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1649244565400,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71694816",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1648808005063,
        "Question_original_content":"unabl imag detail environ version autosav date time provid request match environ batchendpoint recent hit follow error unabl imag detail environ version autosav date time provid request match environ setup batch endpoint yml config environ env env version creat build environ version specifi env version number case weird reason creat extra environ version call autosav date time built base previou creat latest version environ summari instead look version specifi env look env autosav date time throw error messag mention",
        "Question_preprocessed_content":"unabl imag detail environ version provid request match environ batchendpoint recent hit follow error setup config creat build environ version specifi number weird reason creat extra environ version call built base previou creat version environ summari instead look version specifi look throw error messag mention",
        "Answer_original_content":"problem creat environ yaml specif file conda depend cmake need allow instal python modul docker imag exactli previous creat environ remov cmake depend yaml file elimin issu workaround instal dockerfil error messag mislead start got end understand reus cach imag base hash valu environ definit accordingli reason automat creat autosav docker imag refer build happen job sent",
        "Answer_preprocessed_content":"problem creat environ yaml specif file conda depend need allow instal python modul docker imag exactli previous creat environ remov depend yaml file elimin issu workaround instal dockerfil error messag mislead start got end understand reus cach imag base hash valu environ definit accordingli reason automat creat docker imag refer build happen job sent",
        "Question_gpt_summary_original":"the user is encountering an issue where the environment version specified in the request does not match the environment version created by the batch-endpoint, resulting in an error message.",
        "Question_gpt_summary":"user encount issu environ version specifi request match environ version creat batch endpoint result error messag",
        "Answer_gpt_summary_original":"the solution to the issue of environment version mismatch is to remove the cmake dependency from the yaml file and install it using a dockerfile. the error message was misleading, but the problem was caused by reusing a cached image based on the hash value from the environment definition. the autosave docker image references the same build, which only happens once when the first job is sent.",
        "Answer_gpt_summary":"solut issu environ version mismatch remov cmake depend yaml file instal dockerfil error messag mislead problem caus reus cach imag base hash valu environ definit autosav docker imag refer build happen job sent"
    },
    {
        "Question_id":67689868.0,
        "Question_title":"How to generate Inference Schema for Dictionary with nested structure using Azure InferenceSchema package?",
        "Question_body":"<p>In Advanced Scoring Scripting for AzureML webservice, to automatically generate a schema for our web service, we provide a sample of the input and\/or output in the constructor for one of the defined type objects. The type and sample are used to automatically create the schema.\nTo use schema generation, we include the open-source inference-schema package version 1.1.0 or above. The types that I can find include Numpy Type, Pandas Type, Abstract Parameter type.\nHow do we define the schema for a Nested Dictionary of (generalized) format:<\/p>\n<pre><code>{    &quot;top_level_key&quot;: [\n                         {&quot;nested_key_1&quot;: &quot;string_1&quot;,\n                          &quot;nested_key_2&quot;: &lt;float_number&gt;, \n                          &quot;nested_key_3&quot;: &lt;True\/False&gt;}\n                      ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1621952227770,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":172.0,
        "Answer_body":"<p>we don\u2019t have a good way to extend the handling for generic Python class objects. However, we are planning to add support for that, basically by providing more information on the necessary hooks, and allowing users to extend a base class to implement the hook to match the desired class structure.\nThese types are currently supported:<\/p>\n<p>pandas\nnumpy\npyspark\nStandard Python object<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script#automatically-generate-a-swagger-schema<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67689868",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1622006064007,
        "Question_original_content":"gener infer schema dictionari nest structur azur inferenceschema packag advanc score script webservic automat gener schema web servic provid sampl input output constructor defin type object type sampl automat creat schema us schema gener includ open sourc infer schema packag version type includ numpi type panda type abstract paramet type defin schema nest dictionari gener format level kei nest kei string nest kei nest kei",
        "Question_preprocessed_content":"gener infer schema dictionari nest structur azur inferenceschema packag advanc score script webservic automat gener schema web servic provid sampl input output constructor defin type object type sampl automat creat schema us schema gener includ packag version type includ numpi type panda type abstract paramet type defin schema nest dictionari format",
        "Answer_original_content":"dont good wai extend handl gener python class object plan add support basic provid inform necessari hook allow user extend base class implement hook match desir class structur type current support panda numpi pyspark standard python object http doc microsoft com azur machin learn deploi advanc entri script automat gener swagger schema",
        "Answer_preprocessed_content":"dont good wai extend handl gener python class object plan add support basic provid inform necessari hook allow user extend base class implement hook match desir class structur type current support panda numpi pyspark standard python object",
        "Question_gpt_summary_original":"the user is encountering challenges in generating an inference schema for a dictionary with a nested structure using the azure inferenceschema package.",
        "Question_gpt_summary":"user encount challeng gener infer schema dictionari nest structur azur inferenceschema packag",
        "Answer_gpt_summary_original":"possible solutions from the answer are that the azure inferenceschema package currently does not have a good way to extend the handling for generic python class objects. however, the package is planning to add support for that by providing more information on the necessary hooks and allowing users to extend a base class to implement the hook to match the desired class structure. the types currently supported by the package are pandas, numpy, pyspark, and standard python object.",
        "Answer_gpt_summary":"possibl solut answer azur inferenceschema packag current good wai extend handl gener python class object packag plan add support provid inform necessari hook allow user extend base class implement hook match desir class structur type current support packag panda numpi pyspark standard python object"
    },
    {
        "Question_id":null,
        "Question_title":"DSVM can support SQL Server Developer Edition for Ubuntu",
        "Question_body":"According to this\n- https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/tools-included#store-retrieve-and-manipulate-data,\nit appears that the SQL Server Developer Edition (Ubuntu) is being supported in DSVM but I couldn\u2019t find the name in the supported list here https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/data-science-virtual-machine\/dsvm-tools-data-platforms#sql-server-developer-edition,\nfurthermore there is no guide line for Linux Guide line but only windows guideline is there.\nI\u2019d like make sure the followings :\n\nCan DSVM support SQL Server Developer Edition for Ubuntu?\n\n\nIf yes, where is the guideline for this?\n\n\nIf no, the documentation is wrong? And any particular supporting plan for SQL Server Developer Edition for Ubuntu?\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1618980125337,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/365284\/dsvm-can-support-sql-server-developer-edition-for.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-22T05:37:42.617Z",
                "Answer_score":0,
                "Answer_body":"@JunghyeonRyu-6784 Thanks for the question. We don't preinstall SQL Server Developer Edition on the Ubuntu images but you can install it on your own. Here is the documentation and steps.",
                "Answer_comment_count":5,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"dsvm support sql server develop edit ubuntu accord http doc microsoft com azur machin learn data scienc virtual machin tool includ store retriev manipul data appear sql server develop edit ubuntu support dsvm support list http doc microsoft com azur machin learn data scienc virtual machin dsvm tool data platform sql server develop edit furthermor guid line linux guid line window guidelin like sure follow dsvm support sql server develop edit ubuntu ye guidelin document wrong particular support plan sql server develop edit ubuntu thank",
        "Question_preprocessed_content":"dsvm support sql server develop edit ubuntu accord appear sql server develop edit support dsvm support list furthermor guid line linux guid line window guidelin like sure follow dsvm support sql server develop edit ubuntu ye guidelin document wrong particular support plan sql server develop edit ubuntu thank",
        "Answer_original_content":"junghyeonryu thank question preinstal sql server develop edit ubuntu imag instal document step",
        "Answer_preprocessed_content":"thank question preinstal sql server develop edit ubuntu imag instal document step",
        "Question_gpt_summary_original":"the user is trying to determine if the data science virtual machine (dsvm) can support sql server developer edition for ubuntu, and if so, where to find the guidelines for this.",
        "Question_gpt_summary":"user try determin data scienc virtual machin dsvm support sql server develop edit ubuntu guidelin",
        "Answer_gpt_summary_original":"possible solutions: \n- the dsvm does not come with preinstalled sql server developer edition for ubuntu.\n- the user can install sql server developer edition on their own.\n- the documentation and steps for installing sql server developer edition on ubuntu can be found.",
        "Answer_gpt_summary":"possibl solut dsvm come preinstal sql server develop edit ubuntu user instal sql server develop edit document step instal sql server develop edit ubuntu"
    },
    {
        "Question_id":null,
        "Question_title":"Guild runs very slow",
        "Question_body":"<p>Similarly to what has been described in <a href=\"https:\/\/my.guild.ai\/t\/guild-run-hangs-very-slow\/362\">this thread<\/a>, running the following command results in hanging for a while before we see any results:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">guild run train\n<\/code><\/pre>\n<p>The Guild file contains the following:<\/p>\n<pre data-code-wrap=\"plaintext\"><code class=\"lang-nohighlight\">train:\n  main: script\n<\/code><\/pre>\n<p>The script is simply:<\/p>\n<pre data-code-wrap=\"py\"><code class=\"lang-nohighlight\">print(\"Hello, world!\")\n<\/code><\/pre>\n<p>I believe the hanging is related to the fact that I have a data folder which contains thousands of files each corresponding to a sample of the dataset I\u2019m using in my experiments. This is because:<\/p>\n<ol>\n<li>When using the <code>--debug<\/code> flag, Guild hangs just after copying one <code>.csv<\/code> file within the data folder (none of the thousands of sample files are copied to the target directory, though).<\/li>\n<li>When I remove the data folder, the hanging stops.<\/li>\n<\/ol>\n<hr>\n<p>My questions are:<\/p>\n<ol>\n<li>Is this a bug? If so, how can I circumvent this issue?<\/li>\n<li>How am I supposed to make my dataset accessible by the training script, since it is run from a totally different directory and there are thousands of files it needs to access?<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663441155671,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":105.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/guild-runs-very-slow\/919",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-28T15:05:55.374Z",
                "Answer_body":"<p>Apologies for the late reply here! I\u2019m looking into this now. I suspect it\u2019s Guild\u2019s scanning for source code files to copy but I want to confirm this before getting the correct config to you.<\/p>\n<p>As for getting your script access to the data dir, you\u2019d do that using a <code>file<\/code> dependency in your Guild file.<\/p>\n<pre><code class=\"lang-yaml\">train:\n  main: script\n  requires:\n    - file: data\n      target-type: link\n<\/code><\/pre>\n<p>The target type \u2018link\u2019 there tells Guild NOT to copy all those files per run but just to link to the directory. If you want to copy, omit <code>targe-type<\/code> or use the value <code>copy<\/code> (the default behavior since 0.8).<\/p>",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run slow similarli describ thread run follow command result hang result run train file contain follow train main script script simpli print hello world believ hang relat fact data folder contain thousand file correspond sampl dataset experi debug flag hang copi csv file data folder thousand sampl file copi target directori remov data folder hang stop question bug circumv issu suppos dataset access train script run total differ directori thousand file need access",
        "Question_preprocessed_content":"run slow similarli describ thread run follow command result hang result file contain follow script simpli believ hang relat fact data folder contain thousand file correspond sampl dataset experi flag hang copi file data folder remov data folder hang stop question bug circumv issu suppos dataset access train script run total differ directori thousand file need access",
        "Answer_original_content":"apolog late repli look suspect scan sourc code file copi want confirm get correct config get script access data dir youd file depend file train main script requir file data target type link target type link tell copi file run link directori want copi omit targ type us valu copi default behavior",
        "Answer_preprocessed_content":"apolog late repli look suspect scan sourc code file copi want confirm get correct config get script access data dir youd depend file target type link tell copi file run link directori want copi omit us valu",
        "Question_gpt_summary_original":"the user is experiencing an issue where running a guild command hangs due to a data folder containing thousands of files, and is seeking a solution to make the dataset accessible to the training script.",
        "Question_gpt_summary":"user experienc issu run command hang data folder contain thousand file seek solut dataset access train script",
        "Answer_gpt_summary_original":"the solution to the issue of a guild command hanging due to a data folder containing thousands of files is to use a file dependency in the guild file and specify the target type as \"link\". this will allow the training script to access the data directory without copying all the files per run. if copying is desired, the target type can be omitted or set to \"copy\".",
        "Answer_gpt_summary":"solut issu command hang data folder contain thousand file us file depend file specifi target type link allow train script access data directori copi file run copi desir target type omit set copi"
    },
    {
        "Question_id":null,
        "Question_title":"How to resolve error - Creating conda environment failed with exit code: 1?",
        "Question_body":"I am getting this error when I run script job_submit.py. I do not know how to debug this issue, would appreciate help to solve this.\n\n\n\n\nPS: I have just started learning azure so I am not sure what I am missing.\n\n\n\n\nscript_to_run.py\nfrom azureml.core import Workspace, Dataset, Run\nws = Workspace.from_config()\naz_dataset = Dataset.get_by_name(workspace=ws, name='titanic-dataset')\n# Get the context of the experiment\nnew_run = Run.get_context()\ndf = az_dataset.to_pandas_dataframe()\n### count the observations\ntotal_obs = len(df)\n### get the gender count\ngender_count = df['Sex'].value_counts()\n# log the metrics to workspace\nnew_run.log(name = \"Total observations\", value = total_obs)\n### Log the gender data values\nfor val in df['Sex'].unique():\n    new_run.log(name = val, value = gender_count[val])\n# complete an experiment run\nnew_run.complete()\n\n\n\njob_submit.py\nfrom azureml.core import Workspace, Datastore, Dataset, Experiment, ScriptRunConfig, Environment\n# Access workspace\nws = Workspace.from_config()\n# create an experiment object\nexp = Experiment(workspace=ws, name = \"Titanic_exp\")\n# create custom env - myenv\nmyenv = Environment(name = 'MyEnvironment')\n# to install dependencies\nfrom azureml.core.environment import CondaDependencies\n# from CondaDependencies class we need to create an object which will have all the required dependencies\n# create the dependencies object\npackages = CondaDependencies.create(conda_packages=['pandas', 'scikit-learn']) # this will have list of all packages we will need\nmyenv.python.conda_dependencies = packages # this will tell to install the packages\n# register environment to workspace so that we have access to it\nmyenv.register(ws)\n# create a script configuration for custom env\nscript_config = ScriptRunConfig(source_directory = '.', script = \"script_to_run.py\", environment = myenv)\nnew_run = exp.submit(config = script_config)",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1629557858217,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/522968\/how-to-resolve-error-creating-conda-environment-fa.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-24T01:12:50.137Z",
                "Answer_score":0,
                "Answer_body":"Hi @ramr-msft thanks for replying.\nI am following this link to learn azureml: 4-azure-ml-experiment\n\nI did the following steps:\n1. Created the workspace, downloaded the config.json file and uploaded it in the .azureml folder created by me.\n2. I was able to successfully run this script using experiment.start_logging() function as given in the module. \n3. The problem comes when i try to run the script as an experiment using Run.get_context()125769-60-control-log.txt\n\n4. I am attaching the 60_control_log.txt for reference.\n\nPlease let me know if this answers your question and helps you to figure out the issue.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-07T11:52:30.913Z",
                "Answer_score":0,
                "Answer_body":"Hey, I was having the same issue. resolved in two steps, however i think you can skip step 1\n\nstep 1 (not sure if this actually had any impact but did not retest with default setup)\nI have modified the environment.yml fiIe and specified a python version as 3.8.10\n\n\nstep 2 (I think this did the trick)\nCreated new compute instance and rerun\n\nHope same works for you",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"resolv error creat conda environ fail exit code get error run script job submit know debug issu appreci help solv start learn azur sure miss script run core import workspac dataset run workspac config dataset dataset workspac titan dataset context experi new run run context dataset panda datafram count observ total ob len gender count gender count sex valu count log metric workspac new run log total observ valu total ob log gender data valu val sex uniqu new run log val valu gender count val complet experi run new run complet job submit core import workspac datastor dataset experi scriptrunconfig environ access workspac workspac config creat experi object exp experi workspac titan exp creat custom env myenv myenv environ myenviron instal depend core environ import condadepend condadepend class need creat object requir depend creat depend object packag condadepend creat conda packag panda scikit learn list packag need myenv python conda depend packag tell instal packag regist environ workspac access myenv regist creat script configur custom env script config scriptrunconfig sourc directori script script run environ myenv new run exp submit config script config",
        "Question_preprocessed_content":"resolv error creat conda environ fail exit code get error run script know debug issu appreci help solv start learn azur sure miss core import workspac dataset run context experi count observ len gender count log metric workspac total observ valu log gender data valu val val valu complet experi run core import workspac datastor dataset experi scriptrunconfig environ access workspac creat experi object exp experi creat custom env myenv myenv environ instal depend import condadepend condadepend class need creat object requir depend creat depend object packag list packag need packag tell instal packag regist environ workspac access creat script configur custom env script environ myenv",
        "Answer_original_content":"ramr msft thank repli follow link learn azur experi follow step creat workspac download config json file upload folder creat abl successfulli run script experi start log function given modul problem come try run script experi run context control log txt attach control log txt refer let know answer question help figur issu hei have issu resolv step think skip step step sure actual impact retest default setup modifi environ yml fiie specifi python version step think trick creat new comput instanc rerun hope work",
        "Answer_preprocessed_content":"thank repli follow link learn follow step creat workspac download file upload folder creat abl successfulli run script function given modul problem come try run script experi attach refer let know answer question help figur issu hei have issu resolv step think skip step step modifi fiie specifi python version step creat new comput instanc rerun hope work",
        "Question_gpt_summary_original":"the user is encountering an error when running a script job_submit.py and is seeking help to debug and resolve the issue.",
        "Question_gpt_summary":"user encount error run script job submit seek help debug resolv issu",
        "Answer_gpt_summary_original":"possible solutions to the error encountered when running the script job_submit.py are: \n1. modify the environment.yml file and specify a python version as 3.8.10.\n2. create a new compute instance and rerun the script.",
        "Answer_gpt_summary":"possibl solut error encount run script job submit modifi environ yml file specifi python version creat new comput instanc rerun script"
    },
    {
        "Question_id":null,
        "Question_title":"DVC local storage usecase",
        "Question_body":"<p>Hi, I\u2019m trying to understand if DVC is a good solution for our company use-case. We currently have several tens of TB of data, and constantly adding to it every week. I would like to add versioning to this, so that our scientists can run experiments on various subsets and track all changes.<\/p>\n<p>All the data is stored locally in a network drive accessible from the scientists\u2019 computers. My question would be if DVC can be used in this way for this use-case, and how to get around copying the data multiple time (e.g. if 5 scientists access 40TB worth of data, this shouldn\u2019t be copied to their versioned repos).<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1611043180303,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":842.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-local-storage-usecase\/628",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-19T08:39:42.177Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cotton84\">@cotton84<\/a>,<\/p>\n<aside class=\"quote no-group\" data-username=\"cotton84\" data-post=\"1\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>We currently have several tens of TB of data, and constantly adding to it every week.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Can you give a bit more details on what your data storage looks like? Is it a single dataset, several ones, are you adding new files or appending to existing files? The best implementation of data versioning with DVC depends on those factors, but in general yes: this is something we aim to solve <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<p>For now I would recommend you check out <a href=\"https:\/\/dvc.org\/doc\/use-cases\/versioning-data-and-model-files\">https:\/\/dvc.org\/doc\/use-cases\/versioning-data-and-model-files<\/a> (and the tutorial under that).<\/p>\n<aside class=\"quote no-group\" data-username=\"cotton84\" data-post=\"1\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>run experiments on various subsets<\/p>\n<\/blockquote>\n<\/aside>\n<p>It would also be great to know how these subsets are formed to give you a more specific answer. For now I can share <a href=\"https:\/\/dvc.org\/doc\/start\/experiments\">https:\/\/dvc.org\/doc\/start\/experiments<\/a> on this.<\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"cotton84\" data-post=\"1\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>network drive accessible from the scientists\u2019 computers\u2026<br>\nhow to get around copying the data multiple time<\/p>\n<\/blockquote>\n<\/aside>\n<p>This is actually a use case we have documented specifically in: <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\">https:\/\/dvc.org\/doc\/use-cases\/shared-development-server<\/a> \u2014 please take a look and lmk if that was not what you meant. Thanks!<\/p>",
                "Answer_score":59.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-19T10:03:56.368Z",
                "Answer_body":"<p>Hey, thanks for the really quick reply!<\/p>\n<p>Some more details about how our data is structured:<\/p>\n<ul>\n<li>\n<p>We have several recording campaigns during the year, almost every month, where we go to various customers and record data on their premises. These are put into several folders i.e.<\/p>\n<ul>\n<li>Client1\/2020-01<\/li>\n<li>Client2\/2020-01<\/li>\n<li>Client2\/2020-04<\/li>\n<li>Client3\/2020-01 etc.<\/li>\n<\/ul>\n<\/li>\n<li>\n<p>The files we are working with range from several hundred MB to several tens of GB in size <strong>per file<\/strong><\/p>\n<\/li>\n<li>\n<p>We then have projects using combinations of these for each client, e.g.:<\/p>\n<ul>\n<li>Project1: Client1\/2020-01, Client2\/2020-01, Client3\/2020-01<\/li>\n<li>Project2: Client2\/2020-01, Client2\/2020-04<\/li>\n<\/ul>\n<\/li>\n<li>\n<p>For each project we have several experiments that can have subsets of these datasets for debugging the feature extraction stages<\/p>\n<\/li>\n<li>\n<p>At any point we can add data to the existing datasets \/ projects<\/p>\n<\/li>\n<li>\n<p>There can be 2-3 people working on one project at once, with various subsets of the datasets<\/p>\n<\/li>\n<\/ul>\n<p>I think this is a very common use-case in the industry. Right now we are organizing these with simple folders, and with files that list the current dataset that each experiment is using. This is getting out of hand as we add more and more data, and very easy to miss something.<\/p>\n<p>The problem is that although there are many tutorials on DVC, there is not one that goes over more complex on-prem setups with \u2018big\u2019 data such as this one.<\/p>",
                "Answer_score":38.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-19T17:54:21.829Z",
                "Answer_body":"<aside class=\"quote no-group quote-modified\" data-username=\"cotton84\" data-post=\"3\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>The files we are working with range from several hundred MB to several tens of GB\u2026<br>\nThese are put into several folders\u2026<br>\nAt any point we can add data\u2026<\/p>\n<\/blockquote>\n<\/aside>\n<p>Thanks for the info! So it sounds like you don\u2019t append to existing files, right? This is the most efficient way to version large datasets with DVC at the moment (although when <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/829\">chunking<\/a> is implemented, it won\u2019t matter as much). That\u2019s because DVC has no awareness of data formats inside files, so changing even a single byte requires storing the whole file again as a separate version.<\/p>\n<p>Were you able to check out the <strong>Data Versioning Tutorial<\/strong>? The <a href=\"https:\/\/dvc.org\/doc\/use-cases\/versioning-data-and-model-files\/tutorial#second-model-version\">Second model version<\/a> section shows how to add files to tracked directories to create a new dataset version.<\/p>\n<p>It\u2019s just important to think through which directories you consider dataset \u201cunits\u201d and will be tracking <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#adding-entire-directories\">with <code>dvc add<\/code><\/a>. For example, in your case you can pick between tracking each <code>Client*\/&lt;Y-m&gt;<\/code> or going higher level (just each <code>Client*\/<\/code>)?<\/p>\n<p>Each tracked directory will produce a .dvc file, so each strategy results in less or more .dvc files to manage (with Git). But keep in mind that \u201csync\u201d commands support targeting files\/dir inside tracked dirs granularly (see for ex. <a href=\"https:\/\/dvc.org\/doc\/command-reference\/checkout\"><code>dvc checkout<\/code><\/a>) so in either case your ability to pull and push specific files is the same.<\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"cotton84\" data-post=\"3\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>We then have projects using combinations of these\u2026<br>\nFor each project we have several experiments that can have subsets of these data\u2026<br>\n\u2026files that list the current dataset that each experiment is using.<\/p>\n<\/blockquote>\n<\/aside>\n<p>For this pattern we have the <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\">Data Registries<\/a> use case: you can have a DVC repo dedicated to versioning all your datasets, and then secondary DVC projects that <a href=\"https:\/\/dvc.org\/doc\/command-reference\/get\"><code>get<\/code><\/a> or <a href=\"https:\/\/dvc.org\/doc\/command-reference\/import\"><code>import<\/code><\/a> the specific dirs or files you need (also supports granularity)<\/p>\n<p>Please also take a look at <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\">Pipelines<\/a> as a way to start codifying your experiments in a manageable way.<\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"cotton84\" data-post=\"3\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>There can be 2-3 people working on one project at once, with various subsets\u2026<br>\nvery common use-case in the industry\u2026 complex on-prem setups with \u2018big\u2019 data\u2026<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yep. What did you think of the \u201cshared external cache\u201d pattern? Does it help in your team\u2019s org? In this case that setup would apply mainly to the secondary DVC projects consuming from the data registry (although even that repo could share it too).<\/p>\n<aside class=\"onebox allowlistedgeneric\">\n  <header class=\"source\">\n      <img src=\"https:\/\/dvc.org\/favicon.ico\" class=\"site-icon\" width=\"64\" height=\"64\">\n      <a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\" target=\"_blank\" rel=\"noopener\">dvc.org<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/dvc.org\/social-share.png\" class=\"thumbnail\" width=\"690\" height=\"362\"><\/div>\n\n<h3><a href=\"https:\/\/dvc.org\/doc\/use-cases\/shared-development-server\" target=\"_blank\" rel=\"noopener\">Shared Development Server<\/a><\/h3>\n\n<p>Open-source version control system for Data Science and Machine Learning projects. Git-like experience to organize your data, models, and experiments.<\/p>\n\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":43.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T03:57:13.932Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/cotton84\">@cotton84<\/a> we have essentially an <em>append-only immutable<\/em> pool of many, many files and each data scientist could pick any subset of those file to do some experimentation with them, right?<\/p>\n<p>In this case I usually don\u2019t recommend using the current version of DVC at all <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> You are totally fine versioning a \u201cfilter\u201d file- a file that contains specific files that are being used in the specific commit, specific project. It should be enough in terms of reproducibility and versioning if <strong>data in the storage is immutable and only new files are being added.<\/strong><\/p>\n<p>Now, you have another requirement though - <code>avoid copying the data multiple time<\/code>! This is indeed what shared cache\/shared dev server DVC use case for. It\u2019s very powerful and solves the problem. The question here is how to solve both simultaneously - flexibility (give data scientists and interface to checkout (in DVC terms) arbitrary subset of files to work with from the pool + avoid copying.<\/p>\n<p>To come with a workaround, it would be helpful to understand how do you create this file in the:<\/p>\n<blockquote>\n<p>with files that list the current dataset that each experiment is using<\/p>\n<\/blockquote>\n<p>how do data scientists select the files they need from the pool of all those directories?<\/p>",
                "Answer_score":28.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T07:24:55.186Z",
                "Answer_body":"<p>Thank you both for the quick replies! I was able to go through the tutorial and set up an external cache with a small-ish dataset yesterday, so everything is a bit more clear.<\/p>\n<p>To answer your questions:<\/p>\n<ul>\n<li>Yes, the files are generally immutable, we only add more files, or if necessary cull some (e.g. corrupted files or very old ones).<\/li>\n<li>We would like to have fine granularity when working with datasets (individual files), but you are saying that it doesn\u2019t matter if I add specific files (or recurse through directories), or directories themselves, they can be accessed individually either way, right?<\/li>\n<li>With regards to external caches, I have a question - right now after I push the commit, the data gets copied to the external cache. Doe DVC automatically delete the files in the repository if it is set to hard link the files, so that there is only one copy of the file, the one in the cache?<\/li>\n<li>The scientists have the big pool of files in the dataset. Depending on their experiment needs, they can select subsets, e.g. files that were recorded in one specific location, or within a specific timeframe. The files are not copied, just the lists are used to access them during training. To filter the files they use labels and metadata that are in separate ground-truth files, which brings me to another question:<\/li>\n<li>Is there a recommended way to handle label files? These could change over time, adding of modifying metadata. Right now they are stored in a separate repository and they point to each of the dataset files in particular (have the same name). Would it be a good idea to let DVC also track this? Or is there a preferred tool (e.g. database) that can be used to better manage it?<\/li>\n<\/ul>\n<p>Thanks again for the help, much appreciated!<\/p>",
                "Answer_score":32.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T19:02:00.057Z",
                "Answer_body":"<aside class=\"quote no-group quote-modified\" data-username=\"shcheklein\" data-post=\"5\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/shcheklein\/40\/11_2.png\" class=\"avatar\"> shcheklein:<\/div>\n<blockquote>\n<p><em>append-only immutable<\/em> pool\u2026<br>\nI usually don\u2019t recommend using the current version of DVC<\/p>\n<\/blockquote>\n<\/aside>\n<p>This is a good point, the proposed file structure already implements basic versioning by file name (<code>Client*\/&lt;Y-m&gt;<\/code> format).I think that this goes back to asking ourselves what we want to consider \u201cdata versioning\u201d (more info: <a href=\"https:\/\/www.w3.org\/TR\/dwbp\/#dataVersioning\" class=\"inline-onebox\">Data on the Web Best Practices<\/a>).<\/p>\n<p>But, regardless, the patterns previously discussed could still make it desirable to track those assets with DVC. It\u2019s up to you <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<aside class=\"quote no-group\" data-username=\"cotton84\" data-post=\"6\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<ul>\n<li>you are saying that it doesn\u2019t matter if I add specific files (or recurse through directories), or directories themselves, they can be accessed individually either way, right?<\/li>\n<\/ul>\n<\/blockquote>\n<\/aside>\n<p>Correct.<\/p>\n<aside class=\"quote no-group\" data-username=\"cotton84\" data-post=\"6\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>Doe DVC automatically delete the files in the repository if it is set to hard link the files, so that there is only one copy of the file, the one in the cache?<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yes, DVC tries to preserve only one copy of that data in the drive whether the cache is local or external to the project (unless configured to copy or if the file system doesn\u2019t support file links). See <a href=\"https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization\">https:\/\/dvc.org\/doc\/user-guide\/large-dataset-optimization<\/a><\/p>\n<aside class=\"quote no-group quote-modified\" data-username=\"cotton84\" data-post=\"6\" data-topic=\"628\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/c\/4491bb\/40.png\" class=\"avatar\"> cotton84:<\/div>\n<blockquote>\n<p>Is there a recommended way to handle label files? Right now they are stored in a separate repository<\/p>\n<\/blockquote>\n<\/aside>\n<p>They can be in the same repository, and you can version those directly with Git along with any other code or config files.<\/p>\n<p>But it\u2019s not cleat to me that you\u2019d still need some of those files (e.g. the lists that define a data subset) if\/after adopting DVC. For example, <a href=\"https:\/\/dvc.org\/doc\/user-guide\/dvc-files\/dvc-yaml\">dvc.yaml<\/a> files already codify the dependencies for each stage in a <a href=\"https:\/\/dvc.org\/doc\/start\/data-pipelines\">data pipeline<\/a>.<\/p>",
                "Answer_score":47.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"local storag usecas try understand good solut compani us case current ten data constantli ad week like add version scientist run experi subset track chang data store local network drive access scientist comput question wai us case copi data multipl time scientist access worth data shouldnt copi version repo",
        "Question_preprocessed_content":"local storag usecas try understand good solut compani current ten data constantli ad week like add version scientist run experi subset track chang data store local network drive access scientist comput question wai copi data multipl time",
        "Answer_original_content":"cotton cotton current ten data constantli ad week bit detail data storag look like singl dataset on ad new file append exist file best implement data version depend factor gener ye aim solv recommend check http org doc us case version data model file tutori cotton run experi subset great know subset form specif answer share http org doc start experi cotton network drive access scientist comput copi data multipl time actual us case document specif http org doc us case share develop server look lmk meant thank hei thank quick repli detail data structur record campaign year month custom record data premis folder client client client client file work rang ten size file project combin client project client client client project client client project experi subset dataset debug featur extract stage point add data exist dataset project peopl work project subset dataset think common us case industri right organ simpl folder file list current dataset experi get hand add data easi miss problem tutori goe complex prem setup big data cotton file work rang ten folder point add data thank info sound like dont append exist file right effici wai version larg dataset moment chunk implement wont matter that awar data format insid file chang singl byte requir store file separ version abl check data version tutori second model version section show add file track directori creat new dataset version import think directori consid dataset unit track add exampl case pick track client go higher level client track directori produc file strategi result file manag git mind sync command support target file dir insid track dir granularli checkout case abil pull push specif file cotton project combin project experi subset data file list current dataset experi pattern data registri us case repo dedic version dataset secondari project import specif dir file need support granular look pipelin wai start codifi experi manag wai cotton peopl work project subset common us case industri complex prem setup big data yep think share extern cach pattern help team org case setup appli mainli secondari project consum data registri repo share org share develop server open sourc version control data scienc machin learn project git like experi organ data model experi cotton essenti append immut pool file data scientist pick subset file experiment right case usual dont recommend current version total fine version filter file file contain specif file specif commit specif project term reproduc version data storag immut new file ad requir avoid copi data multipl time share cach share dev server us case power solv problem question solv simultan flexibl data scientist interfac checkout term arbitrari subset file work pool avoid copi come workaround help understand creat file file list current dataset experi data scientist select file need pool directori thank quick repli abl tutori set extern cach small ish dataset yesterdai bit clear answer question ye file gener immut add file necessari cull corrupt file old on like fine granular work dataset individu file sai doesnt matter add specif file recurs directori directori access individu wai right regard extern cach question right push commit data get copi extern cach doe automat delet file repositori set hard link file copi file cach scientist big pool file dataset depend experi need select subset file record specif locat specif timefram file copi list access train filter file us label metadata separ ground truth file bring question recommend wai handl label file chang time ad modifi metadata right store separ repositori point dataset file particular good idea let track prefer tool databas better manag thank help appreci shcheklein append immut pool usual dont recommend current version good point propos file structur implement basic version file client format think goe ask want consid data version info data web best practic regardless pattern previous discuss desir track asset cotton sai doesnt matter add specif file recurs directori directori access individu wai right correct cotton doe automat delet file repositori set hard link file copi file cach ye tri preserv copi data drive cach local extern project configur copi file doesnt support file link http org doc user guid larg dataset optim cotton recommend wai handl label file right store separ repositori repositori version directli git code config file cleat youd need file list defin data subset adopt exampl yaml file codifi depend stage data pipelin",
        "Answer_preprocessed_content":"cotton current ten data constantli ad week bit detail data storag look like singl dataset on ad new file append exist file best implement data version depend factor gener ye aim solv recommend check cotton run experi subset great know subset form specif answer share cotton network drive access scientist comput copi data multipl time actual us case document specif look lmk meant thank hei thank quick repli detail data structur record campaign year month custom record data premis folder file work rang ten size file project combin client project project project experi subset dataset debug featur extract stage point add data exist dataset project peopl work project subset dataset think common industri right organ simpl folder file list current dataset experi get hand add data easi miss problem tutori goe complex setup big data cotton file work rang ten folder point add data thank info sound like dont append exist file right effici wai version larg dataset moment that awar data format insid file chang singl byte requir store file separ version abl check data version tutori second model version section show add file track directori creat new dataset version import think directori consid dataset unit track exampl case pick track go higher level track directori produc file strategi result file manag mind sync command support target insid track dir granularli case abil pull push specif file cotton project combin project experi subset data file list current dataset experi pattern data registri us case repo dedic version dataset secondari project specif dir file need look pipelin wai start codifi experi manag wai cotton peopl work project subset common industri complex setup big data yep think share extern cach pattern help team org case setup appli mainli secondari project consum data registri org share develop server version control data scienc machin learn project experi organ data model experi essenti immut pool file data scientist pick subset file experiment right case usual dont recommend current version total fine version filter file file contain specif file specif commit specif project term reproduc version data storag immut new file ad requir share dev server us case power solv problem question solv simultan flexibl arbitrari subset file work pool avoid copi come workaround help understand creat file file list current dataset experi data scientist select file need pool directori thank quick repli abl tutori set extern cach dataset yesterdai bit clear answer question ye file gener immut add file necessari cull like fine granular work dataset sai doesnt matter add specif file directori access individu wai right regard extern cach question right push commit data get copi extern cach doe automat delet file repositori set hard link file copi file cach scientist big pool file dataset depend experi need select subset file record specif locat specif timefram file copi list access train filter file us label metadata separ file bring question recommend wai handl label file chang time ad modifi metadata right store separ repositori point dataset file particular good idea let track prefer tool better manag thank help appreci shcheklein immut pool usual dont recommend current version good point propos file structur implement basic version file think goe ask want consid data version regardless pattern previous discuss desir track asset cotton sai doesnt matter add specif file directori access individu wai right correct cotton doe automat delet file repositori set hard link file copi file cach ye tri preserv copi data drive cach local extern project cotton recommend wai handl label file right store separ repositori repositori version directli git code config file cleat youd need file adopt exampl yaml file codifi depend stage data pipelin",
        "Question_gpt_summary_original":"the user is trying to determine if local storage is a viable solution for their company's use-case, which involves versioning tens of terabytes of data stored in a network drive and accessed by multiple scientists.",
        "Question_gpt_summary":"user try determin local storag viabl solut compani us case involv version ten terabyt data store network drive access multipl scientist",
        "Answer_gpt_summary_original":"possible solutions for the user's use-case of versioning tens of terabytes of data stored in a network drive and accessed by multiple scientists include using data registries, pipelines, and shared development servers. the user can track each client folder or go higher level, and can use filter files to select specific files or directories. it is recommended to use an external cache to avoid copying data multiple times. label files can be stored in the same repository and versioned directly with git.",
        "Answer_gpt_summary":"possibl solut user us case version ten terabyt data store network drive access multipl scientist includ data registri pipelin share develop server user track client folder higher level us filter file select specif file directori recommend us extern cach avoid copi data multipl time label file store repositori version directli git"
    },
    {
        "Question_id":69099627.0,
        "Question_title":"what causes an unpickling stack underflow when trying to serialize a succesfully generated SageMaker model",
        "Question_body":"<p>I am currently working on setting up a pipeline in Amazon Sagemaker. For that I set up an xgboost-estimator and trained it on my dataset. The training job runs as expected and the freshly trained model is saved to the specified output bucket. Later I want to reimport the model, which is done by getting the mode.tar.gz from the output bucket, extracting the model and serializing the binary via pickle.<\/p>\n<pre><code># download the model artifact from AWS S3\n!aws s3 cp s3:\/\/my-bucket\/output\/sagemaker-xgboost-2021-09-06-12-19-41-306\/output\/model.tar.gz .\n\n# opens the downloaded model artifcat and loads it as 'model' variable\nmodel_path = &quot;model.tar.gz&quot;\nwith tarfile.open(model_path) as tar:\n    tar.extractall(path=&quot;.&quot;)\n\nmodel = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n<\/code><\/pre>\n<p>Whenever I try to tun this I receive an unpickling stack underflow:<\/p>\n<pre><code>---------------------------------------------------------------------------\nUnpicklingError                           Traceback (most recent call last)\n&lt;ipython-input-9-b88a7424f790&gt; in &lt;module&gt;\n     10     tar.extractall(path=&quot;.&quot;)\n     11 \n---&gt; 12 model = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n     13 \n\nUnpicklingError: unpickling stack underflow\n<\/code><\/pre>\n<p>So far I retrained the model to see, if the error occurs with a different model file and it does. I also downloaded the model.tar.gz and validated it via gunzip. When extracting the binary file xgboost-model is extracted correctly, I just can't pickle it. Every occurence of the error I found on stackoverflow points at a damaged file, but this one is generated directly by SageMaker and I do note perform any transformation on it, but extracting it from the model.tar.gz. Reloading a model like this seems to be quite a common use case, referring to the documentation and different tutorials.\nLocally I receive the same error with the downloaded file. I tried to step directly into pickle for debugging it but couldn't make much sense of it. The complete error stack looks like this:<\/p>\n<pre><code>Exception has occurred: UnpicklingError       (note: full exception trace is shown but execution is paused at: _run_module_as_main)\nunpickling stack underflow\n  File &quot;\/sagemaker_model.py&quot;, line 10, in &lt;module&gt;\n    model = pkl.load(open('xgboost-model', 'rb'))\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 97, in _run_module_code\n    _run_code(code, mod_globals, init_globals,\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 268, in run_path\n    return _run_module_code(code, init_globals, run_name,\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 87, in _run_code\n    exec(code, run_globals)\n  File &quot;\/usr\/local\/Cellar\/python@3.9\/3.9.1_5\/Frameworks\/Python.framework\/Versions\/3.9\/lib\/python3.9\/runpy.py&quot;, line 197, in _run_module_as_main (Current frame)\n    return _run_code(code, main_globals, None,\n<\/code><\/pre>\n<p>What could cause this issue and at which step during the process could I apply changes to fix or workaround the problem.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":4,
        "Question_creation_time":1631089966887,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":1747.0,
        "Answer_body":"<p>The issue rooted in the model version used for the xgboost framework. from 1.3.0 on the default output changed from pickle to json and the sagemaker documentation does not seem to have been updated accordingly. So if you want to read the model via<\/p>\n<pre><code>    tar.extractall(path=&quot;.&quot;)\n\nmodel = pkl.load(open(&quot;xgboost-model&quot;, &quot;rb&quot;))\n<\/code><\/pre>\n<p>as described in the sagemaker docs, you need to import the XGBOOST framework with with a former version, e.g. 1.2.1.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69099627",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1632479560168,
        "Question_original_content":"caus unpickl stack underflow try serial succesfulli gener model current work set pipelin set xgboost estim train dataset train job run expect freshli train model save specifi output bucket later want reimport model get mode tar output bucket extract model serial binari pickl download model artifact aw aw bucket output xgboost output model tar open download model artifcat load model variabl model path model tar tarfil open model path tar tar extractal path model pkl load open xgboost model try tun receiv unpickl stack underflow unpicklingerror traceback recent tar extractal path model pkl load open xgboost model unpicklingerror unpickl stack underflow far retrain model error occur differ model file download model tar valid gunzip extract binari file xgboost model extract correctli pickl occur error stackoverflow point damag file gener directli note perform transform extract model tar reload model like common us case refer document differ tutori local receiv error download file tri step directli pickl debug couldn sens complet error stack look like except occur unpicklingerror note except trace shown execut paus run modul main unpickl stack underflow file model line model pkl load open xgboost model file usr local cellar python framework python framework version lib python runpi line run code exec code run global file usr local cellar python framework python framework version lib python runpi line run modul code run code code mod global init global file usr local cellar python framework python framework version lib python runpi line run path return run modul code code init global run file usr local cellar python framework python framework version lib python runpi line run code exec code run global file usr local cellar python framework python framework version lib python runpi line run modul main current frame return run code code main global caus issu step process appli chang fix workaround problem",
        "Question_preprocessed_content":"caus unpickl stack underflow try serial succesfulli gener model current work set pipelin set train dataset train job run expect freshli train model save specifi output bucket later want reimport model get output bucket extract model serial binari pickl try tun receiv unpickl stack underflow far retrain model error occur differ model file download valid gunzip extract binari file extract correctli pickl occur error stackoverflow point damag file gener directli note perform transform extract reload model like common us case refer document differ tutori local receiv error download file tri step directli pickl debug couldn sens complet error stack look like caus issu step process appli chang fix workaround problem",
        "Answer_original_content":"issu root model version xgboost framework default output chang pickl json document updat accordingli want read model tar extractal path model pkl load open xgboost model describ doc need import xgboost framework version",
        "Answer_preprocessed_content":"issu root model version xgboost framework default output chang pickl json document updat accordingli want read model describ doc need import xgboost framework version",
        "Question_gpt_summary_original":"the user is encountering an unpickling stack underflow error when trying to serialize a successfully generated model after downloading it from an output bucket.",
        "Question_gpt_summary":"user encount unpickl stack underflow error try serial successfulli gener model download output bucket",
        "Answer_gpt_summary_original":"the solution to the unpickling stack underflow error is to import the xgboost framework with a former version, such as 1.2.1, as the default output changed from pickle to json in version 1.3.0 and the documentation was not updated accordingly. this will allow the user to read the model via tar.extractall(path=\".\") model = pkl.load(open(\"xgboost-model\", \"rb\")) as described in the documentation.",
        "Answer_gpt_summary":"solut unpickl stack underflow error import xgboost framework version default output chang pickl json version document updat accordingli allow user read model tar extractal path model pkl load open xgboost model describ document"
    },
    {
        "Question_id":null,
        "Question_title":"Problem with Sweep; how to use run.finish() and log without error + Question about defined metric",
        "Question_body":"<p>Hi all,<\/p>\n<p>Nice to meet you!<\/p>\n<p>Currently I\u2019m not understanding how to use run.finish() and wandb.init for logging correctly. I\u2019m constantly getting an error when the wandb.agent sweeps to another model configuration. It\u2019s successfully doing the K-fold split, I dont see any errors. But it\u2019s right after the K-fold split  when the now model configuration is applied by the sweep.<\/p>\n<p><strong>Information about my code:<\/strong><br>\nMy code is a bit messy. I think there is no way to use K cross validation from scikitlearn. I\u2019ve tried it many times, but my input and output are (with N = number of datasets):<\/p>\n<p>Input 1: N datasets of 1000 numbers (x-axis)<br>\nInput2 : N datasets of 1000 numbers (y-axis)<br>\nOutput1: 1 number for each N\u2019th dataset<br>\nOutput2: 1 number for each N\u2019th dataset<\/p>\n<p>Input 1 and 2 are concatenated to produce 2 outputs. Lets say N is 300 and split is 0.2 then:<br>\nOutput1.shape, Output2.shape, Output1_test.shape, Output2_test.shape, X.shape, Y.shape, X_test.shape, Y_test.shape<\/p>\n<p>In the same order, their shapes: ((240,), (240,), (60,), (60,), (240, 1000), (240, 1000), (60, 1000), (60, 1000))<br>\nI think there is just no way I can define the cross validation with sklearn with this type of data I think\u2026<\/p>\n<p><strong>Error<\/strong><br>\nI\u2019ve introduced to save to model each time it\u2019s configured. Then load the model in each for loop with zero weigths. This way may cross validation succeeds. However, I\u2019m not sure how to correctly log my files. This code is doing bad at producing the groups I want them to be in; it\u2019s just overwriting them. Also, and as I mentioned; every time when a new model is initiated by the sweep, I get and error:<\/p>\n<p><strong>display<\/strong><\/p>\n<pre><code class=\"lang-auto\">wandb: Sweep Agent: Waiting for job.\nwandb: Job received.\nwandb: While tearing down the service manager. The following error has occured: [WinError 10054] De externe host heeft een verbinding verbroken\nwandb: Agent Starting Run: ekc1s4gm with config:\nwandb: \tbatch_size: 6\nwandb: \tdense_units: 63.48472025895866\nwandb: \tdense_units2: 81.47931201263756\nwandb: \tlearning_rate: 0.0007466619646085462\nwandb: \tnum_layers: 10\nwandb: \toptimizer: Adam\nwandb: WARNING Ignored wandb.init() arg project when running a sweep.\n\nException in thread ChkStopThr: \n.....\nSome file information\n.....\n\nException in thread NetStatThr::\n.....\nSome file information\n.....\n\nEnding in: \n\nConnectionResetError: [WinError 10054] De externe host heeft een verbinding verbroken\n    sent = self._sock.send(data)\nConnectionResetError: [WinError 10054] De externe host heeft een verbinding verbroken\n<\/code><\/pre>\n<p>So when this error occurs, it just continues right after  it created the new model configuration. The last time this error occurs is when the last loop and last model gets evaluated.  Also this error occurs every loop<\/p>\n<blockquote>\n<p>wandb: WARNING Ignored wandb.init() arg project when running a sweep.<\/p>\n<\/blockquote>\n<p>I think it definitely has to do something with the Groups I want certain logs to be in and therefore also my run.finish() command (<strong>The error does not error without any wandb.init()!!! The error also occurs with just the wandb.init() in the for loop, also if I add this, each sweep is overwritten by the other sweep so it\u2019s not creating groups aswell!!<\/strong>.  I\u2019m unsure if I placed them correctly.  it sounded logical to me to define run just once and to have to others as just wandb.init()\u2026  (see me code). I just don\u2019t understand how to use it in this case\u2026  I hope you do ? <strong>How can I group my folds and my validation seperately without  using wandb.init()<\/strong> ? Any recommendations on this would be very welcome!<\/p>\n<p><strong>My code<\/strong><br>\nHere is my code\u2026 A little messy, sorry.<\/p>\n<pre><code class=\"lang-auto\">def seed_all(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">def build_model(config):\n  \n    activ = config.activation   \n    dense_units = config.dense_units  \n    dense_units2 = config.dense_units2\n    num_layers = config.num_layers\n    batch_size = config.batch_size\n    batch_norm = False\n    optimizer = config.optimizer\n    learning_rate = config.learning_rate\n    \n    x = tf.keras.layers.Concatenate()([input1, input2])\n                   \n        #input_layer = Input(shape=(len(norm_train_X .columns), len(norm_train_X.iloc[0][0]))\n    x = Dense(units=dense_units, activation=activ)(x)\n        \n    for _ in range(num_layers):\n        x = Dense(units=dense_units, activation=activ)(x)\n        \n        \n    x1 = Dense(units=dense_units2, activation=activ)(x)\n    \n        # Y1 output will be fed directly from the second dense\n    \n    y1_output = Dense(units='1', name='y1_output')(x1)\n\n    third_dense = Dense(units=dense_units2, activation=activ)(x1)\n\n         # Y2 output will come via the third dense\n    y2_output = Dense(units='1', name='y2_output')(third_dense)\n        \n    model = Model(inputs=[input1, input2], outputs=[y1_output, y2_output])\n    print(model.summary())   \n    model.save_weights('model.h5', overwrite = True)\n    return model\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">def train():\n    test_loss_sum =np.array([0])\n    Hp_loss_sum = np.array([0])\n    MDp_loss_sum = np.array([0])\n    Hp_rmse_sum = np.array([0])\n    MDp_rmse_sum = np.array([0])\n    loss_sum = np.array([0])\n    loss_sum_tot =0 \n    Hp_R2_append = []\n    test_loss_sum_tot =0\n    hp_append = []\n    MDp_append = []\n    hp_pred_append = []\n    MDp_R2_append = []\n    MDp_sum = 0\n    Hp_sum = 0\n    test_loss_mean = 0 \n    hp_pred_append = []\n    MDp_pred_append = []\n    Hp_loss_sum_tot = 0\n    MDp_loss_sum_tot =0\n    Hp_rmse_sum_tot =0\n    MDp_rmse_sum_tot=0\n    \n    X = np.vstack(np.asarray(norm1.numpy()[:]))\n    Y = np.vstack(np.asarray(norm2.numpy()[:]))\n    max_trials = 2\n    epochs = 100\n    test_loss_sum = np.array([0])\n    Hp = train_Y_1_t\n    MDp = train_Y_2_t\n    Hp_test = test_Y_1_t\n    MDp_test = test_Y_2_t\n    hyperparams = dict(\n        lr = 0.0001,\n        optimizer = 'Adam',\n        dense_units = 256,\n        batch_size = 64,\n        epochs = 1,        \n        ense_units2 = 64,\n        activation = 'relu',)\n    \n    cb_reducelr = tf.keras.callbacks.ReduceLROnPlateau(\n        monitor = \"val_loss\",\n        mode = 'auto',\n        factor = 0.1,\n        patience = 20,\n        min_delta = 1e-04, #default\n        min_lr = 1e-07,\n        verbose = 1)\n\n    cb_earlystop= tf.keras.callbacks.EarlyStopping(\n        monitor=\"val_loss\",\n        mode=\"auto\",\n        min_delta = 0,\n        patience=25,\n        verbose=1)\n    \n    run = wandb.init(project=\"custom-charts\", config=hyperparams, reinit = True) #Note the Reinit here!\n    config = wandb.config\n    \n    Wandcalback = WandbCallback(monitor='val_loss')\n    \n    model =  build_model(config=config)\n    \n    LR = config.learning_rate     # 0.001\n    \n    if config.optimizer=='Adam':\n        optimizer = tf.keras.optimizers.Adam(lr = LR)\n    elif config.optimizer=='RMSprop':\n        optimizer = tf.keras.optimizers.RMSprop(lr=LR, rho=0.9, epsilon=1e-08, decay=0.0)\n    else: \n        raise\n    \n    # Compile the model\n    model.compile(optimizer=optimizer,\n    loss={'y1_output': 'mse', 'y2_output': 'mse'},\n    metrics={'y1_output': tf.keras.metrics.RootMeanSquaredError(),'y2_output': tf.keras.metrics.RootMeanSquaredError()})\n   \n    n_splits = 4\n    skf = KFold(n_splits, shuffle = True)\n    skf.get_n_splits(X, Y)\n    i=0\n    vall_loss = []\n    for train_index, test_index in skf.split(X, Y):\n        \n        wandb.init(project=\"custom-charts\", group = \"folds_experiment\", job_type = \"fold{}\".format(i)) \n        model.load_weights('model.h5')\n        \n        train_index = train_index.astype(int)\n        test_index = test_index.astype(int)\n        X = np.array(X)\n        Y = np.array(Y)\n        Hp = np.array(Hp)\n        MDp = np.array(MDp)\n        X_train, X_test = X[train_index], X[test_index]\n        Y_train, Y_test = Y[train_index], Y[test_index]\n        Hp_train, Hp_test = Hp[train_index], Hp[test_index]\n        MDp_train, MDp_test = MDp[train_index],MDp[test_index]      \n        \n        history = model.fit([tf.convert_to_tensor(X), tf.convert_to_tensor(Y)], [Hp, MDp], validation_data = ([tf.convert_to_tensor(X_test), tf.convert_to_tensor(Y_test)], [Hp_test, MDp_test]), \n                     batch_size=config.batch_size,    \n                     epochs=2,             \n                     callbacks=[Wandcalback,cb_earlystop,cb_reducelr],\n                     verbose=1)\n        \n        loss_sum = pd.DataFrame(history.history)['loss'].iloc[-1]  + loss_sum\n        test_loss_sum = pd.DataFrame(history.history)['val_loss'].iloc[-1]  + test_loss_sum\n        Hp_loss_sum = pd.DataFrame(history.history)['val_y1_output_loss'].iloc[-1]  + Hp_loss_sum\n        MDp_loss_sum = pd.DataFrame(history.history)['val_y2_output_loss'].iloc[-1]  + MDp_loss_sum\n        Hp_rmse_sum = pd.DataFrame(history.history)['val_y1_output_root_mean_squared_error'].iloc[-1]  + Hp_rmse_sum\n        MDp_rmse_sum = pd.DataFrame(history.history)['val_y2_output_root_mean_squared_error'].iloc[-1]  + MDp_rmse_sum\n            \n        loss_sum_tot = pd.DataFrame(history.history)['loss']  + loss_sum_tot\n        test_loss_sum_tot = pd.DataFrame(history.history)['val_loss']  + test_loss_sum_tot\n        Hp_loss_sum_tot = pd.DataFrame(history.history)['val_y1_output_loss'] +  Hp_loss_sum_tot\n        MDp_loss_sum_tot = pd.DataFrame(history.history)['val_y2_output_loss']  + MDp_loss_sum_tot\n        Hp_rmse_sum_tot = pd.DataFrame(history.history)['val_y1_output_root_mean_squared_error']  +  Hp_rmse_sum_tot\n        MDp_rmse_sum_tot = pd.DataFrame(history.history)['val_y2_output_root_mean_squared_error']  +  MDp_rmse_sum_tot   \n\n        Y_pred = model.predict([tf.convert_to_tensor(X_test), tf.convert_to_tensor(Y_test)])\n        metric = tfa.metrics.r_square.RSquare()\n            \n        metric.update_state(Hp_test, Y_pred[0].flatten())\n        result = metric.result()\n        R_2_Hp = result.numpy()\n        Hp_R2_append.append(R_2_Hp)\n           \n        metric.update_state(MDp_test, Y_pred[1].flatten())\n        result = metric.result()\n        R_2_MDp = result.numpy()\n        MDp_R2_append.append(R_2_MDp)\n        \n        hp_append.append(Hp_test)\n        MDp_append.append(MDp_test)\n        hp_pred_append.append(Y_pred[0])\n        MDp_pred_append.append(Y_pred[1])\n        MDp_sum = MDp_sum + Y_pred[1]\n        Hp_sum = Hp_sum + Y_pred[0]\n        i = i + 1 \n    \n    test_loss_mean = test_loss_sum\/n_splits\n    loss_sum_mean = loss_sum\/n_splits\n    test_loss_sum_mean =  test_loss_sum\/n_splits\n    Hp_loss_sum_mean = Hp_loss_sum\/n_splits\n    MDp_loss_sum_mean = MDp_loss_sum\/n_splits\n    Hp_rmse_sum_mean = Hp_rmse_sum\/n_splits\n    MDp_rmse_sum_mean = MDp_rmse_sum\/n_splits\n         \n    test_MDp_R2 = np.mean(MDp_R2_append)\n    test_Hp_R2 = np.mean(Hp_R2_append)\n                \n        \n    Hp_mean = Hp_sum\/n_splits\n    MDp_mean = MDp_sum\/n_splits    \n        # wandb.init(project= \"sweep &amp; optimalisation RandomSearch\", group=\"experimentfold{}\".format(i), job_type=\"validation\")\n    wandb.init(project=\"custom-charts\", group =\"folds_experiment\", job_type = \"validation\")\n    for val_los in range(len(test_loss_sum_tot)):\n        wandb.log({\"val_loss_mean\" : test_loss_sum_tot[val_los]\/n_splits})\n    for loss in range(len(loss_sum_tot)):\n        wandb.log({\"loss_mean\": loss_sum_tot[loss]\/n_splits})\n    for val_MDp_los in range(len(test_loss_sum_tot)):\n        wandb.log({\"val_MDp_loss_mean\" : test_loss_sum_tot[val_MDp_los]\/n_splits})\n    for val_hp_los in range(len( Hp_loss_sum_tot)):\n        wandb.log({\"val_hp_loss_mean\": Hp_loss_sum_tot[val_hp_los]\/n_splits})\n    for val_MDp_rmse in range(len(MDp_rmse_sum_tot)):\n        wandb.log({\"val_MDp_rmse_mean\" : MDp_loss_sum_tot[val_MDp_rmse]\/n_splits})\n    for val_hp_rmse in range(len(Hp_rmse_sum_tot)):\n        wandb.log({\"val_MDp_rmse_mean\": Hp_rmse_sum_tot[val_hp_rmse]\/n_splits})\n       \n    hp_append = np.concatenate(hp_append)\n    MDp_append = np.concatenate(MDp_append)\n    hp_pred_append = np.concatenate(hp_pred_append)\n    MDp_pred_append = np.concatenate(MDp_pred_append)\n            \n    Hp_score = np.sqrt(mean_squared_error(hp_pred_append,hp_append))\n    MDp_score = np.sqrt(mean_squared_error(MDp_pred_append,MDp_append))  \n    test_MDp_R2 = np.mean(MDp_R2_append)\n    test_Hp_R2 = np.mean(Hp_R2_append)\n        \n    wandb.log({\"R2_score_hp\":Hp_score, \"R2_score_MDp\":MDp_score, \"R2_hp\":test_Hp_R2, \"R2_MDp\":test_MDp_R2})\n    Hp = np.asarray(Hp_mean.flatten())\n    MDp = np.asarray(MDp_mean.flatten())\n    Hp_testt = np.asarray(Hp_test.flatten())\n    MDp_testt = np.asarray(MDp_test.flatten())\n        \n        \n    fd = pd.DataFrame({\"pred\": Hp,\"actual\":Hp_testt})\n    print(fd)\n    table = wandb.Table(dataframe=fd)\n    wandb.log({'scatter-plot1': wandb.plot.scatter(table, \"pred\", \"actual\")})\n        \n    fd2 = pd.DataFrame({\"pred\": MDp,\"actual\":MDp_testt})\n    print(fd2)\n    table2 = wandb.Table(dataframe=fd2)\n    wandb.log({'scatter-plot2': wandb.plot.scatter(table2, \"pred\", \"actual\")})\n        \n    predictions_h = [s for s in Hp_mean]\n    predictions_h\n    table2 = wandb.Table(data=predictions_h, columns=[\"h_predictions\"])\n    wandb.log({'my_histogramM': wandb.plot.histogram(table2, \"h_predictions\",\n    title=\"Prediction Score Distribution Hubble Parameter\")})\n        # hist = np.histogram(predictions_h)\n        # wandb.log({'Hubble parameter': wandb.plot.histogram(hist)})\n        \n        \n    predictions_hh = [ s for s in MDp_mean]\n    predictions_hh\n    table3 = wandb.Table(data=predictions_hh, columns=[\"h_predictions\"])\n    wandb.log({'my_histogram': wandb.plot.histogram(table3, \"h_predictions\",\n    title=\"Prediction Score Distribution Mass Density\")})\n        # hist = np.histogram(predictions_hh)\n        # wandb.log({'Mass Density parameter': wandb.plot.histogram(hist)})\n    run.finish()\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">sweep_config = {\n    'method': 'random',         \n    'metric': {\n        'name': 'test_loss_mean',     \n        'goal': 'minimize'      \n    },\n    'parameters': {\n        'dense_units': {\n            'distribution': 'log_uniform_values',\n            'min': 32,\n            'max': 256\n        },\n        'learning_rate': {\n            'distribution': 'log_uniform_values',\n            'min': 0.0000001,\n            'max': 0.1\n        },\n        'dense_units2': {\n            'distribution': 'log_uniform_values',\n            'min': 32,\n            'max': 256\n        },\n        'batch_size': {\n            #Integers between 32 and 256 \n            # with evenly distributed logarithms\n            'values': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20],\n               \n        },\n        'optimizer': {\n            'values': ['Adam', 'RMSprop']\n        },\n        'num_layers': {\n            'values': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]\n        }\n\n    }\n}\n<\/code><\/pre>\n<pre><code class=\"lang-auto\">sweep_id = wandb.sweep(sweep_config, entity=\"stijnvdbosch\", project=\"custom-charts\")\nwandb.agent(sweep_id, function=train, count=2, project=\"custom-charts\")\n<\/code><\/pre>\n<p>You can also see i\u2019m updating my own <strong>defined metric<\/strong> (not from model.fit) called <strong>test_loss_mean<\/strong>. I suppose I did that correct?<br>\nIf there is any more information you need to help me, then, please, send  me a message and I will reply in a blink. <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/blush.png?v=12\" title=\":blush:\" class=\"emoji\" alt=\":blush:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1665899098604,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":155.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/problem-with-sweep-how-to-use-run-finish-and-log-without-error-question-about-defined-metric\/3260",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-15T05:45:13.425Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"problem sweep us run finish log error question defin metric nice meet current understand us run finish init log correctli constantli get error agent sweep model configur successfulli fold split dont error right fold split model configur appli sweep inform code code bit messi think wai us cross valid scikitlearn iv tri time input output number dataset input dataset number axi input dataset number axi output number nth dataset output number nth dataset input concaten produc output let split output shape output shape output test shape output test shape shape shape test shape test shape order shape think wai defin cross valid sklearn type data think error iv introduc save model time configur load model loop zero weigth wai cross valid succe sure correctli log file code bad produc group want overwrit mention time new model initi sweep error displai sweep agent wait job job receiv tear servic manag follow error occur winerror extern host heeft een verbind verbroken agent start run ekcsgm config batch size dens unit dens unit learn rate num layer optim adam warn ignor init arg project run sweep except thread chkstopthr file inform except thread netstatthr file inform end connectionreseterror winerror extern host heeft een verbind verbroken sent self sock send data connectionreseterror winerror extern host heeft een verbind verbroken error occur continu right creat new model configur time error occur loop model get evalu error occur loop warn ignor init arg project run sweep think definit group want certain log run finish command error error init error occur init loop add sweep overwritten sweep creat group aswel unsur place correctli sound logic defin run init code dont understand us case hope group fold valid seper init recommend welcom code code littl messi sorri def seed seed random seed seed environ pythonhashse str seed random set seed seed def build model config activ config activ dens unit config dens unit dens unit config dens unit num layer config num layer batch size config batch size batch norm fals optim config optim learn rate config learn rate kera layer concaten input input input layer input shape len norm train column len norm train iloc dens unit dens unit activ activ rang num layer dens unit dens unit activ activ dens unit dens unit activ activ output fed directli second dens output dens unit output dens dens unit dens unit activ activ output come dens output dens unit output dens model model input input input output output output print model summari model save weight model overwrit true return model def train test loss sum arrai loss sum arrai mdp loss sum arrai rmse sum arrai mdp rmse sum arrai loss sum arrai loss sum tot append test loss sum tot append mdp append pred append mdp append mdp sum sum test loss mean pred append mdp pred append loss sum tot mdp loss sum tot rmse sum tot mdp rmse sum tot vstack asarrai norm numpi vstack asarrai norm numpi max trial epoch test loss sum arrai train mdp train test test mdp test test hyperparam dict optim adam dens unit batch size epoch ens unit activ relu reducelr kera callback reducelronplateau monitor val loss mode auto factor patienc min delta default min verbos earlystop kera callback earlystop monitor val loss mode auto min delta patienc verbos run init project custom chart config hyperparam reinit true note reinit config config wandcalback callback monitor val loss model build model config config config learn rate config optim adam optim kera optim adam elif config optim rmsprop optim kera optim rmsprop rho epsilon decai rais compil model model compil optim optim loss output mse output mse metric output kera metric rootmeansquarederror output kera metric rootmeansquarederror split skf kfold split shuffl true skf split vall loss train index test index skf split init project custom chart group fold experi job type fold format model load weight model train index train index astyp int test index test index astyp int arrai arrai arrai mdp arrai mdp train test train index test index train test train index test index train test train index test index mdp train mdp test mdp train index mdp test index histori model fit convert tensor convert tensor mdp valid data convert tensor test convert tensor test test mdp test batch size config batch size epoch callback wandcalback earlystop reducelr verbos loss sum datafram histori histori loss iloc loss sum test loss sum datafram histori histori val loss iloc test loss sum loss sum datafram histori histori val output loss iloc loss sum mdp loss sum datafram histori histori val output loss iloc mdp loss sum rmse sum datafram histori histori val output root mean squar error iloc rmse sum mdp rmse sum datafram histori histori val output root mean squar error iloc mdp rmse sum loss sum tot datafram histori histori loss loss sum tot test loss sum tot datafram histori histori val loss test loss sum tot loss sum tot datafram histori histori val output loss loss sum tot mdp loss sum tot datafram histori histori val output loss mdp loss sum tot rmse sum tot datafram histori histori val output root mean squar error rmse sum tot mdp rmse sum tot datafram histori histori val output root mean squar error mdp rmse sum tot pred model predict convert tensor test convert tensor test metric tfa metric squar rsquar metric updat state test pred flatten result metric result result numpi append append metric updat state mdp test pred flatten result metric result mdp result numpi mdp append append mdp append append test mdp append append mdp test pred append append pred mdp pred append append pred mdp sum mdp sum pred sum sum pred test loss mean test loss sum split loss sum mean loss sum split test loss sum mean test loss sum split loss sum mean loss sum split mdp loss sum mean mdp loss sum split rmse sum mean rmse sum split mdp rmse sum mean mdp rmse sum split test mdp mean mdp append test mean append mean sum split mdp mean mdp sum split init project sweep optimalis randomsearch group experimentfold format job type valid init project custom chart group fold experi job type valid val lo rang len test loss sum tot log val loss mean test loss sum tot val lo split loss rang len loss sum tot log loss mean loss sum tot loss split val mdp lo rang len test loss sum tot log val mdp loss mean test loss sum tot val mdp lo split val lo rang len loss sum tot log val loss mean loss sum tot val lo split val mdp rmse rang len mdp rmse sum tot log val mdp rmse mean mdp loss sum tot val mdp rmse split val rmse rang len rmse sum tot log val mdp rmse mean rmse sum tot val rmse split append concaten append mdp append concaten mdp append pred append concaten pred append mdp pred append concaten mdp pred append score sqrt mean squar error pred append append mdp score sqrt mean squar error mdp pred append mdp append test mdp mean mdp append test mean append log score score score mdp mdp score test mdp test mdp asarrai mean flatten mdp asarrai mdp mean flatten testt asarrai test flatten mdp testt asarrai mdp test flatten datafram pred actual testt print tabl tabl datafram log scatter plot plot scatter tabl pred actual datafram pred mdp actual mdp testt print tabl tabl datafram log scatter plot plot scatter tabl pred actual predict mean predict tabl tabl data predict column predict log histogramm plot histogram tabl predict titl predict score distribut hubbl paramet hist histogram predict log hubbl paramet plot histogram hist predict mdp mean predict tabl tabl data predict column predict log histogram plot histogram tabl predict titl predict score distribut mass densiti hist histogram predict log mass densiti paramet plot histogram hist run finish sweep config method random metric test loss mean goal minim paramet dens unit distribut log uniform valu min max learn rate distribut log uniform valu min max dens unit distribut log uniform valu min max batch size integ evenli distribut logarithm valu optim valu adam rmsprop num layer valu sweep sweep sweep config entiti stijnvdbosch project custom chart agent sweep function train count project custom chart updat defin metric model fit call test loss mean suppos correct inform need help send messag repli blink",
        "Question_preprocessed_content":"problem sweep us log error question defin metric nice meet current understand us init log correctli constantli get error agent sweep model configur successfulli split dont error right split model configur appli sweep inform code code bit messi think wai us cross valid scikitlearn iv tri time input output input dataset number input dataset number output number nth dataset output number nth dataset input concaten produc output let split order shape think wai defin cross valid sklearn type data think error iv introduc save model time configur load model loop zero weigth wai cross valid succe sure correctli log file code bad produc group want overwrit mention time new model initi sweep error displai error occur continu right creat new model configur time error occur loop model get evalu error occur loop warn ignor init arg project run sweep think definit group want certain log command error occur init loop add sweep overwritten sweep creat group aswel unsur place correctli sound logic defin run init dont understand us case hope group fold valid seper init recommend welcom code code littl messi sorri updat defin metric call suppos correct inform need help send messag repli blink",
        "Answer_original_content":"topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering challenges with using run.finish() and .init for logging correctly, as well as defining a metric and grouping folds and validation separately.",
        "Question_gpt_summary":"user encount challeng run finish init log correctli defin metric group fold valid separ",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the user's challenges with using run.finish() and .init for logging correctly, as well as defining a metric and grouping folds and validation separately. the topic has been closed for 60 days and no new replies are allowed.",
        "Answer_gpt_summary":"solut provid answer user challeng run finish init log correctli defin metric group fold valid separ topic close dai new repli allow"
    },
    {
        "Question_id":67941614.0,
        "Question_title":"Kedro install fail to install, but few attempt later it is successful",
        "Question_body":"<p>I have to test if my kedro project works from github so I create a new environment, then :<\/p>\n<pre><code>git clone &lt;my_project&gt;\npip install kedro kedro[pandas] kedro-viz jupyter\nkedro build-reqs\nkedro install\n<\/code><\/pre>\n<p>and the install fails, then I retry a few time (sometimes 2 or 3) then the next attempt it is successful<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/4KTui.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4KTui.png\" alt=\"see image\" \/><\/a><\/p>\n<p>EDIT:\npython -V : Python 3.7.10\nkedro --version : kedro, version 0.17.3<\/p>\n<p>i cant post my requirement.txt (post is mostly code) so here is my requirement.in<\/p>\n<pre><code>black==v19.10b0\nflake8&gt;=3.7.9, &lt;4.0\nipython==7.10\nisort&gt;=4.3.21, &lt;5.0\njupyter~=1.0\njupyter_client&gt;=5.1, &lt;7.0\njupyterlab==0.31.1\nkedro==0.17.3\nnbstripout==0.3.3\npytest-cov~=2.5\npytest-mock&gt;=1.7.1, &lt;2.0\npytest~=6.1.2\nwheel==0.32.2\nspacy&gt;=3.0.0,&lt;4.0.0\nscikit-learn == 0.24.2\nkedro-viz==3.11.0\nwordcloud== 1.8.1\nhttps:\/\/github.com\/explosion\/spacy-models\/releases\/download\/fr_core_news_sm-3.0.0\/fr_core_news_sm-3.0.0.tar.gz#egg=fr_core_news_sm\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1623434458303,
        "Question_favorite_count":null,
        "Question_last_edit_time":1623788215208,
        "Question_score":2.0,
        "Question_view_count":594.0,
        "Answer_body":"<p>As indicated in the comment, I think there are two issues at play.<\/p>\n<h4>1. Decoding error<\/h4>\n<p>This is the main exception you're getting, i.e.:<\/p>\n<pre><code>UnicodeDecodeError: \u2018utf-8\u2019 codec can\u2019t decode byte 0xe8 in position 69: invalid continuation byte\n<\/code><\/pre>\n<p>This is unexpectedly raised while Kedro itself is handling the errors from <code>pip install<\/code> (see <a href=\"https:\/\/github.com\/quantumblacklabs\/kedro\/blob\/master\/kedro\/framework\/cli\/project.py#L172\" rel=\"nofollow noreferrer\">this line of Kedro's source code<\/a>). I believe the cause might be that you have accented characters in your working directory, which can't be interpreted by Python's standard <code>decode()<\/code> (see <a href=\"https:\/\/stackoverflow.com\/questions\/49898909\/reading-a-file-with-french-characters-in-python\">this<\/a>). Example:<\/p>\n<pre><code>b'acc\u00e9l\u00e9ration'.decode()\n&gt;&gt; SyntaxError: bytes can only contain ASCII literal characters.\n<\/code><\/pre>\n<p>The decoding error is obscuring the actual <code>pip install<\/code> error.<\/p>\n<h4>2. <code>pip install<\/code> error<\/h4>\n<p>As you correctly pointed out, <code>kedro install<\/code> uses <code>pip install<\/code> under the hood. It's a bit difficult to pinpoint the exact cause without seeing the actual error. I could however reproduce a similar issue, in my case getting the following error:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\users\\\\&lt;mu-user&gt;\\\\anaconda3\\\\&lt;my-env&gt;\\\\kedro_project_tests\\\\lib\\\\site-packages\\\\~ydantic\\\\annotated_types.cp37-win_amd64.pyd'\nConsider using the `--user` option or check the permissions.\n<\/code><\/pre>\n<p>I believe this is caused by interactions caused by between different versions Kedro and Kedro-Viz. Simply not <code>pip install<\/code>ing <code>kedro-viz<\/code> <em>before<\/em> doing <code>kedro install<\/code> fixed it for me.<\/p>\n<hr \/>\n<p><em>Note: Related to this, there will surely be an error if the version of Kedro installed through <code>pip<\/code> <em>before<\/em> doing <code>kedro install<\/code> is not the same as the version of Kedro specified in <code>requirements.in<\/code> or <code>requirements.txt<\/code>. This is obvious, as the package currently handling execution will be uninstalled. The error in this case will be something like this:<\/em><\/p>\n<pre><code>ERROR: Could not install packages due to an OSError: [WinError 32] The process cannot access the file because it is being used by another process: 'c:\\\\users\\\\&lt;my-user&gt;\\\\anaconda3\\\\envs\\\\&lt;my-env&gt;\\\\scripts\\\\kedro.exe\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67941614",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1623941002452,
        "Question_original_content":"instal fail instal attempt later success test project work github creat new environ git clone pip instal panda viz jupyt build req instal instal fail retri time attempt success edit python python version version post requir txt post code requir black flake scikit learn viz wordcloud http github com explos spaci model releas download core new core new tar egg core new",
        "Question_preprocessed_content":"instal fail instal attempt later success test project work github creat new environ instal fail retri time attempt success edit python python version post",
        "Answer_original_content":"indic comment think issu plai decod error main except get unicodedecodeerror utf codec decod byte posit invalid continu byte unexpectedli rais handl error pip instal line sourc code believ caus accent charact work directori interpret python standard decod exampl acclrat decod syntaxerror byte contain ascii liter charact decod error obscur actual pip instal error pip instal error correctli point instal us pip instal hood bit difficult pinpoint exact caus see actual error reproduc similar issu case get follow error error instal packag oserror winerror access deni user anaconda project test lib site packag ydantic annot type win amd pyd consid user option check permiss believ caus interact caus differ version viz simpli pip instal viz instal fix note relat sure error version instal pip instal version specifi requir requir txt obviou packag current handl execut uninstal error case like error instal packag oserror winerror process access file process user anaconda env script ex",
        "Answer_preprocessed_content":"indic comment think issu plai decod error main except get unexpectedli rais handl error believ caus accent charact work directori interpret python standard exampl decod error obscur actual error error correctli point us hood bit difficult pinpoint exact caus see actual error reproduc similar issu case get follow error believ caus interact caus differ version viz simpli ing fix note relat sure error version instal version specifi obviou packag current handl execut uninstal error case like",
        "Question_gpt_summary_original":"the user encountered challenges while attempting to install a project from github, requiring multiple attempts before the install was successful.",
        "Question_gpt_summary":"user encount challeng attempt instal project github requir multipl attempt instal success",
        "Answer_gpt_summary_original":"possible solutions from the answer include: \n1. the decoding error might be caused by accented characters in the working directory, which can't be interpreted by python's standard decode(). \n2. the pip install error might be caused by interactions between different versions and can be fixed by using the `--user` option or checking the permissions. \n3. there might be an error if the version of the package installed through pip before doing install is not the same as the version specified in requirements.in or requirements.txt.",
        "Answer_gpt_summary":"possibl solut answer includ decod error caus accent charact work directori interpret python standard decod pip instal error caus interact differ version fix user option check permiss error version packag instal pip instal version specifi requir requir txt"
    },
    {
        "Question_id":null,
        "Question_title":"Response status code does not indicate success: 400 (Conda dependencies were not specified. Please make sure that all conda dependencies were specified i).",
        "Question_body":"I was trying to setup my env runconfig = ScriptRunConfig(source_directory='script\/', script='my-script.py', arguments=script_params)\nrunconfig.run_config.target = compute_target\nrunconfig.run_config.environment = env\nrun = exp.submit(runconfig)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1667250048510,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1069928\/response-status-code-does-not-indicate-success-400.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-01T07:37:12.75Z",
                "Answer_score":0,
                "Answer_body":"Hello @jackson-0025\n\nThanks for using Microsoft Q&A platform. I have seen a very similar question as your, and the solution is you need to use RunConfiguration instead of ScriptRunConfig. More info here\n\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-getting-started.ipynb\n\n from azureml.core.runconfig import RunConfiguration\n    \n env = Environment.get(workspace=ws, name='my-environment', version='1')\n # create a new runconfig object\n runconfig = RunConfiguration()\n runconfig.environment = env\n    \n pipeline_step = PythonScriptStep(\n     source_directory='script', script_name='my-script.py',\n     arguments=['-a', param1, '-b', param2],\n     compute_target=compute_target,\n     runconfig=runconfig\n )\n    \n pipeline = Pipeline(workspace=ws, steps=[pipeline_step])\n    \n pipeline_run = Experiment(ws, 'my_pipeline_run').submit(pipeline)\n\nReference for the issue - https:\/\/stackoverflow.com\/questions\/60506398\/how-do-i-use-an-environment-in-an-ml-azure-pipeline\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpfuk to support the community, thanks a lot.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"respons statu code indic success conda depend specifi sure conda depend specifi try setup env runconfig scriptrunconfig sourc directori script script script argument script param runconfig run config target comput target runconfig run config environ env run exp submit runconfig",
        "Question_preprocessed_content":"respons statu code indic success try setup env runconfig env run",
        "Answer_original_content":"hello jackson thank microsoft platform seen similar question solut need us runconfigur instead scriptrunconfig info http github com azur machinelearningnotebook blob master us machin learn pipelin intro pipelin aml pipelin get start ipynb core runconfig import runconfigur env environ workspac environ version creat new runconfig object runconfig runconfigur runconfig environ env pipelin step pythonscriptstep sourc directori script script script argument param param comput target comput target runconfig runconfig pipelin pipelin workspac step pipelin step pipelin run experi pipelin run submit pipelin refer issu http stackoverflow com question us environ azur pipelin hope help regard yutong kindli accept answer feel helpfuk support commun thank lot",
        "Answer_preprocessed_content":"hello thank microsoft platform seen similar question solut need us runconfigur instead scriptrunconfig info import runconfigur env version creat new runconfig object runconfig runconfigur env pythonscriptstep pipelin pipelin experi submit refer issu hope help regard yutong kindli accept answer feel helpfuk support commun thank lot",
        "Question_gpt_summary_original":"the user encountered a challenge with setting up their environment, as the response status code indicated a failure due to conda dependencies not being specified.",
        "Question_gpt_summary":"user encount challeng set environ respons statu code indic failur conda depend specifi",
        "Answer_gpt_summary_original":"the solution to the challenge of setting up the environment is to use runconfiguration instead of scriptrunconfig. the user needs to create a new runconfig object and set the environment to the specified dependencies. the reference for the issue is provided in the answer.",
        "Answer_gpt_summary":"solut challeng set environ us runconfigur instead scriptrunconfig user need creat new runconfig object set environ specifi depend refer issu provid answer"
    },
    {
        "Question_id":null,
        "Question_title":"Vision API quota\/budget limit and API key help",
        "Question_body":"Hello, I have never used Vision API before but I recently found it very powerful for a project of mine. However I have two concerns regarding its budget limiting, in order to not get an unexpected bill:Thanks everyone for any help!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1648805580000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":66.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vision-API-quota-budget-limit-and-API-key-help\/td-p\/409566\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-14T12:31:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi\u00a0\n\nYou can set a cap of api usage\u00a0or a billing budget might help you on this issue. Does it meet your needs?\n\nAbout your security question about \"hard code the key in Android\/iOS\", I recommend you to discuss\u00a0on Stack Overflow for better help."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"vision api quota budget limit api kei help hello vision api recent power project concern budget limit order unexpect thank help",
        "Question_preprocessed_content":"vision api limit api kei help hello vision api recent power project concern budget limit order unexpect thank help",
        "Answer_original_content":"set cap api usageor bill budget help issu meet need secur question hard code kei android io recommend discusson stack overflow better help",
        "Answer_preprocessed_content":"set cap api usageor bill budget help issu meet need secur question hard code kei recommend discusson stack overflow better help",
        "Question_gpt_summary_original":"the user is seeking help with understanding the budget limiting of the vision api and obtaining an api key.",
        "Question_gpt_summary":"user seek help understand budget limit vision api obtain api kei",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. set a cap on api usage.\n2. set a billing budget.\n3. seek help on stack overflow for better understanding of how to hard code the api key in android\/ios.",
        "Answer_gpt_summary":"possibl solut answer set cap api usag set bill budget seek help stack overflow better understand hard code api kei android io"
    },
    {
        "Question_id":null,
        "Question_title":"Use S3 as a git repo",
        "Question_body":"I have a sagemaker notebook that has no connections to internet or codecommit but has access to 1 s3 bucket. I would like to use that 1 s3 bucket as a place to house git repos, ideally I would like to be able to pull\/push to repos in that bucket from other sagemaker notebooks or ec2 instances that have connections to that bucket. Has anyone tried this before?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1656607653458,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":236.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUeFHB_qvQw67d9knOyw1Ig\/use-s-3-as-a-git-repo",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-30T16:53:27.277Z",
                "Answer_score":1,
                "Answer_body":"Hi! I believe from AWS we don't have an official solution for this strategy. The are some solutions out there like https:\/\/github.com\/bgahagan\/git-remote-s3 you could use if you can upload the latest release to the S3 bucket and install it on your notebook. Hope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-01T12:12:07.011Z",
                "Answer_score":0,
                "Answer_body":"Check out these assets to see if they can help to accomplish this: https:\/\/aws.amazon.com\/blogs\/machine-learning\/how-to-use-common-workflows-on-amazon-sagemaker-notebook-instances\/ https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"us git repo notebook connect internet codecommit access bucket like us bucket place hous git repo ideal like abl pull push repo bucket notebook instanc connect bucket tri",
        "Question_preprocessed_content":"us git repo notebook connect internet codecommit access bucket like us bucket place hous git repo ideal like abl repo bucket notebook instanc connect bucket tri",
        "Answer_original_content":"believ aw offici solut strategi solut like http github com bgahagan git remot us upload latest releas bucket instal notebook hope help check asset help accomplish http aw amazon com blog machin learn us common workflow amazon notebook instanc http doc aw amazon com latest nbi git repo html",
        "Answer_preprocessed_content":"believ aw offici solut strategi solut like us upload latest releas bucket instal notebook hope help check asset help accomplish",
        "Question_gpt_summary_original":"the user is looking for a way to use an s3 bucket as a place to house git repos, and to be able to pull\/push to repos in that bucket from other notebooks or ec2 instances that have connections to that bucket.",
        "Question_gpt_summary":"user look wai us bucket place hous git repo abl pull push repo bucket notebook instanc connect bucket",
        "Answer_gpt_summary_original":"possible solutions from the answer include using a third-party solution like git-remote-s3, uploading the latest release to the s3 bucket, and installing it on the notebook. the answer also suggests checking out some aws resources like the common workflows on amazon notebook instances and the documentation on using git repos.",
        "Answer_gpt_summary":"possibl solut answer includ parti solut like git remot upload latest releas bucket instal notebook answer suggest check aw resourc like common workflow amazon notebook instanc document git repo"
    },
    {
        "Question_id":73388133.0,
        "Question_title":"Is there a way for Optuna `suggest_categorical`to return multiple choices from list?",
        "Question_body":"<p>I am using Optuna for hyperparametrization of my model. And i have a field where I want to test multiple combinations from a list. For example: I have <code>[&quot;lisa&quot;,&quot;adam&quot;,&quot;test&quot;]<\/code> and i want <code>suggest_categorical<\/code> to return not just one, but a random combination: maybe <code>[&quot;lisa&quot;, &quot;adam&quot;]<\/code>, maybe <code>[&quot;adam&quot;]<\/code>, maybe <code>[&quot;lisa&quot;, &quot;adam&quot;, &quot;test&quot;]<\/code>. Is there a way to get this with built in Optuna function?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660737627797,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Answer_body":"<p>You could use <code>itertools.combinations<\/code> to generate all possible combinations of list items and then pass them to optuna's <code>suggest_categorical<\/code> as choices:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport itertools\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# generate the combinations\niterable = ['lisa', 'adam', 'test']\ncombinations = []\nfor r in range(1, len(iterable) + 1):\n    combinations.extend([list(x) for x in itertools.combinations(iterable=iterable, r=r)])\nprint(combinations)\n# [['lisa'], ['adam'], ['test'], ['lisa', 'adam'], ['lisa', 'test'], ['adam', 'test'], ['lisa', 'adam', 'test']]\n\n# sample the combinations\ndef objective(trial):\n    combination = trial.suggest_categorical(name='combination', choices=combinations)\n    return round(random.random(), 2)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=3)\n# [I 2022-08-18 08:03:51,658] A new study created in memory with name: no-name-3874ce95-2394-4526-bb19-0d9822d7e45c\n# [I 2022-08-18 08:03:51,659] Trial 0 finished with value: 0.94 and parameters: {'combination': ['adam']}. Best is trial 0 with value: 0.94.\n# [I 2022-08-18 08:03:51,660] Trial 1 finished with value: 0.87 and parameters: {'combination': ['lisa', 'test']}. Best is trial 1 with value: 0.87.\n# [I 2022-08-18 08:03:51,660] Trial 2 finished with value: 0.29 and parameters: {'combination': ['lisa', 'adam']}. Best is trial 2 with value: 0.29.\n<\/code><\/pre>\n<p>Using lists as choices in optuna's <code>suggest_categorical<\/code> throws a warning message, but apparently this is mostly inconsequential (see <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2341\" rel=\"nofollow noreferrer\">this issue<\/a> in optuna's GitHub repository).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1660803027380,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73388133",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660802705056,
        "Question_original_content":"wai suggest categor return multipl choic list hyperparametr model field want test multipl combin list exampl lisa adam test want suggest categor return random combin mayb lisa adam mayb adam mayb lisa adam test wai built function",
        "Question_preprocessed_content":"wai return multipl choic list hyperparametr model field want test multipl combin list exampl want return random combin mayb mayb mayb wai built function",
        "Answer_original_content":"us itertool combin gener possibl combin list item pass suggest categor choic import import itertool import random import warn warn filterwarn ignor gener combin iter lisa adam test combin rang len iter combin extend list itertool combin iter iter print combin lisa adam test lisa adam lisa test adam test lisa adam test sampl combin def object trial combin trial suggest categor combin choic combin return round random random studi creat studi studi optim object trial new studi creat memori ddec trial finish valu paramet combin adam best trial valu trial finish valu paramet combin lisa test best trial valu trial finish valu paramet combin lisa adam best trial valu list choic suggest categor throw warn messag appar inconsequenti issu github repositori",
        "Answer_preprocessed_content":"us gener possibl combin list item pass choic list choic throw warn messag appar inconsequenti",
        "Question_gpt_summary_original":"The user is trying to use Optuna's `suggest_categorical` function to test multiple combinations from a list, but is encountering difficulty in getting the function to return more than one choice at a time.",
        "Question_gpt_summary":"user try us suggest categor function test multipl combin list encount difficulti get function return choic time",
        "Answer_gpt_summary_original":"Possible solutions extracted from the answer are:\n\n- Use itertools.combinations to generate all possible combinations of list items.\n- Pass the generated combinations to optuna's suggest_categorical as choices.\n- Sample the combinations using optuna's create_study and optimize functions.\n\nThe answer provides a code snippet that demonstrates how to implement these solutions. Additionally, it mentions a warning message that may be thrown when using lists as choices in optuna's suggest_categorical, but notes that this warning is mostly inconsequential.",
        "Answer_gpt_summary":"possibl solut extract answer us itertool combin gener possibl combin list item pass gener combin suggest categor choic sampl combin creat studi optim function answer provid code snippet demonstr implement solut addition mention warn messag thrown list choic suggest categor note warn inconsequenti"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker Canvas connect Redshift failed",
        "Question_body":"Try to add the Redshift connection on SageMaker Canvas to import the data\n\nThe cluster identify: redshift-cluster-1\ndatabase name: dev\ndatabase user: awsuser\nunload IAM Role: my-reshift-role\nconnection name: redshift\ntype: IAM\n\nmy-reshift-role trust-relationship is trust the \"redshift.amazonaws.com\" and \"sagemaker.amazonaws.com\"\n\nExpectation: create connection successfully\n\nActually result: RedshiftCreateConnectionError Unable to validate connection. An error occurred when trying to list schema from Redshift",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641573857407,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":135.0,
        "Answer_body":"The sagemaker canvas using sagemaker domain user, so need add the Redshift permission to the IAM Role attached to domain user. After add the permission, the connection can be setup",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJvjatAJaQv-Ist96WT1IIw\/sage-maker-canvas-connect-redshift-failed",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-08T09:30:21.918Z",
                "Answer_score":0,
                "Answer_body":"The sagemaker canvas using sagemaker domain user, so need add the Redshift permission to the IAM Role attached to domain user. After add the permission, the connection can be setup",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641634221918,
        "Question_original_content":"canva connect redshift fail try add redshift connect canva import data cluster identifi redshift cluster databas dev databas user awsus unload iam role reshift role connect redshift type iam reshift role trust relationship trust redshift amazonaw com amazonaw com expect creat connect successfulli actual result redshiftcreateconnectionerror unabl valid connect error occur try list schema redshift",
        "Question_preprocessed_content":"canva connect redshift fail try add redshift connect canva import data cluster identifi databas dev databas user awsus unload iam role connect redshift type iam trust expect creat connect successfulli actual result redshiftcreateconnectionerror unabl valid connect error occur try list schema redshift",
        "Answer_original_content":"canva domain user need add redshift permiss iam role attach domain user add permiss connect setup",
        "Answer_preprocessed_content":"canva domain user need add redshift permiss iam role attach domain user add permiss connect setup",
        "Question_gpt_summary_original":"the user encountered a challenge when attempting to create a connection to redshift on canvas, resulting in a \"redshiftcreateconnectionerror\" due to an inability to validate the connection.",
        "Question_gpt_summary":"user encount challeng attempt creat connect redshift canva result redshiftcreateconnectionerror inabl valid connect",
        "Answer_gpt_summary_original":"to resolve the \"redshiftcreateconnectionerror\" when creating a connection to redshift on canvas, the user needs to add redshift permission to the iam role attached to the domain user. once the permission is added, the connection can be set up successfully.",
        "Answer_gpt_summary":"resolv redshiftcreateconnectionerror creat connect redshift canva user need add redshift permiss iam role attach domain user permiss ad connect set successfulli"
    },
    {
        "Question_id":null,
        "Question_title":"Advantage of clustering algorithm",
        "Question_body":"We don't need the name of attributes in clustering, so, if I do not know my attribute names how can I understand that which data should I enter and also if I do not know the name of the attributes how can I give the axis name of the plotted graph? If it is the advantage of clustering algorithm, then why it is not the advantage of other algorithms, because if we do not know the name of the attributes we can create our models because if we have attribute values that is enough, but why we need attribute names too in other algorithms?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1608719822847,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/208687\/advantage-of-clustering-algorithm.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-23T16:35:59.897Z",
                "Answer_score":0,
                "Answer_body":"@SanniddhaChakrabarti-9451 Thanks, If the products data is not already labeled and ready for training, you can start with a clustering problem.\nOnce you identify those clusters, a Domain Expert can review those clusters and try to set a name for each (the Categories\/classes).\nThen, all the data can be labeled according to those new categories\/classes and finally train a model.\n\nFrom there, with a trained model, you could \u201cpredict\u201d what category\/class a product should be assigned based on its product\u2019s name and description.\n\n\u2022 For the clustering problem you need to directly use a framework such as Scikit-Learn (or even ML.NET in C#).\n\u2022 Defining a category\/class name for each identified cluster needs to be done manually by a Domain Expert.\n\u2022 Labeling the data could be semi-automated with a custom program, based on the multiple clusters defined, it would label each row with a new class-caterogy-name defined for each of the identified clusters.\n\u2022 For the multi-class classification model training, you can use Azure Automated ML as the easiest approach.",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"advantag cluster algorithm need attribut cluster know attribut name understand data enter know attribut axi plot graph advantag cluster algorithm advantag algorithm know attribut creat model attribut valu need attribut name algorithm",
        "Question_preprocessed_content":"advantag cluster algorithm need attribut cluster know attribut name understand data enter know attribut axi plot graph advantag cluster algorithm advantag algorithm know attribut creat model attribut valu need attribut name algorithm",
        "Answer_original_content":"sanniddhachakrabarti thank product data label readi train start cluster problem identifi cluster domain expert review cluster try set categori class data label accord new categori class final train model train model predict categori class product assign base product descript cluster problem need directli us framework scikit learn net defin categori class identifi cluster need manual domain expert label data semi autom custom program base multipl cluster defin label row new class caterogi defin identifi cluster multi class classif model train us azur autom easiest approach",
        "Answer_preprocessed_content":"thank product data label readi train start cluster problem identifi cluster domain expert review cluster try set data label accord new final train model train model predict product assign base product descript cluster problem need directli us framework defin identifi cluster need manual domain expert label data custom program base multipl cluster defin label row new defin identifi cluster classif model train us azur autom easiest approach",
        "Question_gpt_summary_original":"the user is questioning the advantage of clustering algorithms over other algorithms, as clustering does not require knowledge of attribute names, while other algorithms do.",
        "Question_gpt_summary":"user question advantag cluster algorithm algorithm cluster requir knowledg attribut name algorithm",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- clustering can be used to identify clusters of data points without prior knowledge of attribute names.\n- a domain expert can review the clusters and assign names to each cluster.\n- all the data can be labeled according to the new categories\/classes and a model can be trained.\n- labeling the data could be semi-automated with a custom program.\n- azure automated ml can be used for multi-class classification model training. \n\nin summary, clustering can be used to identify clusters of data points, which can be reviewed by a domain expert and labeled accordingly. a model can then be trained using the labeled data, and azure automated ml can be used for multi-class classification model training.",
        "Answer_gpt_summary":"possibl solut extract answer cluster identifi cluster data point prior knowledg attribut name domain expert review cluster assign name cluster data label accord new categori class model train label data semi autom custom program azur autom multi class classif model train summari cluster identifi cluster data point review domain expert label accordingli model train label data azur autom multi class classif model train"
    },
    {
        "Question_id":null,
        "Question_title":"How to save configs?",
        "Question_body":"<p>I can\u2019t figure out how to save configurations.<\/p>\n<p>After initilization I do <code>wandb.config.update(model_config)<\/code> where <code>model_config<\/code> is a dict.<\/p>\n<p>However, it does not sync with the user interface, that\u2019s still true after calling <code>wandb.finish()<\/code>.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b7062521caebaa48838e51e3df8a358947ae83f1.png\" data-download-href=\"\/uploads\/short-url\/q76xEv6yugWsGPOs0ho5RCULGGl.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b7062521caebaa48838e51e3df8a358947ae83f1_2_690x271.png\" alt=\"image\" data-base62-sha1=\"q76xEv6yugWsGPOs0ho5RCULGGl\" width=\"690\" height=\"271\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b7062521caebaa48838e51e3df8a358947ae83f1_2_690x271.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b7062521caebaa48838e51e3df8a358947ae83f1_2_1035x406.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b7062521caebaa48838e51e3df8a358947ae83f1.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b7062521caebaa48838e51e3df8a358947ae83f1_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1245\u00d7490 30.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1654795000865,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-save-configs\/2587",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-10T15:50:11.687Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/scientya\">@scientya<\/a>, can you try passing the config to <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/init\">wandb.init<\/a>?<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-14T02:23:10.824Z",
                "Answer_body":"<p>Hi there, I wanted to follow up on this request. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-14T04:51:57.377Z",
                "Answer_body":"<p>Hey there, since we have not heard back from you, I\u2019ll be closing this ticket. But please message me if you need assistance on this.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-13T04:52:27.317Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save config figur save configur initil config updat model config model config dict sync user interfac that true call finish imag",
        "Question_preprocessed_content":"save config figur save configur initil dict sync user interfac that true call imag",
        "Answer_original_content":"hei scientya try pass config init want follow request let know assist issu resolv hei heard ill close ticket messag need assist topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei try pass config init want follow request let know assist issu resolv hei heard ill close ticket messag need assist topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is struggling to save configurations after initializing and updating a model configuration dict, and is unable to sync the user interface after calling .finish().",
        "Question_gpt_summary":"user struggl save configur initi updat model configur dict unabl sync user interfac call finish",
        "Answer_gpt_summary_original":"there are no specific solutions mentioned in the answer. the responder suggests passing the configuration to .init() and offers further assistance if needed. however, since there was no response from the user, the ticket was closed automatically after 60 days.",
        "Answer_gpt_summary":"specif solut mention answer respond suggest pass configur init offer assist need respons user ticket close automat dai"
    },
    {
        "Question_id":50271174.0,
        "Question_title":"How to make a Python Visualization as service | Integrate with website | specially sagemaker",
        "Question_body":"<p>I am from R background where we can use Plumber kind tool which provide visualization\/graph as Image via end points so we can integrate in our Java application.<\/p>\n\n<p>Now I want to integrate my Python\/Juypter visualization graph with my Java application but not sure how to host it and make it as endpoint. Right now I using AWS sagemaker to host Juypter notebook<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1525949293883,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":"<p>Amazon SageMaker is a set of different services for data scientists. You are using the notebook service that is used for developing ML models in an interactive way. The hosting service in SageMaker is creating an endpoint based on a trained model. You can call this endpoint with invoke-endpoint API call for real time inference. <\/p>\n\n<p>It seems that you are looking for a different type of hosting that is more suitable for serving HTML media rich pages, and doesn\u2019t fit into the hosting model of SageMaker. A combination of EC2 instances, with pre-built AMI or installation scripts, Congnito for authentication, S3 and EBS for object and block storage, and similar building blocks should give you a scalable and cost effective solution. <\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50271174",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1526073355692,
        "Question_original_content":"python visual servic integr websit special background us plumber kind tool provid visual graph imag end point integr java applic want integr python juypter visual graph java applic sure host endpoint right host juypter notebook",
        "Question_preprocessed_content":"python visual servic integr websit special background us plumber kind tool provid imag end point integr java applic want integr visual graph java applic sure host endpoint right host juypter notebook",
        "Answer_original_content":"set differ servic data scientist notebook servic develop model interact wai host servic creat endpoint base train model endpoint invok endpoint api real time infer look differ type host suitabl serv html media rich page doesnt fit host model combin instanc pre built ami instal script congnito authent eb object block storag similar build block scalabl cost effect solut",
        "Answer_preprocessed_content":"set differ servic data scientist notebook servic develop model interact wai host servic creat endpoint base train model endpoint api real time infer look differ type host suitabl serv html media rich page doesnt fit host model combin instanc ami instal script congnito authent eb object block storag similar build block scalabl cost effect solut",
        "Question_gpt_summary_original":"the user is looking for a way to integrate their python\/jupyter visualizations into their java application, and is unsure of how to host it and make it an endpoint.",
        "Question_gpt_summary":"user look wai integr python jupyt visual java applic unsur host endpoint",
        "Answer_gpt_summary_original":"possible solutions for integrating python\/jupyter visualizations into a java application and hosting it as an endpoint include using a combination of ec2 instances, pre-built ami or installation scripts, cognito for authentication, s3 and ebs for object and block storage, and similar building blocks to create a scalable and cost-effective solution. the hosting service in the answer is not suitable for serving html media-rich pages.",
        "Answer_gpt_summary":"possibl solut integr python jupyt visual java applic host endpoint includ combin instanc pre built ami instal script cognito authent eb object block storag similar build block creat scalabl cost effect solut host servic answer suitabl serv html media rich page"
    },
    {
        "Question_id":null,
        "Question_title":"Project in Data Labeling not working, getting only \"Loading project details\"",
        "Question_body":"Project in Azure Machine Learning Studio in Data Labeling was working, we did label it every day. One day we just could not open it, it showed only Loading project details.. for hours.\nSAS token is working, it also work for our admin account, but not for Labellers.\nDo you have any idea?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647676305203,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/778679\/project-in-data-labeling-not-working-getting-only.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-21T16:36:47.9Z",
                "Answer_score":0,
                "Answer_body":"@ika-8686 Thanks for the question. Please share details of your experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue from the back-end? This option is available from the top right hand corner of the portal by clicking the smiley face, Please select the option Microsoft can email you about the feedback along with a screen shot so our service team can lookup and advise through email.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"project data label work get load project detail project studio data label work label dai dai open show load project detail hour sa token work work admin account label idea",
        "Question_preprocessed_content":"project data label work get load project detail project studio data label work label dai dai open show load project hour sa token work work admin account label idea",
        "Answer_original_content":"ika thank question share detail experi issu azur com portal servic engin lookup issu end option avail right hand corner portal click smilei face select option microsoft email feedback screen shot servic team lookup advis email",
        "Answer_preprocessed_content":"thank question share detail experi issu portal servic engin lookup issu option avail right hand corner portal click smilei face select option microsoft email feedback screen shot servic team lookup advis email",
        "Question_gpt_summary_original":"the user encountered a challenge with a data labeling project in studio, where it was only showing \"loading project details\" for hours and not allowing access to the labellers.",
        "Question_gpt_summary":"user encount challeng data label project studio show load project detail hour allow access label",
        "Answer_gpt_summary_original":"the answer suggests that the user should share details of their experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue from the back-end. the user can do this by clicking the smiley face in the top right-hand corner of the portal and selecting the option for microsoft to email them about the feedback along with a screenshot. the service team will then be able to advise the user through email.",
        "Answer_gpt_summary":"answer suggest user share detail experi issu azur com portal servic engin lookup issu end user click smilei face right hand corner portal select option microsoft email feedback screenshot servic team abl advis user email"
    },
    {
        "Question_id":66887340.0,
        "Question_title":"How do you clear the persistent storage for a notebook instance on AWS SageMaker?",
        "Question_body":"<p>So I'm running into the following error on AWS SageMaker when trying to save:<\/p>\n<blockquote>\n<p>Unexpected error while saving file: untitled.ipynb [Errno 28] No space left on device<\/p>\n<\/blockquote>\n<p>If I remove my notebook, create a new identical one and run it, everything works fine. However, I'm suspecting the Jupyter checkpoint takes up too much space if I save the notebook while it's running and therefore I'm running out of space. Sadly, getting more storage is not an option for me, so I'm wondering if there's any command I can use to clear the storage before running my notebook?<\/p>\n<p>More specifically, clearing the persistent storage in the beginning and at the end of the training process.<\/p>\n<p>I have googled like a maniac but there is no suggestion aside from &quot;just increase the amount of storage bro&quot; and that's why I'm asking the question here.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1617191466313,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":973.0,
        "Answer_body":"<p>If you don't want your data to be persistent across multiple notebook runs, just store them in <code>\/tmp<\/code> which is not persistent. You have at least 10GB. More details <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-create-ws.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66887340",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1617198319367,
        "Question_original_content":"clear persist storag notebook instanc run follow error try save unexpect error save file untitl ipynb errno space left devic remov notebook creat new ident run work fine suspect jupyt checkpoint take space save notebook run run space sadli get storag option wonder command us clear storag run notebook specif clear persist storag begin end train process googl like maniac suggest asid increas storag bro ask question thank advanc",
        "Question_preprocessed_content":"clear persist storag notebook instanc run follow error try save unexpect error save file space left devic remov notebook creat new ident run work fine suspect jupyt checkpoint take space save notebook run run space sadli get storag option wonder command us clear storag run notebook specif clear persist storag begin end train process googl like maniac suggest asid increas storag bro ask question thank advanc",
        "Answer_original_content":"want data persist multipl notebook run store tmp persist detail",
        "Answer_preprocessed_content":"want data persist multipl notebook run store persist detail",
        "Question_gpt_summary_original":"the user is encountering an error when trying to save a notebook instance on , and is looking for a way to clear the persistent storage before running the notebook in order to avoid running out of space.",
        "Question_gpt_summary":"user encount error try save notebook instanc look wai clear persist storag run notebook order avoid run space",
        "Answer_gpt_summary_original":"the solution to avoid running out of space when saving a notebook instance is to store the data in \/tmp which is not persistent. this will ensure that the data is not saved across multiple notebook runs. \/tmp has at least 10gb of storage available. more details can be found in the provided link.",
        "Answer_gpt_summary":"solut avoid run space save notebook instanc store data tmp persist ensur data save multipl notebook run tmp storag avail detail provid link"
    },
    {
        "Question_id":null,
        "Question_title":"how do I deploy a ml model with cpu cluster?",
        "Question_body":"Hi..I'm new to Azure ML and I've been trying to deploy a ml model I've created and registered.\nSince I can't deploy a model to aci or aks and don't have those clusters,I need to deploy the model using my cpu cluster.\nI'm looking up docs but couldn't find any tutorial for deploying a model with cpu cluster.\nSo I wonder if any of azure ml experts here could help me with doing this..\n\nI would appreciate your help",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643474757900,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/715101\/how-do-i-deploy-a-ml-model-with-cpu-cluster.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-31T11:08:25.56Z",
                "Answer_score":0,
                "Answer_body":"@YuHazelAPEXSYSTEMSLLC-5897 Thanks for the question. Here is document to create compute cluster.\n\nHere is the notebook sample to Register model and deploy locally.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"deploi model cpu cluster new try deploi model creat regist deploi model aci ak cluster need deploi model cpu cluster look doc couldn tutori deploi model cpu cluster wonder expert help appreci help",
        "Question_preprocessed_content":"deploi model cpu cluster new try deploi model creat regist deploi model aci ak cluster need deploi model cpu cluster look doc couldn tutori deploi model cpu cluster wonder expert help appreci help",
        "Answer_original_content":"yuhazelapexsystemsllc thank question document creat comput cluster notebook sampl regist model deploi local",
        "Answer_preprocessed_content":"thank question document creat comput cluster notebook sampl regist model deploi local",
        "Question_gpt_summary_original":"the user is looking for help deploying a ml model to a cpu cluster, as they are unable to deploy to aci or aks clusters.",
        "Question_gpt_summary":"user look help deploi model cpu cluster unabl deploi aci ak cluster",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. create a compute cluster.\n2. use a notebook sample to register the model and deploy it locally. \n\nin summary, the answer suggests creating a compute cluster and using a notebook sample to deploy the ml model locally.",
        "Answer_gpt_summary":"possibl solut answer creat comput cluster us notebook sampl regist model deploi local summari answer suggest creat comput cluster notebook sampl deploi model local"
    },
    {
        "Question_id":72058686.0,
        "Question_title":"How do I deploy a pre trained sklearn model on AWS sagemaker? (Endpoint stuck on creating)",
        "Question_body":"<p>To start with, I understand that this question has been asked multiple times but I haven't found the solution to my problem.<\/p>\n<p>So, to start with I have used joblib.dump to save a locally trained sklearn RandomForest. I then uploaded this to s3, made a folder called code and put in an inference script there, called inference.py.<\/p>\n<pre><code>import joblib\nimport json\nimport numpy\nimport scipy\nimport sklearn\nimport os\n\n&quot;&quot;&quot;\nDeserialize fitted model\n&quot;&quot;&quot;\ndef model_fn(model_dir):\n    model_path = os.path.join(model_dir, 'test_custom_model')\n    model = joblib.load(model_path)\n    return model\n\n&quot;&quot;&quot;\ninput_fn\n    request_body: The body of the request sent to the model.\n    request_content_type: (string) specifies the format\/variable type of the request\n&quot;&quot;&quot;\ndef input_fn(request_body, request_content_type):\n    if request_content_type == 'application\/json':\n        request_body = json.loads(request_body)\n        inpVar = request_body['Input']\n        return inpVar\n    else:\n        raise ValueError(&quot;This model only supports application\/json input&quot;)\n\n&quot;&quot;&quot;\npredict_fn\n    input_data: returned array from input_fn above\n    model (sklearn model) returned model loaded from model_fn above\n&quot;&quot;&quot;\ndef predict_fn(input_data, model):\n    return model.predict(input_data)\n\n&quot;&quot;&quot;\noutput_fn\n    prediction: the returned value from predict_fn above\n    content_type: the content type the endpoint expects to be returned. Ex: JSON, string\n&quot;&quot;&quot;\n\ndef output_fn(prediction, content_type):\n    res = int(prediction[0])\n    respJSON = {'Output': res}\n    return respJSON\n<\/code><\/pre>\n<p>Very simple so far.<\/p>\n<p>I also put this into the local jupyter sagemaker session<\/p>\n<p>all_files (folder)\ncode (folder)\ninference.py (python file)\ntest_custom_model (joblib dump of model)<\/p>\n<p>The script turns this folder all_files into a tar.gz file<\/p>\n<p>Then comes the main script that I ran on sagemaker:<\/p>\n<pre><code>import boto3\nimport json\nimport os\nimport joblib\nimport pickle\nimport tarfile\nimport sagemaker\nimport time\nfrom time import gmtime, strftime\nimport subprocess\nfrom sagemaker import get_execution_role\n\n#Setup\nclient = boto3.client(service_name=&quot;sagemaker&quot;)\nruntime = boto3.client(service_name=&quot;sagemaker-runtime&quot;)\nboto_session = boto3.session.Session()\ns3 = boto_session.resource('s3')\nregion = boto_session.region_name\nprint(region)\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\n#Bucket for model artifacts\ndefault_bucket = 'pretrained-model-deploy'\nmodel_artifacts = f&quot;s3:\/\/{default_bucket}\/test_custom_model.tar.gz&quot;\n\n#Build tar file with model data + inference code\nbashCommand = &quot;tar -cvpzf test_custom_model.tar.gz all_files&quot;\nprocess = subprocess.Popen(bashCommand.split(), stdout=subprocess.PIPE)\noutput, error = process.communicate()\n\n#Upload tar.gz to bucket\nresponse = s3.meta.client.upload_file('test_custom_model.tar.gz', default_bucket, 'test_custom_model.tar.gz')\n\n# retrieve sklearn image\nimage_uri = sagemaker.image_uris.retrieve(\n    framework=&quot;sklearn&quot;,\n    region=region,\n    version=&quot;0.23-1&quot;,\n    py_version=&quot;py3&quot;,\n    instance_type=&quot;ml.m5.xlarge&quot;,\n)\n\n#Step 1: Model Creation\nmodel_name = &quot;sklearn-test&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nprint(&quot;Model name: &quot; + model_name)\ncreate_model_response = client.create_model(\n    ModelName=model_name,\n    Containers=[\n        {\n            &quot;Image&quot;: image_uri,\n            &quot;ModelDataUrl&quot;: model_artifacts,\n        }\n    ],\n    ExecutionRoleArn=role,\n)\nprint(&quot;Model Arn: &quot; + create_model_response[&quot;ModelArn&quot;])\n\n#Step 2: EPC Creation - Serverless\nsklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\nresponse = client.create_endpoint_config(\n   EndpointConfigName=sklearn_epc_name,\n   ProductionVariants=[\n        {\n            &quot;ModelName&quot;: model_name,\n            &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n            &quot;ServerlessConfig&quot;: {\n                &quot;MemorySizeInMB&quot;: 2048,\n                &quot;MaxConcurrency&quot;: 20\n            }\n        } \n    ]\n)\n\n# #Step 2: EPC Creation - Synchronous\n# sklearn_epc_name = &quot;sklearn-epc&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\n# endpoint_config_response = client.create_endpoint_config(\n#     EndpointConfigName=sklearn_epc_name,\n#     ProductionVariants=[\n#         {\n#             &quot;VariantName&quot;: &quot;sklearnvariant&quot;,\n#             &quot;ModelName&quot;: model_name,\n#             &quot;InstanceType&quot;: &quot;ml.m5.xlarge&quot;,\n#             &quot;InitialInstanceCount&quot;: 1\n#         },\n#     ],\n# )\n# print(&quot;Endpoint Configuration Arn: &quot; + endpoint_config_response[&quot;EndpointConfigArn&quot;])\n\n#Step 3: EP Creation\nendpoint_name = &quot;sklearn-local-ep&quot; + strftime(&quot;%Y-%m-%d-%H-%M-%S&quot;, gmtime())\ncreate_endpoint_response = client.create_endpoint(\n    EndpointName=endpoint_name,\n    EndpointConfigName=sklearn_epc_name,\n)\nprint(&quot;Endpoint Arn: &quot; + create_endpoint_response[&quot;EndpointArn&quot;])\n\n\n#Monitor creation\ndescribe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\nwhile describe_endpoint_response[&quot;EndpointStatus&quot;] == &quot;Creating&quot;:\n    describe_endpoint_response = client.describe_endpoint(EndpointName=endpoint_name)\n    print(describe_endpoint_response)\n    time.sleep(15)\nprint(describe_endpoint_response)\n<\/code><\/pre>\n<p>Now, I mainly just want the serverless deployment but that fails after a while with this error message:<\/p>\n<pre><code>{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 16, 11, 52000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '1d25120e-ddb1-474d-9c5f-025c6be24383', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '305', 'date': 'Fri, 29 Apr 2022 12:21:59 GMT'}, 'RetryAttempts': 0}}\n{'EndpointName': 'sklearn-local-ep2022-04-29-12-16-10', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-12-16-10', 'EndpointConfigName': 'sklearn-epc2022-04-29-12-16-03', 'EndpointStatus': 'Failed', 'FailureReason': 'Unable to successfully stand up your model within the allotted 180 second timeout. Please ensure that downloading your model artifacts, starting your model container and passing the ping health checks can be completed within 180 seconds.', 'CreationTime': datetime.datetime(2022, 4, 29, 12, 16, 10, 290000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 12, 22, 2, 68000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': '59fb8ddd-9d45-41f5-9383-236a2baffb73', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '559', 'date': 'Fri, 29 Apr 2022 12:22:15 GMT'}, 'RetryAttempts': 0}}\n<\/code><\/pre>\n<p>The real time deployment is just permanently stuck at creating.<\/p>\n<p>Cloudwatch has the following errors:\nError handling request \/ping<\/p>\n<p>AttributeError: 'NoneType' object has no attribute 'startswith'<\/p>\n<p>with traceback:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/miniconda3\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base_async.py&quot;, line 55, in handle\n    self.handle_request(listener_name, req, client, addr)\n<\/code><\/pre>\n<p>Copy paste has stopped working so I have attached an image of it instead.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/hw80j.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hw80j.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the error message I get:\nEndpoint Arn: arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09\n{'EndpointName': 'sklearn-local-ep2022-04-29-13-18-09', 'EndpointArn': 'arn:aws:sagemaker:us-east-1:963400650255:endpoint\/sklearn-local-ep2022-04-29-13-18-09', 'EndpointConfigName': 'sklearn-epc2022-04-29-13-18-07', 'EndpointStatus': 'Creating', 'CreationTime': datetime.datetime(2022, 4, 29, 13, 18, 9, 548000, tzinfo=tzlocal()), 'LastModifiedTime': datetime.datetime(2022, 4, 29, 13, 18, 13, 119000, tzinfo=tzlocal()), 'ResponseMetadata': {'RequestId': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amzn-requestid': 'ef0e49ee-618e-45de-9c49-d796206404a4', 'content-type': 'application\/x-amz-json-1.1', 'content-length': '306', 'date': 'Fri, 29 Apr 2022 13:18:24 GMT'}, 'RetryAttempts': 0}}<\/p>\n<p>These are the permissions I have associated with that role:<\/p>\n<pre><code>AmazonSageMaker-ExecutionPolicy\nSecretsManagerReadWrite\nAmazonS3FullAccess\nAmazonSageMakerFullAccess\nEC2InstanceProfileForImageBuilderECRContainerBuilds\nAWSAppRunnerServicePolicyForECRAccess\n<\/code><\/pre>\n<p>What am I doing wrong? I've tried different folder structures for the zip file, different accounts, all to no avail. I don't really want to use the model.deploy() method as I don't know how to use serverless with that, and it's also inconcistent between different model types (I'm trying to make a flexible deployment pipeline where different (xgb \/ sklearn) models can be deployed with minimal changes.<\/p>\n<p>Please send help, I'm very close to smashing my hair and tearing out my laptop, been struggling with this for a whole 4 days now.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1651238636737,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1651255570927,
        "Question_score":1.0,
        "Question_view_count":397.0,
        "Answer_body":"<p>I've solved this problem - I used sagemaker.model.model to load in the model data I already had and I called the deploy method on the aforementioned model object to deploy it. Further, I had the inference script and the model file in the same place as the notebook and directly called them, as this gave me an error earlier as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72058686",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651509693110,
        "Question_original_content":"deploi pre train sklearn model endpoint stuck creat start understand question ask multipl time haven solut problem start joblib dump save local train sklearn randomforest upload folder call code infer script call infer import joblib import json import numpi import scipi import sklearn import deseri fit model def model model dir model path path join model dir test custom model model joblib load model path return model input request bodi bodi request sent model request content type string specifi format variabl type request def input request bodi request content type request content type applic json request bodi json load request bodi inpvar request bodi input return inpvar rais valueerror model support applic json input predict input data return arrai input model sklearn model return model load model def predict input data model return model predict input data output predict return valu predict content type content type endpoint expect return json string def output predict content type re int predict respjson output re return respjson simpl far local jupyt session file folder code folder infer python file test custom model joblib dump model script turn folder file tar file come main script ran import boto import json import import joblib import pickl import tarfil import import time time import gmtime strftime import subprocess import execut role setup client boto client servic runtim boto client servic runtim boto session boto session session boto session resourc region boto session region print region session session role execut role bucket model artifact default bucket pretrain model deploi model artifact default bucket test custom model tar build tar file model data infer code bashcommand tar cvpzf test custom model tar file process subprocess popen bashcommand split stdout subprocess pipe output error process commun upload tar bucket respons meta client upload file test custom model tar default bucket test custom model tar retriev sklearn imag imag uri imag uri retriev framework sklearn region region version version instanc type xlarg step model creation model sklearn test strftime gmtime print model model creat model respons client creat model modelnam model contain imag imag uri modeldataurl model artifact executionrolearn role print model arn creat model respons modelarn step epc creation serverless sklearn epc sklearn epc strftime gmtime respons client creat endpoint config endpointconfignam sklearn epc productionvari modelnam model variantnam sklearnvari serverlessconfig memorysizeinmb maxconcurr step epc creation synchron sklearn epc sklearn epc strftime gmtime endpoint config respons client creat endpoint config endpointconfignam sklearn epc productionvari variantnam sklearnvari modelnam model instancetyp xlarg initialinstancecount print endpoint configur arn endpoint config respons endpointconfigarn step creation endpoint sklearn local strftime gmtime creat endpoint respons client creat endpoint endpointnam endpoint endpointconfignam sklearn epc print endpoint arn creat endpoint respons endpointarn monitor creation endpoint respons client endpoint endpointnam endpoint endpoint respons endpointstatu creat endpoint respons client endpoint endpointnam endpoint print endpoint respons time sleep print endpoint respons mainli want serverless deploy fail error messag endpointnam sklearn local endpointarn arn aw east endpoint sklearn local endpointconfignam sklearn epc endpointstatu creat creationtim datetim datetim tzinfo tzlocal lastmodifiedtim datetim datetim tzinfo tzlocal responsemetadata requestid ddb cbe httpstatuscod httpheader amzn requestid ddb cbe content type applic amz json content length date fri apr gmt retryattempt endpointnam sklearn local endpointarn arn aw east endpoint sklearn local endpointconfignam sklearn epc endpointstatu fail failurereason unabl successfulli stand model allot second timeout ensur download model artifact start model contain pass ping health check complet second creationtim datetim datetim tzinfo tzlocal lastmodifiedtim datetim datetim tzinfo tzlocal responsemetadata requestid abaffb httpstatuscod httpheader amzn requestid abaffb content type applic amz json content length date fri apr gmt retryattempt real time deploy perman stuck creat cloudwatch follow error error handl request ping attributeerror nonetyp object attribut startswith traceback traceback recent file miniconda lib python site packag gunicorn worker base async line handl self handl request listen req client addr copi past stop work attach imag instead error messag endpoint arn arn aw east endpoint sklearn local endpointnam sklearn local endpointarn arn aw east endpoint sklearn local endpointconfignam sklearn epc endpointstatu creat creationtim datetim datetim tzinfo tzlocal lastmodifiedtim datetim datetim tzinfo tzlocal responsemetadata requestid efee httpstatuscod httpheader amzn requestid efee content type applic amz json content length date fri apr gmt retryattempt permiss associ role amazon executionpolici secretsmanagerreadwrit amazonsfullaccess amazonfullaccess ecinstanceprofileforimagebuilderecrcontainerbuild awsapprunnerservicepolicyforecraccess wrong tri differ folder structur zip file differ account avail want us model deploi method know us serverless inconcist differ model type try flexibl deploy pipelin differ xgb sklearn model deploi minim chang send help close smash hair tear laptop struggl dai",
        "Question_preprocessed_content":"deploi pre train sklearn model start understand question ask multipl time haven solut problem start save local train sklearn randomforest upload folder call code infer script call simpl far local jupyt session code script turn folder file come main script ran mainli want serverless deploy fail error messag real time deploy perman stuck creat cloudwatch follow error error handl request attributeerror nonetyp object attribut startswith traceback copi past stop work attach imag instead error messag endpoint arn endpointnam endpointarn endpointconfignam endpointstatu creat creationtim tzinfo tzlocal lastmodifiedtim tzinfo tzlocal responsemetadata retryattempt permiss associ role wrong tri differ folder structur zip file differ account avail want us method know us serverless inconcist differ model type model deploi minim chang send help close smash hair tear laptop struggl dai",
        "Answer_original_content":"solv problem model model load model data call deploi method aforement model object deploi infer script model file place notebook directli call gave error earlier",
        "Answer_preprocessed_content":"solv problem load model data call deploi method aforement model object deploi infer script model file place notebook directli call gave error earlier",
        "Question_gpt_summary_original":"the user is encountering challenges with deploying a pre-trained sklearn model on an endpoint, which is stuck on creating.",
        "Question_gpt_summary":"user encount challeng deploi pre train sklearn model endpoint stuck creat",
        "Answer_gpt_summary_original":"the solution to the problem of a pre-trained sklearn model getting stuck on creating while deploying on an endpoint is to use .model.model to load in the model data and call the deploy method on the model object. additionally, keeping the inference script and model file in the same place as the notebook can also help avoid errors.",
        "Answer_gpt_summary":"solut problem pre train sklearn model get stuck creat deploi endpoint us model model load model data deploi method model object addition keep infer script model file place notebook help avoid error"
    },
    {
        "Question_id":53660590.0,
        "Question_title":"PySpark Using collect_list to collect Arrays of Varying Length",
        "Question_body":"<p>I am attempting to use collect_list to collect arrays (and maintain order) from two different data frames.<\/p>\n\n<p>Test_Data and Train_Data have the same format.<\/p>\n\n<pre><code>from pyspark.sql import functions as F\nfrom pyspark.sql import Window\n\nw = Window.partitionBy('Group').orderBy('date')\n\n# Train_Data has 4 data points\n# Test_Data has 7 data points\n# desired target array:         [1, 1, 2, 3]\n# desired MarchMadInd array:    [0, 0, 0, 1, 0, 0, 1]\n\nsorted_list_diff_array_lens = train_data.withColumn('target', \nF.collect_list('target').over(w)\n                                  )\\\ntest_data.withColumn('MarchMadInd', F.collect_list('MarchMadInd').over(w))\\\n   .groupBy('Group')\\\n   .agg(F.max('target').alias('target'), \n    F.max('MarchMadInd').alias('MarchMadInd')\n)\n<\/code><\/pre>\n\n<p>I realize the syntax is incorrect with \"test_data.withColumn\", but I want to select the array for the <em>MarchMadInd<\/em> from the <strong>test_date<\/strong>, but the array for the <em>target<\/em> from the <strong>train_data<\/strong>. The desired output would look like the following:<\/p>\n\n<pre><code>{\"target\":[1, 1, 2, 3], \"MarchMadInd\":[0, 0, 0, 1, 0, 0, 1]}\n<\/code><\/pre>\n\n<p>Context: this is for a DeepAR time series model (using AWS) that requires dynamic features to include the prediction period, but the target should be historical data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1544134968250,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1544534805888,
        "Question_score":0.0,
        "Question_view_count":345.0,
        "Answer_body":"<p>The solution involves using a join as recommended by pault. <\/p>\n\n<ol>\n<li>Create a dataframe with dynamic features of length equal to Training + Prediction period<\/li>\n<li>Create a dataframe with target values of length equal to just the Training period.<\/li>\n<li>Use a LEFT JOIN (with the dynamic feature data on LEFT) to bring these dataframes together<\/li>\n<\/ol>\n\n<p>Now, using collect_list will create the desired result.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53660590",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1555600483376,
        "Question_original_content":"pyspark collect list collect arrai vari length attempt us collect list collect arrai maintain order differ data frame test data train data format pyspark sql import function pyspark sql import window window partitionbi group orderbi date train data data point test data data point desir target arrai desir marchmadind arrai sort list diff arrai len train data withcolumn target collect list target test data withcolumn marchmadind collect list marchmadind groupbi group agg max target alia target max marchmadind alia marchmadind realiz syntax incorrect test data withcolumn want select arrai marchmadind test date arrai target train data desir output look like follow target marchmadind context deepar time seri model aw requir dynam featur includ predict period target histor data",
        "Question_preprocessed_content":"pyspark collect arrai vari length attempt us collect arrai differ data frame format realiz syntax incorrect want select arrai marchmadind arrai target desir output look like follow context deepar time seri model requir dynam featur includ predict period target histor data",
        "Answer_original_content":"solut involv join recommend pault creat datafram dynam featur length equal train predict period creat datafram target valu length equal train period us left join dynam featur data left bring datafram collect list creat desir result",
        "Answer_preprocessed_content":"solut involv join recommend pault creat datafram dynam featur length equal train predict period creat datafram target valu length equal train period us left join bring datafram creat desir result",
        "Question_gpt_summary_original":"the user is encountering challenges in using pyspark's collect_list to collect arrays of varying length from two different data frames in order to create a desired output of two arrays.",
        "Question_gpt_summary":"user encount challeng pyspark collect list collect arrai vari length differ data frame order creat desir output arrai",
        "Answer_gpt_summary_original":"the solution to the challenge of using pyspark's collect_list to collect arrays of varying length from two different data frames involves creating two data frames, one with dynamic features of length equal to training + prediction period and another with target values of length equal to just the training period. then, a left join is used to bring these data frames together, and collect_list can be used to create the desired output of two arrays.",
        "Answer_gpt_summary":"solut challeng pyspark collect list collect arrai vari length differ data frame involv creat data frame dynam featur length equal train predict period target valu length equal train period left join bring data frame collect list creat desir output arrai"
    },
    {
        "Question_id":69245175.0,
        "Question_title":"Error in azureml \"Non numeric value(s) were encountered in the target column.\"",
        "Question_body":"<p>I am using Automated ML to run a time series forecasting pipeline.<\/p>\n<p>When the AutoMLStep gets triggered, I get this error: <code>Non numeric value(s) were encountered in the target column<\/code>.<\/p>\n<p>The data to this step is passed through an OutputTabularDatasetConfig, after applying the read_delimited_files() on an OutputFileDatasetConfig. I've inspected the prior step, and the data is comprised of a 'Date' column and a numeric column called 'Place' with +80 observations in monthly frequencies.<\/p>\n<p>Nothing seems to be wrong with the column type or the data. I've also applied a number of techniques on the data prep side e.g. pd.to_numeric(), astype(float) to ensure it is numeric.<\/p>\n<p>I've also tried forcing this through the FeaturizationConfig() <code>add_column_purpose('Place','Numeric')<\/code> but in this case, I get another error: <code>Expected column(s) Place in featurization config's column purpose not found in X.<\/code><\/p>\n<p>Any thoughts on how to solve?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1632069680553,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":127.0,
        "Answer_body":"<p>So a few learnings on this interacting with the stellar Azure Machine Learning engineering team.<\/p>\n<ol>\n<li>When calling the <code>read_delimited_files()<\/code> method, ensure that the output folder does not have many inputs or files. For example, if all intermediate outputs are saved to a common folder, it may read all the prior inputs into this folder, and depending upon the shape of the data, borrow the schema from the first file, or confuse all of them together. This can lead to inconsistencies and errors. In my case, I was dumping many files to the same location, hence this was causing confusion for this method. The fix is either to distinctly mark the output folder (e.g. with a UUID) or give different paths.<\/li>\n<li>The dataframe from <code>read_delimiter_files()<\/code> may treat all columns as object type which can lead to a data type check failure (i.e. label_column needs to be numeric). To mitigate, explictly state the type. For example:<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.data import DataType\nprepped_data = prepped_data.read_delimited_files(set_column_types={&quot;Place&quot;:DataType.to_float()})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69245175",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1633037776347,
        "Question_original_content":"error non numer valu encount target column autom run time seri forecast pipelin automlstep get trigger error non numer valu encount target column data step pass outputtabulardatasetconfig appli read delimit file outputfiledatasetconfig inspect prior step data compris date column numer column call place observ monthli frequenc wrong column type data appli number techniqu data prep numer astyp float ensur numer tri forc featurizationconfig add column purpos place numer case error expect column place featur config column purpos thought solv",
        "Question_preprocessed_content":"error non numer valu encount target autom run time seri forecast pipelin automlstep get trigger error data step pass outputtabulardatasetconfig appli outputfiledatasetconfig inspect prior step data compris date column numer column call place observ monthli frequenc wrong column type data appli number techniqu data prep astyp ensur numer tri forc featurizationconfig case error thought solv",
        "Answer_original_content":"learn interact stellar engin team call read delimit file method ensur output folder input file exampl intermedi output save common folder read prior input folder depend shape data borrow schema file confus lead inconsist error case dump file locat caus confus method fix distinctli mark output folder uuid differ path datafram read delimit file treat column object type lead data type check failur label column need numer mitig explictli state type exampl data import datatyp prep data prep data read delimit file set column type place datatyp float",
        "Answer_preprocessed_content":"learn interact stellar engin team call method ensur output folder input file exampl intermedi output save common folder read prior input folder depend shape data borrow schema file confus lead inconsist error case dump file locat caus confus method fix distinctli mark output folder differ path datafram treat column object type lead data type check failur mitig explictli state type exampl",
        "Question_gpt_summary_original":"the user is encountering an error in their automated ml time series forecasting pipeline due to non-numeric values in the target column.",
        "Question_gpt_summary":"user encount error autom time seri forecast pipelin non numer valu target column",
        "Answer_gpt_summary_original":"possible solutions to the error encountered in the automated ml time series forecasting pipeline due to non-numeric values in the target column are: \n1. ensure that the output folder does not have many inputs or files when calling the read_delimited_files() method.\n2. distinctly mark the output folder or give different paths to avoid confusion for the method.\n3. explicitly state the data type of the label_column to avoid data type check failure.",
        "Answer_gpt_summary":"possibl solut error encount autom time seri forecast pipelin non numer valu target column ensur output folder input file call read delimit file method distinctli mark output folder differ path avoid confus method explicitli state data type label column avoid data type check failur"
    },
    {
        "Question_id":64338898.0,
        "Question_title":"How to load an experiment in azureml?",
        "Question_body":"<p>I have many experiment, like:<\/p>\n<p><img src=\"https:\/\/user-images.githubusercontent.com\/40580910\/95883598-82a07d00-0d51-11eb-847d-872452f6caa4.png\" alt=\"image\" \/><\/p>\n<p>and now, i want load an experiment<\/p>\n<pre><code>#%% sumonando os pacotes e verificando azureml.core\nimport azureml.core\nimport pandas as pd\nimport numpy as np\nimport logging\n\nprint(&quot;AzureML SDK Version: &quot;, azureml.core.VERSION)\n\n#%% Conectando ao azure e crinado o exparimento\n\nfrom azureml.core import Workspace, Experiment\n\nws = Workspace.from_config() \nprint(Experiment.list(ws))\n#%%\nExperiment = Experiment.from_directory('teste2-Monitor-Runs') `\n<\/code><\/pre>\n<p>but<\/p>\n<pre><code>&quot;error&quot;: {\n    &quot;message&quot;: &quot;No cache found for current project, try providing resource group and workspace \narguments&quot;\n}`\n<\/code><\/pre>\n<hr \/>\n<p>Content: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.experiment(class)?view=azure-ml-py\" rel=\"nofollow noreferrer\">azureml.core.Experiment class - Azure Machine Learning Python<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1602604847240,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":254.0,
        "Answer_body":"<p>I believe it is that way.<\/p>\n<pre><code>from azureml.core import Experiment, Workspace\nExperiment = ws.experiments[&quot;teste2-Monitor-Runs&quot;]\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64338898",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1602700300720,
        "Question_original_content":"load experi experi like want load experi sumonando pacot verificando core import core import panda import numpi import log print sdk version core version conectando azur crinado exparimento core import workspac experi workspac config print experi list experi experi directori test monitor run error messag cach current project try provid resourc group workspac argument content core experi class python",
        "Question_preprocessed_content":"load experi experi like want load experi content class python",
        "Answer_original_content":"believ wai core import experi workspac experi experi test monitor run",
        "Answer_preprocessed_content":"believ wai",
        "Question_gpt_summary_original":"the user is encountering an error when attempting to load an experiment in .core and is looking for a solution.",
        "Question_gpt_summary":"user encount error attempt load experi core look solut",
        "Answer_gpt_summary_original":"the answer does not provide any possible solutions to the error encountered when attempting to load an experiment in .core. it only shows a code snippet that imports experiment and workspace from .core and assigns a specific experiment to the variable \"experiment\".",
        "Answer_gpt_summary":"answer provid possibl solut error encount attempt load experi core show code snippet import experi workspac core assign specif experi variabl experi"
    },
    {
        "Question_id":70603137.0,
        "Question_title":"Is it possible to integrate AWS sagemaker and delta lake",
        "Question_body":"<p>Is it possible to integrate AWS sage maker and delta lake?<\/p>\n<p>thanks\nRamabadran<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1641448912433,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":174.0,
        "Answer_body":"<p>Yes, though it depends on what part of SageMaker you mean (Training, Notebook, Inference, etc).<\/p>\n<p>Last week, an integration between SageMaker and Delta Lake was documented here (custom docker in the SageMaker Processing API)<\/p>\n<p><a href=\"https:\/\/github.com\/eitansela\/sagemaker-delta-sharing-demo\/tree\/main\/delta_lake_bring_your_own_container_processing\" rel=\"nofollow noreferrer\">https:\/\/github.com\/eitansela\/sagemaker-delta-sharing-demo\/tree\/main\/delta_lake_bring_your_own_container_processing<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70603137",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1641839983127,
        "Question_original_content":"possibl integr delta lake possibl integr aw sage maker delta lake thank ramabadran",
        "Question_preprocessed_content":"possibl integr delta lake possibl integr aw sage maker delta lake thank ramabadran",
        "Answer_original_content":"ye depend mean train notebook infer week integr delta lake document custom docker process api http github com eitansela delta share demo tree main delta lake bring contain process",
        "Answer_preprocessed_content":"ye depend mean week integr delta lake document",
        "Question_gpt_summary_original":"the user ramabadran is asking if it is possible to integrate aws sage maker and delta lake.",
        "Question_gpt_summary":"user ramabadran ask possibl integr aw sage maker delta lake",
        "Answer_gpt_summary_original":"possible solution: it is possible to integrate aws sage maker and delta lake, depending on the specific part of the integration needed (training, notebook, inference, etc). an integration between the two was documented last week using a custom docker in the processing api, which can be found on github.",
        "Answer_gpt_summary":"possibl solut possibl integr aw sage maker delta lake depend specif integr need train notebook infer integr document week custom docker process api github"
    },
    {
        "Question_id":null,
        "Question_title":"'dvc push' multiple small files to aws s3 causes timeout error",
        "Question_body":"<p>Hi,<br>\nI have problem with pushing large amount of small files to s3 via \u2018dvc push\u2019 command (~2000 files few hundred kb each) from a local machine (Ubuntu).<\/p>\n<p>Stable wired internet connection (~50Mbps measured speed), latest DVC (2.6.4).<\/p>\n<p>While \u2018querying cache\u2026\u2019 phase progress slows down at 85% and eventually gives an \u2018unexpected error\u2019 (FSTimeoutError).<\/p>\n<p>Tried to increase max number of opened files (system-wide, by changing \/etc\/sysctl.conf) - no effect.<\/p>\n<p>Thought if it\u2019s worth to change timeout value (if any) but after checking dvc-remote page couldn\u2019t find any info about it.<\/p>\n<p>Could someone advice how to solve this issue?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1630591814110,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":707.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-push-multiple-small-files-to-aws-s3-causes-timeout-error\/865",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-02T15:14:34.456Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/yokohama\">@yokohama<\/a> ! Could you please share the output of <code>dvc doctor<\/code> and <code>dvc push -v<\/code> ?<\/p>\n<p>Are you using aws s3 directly or an s3-compatible storage? I\u2019m asking because there has been a recent issue regarding a <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/6460\" rel=\"noopener nofollow ugc\">Timeout Error related to SeaweedFS<\/a><\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-02T15:42:07.543Z",
                "Answer_body":"<pre><code class=\"lang-auto\">\n<\/code><\/pre>\n<p>Hi, daavoo! Thanks for quick answer!<br>\nAs for AWS - to be sure, I have to check if it\u2019s direct or other option you\u2019ve mentioned.<br>\nAlso, the other thread is really looks like mine at a first glance\u2026 will read it deeply meanwhile\u2026<br>\nHere are logs:<\/p>\n<pre><code class=\"lang-auto\">-----------------\n\n14:06 $ dvc push --jobs 1 FANCY_DVC_FILE.dvc -v\n2021-09-01 14:11:18,710 DEBUG: Preparing to transfer data from '..\/..\/..\/.dvc\/cache' to 's3:\/\/FANCY_S3_PATH'\n2021-09-01 14:11:18,710 DEBUG: Preparing to collect status from 's3:\/\/FANCY_S3_PATH'\n2021-09-01 14:11:18,712 DEBUG: Collecting status from 's3:\/\/FANCY_S3_PATH'\n2021-09-01 14:11:18,712 DEBUG: Querying 1 hashes via object_exists\n2021-09-01 14:11:20,202 DEBUG: list_hashes() returned max '122.0703125' hashes, skipping remaining results                                \n2021-09-01 14:11:20,202 DEBUG: Estimated remote size: 503808 files                                                                          \n2021-09-01 14:11:20,202 DEBUG: Large remote ('2162' hashes &lt; '2519.04' traverse weight), using object_exists for remaining hashes           \n2021-09-01 14:11:20,202 DEBUG: Querying 2162 hashes via object_exists\n2021-09-01 14:13:29,659 ERROR: unexpected error                                                                                             \n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/main.py\", line 55, in main\n    ret = cmd.do_run()\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/base.py\", line 45, in do_run\n    return self.run()\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/command\/data_sync.py\", line 57, in run\n    processed_files_count = self.repo.push(\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/_init_.py\", line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/repo\/push.py\", line 48, in push\n    pushed += self.cloud.push(obj_ids, jobs, remote=remote, odb=odb)\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/data_cloud.py\", line 85, in push\n    return transfer(\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/transfer.py\", line 221, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\", line 160, in compare_status\n    dest_exists, dest_missing = status(\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/status.py\", line 132, in status\n    odb.hashes_exist(hashes, name=str(odb.path_info), **kwargs)\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\", line 521, in hashes_exist\n    return list(hashes &amp; remote_hashes) + self.list_hashes_exists(\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\", line 448, in list_hashes_exists\n    ret = list(itertools.compress(hashes, in_remote))\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 619, in result_iterator\n    yield fs.pop().result()\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 444, in result\n    return self.__get_result()\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/_base.py\", line 389, in __get_result\n    raise self._exception\n  File \"\/usr\/lib\/python3.8\/concurrent\/futures\/thread.py\", line 57, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/objects\/db\/base.py\", line 439, in exists_with_progress\n    ret = self.fs.exists(path_info)\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 93, in exists\n    return self.fs.exists(self._with_bucket(path_info))\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 88, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/home\/USER_NAME\/.local\/lib\/python3.8\/site-packages\/fsspec\/asyn.py\", line 67, in sync\n    raise FSTimeoutError\nfsspec.exceptions.FSTimeoutError\n------------------------------------------------------------\n2021-09-01 14:13:29,692 DEBUG: Version info for developers:\nDVC version: 2.6.4 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on Linux-5.11.0-27-generic-x86_64-with-glibc2.29\nSupports:\n\thttp (requests = 2.22.0),\n\thttps (requests = 2.22.0),\n\ts3 (s3fs = 2021.6.1, boto3 = 1.17.49)\nCache types: hardlink, symlink\nCache directory: ext4 on \/dev\/mapper\/vgubuntu-root\nCaches: local\nRemotes: s3\nWorkspace directory: ext4 on \/dev\/mapper\/vgubuntu-root\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2021-09-01 14:13:29,694 DEBUG: Analytics is enabled.\n2021-09-01 14:13:29,715 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmparniyeu6']'\n2021-09-01 14:13:29,717 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmparniyeu6']'`Preformatted text`\n<\/code><\/pre>",
                "Answer_score":11.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"push multipl small file aw caus timeout error problem push larg small file push command file local machin ubuntu stabl wire internet connect mbp measur speed latest queri cach phase progress slow eventu give unexpect error fstimeouterror tri increas max number open file wide chang sysctl conf effect thought worth chang timeout valu check remot page info advic solv issu thank",
        "Question_preprocessed_content":"push multipl small file aw caus timeout error problem push larg small file push command local machin stabl wire internet connect latest queri cach phase progress slow eventu give unexpect error tri increas max number open file effect thought worth chang timeout valu check remot page info advic solv issu thank",
        "Answer_original_content":"yokohama share output doctor push aw directli compat storag ask recent issu timeout error relat seaweedf daavoo thank quick answer aw sure check direct option youv mention thread look like glanc read deepli log push job fanci file debug prepar transfer data cach fanci path debug prepar collect statu fanci path debug collect statu fanci path debug queri hash object exist debug list hash return max hash skip remain result debug estim remot size file debug larg remot hash travers weight object exist remain hash debug queri hash object exist error unexpect error traceback recent file home user local lib python site packag main line main ret cmd run file home user local lib python site packag command base line run return self run file home user local lib python site packag command data sync line run process file count self repo push file home user local lib python site packag repo init line wrapper return repo arg kwarg file home user local lib python site packag repo push line push push self cloud push obj id job remot remot odb odb file home user local lib python site packag data cloud line push return transfer file home user local lib python site packag object transfer line transfer statu compar statu src dest obj id check delet fals kwarg file home user local lib python site packag object statu line compar statu dest exist dest miss statu file home user local lib python site packag object statu line statu odb hash exist hash str odb path info kwarg file home user local lib python site packag object base line hash exist return list hash remot hash self list hash exist file home user local lib python site packag object base line list hash exist ret list itertool compress hash remot file usr lib python concurr futur base line result iter yield pop result file usr lib python concurr futur base line result return self result file usr lib python concurr futur base line result rais self except file usr lib python concurr futur thread line run result self self arg self kwarg file home user local lib python site packag object base line exist progress ret self exist path info file home user local lib python site packag fsspec wrapper line exist return self exist self bucket path info file home user local lib python site packag fsspec asyn line wrapper return sync self loop func arg kwarg file home user local lib python site packag fsspec asyn line sync rais fstimeouterror fsspec except fstimeouterror debug version info develop version pip platform python linux gener glibc support http request http request sf boto cach type hardlink symlink cach directori ext dev mapper vgubuntu root cach local remot workspac directori ext dev mapper vgubuntu root repo git have troubl hit http org support happi help debug analyt enabl debug try spawn daemon analyt tmp tmparniyeu debug spawn daemon analyt tmp tmparniyeu preformat text",
        "Answer_preprocessed_content":"share output aw directli storag ask recent issu timeout error relat seaweedf daavoo thank quick answer aw sure check direct option youv mention thread look like glanc read deepli log",
        "Question_gpt_summary_original":"the user is encountering a timeout error when attempting to 'push' multiple small files to aws s3.",
        "Question_gpt_summary":"user encount timeout error attempt push multipl small file aw",
        "Answer_gpt_summary_original":"possible solutions to the timeout error encountered when pushing multiple small files to aws s3 are not explicitly mentioned in the answer. however, the answer suggests checking if the user is using aws s3 directly or an s3-compatible storage and reading a related thread that looks similar to the issue. the logs are also provided, which can be used to identify the root cause of the error. the user is also encouraged to seek help from the support team if they are having any troubles.",
        "Answer_gpt_summary":"possibl solut timeout error encount push multipl small file aw explicitli mention answer answer suggest check user aw directli compat storag read relat thread look similar issu log provid identifi root caus error user encourag seek help support team have troubl"
    },
    {
        "Question_id":51450610.0,
        "Question_title":"Athena query works in console but not with boto3 client in sagemaker (convert csv into table)",
        "Question_body":"<p>I am trying to convert a csv file from s3 into a table in Athena. When I run the query on Athena console it works but when I run it on Sagemaker Jupyter notebook with boto3 client it returns: <\/p>\n\n<pre><code>\"**InvalidRequestException**: An error occurred (InvalidRequestException) when calling the StartQueryExecution operation: line 1:8: no viable alternative at input 'CREATE EXTERNAL'\"\n<\/code><\/pre>\n\n<p>Here is my code <\/p>\n\n<pre><code>def run_query(query):\n    client = boto3.client('athena')\n    response = client.start_query_execution(\n        QueryString=query,\n        ResultConfiguration={\n            'OutputLocation': 's3:\/\/path\/to\/s3output',\n            }\n        )\n    print('Execution ID: ' + response['QueryExecutionId'])\n    return response\n\ncreateTable = \\\n\"\"\"CREATE EXTERNAL TABLE TestTable (\n    ID string,\n    CustomerId string, \n    Ip string,\n    MessageFilename string\n\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n   'separatorChar' = ',',\n   'quoteChar' = '\\\"',\n   'escapeChar' = '\\\\'\n )\nSTORED AS TEXTFILE\nLOCATION 's3:\/\/bucket_name\/results\/csv\/'\nTBLPROPERTIES (\"skip.header.line.count\"=\"1\")\"\"\"\n\nresponse = run_query(createTable, s3_output)\nprint(response)\n<\/code><\/pre>\n\n<p>I have run queries through boto3 client in json format (so, using  ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe') which have worked well but somehow this doesn't. I have tried changing names, syntax, quotes but that doesn't seem to work. <\/p>\n\n<p>Any suggestion would be very appreciated, \nThank you! <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1532122235887,
        "Question_favorite_count":null,
        "Question_last_edit_time":1532122372580,
        "Question_score":2.0,
        "Question_view_count":1635.0,
        "Answer_body":"<p>Thanks for sharing complete example. The issue is with the escaping in <code>SERDEPROPERTIES<\/code>. After modifying <code>createTable<\/code> as below it works<\/p>\n\n<pre><code>createTable = \\\n\"\"\"CREATE EXTERNAL TABLE testtable (\n    `id` string,\n    `customerid` string, \n    `ip` string,\n    `messagefilename` string\n)\nROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'\nWITH SERDEPROPERTIES (\n  'separatorChar' = ',', \n  'quoteChar' = '\\\\\\\"', \n  'escapeChar' = '\\\\\\\\' )\nSTORED AS TEXTFILE\nLOCATION 's3:\/\/bucket_name\/results\/csv\/'\nTBLPROPERTIES (\"skip.header.line.count\"=\"1\");\"\"\"\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1532127518448,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51450610",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1532127104847,
        "Question_original_content":"athena queri work consol boto client convert csv tabl try convert csv file tabl athena run queri athena consol work run jupyt notebook boto client return invalidrequestexcept error occur invalidrequestexcept call startqueryexecut oper line viabl altern input creat extern code def run queri queri client boto client athena respons client start queri execut querystr queri resultconfigur outputloc path soutput print execut respons queryexecutionid return respons createt creat extern tabl testtabl string customerid string string messagefilenam string row format serd org apach hadoop hive serd opencsvserd serdeproperti separatorchar quotechar escapechar store textfil locat bucket result csv tblproperti skip header line count respons run queri createt output print respons run queri boto client json format row format serd org openx data jsonserd jsonserd work tri chang name syntax quot work suggest appreci thank",
        "Question_preprocessed_content":"athena queri work consol boto client try convert csv file tabl athena run queri athena consol work run jupyt notebook boto client return code run queri boto client json format work tri chang name syntax quot work suggest appreci thank",
        "Answer_original_content":"thank share complet exampl issu escap serdeproperti modifi createt work createt creat extern tabl testtabl string customerid string string messagefilenam string row format serd org apach hadoop hive serd opencsvserd serdeproperti separatorchar quotechar escapechar store textfil locat bucket result csv tblproperti skip header line count",
        "Answer_preprocessed_content":"thank share complet exampl issu escap modifi work",
        "Question_gpt_summary_original":"the user is encountering an \"invalidrequestexception\" when attempting to convert a csv file from s3 into a table in athena using boto3 client, despite the query working in the athena console.",
        "Question_gpt_summary":"user encount invalidrequestexcept attempt convert csv file tabl athena boto client despit queri work athena consol",
        "Answer_gpt_summary_original":"the solution to the \"invalidrequestexception\" error when converting a csv file from s3 into a table in athena using boto3 client is to modify the createtable code by adjusting the escaping in serdeproperties. specifically, the solution involves changing the quotechar and escapechar values in the code.",
        "Answer_gpt_summary":"solut invalidrequestexcept error convert csv file tabl athena boto client modifi createt code adjust escap serdeproperti specif solut involv chang quotechar escapechar valu code"
    },
    {
        "Question_id":38119062.0,
        "Question_title":"Pulling data from Stream Analytics to Azure Machine Learning",
        "Question_body":"<p>Working on a IoT telemetry project that receives humidity and weather pollution data from different sites on the field. I will then apply Machine Learning on the collected data. I'm using Event Hubs and Stream Analytics. Is there a way of pulling the data to Azure Machine Learning without the hassle of writing an application to get it from Stream Analytics and push to AML web service?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1467278958850,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":627.0,
        "Answer_body":"<p>Stream Analytics has a functionality called the \u201c<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">Functions<\/a>\u201d. You can call any web service you\u2019ve published using AML from within Stream Analytics and apply it within your Stream Analytics query. Check this <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link for a tutorial<\/a>.\nExample workflow in your case would be like the following;<\/p>\n\n<ul>\n<li>Telemetry arrives and reaches Stream Analytics<\/li>\n<li>Streaming Analytics (SA) calls the Machine Learning function to apply it on the data<\/li>\n<li>SA redirects it to the output accordingly, here you can use the PowerBI to create a predictions dashboards.<\/li>\n<\/ul>\n\n<p>Another way would be using R, and here\u2019s a good tutorial showing that <a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/\" rel=\"nofollow\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/12\/10\/azure-ml-now-available-as-a-function-in-azure-stream-analytics\/<\/a> . \nIt is more work of course but can give you more control as you control the code.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38119062",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1467284456456,
        "Question_original_content":"pull data stream analyt work iot telemetri project receiv humid weather pollut data differ site field appli machin learn collect data event hub stream analyt wai pull data hassl write applic stream analyt push aml web servic",
        "Question_preprocessed_content":"pull data stream analyt work iot telemetri project receiv humid weather pollut data differ site field appli machin learn collect data event hub stream analyt wai pull data hassl write applic stream analyt push aml web servic",
        "Answer_original_content":"stream analyt function call function web servic youv publish aml stream analyt appli stream analyt queri check link tutori exampl workflow case like follow telemetri arriv reach stream analyt stream analyt call machin learn function appli data redirect output accordingli us powerbi creat predict dashboard wai here good tutori show http blog technet microsoft com machinelearn azur avail function azur stream analyt work cours control control code",
        "Answer_preprocessed_content":"stream analyt function call function web servic youv publish aml stream analyt appli stream analyt queri check link tutori exampl workflow case like follow telemetri arriv reach stream analyt stream analyt call machin learn function appli data redirect output accordingli us powerbi creat predict dashboard wai here good tutori show work cours control control code",
        "Question_gpt_summary_original":"the user is working on an iot telemetry project that requires them to pull data from stream analytics to an  web service without having to write an application.",
        "Question_gpt_summary":"user work iot telemetri project requir pull data stream analyt web servic have write applic",
        "Answer_gpt_summary_original":"possible solutions to the user's problem of pulling data from stream analytics to an  web service without writing an application are: using stream analytics' functions to call the web service and apply it within the query, and using r to have more control over the code. the first solution is easier and has a tutorial available, while the second solution requires more work but can provide more control. the workflow for the first solution involves telemetry arriving, reaching stream analytics, calling the machine learning function, redirecting it to the output, and using powerbi to create predictions dashboards.",
        "Answer_gpt_summary":"possibl solut user problem pull data stream analyt web servic write applic stream analyt function web servic appli queri control code solut easier tutori avail second solut requir work provid control workflow solut involv telemetri arriv reach stream analyt call machin learn function redirect output powerbi creat predict dashboard"
    },
    {
        "Question_id":null,
        "Question_title":"How to change Sklearn flavors version in mlflow on azure machine learning?",
        "Question_body":"I need to change the flavors \"sklearn_version\" in mlflow from \"0.22.1\" to \"1.0.0\" on azure machine learning when I log my trained model, since this model will be incompatible with the sklearn version that I am using for deployment during inference. I could change the version of conda by setting \"conda_env\" in\n\nmlflow.sklearn.log_model(conda_env= 'my_env')\n\n\n\n\nin the conda.yaml file, however it still remains unchanged in flavors in MLmodel file\n\nand here is script that I use to create this mlflow experiment in azure machine learning notebooks.\n\n import mlflow\n from sklearn.tree import DecisionTreeRegressor\n    \n from azureml.core import Workspace\n from azureml.core.model import Model\n from azureml.mlflow import register_model\n    \n    \n def run_model(ws, experiment_name, run_name, x_train, y_train):\n        \n     # set up MLflow to track the metrics\n     mlflow.set_tracking_uri(ws.get_mlflow_tracking_uri())\n     mlflow.set_experiment(experiment_name)  \n        \n     with mlflow.start_run(run_name=run_name) as run:\n            \n         # fit model\n         regression_model = DecisionTreeRegressor()\n         regression_model.fit(x_train, y_train)\n        \n         # log training score \n         training_score = regression_model.score(x_train, y_train)\n         mlflow.log_metric(\"Training score\", training_score)\n    \n         my_conda_env = {\n                     \"name\": \"mlflow-env\",\n                     \"channels\": [\"conda-forge\"],\n                     \"dependencies\": [\n                         \"python=3.8.5\",\n                         {\n                             \"pip\": [\n                                 \"pip\",\n                                 \"scikit-learn~=1.0.0\",\n                                 \"uuid==1.30\",\n                                 \"lz4==4.0.0\",\n                                 \"psutil==5.9.0\",\n                                 \"cloudpickle==1.6.0\",\n                                 \"mlflow\",\n                             ],\n                         },\n                     ],\n                 }\n    \n            \n         # register the model\n         mlflow.sklearn.log_model(regression_model, \"model\", conda_env=my_conda_env)\n    \n     model_uri = f\"runs:\/{run.info.run_id}\/model\"\n     model = mlflow.register_model(model_uri, \"sklearn_regression_model\")\n    \n if __name__ == '__main__':\n    \n     # connect to your workspace\n     ws = Workspace.from_config()\n    \n     # create experiment and start logging to a new run in the experiment\n     experiment_name = \"exp_name\"\n    \n     # mlflow run name\n     run_name= '1234'\n    \n      \n     # get train data\n     x_train, y_train  = get_train_data()\n        \n     run_model(ws, experiment_name, run_name, x_train, y_train)\n\n\n\nAny idea how can change the flavor sklearn version in \"MLmodel\" file in my script?\n\n\n\n\nWith many thanks in advance!",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1655725906710,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/895819\/how-to-change-sklearn-flavors-version-in-mlflow-on.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-21T10:43:01.38Z",
                "Answer_score":0,
                "Answer_body":"@SaeidHedayati-9187 Thanks for the question. Which version of Azure ML SDK are you using?. Here is the sample that could help to custom MLmodel.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-21T16:26:59.363Z",
                "Answer_score":1,
                "Answer_body":"Thanks for your response!! I was able to solve this issue by updating scikit-learn within my workspace. Mlflow MLmodel takes that version of scikit-learn to generate flavors. But I think your solution is also correct.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"chang sklearn flavor version need chang flavor sklearn version log train model model incompat sklearn version deploy infer chang version conda set conda env sklearn log model conda env env conda yaml file remain unchang flavor mlmodel file script us creat experi notebook import sklearn tree import decisiontreeregressor core import workspac core model import model import regist model def run model experi run train train set track metric set track uri track uri set experi experi start run run run run fit model regress model decisiontreeregressor regress model fit train train log train score train score regress model score train train log metric train score train score conda env env channel conda forg depend python pip pip scikit learn uuid psutil cloudpickl regist model sklearn log model regress model model conda env conda env model uri run run info run model model regist model model uri sklearn regress model main connect workspac workspac config creat experi start log new run experi experi exp run run train data train train train data run model experi run train train idea chang flavor sklearn version mlmodel file script thank advanc",
        "Question_preprocessed_content":"chang sklearn flavor version need chang flavor log train model model incompat sklearn version deploy infer chang version conda set file remain unchang flavor mlmodel file script us creat experi notebook import import decisiontreeregressor core import workspac import model import def set track metric run fit model decisiontreeregressor log train score score regist model model model connect workspac creat experi start log new run experi run train data idea chang flavor sklearn version mlmodel file script thank advanc",
        "Answer_original_content":"saeidhedayati thank question version sdk sampl help custom mlmodel thank respons abl solv issu updat scikit learn workspac mlmodel take version scikit learn gener flavor think solut correct",
        "Answer_preprocessed_content":"thank question version sdk sampl help custom mlmodel thank respons abl solv issu updat workspac mlmodel take version gener flavor think solut correct",
        "Question_gpt_summary_original":"the user is encountering a challenge in changing the sklearn version in the mlmodel file from \"0.22.1\" to \"1.0.0\" when logging a trained model in .",
        "Question_gpt_summary":"user encount challeng chang sklearn version mlmodel file log train model",
        "Answer_gpt_summary_original":"the solution to the challenge of changing the sklearn version in the mlmodel file from \"0.22.1\" to \"1.0.0\" when logging a trained model in  is to update scikit-learn within the workspace. the  mlmodel takes that version of scikit-learn to generate flavors. another possible solution is to use a custom mlmodel.",
        "Answer_gpt_summary":"solut challeng chang sklearn version mlmodel file log train model updat scikit learn workspac mlmodel take version scikit learn gener flavor possibl solut us custom mlmodel"
    },
    {
        "Question_id":null,
        "Question_title":"Access sweep_id and run_id within train() function for local weight storage",
        "Question_body":"<p>Hello,<\/p>\n<p>As I cannot simply upload infinitely many weights using artifacts, I also want to store some locally.<br>\nFor naming, I would like to use the sweep id and\/or the run id.<\/p>\n<p>Can I access that somehow in the train function I hand over to the agent?<\/p>\n<p>Thanks<\/p>\n<p>Markus<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660719705149,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":113.0,
        "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>!<\/p>\n<p>The <code>wandb.Run<\/code> object that is returned from <code>wandb.init<\/code> contains this information as properties. You should be able to access <code>run.id<\/code> and <code>run.sweep_id<\/code> in the train function after calling <code>run = wandb.init(...)<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/access-sweep-id-and-run-id-within-train-function-for-local-weight-storage\/2948",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-18T22:04:29.034Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/markuskarner\">@markuskarner<\/a>!<\/p>\n<p>The <code>wandb.Run<\/code> object that is returned from <code>wandb.init<\/code> contains this information as properties. You should be able to access <code>run.id<\/code> and <code>run.sweep_id<\/code> in the train function after calling <code>run = wandb.init(...)<\/code>.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":31.4,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-10-17T22:05:26.069Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1660860269032,
        "Question_original_content":"access sweep run train function local weight storag hello simpli upload infinit weight artifact want store local name like us sweep run access train function hand agent thank marku",
        "Question_preprocessed_content":"access train function local weight storag hello simpli upload infinit weight artifact want store local name like us sweep run access train function hand agent thank marku",
        "Answer_original_content":"hei markuskarn run object return init contain inform properti abl access run run sweep train function call run init thank ramit",
        "Answer_preprocessed_content":"hei object return contain inform properti abl access train function call thank ramit",
        "Question_gpt_summary_original":"the user is looking for a way to access the sweep_id and run_id within the train() function in order to store weights locally.",
        "Question_gpt_summary":"user look wai access sweep run train function order store weight local",
        "Answer_gpt_summary_original":"possible solutions: \n- access the run object returned from .init() function.\n- use run.id and run.sweep_id properties to access sweep_id and run_id respectively.\n- call .init(...) function before the train() function to ensure the run object is available. \n\nsummary: the run object returned from .init() function contains the sweep_id and run_id properties that can be accessed within the train() function.",
        "Answer_gpt_summary":"possibl solut access run object return init function us run run sweep properti access sweep run respect init function train function ensur run object avail summari run object return init function contain sweep run properti access train function"
    },
    {
        "Question_id":null,
        "Question_title":"Create TPU Node - Malformed Name",
        "Question_body":"Hi! Im trying to create a Google Cloud TPU node using TPU client API and I cannot figure out the parent resource name of a TPU node in Google Cloud.I tried all the possible combinations, for example:And I always get the same error (google.api_core.exceptions.InvalidArgument: 400 Malformed name) :        Below you can find the full code I'm using to create the node. Im using Python 3.8, google-cloud-tpu v1.2.1, on a Conda virtualenv. Any help would be much apprecciated!",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1645851780000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":180.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Create-TPU-Node-Malformed-Name\/td-p\/397566\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-02T06:35:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"It appears that you have created a StackOverflow thread where a Google Cloud Platform Engineer has already replied.\u00a0\n\nHe has suggested you that\u00a0you can find the expected format of\u00a0parent\u00a0in the documentation for the underlying API method:\u00a0projects.locations.nodes.create.parent\u00a0should be formatted as\u00a0projects\/*\/locations\/*. That is, change\u00a0zones\u00a0to\u00a0locations\u00a0and remove the\u00a0\/tpus\u00a0from the end which you had included at the StackOverflow thread.\n\nThe Google Cloud Platform Engineer has further suggested you to remove\u00a0nodes\u00a0from the path. i.e. change\u00a0projects\/my-project-id\/locations\/europe-west4-a\/nodes\/\u00a0that is shown at the stack trace to\u00a0projects\/my-project-id\/locations\/europe-west4-a\/."
            },
            {
                "Answer_creation_time":"2022-03-03T23:29:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi!\n\nAs I answered in the same stackoverflow, it appears that following the recommended parent=projects\/*\/locations\/* (to be 100% clear: without \/nodes\/ ) does not work and gives the error actually shared by the authors.\n\nWe cannot remove a \/nodes\/ that we do not set in the first place.\n\nLibraries version:\ngoogle-api-core 2.6.0\ngoogle-auth 2.6.0\ngoogle-cloud-tpu 1.3.1\ngoogleapis-common-protos 1.55.0"
            },
            {
                "Answer_creation_time":"2022-03-07T13:19:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"The Google Cloud Engineer has updated the response along with the code here. Please let us know if you can use the code and whether that works."
            },
            {
                "Answer_creation_time":"2022-03-11T12:21:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi, it worked.\n\nWhen cleaning the resources though, there seems to be an issue with the lib:\n\n\u00a0\n\nNAME = f\"projects\/{manifest.tpu.gcpProject}\/locations\/{manifest.tpu.gcpZone}\/nodes\/{manifest.name}\"\n\nclient = tpu_v2alpha1.TpuClient()\n        \nrequest = tpu_v2alpha1.DeleteNodeRequest(\n    name=NAME,\n)\n\n# Make the request\noperation = client.delete_node(request=request)\n\nlogging.info(\"Waiting for operation to complete...\")\nresponse = operation.result()\n\n\u00a0\n\n\nThe TPU VM is successfully deleted, but the python code eventually fails:\n\n\u00a0\n\nCleaning TPU\nWaiting for operation to complete...\nTraceback (most recent call last):\nFile \"\/argo\/staging\/script\", line 29, in <module>\nresponse = operation.result()\nFile \"\/root\/.local\/lib\/python3.9\/site-packages\/google\/api_core\/future\/polling.py\", line 132, in result\nself._blocking_poll(timeout=timeout, **kwargs)\nFile \"\/root\/.local\/lib\/python3.9\/site-packages\/google\/api_core\/future\/polling.py\", line 110, in _blocking_poll\nretry_(self._done_or_raise)(**kwargs)\nFile \"\/root\/.local\/lib\/python3.9\/site-packages\/google\/api_core\/retry.py\", line 283, in retry_wrapped_func\nreturn retry_target(\nFile \"\/root\/.local\/lib\/python3.9\/site-packages\/google\/api_core\/retry.py\", line 190, in retry_target\nreturn target()\nFile \"\/root\/.local\/lib\/python3.9\/site-packages\/google\/api_core\/future\/polling.py\", line 88, in _done_or_raise\nif not self.done(**kwargs):\nFile \"\/root\/.local\/lib\/python3.9\/site-packages\/google\/api_core\/operation.py\", line 170, in done\nself._refresh_and_update(retry)\nFile \"\/root\/.local\/lib\/python3.9\/site-packages\/google\/api_core\/operation.py\", line 159, in _refresh_and_update\nself._set_result_from_operation()\nFile \"\/root\/.local\/lib\/python3.9\/site-packages\/google\/api_core\/operation.py\", line 130, in _set_result_from_operation\nresponse = protobuf_helpers.from_any_pb(\nFile \"\/root\/.local\/lib\/python3.9\/site-packages\/google\/api_core\/protobuf_helpers.py\", line 65, in from_any_pb\nraise TypeError(\nTypeError: Could not convert Any to Node"
            },
            {
                "Answer_creation_time":"2022-03-23T07:40:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"@Mohammad_I\u00a0could you please have a look at my response, I also created a github ticket:\u00a0https:\/\/github.com\/googleapis\/python-tpu\/issues\/92\nIt creates wrong fail warnings in our Pipelines today"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"creat tpu node malform try creat googl cloud tpu node tpu client api figur parent resourc tpu node googl cloud tri possibl combin exampl error googl api core except invalidargu malform code creat node python googl cloud tpu conda virtualenv help apprecci",
        "Question_preprocessed_content":"creat tpu node malform try creat googl cloud tpu node tpu client api figur parent resourc tpu node googl tri possibl combin exampl error code creat node python conda virtualenv help apprecci",
        "Answer_original_content":"appear creat stackoverflow thread googl cloud platform engin repli suggest thatyou expect format ofparentin document underli api method project locat node creat parentshould format asproject locat changezonestolocationsand remov tpusfrom end includ stackoverflow thread googl cloud platform engin suggest removenodesfrom path changeproject project locat europ west node shown stack trace toproject project locat europ west answer stackoverflow appear follow recommend parent project locat clear node work give error actual share author remov node set place librari version googl api core googl auth googl cloud tpu googleapi common proto googl cloud engin updat respons code let know us code work work clean resourc issu lib project manifest tpu gcpproject locat manifest tpu gcpzone node manifest client tpu valpha tpuclient request tpu valpha deletenoderequest request oper client delet node request request log info wait oper complet respons oper result tpu successfulli delet python code eventu fail clean tpu wait oper complet traceback recent file argo stage script line respons oper result file root local lib python site packag googl api core futur poll line result self block poll timeout timeout kwarg file root local lib python site packag googl api core futur poll line block poll retri self rais kwarg file root local lib python site packag googl api core retri line retri wrap func return retri target file root local lib python site packag googl api core retri line retri target return target file root local lib python site packag googl api core futur poll line rais self kwarg file root local lib python site packag googl api core oper line self refresh updat retri file root local lib python site packag googl api core oper line refresh updat self set result oper file root local lib python site packag googl api core oper line set result oper respons protobuf helper file root local lib python site packag googl api core protobuf helper line rais typeerror typeerror convert node mohammad icould look respons creat github ticket http github com googleapi python tpu issu creat wrong fail warn pipelin todai",
        "Answer_preprocessed_content":"appear creat stackoverflow thread googl cloud platform engin repli suggest thatyou expect format ofparentin document underli api format changezonestolocationsand remov end includ stackoverflow thread googl cloud platform engin suggest removenodesfrom path shown stack trace answer stackoverflow appear follow recommend work give error actual share author remov set place librari version googl cloud engin updat respons code let know us code work work clean resourc issu lib client request request oper oper respons tpu successfulli delet python code eventu fail clean tpu wait oper traceback file line respons file line result kwarg file line file line return file line return target file line file line file line file line respons file line rais typeerror typeerror convert node look respons creat github creat wrong fail warn pipelin todai",
        "Question_gpt_summary_original":"the user is encountering a challenge with creating a google cloud tpu node using the tpu client api, receiving a \"400 malformed name\" error.",
        "Question_gpt_summary":"user encount challeng creat googl cloud tpu node tpu client api receiv malform error",
        "Answer_gpt_summary_original":"the solution to the \"400 malformed name\" error when creating a google cloud tpu node using the tpu client api is to format the parent as \"projects\/*\/locations\/*\" without \"\/nodes\/\" and remove \"nodes\" from the path. a google cloud platform engineer has provided updated code to fix the issue. however, there may be an issue with the library when cleaning the tpu resources. a github ticket has been created to address this issue.",
        "Answer_gpt_summary":"solut malform error creat googl cloud tpu node tpu client api format parent project locat node remov node path googl cloud platform engin provid updat code fix issu issu librari clean tpu resourc github ticket creat address issu"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML - Notebook - Jupyter Kernel Error - No Kernel connection",
        "Question_body":"In ML Studio, when I create a notebook the top of my screen says \"Jupyter kernel error\" in red. I have a compute instance running (it's green), but it also says \"No Kernel connected\".\n\nTo correct this matter, can you please provide explicit, step by step instructions on how to review. Screen shots help too.",
        "Question_answer_count":8,
        "Question_comment_count":3,
        "Question_creation_time":1612145607873,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":6.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/252893\/azure-ml-notebook-jupyter-kernel-error-no-kernel-c-1.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-11T19:39:03.553Z",
                "Answer_score":0,
                "Answer_body":"i am having exactly same issue with \"No kernel connected\", have you guys resolved this yet? how?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-06T16:01:38.527Z",
                "Answer_score":0,
                "Answer_body":"Has anyone resolved this issue? Can someone please post the solution.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-15T20:18:50.39Z",
                "Answer_score":0,
                "Answer_body":"Part of our team had this issue this week, the root cause for us was some language-packs for pt-br not loading correctly, once the affected team members changed the page\/browser language to en-us the problem was solved.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-27T14:04:50.417Z",
                "Answer_score":0,
                "Answer_body":"I had this same issue and resolved it by stopping and restarting the compute. This may be overly simplistic as I am a learner too.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-29T19:04:08.963Z",
                "Answer_score":0,
                "Answer_body":"Hi! I got this problem right now. I don't know what happened, but I've been working with Azure ML for 6 months already and it's the first time I get this kind of error.\nDid you found the solution already? @danielgo-9074 I've got page\/browser language set to en-us ;\/",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-18T06:11:45.067Z",
                "Answer_score":0,
                "Answer_body":"Hi I also have the same problem. My compute is running but I cannot connect to any of the kernel. It keeps saying that the kernel not found or was deleted.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-26T12:38:07.16Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI have the same issue (eastus location):",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-24T19:52:41.67Z",
                "Answer_score":1,
                "Answer_body":"i have been facing the same issue my compute was running still there was no kernel, i resolved it after a long search. It seemed my firewall was preventing me from doing that.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":15.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"notebook jupyt kernel error kernel connect studio creat notebook screen sai jupyt kernel error red comput instanc run green sai kernel connect correct matter provid explicit step step instruct review screen shot help",
        "Question_preprocessed_content":"notebook jupyt kernel error kernel connect studio creat notebook screen sai jupyt kernel error red comput instanc run sai kernel connect correct matter provid explicit step step instruct review screen shot help",
        "Answer_original_content":"have exactli issu kernel connect gui resolv resolv issu post solut team issu week root caus languag pack load correctli affect team member chang page browser languag problem solv issu resolv stop restart comput overli simplist learner got problem right know happen work month time kind error solut danielgo got page browser languag set problem comput run connect kernel keep sai kernel delet hello issu eastu locat face issu comput run kernel resolv long search firewal prevent",
        "Answer_preprocessed_content":"have exactli issu kernel connect gui resolv resolv issu post solut team issu week root caus load correctli affect team member chang languag problem solv issu resolv stop restart comput overli simplist learner got problem right know happen work month time kind error solut got languag set problem comput run connect kernel keep sai kernel delet hello issu face issu comput run kernel resolv long search firewal prevent",
        "Question_gpt_summary_original":"the user is encountering a jupyter kernel error in ml studio and is requesting explicit, step-by-step instructions on how to review and correct the issue.",
        "Question_gpt_summary":"user encount jupyt kernel error studio request explicit step step instruct review correct issu",
        "Answer_gpt_summary_original":"possible solutions to the jupyter kernel error in ml studio include changing the page\/browser language to en-us, stopping and restarting the compute, and checking if the firewall is preventing the connection.",
        "Answer_gpt_summary":"possibl solut jupyt kernel error studio includ chang page browser languag stop restart comput check firewal prevent connect"
    },
    {
        "Question_id":68559059.0,
        "Question_title":"DVC connect to Min.IO to access S3",
        "Question_body":"<p>What is the proper way to connect DVC to Min.IO that is connected to some buckets on S3.<\/p>\n<pre><code>AWS-S3(My_Bucket) &gt; Min.io(MY_Bucket aliased as S3)\n<\/code><\/pre>\n<p>Right now i am accessing my bucket by using mc for example <code>mc cp s3\/my_bucket\/datasets datasets<\/code> to copy stuff from there. But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC so i can use for example <code>&quot;DVC mc-S3 pull&quot;<\/code> and <code>&quot;DVC AWS-S3 pull&quot;<\/code>.<\/p>\n<p>How do i got for it because while googling i couldn't find anything that i could easily follow.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1627469557323,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":375.0,
        "Answer_body":"<p>It looks like you are looking for a combination of things.<\/p>\n<p>First, Jorge mentioned you can set <code>endpointurl<\/code> to access Minio the same way as you would access regular S3:<\/p>\n<pre><code>dvc remote add -d minio-remote s3:\/\/mybucket\/path\ndvc remote modify minio-remote endpointurl https:\/\/minio.example.com                          \n<\/code><\/pre>\n<p>Second, it seems you can create <em>two<\/em> remotes - one for S3, one for Minio and use <code>-r<\/code> option that is available for many data management related commands:<\/p>\n<pre><code>dvc pull -r minio-remote\ndvc pull -r s3-remote\ndvc push -r minio-remote\n...\n<\/code><\/pre>\n<p>This way you could <code>push<\/code>\/<code>pull<\/code> data to\/from a specific storage.<\/p>\n<blockquote>\n<p>But I need to setup my DVC to work with min.io as a hub between AWS.S3 and DVC<\/p>\n<\/blockquote>\n<p>There are other possible ways, I think to organize this. It indeed depends on what semantics you expect from <code>DVC mc-S3 pull<\/code>. Please let us know if <code>-r<\/code> is not enough and clarify the question- that would help us here.<\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68559059",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1627513406643,
        "Question_original_content":"connect min access proper wai connect min connect bucket aw bucket min bucket alias right access bucket exampl bucket dataset dataset copi stuff need setup work min hub aw us exampl pull aw pull got googl couldn easili follow",
        "Question_preprocessed_content":"connect access proper wai connect connect bucket right access bucket exampl copi stuff need setup work hub us exampl got googl couldn easili follow",
        "Answer_original_content":"look like look combin thing jorg mention set endpointurl access minio wai access regular remot add minio remot mybucket path remot modifi minio remot endpointurl http minio exampl com second creat remot minio us option avail data manag relat command pull minio remot pull remot push minio remot wai push pull data specif storag need setup work min hub aw possibl wai think organ depend semant expect pull let know clarifi question help",
        "Answer_preprocessed_content":"look like look combin thing jorg mention set access minio wai access regular second creat remot minio us option avail data manag relat command wai data specif storag need setup work hub possibl wai think organ depend semant expect let know clarifi question help",
        "Question_gpt_summary_original":"the user is facing challenges in connecting to min.io to access s3 buckets and is looking for a way to set up their environment to use min.io as a hub between aws.s3 and s3.",
        "Question_gpt_summary":"user face challeng connect min access bucket look wai set environ us min hub aw",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions for the user's challenge of connecting to min.io to access s3 buckets. the first solution involves setting the endpoint url to access minio the same way as regular s3. the second solution involves creating two remotes - one for s3 and one for minio - and using the -r option available for many data management related commands to push\/pull data to\/from a specific storage. however, the answer also mentions that there may be other possible ways to organize this depending on the user's expected semantics, and requests clarification from the user to provide further assistance.",
        "Answer_gpt_summary":"answer suggest possibl solut user challeng connect min access bucket solut involv set endpoint url access minio wai regular second solut involv creat remot minio option avail data manag relat command push pull data specif storag answer mention possibl wai organ depend user expect semant request clarif user provid assist"
    },
    {
        "Question_id":null,
        "Question_title":"Given allow_reuse set to false and regenerate_outputs set to True when the pipeline is submitted then it stucks at the running stage with first step saying \"Not Started\"",
        "Question_body":"Hi,\n\nI am using Azure Machine Learning SDK in python to create a pipeline which needs to read data from Azure SQL Database, perform transformation, model the data as per need and store the output back to Azure SQL Database. In this scenario, I need to run the published pipeline every time(without reusing output from previous run) because underlying data changes. To resolve this problem I set allow_reuse flag to False in PythonScriptStep(). Also, I set regenerate_outputs=True while submitting the pipeline. Following is the code:\n\nfrom azureml.pipeline.steps import PythonScriptStep\ndataprep_source_dir = \".\/\"\nentry_point = \"Fetch_Data.py\"\ndata_fetch_step = PythonScriptStep(\nname=\"Fetch step\",\nscript_name=entry_point,\nsource_directory=dataprep_source_dir,\narguments=[\"--fetched-data\", fetched_data_folder],\noutputs=[fetched_data_folder],\ncompute_target=target_compute,\nrunconfig=aml_run_config,\nallow_reuse=False\n)\n\npipeline_run = Experiment(workspace, 'exp_name').submit(pipeline1, regenerate_outputs=True)\n\nIt was working fine until last month and every time pipeline was generating outputs which I intend to (not using result from previous run) but this week it started to give me another weird problem. When I am submitting the pipeline first time, I see the first step is \"Not Started\" saying that rerun will be used (which it should not as allow_reuse set to false) and weirdly the rerun id of that step and current runId is same. So finally nothing happens and pipeline stays in running stage for like 12 hrs until I cancel it.\n\n\n\n\n\n\n\n\nPlease help me fix this issue. It is very weird that I can't submit pipeline where I don't want to reuse previous job run results.\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657676507390,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/924406\/given-allow-reuse-set-to-false-and-regenerate-outp.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-19T03:19:31.13Z",
                "Answer_score":0,
                "Answer_body":"@annushokeenteamtelstracom-0813 Thanks for the details. We would recommend to raise a Azure support desk ticket from Help+Support blade from Azure portal for your resource if you have a support plan for your subscription. This will help you to share the details securely and work with an engineer who can provide more insights about the issue that if it can be replicated.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"given allow reus set fals regener output set true pipelin submit stuck run stage step sai start sdk python creat pipelin need read data azur sql databas perform transform model data need store output azur sql databas scenario need run publish pipelin time reus output previou run underli data chang resolv problem set allow reus flag fals pythonscriptstep set regener output true submit pipelin follow code pipelin step import pythonscriptstep dataprep sourc dir entri point fetch data data fetch step pythonscriptstep fetch step script entri point sourc directori dataprep sourc dir argument fetch data fetch data folder output fetch data folder comput target target comput runconfig aml run config allow reus fals pipelin run experi workspac exp submit pipelin regener output true work fine month time pipelin gener output intend result previou run week start weird problem submit pipelin time step start sai rerun allow reus set fals weirdli rerun step current runid final happen pipelin stai run stage like hr cancel help fix issu weird submit pipelin want reus previou job run result thank",
        "Question_preprocessed_content":"given set fals set true pipelin submit stuck run stage step sai start sdk python creat pipelin need read data azur sql databas perform transform model data need store output azur sql databas scenario need run publish pipelin time underli data chang resolv problem set flag fals pythonscriptstep set submit pipelin follow code import pythonscriptstep pythonscriptstep experi submit work fine month time pipelin gener output intend week start weird problem submit pipelin time step start sai rerun weirdli rerun step current runid final happen pipelin stai run stage like hr cancel help fix issu weird submit pipelin want reus previou job run result thank",
        "Answer_original_content":"annushokeenteamtelstracom thank detail recommend rais azur support desk ticket help support blade azur portal resourc support plan subscript help share detail secur work engin provid insight issu replic",
        "Answer_preprocessed_content":"thank detail recommend rais azur support desk ticket help support blade azur portal resourc support plan subscript help share detail secur work engin provid insight issu replic",
        "Question_gpt_summary_original":"the user is encountering a challenge where their pipeline is stuck in the running stage with the first step saying \"not started\" despite setting allow_reuse to false and regenerate_outputs to true when submitting the pipeline.",
        "Question_gpt_summary":"user encount challeng pipelin stuck run stage step sai start despit set allow reus fals regener output true submit pipelin",
        "Answer_gpt_summary_original":"possible solution: the user is recommended to raise an azure support desk ticket from the help+support blade in the azure portal if they have a support plan for their subscription. this will allow them to share the details securely and work with an engineer who can provide more insights about the issue and determine if it can be replicated.",
        "Answer_gpt_summary":"possibl solut user recommend rais azur support desk ticket help support blade azur portal support plan subscript allow share detail secur work engin provid insight issu determin replic"
    },
    {
        "Question_id":null,
        "Question_title":"How do I get the version of an artifact?",
        "Question_body":"<p>Hey,<br>\nI\u2019m trying to get the version of an artifact directly after logging my model (encoder) as an artifact to WandB.<\/p>\n<p><strong>Code:<\/strong><\/p>\n<pre><code class=\"lang-auto\">artifact = wandb.Artifact('my_artifact_name', type='model')\nartifact.add_file('\/home\/dezzardhd\/encoder.pth')\nwandb.log_artifact(artifact)\nversion = artifact.version\n<\/code><\/pre>\n<p>Logging works so far, but\u2026<br>\nwhen trying to access the version of the artifact I get an error.<br>\n<strong>Error:<\/strong><\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/main.py\", line 48, in &lt;module&gt;\n    train_setups.start_training_sessions(project=project)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/train_setups.py\", line 18, in start_training_sessions\n    model_pipeline(config, project=project)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/learning.py\", line 84, in model_pipeline\n    save_model(model_ae=model, model_encoder=model_encoder, model_decoder=model_decoder)\n  File \"\/home\/moritz\/PycharmProjects\/bachelorarbeit\/learning.py\", line 124, in save_model\n    version = artifact_enc.version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_artifacts.py\", line 191, in version\n    return self._logged_artifact.version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 2899, in version\n    return self._assert_instance().version\n  File \"\/home\/moritz\/anaconda3\/envs\/bachelorarbeit\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 2871, in _assert_instance\n    raise ValueError(\nValueError: Must call wait() before accessing logged artifact properties\n<\/code><\/pre>\n<p>What should I do now?<\/p>\n<p>For context:<br>\nI want to print out the version number with some other parameters so that I can easier start my evaluation process for certain runs.<\/p>\n<p>Best regards<br>\nDezzardHD<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1646690111212,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":345.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dezzardhd\">@dezzardhd<\/a>,<\/p>\n<p>Could you try running your code as the following?<\/p>\n<pre><code class=\"lang-python\">artifact = wandb.Artifact('my_artifact_name', type='model')\nartifact.add_file('\/home\/dezzardhd\/encoder.pth')\nwandb.log_artifact(artifact).wait()\nversion = artifact.version\n<\/code><\/pre>\n<p>Calling <code>wait()<\/code> after <code>log_artifact()<\/code> should resolve this for you.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-do-i-get-the-version-of-an-artifact\/2035",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-07T23:48:08.233Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dezzardhd\">@dezzardhd<\/a>,<\/p>\n<p>Could you try running your code as the following?<\/p>\n<pre><code class=\"lang-python\">artifact = wandb.Artifact('my_artifact_name', type='model')\nartifact.add_file('\/home\/dezzardhd\/encoder.pth')\nwandb.log_artifact(artifact).wait()\nversion = artifact.version\n<\/code><\/pre>\n<p>Calling <code>wait()<\/code> after <code>log_artifact()<\/code> should resolve this for you.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":21.4,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-03-08T13:03:55.518Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a>,<\/p>\n<p>this resolved the problem. Thanks!<\/p>\n<p>DezzardHD<\/p>",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-07T13:04:12.391Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1646696888232,
        "Question_original_content":"version artifact hei try version artifact directli log model encod artifact code artifact artifact artifact type model artifact add file home dezzardhd encod pth log artifact artifact version artifact version log work far try access version artifact error error traceback recent file home moritz pycharmproject bachelorarbeit main line train setup start train session project project file home moritz pycharmproject bachelorarbeit train setup line start train session model pipelin config project project file home moritz pycharmproject bachelorarbeit learn line model pipelin save model model model model encod model encod model decod model decod file home moritz pycharmproject bachelorarbeit learn line save model version artifact enc version file home moritz anaconda env bachelorarbeit lib python site packag sdk artifact line version return self log artifact version file home moritz anaconda env bachelorarbeit lib python site packag sdk run line version return self assert instanc version file home moritz anaconda env bachelorarbeit lib python site packag sdk run line assert instanc rais valueerror valueerror wait access log artifact properti context want print version number paramet easier start evalu process certain run best regard dezzardhd",
        "Question_preprocessed_content":"version artifact hei try version artifact directli log model artifact code log work far try access version artifact error error context want print version number paramet easier start evalu process certain run best regard dezzardhd",
        "Answer_original_content":"dezzardhd try run code follow artifact artifact artifact type model artifact add file home dezzardhd encod pth log artifact artifact wait version artifact version call wait log artifact resolv thank ramit",
        "Answer_preprocessed_content":"try run code follow call resolv thank ramit",
        "Question_gpt_summary_original":"the user is encountering an error when trying to access the version of an artifact they have logged, and is looking for a way to print out the version number in order to facilitate their evaluation process.",
        "Question_gpt_summary":"user encount error try access version artifact log look wai print version number order facilit evalu process",
        "Answer_gpt_summary_original":"the solution to the user's problem is to add the code \"version = artifact.version\" after logging the artifact and calling the \"wait()\" function. this should resolve the error and allow the user to print out the version number for evaluation.",
        "Answer_gpt_summary":"solut user problem add code version artifact version log artifact call wait function resolv error allow user print version number evalu"
    },
    {
        "Question_id":68131742.0,
        "Question_title":"How to select a target column in a Vertex AI AutoML time series model",
        "Question_body":"<p>I am testing out Google Cloud Vertex AI with a time series AutoML model.<\/p>\n<p>I have created a dataset, from a Biguery table, with 2 columns, one of a timestamp and another of a numeric value I want to predict:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/t5R7S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><code>salesorderdate<\/code> is my <code>TIMESTAMP<\/code> column and <code>orders<\/code> is the value I want to predict.<\/p>\n<p>When I proceed to the next step I cannot select <code>orders<\/code> as my value to predict, there are no available options for this field:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/HOed3.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/HOed3.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>What am I missing here? Surely the time series value <em>is<\/em> the target value in this case? Is there an expectation of more fields here, and can one actually add additional features as columns to a time series model in this way?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624627510520,
        "Question_favorite_count":null,
        "Question_last_edit_time":1624971874476,
        "Question_score":1.0,
        "Question_view_count":606.0,
        "Answer_body":"<p>I guess from your question that you are using &quot;forecasting models&quot;. Please note that it is in &quot;Preview&quot; <a href=\"https:\/\/cloud.google.com\/products#product-launch-stages\" rel=\"nofollow noreferrer\">Product launch stage<\/a> with all consequences of that fact.<\/p>\n<p>In the documentation you may find <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/prepare-tabular#data-structure\" rel=\"nofollow noreferrer\">Training data structure<\/a> following information:<\/p>\n<blockquote>\n<ul>\n<li>There must be at least two and no more than 1,000 columns.<\/li>\n<\/ul>\n<p>For datasets that train AutoML models, one column must be the target,\nand there must be at least one feature available to train the model.\nIf the training data does not include the target column, Vertex AI\ncannot associate the training data with the desired result.<\/p>\n<\/blockquote>\n<p>I suppose you are using AutoML models so in this situation you need to have 3 columns in the data set:<\/p>\n<ul>\n<li>Time column - used to place the observation represented by that row in time<\/li>\n<li>time series identifier column as &quot;Forecasting training data usually includes multiple time series&quot;<\/li>\n<li>and target column which is value that model should learn to predict.<\/li>\n<\/ul>\n<p>If you want to predict <code>orders<\/code> this should be target column. But before you are choosing this target this &quot;time series identifier column&quot; is already chosen in previous step, so you do not have available column to choose.<\/p>\n<p>So you need to add to your BigQuery table at least one additional column with will be used as time series column. You can add to your data set column with the same value in each row. This concept is presented in <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/datasets\/bp-tabular#data_preparation_best_practices\" rel=\"nofollow noreferrer\">Forecasting data preparation best practices<\/a>:<\/p>\n<blockquote>\n<p>You can train a forecasting model on a single time series (in other\nwords, the time series identifier column contains the same value for\nall rows). However, Vertex AI is a better fit for training data that\ncontains two or more time series. For best results, you should have at\nleast 10 time series for every column used to train the model.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1624960598496,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68131742",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1624889338667,
        "Question_original_content":"select target column automl time seri model test googl cloud time seri automl model creat dataset bigueri tabl column timestamp numer valu want predict salesorderd timestamp column order valu want predict proce step select order valu predict avail option field miss sure time seri valu target valu case expect field actual add addit featur column time seri model wai",
        "Question_preprocessed_content":"select target column automl time seri model test googl cloud time seri automl model creat dataset bigueri tabl column timestamp numer valu want predict column valu want predict proce step select valu predict avail option field miss sure time seri valu target valu case expect field actual add addit featur column time seri model wai",
        "Answer_original_content":"guess question forecast model note preview product launch stage consequ fact document train data structur follow inform column dataset train automl model column target featur avail train model train data includ target column associ train data desir result suppos automl model situat need column data set time column place observ repres row time time seri identifi column forecast train data usual includ multipl time seri target column valu model learn predict want predict order target column choos target time seri identifi column chosen previou step avail column choos need add bigqueri tabl addit column time seri column add data set column valu row concept present forecast data prepar best practic train forecast model singl time seri word time seri identifi column contain valu row better fit train data contain time seri best result time seri column train model",
        "Answer_preprocessed_content":"guess question forecast model note preview product launch stage consequ fact document train data structur follow inform column dataset train automl model column target featur avail train model train data includ target column associ train data desir result suppos automl model situat need column data set time column place observ repres row time time seri identifi column forecast train data usual includ multipl time seri target column valu model learn predict want predict target column choos target time seri identifi column chosen previou step avail column choos need add bigqueri tabl addit column time seri column add data set column valu row concept present forecast data prepar best practic train forecast model singl time seri better fit train data contain time seri best result time seri column train model",
        "Question_gpt_summary_original":"the user is encountering challenges in selecting a target column in a google cloud automl time series model.",
        "Question_gpt_summary":"user encount challeng select target column googl cloud automl time seri model",
        "Answer_gpt_summary_original":"possible solutions to the challenge of selecting a target column in a google cloud automl time series model are: \n\n- ensure that the training data structure has at least two and no more than 1,000 columns, with one column designated as the target and at least one feature available to train the model.\n- use a time column to place the observation represented by each row in time, a time series identifier column to identify the multiple time series in the forecasting training data, and a target column that represents the value the model should learn to predict.\n- add at least one additional column to the bigquery table to be used as a time series column if the time series identifier column has already been chosen in a previous step.\n- train the forecasting model on a single time series, but it is better to have at least 10 time series for every column used to train the model for best results.",
        "Answer_gpt_summary":"possibl solut challeng select target column googl cloud automl time seri model ensur train data structur column column design target featur avail train model us time column place observ repres row time time seri identifi column identifi multipl time seri forecast train data target column repres valu model learn predict add addit column bigqueri tabl time seri column time seri identifi column chosen previou step train forecast model singl time seri better time seri column train model best result"
    },
    {
        "Question_id":null,
        "Question_title":"Dataset selection for hyperparameter optimization and training",
        "Question_body":"<p>Hi,<br>\ni want to make a multiclass classifier using a bert model. For this i would like to compare the performance of (at least) two domain specific bert models. But  before i compare the model performance i would like to find the best hyperparameters using wandb sweeps und the simpletransformers api (the simpletransformers api, has an easy integration with wandb).<\/p>\n<p>Currently i\u2019m a bit confused how to select a good dataset for<\/p>\n<ol>\n<li>the hyperparameter optimization<\/li>\n<li>the training with the best hyperparams.<\/li>\n<\/ol>\n<p>So for the hyperparams, should i create n cross-validation sets and then run a training cycle with the current selected hyperparams for every m in n dataset?<br>\nE.g. i created 2 train\/test sets and i only want to find the best n of episodes out of [1,2]:<br>\nFor both train\/test sets, the training is done for 1 episode and in the nex cycle for 2 episodes?<\/p>\n<p>And if i found the best hyperparameters, should i train the final model afterwards using my full dataset?<\/p>\n<p>Hope my questions are kind of clear<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1669276235658,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":65.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/dataset-selection-for-hyperparameter-optimization-and-training\/3455",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-30T06:25:38.012Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/simonkleinfeld\">@simonkleinfeld<\/a>!<\/p>\n<p>Thank you for writing in!  The W&amp;B Help channel is usually meant for support with W&amp;B issues, you would probably get a better response on the community channel : <a href=\"https:\/\/community.wandb.ai\/c\/show-the-community\/43\" class=\"inline-onebox\">Show the Community! - W&amp;B Community<\/a>.<\/p>\n<p>In any case, I\u2019ll take a stab at helping here : A good dataset for your model would be the same (or similar) to the dataset you plan to finally use for training and inference. Good hyperparameters are usually dataset dependent.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-29T06:26:30.474Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"dataset select hyperparamet optim train want multiclass classifi bert model like compar perform domain specif bert model compar model perform like best hyperparamet sweep und simpletransform api simpletransform api easi integr current bit confus select good dataset hyperparamet optim train best hyperparam hyperparam creat cross valid set run train cycl current select hyperparam dataset creat train test set want best episod train test set train episod nex cycl episod best hyperparamet train final model dataset hope question kind clear",
        "Question_preprocessed_content":"dataset select hyperparamet optim train want multiclass classifi bert model like compar perform domain specif bert model compar model perform like best hyperparamet sweep und simpletransform api current bit confus select good dataset hyperparamet optim train best hyperparam hyperparam creat set run train cycl current select hyperparam dataset creat set want best episod set train episod nex cycl episod best hyperparamet train final model dataset hope question kind clear",
        "Answer_original_content":"simonkleinfeld thank write help channel usual meant support issu probabl better respons commun channel commun commun case ill stab help good dataset model similar dataset plan final us train infer good hyperparamet usual dataset depend topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thank write help channel usual meant support issu probabl better respons commun channel commun commun case ill stab help good dataset model dataset plan final us train infer good hyperparamet usual dataset depend topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is seeking guidance on how to select a dataset for hyperparameter optimization and training with a bert model for multiclass classification.",
        "Question_gpt_summary":"user seek guidanc select dataset hyperparamet optim train bert model multiclass classif",
        "Answer_gpt_summary_original":"the answer suggests that a good dataset for hyperparameter optimization and training with a bert model for multiclass classification would be the same or similar to the dataset that will be used for training and inference. it also mentions that good hyperparameters are usually dataset dependent.",
        "Answer_gpt_summary":"answer suggest good dataset hyperparamet optim train bert model multiclass classif similar dataset train infer mention good hyperparamet usual dataset depend"
    },
    {
        "Question_id":null,
        "Question_title":"Error creating endpoint from mlflow model (tensorflow job)",
        "Question_body":"Hello everybody,\nI am trying to deploy a realtime endpoint from a registered mlflow model obtained from a tensorflow training job.\nIn this repository, you will find the training scripts:\n\nhttps:\/\/github.com\/antigones\/py-hands-ml-tf\/tree\/main\/azure_ml\/job_script\n\nThe job outputs a MLFlow model with its conda environment yml file.\n\n\n\n\nWhen I try to deploy the model to a realtime endpoint, I get the following error:\n\n257528-azure-ml-deploy-error.txt\n\nIt seems to be an error related to protobuf, when loading the model:\n\n  File \"\/opt\/miniconda\/envs\/userenv\/lib\/python3.8\/site-packages\/google\/protobuf\/descriptor.py\", line 560, in __new__\n     _message.Message._CheckCalledFromGeneratedFile()\n TypeError: Descriptors cannot not be created directly.\n If this call came from a _pb2.py file, your generated code is out of date and must be regenerated with protoc >= 3.19.0.\n If you cannot immediately regenerate your protos, some other possible workarounds are:\n  1. Downgrade the protobuf package to 3.20.x or lower.\n  2. Set PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION=python (but this will use pure-Python parsing and will be much slower).\n    \n More information: https:\/\/developers.google.com\/protocol-buffers\/docs\/news\/2022-05-06#python-updates\n\n\n\nThe environment is deployed automatically (the scoring script is also generated).\nI have also tried different images, with different python versions (3.7) and Tensorflow versions (2.4) with no luck.\n\nHow can I solve this issue?\n\n\n\n\nThank you in advance for your support.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1667731273557,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1077214\/error-creating-endpoint-from-mlflow-model-tensorfl.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-06T14:01:03.993Z",
                "Answer_score":1,
                "Answer_body":"Seems like the problem is in azureml-inference-server-http package, where there is a mismatch with protobuf version.\n\nAs a workaround, I created a custom managed online deployment via CLI, specifing the following environment variable:\n\n environment_variables:\n   \"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\": \"python\"\n\n\n\nand then I was able to publish the endpoint.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-10T19:57:59.41Z",
                "Answer_score":0,
                "Answer_body":"I have a Keras model and I had to develop and upload my own score.py to override the init() function in order to load the model using load_model() for Keras models, instead of using joblib.load(model_path) as it was by default.\nYou probably also have to override the run() function to customize the inference.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error creat endpoint model tensorflow job hello everybodi try deploi realtim endpoint regist model obtain tensorflow train job repositori train script http github com antigon hand tree main azur job script job output model conda environ yml file try deploi model realtim endpoint follow error azur deploi error txt error relat protobuf load model file opt miniconda env userenv lib python site packag googl protobuf descriptor line new messag messag checkcalledfromgeneratedfil typeerror descriptor creat directli came file gener code date regener protoc immedi regener proto possibl workaround downgrad protobuf packag lower set protocol buffer python implement python us pure python pars slower inform http develop googl com protocol buffer doc new python updat environ deploi automat score script gener tri differ imag differ python version tensorflow version luck solv issu thank advanc support",
        "Question_preprocessed_content":"error creat endpoint model hello everybodi try deploi realtim endpoint regist model obtain tensorflow train job repositori train script job output model conda environ yml file try deploi model realtim endpoint follow error error relat protobuf load model file line typeerror descriptor creat directli came file gener code date regener protoc immedi regener proto possibl workaround downgrad protobuf packag lower set inform environ deploi automat tri differ imag differ python version tensorflow version luck solv issu thank advanc support",
        "Answer_original_content":"like problem infer server http packag mismatch protobuf version workaround creat custom manag onlin deploy cli specif follow environ variabl environ variabl protocol buffer python implement python abl publish endpoint kera model develop upload score overrid init function order load model load model kera model instead joblib load model path default probabl overrid run function custom infer",
        "Answer_preprocessed_content":"like problem packag mismatch protobuf version workaround creat custom manag onlin deploy cli specif follow environ variabl python abl publish endpoint kera model develop upload overrid init function order load model kera model instead default probabl overrid run function custom infer",
        "Question_gpt_summary_original":"the user is encountering an error when attempting to deploy a realtime endpoint from a registered  model obtained from a tensorflow training job.",
        "Question_gpt_summary":"user encount error attempt deploi realtim endpoint regist model obtain tensorflow train job",
        "Answer_gpt_summary_original":"the solution to the error encountered when deploying a realtime endpoint from a registered  model obtained from a tensorflow training job is to create a custom managed online deployment via cli and specify the environment variable \"protocol_buffers_python_implementation\": \"python\". additionally, for keras models, the user needs to develop and upload their own score.py to override the init() function and load the model using load_model() instead of joblib.load(model_path). the run() function may also need to be overridden to customize the inference.",
        "Answer_gpt_summary":"solut error encount deploi realtim endpoint regist model obtain tensorflow train job creat custom manag onlin deploy cli specifi environ variabl protocol buffer python implement python addition kera model user need develop upload score overrid init function load model load model instead joblib load model path run function need overridden custom infer"
    },
    {
        "Question_id":61737613.0,
        "Question_title":"Is there IO functionality to store trained models in kedro?",
        "Question_body":"<p>In the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.html#\" rel=\"nofollow noreferrer\">IO section of the kedro API docs<\/a> I could not find functionality w.r.t. storing trained models (e.g. <code>.pkl<\/code>, <code>.joblib<\/code>, <code>ONNX<\/code>, <code>PMML<\/code>)? Have I missed something?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1589224779627,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":795.0,
        "Answer_body":"<p>There is the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PickleLocalDataSet.html#kedro.io.PickleLocalDataSet\" rel=\"nofollow noreferrer\"><code>pickle<\/code><\/a> dataset in <code>kedro.io<\/code>, that you can use to save trained models and\/or anything you want to pickle and is serialisable (models being a common object). It accepts a <code>backend<\/code> that defaults to <code>pickle<\/code> but can be set to <code>joblib<\/code> if you want to use <code>joblib<\/code> instead.<\/p>\n\n<p>I'm just going to quickly note that Kedro is moving to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.extras.datasets.html\" rel=\"nofollow noreferrer\"><code>kedro.extras.datasets<\/code><\/a> for its datasets and moving away from having non-core datasets in <code>kedro.io<\/code>. You might want to look at <code>kedro.extras.datasets<\/code> and in Kedro 0.16 onwards <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/kedro.extras.datasets.pickle.PickleDataSet.html#kedro.extras.datasets.pickle.PickleDataSet\" rel=\"nofollow noreferrer\"><code>pickle.PickleDataSet<\/code><\/a> with <code>joblib<\/code> support.<\/p>\n\n<p>The Kedro <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/02_tutorial_template.html\" rel=\"nofollow noreferrer\"><code>spaceflights<\/code><\/a> tutorial in the documentation actually saves the trained linear regression model using the <code>pickle<\/code> dataset if you want to see an example of it. The relevant section is <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/03_tutorial\/04_create_pipelines.html#working-with-multiple-pipelines\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1589277661208,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61737613",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1589225919163,
        "Question_original_content":"function store train model section api doc function store train model pkl joblib onnx pmml miss",
        "Question_preprocessed_content":"function store train model section api doc function store train model miss",
        "Answer_original_content":"pickl dataset us save train model want pickl serialis model common object accept backend default pickl set joblib want us joblib instead go quickli note move extra dataset dataset move awai have non core dataset want look extra dataset onward pickl pickledataset joblib support spaceflight tutori document actual save train linear regress model pickl dataset want exampl relev section",
        "Answer_preprocessed_content":"dataset us save train model want pickl serialis accept default set want us instead go quickli note move dataset move awai have dataset want look onward support tutori document actual save train linear regress model dataset want exampl relev section",
        "Question_gpt_summary_original":"the user is looking for functionality to store trained models in the io section of the api docs, but has not been able to find it.",
        "Question_gpt_summary":"user look function store train model section api doc abl",
        "Answer_gpt_summary_original":"possible solutions from the answer include using the pickle dataset in .io to save trained models and\/or anything that can be pickled and is serializable. the backend can be set to pickle or joblib. the user can also look at .extras.datasets and in 0.16 onwards pickle.pickledataset with joblib support. the spaceflights tutorial in the documentation provides an example of saving a trained linear regression model using the pickle dataset.",
        "Answer_gpt_summary":"possibl solut answer includ pickl dataset save train model pickl serializ backend set pickl joblib user look extra dataset onward pickl pickledataset joblib support spaceflight tutori document provid exampl save train linear regress model pickl dataset"
    },
    {
        "Question_id":55621967.0,
        "Question_title":"Feature Importance for XGBoost in Sagemaker",
        "Question_body":"<p>I have built an XGBoost model using Amazon Sagemaker, but I was unable to find anything which will help me interpret the model and validate if it has learned the right dependencies.<\/p>\n\n<p>Generally, we can see Feature Importance for XGBoost by get_fscore() function in the python API (<a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html<\/a>) I see nothing of that sort in the sagemaker api(<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/estimators.html<\/a>).<\/p>\n\n<p>I know I can build my own model and then deploy that using sagemaker but I am curious if anyone has faced this problem and how they overcame it.<\/p>\n\n<p>Thanks.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1554934002317,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":3637.0,
        "Answer_body":"<p>SageMaker XGBoost currently does not provide interface to retrieve feature importance from the model. You can write some code to get the feature importance from the XGBoost model. You have to get the booster object artifacts from the model in S3 and then use the following snippet <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle as pkl\nimport xgboost\nbooster = pkl.load(open(model_file, 'rb'))\nbooster.get_score()\nbooster.get_fscore()\n<\/code><\/pre>\n\n<p>Refer <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/python\/python_api.html\" rel=\"nofollow noreferrer\">XGBoost doc<\/a> for methods to get feature importance from the Booster object such as <code>get_score()<\/code> or <code>get_fscore()<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55621967",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1555001695856,
        "Question_original_content":"featur import xgboost built xgboost model unabl help interpret model valid learn right depend gener featur import xgboost fscore function python api http xgboost readthedoc latest python python api html sort api http readthedoc stabl estim html know build model deploi curiou face problem overcam thank",
        "Question_preprocessed_content":"featur import xgboost built xgboost model unabl help interpret model valid learn right depend gener featur import xgboost function python api sort know build model deploi curiou face problem overcam thank",
        "Answer_original_content":"xgboost current provid interfac retriev featur import model write code featur import xgboost model booster object artifact model us follow snippet import pickl pkl import xgboost booster pkl load open model file booster score booster fscore refer xgboost doc method featur import booster object score fscore",
        "Answer_preprocessed_content":"xgboost current provid interfac retriev featur import model write code featur import xgboost model booster object artifact model us follow snippet refer xgboost doc method featur import booster object",
        "Question_gpt_summary_original":"the user is facing a challenge in finding a way to interpret and validate an xgboost model built using , and is looking for solutions to overcome this issue.",
        "Question_gpt_summary":"user face challeng find wai interpret valid xgboost model built look solut overcom issu",
        "Answer_gpt_summary_original":"the answer suggests that xgboost does not provide an interface to retrieve feature importance from the model, but provides a solution to get the feature importance by writing some code. the solution involves getting the booster object artifacts from the model in s3 and using the provided code snippet to import pickle and xgboost, and then using the methods get_score() or get_fscore() to get the feature importance. the user is advised to refer to the xgboost documentation for more information on these methods.",
        "Answer_gpt_summary":"answer suggest xgboost provid interfac retriev featur import model provid solut featur import write code solut involv get booster object artifact model provid code snippet import pickl xgboost method score fscore featur import user advis refer xgboost document inform method"
    },
    {
        "Question_id":null,
        "Question_title":"What is AML_MODEL_URI - PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Question_body":"Hi all,\n\nI am following the steps on this tutorial:\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools https:\/\/docs.microsoft.com\/en-us\/azure\/synapse-analytics\/machine-learning\/tutorial-score-model-predict-spark-pool\n\nI don't know what is the AML_MODEL_URI. I thought it was the REST endpoint or the Swagger URI from the endpoint.\n\n\nBut it is not working. I am getting this error on Synapse: \"RuntimeError: Load model failed\nTraceback (most recent call last):\"\n\n\nI appreciate you help.\n\nKind regards,\nAnaid",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1637180271803,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @Anaid-6816,\n\nThanks for the question and using MS Q&A platform.\n\nAML_MODEL_URL is the same name of the model in the ML workspace with (follow the format of aml:\/\/ + Name of the Model).\n\nExample: aml:\/\/sklearn_regression_model:1 (follow the format of aml:\/\/ + Name of the Model).\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-23T11:39:05.273Z",
                "Answer_score":1,
                "Answer_body":"Hello @Anaid-6816,\n\nThanks for the question and using MS Q&A platform.\n\nAML_MODEL_URL is the same name of the model in the ML workspace with (follow the format of aml:\/\/ + Name of the Model).\n\nExample: aml:\/\/sklearn_regression_model:1 (follow the format of aml:\/\/ + Name of the Model).\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1637667545272,
        "Question_original_content":"aml model uri predict serverless apach spark pool synaps aml follow step tutori tutori score machin learn model predict serverless apach spark pool http doc microsoft com azur synaps analyt machin learn tutori score model predict spark pool know aml model uri thought rest endpoint swagger uri endpoint work get error synaps runtimeerror load model fail traceback recent appreci help kind regard anaid",
        "Question_preprocessed_content":"predict serverless apach spark pool follow step tutori tutori score machin learn model predict serverless apach spark pool know thought rest endpoint swagger uri endpoint work get error synaps runtimeerror load model fail traceback appreci help kind regard anaid",
        "Answer_original_content":"hello anaid thank question platform aml model url model workspac follow format aml model exampl aml sklearn regress model follow format aml model hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"hello thank question platform model workspac exampl hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Question_gpt_summary_original":"the user is encountering challenges with understanding what the aml_model_uri is and how to use it to score machine learning models with predict in serverless apache spark pools.",
        "Question_gpt_summary":"user encount challeng understand aml model uri us score machin learn model predict serverless apach spark pool",
        "Answer_gpt_summary_original":"the answer provides a solution to the user's challenge by explaining that the aml_model_uri is the same name of the model in the ml workspace with the format of aml:\/\/ + name of the model. an example is given to illustrate this format. the user is encouraged to upvote the answer if it helps and to ask further queries if needed. additionally, the answer provides information on how to subscribe to notifications and become a volunteer moderator.",
        "Answer_gpt_summary":"answer provid solut user challeng explain aml model uri model workspac format aml model exampl given illustr format user encourag upvot answer help ask queri need addition answer provid inform subscrib notif volunt moder"
    },
    {
        "Question_id":null,
        "Question_title":"Setting up data for DeepAR, targets and categories for simultaneous data?",
        "Question_body":"I would like to try out DeepAR for an engineering problem that I have some sensor datasets for, but I am unsure how to set it up for ingestion into DeepAR to get a predictive model.\n\nThe data is essentially the positions, orientations, and a few other timeseries sensor readings of an assortment of objects (animals, in this case, actually) over time. Data is both noisy and sometimes missing.\n\nSo, in this case, there are N individuals and for each individual, there are Z variables of interest per individual. None of the variables are \"static\" (color, size, etc), they are all expected to be time-varying on the same time scale. Ultimately, I would like to try and predict all Z targets for all N individuals.\n\nHow do I set up the timeseries to feed into DeepAR? The premise is that all these individuals are implicitly interacting in the observed space, so all the target values have some interdependence on each other, which is what I would like to see if DeepAR can take into account to make predictions.\n\nShould I be using a category vector of length 2, such that the first cat variable corresponds to the individual, and the second corresponds to one of the variables associated with the individual? Then there would be N*Z targets in my input dataset, each with cat = [ n , z ], where there are N distinct values for n, and z for Z?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1646522787273,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":95.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUT2YRXdWxS2-ORoyS5CXF7w\/setting-up-data-for-deep-ar-targets-and-categories-for-simultaneous-data",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-07T08:20:30.261Z",
                "Answer_score":1,
                "Answer_body":"\"Yes, but...\"\n\nI agree it sounds like all your N*Z timeseries are prediction targets: You don't know them into the future, so can't provide them as dynamic_feats. Creating each as a target record with a 2D cat encoding the individual n and the variable z would probably be the \"right\" way to submit this data to SageMaker DeepAR per the algorithm docs.\n\nBut I'm not overly optimistic about the success of the DeepAR algorithm for this task... As described in the original paper, it's mainly an auto-regressive model from past target and dynamic_feats to output, with some global conditioning\/encoding on the cat features. It's true there's scope in there to learn correlations\/interactions between series, but I'm not sure whether there'll be enough bandwidth\/capacity in that coupling to learn your inter-individual interactions if those are really dominant.\n\nTo illustrate what I mean at a high level:\n\nIf we were talking about global position of migratory birds over months, I'd guess (only a guess) that basic auto-regression over what time of year it is and where bird A usually hangs out in March could already tell you quite a lot... Maybe bird A and bird B always travel together - this also seems like a manageable pattern for a primarily forecasting-based model.\nIf we were watching pigs bounce around in a small pen over seconds, I'd guess the position and direction of each pig would be tightly coupled. Lots of information about the current location of other pigs would be important for deducing where pig X goes next. Seems more like the domain of dynamic system identification to me.\n\nOf course absolutes are pretty hard to come by in ML and DeepAR+ is pre-built - so could still be worth trying. You might even consider a custom domain model in Amazon Forecast to see if the automatic ensembling and extra algorithms it offers could help out... But might be that there are other modelling approaches out there if the forecasting angle doesn't produce the results you're looking for.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"set data deepar target categori simultan data like try deepar engin problem sensor dataset unsur set ingest deepar predict model data essenti posit orient timeseri sensor read assort object anim case actual time data noisi miss case individu individu variabl individu variabl static color size expect time vari time scale ultim like try predict target individu set timeseri feed deepar premis individu implicitli interact observ space target valu interdepend like deepar account predict categori vector length cat variabl correspond individu second correspond variabl associ individu target input dataset cat distinct valu",
        "Question_preprocessed_content":"set data deepar target categori simultan data like try deepar engin problem sensor dataset unsur set ingest deepar predict model data essenti posit orient timeseri sensor read assort object time data noisi miss case individu individu variabl individu variabl static expect time scale ultim like try predict target individu set timeseri feed deepar premis individu implicitli interact observ space target valu interdepend like deepar account predict categori vector length cat variabl correspond individu second correspond variabl associ individu target input dataset cat distinct valu",
        "Answer_original_content":"ye agre sound like timeseri predict target know futur provid dynam feat creat target record cat encod individu variabl probabl right wai submit data deepar algorithm doc overli optimist success deepar algorithm task describ origin paper mainli auto regress model past target dynam feat output global condit encod cat featur true scope learn correl interact seri sure bandwidth capac coupl learn inter individu interact domin illustr mean high level talk global posit migratori bird month guess guess basic auto regress time year bird usual hang march tell lot mayb bird bird travel like manag pattern primarili forecast base model watch pig bounc small pen second guess posit direct pig tightli coupl lot inform current locat pig import deduc pig goe like domain dynam identif cours absolut pretti hard come deepar pre built worth try consid custom domain model amazon forecast automat ensembl extra algorithm offer help model approach forecast angl produc result look",
        "Answer_preprocessed_content":"ye agre sound like timeseri predict target know futur provid creat target record cat encod individu variabl probabl right wai submit data deepar algorithm doc overli optimist success deepar algorithm describ origin paper mainli model past target output global cat featur true scope learn seri sure coupl learn interact domin illustr mean high level talk global posit migratori bird month guess basic time year bird usual hang march tell mayb bird bird travel like manag pattern primarili model watch pig bounc small pen second guess posit direct pig tightli coupl lot inform current locat pig import deduc pig goe like domain dynam identif cours absolut pretti hard come deepar worth try consid custom domain model amazon forecast automat ensembl extra algorithm offer help model approach forecast angl produc result look",
        "Question_gpt_summary_original":"the user is facing a challenge of setting up their data for ingestion into deepar to get a predictive model, with the data consisting of positions, orientations, and other timeseries sensor readings of an assortment of objects over time.",
        "Question_gpt_summary":"user face challeng set data ingest deepar predict model data consist posit orient timeseri sensor read assort object time",
        "Answer_gpt_summary_original":"the answer suggests that the user should create each target record with a 2d cat encoding the individual n and the variable z to submit the data to deepar. however, the answer also expresses skepticism about the success of the deepar algorithm for this task, as it is mainly an auto-regressive model that may not have enough capacity to learn inter-individual interactions. the answer suggests that a custom domain model in amazon forecast or other modeling approaches may be worth trying if the forecasting angle doesn't produce the desired results.",
        "Answer_gpt_summary":"answer suggest user creat target record cat encod individu variabl submit data deepar answer express skeptic success deepar algorithm task mainli auto regress model capac learn inter individu interact answer suggest custom domain model amazon forecast model approach worth try forecast angl produc desir result"
    },
    {
        "Question_id":67925860.0,
        "Question_title":"Kedro Data Modelling",
        "Question_body":"<p>We are struggling to model our data correctly for use in Kedro - we are using the recommended Raw\\Int\\Prm\\Ft\\Mst model but are struggling with some of the concepts....e.g.<\/p>\n<ul>\n<li>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/li>\n<li>Is it OK for a primary dataset to consume data from another primary dataset?<\/li>\n<li>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/li>\n<\/ul>\n<p>I appreciate there are no hard &amp; fast rules with data modelling but these are big modelling decisions &amp; any guidance or best practice on Kedro modelling would be really helpful, I can find just one table defining the layers in the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/12_faq\/01_faq.html#what-is-data-engineering-convention\" rel=\"nofollow noreferrer\">Kedro docs<\/a><\/p>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1623345871000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":180.0,
        "Answer_body":"<p>Great question. As you say, there are no hard and fast rules here and opinions do vary, but let me share my perspective as a QB data scientist and kedro maintainer who has used the layering convention you referred to several times.<\/p>\n<p>For a start, let me emphasise that there's absolutely no reason to stick to the data engineering convention suggested by kedro if it's not suitable for your needs. 99% of users don't change the folder structure in <code>data<\/code>. This is not because the kedro default is the right structure for them but because they just don't think of changing it. You should absolutely add\/remove\/rename layers to suit yourself. The most important thing is to choose a set of layers (or even a non-layered structure) that works for your project rather than trying to shoehorn your datasets to fit the kedro default suggestion.<\/p>\n<p>Now, assuming you are following kedro's suggested structure - onto your questions:<\/p>\n<blockquote>\n<p>When is a dataset a feature rather than a primary dataset? The distinction seems vague...<\/p>\n<\/blockquote>\n<p>In the case of simple features, a feature dataset can be very similar to a primary one. The distinction is maybe clearest if you think about more complex features, e.g. formed by aggregating over time windows. A primary dataset would have a column that gives a cleaned version of the original data, but without doing any complex calculations on it, just simple transformations. Say the raw data is the colour of all cars driving past your house over a week. By the time the data is in primary, it will be clean (e.g. correcting &quot;rde&quot; to &quot;red&quot;, maybe mapping &quot;crimson&quot; and &quot;red&quot; to the same colour). Between primary and the feature layer, we will have done some less trivial calculations on it, e.g. to find one-hot encoded most common car colour each day.<\/p>\n<blockquote>\n<p>Is it OK for a primary dataset to consume data from another primary dataset?<\/p>\n<\/blockquote>\n<p>In my opinion, yes. This might be necessary if you want to join multiple primary tables together. In general if you are building complex pipelines it will become very difficult if you don't allow this. e.g. in the feature layer I might want to form a dataset containing <code>composite_feature = feature_1 * feature_2<\/code> from the two inputs <code>feature_1<\/code> and <code>feature_2<\/code>. There's no way of doing this without having multiple sub-layers within the feature layer.<\/p>\n<p>However, something that is generally worth avoiding is a node that consumes data from many different layers. e.g. a node that takes in one dataset from the feature layer and one from the intermediate layer. This seems a bit strange (why has the latter dataset not passed through the feature layer?).<\/p>\n<blockquote>\n<p>Is it good practice to build a feature dataset from the INT layer? or should it always pass through Primary?<\/p>\n<\/blockquote>\n<p>Building features from the intermediate layer isn't unheard of, but it seems a bit weird. The primary layer is typically an important one which forms the basis for all feature engineering. If your data is in a shape that you can build features then that means it's probably primary layer already. In this case, maybe you don't need an intermediate layer.<\/p>\n<p>The above points might be summarised by the following rules (which should no doubt be broken when required):<\/p>\n<ol>\n<li>The input datasets for a node in layer <code>L<\/code> should all be in the same layer, which can be either <code>L<\/code> or <code>L-1<\/code><\/li>\n<li>The output datasets for a node in layer <code>L<\/code> should all be in the same layer <code>L<\/code>, which can be either <code>L<\/code> or <code>L+1<\/code><\/li>\n<\/ol>\n<blockquote>\n<p>If anyone can offer any further advice or blogs\\docs talking about Kedro Data Modelling that would be awesome!<\/p>\n<\/blockquote>\n<p>I'm also interested in seeing what others think here! One possibly useful thing to note is that kedro was inspired by cookiecutter data science, and the kedro layer structure is an extended version of <a href=\"http:\/\/drivendata.github.io\/cookiecutter-data-science\/#directory-structure\" rel=\"nofollow noreferrer\">what's suggested there<\/a>. Maybe other projects have taken this directory structure and adapted it in different ways.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67925860",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1623349806340,
        "Question_original_content":"data model struggl model data correctli us recommend raw int prm mst model struggl concept dataset featur primari dataset distinct vagu primari dataset consum data primari dataset good practic build featur dataset int layer pass primari appreci hard fast rule data model big model decis guidanc best practic model help tabl defin layer doc offer advic blog doc talk data model awesom",
        "Question_preprocessed_content":"data model struggl model data correctli us recommend model struggl dataset featur primari dataset distinct primari dataset consum data primari dataset good practic build featur dataset int layer pass primari appreci hard fast rule data model big model decis guidanc best practic model help tabl defin layer doc offer advic talk data model awesom",
        "Answer_original_content":"great question hard fast rule opinion vari let share perspect data scientist maintain layer convent refer time start let emphasis absolut reason stick data engin convent suggest suitabl need user chang folder structur data default right structur think chang absolut add remov renam layer suit import thing choos set layer non layer structur work project try shoehorn dataset fit default suggest assum follow suggest structur question dataset featur primari dataset distinct vagu case simpl featur featur dataset similar primari distinct mayb clearest think complex featur form aggreg time window primari dataset column give clean version origin data complex calcul simpl transform raw data colour car drive past hous week time data primari clean correct rde red mayb map crimson red colour primari featur layer trivial calcul hot encod common car colour dai primari dataset consum data primari dataset opinion ye necessari want join multipl primari tabl gener build complex pipelin difficult allow featur layer want form dataset contain composit featur featur featur input featur featur wai have multipl sub layer featur layer gener worth avoid node consum data differ layer node take dataset featur layer intermedi layer bit strang dataset pass featur layer good practic build featur dataset int layer pass primari build featur intermedi layer isn unheard bit weird primari layer typic import form basi featur engin data shape build featur mean probabl primari layer case mayb need intermedi layer point summaris follow rule doubt broken requir input dataset node layer layer output dataset node layer layer offer advic blog doc talk data model awesom interest see think possibl us thing note inspir cookiecutt data scienc layer structur extend version suggest mayb project taken directori structur adapt differ wai",
        "Answer_preprocessed_content":"great question hard fast rule opinion vari let share perspect data scientist maintain layer convent refer time start let emphasis absolut reason stick data engin convent suggest suitabl need user chang folder structur default right structur think chang absolut layer suit import thing choos set layer work project try shoehorn dataset fit default suggest assum follow suggest structur question dataset featur primari dataset distinct case simpl featur featur dataset similar primari distinct mayb clearest think complex featur form aggreg time window primari dataset column give clean version origin data complex calcul simpl transform raw data colour car drive past hous week time data primari clean primari featur layer trivial calcul encod common car colour dai primari dataset consum data primari dataset opinion ye necessari want join multipl primari tabl gener build complex pipelin difficult allow featur layer want form dataset contain input wai have multipl featur layer gener worth avoid node consum data differ layer node take dataset featur layer intermedi layer bit strang good practic build featur dataset int layer pass primari build featur intermedi layer isn unheard bit weird primari layer typic import form basi featur engin data shape build featur mean probabl primari layer case mayb need intermedi layer point summaris follow rule input dataset node layer layer output dataset node layer layer offer advic talk data model awesom interest see think possibl us thing note inspir cookiecutt data scienc layer structur extend version suggest mayb project taken directori structur adapt differ wai",
        "Question_gpt_summary_original":"the user is struggling to correctly model their data for use in , and is having difficulty understanding the distinction between a dataset being a feature or a primary dataset, as well as the best practice for building a feature dataset from the int layer.",
        "Question_gpt_summary":"user struggl correctli model data us have difficulti understand distinct dataset featur primari dataset best practic build featur dataset int layer",
        "Answer_gpt_summary_original":"the answer provides some possible solutions for the user struggling to model their data. the most important thing is to choose a set of layers that work for the project rather than trying to fit the default suggestion. the distinction between a dataset being a feature or a primary dataset is vague, but the primary dataset is typically an important one that forms the basis for all feature engineering. it is generally good practice to avoid a node that consumes data from many different layers. the input datasets for a node in layer l should all be in the same layer, which can be either l or l-1, and the output datasets for a node in layer l should all be in the same layer l, which can be either l or l+1.",
        "Answer_gpt_summary":"answer provid possibl solut user struggl model data import thing choos set layer work project try fit default suggest distinct dataset featur primari dataset vagu primari dataset typic import form basi featur engin gener good practic avoid node consum data differ layer input dataset node layer layer output dataset node layer layer"
    },
    {
        "Question_id":63667022.0,
        "Question_title":"Getting KeyError : 'callable_inputs' when trying to save a TF model in S3 bucket",
        "Question_body":"<p>I'm using sagemaker 2.5.1 and tensorflow 2.3.0\nThe weird part is that the same code worked before, the only change that I could think of is the new release of the two libraries<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1598861056290,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":174.0,
        "Answer_body":"<p>The problem is actually coming from smdebug version 0.9.1\nDowngrading to 0.8.1 solves the issue<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63667022",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1599662565023,
        "Question_original_content":"get keyerror callabl input try save model bucket tensorflow weird code work chang think new releas librari",
        "Question_preprocessed_content":"get keyerror try save model bucket tensorflow weird code work chang think new releas librari",
        "Answer_original_content":"problem actual come smdebug version downgrad solv issu",
        "Answer_preprocessed_content":"problem actual come smdebug version downgrad solv issu",
        "Question_gpt_summary_original":"the user is encountering a keyerror when attempting to save a tensorflow model in an s3 bucket, despite the same code having worked previously with the same libraries.",
        "Question_gpt_summary":"user encount keyerror attempt save tensorflow model bucket despit code have work previous librari",
        "Answer_gpt_summary_original":"the solution to the keyerror encountered when saving a tensorflow model in an s3 bucket is to downgrade the smdebug version from 0.9.1 to 0.8.1.",
        "Answer_gpt_summary":"solut keyerror encount save tensorflow model bucket downgrad smdebug version"
    },
    {
        "Question_id":64211755.0,
        "Question_title":"How to upload packages to an instance in a Processing step in Sagemaker?",
        "Question_body":"<p>I have to do large scale feature engineering on some data. My current approach is to spin up an instance using <code>SKLearnProcessor<\/code> and then scale the job by choosing a larger instance size or increasing the number of instances. I require using some packages that are not installed on Sagemaker instances by default and so I want to install the packages using .whl files.<\/p>\n<p>Another hurdle is that the Sagemaker role does not have internet access.<\/p>\n<pre><code>import boto3\nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.sklearn.processing import SKLearnProcessor\n\nsess = sagemaker.Session()\nsess.default_bucket()        \n\nregion = boto3.session.Session().region_name\n\nrole = get_execution_role()\nsklearn_processor = SKLearnProcessor(framework_version='0.20.0',\n                                     role=role,\n                                     sagemaker_session = sess,\n                                     instance_type=&quot;ml.t3.medium&quot;,\n                                     instance_count=1)\n\nsklearn_processor.run(code='script.py')\n<\/code><\/pre>\n<p><strong>Attempted resolutions:<\/strong><\/p>\n<ol>\n<li>Upload the packages to a CodeCommit repository and clone the repo into the SKLearnProcessor instances. Failed with error <code>fatal: could not read Username for 'https:\/\/git-codecommit.eu-west-1.amazonaws.com': No such device or address<\/code>. I tried cloning the repo into a sagemaker notebook instance and it works, so its not a problem with my script.<\/li>\n<li>Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off <a href=\"https:\/\/medium.com\/@shadidc\/installing-custom-python-package-to-sagemaker-notebook-b7b897f4f655\" rel=\"nofollow noreferrer\">this post<\/a>. But the packages never get copied, and an error message is not thrown.<\/li>\n<li>Also looked into using the package <code>s3fs<\/code> but it didn't seem suitable to copy the wheel files.<\/li>\n<\/ol>\n<p><strong>Alternatives<\/strong><\/p>\n<p>My client is hesitant to spin up containers from custom docker images. Any alternatives?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601912020397,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1071.0,
        "Answer_body":"<p><code>2. Use a bash script to copy the packages from s3 using the CLI. The bash script I used is based off this post. But the packages never get copied, and an error message is not thrown.<\/code><\/p>\n<p>This approach seems sound.<\/p>\n<p>You may be better off overriding the <code>command<\/code> field on the <code>SKLearnProcessor<\/code> to <code>\/bin\/bash<\/code>, run a bash script like <code>install_and_run_my_python_code.sh<\/code> that installs the wheel containing your python dependencies, then runs your main python entry point script.<\/p>\n<p>Additionally, instead of using AWS S3 calls to download your code in a script, you could use a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html#sagemaker.processing.ProcessingInput\" rel=\"nofollow noreferrer\">ProcessingInput<\/a> to download your code rather than doing this with AWS CLI calls in a bash script, which is what the <code>SKLearnProcessor<\/code> does to download your entry point <code>script.py<\/code> code across all the instances.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64211755",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1604006575952,
        "Question_original_content":"upload packag instanc process step larg scale featur engin data current approach spin instanc sklearnprocessor scale job choos larger instanc size increas number instanc requir packag instal instanc default want instal packag whl file hurdl role internet access import boto import import execut role sklearn process import sklearnprocessor sess session sess default bucket region boto session session region role execut role sklearn processor sklearnprocessor framework version role role session sess instanc type medium instanc count sklearn processor run code script attempt resolut upload packag codecommit repositori clone repo sklearnprocessor instanc fail error fatal read usernam http git codecommit west amazonaw com devic address tri clone repo notebook instanc work problem script us bash script copi packag cli bash script base post packag copi error messag thrown look packag sf suitabl copi wheel file altern client hesit spin contain custom docker imag altern",
        "Question_preprocessed_content":"upload packag instanc process step larg scale featur engin data current approach spin instanc scale job choos larger instanc size increas number instanc requir packag instal instanc default want instal packag whl file hurdl role internet access attempt resolut upload packag codecommit repositori clone repo sklearnprocessor instanc fail error tri clone repo notebook instanc work problem script us bash script copi packag cli bash script base post packag copi error messag thrown look packag suitabl copi wheel file altern client hesit spin contain custom docker imag altern",
        "Answer_original_content":"us bash script copi packag cli bash script base post packag copi error messag thrown approach sound better overrid command field sklearnprocessor bin bash run bash script like instal run python code instal wheel contain python depend run main python entri point script addition instead aw call download code script us processinginput download code aw cli call bash script sklearnprocessor download entri point script code instanc",
        "Answer_preprocessed_content":"approach sound better overrid field run bash script like instal wheel contain python depend run main python entri point script addition instead aw call download code script us processinginput download code aw cli call bash script download entri point code instanc",
        "Question_gpt_summary_original":"the user is encountering challenges with uploading packages to an instance in a processing step, as the role does not have internet access and attempts to clone a repository and copy packages from s3 have failed.",
        "Question_gpt_summary":"user encount challeng upload packag instanc process step role internet access attempt clone repositori copi packag fail",
        "Answer_gpt_summary_original":"possible solutions to the challenge of uploading packages to an instance in a processing step without internet access are: using a bash script to copy packages from s3 using the cli, overriding the command field on the sklearnprocessor to run a bash script that installs the wheel containing python dependencies and runs the main entry point script, and using a processinginput to download code instead of using aws cli calls in a bash script.",
        "Answer_gpt_summary":"possibl solut challeng upload packag instanc process step internet access bash script copi packag cli overrid command field sklearnprocessor run bash script instal wheel contain python depend run main entri point script processinginput download code instead aw cli call bash script"
    },
    {
        "Question_id":47921875.0,
        "Question_title":"Accessing files in Mongodb",
        "Question_body":"<p>I am using sacred package in python, this allows to keep track of computational experiments i'm running. sacred allows to add observer (<code>mongodb<\/code>) which stores all sorts of information regarding the experiment (<code>configuration<\/code>, <code>source files<\/code> etc).\n<code>sacred<\/code> allows to add artifacts to the db bt using <code>sacred.Experiment.add_artifact(PATH_TO_FILE).<\/code><\/p>\n\n<p>This command essentially adds the file to the DB.<\/p>\n\n<p>I'm using MongoDB compass, I can access the experiment information and see that an artifact has been added. it contains two fields:\n'<code>name<\/code>' and '<code>file_id<\/code>' which contains an <code>ObjectId<\/code>. (see image)<\/p>\n\n<p>I am attempting to access the stored file itself. i have noticed that under my db there is an additional <code>sub-db<\/code> called <code>fs.files<\/code> in it i can filter to find my <code>ObjectId<\/code> but it does not seem to allow me to access to content of the file itself.<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/SBg8m.png\" alt=\"object id under .files\"><\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/B7ymG.png\" alt=\"file_id under artifact\/object\"><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1513848655213,
        "Question_favorite_count":null,
        "Question_last_edit_time":1525602466710,
        "Question_score":1.0,
        "Question_view_count":2457.0,
        "Answer_body":"<p>MongoDB file storage is handled by \"GridFS\" which basically splits up files in chunks and stores them in a collection (fs.files).<\/p>\n\n<p>Tutorial to access: <a href=\"http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html\" rel=\"nofollow noreferrer\">http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47921875",
        "Tool":"Sacred",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1513855746928,
        "Question_original_content":"access file mongodb packag python allow track comput experi run allow add observ mongodb store sort inform experi configur sourc file allow add artifact experi add artifact path file command essenti add file mongodb compass access experi inform artifact ad contain field file contain objectid imag attempt access store file notic addit sub call file filter objectid allow access content file",
        "Question_preprocessed_content":"access file mongodb packag python allow track comput experi run allow add observ store sort inform experi allow add artifact command essenti add file mongodb compass access experi inform artifact ad contain field contain attempt access store file notic addit call filter allow access content file",
        "Answer_original_content":"mongodb file storag handl gridf basic split file chunk store collect file tutori access http api mongodb com python current exampl gridf html",
        "Answer_preprocessed_content":"mongodb file storag handl gridf basic split file chunk store collect tutori access",
        "Question_gpt_summary_original":"the user is encountering challenges accessing the stored files in mongodb, as they are unable to access the content of the file itself despite being able to filter to find the associated object id.",
        "Question_gpt_summary":"user encount challeng access store file mongodb unabl access content file despit abl filter associ object",
        "Answer_gpt_summary_original":"the solution to accessing stored files in mongodb is to use \"gridfs,\" which splits up files into chunks and stores them in a collection called \"fs.files.\" a tutorial on how to access these files can be found at http:\/\/api.mongodb.com\/python\/current\/examples\/gridfs.html.",
        "Answer_gpt_summary":"solut access store file mongodb us gridf split file chunk store collect call file tutori access file http api mongodb com python current exampl gridf html"
    },
    {
        "Question_id":null,
        "Question_title":"Executing pipeline in AML from Logic Apps stopped working",
        "Question_body":"Hello! I have a few logic apps for my company that trigger ML pipelines at specific time intervals. I followed the documentation on how to set up a logic app and trigger pipeline to the letter and for the past 2 months everything was working fine and my logic apps were able to trigger the ML pipelines with no issues. However, on 12\/08\/2021 at exactly in between 1:30PM - 2:30PM CST, every single pipeline starting failing and they continue to do so up until now. I noticed that we are now receiving this error on every execution:\n\n\"UserError: Response status code does not indicate success: 400 (User starting the run is not an owner or assigned user to the Compute Instance). User starting the run is not an owner or assigned user to the Compute Instance\"\n\nMy Logic apps are setup with \"Managed Identities\" of Owners (like the documentation explains). My last successful run for all the logic apps was on 12\/08 before 1:30PM CST. Did something change on both Azure Logic Apps and Azure ML that is now causing this issue? Any help is greatly appreciated as this is impacting my company's business.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1639583614587,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"I ran into this same issue in a slightly different context. I didn't manage to figure out the root cause but managed to resolve it in practice by standing up a Compute Cluster instead of a Compute Instance (see https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-cluster?tabs=python)",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/664994\/executing-pipeline-in-aml-from-logic-apps-stopped.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-16T21:36:31.763Z",
                "Answer_score":0,
                "Answer_body":"I ran into this same issue in a slightly different context. I didn't manage to figure out the root cause but managed to resolve it in practice by standing up a Compute Cluster instead of a Compute Instance (see https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-cluster?tabs=python)",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":31.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639690591763,
        "Question_original_content":"execut pipelin aml logic app stop work hello logic app compani trigger pipelin specif time interv follow document set logic app trigger pipelin letter past month work fine logic app abl trigger pipelin issu exactli cst singl pipelin start fail continu notic receiv error execut usererror respons statu code indic success user start run owner assign user comput instanc user start run owner assign user comput instanc logic app setup manag ident owner like document explain success run logic app cst chang azur logic app caus issu help greatli appreci impact compani busi",
        "Question_preprocessed_content":"execut pipelin aml logic app stop work hello logic app compani trigger pipelin specif time interv follow document set logic app trigger pipelin letter past month work fine logic app abl trigger pipelin issu exactli cst singl pipelin start fail continu notic receiv error execut usererror respons statu code indic success user start run owner assign user comput instanc logic app setup manag ident owner success run logic app cst chang azur logic app caus issu help greatli appreci impact compani busi",
        "Answer_original_content":"ran issu slightli differ context manag figur root caus manag resolv practic stand comput cluster instead comput instanc http doc microsoft com azur machin learn creat attach comput cluster tab python",
        "Answer_preprocessed_content":"ran issu slightli differ context manag figur root caus manag resolv practic stand comput cluster instead comput instanc",
        "Question_gpt_summary_original":"the user encountered a challenge where their logic apps stopped working and began receiving an error message stating \"user starting the run is not an owner or assigned user to the compute instance\" after a successful run on 12\/08 before 1:30pm cst.",
        "Question_gpt_summary":"user encount challeng logic app stop work began receiv error messag state user start run owner assign user comput instanc success run cst",
        "Answer_gpt_summary_original":"the possible solution to the challenge of encountering an error message stating \"user starting the run is not an owner or assigned user to the compute instance\" after a successful run on 12\/08 before 1:30 pm cst is to stand up a compute cluster instead of a compute instance. the answer suggests referring to the documentation on how to create and attach a compute cluster.",
        "Answer_gpt_summary":"possibl solut challeng encount error messag state user start run owner assign user comput instanc success run cst stand comput cluster instead comput instanc answer suggest refer document creat attach comput cluster"
    },
    {
        "Question_id":52360540.0,
        "Question_title":"Azure-ML: How to deploy\/setup my own ML algorithms on Azure platform as pay-to-use SaaS WS API?",
        "Question_body":"<p>Title sums it up. In more detail, I'd like to offer my own proprietary closed-source ML algorithms to MS Azure customers as pay-to-use SaaS through WS API. E.g. sales volumes forecast algorithm SaaS listed on Azure Marketplace licensed monthly or annually. I could not find any info on the web on how to do that on Azure through its native support for SaaS if any.<\/p>\n\n<p>Please keep in mind that this is not licensing software solutions, but rather licensing usage of my ML API. Thus I'd not have VM images to offer on the marketplace. Machine images are great for software vendors offering full software solutions like ERP, Accounting, etc . E.g. an accounting software  vendor sets up a server image with their software fully installed and configured for customers to license as turn-key, ready to be cloned and used. <\/p>\n\n<p>Mine is not software package but SaaS - think of it as weather forecasting api you subscribe to, integrate your systems with it and get weather forecast information. One does not need to buy the entire VM image to get weather forecast through a web api.<\/p>\n\n<p>So, does Azure natively support what I'm trying to do is my question? I.e. Setting up and offering my own ML-algos as SaaS to customers as VM-image-free pay-to-use pure-WS-solution. It seems that Azure platform only offers machine images on its marketplace, while I'm looking to have them support and list only my own WS there. Hopefully, I have missed that option somewhere.<\/p>\n\n<p>Thank you in advance for your help.<\/p>\n\n<p>P.S. MS Cognitive Services are MS-only algos and no 3rd party algos allowed.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1537156477417,
        "Question_favorite_count":null,
        "Question_last_edit_time":1537161427483,
        "Question_score":0.0,
        "Question_view_count":110.0,
        "Answer_body":"<p>I'd like to answer my own question - there's support for this in the platform, which is great. Place to start <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/marketplace\/marketplace-saas-applications-technical-publishing-guide\" rel=\"nofollow noreferrer\">SaaS applications Offer Publishing Guide<\/a><\/p>\n\n<p>P.S. Hopefully, they'll make it even more integrated with the payment\/licensing system - e.g. for certain prod license types limit the max number of simultaneous WS calls and max CPU cores allowed, and implement enhanced trial licenses time\/functional limits support.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52360540",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1537319682408,
        "Question_original_content":"azur deploi setup algorithm azur platform pai us saa api titl sum like offer proprietari close sourc algorithm azur custom pai us saa api sale volum forecast algorithm saa list azur marketplac licens monthli annual info web azur nativ support saa mind licens softwar solut licens usag api imag offer marketplac machin imag great softwar vendor offer softwar solut like erp account account softwar vendor set server imag softwar fulli instal configur custom licens turn kei readi clone softwar packag saa think weather forecast api subscrib integr system weather forecast inform need bui entir imag weather forecast web api azur nativ support try question set offer algo saa custom imag free pai us pure solut azur platform offer machin imag marketplac look support list hopefulli miss option thank advanc help cognit servic algo parti algo allow",
        "Question_preprocessed_content":"algorithm azur platform saa api titl sum like offer proprietari algorithm azur custom saa api sale volum forecast algorithm saa list azur marketplac licens monthli annual info web azur nativ support saa mind licens softwar solut licens usag api imag offer marketplac machin imag great softwar vendor offer softwar solut like erp account account softwar vendor set server imag softwar fulli instal configur custom licens readi clone softwar packag saa think weather forecast api subscrib integr system weather forecast inform need bui entir imag weather forecast web api azur nativ support try question set offer saa custom azur platform offer machin imag marketplac look support list hopefulli miss option thank advanc help cognit servic algo parti algo allow",
        "Answer_original_content":"like answer question support platform great place start saa applic offer publish guid hopefulli integr payment licens certain prod licens type limit max number simultan call max cpu core allow implement enhanc trial licens time function limit support",
        "Answer_preprocessed_content":"like answer question support platform great place start saa applic offer publish guid hopefulli integr certain prod licens type limit max number simultan call max cpu core allow implement enhanc trial licens limit support",
        "Question_gpt_summary_original":"the user is looking for a way to deploy and setup their own proprietary closed-source ml algorithms on the azure platform as a pay-to-use saas ws api.",
        "Question_gpt_summary":"user look wai deploi setup proprietari close sourc algorithm azur platform pai us saa api",
        "Answer_gpt_summary_original":"possible solutions from the answer include: \n\n- the azure platform supports deploying and setting up proprietary closed-source ml algorithms as a pay-to-use saas ws api.\n- the user can start by checking out the publishing guide for saas applications.\n- the user hopes that the payment\/licensing system will be more integrated in the future, with features such as limiting the number of simultaneous ws calls and cpu cores allowed for certain production license types, and offering enhanced trial licenses with time or functional limits.",
        "Answer_gpt_summary":"possibl solut answer includ azur platform support deploi set proprietari close sourc algorithm pai us saa api user start check publish guid saa applic user hope payment licens integr futur featur limit number simultan call cpu core allow certain product licens type offer enhanc trial licens time function limit"
    },
    {
        "Question_id":68258003.0,
        "Question_title":"How does one make AWS SageMaker available to people outside their organization?",
        "Question_body":"<p>I am setting up SageMaker for a group outside my organization, and wondering how I provide access to SageMaker Studio (and any dependent AWS resources SageMaker would use) to these users.\nThe idea is to have then use SageMaker studio to do train and test models.<\/p>\n<p>Ideally, it would be great if I simply send them a link where they sign in, and are then granted access to SageMaker hosted from my AWS account.<\/p>\n<p>I believe to grant temporary access I need to attach a <strong>role<\/strong> to a group of <strong>users<\/strong>, and also attach a <strong>policy<\/strong> to this group, them finally associate this role with SageMaker. But how do I then create a <strong>link<\/strong> to SageMaker so these users can sign in?<\/p>\n<p>The other option would be to set this up using cross-account access, since those I want to give access to also have their own AWS account. But again, how does one generate a link to direct these users to the SageMaker on my AWS account?<\/p>\n<p>There appears to be 2 options, as per <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-onboard.html\" rel=\"nofollow noreferrer\">SageMaker Onboarding<\/a>:<\/p>\n<ol>\n<li><strong>AWS SSO authentication<\/strong><\/li>\n<\/ol>\n<ul>\n<li>access to Studio via unique sign-in URL that directly opens Studio<\/li>\n<li>sign in with their SSO credentials<\/li>\n<li>organizations manages members in AWS SSO instead of Studio<\/li>\n<li>can assign multiple members access to Studio at the same time<\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>IAM authentication<\/strong><\/li>\n<\/ol>\n<ul>\n<li>sign in through the SageMaker console<\/li>\n<li>must add and manage members manually one at time using the Studio Control Panel<\/li>\n<\/ul>\n<p>I don't understand the 2nd approach, since wouldn't this mean users would have to sign-in as root to the console anyway and then have full access. I could edit the policy attached to IAM users, but this begs another question:<\/p>\n<p>...does SageMaker come pre-baked with policies that include its dependent AWS services? For example, SageMaker will use S3 for storage and EC2 for processing; do I need to set these individually in the policy, or can I simply use a SageMaker policy, which will include all those dependencies by default?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625496004123,
        "Question_favorite_count":null,
        "Question_last_edit_time":1625502032280,
        "Question_score":1.0,
        "Question_view_count":1359.0,
        "Answer_body":"<p>First of all, you can generate &quot;disposable&quot; (called pre-signed) URLs which can be used for accessing SageMaker Studio User Profiles without any AWS credentials. These URls can be valid for max 5 minutes and can be generated with a single AWS API call.<\/p>\n<p>One approach to provide Studio access to your users is to set up a service which can authenticate your Studio users and then calls the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePresignedDomainUrl.html\" rel=\"nofollow noreferrer\">CreatePresignedDomainUrl SageMaker API<\/a> method to sends back the generated pre-signed URL to the user.<\/p>\n<p>Alternatively, you can use AWS SSO as well, which can do most of the heavy lifting for you, especially if you'd like to integrate with a single sign-on service. AWS SSO integrates with SageMaker Studio and you can assign Studio user profiles to your onboarded users. Your users then can go through your single sign-on service and can launch the Studio without logging into the AWS Console.<\/p>\n<p>An another approach is to use IAM Federation where you basically provide access to the AWS API and\/or to the Console to your users which authenticated by an (external) identity provider. Federated users can assume specific roles to operate with the AWS API or the Management Console. For accessing SageMaker Studio, users just need to have the CreatePresignedDomainUrl access policy which allows them to create the pre-signed URL by themselves. If you want to isolate your SageMaker user profiles and ensure each federated user can access just those user profiles which are assigned to them, please see the following <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/configuring-amazon-sagemaker-studio-for-teams-and-groups-with-complete-resource-isolation\/\" rel=\"nofollow noreferrer\">blog post<\/a> for more information.<\/p>\n<p>And finally, please note that, once the user has logged in to Studio, the Execution Role configured for the specific user profile will determine what the Studio user can access and is able to do (e.g. spinning up SageMaker training jobs, deploying models, accessing S3, etc). Thus, you don't need to set up these policies for your IAM users or roles used by the federated users.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68258003",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1626467994683,
        "Question_original_content":"avail peopl outsid organ set group outsid organ wonder provid access studio depend aw resourc us user idea us studio train test model ideal great simpli send link sign grant access host aw account believ grant temporari access need attach role group user attach polici group final associ role creat link user sign option set cross account access want access aw account gener link direct user aw account appear option onboard aw sso authent access studio uniqu sign url directli open studio sign sso credenti organ manag member aw sso instead studio assign multipl member access studio time iam authent sign consol add manag member manual time studio control panel understand approach wouldn mean user sign root consol access edit polici attach iam user beg question come pre bake polici includ depend aw servic exampl us storag process need set individu polici simpli us polici includ depend default",
        "Question_preprocessed_content":"avail peopl outsid organ set group outsid organ wonder provid access studio user idea us studio train test model ideal great simpli send link sign grant access host aw account believ grant temporari access need attach role group user attach polici group final associ role creat link user sign option set access want access aw account gener link direct user aw account appear option onboard aw sso authent access studio uniqu url directli open studio sign sso credenti organ manag member aw sso instead studio assign multipl member access studio time iam authent sign consol add manag member manual time studio control panel understand approach wouldn mean user root consol access edit polici attach iam user beg question come polici includ depend aw servic exampl us storag process need set individu polici simpli us polici includ depend default",
        "Answer_original_content":"gener dispos call pre sign url access studio user profil aw credenti url valid max minut gener singl aw api approach provid studio access user set servic authent studio user call createpresigneddomainurl api method send gener pre sign url user altern us aw sso heavi lift especi like integr singl sign servic aw sso integr studio assign studio user profil onboard user user singl sign servic launch studio log aw consol approach us iam feder basic provid access aw api consol user authent extern ident provid feder user assum specif role oper aw api manag consol access studio user need createpresigneddomainurl access polici allow creat pre sign url want isol user profil ensur feder user access user profil assign follow blog post inform final note user log studio execut role configur specif user profil determin studio user access abl spin train job deploi model access need set polici iam user role feder user",
        "Answer_preprocessed_content":"gener dispos url access studio user profil aw credenti url valid max minut gener singl aw api approach provid studio access user set servic authent studio user call createpresigneddomainurl api method send gener url user altern us aw sso heavi lift especi like integr singl servic aw sso integr studio assign studio user profil onboard user user singl servic launch studio log aw consol approach us iam feder basic provid access aw api consol user authent ident provid feder user assum specif role oper aw api manag consol access studio user need createpresigneddomainurl access polici allow creat url want isol user profil ensur feder user access user profil assign follow blog post inform final note user log studio execut role configur specif user profil determin studio user access abl need set polici iam user role feder user",
        "Question_gpt_summary_original":"the user is facing challenges in setting up aws studio for a group outside their organization and providing access to the hosted service from their aws account, and is unsure of the best approach to generate a link for users to sign in and the policies that need to be set up.",
        "Question_gpt_summary":"user face challeng set aw studio group outsid organ provid access host servic aw account unsur best approach gener link user sign polici need set",
        "Answer_gpt_summary_original":"possible solutions for providing access to aws studio for a group outside the organization include generating pre-signed urls, setting up a service to authenticate users and call the createpresigneddomainurl api method, using aws sso to integrate with a single sign-on service, and using iam federation to provide access to the aws api and\/or console. once a user logs in to studio, the execution role configured for their user profile determines their access and capabilities.",
        "Answer_gpt_summary":"possibl solut provid access aw studio group outsid organ includ gener pre sign url set servic authent user createpresigneddomainurl api method aw sso integr singl sign servic iam feder provid access aw api consol user log studio execut role configur user profil determin access capabl"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML Datastore\\Datasets",
        "Question_body":"Hello:\n\nI want to know that if it is possible automate copy file from azure storage to Azure ML folder.\n\nI understand that it is duplication of data, but I want to know if yes, how I can do that.\n\nAny pointer is greatly appreciated.\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1626270339670,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Depending on the frequency at which you would like to move data you can create scripts that could run on crontab to move the data between source storage account to your workspace blob store. For example, use azcopy to perform this activity.\n\nA very comprehensive method to move storage between storage accounts is available as a Microsoft learn module that you could take to understand the possibilities and attain this from code to automate in your application.\n\nI would ideally assume that you would like to pull data when your experiment kicks off because you cannot move data to an experiments run id folder unless the experiment has started, In this case you could use the first option to place the data in your workspace blob store and then use it in your experiment without moving it to any other storage. I hope this helps.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/475768\/azure-ml-datastoredatasets.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-16T11:50:11.017Z",
                "Answer_score":1,
                "Answer_body":"Depending on the frequency at which you would like to move data you can create scripts that could run on crontab to move the data between source storage account to your workspace blob store. For example, use azcopy to perform this activity.\n\nA very comprehensive method to move storage between storage accounts is available as a Microsoft learn module that you could take to understand the possibilities and attain this from code to automate in your application.\n\nI would ideally assume that you would like to pull data when your experiment kicks off because you cannot move data to an experiments run id folder unless the experiment has started, In this case you could use the first option to place the data in your workspace blob store and then use it in your experiment without moving it to any other storage. I hope this helps.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1626436211016,
        "Question_original_content":"datastor dataset hello want know possibl autom copi file azur storag folder understand duplic data want know ye pointer greatli appreci thank",
        "Question_preprocessed_content":" hello want know possibl autom copi file azur storag folder understand duplic data want know ye pointer greatli appreci thank",
        "Answer_original_content":"depend frequenc like data creat script run crontab data sourc storag account workspac blob store exampl us azcopi perform activ comprehens method storag storag account avail microsoft learn modul understand possibl attain code autom applic ideal assum like pull data experi kick data experi run folder experi start case us option place data workspac blob store us experi move storag hope help",
        "Answer_preprocessed_content":"depend frequenc like data creat script run crontab data sourc storag account workspac blob store exampl us azcopi perform activ comprehens method storag storag account avail microsoft learn modul understand possibl attain code autom applic ideal assum like pull data experi kick data experi run folder experi start case us option place data workspac blob store us experi move storag hope help",
        "Question_gpt_summary_original":"the user is looking to automate the process of copying files from azure storage to a folder, and is seeking pointers on how to do so.",
        "Question_gpt_summary":"user look autom process copi file azur storag folder seek pointer",
        "Answer_gpt_summary_original":"possible solutions to automate the process of copying files from azure storage to a folder include creating scripts that could run on crontab to move the data between source storage account to your workspace blob store using azcopy, and taking a microsoft learn module to understand the possibilities and attain this from code to automate in your application. it is also suggested to place the data in your workspace blob store and then use it in your experiment without moving it to any other storage.",
        "Answer_gpt_summary":"possibl solut autom process copi file azur storag folder includ creat script run crontab data sourc storag account workspac blob store azcopi take microsoft learn modul understand possibl attain code autom applic suggest place data workspac blob store us experi move storag"
    },
    {
        "Question_id":56255154.0,
        "Question_title":"How to use a pretrained model from s3 to predict some data?",
        "Question_body":"<p>I have trained a semantic segmentation model using the sagemaker and the out has been saved to a s3 bucket. I want to load this model from the s3 to predict some images in sagemaker. <\/p>\n\n<p>I know how to predict if I leave the notebook instance running after the training as its just an easy deploy but doesn't really help if I want to use an older model.<\/p>\n\n<p>I have looked at these sources and been able to come up with something myself but it doesn't work hence me being here:<\/p>\n\n<p><a href=\"https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker\" rel=\"noreferrer\">https:\/\/course.fast.ai\/deployment_amzn_sagemaker.html#deploy-to-sagemaker<\/a>\n<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a><\/p>\n\n<p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/pipeline.html<\/a><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb<\/a><\/p>\n\n<p>My code is this:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.pipeline import PipelineModel\nfrom sagemaker.model import Model\n\ns3_model_bucket = 'bucket'\ns3_model_key_prefix = 'prefix'\ndata = 's3:\/\/{}\/{}\/{}'.format(s3_model_bucket, s3_model_key_prefix, 'model.tar.gz')\nmodels = ss_model.create_model() # ss_model is my sagemaker.estimator\n\nmodel = PipelineModel(name=data, role=role, models= [models])\nss_predictor = model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1558522248223,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1558524094856,
        "Question_score":6.0,
        "Question_view_count":7404.0,
        "Answer_body":"<p>You can actually instantiate a Python SDK <code>model<\/code> object from existing artifacts, and deploy it to an endpoint. This allows you to deploy a model from trained artifacts, without having to retrain in the notebook. For example, for the semantic segmentation model:<\/p>\n\n<pre><code>trainedmodel = sagemaker.model.Model(\n    model_data='s3:\/\/...model path here..\/model.tar.gz',\n    image='685385470294.dkr.ecr.eu-west-1.amazonaws.com\/semantic-segmentation:latest',  # example path for the semantic segmentation in eu-west-1\n    role=role)  # your role here; could be different name\n\ntrainedmodel.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge')\n<\/code><\/pre>\n\n<p>And similarly, you can instantiate a predictor object on a deployed endpoint from any authenticated client supporting the SDK, with the following command:<\/p>\n\n<pre><code>predictor = sagemaker.predictor.RealTimePredictor(\n    endpoint='endpoint name here',\n    content_type='image\/jpeg',\n    accept='image\/png')\n<\/code><\/pre>\n\n<p>More on those abstractions:<\/p>\n\n<ul>\n<li><code>Model<\/code>: <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/model.html<\/a><\/li>\n<li><code>Predictor<\/code>:\n<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html\" rel=\"noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/predictors.html<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":13.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56255154",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1558621559712,
        "Question_original_content":"us pretrain model predict data train semant segment model save bucket want load model predict imag know predict leav notebook instanc run train easi deploi help want us older model look sourc abl come work http cours fast deploy amzn html deploi http aw amazon com get start tutori build train deploi machin learn model http readthedoc stabl pipelin html http github com awslab amazon exampl blob master advanc function infer pipelin sparkml xgboost abalon infer pipelin sparkml xgboost abalon ipynb code pipelin import pipelinemodel model import model model bucket bucket model kei prefix prefix data format model bucket model kei prefix model tar model model creat model model estim model pipelinemodel data role role model model predictor model deploi initi instanc count instanc type xlarg",
        "Question_preprocessed_content":"us pretrain model predict data train semant segment model save bucket want load model predict imag know predict leav notebook instanc run train easi deploi help want us older model look sourc abl come work code",
        "Answer_original_content":"actual instanti python sdk model object exist artifact deploi endpoint allow deploi model train artifact have retrain notebook exampl semant segment model trainedmodel model model model data model path model tar imag dkr ecr west amazonaw com semant segment latest exampl path semant segment west role role role differ trainedmodel deploi initi instanc count instanc type xlarg similarli instanti predictor object deploi endpoint authent client support sdk follow command predictor predictor realtimepredictor endpoint endpoint content type imag jpeg accept imag png abstract model http readthedoc stabl model html predictor http readthedoc stabl predictor html",
        "Answer_preprocessed_content":"actual instanti python sdk object exist artifact deploi endpoint allow deploi model train artifact have retrain notebook exampl semant segment model similarli instanti predictor object deploi endpoint authent client support sdk follow command abstract",
        "Question_gpt_summary_original":"the user is facing challenges in using a pretrained model from an s3 bucket to predict some data.",
        "Question_gpt_summary":"user face challeng pretrain model bucket predict data",
        "Answer_gpt_summary_original":"the solution to the challenge of using a pretrained model from an s3 bucket to predict some data is to instantiate a python sdk model object from existing artifacts and deploy it to an endpoint. this allows you to deploy a model from trained artifacts without having to retrain in the notebook. you can also instantiate a predictor object on a deployed endpoint from any authenticated client supporting the sdk.",
        "Answer_gpt_summary":"solut challeng pretrain model bucket predict data instanti python sdk model object exist artifact deploi endpoint allow deploi model train artifact have retrain notebook instanti predictor object deploi endpoint authent client support sdk"
    },
    {
        "Question_id":null,
        "Question_title":"C# handwritten text detection using google.cloud.Vision.v1 Api. why can't detect language?",
        "Question_body":"hello mam\/sir,I was used google.cloud.Visiion.v1 for handwritten text recognition for Indian languages .  this code is work but only for Marathi, Hindi languages. but other language like Malayalam, Tamil, Kaneda Telegu its not return a 100 percent result.For Example- i have a 12 months handwritten name(Malayalam, Kaneda etc.) but its recognize only 7 to 8 correct word detection .can you please help me to 100 percent  accurate word detection.please give me a solution .thanks and regards,Bhagyashri ",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1664755140000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":74.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/C-handwritten-text-detection-using-google-cloud-Vision-v1-Api\/td-p\/473437\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-03T13:10:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi Bjagyashri, I think that you would find a better help on StackOverflow\u00a0since your issue seems to be on your code and people there is specialized on programming."
            },
            {
                "Answer_creation_time":"2022-10-03T23:56:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"okay\n\nthank you for advice."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"handwritten text detect googl cloud vision api detect languag hello mam sir googl cloud visiion handwritten text recognit indian languag code work marathi hindi languag languag like malayalam tamil kaneda telegu return percent result exampl month handwritten malayalam kaneda recogn correct word detect help percent accur word detect solut thank regard bhagyashri",
        "Question_preprocessed_content":"handwritten text detect api detect languag hello handwritten text recognit indian languag code work marathi hindi languag languag like malayalam tamil kaneda telegu return percent exampl month handwritten recogn correct word detect help percent accur word solut thank regard bhagyashri",
        "Answer_original_content":"bjagyashri think better help stackoverflowsinc issu code peopl special program okai thank advic",
        "Answer_preprocessed_content":"bjagyashri think better help stackoverflowsinc issu code peopl special program okai thank advic",
        "Question_gpt_summary_original":"the user is encountering challenges with using google cloud vision v1 api for handwritten text recognition for indian languages, as it is not returning 100% accurate results for languages such as malayalam, tamil, and kannada.",
        "Question_gpt_summary":"user encount challeng googl cloud vision api handwritten text recognit indian languag return accur result languag malayalam tamil kannada",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the responder suggests seeking help on stack overflow as the issue seems to be related to the user's code.",
        "Answer_gpt_summary":"solut provid answer respond suggest seek help stack overflow issu relat user code"
    },
    {
        "Question_id":null,
        "Question_title":"How to get logs or print statements from SageMaker PyTorch deployed endpoint?",
        "Question_body":"I've deployed an extended Pytorch model as an endpoint and I'm trying to make inference requests to it. Problem is, the responses from the endpoint get timed out and CloudWatch logs show nothing beyond:\n\ntimestamp\tmessage\n1661544743589\tWARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n1661544749569\tModel server started.\n\nNow in my inference.py file, which I provided as the entry point I've set logging as follows:\n\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\nlogger.info(\"Loading file.\")\nprint(\"Loading file.\")\n\n\nI wish to see those logs\/prints. How can I accomplish that?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1661549640227,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":48.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU74MThjkyRVCtySw-DEozrQ\/how-to-get-logs-or-print-statements-from-sage-maker-py-torch-deployed-endpoint",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-30T16:47:09.082Z",
                "Answer_score":0,
                "Answer_body":"I've come across similar issues in the past of log messages not making it through to CloudWatch, and can suggest setting environment variable PYTHONUNBUFFERED=1 (discussed further here on StackOverflow wrt containerized Python in general).\n\nThe procedure for this may vary a little depending how you're creating your model, endpoint config & endpoint (e.g. direct boto3\/API calls, SageMaker SDK Estimator.deploy() or PyTorchModel). PyTorchModel should accept an env={\"PYTHONUNBUFFERED\": \"1\"} constructor argument for example.\n\nIf you are using the SageMaker Python SDK, do watch out that some methods (especially shortcuts like Estimator.deploy()) may re-use existing models & endpoint configs rather than re-creating each time they're run. Check you see the environment variable set in the SageMaker > Inference > Models > {Your Model Name} details page in AWS Console, and run DeleteModel first if needed to force an update!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-30T16:30:29.746Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nPlease note that you can view the logs under CloudWatch logs. There would be a generated log stream under your pytorch-inference tab when it is inService (Navigate to inference tab under SageMaker console > Endpoints > click on the endpoint name > locate \"view logs\"). The previous steps will take you to CloudWatch logs console. You then click on log groups and locate \/aws\/sagemaker\/Endpoints\/pytorch-inference-YYYY-MM-DD-HH-MM-SS-sss > AllTraffic\/i-instanceId. For example, for the code snippet you shared, If you add the lines of code below for logging purpose within the inference.py script):\n\nimport logging\nimport sys\n\nlogger = logging.getLogger(__name__)\nlogger.setLevel(logging.DEBUG)\nlogger.addHandler(logging.StreamHandler(sys.stdout))\nlogger.info(\"Loading file.\")\nprint(\"Loading file. --> from print statement\")\n\n# rest of the inference script from here\n\n\nThe above will show up under AllTraffic\/instance-id (once the endpoint is inService) as the following:\n\n2022-08-30 15:48:42,183 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Loading file.\n2022-08-30 15:48:42,936 [INFO ] W-9000-model-stdout com.amazonaws.ml.mms.wlm.WorkerLifeCycle - Loading file. --> from print statement\n\n\nFrom above, you can see that print statements will show up as INFO level log.\n\nPlease see the link [1] for more details on how the log stream is captured for inference jobs. If you have logging statements within your input_fn or say within your predict_fn function, those statements will show up when there is a prediction\/scoring made. I hope the shared information is helpful.\n\nReference: [1] https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-logs-metrics.html#inference-pipeline-logs\n\nplease see @Alex_T answer for PYTHONUNBUFFERED=1 which forces print\/logging to stdout. If you are bringing your own container you can define this parameter in the Dockerfile.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"log print statement pytorch deploi endpoint deploi extend pytorch model endpoint try infer request problem respons endpoint time cloudwatch log timestamp messag warn sun reflect reflect getcallerclass support impact perform model server start infer file provid entri point set log follow import log import sy logger log getlogg logger setlevel log debug logger addhandl log streamhandl sy stdout logger info load file print load file wish log print accomplish",
        "Question_preprocessed_content":"log print statement pytorch deploi endpoint deploi extend pytorch model endpoint try infer request problem respons endpoint time cloudwatch log timestamp messag warn support impact perform model server start file provid entri point set log follow import log import sy logger print wish accomplish",
        "Answer_original_content":"come similar issu past log messag make cloudwatch suggest set environ variabl pythonunbuff discuss stackoverflow wrt container python gener procedur vari littl depend creat model endpoint config endpoint direct boto api call sdk estim deploi pytorchmodel pytorchmodel accept env pythonunbuff constructor argument exampl python sdk watch method especi shortcut like estim deploi us exist model endpoint config creat time run check environ variabl set infer model model detail page aw consol run deletemodel need forc updat hello note view log cloudwatch log gener log stream pytorch infer tab inservic navig infer tab consol endpoint click endpoint locat view log previou step cloudwatch log consol click log group locat aw endpoint pytorch infer alltraff instanceid exampl code snippet share add line code log purpos infer script import log import sy logger log getlogg logger setlevel log debug logger addhandl log streamhandl sy stdout logger info load file print load file print statement rest infer script alltraff instanc endpoint inservic follow info model stdout com amazonaw mm wlm workerlifecycl load file info model stdout com amazonaw mm wlm workerlifecycl load file print statement print statement info level log link detail log stream captur infer job log statement input predict function statement predict score hope share inform help refer http doc aw amazon com latest infer pipelin log metric html infer pipelin log alex answer pythonunbuff forc print log stdout bring contain defin paramet dockerfil",
        "Answer_preprocessed_content":"come similar issu past log messag make cloudwatch suggest set environ variabl pythonunbuff procedur vari littl depend creat model endpoint config endpoint pytorchmodel pytorchmodel accept env pythonunbuff constructor argument exampl python sdk watch method exist model endpoint config time run check environ variabl set infer model detail page aw consol run deletemodel need forc updat hello note view log cloudwatch log gener log stream tab inservic previou step cloudwatch log consol click log group locat exampl code snippet share add line code log purpos script import log import sy logger print rest infer script follow load file load file print statement print statement info level log link detail log stream captur infer job log statement function statement hope share inform help refer answer pythonunbuff forc stdout bring contain defin paramet dockerfil",
        "Question_gpt_summary_original":"the user is encountering challenges in trying to get logs or print statements from a pytorch deployed endpoint, and is unable to see the logs\/prints despite setting up logging in the inference.py file.",
        "Question_gpt_summary":"user encount challeng try log print statement pytorch deploi endpoint unabl log print despit set log infer file",
        "Answer_gpt_summary_original":"possible solutions to the challenge of not being able to see logs or print statements from a pytorch deployed endpoint include setting the environment variable pythonunbuffered=1, checking that the environment variable is set in the inference > models > {your model name} details page in aws console, and viewing the logs under cloudwatch logs. additionally, adding logging statements within the input_fn or predict_fn function can help show logs when there is a prediction\/scoring made. the link provided in the answer gives more details on how the log stream is captured for inference jobs.",
        "Answer_gpt_summary":"possibl solut challeng abl log print statement pytorch deploi endpoint includ set environ variabl pythonunbuff check environ variabl set infer model model detail page aw consol view log cloudwatch log addition ad log statement input predict function help log predict score link provid answer give detail log stream captur infer job"
    },
    {
        "Question_id":null,
        "Question_title":"AML - AssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available.",
        "Question_body":"Hello Microsoft Q&A Team,\n\nI get the error\n\nAssetException: Error with code: Can't connect to HTTPS URL because the SSL module is not available\n\nwhen executing the following command:\n\npipeline_job = ml_client.jobs.create_or_update(\npipeline_job, experiment_name=\"data_preparation\"\n)\npipeline_job\n\nYesterday the command worked without an error. I did not make any changes. So I have no idea, what the problem is.\n\nThanks for helping me out.\n\nCheers\n\nLukas",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1667553245710,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@Lukas-6968 Thanks for your question. Can you please add more details about the document\/sample that you are trying.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1075753\/aml-assetexception-error-with-code-can39t-connect.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-04T23:41:10.037Z",
                "Answer_score":0,
                "Answer_body":"@Lukas-6968 Thanks for your question. Can you please add more details about the document\/sample that you are trying.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-11-07T09:10:33.28Z",
                "Answer_score":1,
                "Answer_body":"Hello,\n\nI was able to solve the issue.\n\nThank you.\n\nCheers\n\nLukas",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1667605270036,
        "Question_original_content":"aml assetexcept error code connect http url ssl modul avail hello microsoft team error assetexcept error code connect http url ssl modul avail execut follow command pipelin job client job creat updat pipelin job experi data prepar pipelin job yesterdai command work error chang idea problem thank help cheer luka",
        "Question_preprocessed_content":"aml assetexcept error code connect http url ssl modul avail hello microsoft team error assetexcept error code connect http url ssl modul avail execut follow command yesterdai command work error chang idea problem thank help cheer luka",
        "Answer_original_content":"luka thank question add detail document sampl try",
        "Answer_preprocessed_content":"thank question add detail try",
        "Question_gpt_summary_original":"the user is encountering an assetexception error when executing a command, despite the fact that it worked without an error the day before.",
        "Question_gpt_summary":"user encount assetexcept error execut command despit fact work error dai",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the responder is asking for more details about the document\/sample that the user is trying to execute in order to better understand the issue and provide a solution.",
        "Answer_gpt_summary":"solut provid answer respond ask detail document sampl user try execut order better understand issu provid solut"
    },
    {
        "Question_id":70287087.0,
        "Question_title":"How to deploy sagemaker.workflow.pipeline.Pipeline?",
        "Question_body":"<p>I have a <code>sagemaker.workflow.pipeline.Pipeline<\/code> which contains multiple <code>sagemaker.workflow.steps.ProcessingStep<\/code> and each <code>ProcessingStep<\/code> contains <code>sagemaker.processing.ScriptProcessor<\/code>.<\/p>\n<p>The current pipeline graph look like the below shown image. It will take data from multiple sources from S3, process it and create a final dataset using the data from previous steps.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/6XImq.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6XImq.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>As the <code>Pipeline<\/code> object doesn't support <code>.deploy<\/code> method, how to deploy this pipeline?<\/p>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<p>or Sagemaker Pipeline is designed for only data processing and model training on huge\/batch data? Not for the inference with the single data point?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1639040387083,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":313.0,
        "Answer_body":"<blockquote>\n<p>As the Pipeline object doesn't support .deploy method, how to deploy this pipeline?<\/p>\n<\/blockquote>\n<p>Pipeline does not have a <code>.deploy()<\/code> method, no<\/p>\n<p>Use <code>pipeline.upsert(role_arn='...')<\/code> to create\/update the pipeline definition to SageMaker, then call <code>pipeline.start()<\/code> . Docs <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#pipeline\" rel=\"nofollow noreferrer\">here<\/a><\/p>\n<blockquote>\n<p>While inference\/scoring, When we receive a raw data(single row for each source), how to trigger the pipeline?<\/p>\n<\/blockquote>\n<p>There are actually two types of pipelines in SageMaker. Model Building Pipelines (which you have in your question), and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>, which are used for Inference. AWS definitely should have called the former &quot;workflows&quot;<\/p>\n<p>You can use a model building pipeline to setup a serial inference pipeline<\/p>\n<p>To do pre-processing in a serial inference pipeline, you want to train an encoder\/estimator (such as SKLearn) and save its model. Then train a learning algorithm, and save its model, then create a <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/pipeline.html\" rel=\"nofollow noreferrer\">PipelineModel<\/a> using both models<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70287087",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639073178300,
        "Question_original_content":"deploi workflow pipelin pipelin workflow pipelin pipelin contain multipl workflow step processingstep processingstep contain process scriptprocessor current pipelin graph look like shown imag data multipl sourc process creat final dataset data previou step pipelin object support deploi method deploi pipelin infer score receiv raw data singl row sourc trigger pipelin pipelin design data process model train huge batch data infer singl data point",
        "Question_preprocessed_content":"deploi contain multipl contain current pipelin graph look like shown imag data multipl sourc process creat final dataset data previou step object support method deploi pipelin receiv raw data trigger pipelin pipelin design data process model train data infer singl data point",
        "Answer_original_content":"pipelin object support deploi method deploi pipelin pipelin deploi method us pipelin upsert role arn creat updat pipelin definit pipelin start doc infer score receiv raw data singl row sourc trigger pipelin actual type pipelin model build pipelin question serial infer pipelin infer aw definit call workflow us model build pipelin setup serial infer pipelin pre process serial infer pipelin want train encod estim sklearn save model train learn algorithm save model creat pipelinemodel model",
        "Answer_preprocessed_content":"pipelin object support deploi method deploi pipelin pipelin method us pipelin definit doc receiv raw data trigger pipelin actual type pipelin model build pipelin serial infer pipelin infer aw definit call workflow us model build pipelin setup serial infer pipelin serial infer pipelin want train save model train learn algorithm save model creat pipelinemodel model",
        "Question_gpt_summary_original":"the user is facing challenges in deploying a .workflow.pipeline.pipeline containing multiple .workflow.steps.processingstep and each processingstep containing .processing.scriptprocessor, as well as triggering the pipeline for inference\/scoring with a single data point.",
        "Question_gpt_summary":"user face challeng deploi workflow pipelin pipelin contain multipl workflow step processingstep processingstep contain process scriptprocessor trigger pipelin infer score singl data point",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- use pipeline.upsert(role_arn='...') to create\/update the pipeline definition, then call pipeline.start() to deploy the pipeline.\n- to trigger the pipeline for inference\/scoring with a single data point, use a model building pipeline to setup a serial inference pipeline to do pre-processing. train an encoder\/estimator and save its model, then train a learning algorithm and save its model, then create a pipelinemodel using both models.",
        "Answer_gpt_summary":"possibl solut answer us pipelin upsert role arn creat updat pipelin definit pipelin start deploi pipelin trigger pipelin infer score singl data point us model build pipelin setup serial infer pipelin pre process train encod estim save model train learn algorithm save model creat pipelinemodel model"
    },
    {
        "Question_id":70258080.0,
        "Question_title":"How to install python packages within Amazon Sagemaker Processing Job?",
        "Question_body":"<p>I am trying to create a Sklearn processing job in Amazon Sagemekar to perform some data transformation of my input data before I do model training.<\/p>\n<p>I wrote a custom python script <code>preprocessing.py<\/code> which does the needful. I use some python package in this script. <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/sagemaker_processing\/scikit_learn_data_processing_and_model_evaluation\/scikit_learn_data_processing_and_model_evaluation.ipynb\" rel=\"nofollow noreferrer\">Here is the Sagemaker example I followed<\/a>.<\/p>\n<p>When I try to submit the Processing Job I get an error -<\/p>\n<pre><code>............................Traceback (most recent call last):\n  File &quot;\/opt\/ml\/processing\/input\/code\/preprocessing.py&quot;, line 6, in &lt;module&gt;\n    import snowflake.connector\nModuleNotFoundError: No module named 'snowflake.connector'\n<\/code><\/pre>\n<p>I understand that my processing job is unable to find this package and I need to install it. My question is how can I accomplish this using Sagemaker Processing Job API? Ideally there should be a way to define a <code>requirements.txt<\/code> in the API call, but I don't see such functionality in the docs.<\/p>\n<p>I know I can <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/processing-container-run-scripts.html\" rel=\"nofollow noreferrer\">create a custom Image with relevant packages<\/a> and later use this image in the Processing Job, but this seems too much work for something that should be built-in?<\/p>\n<p>Is there an easier\/elegant way to install packages needed in Sagemaker Processing Job ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1638871133180,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1638872002100,
        "Question_score":5.0,
        "Question_view_count":2087.0,
        "Answer_body":"<p>One way would be to <a href=\"https:\/\/stackoverflow.com\/questions\/12332975\/installing-python-module-within-code\">call pip from Python<\/a>:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>subprocess.check_call([sys.executable, &quot;-m&quot;, &quot;pip&quot;, &quot;install&quot;, package])\n<\/code><\/pre>\n<p>Another way would be to use an <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/sagemaker.sklearn.html\" rel=\"nofollow noreferrer\">SKLearn Estimator<\/a> (training job) instead, to do the same thing. You can provide the <code>source_dir<\/code>, which can include a <code>requirements.txt<\/code> file, and these requirements will be installed for you<\/p>\n<pre class=\"lang-python prettyprint-override\"><code>estimator = SKLearn(\n    entry_point=&quot;foo.py&quot;,\n    source_dir=&quot;.\/foo&quot;, # no trailing slash! put requirements.txt here\n    framework_version=&quot;0.23-1&quot;,\n    role = ...,\n    instance_count = 1,\n    instance_type = &quot;ml.m5.large&quot;\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1654197994423,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70258080",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1638909512683,
        "Question_original_content":"instal python packag process job try creat sklearn process job amazon sagemekar perform data transform input data model train wrote custom python script preprocess need us python packag script exampl follow try submit process job error traceback recent file opt process input code preprocess line import snowflak connector modulenotfounderror modul name snowflak connector understand process job unabl packag need instal question accomplish process job api ideal wai defin requir txt api function doc know creat custom imag relev packag later us imag process job work built easier eleg wai instal packag need process job",
        "Question_preprocessed_content":"instal python packag process job try creat sklearn process job amazon sagemekar perform data transform input data model train wrote custom python script need us python packag script exampl follow try submit process job error understand process job unabl packag need instal question accomplish process job api ideal wai defin api function doc know creat custom imag relev packag later us imag process job work wai instal packag need process job",
        "Answer_original_content":"wai pip python subprocess check sy execut pip instal packag wai us sklearn estim train job instead thing provid sourc dir includ requir txt file requir instal estim sklearn entri point foo sourc dir foo trail slash requir txt framework version role instanc count instanc type larg",
        "Answer_preprocessed_content":"wai pip python wai us sklearn estim instead thing provid includ file requir instal",
        "Question_gpt_summary_original":"the user is encountering challenges in installing python packages within an  processing job, and is looking for an easier and more elegant way to do so.",
        "Question_gpt_summary":"user encount challeng instal python packag process job look easier eleg wai",
        "Answer_gpt_summary_original":"possible solutions to the challenge of installing python packages within an  processing job are: calling pip from python using subprocess.check_call, or using an sklearn estimator (training job) and providing the source_dir which can include a requirements.txt file, and these requirements will be installed automatically.",
        "Answer_gpt_summary":"possibl solut challeng instal python packag process job call pip python subprocess check sklearn estim train job provid sourc dir includ requir txt file requir instal automat"
    },
    {
        "Question_id":null,
        "Question_title":"OperatorNotAllowedInGraphError: using a `tf.Tensor` as a Python `bool` is not allowed",
        "Question_body":"<p>When I try to run the mnist example I get the following:<\/p>\n<p>OperatorNotAllowedInGraphError: using a <code>tf.Tensor<\/code> as a Python <code>bool<\/code> is not allowed: AutoGraph did convert this function. This might indicate you are trying to use an unsupported feature.<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1627001084083,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":2143.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/operatornotallowedingrapherror-using-a-tf-tensor-as-a-python-bool-is-not-allowed\/739",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-26T18:05:51.830Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/melnimr\">@melnimr<\/a> - this may have been resolved. What version of Guild are you using?<\/p>",
                "Answer_score":11.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-07-27T17:48:05.864Z",
                "Answer_body":"<p>It is 0.7.3<\/p>",
                "Answer_score":11.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-07-27T18:14:00.519Z",
                "Answer_body":"<p>This would have been fixed in 0.7.4, which isn\u2019t generally available. However, you can install a pre-release version this way:<\/p>\n<pre><code class=\"lang-command\">pip install --pre guildai\n<\/code><\/pre>",
                "Answer_score":11.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-21T08:41:24.955Z",
                "Answer_body":"<p>I have the same issue even I execute this command.  Is there a solution or not yet please ?<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-21T13:26:16.832Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/diiib98\">@DiiiB98<\/a> and welcome!<\/p>\n<p>Just to make sure I\u2019m looking at the same example, could you give me a link to what you\u2019re running here?<\/p>\n<p>Thanks!<\/p>",
                "Answer_score":21.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-21T16:00:00.484Z",
                "Answer_body":"<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/976e03124f469e8d8257ae3e1e0b364c332ec0e2.jpeg\" data-download-href=\"\/uploads\/short-url\/lBBNsbWugxXYuJ9NPVLbsZNwW4i.jpeg?dl=1\" title=\"resulat\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/976e03124f469e8d8257ae3e1e0b364c332ec0e2_2_547x500.jpeg\" alt=\"resulat\" data-base62-sha1=\"lBBNsbWugxXYuJ9NPVLbsZNwW4i\" width=\"547\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/976e03124f469e8d8257ae3e1e0b364c332ec0e2_2_547x500.jpeg, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/976e03124f469e8d8257ae3e1e0b364c332ec0e2_2_820x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/976e03124f469e8d8257ae3e1e0b364c332ec0e2_2_1094x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/976e03124f469e8d8257ae3e1e0b364c332ec0e2_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">resulat<\/span><span class=\"informations\">1920\u00d71753 327 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nYou find in this link the files used and this is the result after runninf the gridsearch with this command.<br>\n<a href=\"https:\/\/drive.google.com\/drive\/folders\/19GGoQ08vQDrBKdswh8R6gcSvKTQOd6an?usp=sharing\" rel=\"noopener nofollow ugc\">Files<\/a><br>\nThank you for your answer<\/p>",
                "Answer_score":61.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-18T19:45:51.369Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a> I\u2019m hitting this as well. Is this the same issue: <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/278\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Summary plugins not working with TensorFlow 2 \u00b7 Issue #278 \u00b7 guildai\/guildai \u00b7 GitHub<\/a><\/p>\n<p>if so, what\u2019s the easiest way to disable the plugins?<\/p>",
                "Answer_score":71.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-08T02:25:46.203Z",
                "Answer_body":"<p>I just committed a <a href=\"https:\/\/github.com\/guildai\/guildai\/commit\/5ead7bd931178a6cafb8ad7687486a3319ecd28e\">change<\/a> to address this \u2014 these plugins are not handling the TF 2.2+ API changes atm and so in this case they at least don\u2019t break the operation.<\/p>\n<p>Look for this in the next pre-release (probably tomorrow) or officially in 0.7.5.<\/p>",
                "Answer_score":65.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"operatornotallowedingrapherror tensor python bool allow try run mnist exampl follow operatornotallowedingrapherror tensor python bool allow autograph convert function indic try us unsupport featur",
        "Question_preprocessed_content":"operatornotallowedingrapherror python allow try run mnist exampl follow operatornotallowedingrapherror python allow autograph convert function indic try us unsupport featur",
        "Answer_original_content":"melnimr resolv version fix isnt gener avail instal pre releas version wai pip instal pre issu execut command solut welcom sure look exampl link your run thank resulat link file result runninf gridsearch command file thank answer garrett hit issu summari plugin work tensorflow issu github what easiest wai disabl plugin commit chang address plugin handl api chang atm case dont break oper look pre releas probabl tomorrow offici",
        "Answer_preprocessed_content":"resolv version fix isnt gener avail instal version wai issu execut command solut welcom sure look exampl link your run thank resulat link file result runninf gridsearch command file thank answer hit issu summari plugin work tensorflow issu github what easiest wai disabl plugin commit chang address plugin handl api chang atm case dont break oper look offici",
        "Question_gpt_summary_original":"the user encountered an \"operatornotallowedingrapherror\" when attempting to run the mnist example, indicating they were trying to use an unsupported feature.",
        "Question_gpt_summary":"user encount operatornotallowedingrapherror attempt run mnist exampl indic try us unsupport featur",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer include upgrading to a pre-release version of guildai (0.7.4) or disabling plugins that are not handling the tensorflow 2.2+ api changes. the issue may have been resolved in the pre-release version or will be officially fixed in version 0.7.5.",
        "Answer_gpt_summary":"possibl solut mention answer includ upgrad pre releas version disabl plugin handl tensorflow api chang issu resolv pre releas version offici fix version"
    },
    {
        "Question_id":null,
        "Question_title":"Getting stuck on SageMaker domain creation through the standard setup wizard",
        "Question_body":"I am having difficulty in getting started on SageMaker Studio using the Standard setup wizard of SageMaker Domain creation. The wizard prevents me from moving forward at the RStudio setup step even though RStudio is indicated to be optional and which I don't need or want to pay for. I have tried a lot of things (looked at the videos, searched the web) to no avail. Tried as a root user as well as a IAM user and have the same issue. Thanks for any help.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1650593854106,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":122.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUaiMX9ZU3SdqFnjxiNn_XRg\/getting-stuck-on-sage-maker-domain-creation-through-the-standard-setup-wizard",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-25T22:55:14.358Z",
                "Answer_score":0,
                "Answer_body":"What is the region you are trying this into? Can you share some screenshots to explain the difficulty in moving forward? Does it give an error, does it keep loading? When you are using standard setup, are you using any vpc to launch studio? Knowing the answer of these questions will help provide a better solution to your problem",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-22T09:01:11.976Z",
                "Answer_score":0,
                "Answer_body":"Hi! You'll need to specify a VPC for setting up a SageMaker domain. If you click on the drop down, you'll see a default VPC (in any region) to set up Studio in. The second configuration (public internet or VPC only) is to whether restrict traffic to only the VPC or internet. You don't need an RStudio license to use SageMaker Studio.\n\nSee here for detailed reference - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/onboard-vpc.html",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get stuck domain creation standard setup wizard have difficulti get start studio standard setup wizard domain creation wizard prevent move forward rstudio setup step rstudio indic option need want pai tri lot thing look video search web avail tri root user iam user issu thank help",
        "Question_preprocessed_content":"get stuck domain creation standard setup wizard have difficulti get start studio standard setup wizard domain creation wizard prevent move forward rstudio setup step rstudio indic option need want pai tri lot thing avail tri root user iam user issu thank help",
        "Answer_original_content":"region try share screenshot explain difficulti move forward error load standard setup vpc launch studio know answer question help provid better solut problem need specifi vpc set domain click drop default vpc region set studio second configur public internet vpc restrict traffic vpc internet need rstudio licens us studio detail refer http doc aw amazon com latest onboard vpc html",
        "Answer_preprocessed_content":"region try share screenshot explain difficulti move forward error load standard setup vpc launch studio know answer question help provid better solut problem need specifi vpc set domain click drop default vpc set studio second configur restrict traffic vpc internet need rstudio licens us studio detail refer",
        "Question_gpt_summary_original":"the user is having difficulty creating a domain through the standard setup wizard, as the wizard prevents them from moving forward at the rstudio setup step even though rstudio is indicated to be optional and they do not want to pay for it.",
        "Question_gpt_summary":"user have difficulti creat domain standard setup wizard wizard prevent move forward rstudio setup step rstudio indic option want pai",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n- the user needs to specify a vpc for setting up a domain and select a default vpc from the dropdown menu.\n- the user does not need an rstudio license to use studio.\n- the user can refer to the provided link for detailed reference on setting up a domain in aws.",
        "Answer_gpt_summary":"possibl solut answer user need specifi vpc set domain select default vpc dropdown menu user need rstudio licens us studio user refer provid link detail refer set domain aw"
    },
    {
        "Question_id":58815367.0,
        "Question_title":"How to solve the error with deploying a model in aws sagemaker?",
        "Question_body":"<p>I have to deploy a custom keras model in AWS Sagemaker. I have a created a notebook instance and I have the following files:<\/p>\n\n<pre><code>AmazonSagemaker-Codeset16\n   -ann\n      -nginx.conf\n      -predictor.py\n      -serve\n      -train.py\n      -wsgi.py\n   -Dockerfile\n<\/code><\/pre>\n\n<p>I now open the AWS terminal and build the docker image and push the image in the ECR repository. Then I open a new jupyter python notebook and try to fit the model and deploy the same. The training is done correctly but while deploying I get the following error:<\/p>\n\n<blockquote>\n  <p>\"Error hosting endpoint sagemaker-example-2019-10-25-06-11-22-366: Failed. >Reason: The primary container for production variant AllTraffic did not pass >the ping health check. Please check CloudWatch logs for this endpoint...\"<\/p>\n<\/blockquote>\n\n<p>When I check the logs, I find the following:<\/p>\n\n<blockquote>\n  <p>2019\/11\/11 11:53:32 [crit] 19#19: *3 connect() to unix:\/tmp\/gunicorn.sock >failed (2: No such file or directory) while connecting to upstream, client: >10.32.0.4, server: , request: \"GET \/ping HTTP\/1.1\", upstream: >\"<a href=\"http:\/\/unix:\/tmp\/gunicorn.sock:\/ping\" rel=\"nofollow noreferrer\">http:\/\/unix:\/tmp\/gunicorn.sock:\/ping<\/a>\", host: \"model.aws.local:8080\"<\/p>\n<\/blockquote>\n\n<p>and <\/p>\n\n<blockquote>\n  <p>Traceback (most recent call last):\n   File \"\/usr\/local\/bin\/serve\", line 8, in \n     sys.exit(main())\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/cli\/serve.py\", line 19, in main\n     server.start(env.ServingEnv().framework_module)\n   File \"\/usr\/local\/lib\/python2.7\/dist->packages\/sagemaker_containers\/_server.py\", line 107, in start\n     module_app,\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 711, in <strong>init<\/strong>\n     errread, errwrite)\n   File \"\/usr\/lib\/python2.7\/subprocess.py\", line 1343, in _execute_child\n     raise child_exception<\/p>\n<\/blockquote>\n\n<p>I tried to deploy the same model in AWS Sagemaker with these files in my local computer and the model was deployed successfully but inside AWS, I am facing this problem.<\/p>\n\n<p>Here is my serve file code:<\/p>\n\n<pre><code>from __future__ import print_function\nimport multiprocessing\nimport os\nimport signal\nimport subprocess\nimport sys\n\ncpu_count = multiprocessing.cpu_count()\n\nmodel_server_timeout = os.environ.get('MODEL_SERVER_TIMEOUT', 60)\nmodel_server_workers = int(os.environ.get('MODEL_SERVER_WORKERS', cpu_count))\n\n\ndef sigterm_handler(nginx_pid, gunicorn_pid):\n    try:\n        os.kill(nginx_pid, signal.SIGQUIT)\n    except OSError:\n        pass\n    try:\n        os.kill(gunicorn_pid, signal.SIGTERM)\n    except OSError:\n        pass\n\n    sys.exit(0)\n\n\ndef start_server():\n    print('Starting the inference server with {} workers.'.format(model_server_workers))\n\n\n    # link the log streams to stdout\/err so they will be logged to the container logs\n    subprocess.check_call(['ln', '-sf', '\/dev\/stdout', '\/var\/log\/nginx\/access.log'])\n    subprocess.check_call(['ln', '-sf', '\/dev\/stderr', '\/var\/log\/nginx\/error.log'])\n\n    nginx = subprocess.Popen(['nginx', '-c', '\/opt\/ml\/code\/nginx.conf'])\n    gunicorn = subprocess.Popen(['gunicorn',\n                                 '--timeout', str(model_server_timeout),\n                                 '-b', 'unix:\/tmp\/gunicorn.sock',\n                                 '-w', str(model_server_workers),\n                                 'wsgi:app'])\n\n    signal.signal(signal.SIGTERM, lambda a, b: sigterm_handler(nginx.pid, gunicorn.pid))\n\n    # If either subprocess exits, so do we.\n    pids = set([nginx.pid, gunicorn.pid])\n    while True:\n        pid, _ = os.wait()\n        if pid in pids:\n            break\n\n    sigterm_handler(nginx.pid, gunicorn.pid)\n    print('Inference server exiting')\n\n\n# The main routine just invokes the start function.\nif __name__ == '__main__':\n    start_server()\n<\/code><\/pre>\n\n<p>I deploy the model using the following:<\/p>\n\n<blockquote>\n  <p>predictor = classifier.deploy(1, 'ml.t2.medium', serializer=csv_serializer)<\/p>\n<\/blockquote>\n\n<p>Kindly let me know the mistake I am doing while deploying.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1573549904240,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1573550204732,
        "Question_score":1.0,
        "Question_view_count":4605.0,
        "Answer_body":"<p>Using Sagemaker script mode can be much simpler than dealing with container and nginx low-level stuff like you're trying to do, have you considered that?<br>\nYou only need to provide the keras script:   <\/p>\n\n<blockquote>\n  <p>With Script Mode, you can use training scripts similar to those you would use outside SageMaker with SageMaker's prebuilt containers for various deep learning frameworks such TensorFlow, PyTorch, and Apache MXNet.<\/p>\n<\/blockquote>\n\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws-samples\/amazon-sagemaker-script-mode\/blob\/master\/tf-sentiment-script-mode\/sentiment-analysis.ipynb<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58815367",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1573630807523,
        "Question_original_content":"solv error deploi model deploi custom kera model creat notebook instanc follow file amazon codeset ann nginx conf predictor serv train wsgi dockerfil open aw termin build docker imag push imag ecr repositori open new jupyt python notebook try fit model deploi train correctli deploi follow error error host endpoint exampl fail reason primari contain product variant alltraff pass ping health check check cloudwatch log endpoint check log follow crit connect unix tmp gunicorn sock fail file directori connect upstream client server request ping http upstream http unix tmp gunicorn sock ping host model aw local traceback recent file usr local bin serv line sy exit main file usr local lib python dist packag contain cli serv line main server start env servingenv framework modul file usr local lib python dist packag contain server line start modul app file usr lib python subprocess line init eead errwrit file usr lib python subprocess line execut child rais child except tri deploi model file local model deploi successfulli insid aw face problem serv file code futur import print function import multiprocess import import signal import subprocess import sy cpu count multiprocess cpu count model server timeout environ model server timeout model server worker int environ model server worker cpu count def sigterm handler nginx pid gunicorn pid try kill nginx pid signal sigquit oserror pass try kill gunicorn pid signal sigterm oserror pass sy exit def start server print start infer server worker format model server worker link log stream stdout err log contain log subprocess check dev stdout var log nginx access log subprocess check dev stderr var log nginx error log nginx subprocess popen nginx opt code nginx conf gunicorn subprocess popen gunicorn timeout str model server timeout unix tmp gunicorn sock str model server worker wsgi app signal signal signal sigterm lambda sigterm handler nginx pid gunicorn pid subprocess exit pid set nginx pid gunicorn pid true pid wait pid pid break sigterm handler nginx pid gunicorn pid print infer server exit main routin invok start function main start server deploi model follow predictor classifi deploi medium serial csv serial kindli let know mistak deploi",
        "Question_preprocessed_content":"solv error deploi model deploi custom kera model creat notebook instanc follow file open aw termin build docker imag push imag ecr repositori open new jupyt python notebook try fit model deploi train correctli deploi follow error error host endpoint fail reason primari contain product variant alltraff pass ping health check check cloudwatch log check log follow connect fail connect upstream client server request upstream host traceback file line file line main file line start file line init eead errwrit file line rais tri deploi model file local model deploi successfulli insid aw face problem serv file code deploi model follow predictor kindli let know mistak deploi",
        "Answer_original_content":"script mode simpler deal contain nginx low level stuff like try consid need provid kera script script mode us train script similar us outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet http github com aw sampl amazon script mode blob master sentiment script mode sentiment analysi ipynb",
        "Answer_preprocessed_content":"script mode simpler deal contain nginx stuff like try consid need provid kera script script mode us train script similar us outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet",
        "Question_gpt_summary_original":"the user is encountering an error while attempting to deploy a custom keras model in aws, and is receiving an error message and log entries that they are unable to interpret.",
        "Question_gpt_summary":"user encount error attempt deploi custom kera model aw receiv error messag log entri unabl interpret",
        "Answer_gpt_summary_original":"the possible solution suggested in the answer is to use script mode instead of dealing with container and nginx low-level stuff. the user can use training scripts similar to those used outside with prebuilt containers for various deep learning frameworks such as tensorflow, pytorch, and apache mxnet. the link to the github repository for an example of using script mode with a custom keras model is also provided.",
        "Answer_gpt_summary":"possibl solut suggest answer us script mode instead deal contain nginx low level stuff user us train script similar outsid prebuilt contain deep learn framework tensorflow pytorch apach mxnet link github repositori exampl script mode custom kera model provid"
    },
    {
        "Question_id":null,
        "Question_title":"How to get Model ID of the Latest Version registered in Azure Machine Learning Service Model Registry using az ml cli?",
        "Question_body":"Hello MS team,\n\nI have registered an ML model in the AML workspace using an Azure Machine learning pipeline and triggered the main control script of the pipeline by linking the repo present in Azure DevOps to the AML workspace(using Service principal).\n\nHow do I download the latest version of the model from the AML workspace to the \"Artifacts\" folder in Azure DevOPs?\n\nAny help is appreciated please.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1643903626503,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@ShivapriyaKatta-8600\n\nI think you are mentioning how to get the latest version of model and download the model in az ml.\n\nThere are 2 steps, one is list the model to get the model ID you want, two is download the model.\n\naz ml model list\nList models in the workspace.\n\n az ml model list [--dataset-id]\n                  [--latest]\n                  [--model-name]\n                  [--path]\n                  [--property]\n                  [--resource-group]\n                  [--run-id]\n                  [--subscription-id]\n                  [--tag]\n                  [--workspace-name]\n                  [-v]\n\n\n\nOptional Parameters\n--dataset-id\nIf provided, will only show models with the specified dataset ID.\n\n--latest -l\nIf provided, will only return models with the latest version.\n\n--model-name -n\nAn optional model name to filter the list by.\n\n--path\nPath to a project folder. Default: current directory.\n\n--property\nKey\/value property to add (e.g. key=value ). Multiple properties can be specified with multiple --property options.\n\n--resource-group -g\nResource group corresponding to the provided workspace.\n\n--run-id\nIf provided, will only show models with the specified Run ID.\n\n--subscription-id\nSpecifies the subscription Id.\n\n--tag\nKey\/value tag to add (e.g. key=value ). Multiple tags can be specified with multiple --tag options.\n\n--workspace-name -w\nName of the workspace containing models to list.\n\n-v\nVerbosity flag.\n\naz ml model download\nDownload a model from the workspace.\n\n az ml model download --model-id\n                      --target-dir\n                      [--overwrite]\n                      [--path]\n                      [--resource-group]\n                      [--subscription-id]\n                      [--workspace-name]\n                      [-v]\n\n\n\n\nRequired Parameters\n--model-id -i\nID of model.\n\n--target-dir -t\nTarget directory to download the model file to.\n\nOptional Parameters\n--overwrite\nOverwrite if the same name file exists in target directory.\n\n--path\nPath to a project folder. Default: current directory.\n\n--resource-group -g\nResource group corresponding to the provided workspace.\n\n--subscription-id\nSpecifies the subscription Id.\n\n--workspace-name -w\nName of the workspace containing model to show.\n\n-v\nVerbosity flag.\n\nHope this helps!\n\nPlease kindly accept the answer if you feel helpful, thank you!\n\nRegards,\nYutong",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/721792\/how-to-get-model-id-of-the-latest-version-register.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-04T00:19:45.587Z",
                "Answer_score":0,
                "Answer_body":"@ShivapriyaKatta-8600\n\nI think you are mentioning how to get the latest version of model and download the model in az ml.\n\nThere are 2 steps, one is list the model to get the model ID you want, two is download the model.\n\naz ml model list\nList models in the workspace.\n\n az ml model list [--dataset-id]\n                  [--latest]\n                  [--model-name]\n                  [--path]\n                  [--property]\n                  [--resource-group]\n                  [--run-id]\n                  [--subscription-id]\n                  [--tag]\n                  [--workspace-name]\n                  [-v]\n\n\n\nOptional Parameters\n--dataset-id\nIf provided, will only show models with the specified dataset ID.\n\n--latest -l\nIf provided, will only return models with the latest version.\n\n--model-name -n\nAn optional model name to filter the list by.\n\n--path\nPath to a project folder. Default: current directory.\n\n--property\nKey\/value property to add (e.g. key=value ). Multiple properties can be specified with multiple --property options.\n\n--resource-group -g\nResource group corresponding to the provided workspace.\n\n--run-id\nIf provided, will only show models with the specified Run ID.\n\n--subscription-id\nSpecifies the subscription Id.\n\n--tag\nKey\/value tag to add (e.g. key=value ). Multiple tags can be specified with multiple --tag options.\n\n--workspace-name -w\nName of the workspace containing models to list.\n\n-v\nVerbosity flag.\n\naz ml model download\nDownload a model from the workspace.\n\n az ml model download --model-id\n                      --target-dir\n                      [--overwrite]\n                      [--path]\n                      [--resource-group]\n                      [--subscription-id]\n                      [--workspace-name]\n                      [-v]\n\n\n\n\nRequired Parameters\n--model-id -i\nID of model.\n\n--target-dir -t\nTarget directory to download the model file to.\n\nOptional Parameters\n--overwrite\nOverwrite if the same name file exists in target directory.\n\n--path\nPath to a project folder. Default: current directory.\n\n--resource-group -g\nResource group corresponding to the provided workspace.\n\n--subscription-id\nSpecifies the subscription Id.\n\n--workspace-name -w\nName of the workspace containing model to show.\n\n-v\nVerbosity flag.\n\nHope this helps!\n\nPlease kindly accept the answer if you feel helpful, thank you!\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-04T03:33:57.637Z",
                "Answer_score":0,
                "Answer_body":"Thanks a lot for the response....but I am trying to get the parameter \"--model-id\" as a variable and pass it into the download command, trying to do something like:\n\naz ml model download --model-id $(az ml model list --query \"[].{ID:id}[0].ID\" -o tsv --name saved_model --resource-group $(rg_name) --workspace-name $(ws_name)) --target-dir .\/models --resource-group $(rg_name) --workspace-name $(ws_name)\n\nNot working though...any idea?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643933985587,
        "Question_original_content":"model latest version regist servic model registri cli hello team regist model aml workspac pipelin trigger main control script pipelin link repo present azur devop aml workspac servic princip download latest version model aml workspac artifact folder azur devop help appreci",
        "Question_preprocessed_content":"model latest version regist servic model registri cli hello team regist model aml workspac pipelin trigger main control script pipelin link repo present azur devop aml workspac download latest version model aml workspac artifact folder azur devop help appreci",
        "Answer_original_content":"shivapriyakatta think mention latest version model download model step list model model want download model model list list model workspac model list dataset latest model path properti resourc group run subscript tag workspac option paramet dataset provid model specifi dataset latest provid return model latest version model option model filter list path path project folder default current directori properti kei valu properti add kei valu multipl properti specifi multipl properti option resourc group resourc group correspond provid workspac run provid model specifi run subscript specifi subscript tag kei valu tag add kei valu multipl tag specifi multipl tag option workspac workspac contain model list verbos flag model download download model workspac model download model target dir overwrit path resourc group subscript workspac requir paramet model model target dir target directori download model file option paramet overwrit overwrit file exist target directori path path project folder default current directori resourc group resourc group correspond provid workspac subscript specifi subscript workspac workspac contain model verbos flag hope help kindli accept answer feel help thank regard yutong",
        "Answer_preprocessed_content":"think mention latest version model download model step list model model want download model model list list model workspac model list option paramet provid model specifi dataset latest provid return model latest version option model filter list path path project folder default current directori properti properti add multipl properti specifi multipl option resourc group correspond provid workspac provid model specifi run specifi subscript tag tag add multipl tag specifi multipl option workspac contain model list verbos flag model download download model workspac model download requir paramet model target directori download model file option paramet overwrit overwrit file exist target directori path path project folder default current directori resourc group correspond provid workspac specifi subscript workspac contain model verbos flag hope help kindli accept answer feel help thank regard yutong",
        "Question_gpt_summary_original":"the user is encountering challenges in retrieving the model id of the latest version registered in a service model registry using the  cli.",
        "Question_gpt_summary":"user encount challeng retriev model latest version regist servic model registri cli",
        "Answer_gpt_summary_original":"the answer provides two steps to retrieve the model id of the latest version registered in a service model registry using the  cli. the first step is to list the models in the workspace using the \"az ml model list\" command with optional parameters such as \"--latest\" to only return models with the latest version. the second step is to download the model using the \"az ml model download\" command with the required parameter \"--model-id\" and optional parameters such as \"--overwrite\" to overwrite if the same name file exists in the target directory.",
        "Answer_gpt_summary":"answer provid step retriev model latest version regist servic model registri cli step list model workspac model list command option paramet latest return model latest version second step download model model download command requir paramet model option paramet overwrit overwrit file exist target directori"
    },
    {
        "Question_id":34545078.0,
        "Question_title":"Azure ML vs Cortana Analytics Suite",
        "Question_body":"<p>I am wondering what is the difference between Cortana Analytics and Azure ML ?<\/p>\n\n<ul>\n<li>those are 2 distincts solutions ? <\/li>\n<li>one is part of the other ?<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1451558192573,
        "Question_favorite_count":null,
        "Question_last_edit_time":1543674264230,
        "Question_score":2.0,
        "Question_view_count":1150.0,
        "Answer_body":"<p>Azure Machine Learning is part of the Cortana analytics suite<\/p>\n\n<p>You will find more info with the link below<\/p>\n\n<p><a href=\"http:\/\/www.sqlchick.com\/entries\/2015\/8\/22\/what-is-the-cortana-analytics-suite\" rel=\"nofollow\">All the details on the Cortana link here<\/a><\/p>\n\n<p>All the best<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1457447097076,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34545078",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1451558834643,
        "Question_original_content":"cortana analyt suit wonder differ cortana analyt distinct solut",
        "Question_preprocessed_content":"cortana analyt suit wonder differ cortana analyt distinct solut",
        "Answer_original_content":"cortana analyt suit info link detail cortana link best",
        "Answer_preprocessed_content":"cortana analyt suit info link detail cortana link best",
        "Question_gpt_summary_original":"the user is wondering what the difference is between vs and cortana analytics suite.",
        "Question_gpt_summary":"user wonder differ cortana analyt suit",
        "Answer_gpt_summary_original":"the answer does not provide any possible solutions to the user's question. it only directs the user to a link where they can find more information about the cortana analytics suite.",
        "Answer_gpt_summary":"answer provid possibl solut user question direct user link inform cortana analyt suit"
    },
    {
        "Question_id":60637170.0,
        "Question_title":"How to pass arguments to scoring file when deploying a Model in AzureML",
        "Question_body":"<p>I am deploying a trained model to an ACI endpoint on Azure Machine Learning, using the Python SDK.\nI have created my score.py file, but I would like that file to be called with an argument being passed (just like with a training file) that I can interpret using <code>argparse<\/code>.\nHowever, I don't seem to find how I can pass arguments\nThis is the code I have to create the InferenceConfig environment and which obviously does not work.  Should I fall back on using the extra Docker file steps or so?<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.environment import Environment\nfrom azureml.core.model import InferenceConfig\n\nenv = Environment('my_hosted_environment')\nenv.python.conda_dependencies = CondaDependencies.create(\n    conda_packages=['scikit-learn'],\n    pip_packages=['azureml-defaults'])\nscoring_script = 'score.py --model_name ' + model_name\ninference_config = InferenceConfig(entry_script=scoring_script, environment=env)\n<\/code><\/pre>\n\n<p>Adding the score.py for reference on how I'd love to use the arguments in that script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>#removed imports\nimport argparse\n\ndef init():\n    global model\n\n    parser = argparse.ArgumentParser(description=\"Load sklearn model\")\n    parser.add_argument('--model_name', dest=\"model_name\", required=True)\n    args, _ = parser.parse_known_args()\n\n    model_path = Model.get_model_path(model_name=args.model_name)\n    model = joblib.load(model_path)\n\ndef run(raw_data):\n    try:\n        data = json.loads(raw_data)['data']\n        data = np.array(data)\n        result = model.predict(data)\n        return result.tolist()\n\n    except Exception as e:\n        result = str(e)\n        return result\n<\/code><\/pre>\n\n<p>Interested to hear your thoughts<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_creation_time":1583933260433,
        "Question_favorite_count":null,
        "Question_last_edit_time":1584005920356,
        "Question_score":4.0,
        "Question_view_count":1681.0,
        "Answer_body":"<p>How to deploy using environments can be found here <a href=\"https:\/\/nam06.safelinks.protection.outlook.com\/?url=https%3A%2F%2Fgithub.com%2FAzure%2FMachineLearningNotebooks%2Fblob%2Fmaster%2Fhow-to-use-azureml%2Fdeployment%2Fdeploy-to-cloud%2Fmodel-register-and-deploy.ipynb&amp;data=02%7C01%7CRamprasad.Mula%40microsoft.com%7Ce06d310b0447416ab46b08d7bc836a81%7C72f988bf86f141af91ab2d7cd011db47%7C1%7C0%7C637185146436156499&amp;sdata=uQo332dpjuiNqWFCguvs3Kgg7UUMN8MBEzLxTPyH4MM%3D&amp;reserved=0\" rel=\"nofollow noreferrer\">model-register-and-deploy.ipynb<\/a> .  InferenceConfig class accepts  source_directory and entry_script <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py#parameters\" rel=\"nofollow noreferrer\">parameters<\/a>, where source_directory  is a path to the folder that contains all files(score.py and any other additional files) to create the image. <\/p>\n\n<p>This <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-multi-model\/multi-model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">multi-model-register-and-deploy.ipynb<\/a> has code snippets on how to create InferenceConfig with source_directory and entry_script.<\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\n\nmyenv = Environment.from_conda_specification(name=\"myenv\", file_path=\"myenv.yml\")\ninference_config = InferenceConfig(entry_script=\"score.py\", environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                       name='sklearn-mnist-svc',\n                       models=[model], \n                       inference_config=inference_config,\n                       deployment_config=aciconfig)\n\nservice.wait_for_deployment(show_output=True)\n\nprint(service.scoring_uri)\n<\/code><\/pre>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1584011988083,
        "Answer_score":-2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60637170",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1584005785480,
        "Question_original_content":"pass argument score file deploi model deploi train model aci endpoint python sdk creat score file like file call argument pass like train file interpret argpars pass argument code creat inferenceconfig environ obvious work fall extra docker file step core conda depend import condadepend core environ import environ core model import inferenceconfig env environ host environ env python conda depend condadepend creat conda packag scikit learn pip packag default score script score model model infer config inferenceconfig entri script score script environ env ad score refer love us argument script remov import import argpars def init global model parser argpars argumentpars descript load sklearn model parser add argument model dest model requir true arg parser pars known arg model path model model path model arg model model joblib load model path def run raw data try data json load raw data data data arrai data result model predict data return result tolist except result str return result interest hear thought",
        "Question_preprocessed_content":"pass argument score file deploi model deploi train model aci endpoint python sdk creat file like file call argument pass interpret pass argument code creat inferenceconfig environ obvious work fall extra docker file step ad refer love us argument script interest hear thought",
        "Answer_original_content":"deploi environ model regist deploi ipynb inferenceconfig class accept sourc directori entri script paramet sourc directori path folder contain file score addit file creat imag multi model regist deploi ipynb code snippet creat inferenceconfig sourc directori entri script core webservic import webservic core model import inferenceconfig core environ import environ myenv environ conda specif myenv file path myenv yml infer config inferenceconfig entri script score environ myenv servic model deploi workspac sklearn mnist svc model model infer config infer config deploy config aciconfig servic wait deploy output true print servic score uri",
        "Answer_preprocessed_content":"deploi environ inferenceconfig class accept paramet path folder contain addit file creat imag code snippet creat inferenceconfig",
        "Question_gpt_summary_original":"the user is encountering challenges in passing arguments to a scoring file when deploying a model in .",
        "Question_gpt_summary":"user encount challeng pass argument score file deploi model",
        "Answer_gpt_summary_original":"the answer suggests that the user can deploy using environments and provides a link to a notebook with instructions. it also mentions the use of the inferenceconfig class, which accepts source_directory and entry_script parameters. the answer provides a link to another notebook with code snippets on how to create inferenceconfig with source_directory and entry_script. additionally, the answer includes code snippets for creating an environment and deploying the model.",
        "Answer_gpt_summary":"answer suggest user deploi environ provid link notebook instruct mention us inferenceconfig class accept sourc directori entri script paramet answer provid link notebook code snippet creat inferenceconfig sourc directori entri script addition answer includ code snippet creat environ deploi model"
    },
    {
        "Question_id":null,
        "Question_title":"Unable to test my ML model created with Azure Automated ML model endpoints",
        "Question_body":"Hi\n\nI am unable to test the model (Microsoft azure Machine learning ->endpoints->modelname ->Test , even though some model i could execute on 09-Aug-2021 late evening. Please help to resolve issue\n\n\n\n\nThanks\nRamabadran",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1628598159740,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/507969\/unable-to-test-my-ml-model-created-with-azure-auto.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-10T13:07:13.11Z",
                "Answer_score":0,
                "Answer_body":"Please follow-up on this thread. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"unabl test model creat azur autom model endpoint unabl test model endpoint modelnam test model execut aug late even help resolv issu thank ramabadran",
        "Question_preprocessed_content":"unabl test model creat azur autom model endpoint unabl test model test model execut late even help resolv issu thank ramabadran",
        "Answer_original_content":"follow thread thank",
        "Answer_preprocessed_content":"thread thank",
        "Question_gpt_summary_original":"the user is unable to test their ml model created with azure automated ml model endpoints.",
        "Question_gpt_summary":"user unabl test model creat azur autom model endpoint",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the responder is simply asking the user to follow-up on the thread.",
        "Answer_gpt_summary":"solut provid answer respond simpli ask user follow thread"
    },
    {
        "Question_id":null,
        "Question_title":"Retail product catalog not sync with Merchant Center after initial import",
        "Question_body":"I followed the documents to import product catalog from Merchant Center Sync. It seems to work at the beginning - but I recently notice it has not synced for last seven days. There is no error I can see that explains why.- There is a warnig on the Product Catalog integration page saying Last import is more than 1 week old.- If I click Import button, select merchant account and branch 0 . It says The branch already has a data source. But it is not syncing at all.I have daily new products added to merchant center so this is serious issue for me as many events become unjoined and recommendations on those new products are all messed up.  ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658072520000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":46.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Retail-product-catalog-not-sync-with-Merchant-Center-after\/td-p\/443901\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-17T15:47:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"And the Import Activity tab shows no activity of any Product catalog import activity. In fact I never see any entry here despite it was syncing last week."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"retail product catalog sync merchant center initi import follow document import product catalog merchant center sync work begin recent notic sync seven dai error explain warnig product catalog integr page sai import week old click import button select merchant account branch sai branch data sourc sync daili new product ad merchant center issu event unjoin recommend new product mess",
        "Question_preprocessed_content":"retail product catalog sync merchant center initi import follow document import product catalog merchant center sync work begin recent notic sync seven dai error explain warnig product catalog integr page sai import week click import button select merchant account branch sai branch data sourc sync daili new product ad merchant center issu event unjoin recommend new product mess",
        "Answer_original_content":"import activ tab show activ product catalog import activ fact entri despit sync week",
        "Answer_preprocessed_content":"import activ tab show activ product catalog import activ fact entri despit sync week",
        "Question_gpt_summary_original":"the user is facing a challenge with their retail product catalog not syncing with their merchant center after an initial import, despite there being no visible errors and a warning that the last import is more than a week old.",
        "Question_gpt_summary":"user face challeng retail product catalog sync merchant center initi import despit visibl error warn import week old",
        "Answer_gpt_summary_original":"possible solutions: \n- check if the product catalog is correctly formatted and meets the requirements of the merchant center.\n- check if there are any issues with the data feed or the connection between the product catalog and the merchant center.\n- check if there are any updates or changes made to the product catalog that may have caused the syncing issue.\n- contact the support team of the merchant center for further assistance.",
        "Answer_gpt_summary":"possibl solut check product catalog correctli format meet requir merchant center check issu data feed connect product catalog merchant center check updat chang product catalog caus sync issu contact support team merchant center assist"
    },
    {
        "Question_id":49006174.0,
        "Question_title":"How is the entry point to the code specified in AWS sagemaker bring your own code?",
        "Question_body":"<p>I'm trying out the sample notebooks in AWS Sagemaker, currently in the mxnet mnist example which demonstrates bringing your own code. The entry point parameter passed in when instantiating an estimator instance, only mentions the source file (mnist.py) and not a method name or any other point inside the source file.<\/p>\n\n<p>So how does aws sagemaker figure out which method to send the training data to? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1519726485703,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1519802441487,
        "Question_score":1.0,
        "Question_view_count":3327.0,
        "Answer_body":"<p>Your python script should implement a few methods like train, model_fn, transform_fn, input_fn etc. SagaMaker would call appropriate method when needed. <\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49006174",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1520316895088,
        "Question_original_content":"entri point code specifi bring code try sampl notebook current mxnet mnist exampl demonstr bring code entri point paramet pass instanti estim instanc mention sourc file mnist method point insid sourc file figur method send train data",
        "Question_preprocessed_content":"entri point code specifi bring code try sampl notebook current mxnet mnist exampl demonstr bring code entri point paramet pass instanti estim instanc mention sourc file method point insid sourc file figur method send train data",
        "Answer_original_content":"python script implement method like train model transform input sagamak appropri method need http doc aw amazon com latest mxnet train infer code templat html",
        "Answer_preprocessed_content":"python script implement method like train sagamak appropri method need",
        "Question_gpt_summary_original":"the user is trying to understand how to specify the entry point to the code when using bring your own code in the mxnet mnist example.",
        "Question_gpt_summary":"user try understand specifi entri point code bring code mxnet mnist exampl",
        "Answer_gpt_summary_original":"to specify the entry point to the code when using bring your own code in the mxnet mnist example, the user needs to implement methods like train, model_fn, transform_fn, input_fn, etc. in their python script.  will then call the appropriate method when needed. the documentation provides a code template for reference.",
        "Answer_gpt_summary":"specifi entri point code bring code mxnet mnist exampl user need implement method like train model transform input python script appropri method need document provid code templat refer"
    },
    {
        "Question_id":68251533.0,
        "Question_title":"Why is AWS Sagemaker notebook instance designed to only persist data under ~\/Sagemaker?",
        "Question_body":"<p>In my current job we use AWS managed notebooks on Sagemaker EC2. I am largely okay with the user experience but the lack of data persistency outside <code>~\/Sagemaker<\/code> has been quite inconvenient. Every time should the instance need restarting, I'd lose all the settings and python packages. Wonder why AWS would make this particular decision for Sagemaker. Have used Google Cloud's AI platform before and it does not have such settings and my configurations would always persist.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625466236780,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":266.0,
        "Answer_body":"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read\/write that persist across the stop\/restart cycle.\nReguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68251533",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1625487892607,
        "Question_original_content":"notebook instanc design persist data current job us aw manag notebook larg okai user experi lack data persist outsid inconveni time instanc need restart lose set python packag wonder aw particular decis googl cloud platform set configur persist",
        "Question_preprocessed_content":"notebook instanc design persist data current job us aw manag notebook larg okai user experi lack data persist outsid inconveni time instanc need restart lose set python packag wonder aw particular decis googl cloud platform set configur persist",
        "Answer_original_content":"face similar issu aw servic usual manag servic aw us read contain approach leav folder filesystem read write persist stop restart cycl reguard packag instal instal custom environ notebook instanc amazon eb volum describ",
        "Answer_preprocessed_content":"face similar issu aw servic usual manag servic aw us contain approach leav folder filesystem persist cycl reguard packag instal instal custom environ notebook instanc amazon eb volum describ",
        "Question_gpt_summary_original":"the user is encountering challenges with aws managed notebooks on ec2, as the lack of data persistency outside of the ~\/ directory is causing them to lose all settings and python packages every time the instance needs restarting.",
        "Question_gpt_summary":"user encount challeng aw manag notebook lack data persist outsid directori caus lose set python packag time instanc need restart",
        "Answer_gpt_summary_original":"possible solutions to the challenge of losing settings and python packages on aws managed notebooks on ec2 are to use a read-only containers approach and install custom environment on the notebook instance's amazon ebs volume.",
        "Answer_gpt_summary":"possibl solut challeng lose set python packag aw manag notebook us read contain approach instal custom environ notebook instanc amazon eb volum"
    },
    {
        "Question_id":null,
        "Question_title":"Getting Authenticatin error for python script step in pipeline",
        "Question_body":"I am trying to create a azure pipeline for learning purpose. I followed the steps mentioned in this notebook:\nhttps:\/\/github.com\/MicrosoftLearning\/mslearn-dp100\/blob\/main\/08%20-%20Create%20a%20Pipeline.ipynb\n\n\n\n     #%% connect to a workspace from config\n # use service principal authentication - https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\n    \n azure_svppwd = os.environ.get(\"AZUREML_PASSWD\") # use os.environ[\"AZUREML_PASSWD\"] = ' from azure service client secret\"\n auth_pwd = ServicePrincipalAuthentication(\n     tenant_id=\"7da05296-d70e-4398-8a3d-f113e0dad997\",\n     service_principal_id=\"57bac230-cbb8-409e-be64-5c4516d40771\",\n     service_principal_password=azure_svppwd)\n ws = Workspace.from_config('config',auth=auth_pwd)    \n print(\"Found workspace {} at location {}\".format(ws.name, ws.location))    \n # get the data from datasets\n    \n in_data = ws.datasets.get('testparquet') # dataset is registered through Azure UI which refers to some parquet files in another container\n    \n #%% create environment\n env = Environment.from_conda_specification(name='azenv',file_path='envspec.yml')\n env.register(ws)  # Save the environment for future use and retreival\n #%% create compute target - compute clusters\n try:\n     comp_cluster = ComputeTarget(ws,'amltryoutcluster')\n     print('amltryoutcluster found existing and will be used')\n except 'ComputeTargetException':\n        \n     comp_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D3',min_nodes=0,max_nodes=6)\n     # vm_size in above line indicates the type of computation powe we need\n     comp_cluster = ComputeTarget.create(ws,'amltryoutcluster',comp_config)\n     comp_cluster.wait_for_completion(show_output=True)\n #%% Create pipeline data object and steps\n prep_data = PipelineData(name=\"pipedata\",datastore=ws.get_default_datastore())\n #%% Create pipeline steps\n #%% pipeline configuration\n pipe_config = RunConfiguration()\n pipe_config.target = comp_cluster\n pipe_config.environment= Environment.get(ws,\"azenv\")\n prep_step = PythonScriptStep(name=\"prep\",\n                              script_name=\"prep_script.py\",             \n                             source_directory='.\/',                \n                              arguments=['--ipdata',in_data.as_named_input(\"inparquet\"),\n                                         '--oppath',prep_data],\n                              outputs=[prep_data],\n                              compute_target = comp_cluster,\n                              runconfig =pipe_config,\n                              allow_reuse=True)\n train_step = PythonScriptStep(name='train',\n                               script_name='train_script.py',\n                               source_directory='.\/',\n                               arguments=['--prepdata',prep_data],\n                               inputs=[prep_data],\n                               compute_target=comp_cluster,\n                               runconfig=pipe_config,\n                               allow_reuse=True)\n # get experiment and run pipeline\n # Construct the pipeline\n pipeline_steps = [prep_step, train_step]\n pipeline = Pipeline(workspace=ws, steps=pipeline_steps,default_datastore='parquet_ingestion')\n exp =Experiment(ws,'amltryout')\n pipeline_run = exp.submit(pipeline, regenerate_outputs=True)\n\n\n\nIn the prep_script.py, I have the following :\n\n from azureml.core import Experiment,Workspace,Datastore,Dataset\n import pandas as pd\n import argparse\n from azureml.core import Run\n from sklearn import preprocessing\n import os\n    \n parse = argparse.ArgumentParser()\n parse.add_argument('--ipdata')\n parse.add_argument('--oppath')\n args = parse.parse_args()\n save_folder = args.oppath\n    \n run = Run.get_context()\n    \n #https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/work-with-data-in-aml\/5-using-datasets\n pqdata = run.input_datasets['inparquet'].to_pandas_dataframe()\n # ws = run.experiment.workspace\n # pqdata = Dataset.get_by_id(ws, id='testparquet')\n run.log(\"count\",pqdata.count())\n    \n sel_data = pqdata[['title','country','salary']]\n str_enc = preprocessing.LabelEncoder()\n    \n sel_data['title','country'] = str_enc.fit_transform(sel_data['title','country']) \n    \n    \n os.makedirs(save_folder, exist_ok=True)\n save_path = os.path.join(save_folder,'data_prep.csv')\n sel_data.to_csv(save_path)\n    \n run.complete()\n\n\n\n\nAlso some steps in train_script , but when I submit the pipeline it fails at prep step with the following error :\n\nAuthentication failed for Container Registry: 3495c636784040b2ae0f5c8e7ee4133a.azurecr.io.\n\nAm I missing something here?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623207954850,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/427884\/getting-authenticatin-error-for-python-script-step.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-09T13:11:50.927Z",
                "Answer_score":1,
                "Answer_body":"@RaghuvarranVH-5191 Thanks for the question. Can you please add more details about the Docker Container Registry that you are trying and share error log snapshot to check.\nTo Initialize Workspace, Please follow the below doc.\nInitialize a workspace object from persisted configuration.\n\n ws = Workspace.from_config()\n print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep = '\\n')\n\n\n\nFor Azure machine learning pipelines quickstart please follow the below notebook.\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/intro-to-pipelines\/aml-pipelines-getting-started.ipynb",
                "Answer_comment_count":5,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get authenticatin error python script step pipelin try creat azur pipelin learn purpos follow step mention notebook http github com microsoftlearn mslearn blob main creat pipelin ipynb connect workspac config us servic princip authent http github com azur machinelearningnotebook blob master us manag servic authent authent ipynb azur svppwd environ passwd us environ passwd azur servic client secret auth pwd serviceprincipalauthent tenant fedad servic princip bac cbb servic princip password azur svppwd workspac config config auth auth pwd print workspac locat format locat data dataset data dataset testparquet dataset regist azur refer parquet file contain creat environ env environ conda specif azenv file path envspec yml env regist save environ futur us retreiv creat comput target comput cluster try comp cluster computetarget amltryoutclust print amltryoutclust exist computetargetexcept comp config amlcomput provis configur size standard min node max node size line indic type comput pow need comp cluster computetarget creat amltryoutclust comp config comp cluster wait complet output true creat pipelin data object step prep data pipelinedata pipedata datastor default datastor creat pipelin step pipelin configur pipe config runconfigur pipe config target comp cluster pipe config environ environ azenv prep step pythonscriptstep prep script prep script sourc directori argument ipdata data name input inparquet oppath prep data output prep data comput target comp cluster runconfig pipe config allow reus true train step pythonscriptstep train script train script sourc directori argument prepdata prep data input prep data comput target comp cluster runconfig pipe config allow reus true experi run pipelin construct pipelin pipelin step prep step train step pipelin pipelin workspac step pipelin step default datastor parquet ingest exp experi amltryout pipelin run exp submit pipelin regener output true prep script follow core import experi workspac datastor dataset import panda import argpars core import run sklearn import preprocess import pars argpars argumentpars pars add argument ipdata pars add argument oppath arg pars pars arg save folder arg oppath run run context http doc microsoft com learn modul work data aml dataset pqdata run input dataset inparquet panda datafram run experi workspac pqdata dataset testparquet run log count pqdata count sel data pqdata titl countri salari str enc preprocess labelencod sel data titl countri str enc fit transform sel data titl countri makedir save folder exist true save path path join save folder data prep csv sel data csv save path run complet step train script submit pipelin fail prep step follow error authent fail contain registri cbaefceeea azurecr miss",
        "Question_preprocessed_content":"get authenticatin error python script step pipelin try creat azur pipelin learn purpos follow step mention notebook connect workspac config us servic princip authent us azur servic client secret serviceprincipalauthent print data dataset dataset regist azur refer parquet file contain creat environ env save environ futur us retreiv creat comput target comput cluster try computetarget print computetargetexcept line indic type comput pow need creat pipelin data object step creat pipelin step pipelin configur runconfigur pythonscriptstep runconfig pythonscriptstep experi run pipelin construct pipelin pipelin pipelin exp experi follow core import experi workspac datastor dataset import panda import argpars core import run sklearn import preprocess import pars arg run pqdata pqdata testparquet pqdata step submit pipelin fail prep step follow error authent fail contain registri miss",
        "Answer_original_content":"raghuvarranvh thank question add detail docker contain registri try share error log snapshot check initi workspac follow doc initi workspac object persist configur workspac config print resourc group locat subscript sep pipelin quickstart follow notebook http github com azur machinelearningnotebook blob master us machin learn pipelin intro pipelin aml pipelin get start ipynb",
        "Answer_preprocessed_content":"thank question add detail docker contain registri try share error log snapshot check initi workspac follow doc initi workspac object persist configur sep pipelin quickstart follow notebook",
        "Question_gpt_summary_original":"the user is encountering an authentication error when attempting to create an azure pipeline for learning purposes.",
        "Question_gpt_summary":"user encount authent error attempt creat azur pipelin learn purpos",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the authentication error encountered when attempting to create an azure pipeline. the answer requests for more details about the docker container registry and error log snapshot to check. it also provides a link to initialize a workspace object from persisted configuration and a notebook for pipelines quickstart.",
        "Answer_gpt_summary":"solut provid answer authent error encount attempt creat azur pipelin answer request detail docker contain registri error log snapshot check provid link initi workspac object persist configur notebook pipelin quickstart"
    },
    {
        "Question_id":65721061.0,
        "Question_title":"How to parse stepfunction executionId to SageMaker batch transform job name?",
        "Question_body":"<p>I have created a stepfunction, the definition for this statemachine below (<code>step-function.json<\/code>) is used in terraform (using the syntax in this page:<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html<\/a>)<\/p>\n<p>The first time if I execute this statemachine, it will create a SageMaker batch transform job named <code>example-jobname<\/code>, but I need to exeucute this statemachine everyday, then it will give me error <code>&quot;error&quot;: &quot;SageMaker.ResourceInUseException&quot;, &quot;cause&quot;: &quot;Job name must be unique within an AWS account and region, and a job with this name already exists <\/code>.<\/p>\n<p>The cause is because the job name is hard-coded as <code>example-jobname<\/code> so if the state machine gets executed after the first time, since the job name needs to be unique, the task will fail, just wondering how I can add a string (something like ExecutionId at the end of the job name). Here's what I have tried:<\/p>\n<ol>\n<li><p>I added <code>&quot;executionId.$&quot;: &quot;States.Format('somestring {}', $$.Execution.Id)&quot;<\/code> in the <code>Parameters<\/code> section in the json file, but when I execute the task I got error <code> &quot;error&quot;: &quot;States.Runtime&quot;, &quot;cause&quot;: &quot;An error occurred while executing the state 'SageMaker CreateTransformJob' (entered at the event id #2). The Parameters '{\\&quot;BatchStrategy\\&quot;:\\&quot;SingleRecord\\&quot;,..............\\&quot;executionId\\&quot;:\\&quot;somestring arn:aws:states:us-east-1:xxxxx:execution:xxxxx-state-machine:xxxxxxxx72950\\&quot;}' could not be used to start the Task: [The field \\&quot;executionId\\&quot; is not supported by Step Functions]&quot;}<\/code><\/p>\n<\/li>\n<li><p>I modified the jobname in the json file to  <code>&quot;TransformJobName&quot;: &quot;example-jobname-States.Format('somestring {}', $$.Execution.Id)&quot;,<\/code>, when I execute the statemachine, it gave me error: <code>&quot;error&quot;: &quot;SageMaker.AmazonSageMakerException&quot;, &quot;cause&quot;: &quot;2 validation errors detected: Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must satisfy regular expression pattern: ^[a-zA-Z0-9](-*[a-zA-Z0-9]){0,62}; Value 'example-jobname-States.Format('somestring {}', $$.Execution.Id)' at 'transformJobName' failed to satisfy constraint: Member must have length less than or equal to 63<\/code><\/p>\n<\/li>\n<\/ol>\n<p>I really run out of ideas, can someone help please? Many thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1610635167150,
        "Question_favorite_count":null,
        "Question_last_edit_time":1611071837132,
        "Question_score":2.0,
        "Question_view_count":1209.0,
        "Answer_body":"<p>So as per the <a href=\"https:\/\/docs.aws.amazon.com\/step-functions\/latest\/dg\/sample-train-model.html#sample-train-model-code-examples\" rel=\"nofollow noreferrer\">documentation<\/a>, we should be passing the parameters in the following format<\/p>\n<pre><code>        &quot;Parameters&quot;: {\n            &quot;ModelName.$&quot;: &quot;$$.Execution.Name&quot;,  \n            ....\n        },\n<\/code><\/pre>\n<p>If you take a close look this is something missing from your definition, So your step function definition should be something like below:<\/p>\n<p>either<\/p>\n<pre><code>      &quot;TransformJobName.$&quot;: &quot;$$.Execution.Id&quot;,\n<\/code><\/pre>\n<p>OR<\/p>\n<pre><code>      &quot;TransformJobName.$: &quot;States.Format('mytransformjob{}', $$.Execution.Id)&quot;\n<\/code><\/pre>\n<p>full State machine definition:<\/p>\n<pre><code>    {\n        &quot;Comment&quot;: &quot;Defines the statemachine.&quot;,\n        &quot;StartAt&quot;: &quot;Generate Random String&quot;,\n        &quot;States&quot;: {\n            &quot;Generate Random String&quot;: {\n                &quot;Type&quot;: &quot;Task&quot;,\n                &quot;Resource&quot;: &quot;arn:aws:lambda:eu-central-1:1234567890:function:randomstring&quot;,\n                &quot;ResultPath&quot;: &quot;$.executionid&quot;,\n                &quot;Parameters&quot;: {\n                &quot;executionId.$&quot;: &quot;$$.Execution.Id&quot;\n                },\n                &quot;Next&quot;: &quot;SageMaker CreateTransformJob&quot;\n            },\n        &quot;SageMaker CreateTransformJob&quot;: {\n            &quot;Type&quot;: &quot;Task&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:states:::sagemaker:createTransformJob.sync&quot;,\n            &quot;Parameters&quot;: {\n            &quot;BatchStrategy&quot;: &quot;SingleRecord&quot;,\n            &quot;DataProcessing&quot;: {\n                &quot;InputFilter&quot;: &quot;$&quot;,\n                &quot;JoinSource&quot;: &quot;Input&quot;,\n                &quot;OutputFilter&quot;: &quot;xxx&quot;\n            },\n            &quot;Environment&quot;: {\n                &quot;SAGEMAKER_MODEL_SERVER_TIMEOUT&quot;: &quot;300&quot;\n            },\n            &quot;MaxConcurrentTransforms&quot;: 100,\n            &quot;MaxPayloadInMB&quot;: 1,\n            &quot;ModelName&quot;: &quot;${model_name}&quot;,\n            &quot;TransformInput&quot;: {\n                &quot;DataSource&quot;: {\n                    &quot;S3DataSource&quot;: {\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3Uri&quot;: &quot;${s3_input_path}&quot;\n                    }\n                },\n                &quot;ContentType&quot;: &quot;application\/jsonlines&quot;,\n                &quot;CompressionType&quot;: &quot;Gzip&quot;,\n                &quot;SplitType&quot;: &quot;Line&quot;\n            },\n            &quot;TransformJobName.$&quot;: &quot;$.executionid&quot;,\n            &quot;TransformOutput&quot;: {\n                &quot;S3OutputPath&quot;: &quot;${s3_output_path}&quot;,\n                &quot;Accept&quot;: &quot;application\/jsonlines&quot;,\n                &quot;AssembleWith&quot;: &quot;Line&quot;\n            },    \n            &quot;TransformResources&quot;: {\n                &quot;InstanceType&quot;: &quot;xxx&quot;,\n                &quot;InstanceCount&quot;: 1\n            }\n        },\n            &quot;End&quot;: true\n        }\n        }\n    }\n<\/code><\/pre>\n<p>In the above definition the lambda could be a function which parses the execution id arn which I am passing via the parameters section:<\/p>\n<pre><code> def lambda_handler(event, context):\n    return(event.get('executionId').split(':')[-1])\n<\/code><\/pre>\n<p>Or if you dont wanna pass the execution id , it can simply return the random string like<\/p>\n<pre><code> import string\n def lambda_handler(event, context):\n    return(string.ascii_uppercase + string.digits)\n<\/code><\/pre>\n<p>you can generate all kinds of random string or do generate anything in the lambda and pass that to the transform job name.<\/p>",
        "Answer_comment_count":23.0,
        "Answer_last_edit_time":1610641519768,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65721061",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1610637215396,
        "Question_original_content":"pars stepfunct executionid batch transform job creat stepfunct definit statemachin step function json terraform syntax page http doc aw amazon com latest apirefer api createtransformjob html time execut statemachin creat batch transform job name exampl jobnam need exeucut statemachin everydai error error resourceinuseexcept caus job uniqu aw account region job exist caus job hard code exampl jobnam state machin get execut time job need uniqu task fail wonder add string like executionid end job tri ad executionid state format somestr execut paramet section json file execut task got error error state runtim caus error occur execut state createtransformjob enter event paramet batchstrategi singlerecord executionid somestr arn aw state east execut state machin start task field executionid support step function modifi jobnam json file transformjobnam exampl jobnam state format somestr execut execut statemachin gave error error amazonexcept caus valid error detect valu exampl jobnam state format somestr execut transformjobnam fail satisfi constraint member satisfi regular express pattern valu exampl jobnam state format somestr execut transformjobnam fail satisfi constraint member length equal run idea help thank",
        "Question_preprocessed_content":"pars stepfunct executionid batch transform job creat stepfunct definit statemachin terraform time execut statemachin creat batch transform job name need exeucut statemachin everydai error caus job state machin get execut time job need uniqu task fail wonder add string tri ad section json file execut task got error modifi jobnam json file execut statemachin gave error run idea help thank",
        "Answer_original_content":"document pass paramet follow format paramet modelnam execut close look miss definit step function definit like transformjobnam execut transformjobnam state format mytransformjob execut state machin definit comment defin statemachin startat gener random string state gener random string type task resourc arn aw lambda central function randomstr resultpath executionid paramet executionid execut createtransformjob createtransformjob type task resourc arn aw state createtransformjob sync paramet batchstrategi singlerecord dataprocess inputfilt joinsourc input outputfilt environ model server timeout maxconcurrenttransform maxpayloadinmb modelnam model transforminput datasourc sdatasourc sdatatyp sprefix suri input path contenttyp applic jsonlin compressiontyp gzip splittyp line transformjobnam executionid transformoutput soutputpath output path accept applic jsonlin assemblewith line transformresourc instancetyp instancecount end true definit lambda function pars execut arn pass paramet section def lambda handler event context return event executionid split dont wanna pass execut simpli return random string like import string def lambda handler event context return string ascii uppercas string digit gener kind random string gener lambda pass transform job",
        "Answer_preprocessed_content":"document pass paramet follow format close look miss definit step function definit like state machin definit definit lambda function pars execut arn pass paramet section dont wanna pass execut simpli return random string like gener kind random string gener lambda pass transform job",
        "Question_gpt_summary_original":"the user is encountering challenges in parsing the stepfunction execution id to a batch transform job name in order to avoid the error of the job name needing to be unique.",
        "Question_gpt_summary":"user encount challeng pars stepfunct execut batch transform job order avoid error job need uniqu",
        "Answer_gpt_summary_original":"the solution to the challenge of parsing the stepfunction execution id to a batch transform job name is to pass the parameters in the correct format, which is \"parameters\": { \"modelname.$\": \"$$.execution.name\", .... }. the step function definition should include either \"transformjobname.$\": \"$$.execution.id\", or \"transformjobname.$: \"states.format('mytransformjob{}', $$.execution.id)\". additionally, a lambda function can be used to parse the execution id arn or generate a random string to be passed as the transform job name.",
        "Answer_gpt_summary":"solut challeng pars stepfunct execut batch transform job pass paramet correct format paramet modelnam execut step function definit includ transformjobnam execut transformjobnam state format mytransformjob execut addition lambda function pars execut arn gener random string pass transform job"
    },
    {
        "Question_id":null,
        "Question_title":"What are you looking forward to in Google Cloud NEXT '21?",
        "Question_body":"Google Cloud Next conference is happening this year on October 12-14, 2021!What exciting new technologies are you looking forward to?",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1624439940000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":372.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-are-you-looking-forward-to-in-Google-Cloud-NEXT-21\/td-p\/161539\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-23T09:19:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Google Cloud Next conference is happening this year on October 12-14, 2021!\n\nWhat exciting new technologies are you looking forward to?"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"look forward googl cloud googl cloud confer happen year octob excit new technolog look forward",
        "Question_preprocessed_content":"look forward googl cloud googl cloud confer happen year octob excit new technolog look forward",
        "Answer_original_content":"googl cloud confer happen year octob excit new technolog look forward",
        "Answer_preprocessed_content":"googl cloud confer happen year octob excit new technolog look forward",
        "Question_gpt_summary_original":"the user is looking forward to the upcoming google cloud next '21 conference to explore new technologies.",
        "Question_gpt_summary":"user look forward upcom googl cloud confer explor new technolog",
        "Answer_gpt_summary_original":"summary: the google cloud next '21 conference is happening on october 12-14, 2021. the answer does not provide any solutions but asks the user about the exciting new technologies they are looking forward to exploring at the conference.",
        "Answer_gpt_summary":"summari googl cloud confer happen octob answer provid solut ask user excit new technolog look forward explor confer"
    },
    {
        "Question_id":59176241.0,
        "Question_title":"import custom python module in azure ml deployment environment",
        "Question_body":"<p>I have an sklearn k-means model. I am training the model and saving it in a pickle file so I can deploy it later using azure ml library. The model that I am training uses a custom Feature Encoder called <strong>MultiColumnLabelEncoder<\/strong>.\nThe pipeline model is defined as follow :<\/p>\n\n<pre><code># Pipeline\nkmeans = KMeans(n_clusters=3, random_state=0)\npipe = Pipeline([\n(\"encoder\", MultiColumnLabelEncoder()),\n('k-means', kmeans),\n])\n#Training the pipeline\nmodel = pipe.fit(visitors_df)\nprediction = model.predict(visitors_df)\n#save the model in pickle\/joblib format\nfilename = 'k_means_model.pkl'\njoblib.dump(model, filename)\n<\/code><\/pre>\n\n<p>The model saving works fine. The Deployment steps are the same as the steps in this link : <\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/azureml\/projects\/azureml-getting-started\/html\/how-to-use-azureml\/deploy-to-cloud\/model-register-and-deploy.ipynb<\/a><\/p>\n\n<p>However the deployment always fails with this error :<\/p>\n\n<pre><code>  File \"\/var\/azureml-server\/create_app.py\", line 3, in &lt;module&gt;\n    from app import main\n  File \"\/var\/azureml-server\/app.py\", line 27, in &lt;module&gt;\n    import main as user_main\n  File \"\/var\/azureml-app\/main.py\", line 19, in &lt;module&gt;\n    driver_module_spec.loader.exec_module(driver_module)\n  File \"\/structure\/azureml-app\/score.py\", line 22, in &lt;module&gt;\n    importlib.import_module(\"multilabelencoder\")\n  File \"\/azureml-envs\/azureml_b707e8c15a41fd316cf6c660941cf3d5\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\nModuleNotFoundError: No module named 'multilabelencoder'\n<\/code><\/pre>\n\n<p>I understand that pickle\/joblib has some problems unpickling the custom function MultiLabelEncoder. That's why I defined this class in a separate python script (which I executed also). I called this custom function in the training python script, in the deployment script and in the scoring python file (score.py). The importing in the score.py file is not successful. \nSo my question is how can I import custom python module to azure ml deployment environment ?<\/p>\n\n<p>Thank you in advance.<\/p>\n\n<p>EDIT: \nThis is my .yml file<\/p>\n\n<pre><code>name: project_environment\ndependencies:\n  # The python interpreter version.\n  # Currently Azure ML only supports 3.5.2 and later.\n- python=3.6.2\n\n- pip:\n  - multilabelencoder==1.0.4\n  - scikit-learn\n  - azureml-defaults==1.0.74.*\n  - pandas\nchannels:\n- conda-forge\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1575463048677,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1575634698240,
        "Question_score":4.0,
        "Question_view_count":2611.0,
        "Answer_body":"<p>In fact, the solution was to import my customized class <strong>MultiColumnLabelEncoder<\/strong> as a pip package (You can find it through pip install multilllabelencoder==1.0.5).\nThen I passed the pip package to the .yml file or in the InferenceConfig of the azure ml environment.\nIn the score.py file, I imported the class as follows :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder\ndef init():\n    global model\n\n    # Call the custom encoder to be used dfor unpickling the model\n    encoder = multilabelencoder.MultiColumnLabelEncoder() \n    # Get the path where the deployed model can be found.\n    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'k_means_model_45.pkl')\n    model = joblib.load(model_path)\n<\/code><\/pre>\n\n<p>Then the deployment was successful. \nOne more important thing is I had to use the same pip package (multilabelencoder) in the training pipeline as here :<\/p>\n\n<pre><code>from multilabelencoder import multilabelencoder \npipe = Pipeline([\n    (\"encoder\", multilabelencoder.MultiColumnLabelEncoder(columns)),\n    ('k-means', kmeans),\n])\n#Training the pipeline\ntrainedModel = pipe.fit(df)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59176241",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1575635052340,
        "Question_original_content":"import custom python modul deploy environ sklearn mean model train model save pickl file deploi later librari model train us custom featur encod call multicolumnlabelencod pipelin model defin follow pipelin kmean kmean cluster random state pipe pipelin encod multicolumnlabelencod mean kmean train pipelin model pipe fit visitor predict model predict visitor save model pickl joblib format filenam mean model pkl joblib dump model filenam model save work fine deploy step step link http notebook azur com project get start html us deploi cloud model regist deploi ipynb deploy fail error file var server creat app line app import main file var server app line import main user main file var app main line driver modul spec loader exec modul driver modul file structur app score line importlib import modul multilabelencod file env becafdcfccfd lib python importlib init line import modul return bootstrap gcd import level packag level modulenotfounderror modul name multilabelencod understand pickl joblib problem unpickl custom function multilabelencod defin class separ python script execut call custom function train python script deploy script score python file score import score file success question import custom python modul deploy environ thank advanc edit yml file project environ depend python interpret version current support later python pip multilabelencod scikit learn default panda channel conda forg",
        "Question_preprocessed_content":"import custom python modul deploy environ sklearn model train model save pickl file deploi later librari model train us custom featur encod call multicolumnlabelencod pipelin model defin follow model save work fine deploy step step link deploy fail error understand problem unpickl custom function multilabelencod defin class separ python script call custom function train python script deploy script score python file import file success question import custom python modul deploy environ thank advanc edit yml file",
        "Answer_original_content":"fact solut import custom class multicolumnlabelencod pip packag pip instal multiabelencod pass pip packag yml file inferenceconfig environ score file import class follow multilabelencod import multilabelencod def init global model custom encod dfor unpickl model encod multilabelencod multicolumnlabelencod path deploi model model path path join getenv model dir mean model pkl model joblib load model path deploy success import thing us pip packag multilabelencod train pipelin multilabelencod import multilabelencod pipe pipelin encod multilabelencod multicolumnlabelencod column mean kmean train pipelin trainedmodel pipe fit",
        "Answer_preprocessed_content":"fact solut import custom class multicolumnlabelencod pip packag pass pip packag yml file inferenceconfig environ file import class follow deploy success import thing us pip packag train pipelin",
        "Question_gpt_summary_original":"the user is encountering challenges with importing a custom python module in their deployment environment, resulting in a \"modulenotfounderror\".",
        "Question_gpt_summary":"user encount challeng import custom python modul deploy environ result modulenotfounderror",
        "Answer_gpt_summary_original":"the solution to the \"modulenotfounderror\" issue when importing a custom python module in a deployment environment is to import the customized class as a pip package and pass it to the .yml file or inferenceconfig of the environment. in the score.py file, import the class and call it for unpickling the model. it is also important to use the same pip package in the training pipeline.",
        "Answer_gpt_summary":"solut modulenotfounderror issu import custom python modul deploy environ import custom class pip packag pass yml file inferenceconfig environ score file import class unpickl model import us pip packag train pipelin"
    },
    {
        "Question_id":null,
        "Question_title":"How to change metrics computation and recompute",
        "Question_body":"<p>Hi all,<br>\nI trained several models and computed a simple metrics summary for each of them. Now, those models (and their corresponding metrics.json files) are versioned by dvc and git and there is a git tag associated with each model.<br>\nHaving the models trained and stored, I would like to enrich the metrics summary and go back and recompute it for each model \u2013 and store it in git as with the simple summary. This means the last part of the pipeline, say compute_metrics.py, will be changed and I would like to run dvc repro again. Is there a simple way of recomputing the metrics for all models (that is for all tags)?<\/p>\n<p>Thank you,<br>\nMichal<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1575979302220,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":420.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-change-metrics-computation-and-recompute\/278",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-12-10T17:46:20.045Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/michal.najman\">@michal.najman<\/a><br>\nSo what you are looking for is probably <code>dvc repro<\/code><br>\n<a href=\"https:\/\/dvc.org\/doc\/command-reference\/repro#repro\">docs<\/a><br>\nIf you have only modified the code responsible for metrics calculation, dvc should detect that and recalculate only metrics. If you want to be sure, you can explicitly tell dvc to reproduce metrics stage only with <code>--single-item<\/code> (eg <code>dvc repro --single-item metrics_calc.dvc<\/code>) param. I am afraid that currently, we do not support <code>repro<\/code> for multiple git revisions, which would probably have to be done manually. Maybe you want to create feature request on GitHub, with a short description how would you expect that to work?<\/p>",
                "Answer_score":46.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"chang metric comput recomput train model comput simpl metric summari model correspond metric json file version git git tag associ model have model train store like enrich metric summari recomput model store git simpl summari mean pipelin comput metric chang like run repro simpl wai recomput metric model tag thank michal",
        "Question_preprocessed_content":"chang metric comput recomput train model comput simpl metric summari model version git git tag associ model have model train store like enrich metric summari recomput model store git simpl summari mean pipelin chang like run repro simpl wai recomput metric model thank michal",
        "Answer_original_content":"michal najman look probabl repro doc modifi code respons metric calcul detect recalcul metric want sure explicitli tell reproduc metric stage singl item repro singl item metric calc param afraid current support repro multipl git revis probabl manual mayb want creat featur request github short descript expect work",
        "Answer_preprocessed_content":"look probabl doc modifi code respons metric calcul detect recalcul metric want sure explicitli tell reproduc metric stage param afraid current support multipl git revis probabl manual mayb want creat featur request github short descript expect work",
        "Question_gpt_summary_original":"the user michal is looking for a way to change the metrics computation and recompute them for all models associated with git tags.",
        "Question_gpt_summary":"user michal look wai chang metric comput recomput model associ git tag",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n- use repro docs to recalculate metrics for all models associated with git tags.\n- use the --single-item parameter to explicitly tell to reproduce metrics stage only.\n- create a feature request on github for repro support for multiple git revisions.",
        "Answer_gpt_summary":"possibl solut answer us repro doc recalcul metric model associ git tag us singl item paramet explicitli tell reproduc metric stage creat featur request github repro support multipl git revis"
    },
    {
        "Question_id":50068941.0,
        "Question_title":"SageMaker Tensorflow - how to write my serving_input_fn()",
        "Question_body":"<p>I'm pretty new to Tensorflow and SageMaker and I'm trying to figure out how to write my <code>serving_input_fn()<\/code>. I've tried a number of ways to do it, but to no avail. <\/p>\n\n<p>my input function has 3 feature columns: <code>amount_normalized, x_month and y_month<\/code>:<\/p>\n\n<pre><code>def construct_feature_columns():\n    amount_normalized = tf.feature_column.numeric_column(key='amount_normalized')\n    x_month = tf.feature_column.numeric_column(key='x_month')\n    y_month = tf.feature_column.numeric_column(key='y_month')\n    return set([amount_normalized, x_month, y_month])\n<\/code><\/pre>\n\n<p>I want to be able to call my deployed model using something like <code>deployed_model.predict([1.23,0.3,0.8])<\/code> <\/p>\n\n<p>Where the first element is <code>amount_normalized<\/code>, second is <code>x_month<\/code> third is <code>y_month<\/code><\/p>\n\n<p>I've tried this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(params):\n    feature_placeholders = {\n      key : tf.placeholder(tf.float32, [None]) \\\n        for key in FEATURES\n    }\nreturn tf.estimator.export.build_raw_serving_input_receiver_fn(feature_placeholders)()\n<\/code><\/pre>\n\n<p>But all I get is:\n<code>An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"\".<\/code><\/p>\n\n<p>Any help would be <strong>really<\/strong> appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1524856485467,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1337.0,
        "Answer_body":"<p>Posting this here in case anyone else has this issue.<\/p>\n\n<p>After a bunch of trial and error I managed to solve my issue by writing my serving input function like this:<\/p>\n\n<pre><code>FEATURES = ['amount_normalized', 'x_month', 'y_month']\ndef serving_input_fn(hyperparameters):\n    feature_spec = {\n        key : tf.FixedLenFeature(shape=[], dtype = tf.float32) \\\n          for key in FEATURES\n    }\n    return tf.estimator.export.build_parsing_serving_input_receiver_fn(feature_spec)()\n<\/code><\/pre>\n\n<p>I can then call my deployed model by passing in a hash:<\/p>\n\n<pre><code>deployed_model.predict({\"amount_normalized\": 2.3, \"x_month\": 0.2, \"y_month\": -0.3})\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50068941",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1525019634972,
        "Question_original_content":"tensorflow write serv input pretti new tensorflow try figur write serv input tri number wai avail input function featur column normal month month def construct featur column normal featur column numer column kei normal month featur column numer column kei month month featur column numer column kei month return set normal month month want abl deploi model like deploi model predict element normal second month month tri featur normal month month def serv input param featur placehold kei placehold float kei featur return estim export build raw serv input receiv featur placehold error occur modelerror call invokeendpoint oper receiv server error model messag help appreci",
        "Question_preprocessed_content":"tensorflow write pretti new tensorflow try figur write tri number wai avail input function featur column want abl deploi model like element second tri help appreci",
        "Answer_original_content":"post case issu bunch trial error manag solv issu write serv input function like featur normal month month def serv input hyperparamet featur spec kei fixedlenfeatur shape dtype float kei featur return estim export build pars serv input receiv featur spec deploi model pass hash deploi model predict normal month month",
        "Answer_preprocessed_content":"post case issu bunch trial error manag solv issu write serv input function like deploi model pass hash",
        "Question_gpt_summary_original":"the user is encountering challenges in writing their serving_input_fn() function for their tensorflow model, which has three feature columns: amount_normalized, x_month and y_month, and is receiving an error when attempting to call the deployed model.",
        "Question_gpt_summary":"user encount challeng write serv input function tensorflow model featur column normal month month receiv error attempt deploi model",
        "Answer_gpt_summary_original":"the solution to the challenge of writing a serving_input_fn() function for a tensorflow model with three feature columns is to define the feature columns as a list, create a feature specification dictionary, and use the build_parsing_serving_input_receiver_fn() function to return a serving input function. the deployed model can then be called by passing in a hash of the feature values.",
        "Answer_gpt_summary":"solut challeng write serv input function tensorflow model featur column defin featur column list creat featur specif dictionari us build pars serv input receiv function return serv input function deploi model call pass hash featur valu"
    },
    {
        "Question_id":63571552.0,
        "Question_title":"joblib.dump() fails when saving model to temporary data store in AMLS",
        "Question_body":"<p>I am training a model using AMLS. I have a training pipeline in which step 1 trains a model then saves the output in temporary datastore model_folder using<\/p>\n<pre><code>os.makedirs(output_folder, exist_ok=True)\noutput_path = output_folder + &quot;\/model.pkl&quot;\njoblib.dump(value=model, filename=output_path)\n<\/code><\/pre>\n<p>Step 2 loads the model and registers it. The model folder is defined in the pipeline as<\/p>\n<pre><code>model_folder = PipelineData(&quot;model_folder&quot;, datastore=ws.get_default_datastore())\n<\/code><\/pre>\n<p>However, step 1 fails when it tries to save the model with the following ServiceError:<\/p>\n<p>Failed to upload outputs due to Exception: Microsoft.RelInfra.Common.Exceptions.OperationFailedException: Cannot upload output xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxxxxxxx. ---&gt; Microsoft.WindowsAzure.Storage.StorageException: This request is not authorized to perform this operation using this permission.<\/p>\n<p>How can I solve this? Earlier in my code I had no problem interacting with the default datastore using<\/p>\n<pre><code>default_ds = ws.get_default_datastore()\ndefault_ds.upload_files(...)\n<\/code><\/pre>\n<p>My <code>70_driver_log.txt<\/code> is as follows:<\/p>\n<pre><code>[2020-08-25T04:03:27.315114] Entering context manager injector.\n[context_manager_injector.py] Command line Options: Namespace(inject=['ProjectPythonPath:context_managers.ProjectPythonPath', 'RunHistory:context_managers.RunHistory', 'TrackUserError:context_managers.TrackUserError'], invocation=['train_word2vec.py', '--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10'])\nStarting the daemon thread to refresh tokens in background for process with pid = 113\nEntering Run History Context Manager.\nCurrent directory:  \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\nPreparing to call script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\nAfter variable expansion, calling script [ train_word2vec.py ] with arguments: ['--output_folder', '\/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder', '--model_type', 'WO', '--training_field', 'task_title', '--regex', '1', '--stopword_removal', '1', '--tokenize_basic', '0', '--remove_punctuation', '0', '--autocorrect', '0', '--lemmatization', '1', '--word_vector_length', '152', '--model_learning_rate', '0.025', '--model_min_count', '0', '--model_window', '7', '--num_epochs', '10']\n\nScript type = None\n[nltk_data] Downloading package stopwords to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/stopwords.zip.\n[nltk_data] Downloading package wordnet to \/root\/nltk_data...\n[nltk_data]   Unzipping corpora\/wordnet.zip.\nOUTPUT FOLDER: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/aiworkspace\/azureml\/xxxxx\/mounts\/workspaceblobstore\/azureml\/xxxxx\/model_folder\nLoading SQL data...\nLoading abbreviation data...\n\/azureml-envs\/azureml_xxxxx\/lib\/python3.6\/site-packages\/pandas\/core\/indexing.py:1783: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/user_guide\/indexing.html#returning-a-view-versus-a-copy\n  self.obj[item_labels[indexer[info_axis]]] = value\nPre-processing data...\nSuccesfully pre-processed the the text data\nTraining Word2Vec model...\nSaving the model...\nStarting the daemon thread to refresh tokens in background for process with pid = 113\n\n\nThe experiment completed successfully. Finalizing run...\n[2020-08-25T04:03:52.293994] TimeoutHandler __init__\n[2020-08-25T04:03:52.294149] TimeoutHandler __enter__\nCleaning up all outstanding Run operations, waiting 300.0 seconds\n2 items cleaning up...\nCleanup took 0.44109439849853516 seconds\n[2020-08-25T04:03:52.818991] TimeoutHandler __exit__\n2020\/08\/25 04:04:00 logger.go:293: Process Exiting with Code:  0\n<\/code><\/pre>\n<p>My arg parse arguments include<\/p>\n<pre><code>parser.add_argument('--output_folder', type=str, dest='output_folder', default=&quot;output_folder&quot;, help='output folder')\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_time":1598325386190,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1598328703460,
        "Question_score":2.0,
        "Question_view_count":571.0,
        "Answer_body":"<p>Fixed this problem by adding my AMLS workspace to a 'storage blob data contributor' role in the AMLS default storage account. It seemly like usually this role is added by default, but it didn't happen in my case.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63571552",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1598745717623,
        "Question_original_content":"joblib dump fail save model temporari data store aml train model aml train pipelin step train model save output temporari datastor model folder makedir output folder exist true output path output folder model pkl joblib dump valu model filenam output path step load model regist model folder defin pipelin model folder pipelinedata model folder datastor default datastor step fail tri save model follow serviceerror fail upload output except microsoft relinfra common except operationfailedexcept upload output microsoft windowsazur storag storageexcept request author perform oper permiss solv earlier code problem interact default datastor default default datastor default upload file driver log txt follow enter context manag injector context manag injector command line option namespac inject projectpythonpath context manag projectpythonpath runhistori context manag runhistori trackusererror context manag trackusererror invoc train wordvec output folder mnt batch task share root job aiworkspac mount workspaceblobstor model folder model type train field task titl regex stopword remov token basic remov punctuat autocorrect lemmat word vector length model learn rate model min count model window num epoch start daemon thread refresh token background process pid enter run histori context manag current directori mnt batch task share root job aiworkspac mount workspaceblobstor prepar script train wordvec argument output folder mnt batch task share root job aiworkspac mount workspaceblobstor model folder model type train field task titl regex stopword remov token basic remov punctuat autocorrect lemmat word vector length model learn rate model min count model window num epoch variabl expans call script train wordvec argument output folder mnt batch task share root job aiworkspac mount workspaceblobstor model folder model type train field task titl regex stopword remov token basic remov punctuat autocorrect lemmat word vector length model learn rate model min count model window num epoch script type nltk data download packag stopword root nltk data nltk data unzip corpora stopword zip nltk data download packag wordnet root nltk data nltk data unzip corpora wordnet zip output folder mnt batch task share root job aiworkspac mount workspaceblobstor model folder load sql data load abbrevi data env lib python site packag panda core index settingwithcopywarn valu try set copi slice datafram try loc row index col index valu instead caveat document http panda pydata org panda doc stabl user guid index html return view versu copi self obj item label index info axi valu pre process data succesfulli pre process text data train wordvec model save model start daemon thread refresh token background process pid experi complet successfulli final run timeouthandl init timeouthandl enter clean outstand run oper wait second item clean cleanup took second timeouthandl exit logger process exit code arg pars argument includ parser add argument output folder type str dest output folder default output folder help output folder",
        "Question_preprocessed_content":"fail save model temporari data store aml train model aml train pipelin step train model save output temporari datastor step load model regist model folder defin pipelin step fail tri save model follow serviceerror fail upload output except upload output request author perform oper permiss solv earlier code problem interact default datastor follow arg pars argument includ",
        "Answer_original_content":"fix problem ad aml workspac storag blob data contributor role aml default storag account seemli like usual role ad default happen case",
        "Answer_preprocessed_content":"fix problem ad aml workspac storag blob data contributor role aml default storag account seemli like usual role ad default happen case",
        "Question_gpt_summary_original":"the user is encountering an issue with joblib.dump() failing when saving a model to a temporary data store in amls, resulting in a microsoft.windowsazure.storage.storageexception.",
        "Question_gpt_summary":"user encount issu joblib dump fail save model temporari data store aml result microsoft windowsazur storag storageexcept",
        "Answer_gpt_summary_original":"the solution to the issue with joblib.dump() failing when saving a model to a temporary data store in amls is to add the amls workspace to a 'storage blob data contributor' role in the amls default storage account. this role is usually added by default, but in some cases, it may need to be added manually.",
        "Answer_gpt_summary":"solut issu joblib dump fail save model temporari data store aml add aml workspac storag blob data contributor role aml default storag account role usual ad default case need ad manual"
    },
    {
        "Question_id":68402406.0,
        "Question_title":"MLflow webserver returns 400 status, \"Incompatible input types for column X. Can not safely convert float64 to <U0.\"",
        "Question_body":"<p>I am implementing an anomaly detection web service using <code>MLflow<\/code> and <code>sklearn.pipeline.Pipeline()<\/code>. The aim of the model is to detect web crawlers using server log and <code>response_length<\/code> column is one of my features. After serving model, for testing the web service I send below request that contains the 20 first columns of the train data.<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>$ curl  --location --request POST '127.0.0.1:8000\/invocations'\n        --header 'Content-Type: text\/csv' \\\n        --data-binary 'datasets\/test.csv'\n<\/code><\/pre>\n<p>But response of the web server has status code 400 (BAD REQUEST) and this JSON body:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n    &quot;error_code&quot;: &quot;BAD_REQUEST&quot;,\n    &quot;message&quot;: &quot;Incompatible input types for column response_length. Can not safely convert float64 to &lt;U0.&quot;\n}\n<\/code><\/pre>\n<p>Here is the model compilation MLflow Tracking component log:<\/p>\n<pre class=\"lang-bash prettyprint-override\"><code>[Pipeline] ......... (step 1 of 3) Processing transform, total=11.8min\n[Pipeline] ............... (step 2 of 3) Processing pca, total=   4.8s\n[Pipeline] ........ (step 3 of 3) Processing rule_based, total=   0.0s\n2021\/07\/16 04:55:12 WARNING mlflow.sklearn: Training metrics will not be recorded because training labels were not specified. To automatically record training metrics, provide training labels as inputs to the model training function.\n2021\/07\/16 04:55:12 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: &quot;\/home\/matin\/workspace\/Rahnema College\/venv\/lib\/python3.8\/site-packages\/mlflow\/models\/signature.py:129: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values &lt;https:\/\/www.mlflow.org\/docs\/latest\/models.html#handling-integers-with-missing-values&gt;`_ for more details.&quot;\nLogged data and model in run: 8843336f5c31482c9e246669944b1370\n\n---------- logged params ----------\n{'memory': 'None',\n 'pca': 'PCAEstimator()',\n 'rule_based': 'RuleBasedEstimator()',\n 'steps': &quot;[('transform', &lt;log_transformer.LogTransformer object at &quot;\n          &quot;0x7f05a8b95760&gt;), ('pca', PCAEstimator()), ('rule_based', &quot;\n          'RuleBasedEstimator())]',\n 'transform': '&lt;log_transformer.LogTransformer object at 0x7f05a8b95760&gt;',\n 'verbose': 'True'}\n\n---------- logged metrics ----------\n{}\n\n---------- logged tags ----------\n{'estimator_class': 'sklearn.pipeline.Pipeline', 'estimator_name': 'Pipeline'}\n\n---------- logged artifacts ----------\n['model\/MLmodel',\n 'model\/conda.yaml',\n 'model\/model.pkl',\n 'model\/requirements.txt']\n<\/code><\/pre>\n<p>Could anyone tell me exactly how I can fix this model serve problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626398499507,
        "Question_favorite_count":null,
        "Question_last_edit_time":1657456057947,
        "Question_score":1.0,
        "Question_view_count":787.0,
        "Answer_body":"<p>The problem caused by <code>mlflow.utils.autologging_utils<\/code> WARNING.<\/p>\n<p>When the model is created, data input signature is saved on the <code>MLmodel<\/code> file with some.\nYou should change <code>response_length<\/code> signature input type from <code>string<\/code> to <code>double<\/code> by replacing<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;double&quot;}\n<\/code><\/pre>\n<p>instead of<\/p>\n<pre><code>{&quot;name&quot;: &quot;response_length&quot;, &quot;type&quot;: &quot;string&quot;}\n<\/code><\/pre>\n<p>so it doesn't need to be converted. After serving the model with edited <code>MLmodel<\/code> file, the web server worked as expected.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68402406",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1626398499507,
        "Question_original_content":"webserv return statu incompat input type column safe convert float implement anomali detect web servic sklearn pipelin pipelin aim model detect web crawler server log respons length column featur serv model test web servic send request contain column train data curl locat request post invoc header content type text csv data binari dataset test csv respons web server statu code bad request json bodi error code bad request messag incompat input type column respons length safe convert float detail log data model run fcceb log param memori pca pcaestim rule base rulebasedestim step transform pca pcaestim rule base rulebasedestim transform verbos true log metric log tag estim class sklearn pipelin pipelin estim pipelin log artifact model mlmodel model conda yaml model model pkl model requir txt tell exactli fix model serv problem",
        "Question_preprocessed_content":"webserv return statu incompat input type column safe convert float implement anomali detect web servic aim model detect web crawler server log column featur serv model test web servic send request contain column train data respons web server statu code json bodi model compil track compon log tell exactli fix model serv problem",
        "Answer_original_content":"problem caus util autolog util warn model creat data input signatur save mlmodel file chang respons length signatur input type string doubl replac respons length type doubl instead respons length type string need convert serv model edit mlmodel file web server work expect",
        "Answer_preprocessed_content":"problem caus warn model creat data input signatur save file chang signatur input type replac instead need convert serv model edit file web server work expect",
        "Question_gpt_summary_original":"the user is encountering a challenge with their webserver returning a 400 status code and an \"incompatible input types for column x. can not safely convert float64 to <u0.\" error when attempting to serve a model using sklearn.pipeline.pipeline().",
        "Question_gpt_summary":"user encount challeng webserv return statu code incompat input type column safe convert float error attempt serv model sklearn pipelin pipelin",
        "Answer_gpt_summary_original":"the solution to the problem of encountering a 400 status code and an \"incompatible input types for column x. can not safely convert float64 to <u0.\" error when serving a model using sklearn.pipeline.pipeline() is to change the response_length signature input type from string to double by replacing {\"name\": \"response_length\", \"type\": \"string\"} with {\"name\": \"response_length\", \"type\": \"double\"} in the mlmodel file. this will prevent the need for conversion and allow the web server to work as expected.",
        "Answer_gpt_summary":"solut problem encount statu code incompat input type column safe convert float error serv model sklearn pipelin pipelin chang respons length signatur input type string doubl replac respons length type string respons length type doubl mlmodel file prevent need convers allow web server work expect"
    },
    {
        "Question_id":null,
        "Question_title":"Taking a long time to download artifact of only 300mb",
        "Question_body":"<p>Its taking ages to download an artifact of only 300mb. Its there anyway to enable parallel download ? add multiprocessing ?<\/p>\n<p>wandb: Downloading large artifact Data-Split-802020:v0, 303.21MB. 148470 files\u2026<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1660231132264,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":45.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/taking-a-long-time-to-download-artifact-of-only-300mb\/2909",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-15T22:34:21.610Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/usman94\">@usman94<\/a> , we are actively focusing\/working on improving the performance of Artifact uploads\/downloads. There isn\u2019t an ability to split up and artifact when downloading it. This is under consideration for implementation. Once the engineering team has provided an update, I will let you know.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-14T22:34:53.434Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"take long time download artifact take ag download artifact enabl parallel download add multiprocess download larg artifact data split file",
        "Question_preprocessed_content":"take long time download artifact take ag download artifact enabl parallel download add multiprocess download larg artifact file",
        "Answer_original_content":"usman activ focus work improv perform artifact upload download isnt abil split artifact download consider implement engin team provid updat let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"activ improv perform artifact isnt abil split artifact download consider implement engin team provid updat let know topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering a challenge of taking a long time to download an artifact of only 300mb.",
        "Question_gpt_summary":"user encount challeng take long time download artifact",
        "Answer_gpt_summary_original":"there is currently no solution to split up an artifact when downloading it, but the engineering team is working on improving the performance of artifact uploads\/downloads. the user will be updated once there is progress on this issue.",
        "Answer_gpt_summary":"current solut split artifact download engin team work improv perform artifact upload download user updat progress issu"
    },
    {
        "Question_id":72425907.0,
        "Question_title":"How to log a tensorflow model with mlflow.tensorflow.log_model (error module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants')",
        "Question_body":"<p>I am trying to log a trained model with MLFlow using mlflow.tensorflow.log_model.<\/p>\n<p>After training a simple sequential tf model<\/p>\n<pre><code>history = binary_model.fit(train_ds, validation_data=val_ds, epochs=num_epochs)\n<\/code><\/pre>\n<p>I am trying to log it:<\/p>\n<pre><code>    from tensorflow.python.saved_model import signature_constants\n    tag=[tf.saved_model.tag_constants.SERVING]\n    key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n\n    mlflow.tensorflow.log_model(tf_saved_model_dir=saved_model_path,\n                                tf_meta_graph_tags=tag,\n                                tf_signature_def_key=key,\n                                artifact_path=&quot;tf-models&quot;,\n                                registered_model_name=model_name)\n<\/code><\/pre>\n<p>but I get the error:<\/p>\n<pre><code>    AttributeError                            Traceback (most recent call last)\n    \/var\/folders\/2k\/g7p7j2gx6v54vkwv3v401h2m0000gn\/T\/ipykernel_73638\/562549064.py in &lt;module&gt;\n          1 from tensorflow.python.saved_model import signature_constants\n    ----&gt; 2 tag=[tf.saved_model.tag_constants.SERVING]\n          3 key=signature_constants.DEFAULT_SERVING_SIGNATURE_DEF_KEY\n          4 \n          5 mlflow.tensorflow.log_model(tf_saved_model_dir=saved_model_path,\n\n    AttributeError: module 'tensorflow._api.v2.saved_model' has no attribute 'tag_constants'\n<\/code><\/pre>\n<p>Any idea how to get the tags and keys correctly from the model to log it in MLFlow?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1653845789630,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":319.0,
        "Answer_body":"<p>The <code>tag_constants<\/code> is in <code>tf.compat.v1.saved_model<\/code>.<\/p>\n<p>To resolve the error replace this line<\/p>\n<pre><code>tag=[tf.saved_model.tag_constants.SERVING]\n<\/code><\/pre>\n<p>with this<\/p>\n<pre><code>tag=[tf.compat.v1.saved_model.tag_constants.SERVING]\n<\/code><\/pre>\n<p>Please refer <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/compat\/v1\/saved_model\/tag_constants\" rel=\"nofollow noreferrer\">this<\/a> for more details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72425907",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1655094569432,
        "Question_original_content":"log tensorflow model tensorflow log model error modul tensorflow api save model attribut tag constant try log train model tensorflow log model train simpl sequenti model histori binari model fit train valid data val epoch num epoch try log tensorflow python save model import signatur constant tag save model tag constant serv kei signatur constant default serv signatur def kei tensorflow log model save model dir save model path meta graph tag tag signatur def kei kei artifact path model regist model model error attributeerror traceback recent var folder gpjgxvvkwvvhmgn ipykernel tensorflow python save model import signatur constant tag save model tag constant serv kei signatur constant default serv signatur def kei tensorflow log model save model dir save model path attributeerror modul tensorflow api save model attribut tag constant idea tag kei correctli model log thank advanc",
        "Question_preprocessed_content":"log tensorflow model try log train model train simpl sequenti model try log error idea tag kei correctli model log thank advanc",
        "Answer_original_content":"tag constant compat save model resolv error replac line tag save model tag constant serv tag compat save model tag constant serv refer detail",
        "Answer_preprocessed_content":"resolv error replac line refer detail",
        "Question_gpt_summary_original":"the user is encountering an attributeerror when attempting to log a trained tensorflow model using .tensorflow.log_model.",
        "Question_gpt_summary":"user encount attributeerror attempt log train tensorflow model tensorflow log model",
        "Answer_gpt_summary_original":"the solution to resolve the attributeerror encountered when logging a trained tensorflow model using .tensorflow.log_model is to replace the line tag=[tf.saved_model.tag_constants.serving] with tag=[tf.compat.v1.saved_model.tag_constants.serving]. this change is required because the tag_constants is in tf.compat.v1.saved_model.",
        "Answer_gpt_summary":"solut resolv attributeerror encount log train tensorflow model tensorflow log model replac line tag save model tag constant serv tag compat save model tag constant serv chang requir tag constant compat save model"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML - Deployed to Inference Cluster throws 500 Server Error - MissingFeaturesError",
        "Question_body":"We have an Azure ML model we are ready to deploy to an http endpoint for consumption and testing.\n\nWe are using this tutorial (https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy) as the jumping off point for deploying our ml model. We have created an inference cluster, converted the training model to a real-time inference model and deployed. Deployment looks successful. However, when testing (both via the Test tab in the Azure ML Workspace and via http POST) the server throws a 500. The MissingFeaturesError follows:\n\n\n\n\nFile \"\/azureml-envs\/azureml_9b50686470a92ca74f0d62e2629faaec\/lib\/python3.6\/site-packages\/azureml\/studio\/modules\/ml\/common\/base_learner.py\", line 289, in _validate_no_missing_feature\nErrorMapping.throw(MissingFeaturesError(required_feature_name=';'.join(missing_feature_list)))\n> missing_feature_list = ['Miles', 'Age', 'Gender', 'MarriagetPlans']\n\n\nFile \"\/azureml-envs\/azureml_9b50686470a92ca74f0d62e2629faaec\/lib\/python3.6\/site-packages\/azureml\/studio\/common\/error.py\", line 814, in throw\nraise err\n> err = MissingFeaturesError('Features for Miles;Age;Gender;MarriagetPlans required but not provided.',)\n\n\nMissingFeaturesError: Features for Miles;Age;Gender;MarriagetPlans required but not provided.\n\nIn both test cases (via Test tab in Azure and http POST to the endpoint) all the required data is indeed provided. The request body definitely includes Miles, Age, Gender, MarriagetPlans.\n\nWhat is going on here?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1614113525043,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/285607\/azure-ml-deployed-to-inference-cluster-throws-500.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-24T03:41:55.38Z",
                "Answer_score":0,
                "Answer_body":"You can add a additional tag \"azure-machine-learning\" to make more people know your problem, I believe it's problem of AML studio",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-26T15:00:09.507Z",
                "Answer_score":0,
                "Answer_body":"bump Still a problem.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"deploi infer cluster throw server error missingfeatureserror model readi deploi http endpoint consumpt test tutori http doc microsoft com azur machin learn tutori design automobil price deploi jump point deploi model creat infer cluster convert train model real time infer model deploi deploy look success test test tab workspac http post server throw missingfeatureserror follow file env bacafdefaaec lib python site packag studio modul common base learner line valid miss featur errormap throw missingfeatureserror requir featur join miss featur list miss featur list mile ag gender marriagetplan file env bacafdefaaec lib python site packag studio common error line throw rais err err missingfeatureserror featur mile ag gender marriagetplan requir provid missingfeatureserror featur mile ag gender marriagetplan requir provid test case test tab azur http post endpoint requir data provid request bodi definit includ mile ag gender marriagetplan go",
        "Question_preprocessed_content":"deploi infer cluster throw server error missingfeatureserror model readi deploi http endpoint consumpt test tutori jump point deploi model creat infer cluster convert train model infer model deploi deploy look success test server throw missingfeatureserror follow file line file line throw rais err err missingfeatureserror missingfeatureserror featur mile ag gender marriagetplan requir provid test case requir data provid request bodi definit includ mile ag gender marriagetplan go",
        "Answer_original_content":"add addit tag azur machin learn peopl know problem believ problem aml studio bump problem",
        "Answer_preprocessed_content":"add addit tag peopl know problem believ problem aml studio bump problem",
        "Question_gpt_summary_original":"the user is encountering a 500 server error with a missingfeatureserror when attempting to deploy their ml model to an http endpoint for consumption and testing, despite providing all the required data in the request body.",
        "Question_gpt_summary":"user encount server error missingfeatureserror attempt deploi model http endpoint consumpt test despit provid requir data request bodi",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer for the 500 server error with a missingfeatureserror. however, the answer suggests adding an additional tag \"azure-machine-learning\" to make more people aware of the problem. the answer also mentions that the issue could be related to a problem with aml studio bump.",
        "Answer_gpt_summary":"specif solut provid answer server error missingfeatureserror answer suggest ad addit tag azur machin learn peopl awar problem answer mention issu relat problem aml studio bump"
    },
    {
        "Question_id":64286191.0,
        "Question_title":"How to add keyboard shortcuts to AWS Ground Truth labeler UI?",
        "Question_body":"<p>I'm using AWS Sagemaker Ground Truth for a Custom Labeling Task that involves editing bounding boxes and their labels.  Ground Truth's UI has built-in keyboard shortcuts for doing things like choosing the label for a box, but it seems to lack shortcuts for other built-in UI elements like &quot;No adjustments needed&quot; or the &quot;Submit&quot; button.<\/p>\n<p>Is there a way to add such shortcuts?  I've looked at the crowd-html-elements for customizing the appearance of the page, but can't find anything in there about keyboard shortcuts.  It doesn't even look like crowd-button or crowd-icon-button support specifying a shortcut as an attribute.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1602271699933,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":412.0,
        "Answer_body":"<p>Could try something like:<\/p>\n<pre><code>document.addEventListener('keydown', function(event) {\n  if (event.shiftKey &amp;&amp; event.keyCode === 13) {\n    document.getElementsByTagName('crowd-bounding-box')[0].shadowRoot.getElementById('nothing-to-adjust').querySelector('label').click();\n  }\n});\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64286191",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1605831050088,
        "Question_original_content":"add keyboard shortcut aw ground truth label ground truth custom label task involv edit bound box label ground truth built keyboard shortcut thing like choos label box lack shortcut built element like adjust need submit button wai add shortcut look crowd html element custom appear page keyboard shortcut look like crowd button crowd icon button support specifi shortcut attribut",
        "Question_preprocessed_content":"add keyboard shortcut aw ground truth label ground truth custom label task involv edit bound box label ground truth keyboard shortcut thing like choos label box lack shortcut element like adjust need submit button wai add shortcut look custom appear page keyboard shortcut look like support specifi shortcut attribut",
        "Answer_original_content":"try like document addeventlisten keydown function event event shiftkei event keycod document getelementsbytagnam crowd bound box shadowroot getelementbyid adjust queryselector label click",
        "Answer_preprocessed_content":"try like",
        "Question_gpt_summary_original":"the user is looking for a way to add keyboard shortcuts to the aws ground truth labeler ui for built-in ui elements such as \"no adjustments needed\" and the \"submit\" button.",
        "Question_gpt_summary":"user look wai add keyboard shortcut aw ground truth label built element adjust need submit button",
        "Answer_gpt_summary_original":"the answer suggests adding a keyboard shortcut to the aws ground truth labeler ui by using a javascript code that listens for a specific key combination and then triggers a click event on the desired ui element. specifically, the code provided targets the \"no adjustments needed\" label using the tag name and id of the relevant html elements.",
        "Answer_gpt_summary":"answer suggest ad keyboard shortcut aw ground truth label javascript code listen specif kei combin trigger click event desir element specif code provid target adjust need label tag relev html element"
    },
    {
        "Question_id":null,
        "Question_title":"Plot tables with best results from runs",
        "Question_body":"<p>Hi,<\/p>\n<p>I would like to create a report not just with graph plots of my metrics but also a table with the best metric results (accuracy, mAP, \u2026). So that the viewer does not have to compare each line of  a run, but can instead look at the table with the scalar best results if he wants to identify the best run.<\/p>\n<p>It seems to me as there is currently no such feature implemented?<\/p>\n<p>Best<br>\nKarol<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1637576914409,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":268.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/plot-tables-with-best-results-from-runs\/1365",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-23T00:13:08.161Z",
                "Answer_body":"<p>Hi Karol,<\/p>\n<p>We do provide such a feature, it is called a Scalar Chart. You should be able to add a scalar chart to any panel by clicking the top right of the panel and selecting \u201cScalar Chart\u201d. You can then define the metric you want to consider in that one chart.<\/p>\n<p>I hope this answers your question.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-23T08:34:53.261Z",
                "Answer_body":"<p>Sorry, but that is not really what I want. The scalar chart can only be used for tracked parameters and then aggregates over all values of that parameter for every run.<br>\nThe result is always just a single value for all runs combined.<br>\nExample: I select the parameter accuracy and choose \u201cAgg=Max\u201d. Then the mean accuracy for every run is computed and then the maximum mean value of all runs is chosen.<br>\nThis is not what I want. I want that the max of a single run is chosen and plotted in a table along the max values of every other run.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-23T18:21:55.008Z",
                "Answer_body":"<p>This is a workflow that we\u2019ll be making much easier in the future, making it easier to get min, max etc. for a given metric for a given run \/ group of runs. In the meantime, you can just log the max value at the end of your runs and get the median of that. We have a convenience method called <code>define_metric<\/code> which you can use to automatically do this for you.<\/p>\n<pre><code class=\"lang-python\">wandb.init(entity=\"wandb\", project=\"define-metric-demo\")\n# define a metric we are interested in the minimum of\nwandb.define_metric(\"loss\", summary=\"min\")\n# define a metric we are interested in the maximum of\nwandb.define_metric(\"acc\", summary=\"max\")\n<\/code><\/pre>\n<p>This will then log the max\/min value of a given logged metric for you.<br>\nHere\u2019s more documentation of this method:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-axes-and-summaries-with-define_metric\">\n  <header class=\"source\">\n      \n\n      <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-axes-and-summaries-with-define_metric\" target=\"_blank\" rel=\"noopener\">docs.wandb.ai<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/362;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/dc5ecc73965b87f6468f2495eed0a5dd9d21ad63_2_690x362.png\" class=\"thumbnail\" width=\"690\" height=\"362\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/dc5ecc73965b87f6468f2495eed0a5dd9d21ad63_2_690x362.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/dc5ecc73965b87f6468f2495eed0a5dd9d21ad63_2_1035x543.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/dc5ecc73965b87f6468f2495eed0a5dd9d21ad63.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/dc5ecc73965b87f6468f2495eed0a5dd9d21ad63_2_10x10.png\"><\/div>\n\n<h3><a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-axes-and-summaries-with-define_metric\" target=\"_blank\" rel=\"noopener\">Log Data with wandb.log<\/a><\/h3>\n\n  <p>Keep track of metrics, videos, custom plots, and more<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-22T18:22:29.688Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"plot tabl best result run like creat report graph plot metric tabl best metric result accuraci map viewer compar line run instead look tabl scalar best result want identifi best run current featur implement best karol",
        "Question_preprocessed_content":"plot tabl best result run like creat report graph plot metric tabl best metric result viewer compar line run instead look tabl scalar best result want identifi best run current featur implement best karol",
        "Answer_original_content":"karol provid featur call scalar chart abl add scalar chart panel click right panel select scalar chart defin metric want consid chart hope answer question thank ramit sorri want scalar chart track paramet aggreg valu paramet run result singl valu run combin exampl select paramet accuraci choos agg max mean accuraci run comput maximum mean valu run chosen want want max singl run chosen plot tabl max valu run workflow make easier futur make easier min max given metric given run group run meantim log max valu end run median conveni method call defin metric us automat init entiti project defin metric demo defin metric interest minimum defin metric loss summari min defin metric interest maximum defin metric acc summari max log max min valu given log metric here document method doc log data log track metric video custom plot topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"karol provid featur call scalar chart abl add scalar chart panel click right panel select scalar chart defin metric want consid chart hope answer question thank ramit sorri want scalar chart track paramet aggreg valu paramet run result singl valu run combin exampl select paramet accuraci choos agg max mean accuraci run comput maximum mean valu run chosen want want max singl run chosen plot tabl max valu run workflow make easier futur make easier min max given metric given run group run meantim log max valu end run median conveni method call us automat log valu given log metric here document method log data log track metric video custom plot topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking for a way to create a report with a table of the best metric results from their runs, so that viewers can easily identify the best run without having to compare each line.",
        "Question_gpt_summary":"user look wai creat report tabl best metric result run viewer easili identifi best run have compar line",
        "Answer_gpt_summary_original":"there is currently no built-in feature to create a table of the best metric results from runs. however, the user can log the max value at the end of their runs and get the median of that using a convenience method called define_metric. this method logs the max\/min value of a given logged metric for the user. the company is also working on making this workflow easier in the future.",
        "Answer_gpt_summary":"current built featur creat tabl best metric result run user log max valu end run median conveni method call defin metric method log max min valu given log metric user compani work make workflow easier futur"
    },
    {
        "Question_id":null,
        "Question_title":"How to create custom templates for model training and building in sagemaker studio?",
        "Question_body":"I am going through documentation provided here , https:\/\/github.com\/aws-samples\/amazon-sagemaker-build-train-deploy and would like to create my own model building, training and deploying templates. can I download\/clone the sagemaker provided templates and start modifying to my own needs. I understand , we need to set up a aws catalog portfolio and products under it, to be able to use such templates. my question is which project do i need to clone and modify , say if i want to build my own training and model building template, which particular code base or code file do i need to change. I assume , the train.py file here -> https:\/\/github.com\/aws-samples\/amazon-sagemaker-build-train-deploy\/blob\/master\/08_projects\/modelbuild\/pipelines\/endtoendmlsm\/train\/train.py would be one i will customize. but once i change these files, how do i use them. how would i set up my custom template, create a product under my catalog? if yes, how to I link my custom train.py code to this new custom sagemaker project template. the documentation or samples only show how to use the prebuild templates under sagemaker project templates, but how do i get my own template pushed there ? can this be done via say cloudformation ?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1663805155971,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":34.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU-fW_jQSjSKiwACGXqFh1sA\/how-to-create-custom-templates-for-model-training-and-building-in-sagemaker-studio",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-22T03:53:38.010Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nThis workshop works through the typical ML process to build and deploy a model to predict the fault on machine based on synthetic datasets. To start building it, you will need to have a AWS account, the link is here.\n\nOnce you have an account, you can start from module 1 from this step. I would recommend follow each module in order and try to implement in your account. This will help you understand the methodology and familiar with the service.\n\nTo modify it, you will need to collect and use your datasets, identifying your business problem. You could try to re-use the same algorithm in the module but it depends on the issue you try to resolve.\n\nHope it helps\n\nThanks,",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"creat custom templat model train build studio go document provid http github com aw sampl amazon build train deploi like creat model build train deploi templat download clone provid templat start modifi need understand need set aw catalog portfolio product abl us templat question project need clone modifi want build train model build templat particular code base code file need chang assum train file http github com aw sampl amazon build train deploi blob master project modelbuild pipelin endtoendmlsm train train custom chang file us set custom templat creat product catalog ye link custom train code new custom project templat document sampl us prebuild templat project templat templat push cloudform",
        "Question_preprocessed_content":"creat custom templat model train build studio go document provid like creat model build train deploi templat provid templat start modifi need understand need set aw catalog portfolio product abl us templat question project need clone modifi want build train model build templat particular code base code file need chang assum file custom chang file us set custom templat creat product catalog ye link custom code new custom project templat document sampl us prebuild templat project templat templat push cloudform",
        "Answer_original_content":"workshop work typic process build deploi model predict fault machin base synthet dataset start build need aw account link account start modul step recommend follow modul order try implement account help understand methodolog familiar servic modifi need collect us dataset identifi busi problem try us algorithm modul depend issu try resolv hope help thank",
        "Answer_preprocessed_content":"workshop work typic process build deploi model predict fault machin base synthet dataset start build need aw account link account start modul step recommend follow modul order try implement account help understand methodolog familiar servic modifi need collect us dataset identifi busi problem try algorithm modul depend issu try resolv hope help thank",
        "Question_gpt_summary_original":"the user is looking for guidance on how to create custom templates for model training and building in studio, including how to link their custom code to a new project template and how to set up their custom template in a catalog portfolio.",
        "Question_gpt_summary":"user look guidanc creat custom templat model train build studio includ link custom code new project templat set custom templat catalog portfolio",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer for creating custom templates for model training and building in studio, linking custom code to a new project template, or setting up a custom template in a catalog portfolio. the answer provides guidance on building and deploying a model to predict machine faults using synthetic datasets, starting with an aws account and following the modules in order. the answer also suggests collecting and using your own datasets and identifying your business problem to modify the methodology.",
        "Answer_gpt_summary":"solut provid answer creat custom templat model train build studio link custom code new project templat set custom templat catalog portfolio answer provid guidanc build deploi model predict machin fault synthet dataset start aw account follow modul order answer suggest collect dataset identifi busi problem modifi methodolog"
    },
    {
        "Question_id":64584295.0,
        "Question_title":"Sagemaker XGBoost Hyperparameter Tuning Error",
        "Question_body":"<p>I am new to Sagemaker and trying to set up a hyperparameter tuning job for xgboost algorithm in Sagemaker. I have very imbalanced data (98% majority class, 2% minority\u00a0class) and would like to use the\u00a0&quot;scale_pos_weight&quot; parameter but the below error happens.<\/p>\n<pre><code>ClientError: An error occurred (ValidationException) when calling the CreateHyperParameterTuningJob operation: The hyperparameter tuning job that you requested has the following untunable hyperparameters: [scale_pos_weight]. For the algorithm, ---------------.us-east-1.amazonaws.com\/xgboost:1, you can tune only [colsample_bytree, lambda, eta, max_depth, alpha, num_round, colsample_bylevel, subsample, min_child_weight, max_delta_step, gamma]. Delete untunable hyperparameters.\u00a0\u00a0\n<\/code><\/pre>\n<p>I have upgraded the sagemaker package, restarted my kernel (I am using juptyer notebook), and instance but the problem still exists.<\/p>\n<p>Does anyone have any ideas why this error happens and how I can fix it? I appreciate the help.<\/p>\n<p>\u00a0\nHere is my code that I followed from an example in AWS.\u00a0<\/p>\n<pre><code>sess = sagemaker.Session()\ncontainer = get_image_uri(region, 'xgboost', '1.0-1')\n\nxgb = sagemaker.estimator.Estimator(container,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 role, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_count=1, \n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 train_instance_type='ml.m4.4xlarge',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 output_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 sagemaker_session=sess)\n\nxgb.set_hyperparameters(eval_metric='auc',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective='binary:logistic',\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 num_round=100,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 rate_drop=0.3,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 tweedie_variance_power=1.4)\n\nhyperparameter_ranges = {'eta': ContinuousParameter(0, 1),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'min_child_weight': ContinuousParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'scale_pos_weight' : ContinuousParameter(700, 800),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'alpha': ContinuousParameter(0, 2),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'max_depth': IntegerParameter(1, 10),\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 'colsample_bytree' : ContinuousParameter(0.1, 0.9)\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 }\nobjective_metric_name = 'validation:auc'\n\ntuner = HyperparameterTuner(xgb,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 objective_metric_name,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 hyperparameter_ranges,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_jobs=10,\n\u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 \u00a0 max_parallel_jobs=2)\n\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train'.format(bucket, prefix), content_type='csv')\ns3_input_validation = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/validation\/'.format(bucket, prefix), content_type='csv')\n\ntuner.fit({'train': s3_input_train, 'validation': s3_input_validation}, include_cls_metadata=False)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1603939946643,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":827.0,
        "Answer_body":"<p>Based on the Sagemaker developer documentation, <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost-tuning.html<\/a>, the hyperparameter <code>scale_pos_weight<\/code> is NOT tunable. The only parameters that you can tune are given in the link.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64584295",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1603941358963,
        "Question_original_content":"xgboost hyperparamet tune error new try set hyperparamet tune job xgboost algorithm imbalanc data major class minorityclass like us scale po weight paramet error happen clienterror error occur validationexcept call createhyperparametertuningjob oper hyperparamet tune job request follow untun hyperparamet scale po weight algorithm east amazonaw com xgboost tune colsampl bytre lambda eta max depth alpha num round colsampl bylevel subsampl min child weight max delta step gamma delet untun hyperparamet upgrad packag restart kernel juptyer notebook instanc problem exist idea error happen fix appreci help code follow exampl aw sess session contain imag uri region xgboost xgb estim estim contain role train instanc count train instanc type xlarg output path output format bucket prefix session sess xgb set hyperparamet eval metric auc object binari logist num round rate drop tweedi varianc power hyperparamet rang eta continuousparamet min child weight continuousparamet scale po weight continuousparamet alpha continuousparamet max depth integerparamet colsampl bytre continuousparamet object metric valid auc tuner hyperparametertun xgb object metric hyperparamet rang max job max parallel job input train input data train format bucket prefix content type csv input valid input data valid format bucket prefix content type csv tuner fit train input train valid input valid includ cl metadata fals",
        "Question_preprocessed_content":"xgboost hyperparamet tune error new try set hyperparamet tune job xgboost algorithm imbalanc data like us paramet error happen upgrad packag restart kernel instanc problem exist idea error happen fix appreci help code follow exampl aw",
        "Answer_original_content":"base develop document http doc aw amazon com latest xgboost tune html hyperparamet scale po weight tunabl paramet tune given link",
        "Answer_preprocessed_content":"base develop document hyperparamet tunabl paramet tune given link",
        "Question_gpt_summary_original":"the user is encountering an error when attempting to set up a hyperparameter tuning job for xgboost algorithm in , due to an untunable hyperparameter.",
        "Question_gpt_summary":"user encount error attempt set hyperparamet tune job xgboost algorithm untun hyperparamet",
        "Answer_gpt_summary_original":"summary: the user is unable to tune the hyperparameter \"scale_pos_weight\" when setting up a hyperparameter tuning job for xgboost algorithm in . according to the developer documentation, this hyperparameter is not tunable. the only parameters that can be tuned are listed in the provided link.",
        "Answer_gpt_summary":"summari user unabl tune hyperparamet scale po weight set hyperparamet tune job xgboost algorithm accord develop document hyperparamet tunabl paramet tune list provid link"
    },
    {
        "Question_id":null,
        "Question_title":"Google vertex AI support is terrible",
        "Question_body":"I usually don't post things like this but I have been trying to work with Google support for Vertex AI for a while. It has been a month on a P2 ticket and no help or support so far. I guess I don't pay the big bucks to get Google's attention. So just wanted to post a warning that is helpful hopefully. Use Vertex AI at your own risk. If something fails you are on your own.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1669150740000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":51.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-vertex-AI-support-is-terrible\/td-p\/492001\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-22T20:59:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"I pay for support by the way. Not expecting free support."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"support terribl usual post thing like try work googl support month ticket help support far guess pai big buck googl attent want post warn help hopefulli us risk fail",
        "Question_preprocessed_content":"support terribl usual post thing like try work googl support month ticket help support far guess pai big buck googl attent want post warn help hopefulli us risk fail",
        "Answer_original_content":"pai support wai expect free support",
        "Answer_preprocessed_content":"pai support wai expect free support",
        "Question_gpt_summary_original":"the user has encountered terrible support from google, with no help or support after a month on a p2 ticket.",
        "Question_gpt_summary":"user encount terribl support googl help support month ticket",
        "Answer_gpt_summary_original":"possible solution: the user can try reaching out to google's paid support services for assistance with their p2 ticket.",
        "Answer_gpt_summary":"possibl solut user try reach googl paid support servic assist ticket"
    },
    {
        "Question_id":null,
        "Question_title":"MLnet Choosing an Algorithm for ranking categories matching a sentence",
        "Question_body":"I am looking to train a model to suggest tags\/categories for a given text string.\n\neg: \"the fox is weak and limping\" = [1-animal],[34-weak],[2667-injury],[16-foot] (a list of tags each with probabilities generated by past associations)\n\nThis data would be trained from a data set of many instances of text each with a corresponding string representing the list of tags that match the text.\n\nIs there a way to featurize the text AND the result tags? And apply an algorithm to cross reference them?\nThe closest I have come is the idea of duplicating each of the training data rows so that each row has only one tag at a time.\n\nI have been researching this question for a week and am thinking the problem is how I am asking it! Everything I have read does not hint at an existing algorithm to match this use case so should I look towards manipulating the data to a different structure.\n\nAny help greatly appreciated.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1636351700077,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/618483\/mlnet-choosing-an-algorithm-for-ranking-categories.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-09T11:34:41.42Z",
                "Answer_score":0,
                "Answer_body":"@Ide-2761 Thanks, Here is the sample to finetune using BERT to identify the tags.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"mlnet choos algorithm rank categori match sentenc look train model suggest tag categori given text string fox weak limp anim weak injuri foot list tag probabl gener past associ data train data set instanc text correspond string repres list tag match text wai featur text result tag appli algorithm cross refer closest come idea duplic train data row row tag time research question week think problem ask read hint exist algorithm match us case look manipul data differ structur help greatli appreci",
        "Question_preprocessed_content":"mlnet choos algorithm rank categori match sentenc look train model suggest given text string fox weak limp data train data set instanc text correspond string repres list tag match text wai featur text result tag appli algorithm cross refer closest come idea duplic train data row row tag time research question week think problem ask read hint exist algorithm match us case look manipul data differ structur help greatli appreci",
        "Answer_original_content":"id thank sampl finetun bert identifi tag",
        "Answer_preprocessed_content":"thank sampl finetun bert identifi tag",
        "Question_gpt_summary_original":"the user is struggling to find an algorithm to match text strings to corresponding tags in order to train a model to suggest tags for a given text string.",
        "Question_gpt_summary":"user struggl algorithm match text string correspond tag order train model suggest tag given text string",
        "Answer_gpt_summary_original":"possible solution: the answer suggests using bert to identify tags and provides a sample for finetuning.",
        "Answer_gpt_summary":"possibl solut answer suggest bert identifi tag provid sampl finetun"
    },
    {
        "Question_id":58307950.0,
        "Question_title":"Azure Machine Learning Compute quota?",
        "Question_body":"<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-manage-quotas#azure-machine-learning-compute\" rel=\"nofollow noreferrer\">Manage and request quotas for Azure resources<\/a> documentation page states that the default quota depends \"on your subscription offer type\". The quota doesn't show up in Azure web portal. Is there a way to find out current quota values using SDK, CLI, REST API?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1570636386640,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":354.0,
        "Answer_body":"<p>You probably want to try something like this command : <\/p>\n\n<pre class=\"lang-sh prettyprint-override\"><code>az vm list-usage --location eastus --out table\n<\/code><\/pre>\n\n<p>It would get you the core usage for the region, which is what is important for deployment of resources.<\/p>\n\n<p>Other choices (az + Powershell) are available <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/networking\/check-usage-against-limits\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Hope this helps!<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58307950",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1570669528590,
        "Question_original_content":"comput quota manag request quota azur resourc document page state default quota depend subscript offer type quota azur web portal wai current quota valu sdk cli rest api",
        "Question_preprocessed_content":"comput quota manag request quota azur resourc document page state default quota depend subscript offer type quota azur web portal wai current quota valu sdk cli rest api",
        "Answer_original_content":"probabl want try like command list usag locat eastu tabl core usag region import deploy resourc choic powershel avail hope help",
        "Answer_preprocessed_content":"probabl want try like command core usag region import deploy resourc choic avail hope help",
        "Question_gpt_summary_original":"the user is trying to find out their current quota values for azure resources, but the azure web portal does not show the quota and the documentation page states that the default quota depends on the subscription offer type.",
        "Question_gpt_summary":"user try current quota valu azur resourc azur web portal quota document page state default quota depend subscript offer type",
        "Answer_gpt_summary_original":"possible solutions: \n- use the command \"az vm list-usage --location eastus --out table\" to get the core usage for the region, which is important for deployment of resources. \n- explore other choices such as using azure powershell.",
        "Answer_gpt_summary":"possibl solut us command list usag locat eastu tabl core usag region import deploy resourc explor choic azur powershel"
    },
    {
        "Question_id":null,
        "Question_title":"Strategy for adding referenced files to an artifact",
        "Question_body":"<p>We are new to WandB and working out best-practices for using referenced-artifacts.<br>\nWe have an S3 bucket were we keep our data corpus, so that it can be shared between machines.<br>\nWe want to use WandB to track these files and to use it to download\/synchronize copies of the datasets to local machines.<br>\nThere seem to be two ways to add files to an artifact:<\/p>\n<ol>\n<li>By-Group: Create the file locally and put it to S3. Repeat with all other data files until done.  Then use the <code>artifact.add_reference()<\/code> command and point it to the S3 prefix\/directory for the files.  This will add the directory and its files to the artifact.  The artifact will report that only a single \u201cfile\u201d exists (since we only added the directory) \u2013 which I think is weird, by the way \u2013 but all the files seem to be there.<\/li>\n<li>One-by-one: create the file locally, put it to S3 and immediately add the S3 file to the artifact.  Repeat until all files are done and then close the artifact and the run.  The artifact will now properly note that n-files have been added.<\/li>\n<\/ol>\n<p>The real question is, when I later execute a <code>download(root=my-local-path)<\/code> operation, will I be able to cleanly load the files from the artifact to my local directory.  That is, without having to fight a path mismatch between the S3 paths and my local paths.<br>\nThat is, if the S3 path is: \/really\/deep\/s3\/path\/to\/my\/dataset\/files<br>\nAnd my local path is: \/Users\/user\/<br>\nCan the files end up here: \/Users\/user\/dataset\/files<\/p>\n<p>Thank you,<br>\nKevin<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1662874185197,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":161.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/strategy-for-adding-referenced-files-to-an-artifact\/3094",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-14T18:15:56.567Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a> , when <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/artifact#download\">downloading<\/a> the Artifact, set the argument <code>recursive= True<\/code>and define your <code>root<\/code> directory to download Artifacts to. The artifacts will download cleanly.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-20T18:17:33.822Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a>, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":15.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-20T18:40:26.157Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> Thank you for the response.  However my core question still stands.  What sets the base path or root path for a given file.  If I add the files one by one how does it know the root path of the file?<br>\nFor example:<br>\nIf the S3 path for a file is: \/really\/deep\/s3\/path\/to\/my\/dataset\/files\/my_file<br>\nAnd my local path is: \/Users\/user\/<br>\nDo the files end up here: \/Users\/user\/dataset\/files\/my_file<br>\nOr do I get: \/Users\/user\/really\/deep\/s3\/path\/to\/my\/dataset\/files\/my_file<\/p>\n<p>How do I control this?<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-05T04:28:17.503Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> Do you have any thoughts on this?  Thank you.<\/p>",
                "Answer_score":10.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-04T04:28:55.592Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"strategi ad referenc file artifact new work best practic referenc artifact bucket data corpu share machin want us track file us download synchron copi dataset local machin wai add file artifact group creat file local repeat data file us artifact add refer command point prefix directori file add directori file artifact artifact report singl file exist ad directori think weird wai file creat file local immedi add file artifact repeat file close artifact run artifact properli note file ad real question later execut download root local path oper abl cleanli load file artifact local directori have fight path mismatch path local path path deep path dataset file local path user user file end user user dataset file thank kevin",
        "Question_preprocessed_content":"strategi ad referenc file artifact new work bucket data corpu share machin want us track file us copi dataset local machin wai add file artifact creat file local repeat data file us command point file add directori file artifact artifact report singl file exist think weird wai file creat file local immedi add file artifact repeat file close artifact run artifact properli note ad real question later execut oper abl cleanli load file artifact local directori have fight path mismatch path local path path local path file end thank kevin",
        "Answer_original_content":"kevinashaw download artifact set argument recurs trueand defin root directori download artifact artifact download cleanli kevinashaw heard go close request like open convers let know mohammadbakir thank respons core question stand set base path root path given file add file know root path file exampl path file deep path dataset file file local path user user file end user user dataset file file user user deep path dataset file file control mohammadbakir thought thank topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"download artifact set argument defin directori download artifact artifact download cleanli heard go close request like convers let know thank respons core question stand set base path root path given file add file know root path file exampl path file local path file end control thought thank topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering challenges with adding referenced files to an artifact, and is trying to figure out the best way to download\/synchronize copies of the datasets to local machines while avoiding path mismatch issues.",
        "Question_gpt_summary":"user encount challeng ad referenc file artifact try figur best wai download synchron copi dataset local machin avoid path mismatch issu",
        "Answer_gpt_summary_original":"the answer suggests setting the argument recursive=true and defining a root directory to download artifacts to when downloading the artifact to avoid path mismatch issues. however, the answer does not provide a solution to the user's core question about how to control the base path or root path for a given file.",
        "Answer_gpt_summary":"answer suggest set argument recurs true defin root directori download artifact download artifact avoid path mismatch issu answer provid solut user core question control base path root path given file"
    },
    {
        "Question_id":null,
        "Question_title":"Information in tables disappearing",
        "Question_body":"<p>Hello!<\/p>\n<p>I have 39 runs in an experiment, all correctly finished with the corresponding tables per run correctly uploaded and available (when clicking on the specific run).<\/p>\n<p>I have 4 different tables, and combine the runs depending on the table ID (runs .summary[\u201ca\u201d], runs .summary[\u201cb\u201d], \u2026).<\/p>\n<p>2 of the combined tables are correctly outputted, but in 1 of the tables there is half the data available, and in the other a message of \u201cno rows to display\u201d is shown. If I re-upload the data of the \u201cno rows to display\u201d table, it is shown properly, but another table becomes empty, with the \u201cno rows to display\u201d message.<\/p>\n<p>Probably is due to the amount of rows can be processed at the same time? The total number of rows per run is 42, so 39x42 = 1638, which shouldn\u2019t be that much?<\/p>\n<p>Thank you!<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1649654781916,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":195.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/information-in-tables-disappearing\/2215",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-12T13:43:39.133Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/agirbau\">@agirbau<\/a>,<br>\nWould you mind sharing a link to the workspace so I can see what is going on?<\/p>\n<p>If you would rather not share here you can email me at <a href=\"mailto:support@wandb.com\">support@wandb.com<\/a><\/p>\n<p>I don\u2019t think this should be an issue with the number of rows since we support up to 200k rows so I would be interested to see what is going on here.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-13T04:29:36.661Z",
                "Answer_body":"<p>Hello Nate,<\/p>\n<p>I will send you an email with the link. Thanks!<\/p>\n<p>Andreu<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-26T22:46:20.173Z",
                "Answer_body":"<p>Hi Andreu,<br>\nI apologize for the delay on this. Are the panels working how you would expect now or is this still an issue? I see that there is data in all of the panels now.<\/p>\n<p>If not, let me know and I will look into this further.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-02T13:23:10.438Z",
                "Answer_body":"<p>Hi Andreu, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-12T04:29:47.510Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"inform tabl disappear hello run experi correctli finish correspond tabl run correctli upload avail click specif run differ tabl combin run depend tabl run summari run summari combin tabl correctli output tabl half data avail messag row displai shown upload data row displai tabl shown properli tabl row displai messag probabl row process time total number row run shouldnt thank",
        "Question_preprocessed_content":"inform tabl disappear hello run experi correctli finish correspond tabl run correctli upload avail differ tabl combin run depend tabl combin tabl correctli output tabl half data avail messag row displai shown data row displai tabl shown properli tabl row displai messag probabl row process time total number row run shouldnt thank",
        "Answer_original_content":"agirbau mind share link workspac go share email support com dont think issu number row support row interest go thank nate hello nate send email link thank andreu andreu apolog delai panel work expect issu data panel let know look thank nate andreu heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"mind share link workspac go share email dont think issu number row support row interest go thank nate hello nate send email link thank andreu andreu apolog delai panel work expect issu data panel let know look thank nate andreu heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering an issue where information in tables is disappearing, with some tables showing a \"no rows to display\" message and others becoming empty, despite the total number of rows not being too large.",
        "Question_gpt_summary":"user encount issu inform tabl disappear tabl show row displai messag despit total number row larg",
        "Answer_gpt_summary_original":"there are no specific solutions mentioned in the answer. the support team has requested the user to share the workspace link to investigate the issue. the support team also mentioned that the disappearing information in tables is unlikely to be caused by the number of rows since they support up to 200k rows. the support team later followed up with the user to check if the issue was resolved and closed the request after not hearing back from the user.",
        "Answer_gpt_summary":"specif solut mention answer support team request user share workspac link investig issu support team mention disappear inform tabl unlik caus number row support row support team later follow user check issu resolv close request hear user"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML real-time inference endpoint deloyment stuck - with deployment state as Transitioning for over 2 hours.",
        "Question_body":"Hi,\n\nI was deploying a real-time inference pipeline into an AKS compute in East US region today. The endpoint deployment state was stuck at Transitioning for over 2 hours and never finished and I had to delete it. A separate deployment to region East US 2 got stuck as well. I was able to deploy the same pipeline to East US the day before yesterday.\n\nI wonder if this is likely an error related to my account\/resources or a system wide issue? Did anyone else encounter the similar issue?\n\nthanks in advance!",
        "Question_answer_count":2,
        "Question_comment_count":6,
        "Question_creation_time":1591823291753,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":"Hello All,\n\nWe have deployed a fix now to all regions and this should be fixed. Could you please retry and let us know if there are any issues.\n\n-Rohit",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/34653\/azure-ml-real-time-inference-endpoint-deloyment-st.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-12T05:42:20.98Z",
                "Answer_score":1,
                "Answer_body":"Hello All,\n\nWe have deployed a fix now to all regions and this should be fixed. Could you please retry and let us know if there are any issues.\n\n-Rohit",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-07-13T13:45:42.92Z",
                "Answer_score":0,
                "Answer_body":"A \"real-time endpoint deployment\" that \"failed\" cannot be deleted\n\nI also can't update the deployment to a working version.\n\nI tried deleting using \"Azure CLI\", \"Azure Portal\", \"Azure ML Studio\" and Python SDK.\n\nNow I'm trying to update the deployment with \"bicep\" but it's been running for 20 minutes now so I think it will fail again.\n\nI also can't delete the endpoint that contains the deployment.\n\nNow that this is day 3, my next try is to delete the entire \"Azure ML\"",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1591940540980,
        "Question_original_content":"real time infer endpoint deloy stuck deploy state transit hour deploi real time infer pipelin ak comput east region todai endpoint deploy state stuck transit hour finish delet separ deploy region east got stuck abl deploi pipelin east dai yesterdai wonder like error relat account resourc wide issu encount similar issu thank advanc",
        "Question_preprocessed_content":"infer endpoint deloy stuck deploy state transit hour deploi infer pipelin ak comput east region todai endpoint deploy state stuck transit hour finish delet separ deploy region east got stuck abl deploi pipelin east dai yesterdai wonder like error relat wide issu encount similar issu thank advanc",
        "Answer_original_content":"hello deploi fix region fix retri let know issu rohit",
        "Answer_preprocessed_content":"hello deploi fix region fix retri let know issu rohit",
        "Question_gpt_summary_original":"the user encountered an issue with deploying a real-time inference pipeline into an aks compute in the east us region, where the endpoint deployment state was stuck at transitioning for over two hours and never finished, and they had to delete it.",
        "Question_gpt_summary":"user encount issu deploi real time infer pipelin ak comput east region endpoint deploy state stuck transit hour finish delet",
        "Answer_gpt_summary_original":"possible solution: the issue with deploying a real-time inference pipeline into an aks compute in the east us region has been fixed. the user can retry the deployment and report any issues if encountered.",
        "Answer_gpt_summary":"possibl solut issu deploi real time infer pipelin ak comput east region fix user retri deploy report issu encount"
    },
    {
        "Question_id":null,
        "Question_title":"ML Studio and Private Endpoint issue",
        "Question_body":"We are trying to setup a Machine Learning Workspace and only have it accessible via Private Endpoint Connection. When we have it this setup and try to connect from our company workstations it loads the page but we get an \"Error Loading recent runs\"\n\n\n\n\n\nIf we run this from a VM inside Azure it is fine.\n\nWe do have a VPN Gateway set up to access our on-prem which works for other Vnets we have in Azure. We peered the VNET that ML sits in with VNET where Gateway network is setup. unfortunately we had a 3rd party set up the original connection and did not fully document. We have tried to match all settings we have in the working VNET with the ML Vnet but can't seem to see what we are missing. I also can't seem to find what logs to check to see where connections are being blocked.\nHoping I explained our issue will enough.",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1646161833773,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/755259\/ml-studio-and-private-endpoint-issue.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-30T14:17:17.327Z",
                "Answer_score":1,
                "Answer_body":"So we have resolved the issue. We found that we had to add a rule to allow that traffic to the private endpoint. Once we did that and add adding a Forward Zone lookup in our DNS for the Private IPs resolved our issues.\nAppreciate all that help on this",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"studio privat endpoint issu try setup machin learn workspac access privat endpoint connect setup try connect compani workstat load page error load recent run run insid azur fine vpn gatewai set access prem work vnet azur peer vnet sit vnet gatewai network setup unfortun parti set origin connect fulli document tri match set work vnet vnet miss log check connect block hope explain issu",
        "Question_preprocessed_content":"studio privat endpoint issu try setup machin learn workspac access privat endpoint connect setup try connect compani workstat load page error load recent run run insid azur fine vpn gatewai set access work vnet azur peer vnet sit vnet gatewai network setup unfortun parti set origin connect fulli document tri match set work vnet vnet miss log check connect block hope explain issu",
        "Answer_original_content":"resolv issu add rule allow traffic privat endpoint add ad forward zone lookup dn privat ip resolv issu appreci help",
        "Answer_preprocessed_content":"resolv issu add rule allow traffic privat endpoint add ad forward zone lookup dn privat ip resolv issu appreci help",
        "Question_gpt_summary_original":"the user is encountering challenges in setting up a machine learning workspace that is only accessible via a private endpoint connection, and is having difficulty connecting from their company workstations, troubleshooting the issue, and finding the relevant logs to check.",
        "Question_gpt_summary":"user encount challeng set machin learn workspac access privat endpoint connect have difficulti connect compani workstat troubleshoot issu find relev log check",
        "Answer_gpt_summary_original":"the solution to the challenge of setting up a machine learning workspace that is only accessible via a private endpoint connection is to add a rule to allow traffic to the private endpoint and add a forward zone lookup in the dns for the private ips.",
        "Answer_gpt_summary":"solut challeng set machin learn workspac access privat endpoint connect add rule allow traffic privat endpoint add forward zone lookup dn privat ip"
    },
    {
        "Question_id":null,
        "Question_title":"DVC conflict may arrive if multiple user working on same repository?",
        "Question_body":"<p>Hi,<\/p>\n<p>I am trying out dvc for one of my ML pipeline poc. I see dvc add command to keep track of changes in data files. And i will push the data to S3 using dvc push. In our project multiple people are working together if another person already push the data changes to dvc if im going to push same it will through dvc conflict. In git we are getting git conflict<\/p>\n<p>please let me know<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1535600671377,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":821.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-conflict-may-arrive-if-multiple-user-working-on-same-repository\/86",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-08-30T07:56:02.306Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/anup\">@anup<\/a> !<\/p>\n<p>The only conflict that might occur is <code>git merge<\/code> conflict in respective <code>.dvc<\/code> file, which is easily resolved by hand by simply picking the desired <code>md5:<\/code> field. On the dvc side there would be no conflicts when pushing to <code>s3<\/code>.<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":7.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-09-03T11:29:16.593Z",
                "Answer_body":"<p>Thanks, <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> for your clarification<\/p>",
                "Answer_score":12.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"conflict arriv multipl user work repositori try pipelin poc add command track chang data file push data push project multipl peopl work person push data chang go push conflict git get git conflict let know thank",
        "Question_preprocessed_content":"conflict arriv multipl user work repositori try pipelin poc add command track chang data file push data push project multipl peopl work person push data chang go push conflict git get git conflict let know thank",
        "Answer_original_content":"anup conflict occur git merg conflict respect file easili resolv hand simpli pick desir field conflict push thank ruslan thank kuprus clarif",
        "Answer_preprocessed_content":"conflict occur conflict respect file easili resolv hand simpli pick desir field conflict push thank ruslan thank clarif",
        "Question_gpt_summary_original":"the user is trying to understand how to handle potential conflicts when multiple people are working on the same repository.",
        "Question_gpt_summary":"user try understand handl potenti conflict multipl peopl work repositori",
        "Answer_gpt_summary_original":"possible solutions to handle conflicts when multiple people are working on the same repository are to resolve any git merge conflicts in the respective file by hand and pick the desired md5 field. additionally, there would be no conflicts when pushing to s3.",
        "Answer_gpt_summary":"possibl solut handl conflict multipl peopl work repositori resolv git merg conflict respect file hand pick desir field addition conflict push"
    },
    {
        "Question_id":null,
        "Question_title":"Auth Problems with Machine Learning Execute Pipeline Activity.",
        "Question_body":"Hello. Can anyone help with this error? Can not execute Azure ML activity from ADF.\nEverything was ok, no changes was done but suddenly(two-three days ago) I got this error.\n\n Request sent to Azure ML Service for operation 'submitMLPipelineRun' failed with http status code 'Forbidden'. Error message from Azure ML Service: '{ \"error\": { \"code\": \"UserError\", \"severity\": null, \"message\": \"Identity does not have permissions for Microsoft.MachineLearningServices\/workspaces\/experiments\/runs\/submit\/action, Microsoft.MachineLearningServices\/workspaces\/endpoints\/pipelines\/read actions.\", \"messageFormat\": null, \"messageParameters\": null, \"referenceCode\": null, \"detailsUri\": null, \"target\": null, \"details\": [], \"innerError\": { \"code\": \"ForbiddenError\", \"innerError\": null } '.",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_creation_time":1621952386747,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@DenisBruk-6507\n\nWe have identified the issue and a hot fix is rolling out. It will be fixed in all regions by end of today. Sorry for the experience.\n\n\n\n\nRegards,\nYutong",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/408869\/auth-problems-with-machine-learning-execute-pipeli.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-25T19:35:57.963Z",
                "Answer_score":1,
                "Answer_body":"@DenisBruk-6507\n\nWe have identified the issue and a hot fix is rolling out. It will be fixed in all regions by end of today. Sorry for the experience.\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-05-25T17:34:38.503Z",
                "Answer_score":1,
                "Answer_body":"@YutongTie-5848 I think that this may also be related to our issue. I'm also getting a \"code\": \"ForbiddenError\" and the problem for this user arose at almost exactly the same time that it arose for us.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-05-26T17:41:58.203Z",
                "Answer_score":1,
                "Answer_body":"Hello everyone,\n\nThis issue should be fixed. Please check and let me know if you are still facing any issue.\n\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1621971357963,
        "Question_original_content":"auth problem machin learn execut pipelin activ hello help error execut activ adf chang suddenli dai ago got error request sent servic oper submitmlpipelinerun fail http statu code forbidden error messag servic error code usererror sever null messag ident permiss microsoft machinelearningservic workspac experi run submit action microsoft machinelearningservic workspac endpoint pipelin read action messageformat null messageparamet null referencecod null detailsuri null target null detail innererror code forbiddenerror innererror null",
        "Question_preprocessed_content":"auth problem machin learn execut pipelin activ hello help error execut activ adf chang dai ago got error request sent servic oper submitmlpipelinerun fail http statu code forbidden error messag servic",
        "Answer_original_content":"denisbruk identifi issu hot fix roll fix region end todai sorri experi regard yutong",
        "Answer_preprocessed_content":"identifi issu hot fix roll fix region end todai sorri experi regard yutong",
        "Question_gpt_summary_original":"the user encountered an auth problem with their machine learning execute pipeline activity, resulting in a 'forbidden' error message.",
        "Question_gpt_summary":"user encount auth problem machin learn execut pipelin activ result forbidden error messag",
        "Answer_gpt_summary_original":"possible solution: a hot fix is being rolled out to fix the auth problem with the machine learning execute pipeline activity. the issue will be fixed in all regions by the end of the day.",
        "Answer_gpt_summary":"possibl solut hot fix roll fix auth problem machin learn execut pipelin activ issu fix region end dai"
    },
    {
        "Question_id":null,
        "Question_title":"No Data being exported from 'Export Data' module in Azure ML",
        "Question_body":"Hi,\n\nI am trying to export data from Azure ML to an Azure SQL Database using the 'Export Data' module but the log file contains the following messages and no data is exported to the database.\n\n\"Not exporting to run RunHistory as the exporter is either stopped or there is no data\"\n\n\"Process exiting with code: 0\n\nThere is definitely data flowing to the 'Export Data' module from an 'Execute R Script' module as I have checked the Result dataset.\n\nWould appreciate some assistance.\n\nThank you.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629008927050,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi,\n\nI have resolved this issue. I had set the export table to be dbo.TestTable rather than just TestTable. As the table dbo.TestTable did not exist the 'Export module' created it in the dbo schema so the table name effectively became dbo.dbo.TestTable.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/514067\/no-data-being-exported-from-39export-data39-module.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-16T09:39:07.877Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nI have resolved this issue. I had set the export table to be dbo.TestTable rather than just TestTable. As the table dbo.TestTable did not exist the 'Export module' created it in the dbo schema so the table name effectively became dbo.dbo.TestTable.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1629106747876,
        "Question_original_content":"data export export data modul try export data azur sql databas export data modul log file contain follow messag data export databas export run runhistori export stop data process exit code definit data flow export data modul execut script modul check result dataset appreci assist thank",
        "Question_preprocessed_content":"data export export data modul try export data azur sql databas export data modul log file contain follow messag data export databas export run runhistori export stop data process exit code definit data flow export data modul execut script modul check result dataset appreci assist thank",
        "Answer_original_content":"resolv issu set export tabl dbo testtabl testtabl tabl dbo testtabl exist export modul creat dbo schema tabl effect dbo dbo testtabl",
        "Answer_preprocessed_content":"resolv issu set export tabl testtabl tabl exist export modul creat dbo schema tabl effect",
        "Question_gpt_summary_original":"the user is encountering an issue with the 'export data' module in not exporting data to an azure sql database, despite data being present in the 'execute r script' module.",
        "Question_gpt_summary":"user encount issu export data modul export data azur sql databas despit data present execut script modul",
        "Answer_gpt_summary_original":"the solution to the issue of data not being exported to an azure sql database is to ensure that the export table is set correctly, without including the 'dbo' schema. this is because if the table does not exist, the 'export module' will create it in the 'dbo' schema, resulting in a table name of 'dbo.dbo.testtable'.",
        "Answer_gpt_summary":"solut issu data export azur sql databas ensur export tabl set correctli includ dbo schema tabl exist export modul creat dbo schema result tabl dbo dbo testtabl"
    },
    {
        "Question_id":44367367.0,
        "Question_title":"Converting rating column into boolean column with custom filter in Azure ML",
        "Question_body":"<p>I have a dataset in which the Rating column is an integer column with values ranging from 1 to 10.<\/p>\n\n<p>I would like to convert that column into a simple boolean positive\/negative categorical column, so that if the value is less than 6 it is a negative rating, and if it is greater or equal 6 it would become a positive rating.<\/p>\n\n<p>I'm not sure how to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1496660018143,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":240.0,
        "Answer_body":"<p>Azure Machine Learning allows at least 3 options to do that:<\/p>\n\n<ul>\n<li>Apply SQL Transformation <code>select *,case when rating&lt;6 then 0 else 1 end RatingB from t1<\/code><\/li>\n<li>Execute Python Script <code>return dataframe1.rating[dataframe1.rating &lt; 6] = 0<\/code><\/li>\n<li>Execute R Script <code>dataset1$rating[dataset1$rating &lt; 6] &lt;- 0<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44367367",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1498563773572,
        "Question_original_content":"convert rate column boolean column custom filter dataset rate column integ column valu rang like convert column simpl boolean posit neg categor column valu neg rate greater equal posit rate sure",
        "Question_preprocessed_content":"convert rate column boolean column custom filter dataset rate column integ column valu rang like convert column simpl boolean categor column valu neg rate greater equal posit rate sure",
        "Answer_original_content":"allow option appli sql transform select case rate end ratingb execut python script return datafram rate datafram rate execut script dataset rate dataset rate",
        "Answer_preprocessed_content":"allow option appli sql transform execut python script execut script",
        "Question_gpt_summary_original":"the user is facing a challenge of converting a rating column with integer values ranging from 1 to 10 into a boolean positive\/negative categorical column.",
        "Question_gpt_summary":"user face challeng convert rate column integ valu rang boolean posit neg categor column",
        "Answer_gpt_summary_original":"there are three possible solutions to convert a rating column with integer values ranging from 1 to 10 into a boolean positive\/negative categorical column. the first solution involves applying an sql transformation using a case statement. the second solution involves executing a python script to return a dataframe with the rating column values less than 6 set to 0. the third solution involves executing an r script to set the rating column values less than 6 to 0 in the dataset.",
        "Answer_gpt_summary":"possibl solut convert rate column integ valu rang boolean posit neg categor column solut involv appli sql transform case statement second solut involv execut python script return datafram rate column valu set solut involv execut script set rate column valu dataset"
    },
    {
        "Question_id":null,
        "Question_title":"Output filename for Translate Document API",
        "Question_body":"Hello,I have been developing an iOS mobile app to translate PDF documents and currently using Google Translate documents API without any issue.Is there any way, we can update or change the output filename coming from the API? Currently, the output file name includes all the attributes starting from the project name, bucket name and finally the file name. This won't be appropriate for the users to showcase these things.Is there any configuration to change this filename to more user friendly format?e.g. Input file name - Test_Translation_En_Fr_Sp.pdfOutput filename from Translate API -scantranslatorapp.appspot.com_uploaded_documents_FuOrb0L4tudAZvhZ99IwsFYg83M2_Test_Translation_En_Fr_Sp_fr_translations.pdfIt would be appropriate to showcase only - Test_Translation_En_Fr_Sp_fr_translations.pdf",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661501220000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":43.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Output-filename-for-Translate-Document-API\/td-p\/460318\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-09T08:47:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"You can change the output file prefix for this you need to do a REST request to the as a POST to the API URL:\n\n\n\nhttps:\/\/translation.googleapis.com\/v3\/projects\/PROJECT_NUMBER_OR_ID\/locations\/LOCATION:translateDocument\n\nWith the following request JSON body:\n\n\n\n{\n  \"source_language_code\": \"SOURCE_LANGUAGE\",\n  \"target_language_code\": \"TARGET_LANGUAGE\",\n  \"document_input_config\": {\n    \"mimeType\": \"MIME_TYPE\",\n    \"content\": \"INPUT_BYTE_STREAM\"\n  },\n  \"document_output_config\": {\n    \"gcsDestination\": {\n      \"outputUriPrefix\": \"gs:\/\/OUTPUT_FILE_PREFIX\"\n    }\n  }\n}"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"output filenam translat document api hello develop io mobil app translat pdf document current googl translat document api issu wai updat chang output filenam come api current output file includ attribut start project bucket final file won appropri user showcas thing configur chang filenam user friendli format input file test translat pdfoutput filenam translat api scantranslatorapp appspot com upload document fuorbltudazvhziwsfygm test translat translat pdfit appropri showcas test translat translat pdf",
        "Question_preprocessed_content":"output filenam translat document api hello develop io mobil app translat pdf document current googl translat document api wai updat chang output filenam come api current output file includ attribut start project bucket final file won appropri user showcas configur chang filenam user friendli input file filenam translat api appropri showcas",
        "Answer_original_content":"chang output file prefix need rest request post api url http translat googleapi com project project number locat locat translatedocu follow request json bodi sourc languag code sourc languag target languag code target languag document input config mimetyp mime type content input byte stream document output config gcsdestin outputuriprefix output file prefix",
        "Answer_preprocessed_content":"chang output file prefix need rest request post api url follow request json bodi",
        "Question_gpt_summary_original":"the user is encountering a challenge with the output filename from the google translate documents api, which includes all the attributes starting from the project name, bucket name and finally the file name, and is looking for a way to change the filename to a more user-friendly format.",
        "Question_gpt_summary":"user encount challeng output filenam googl translat document api includ attribut start project bucket final file look wai chang filenam user friendli format",
        "Answer_gpt_summary_original":"the solution to change the output filename from the google translate documents api to a more user-friendly format is to change the output file prefix. this can be done by making a rest request to the api url and providing a json body with the desired output file prefix.",
        "Answer_gpt_summary":"solut chang output filenam googl translat document api user friendli format chang output file prefix make rest request api url provid json bodi desir output file prefix"
    },
    {
        "Question_id":null,
        "Question_title":"Programmatically accessing artifact object very slow for first call for large artifacts",
        "Question_body":"<p>Hello!<\/p>\n<p>I came across this issue recently, and I was wondering whether anything can be done to speed up this process. We are using W&amp;B as source of truth for versioning of our datasets. Each dataset is an artefact in a specific project, and files making up this dataset are added as references (everything is stored on S3).<\/p>\n<p>We sometime need to retrieve the path (including version) to a specific file in the artefact. This is typically very fast (&lt;1s) but for larger artefact (made up of &gt;10K references), the process can slow down significantly and take up to 30 seconds. We realized that this holds true whenever we try to access the artifact for the first time (e.g. getting its digest).<\/p>\n<p>Is it expected that artifacts with a large number of files will result in long wait for the first operation when accessing in programmatically in Python?<\/p>\n<p>We typically use the public API to access the artifact (see below for example) but the same happens when using a run.<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nartifact = api.artifact(\"my_org\/my_project\/my_artifact:latest\")\nfile_info = artifact.get_path(\"example_file_in_artifact\")\ns3_path = file_info.ref\ns3_version = file_info.extra[\"versionID\"]\n<\/code><\/pre>\n<p>Thanks!<\/p>\n<p>Nicolas<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1635527246634,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":301.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/programmatically-accessing-artifact-object-very-slow-for-first-call-for-large-artifacts\/1158",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-01T13:19:04.665Z",
                "Answer_body":"<p>Hi Nicolas, if the artifact is large, it will take longer for it to download for the first operation. As you already stated, if you indicate the path, it\u2019ll be faster to download, and you can also indicate the type of artifact you want in order to get a specific datatype from the artifact by using the following code:<\/p>\n<p>runs = api.runs(\u2026) for run in runs:<br>\nfor artifact in run.logged_artifacts():<br>\nif artifact.type == \u201cmodel\u201d:<br>\nartifact.download()<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-02T17:36:50.999Z",
                "Answer_body":"<p>Hi <span class=\"mention\">@Leslie<\/span>,<\/p>\n<p>Thank you for your reply.<\/p>\n<p>In my case, I am not trying to download the artefact, but merely to get the path to one of the file it includes. It is not obvious to me that the length of this operation should be proportional to the size of the dataset?<\/p>\n<p>Happy to provide more details if useful.<\/p>\n<p>Thanks.<\/p>\n<p>Regards,<br>\nNicolas<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-04T19:16:48.510Z",
                "Answer_body":"<p>Thank you for the clarification. We are currently working on optimizing our artifacts to speed up artifact.get, but yes currently to get the time to get these artifacts is correlated to the size of the artifacts.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-01T17:36:58.869Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-05T21:10:17.472Z",
                "Answer_body":"<p>Hi again Nicolas, when our engineers tried to repro this, two artifacts with S3 references (one with 10K and one with 100K), it takes 0.5 seconds and 2 seconds, respectively using the code that is given. Is it possible for you to give us a more detailed script of what you are doing to get this lag?<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-18T19:58:49.879Z",
                "Answer_body":"<p>Hi Nicolas,<\/p>\n<p>Is it possible for you to give us a more detailed script so we can fix this issue?<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-22T14:12:55.701Z",
                "Answer_body":"<p>Hi Nicolas,<\/p>\n<p>Since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"programmat access artifact object slow larg artifact hello came issu recent wonder speed process sourc truth version dataset dataset artefact specif project file make dataset ad refer store need retriev path includ version specif file artefact typic fast refer process slow significantli second realiz hold true try access artifact time get digest expect artifact larg number file result long wait oper access programmat python typic us public api access artifact exampl happen run api api artifact api artifact org project artifact latest file info artifact path exampl file artifact path file info ref version file info extra versionid thank nicola",
        "Question_preprocessed_content":"programmat access artifact object slow larg artifact hello came issu recent wonder speed process sourc truth version dataset dataset artefact specif project file make dataset ad refer need retriev path specif file artefact typic fast larger artefact process slow significantli second realiz hold true try access artifact time expect artifact larg number file result long wait oper access programmat python typic us public api access artifact happen run thank nicola",
        "Answer_original_content":"nicola artifact larg longer download oper state indic path itll faster download indic type artifact want order specif datatyp artifact follow code run api run run run artifact run log artifact artifact type model artifact download lesli thank repli case try download artefact mere path file includ obviou length oper proport size dataset happi provid detail us thank regard nicola thank clarif current work optim artifact speed artifact ye current time artifact correl size artifact topic automat close dai repli new repli longer allow nicola engin tri repro artifact refer take second second respect code given possibl detail script lag nicola possibl detail script fix issu warmli lesli nicola heard go close request like open convers let know warmli lesli",
        "Answer_preprocessed_content":"nicola artifact larg longer download oper state indic path itll faster download indic type artifact want order specif datatyp artifact follow code run run run artifact model thank repli case try download artefact mere path file includ obviou length oper proport size dataset happi provid detail us thank regard nicola thank clarif current work optim artifact speed ye current time artifact correl size artifact topic automat close dai repli new repli longer allow nicola engin tri repro artifact refer take second second respect code given possibl detail script lag nicola possibl detail script fix issu warmli lesli nicola heard go close request like convers let know warmli lesli",
        "Question_gpt_summary_original":"the user encountered a challenge where programmatically accessing an artifact object with a large number of references took up to 30 seconds for the first call.",
        "Question_gpt_summary":"user encount challeng programmat access artifact object larg number refer took second",
        "Answer_gpt_summary_original":"possible solutions to the challenge of slow programmatically accessing of an artifact object with a large number of references include indicating the path and type of artifact to download, optimizing artifacts to speed up artifact.get, and providing a detailed script to engineers to fix the issue. the time to get artifacts is currently correlated to the size of the artifacts.",
        "Answer_gpt_summary":"possibl solut challeng slow programmat access artifact object larg number refer includ indic path type artifact download optim artifact speed artifact provid detail script engin fix issu time artifact current correl size artifact"
    },
    {
        "Question_id":65671395.0,
        "Question_title":"ClearML SSH port forwarding fileserver not available in WEB Ui",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5 with SSH Port Forwarding and not beeing able to see my debug samples.<\/p>\n<p>My setup:<\/p>\n<ul>\n<li>ClearML server on hostA<\/li>\n<li>SSH Tunnel connections to access Web App from working machine via localhost:18080<\/li>\n<li>Web App: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<li>Fileserver: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<\/ul>\n<p>In Web App under Task-&gt;Results-&gt;Debug Samples the Images are still refrenced by localhost:8081<\/p>\n<p>Where can I set the fileserver URL to be localhost:18081 in Web App?\nI tried ~\/clearml.conf, but this did not work ( I think it is for my python script ).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610384778623,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":121.0,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<p>In ClearML, debug images' URL is registered once they are uploaded to the fileserver. The WebApp doesn't actually decide on the URL for each debug image, but rather obtains it for each debug image from the server. This allows you to potentially upload debug images to a variety of storage targets, ClearML File Server simply being the most convenient, built-in option.<\/p>\n<p>So, the WebApp will always look for <code>localhost:8008<\/code> for debug images that have already been uploaded to the fileserver and contain <code>localhost:8080<\/code> in their URL.\nA possible solution is to simply add another tunnel in the form of <code>ssh -N -L 8081:127.0.0.1:8081 user@hostA<\/code>.<\/p>\n<p>For future experiments, you can choose to keep using <code>8081<\/code> (and keep using this new tunnel), or to change the default fileserver URL in <code>clearml.conf<\/code> to point to port <code>localhost:18081<\/code>, assuming you're running your experiments from the same machine where the tunnel to <code>18081<\/code> exists.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65671395",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1610390345336,
        "Question_original_content":"ssh port forward fileserv avail web try us server ubuntu ssh port forward bee abl debug sampl setup server hosta ssh tunnel connect access web app work machin localhost web app ssh user hosta fileserv ssh user hosta web app task result debug sampl imag refrenc localhost set fileserv url localhost web app tri conf work think python script",
        "Question_preprocessed_content":"ssh port forward fileserv avail web try us server ubuntu ssh port forward bee abl debug sampl setup server hosta ssh tunnel connect access web app work machin localhost web app fileserv web app sampl imag refrenc localhost set fileserv url localhost web app tri work",
        "Answer_original_content":"disclaim member team train debug imag url regist upload fileserv webapp actual decid url debug imag obtain debug imag server allow potenti upload debug imag varieti storag target file server simpli conveni built option webapp look localhost debug imag upload fileserv contain localhost url possibl solut simpli add tunnel form ssh user hosta futur experi choos new tunnel chang default fileserv url conf point port localhost assum run experi machin tunnel exist",
        "Answer_preprocessed_content":"disclaim member team debug imag url regist upload fileserv webapp actual decid url debug imag obtain debug imag server allow potenti upload debug imag varieti storag target file server simpli conveni option webapp look debug imag upload fileserv contain url possibl solut simpli add tunnel form futur experi choos chang default fileserv url point port assum run experi machin tunnel exist",
        "Question_gpt_summary_original":"the user is encountering challenges with setting up an ssh port forwarding fileserver on their ubuntu 18.04.5 machine, and is unable to see their debug samples in the web app.",
        "Question_gpt_summary":"user encount challeng set ssh port forward fileserv ubuntu machin unabl debug sampl web app",
        "Answer_gpt_summary_original":"the answer suggests that the web app obtains the url for each debug image from the server, and that the file server is the most convenient option for storing debug images. to resolve the issue, the user can add another tunnel in the form of ssh -n -l 8081:127.0.0.1:8081 user@hosta. for future experiments, the user can choose to keep using 8081 or change the default file server url in .conf to point to port localhost:18081, assuming they are running experiments from the same machine where the tunnel to 18081 exists.",
        "Answer_gpt_summary":"answer suggest web app obtain url debug imag server file server conveni option store debug imag resolv issu user add tunnel form ssh user hosta futur experi user choos chang default file server url conf point port localhost assum run experi machin tunnel exist"
    },
    {
        "Question_id":null,
        "Question_title":"How to do \"Pnael Export\", I found no button of it",
        "Question_body":"<p>I want to export all log from cloud like this \" <a href=\"https:\/\/wandb.ai\/site\/articles\/export-data-from-wb\">Export Your Data from W&amp;B on Weights &amp; Biases (wandb.ai)<\/a>\". However, there  is no option for \u201cPanel Export\u201d now. How should I do?<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1661400785513,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":126.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-do-pnael-export-i-found-no-button-of-it\/2991",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-26T23:51:13.905Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/wu-dongdong\">@wu-dongdong<\/a> , the export panel option is still present for all panels in the UI, see below image. Can you provide a screen shot from your end of what you are seeing?<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/78a5f63d64761a0cac73d40531e5aaf007535fd4.png\" alt=\"ExportPanel\" data-base62-sha1=\"hdiRP5BfhYQDZvOBDSBtYOJ3Ehu\" width=\"545\" height=\"359\"><\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-01T22:30:13.448Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/wu-dongdong\">@wu-dongdong<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":10.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-09T16:31:07.224Z",
                "Answer_body":"<p>Oh Thanks for you!@ mohammadbakir.  My question is how to export data of multi runs in a table. Like the second picture of that link. However, in my test, it only export the mean,  maximum and minimum value of all runs(refer the last three columns of the following picture)<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/d1696624d41951b1cb8f06734e58ae4a8c90a4c1.png\" data-download-href=\"\/uploads\/short-url\/tSxBFLka2X67A5GGYFdA5rgFQGJ.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d1696624d41951b1cb8f06734e58ae4a8c90a4c1_2_690x487.png\" alt=\"image\" data-base62-sha1=\"tSxBFLka2X67A5GGYFdA5rgFQGJ\" width=\"690\" height=\"487\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d1696624d41951b1cb8f06734e58ae4a8c90a4c1_2_690x487.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d1696624d41951b1cb8f06734e58ae4a8c90a4c1_2_1035x730.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d1696624d41951b1cb8f06734e58ae4a8c90a4c1_2_1380x974.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d1696624d41951b1cb8f06734e58ae4a8c90a4c1_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1467\u00d71036 94.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-09T16:34:07.771Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a>  Hi, I want to export the data of all runs (not mean value\u2026), then how should I do? thanks for your reply.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/d03535d9d7e5b47c0e85b664b677484545d5c453.png\" data-download-href=\"\/uploads\/short-url\/tHTjCG6lYP5XAgqlUl9ytWDPaX9.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d03535d9d7e5b47c0e85b664b677484545d5c453_2_690x211.png\" alt=\"image\" data-base62-sha1=\"tHTjCG6lYP5XAgqlUl9ytWDPaX9\" width=\"690\" height=\"211\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d03535d9d7e5b47c0e85b664b677484545d5c453_2_690x211.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d03535d9d7e5b47c0e85b664b677484545d5c453_2_1035x316.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d03535d9d7e5b47c0e85b664b677484545d5c453_2_1380x422.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d03535d9d7e5b47c0e85b664b677484545d5c453_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2477\u00d7758 126 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-14T23:19:10.908Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/wu-dongdong\">@wu-dongdong<\/a> , one way to do this is by hiding all the columns you don\u2019t want to export to a CSV. To hide the columns select the <code>Columns<\/code> option menu in the top right of the screen. Once ready, select the <code>bottom arrow<\/code> option, also in the top right of the screen, and select <code>CSV Export<\/code>. Alternatively you can also do this via the API, see more <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/public-api-guide\">here<\/a> for exporting data.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-13T23:20:01.530Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pnael export button want export log cloud like export data option panel export",
        "Question_preprocessed_content":"pnael export button want export log cloud like export data option panel export",
        "Answer_original_content":"dongdong export panel option present panel imag provid screen shot end see dongdong heard go close request like open convers let know thank mohammadbakir question export data multi run tabl like second pictur link test export mean maximum minimum valu run refer column follow pictur imag mohammadbakir want export data run mean valu thank repli imag dongdong wai hide column dont want export csv hide column select column option menu right screen readi select arrow option right screen select csv export altern api export data topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"export panel option present panel imag provid screen shot end see heard go close request like convers let know thank mohammadbakir question export data multi run tabl like second pictur link test export mean maximum minimum valu run imag want export data run thank repli imag wai hide column dont want export csv hide column select option menu right screen readi select option right screen select altern api export data topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking for a way to export all log data from the cloud using the \"panel export\" feature, but is unable to find the option.",
        "Question_gpt_summary":"user look wai export log data cloud panel export featur unabl option",
        "Answer_gpt_summary_original":"possible solutions to export log data from the cloud using the \"panel export\" feature are: \n1. check if the export panel option is still present for all panels in the ui.\n2. hide all the columns you don't want to export to a csv and then select the bottom arrow option and select csv export.\n3. export data via the api.",
        "Answer_gpt_summary":"possibl solut export log data cloud panel export featur check export panel option present panel hide column want export csv select arrow option select csv export export data api"
    },
    {
        "Question_id":null,
        "Question_title":"Why the storage account assosiated to azure machine learning has differenent region compared ML workspace?",
        "Question_body":"I have created a machine learning workspace in West Europe region. But the storage account, key vault and application insights got created in East US region. All these got created by default with creation on ML workspace.\nSo I want to know the reason for different region and also want to move the storage account to West Europe region.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613120096110,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, this is unusual. There's no way to move your default AML storage account to a different region. I recommend creating a new workspace or contacting Azure Support to investigate further.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/270693\/why-the-storage-account-assosiated-to-azure-machin.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-12T22:09:04.633Z",
                "Answer_score":0,
                "Answer_body":"Hi, this is unusual. There's no way to move your default AML storage account to a different region. I recommend creating a new workspace or contacting Azure Support to investigate further.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1613167744632,
        "Question_original_content":"storag account assosi differen region compar workspac creat machin learn workspac west europ region storag account kei vault applic insight got creat east region got creat default creation workspac want know reason differ region want storag account west europ region",
        "Question_preprocessed_content":"storag account assosi differen region compar workspac creat machin learn workspac west europ region storag account kei vault applic insight got creat east region got creat default creation workspac want know reason differ region want storag account west europ region",
        "Answer_original_content":"unusu wai default aml storag account differ region recommend creat new workspac contact azur support investig",
        "Answer_preprocessed_content":"unusu wai default aml storag account differ region recommend creat new workspac contact azur support investig",
        "Question_gpt_summary_original":"the user is encountering a challenge with their machine learning workspace, as the associated storage account, key vault, and application insights were created in a different region than the one they specified.",
        "Question_gpt_summary":"user encount challeng machin learn workspac associ storag account kei vault applic insight creat differ region specifi",
        "Answer_gpt_summary_original":"possible solutions to the challenge of having a machine learning workspace with associated storage account, key vault, and application insights created in a different region than the one specified are to create a new workspace or contact azure support to investigate further. unfortunately, there is no way to move the default aml storage account to a different region.",
        "Answer_gpt_summary":"possibl solut challeng have machin learn workspac associ storag account kei vault applic insight creat differ region specifi creat new workspac contact azur support investig unfortun wai default aml storag account differ region"
    },
    {
        "Question_id":null,
        "Question_title":"AutoML - pre-trained models?",
        "Question_body":"I know that for AutoML, the user has to train the model. But are there existing \"pre-trained\" models that you can leverage to identify sentiments or classifications like profanity, irony, and bullying? ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625143740000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":496.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/AutoML-pre-trained-models\/td-p\/162864\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-01T23:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Can you please add more details about the sample text\/document that you are trying?. The powerful pre-trained models of the\u00a0Natural Language API\u00a0\u00a0empowers developers to easily apply natural language understanding (NLU) to their applications with features including sentiment analysis, entity analysis, entity sentiment analysis, content classification, and syntax analysis.\n\nSamples: https:\/\/cloud.google.com\/natural-language\/automl\/docs\/samples"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"automl pre train model know automl user train model exist pre train model leverag identifi sentiment classif like profan ironi bulli",
        "Question_preprocessed_content":"automl model know automl user train model exist model leverag identifi sentiment classif like profan ironi bulli",
        "Answer_original_content":"add detail sampl text document try power pre train model thenatur languag apiempow develop easili appli natur languag understand nlu applic featur includ sentiment analysi entiti analysi entiti sentiment analysi content classif syntax analysi sampl http cloud googl com natur languag automl doc sampl",
        "Answer_preprocessed_content":"add detail sampl try power model thenatur languag apiempow develop easili appli natur languag understand applic featur includ sentiment analysi entiti analysi entiti sentiment analysi content classif syntax analysi sampl",
        "Question_gpt_summary_original":"the user is looking for existing \"pre-trained\" models that can be leveraged to identify sentiments or classifications such as profanity, irony, and bullying for automl.",
        "Question_gpt_summary":"user look exist pre train model leverag identifi sentiment classif profan ironi bulli automl",
        "Answer_gpt_summary_original":"possible solutions from the answer include using the pre-trained models of the natural language api to apply natural language understanding to applications with features such as sentiment analysis, entity analysis, entity sentiment analysis, content classification, and syntax analysis. the user can also refer to the provided samples for more information.",
        "Answer_gpt_summary":"possibl solut answer includ pre train model natur languag api appli natur languag understand applic featur sentiment analysi entiti analysi entiti sentiment analysi content classif syntax analysi user refer provid sampl inform"
    },
    {
        "Question_id":73058582.0,
        "Question_title":"Access denied for aws public sagemaker xgboost registry",
        "Question_body":"<p>I am trying to pull a prebuilt xgboost image from the public aws xgboost registry specified here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title<\/a>, however whenever I run the sagemaker pipeline I get the error:<\/p>\n<pre><code>ClientError: Failed to invoke sagemaker:CreateModelPackage. \nError Details: Access denied for registry ID: 246618743249, repository name: sagemaker-xgboost. \nPlease check if your ECR image exists and has proper pull permissions for SageMaker.\n<\/code><\/pre>\n<p>Here is the attached role boundary I am using to run the pipeline:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: [\n                &quot;codebuild:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;codepipeline:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;events:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:DescribeLogGroups&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:iam::xxxxxxxxxxxx:role\/ml-*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:*&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:ecr:us-west-2:246618743249:repository\/246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>and below is the attached policies for the role:<\/p>\n<pre><code>{\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: &quot;ecr:*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Sid&quot;: &quot;&quot;\n        }\n    ],\n    &quot;Version&quot;: &quot;2012-10-17&quot;\n}\n<\/code><\/pre>\n<p>plus the AWSCodePipelineFullAccess, AWSCodeBuildAdminAccess, and AmazonSageMakerFullAccess managed policies.<\/p>\n<p>Why can't I access the image\/why am I getting this error? As you can see I gave my role full permissions for the ecr registry in the boundary, and full permissions for ecr in the attached policy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1658354562373,
        "Question_favorite_count":null,
        "Question_last_edit_time":1658354892732,
        "Question_score":1.0,
        "Question_view_count":61.0,
        "Answer_body":"<p>I had to change the boundary to be this: <code> arn:aws:ecr:us-west-2:246618743249:repository\/sagemaker-xgboost<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73058582",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1658508649572,
        "Question_original_content":"access deni aw public xgboost registri try pull prebuilt xgboost imag public aw xgboost registri specifi http doc aw amazon com latest ecr west html xgboost west titl run pipelin error clienterror fail invok createmodelpackag error detail access deni registri repositori xgboost check ecr imag exist proper pull permiss attach role boundari run pipelin version statement action codebuild resourc effect allow action codepipelin resourc effect allow action event resourc effect allow action log createloggroup log createlogstream log describelogstream log describeloggroup log putlogev resourc effect allow action iam passrol resourc arn aw iam role effect allow action ecr resourc arn aw ecr west repositori dkr ecr west amazonaw com xgboost effect allow action ecr getauthorizationtoken resourc effect allow attach polici role statement action ecr effect allow resourc sid version plu awscodepipelinefullaccess awscodebuildadminaccess amazonfullaccess manag polici access imag get error gave role permiss ecr registri boundari permiss ecr attach polici",
        "Question_preprocessed_content":"access deni aw public xgboost registri try pull prebuilt xgboost imag public aw xgboost registri specifi run pipelin error attach role boundari run pipelin attach polici role plu awscodepipelinefullaccess awscodebuildadminaccess amazonfullaccess manag polici access get error gave role permiss ecr registri boundari permiss ecr attach polici",
        "Answer_original_content":"chang boundari arn aw ecr west repositori xgboost",
        "Answer_preprocessed_content":"chang boundari",
        "Question_gpt_summary_original":"the user is encountering an \"access denied\" error when attempting to pull a prebuilt xgboost image from the public aws xgboost registry, despite having given their role full permissions for the ecr registry.",
        "Question_gpt_summary":"user encount access deni error attempt pull prebuilt xgboost imag public aw xgboost registri despit have given role permiss ecr registri",
        "Answer_gpt_summary_original":"the solution to the \"access denied\" error when pulling a prebuilt xgboost image from the public aws xgboost registry is to change the boundary to \"arn:aws:ecr:us-west-2:246618743249:repository\/-xgboost\".",
        "Answer_gpt_summary":"solut access deni error pull prebuilt xgboost imag public aw xgboost registri chang boundari arn aw ecr west repositori xgboost"
    },
    {
        "Question_id":65902366.0,
        "Question_title":"AWS Sagemaker - ClientError: Data download failed:Could not download",
        "Question_body":"<p>I encountered and error when I deploy my training job in my notebook instance.\nThis what it says:\n<code>&quot;UnexpectedStatusException: Error for Training job tensorflow-training-2021-01-26-09-55-05-768: Failed. Reason: ClientError: Data download failed:Could not download s3:\/\/forex-model-data\/data\/train2001_2020.npz: insufficient disk space&quot;<\/code><\/p>\n<p>I deploy training job to try running it to different instances in 3 epoch. I use ml.c5.4xlarge, ml.c5.18xlarge, ml.m5.24xlarge, also I have two sets of training data, train2001_2020.npz and train2016_2020.npz.<\/p>\n<p>First, I run train2001_2020 to ml.c5.18xlarge and ml.c5.18xlarge and the training job completed, then I switch to train2016_2020 and run it to ml.c5.4xlarge and ml.c5.18xlarge and it goes well. Then when I tried to run it using ml.m5.24xlarge I got an error (quoted above), but my dataset is train2016_2020 not train2001_2020 then when I rerun it again with all other instances it has the same error. What happen?<\/p>\n<p>I stopped the instances and refresh everything, but I encountered same issue.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611668079743,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":774.0,
        "Answer_body":"<p>It's not really clear to all the test are you doing, but that error usually means that there is not enough disk space on the instance you are using for the training job. You can try to increase the additional storage for the instance (you can do in the estimator parameters if you are using the sagemaker SDK in a notebook).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65902366",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1611831268823,
        "Question_original_content":"clienterror data download fail download encount error deploi train job notebook instanc sai unexpectedstatusexcept error train job tensorflow train fail reason clienterror data download fail download forex model data data train npz insuffici disk space deploi train job try run differ instanc epoch us xlarg xlarg xlarg set train data train npz train npz run train xlarg xlarg train job complet switch train run xlarg xlarg goe tri run xlarg got error quot dataset train train rerun instanc error happen stop instanc refresh encount issu",
        "Question_preprocessed_content":"clienterror data download fail download encount error deploi train job notebook instanc sai deploi train job try run differ instanc epoch us set train data run train job complet switch run goe tri run got error dataset rerun instanc error happen stop instanc refresh encount issu",
        "Answer_original_content":"clear test error usual mean disk space instanc train job try increas addit storag instanc estim paramet sdk notebook",
        "Answer_preprocessed_content":"clear test error usual mean disk space instanc train job try increas addit storag instanc",
        "Question_gpt_summary_original":"the user encountered an unexpected error when attempting to deploy a training job to different instances, resulting in insufficient disk space to download the data.",
        "Question_gpt_summary":"user encount unexpect error attempt deploi train job differ instanc result insuffici disk space download data",
        "Answer_gpt_summary_original":"possible solutions to the error of insufficient disk space when deploying a training job to different instances include increasing the additional storage for the instance and adjusting the estimator parameters if using the sdk in a notebook.",
        "Answer_gpt_summary":"possibl solut error insuffici disk space deploi train job differ instanc includ increas addit storag instanc adjust estim paramet sdk notebook"
    },
    {
        "Question_id":null,
        "Question_title":"Storing the guild artifacts on s3 using `guild run -r s3-dev`",
        "Question_body":"<p>I wanted to store all my runs in s3 bucket directly instead of the local guildai home directory. So,  I configured my <code>~\/.guild\/config.yml<\/code> in the following way which I thought will setup my s3 remote location and I can automatically save the sourcecode and artifacts that I save locally into s3 bucket.<\/p>\n<pre><code>remotes:\n  s3-dev:\n    type: s3\n    description: Production runs\n    bucket: cortex-model-data\n    region: eu-central-1\n<\/code><\/pre>\n<p>and I have my <code>guild.yml<\/code>as follows<\/p>\n<pre><code>- model: AlwaysPredictMean\n  description: A dummy model which always predicts mean\n  operations:\n    train:\n      description: Training Pipeline Sample Code\n      main: training\/train\n      flags-import: all\n      output-scalars: '(\\key): (\\value)'\n<\/code><\/pre>\n<p>when I run the script using <code>guild run --remote s3-dev<\/code> , I get the following error message<\/p>\n<pre><code>\u00b1 |feature\/guildai U:1 \u2717| \u2192 guild run -r s3-dev\nYou are about to run AlwaysPredictMean:train on s3-dev\n  comment: Description for a given training run\n  config: training\/config\/example.yml\n  data: tests\/test_df.csv\n  epochs: 10\n  model_class: training.example_model::AlwaysPredictMean\n  use_case: example_use_case\nContinue? (Y\/n) y\nguild: remote 's3-dev' does not support this operation\n<\/code><\/pre>\n<p>Can someone let me know what exactly is the problem and why can\u2019t I use guild.ai to store in the specified s3. Thanks<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1605200901861,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":528.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/storing-the-guild-artifacts-on-s3-using-guild-run-r-s3-dev\/449",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-12T17:57:29.291Z",
                "Answer_body":"<p>S3 remotes are only for file storage. You can\u2019t run anything in S3. If you want to run on a remote server, which is what you\u2019re asking for with the <code>--remote<\/code> option, you need either an <a href=\"\/reference\/remotes#ssh\"><code>ssh<\/code><\/a> or <a href=\"https:\/\/my.guild.ai\/reference\/remotes#ec2\"><code>ec2<\/code><\/a> remote type.<\/p>\n<p>If you want to run your operation locally, omit the <code>--remote<\/code> option \u2014 you\u2019ll get runs in your current Guild environment. Then use <a href=\"\/commands\/push\"><code>guild push s3-dev<\/code><\/a> to copy those runs to S3.<\/p>",
                "Answer_score":16.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-13T08:52:27.693Z",
                "Answer_body":"<p>Thanks a lot. Now it is more clear. I misunderstood that if I run a script with a remote s3 config, my runs (including sourcecode and artifacts) are automatically saved to s3. I will do it manually by using <code>guild push s3-dev<\/code><\/p>\n<p>Thanks also for building this nice tool.<\/p>",
                "Answer_score":56.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-13T15:50:12.734Z",
                "Answer_body":"<p>I could see the value of automatically sync\u2019ing with a remote env during and especially after a run. Guild does not currently support this. I could see an enhancement to Guild along the line of <code>--push-to-remote<\/code> or <code>--push-on-success<\/code> that does this. That\u2019s a good idea. Though that poor run command already has quite a few options - it\u2019s going to evolve into it\u2019s own language grammar <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> Still, I like the thinking!<\/p>\n<p>In defense of Guild\u2019s \u201cseparation of concerns\u201d Tao, you can accomplish what you\u2019re looking for this way:<\/p>\n<pre><code class=\"lang-command\">guild run &lt;options&gt; &amp;&amp; guild push s3-dev\n<\/code><\/pre>\n<p>This works on shells that support <code>&amp;&amp;<\/code>, which will execute the second command only when the first command succeeds (exit code of 0).<\/p>\n<p>If you wanted to push regardless of the result, use:<\/p>\n<pre><code class=\"lang-command\">guild run &lt;options&gt;; guild push s3-dev\n<\/code><\/pre>\n<p>Note this will push all runs, not just the latest. As Guild uses rsync (or similar) protocols, this is efficient. But you may want to just push the latest. In that case add the <code>1<\/code> argument to the push command. This tells Guild to only push the latest run (i.e. the run with index <code>1<\/code>).<\/p>\n<p>Wait, there\u2019s more!<\/p>\n<p>If you would like to always sync runs to your S3 bucket, you could create a repeating command <code>guild push<\/code> that runs, say, every 10 minutes, 30 minutes, etc. Most POSIX systems offer cron for this, but there are myriad ways to run scheduled commands. Now this is moving you into \u201csys op\u201d territory, which you might not want enter - beware there be dragons <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=9\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"> But such is life. If this logic was moved into the <code>run<\/code> command you\u2019d need to worry about command failure (e.g. SIGKILLs) or system failure (e.g. batter\/power loss, etc.) Using something like cron is a nice separation of concerns because cron can back-fill on various failures to complete your backups even when Guild or the system unexpected crashes. E.g. your system loses power during a long training run where you have various interim checkpoints. You restart, cron runs automatically to backup your partial runs. You can then restart the run using <code>guild run --restart &lt;run ID&gt;<\/code> and be on your merry way, letting cron run every so often to refresh the backup. I don\u2019t think this scheme is terribly complicated yet it\u2019s quite robust.<\/p>",
                "Answer_score":26.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-16T06:18:10.999Z",
                "Answer_body":"<p>Thanks garrett, I understand the concern. Coming from mlflow where you can set_tracking_uri to s3 remote, I thought guild also has this option. Having the artifacts stored in a remote s3 bucket makes it easier to collaborate in a small team and track the progress. But after looking at the number of options available for <code>guild run<\/code> and <code>guild push<\/code>, it makes sense to have these two functions separate.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-16T13:26:30.056Z",
                "Answer_body":"<p>I think it\u2019s a great idea and one more option to <code>run<\/code> isn\u2019t a problem. I opened an issue on GitHub to track progress this.<\/p>\n<aside class=\"onebox githubissue\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/252\" target=\"_blank\" rel=\"noopener\">github.com\/guildai\/guildai<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewBox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/guildai\/guildai\/issues\/252\" target=\"_blank\" rel=\"noopener\">Ability to sync with remote during run without explicit push command<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2020-11-16\" data-time=\"13:25:18\" data-timezone=\"UTC\">01:25PM - 16 Nov 20 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/gar1t\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"gar1t\" src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/84ac3354a76fe15593cedb56fe486a0ed93d5440.jpeg\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          gar1t\n        <\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n<\/div>\n\n<div class=\"github-row\">\n  <p class=\"github-content\">It's convenient to be able to sync run artifacts with a remote (e.g. S3 bucket, etc.) as a function of the...<\/p>\n<\/div>\n\n<div class=\"labels\">\n    <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">enhancement<\/span>\n<\/div>\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":16.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"store artifact run dev want store run bucket directli instead local home directori configur config yml follow wai thought setup remot locat automat save sourcecod artifact save local bucket remot dev type descript product run bucket cortex model data region central ymla follow model alwayspredictmean descript dummi model predict mean oper train descript train pipelin sampl code main train train flag import output scalar kei valu run script run remot dev follow error messag featur run dev run alwayspredictmean train dev comment descript given train run config train config exampl yml data test test csv epoch model class train exampl model alwayspredictmean us case exampl us case continu remot dev support oper let know exactli problem us store specifi thank",
        "Question_preprocessed_content":"store artifact want store run bucket directli instead local home directori configur follow wai thought setup remot locat automat save sourcecod artifact save local bucket follow run script follow error messag let know exactli problem us store specifi thank",
        "Answer_original_content":"remot file storag run want run remot server your ask remot option need ssh remot type want run oper local omit remot option youll run current environ us push dev copi run thank lot clear misunderstood run script remot config run includ sourcecod artifact automat save manual push dev thank build nice tool valu automat sync remot env especi run current support enhanc line push remot push success that good idea poor run command option go evolv languag grammar like think defens separ concern tao accomplish your look wai run push dev work shell support execut second command command succe exit code want push regardless result us run push dev note push run latest us rsync similar protocol effici want push latest case add argument push command tell push latest run run index wait there like sync run bucket creat repeat command push run minut minut posix system offer cron myriad wai run schedul command move sy territori want enter bewar dragon life logic move run command youd need worri command failur sigkil failur batter power loss like cron nice separ concern cron failur complet backup unexpect crash lose power long train run interim checkpoint restart cron run automat backup partial run restart run run restart merri wai let cron run refresh backup dont think scheme terribl complic robust thank garrett understand concern come set track uri remot thought option have artifact store remot bucket make easier collabor small team track progress look number option avail run push make sens function separ think great idea option run isnt problem open issu github track progress github com abil sync remot run explicit push command open nov utc gart conveni abl sync run artifact remot bucket function enhanc",
        "Answer_preprocessed_content":"remot file storag run want run remot server your ask option need remot type want run oper local omit option youll run current environ us copi run thank lot clear misunderstood run script remot config run automat save manual thank build nice tool valu automat sync remot env especi run current support enhanc line that good idea poor run command option go evolv languag grammar like think defens separ concern tao accomplish your look wai work shell support execut second command command succe want push regardless result us note push run latest us rsync protocol effici want push latest case add argument push command tell push latest run wait there like sync run bucket creat repeat command run minut minut posix system offer cron myriad wai run schedul command move sy territori want enter bewar dragon life logic move command youd need worri command failur failur like cron nice separ concern cron failur complet backup unexpect crash lose power long train run interim checkpoint restart cron run automat backup partial run restart run merri wai let cron run refresh backup dont think scheme terribl complic robust thank garrett understand concern come remot thought option have artifact store remot bucket make easier collabor small team track progress look number option avail make sens function separ think great idea option isnt problem open issu github track progress abil sync remot run explicit push command open nov utc gar conveni abl sync run artifact remot function enhanc",
        "Question_gpt_summary_original":"the user is encountering challenges with storing guild artifacts on s3 using `guild run -r s3-dev`, resulting in an error message indicating that the remote 's3-dev' does not support the operation.",
        "Question_gpt_summary":"user encount challeng store artifact run dev result error messag indic remot dev support oper",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- use ssh or ec2 remote type to run on a remote server.\n- omit the --remote option to run the operation locally and use guild push s3-dev to copy those runs to s3.\n- create a repeating command guild push that runs every 10 minutes, 30 minutes, etc. to always sync runs to your s3 bucket.\n- use cron to run scheduled commands for syncing runs to your s3 bucket.\n- open an issue on github to track progress on the enhancement of syncing with remote during run without explicit push command.\n\nin summary, the answer provides various solutions to the user's challenge of storing guild artifacts on s3 using `guild run -r s3-dev`.",
        "Answer_gpt_summary":"possibl solut extract answer us ssh remot type run remot server omit remot option run oper local us push dev copi run creat repeat command push run minut minut sync run bucket us cron run schedul command sync run bucket open issu github track progress enhanc sync remot run explicit push command summari answer provid solut user challeng store artifact run dev"
    },
    {
        "Question_id":62180798.0,
        "Question_title":"How to increase number of tested images in MS Azure Custom Vision?",
        "Question_body":"<p>I've created a project in Azure Custom Vision (Object Detection, General Compact, Tier S0). I uploaded about 70 images, 35 images per tag then started training my model.<\/p>\n\n<p>Checked tags in the Iterations screen after training (Quick Training) was done. For my surprise, only 7 images were tested per tag.<\/p>\n\n<p>Tried to run Advanced Training for 1 hour. Nothing has changed. Only 7 images per tag were tested.<\/p>\n\n<p>Am I doing something wrong?<\/p>\n\n<p>Is there a way to use all images for object detection training so it can give me a better accuracy?<\/p>\n\n<p>Thanks,\n+ftex<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591211290130,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":159.0,
        "Answer_body":"<p>What you are seeing in the test interface after the training is only a part of the total images because these metrics are calculated using k-fold cross validation.<\/p>\n<p>You are not doing something wrong. It would not be logic to test all the images because it would mean testing with your training images.<\/p>\n<p>To have a better accuracy, there's no magic: add more images, relevant to your use-case<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/getting-started-build-a-classifier#evaluate-the-classifier?WT.mc_id=AI-MVP-5003365\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/getting-started-build-a-classifier#evaluate-the-classifier<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1600465129972,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62180798",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1591541807696,
        "Question_original_content":"increas number test imag azur custom vision creat project azur custom vision object detect gener compact tier upload imag imag tag start train model check tag iter screen train quick train surpris imag test tag tri run advanc train hour chang imag tag test wrong wai us imag object detect train better accuraci thank ftex",
        "Question_preprocessed_content":"increas number test imag azur custom vision creat project azur custom vision upload imag imag tag start train model check tag iter screen train surpris imag test tag tri run advanc train hour chang imag tag test wrong wai us imag object detect train better accuraci thank ftex",
        "Answer_original_content":"see test interfac train total imag metric calcul fold cross valid wrong logic test imag mean test train imag better accuraci magic add imag relev us case http doc microsoft com azur cognit servic custom vision servic get start build classifi evalu classifi",
        "Answer_preprocessed_content":"see test interfac train total imag metric calcul cross valid wrong logic test imag mean test train imag better accuraci magic add imag relev",
        "Question_gpt_summary_original":"the user is encountering challenges with increasing the number of tested images in ms azure custom vision, and is looking for a way to use all images for object detection training to improve accuracy.",
        "Question_gpt_summary":"user encount challeng increas number test imag azur custom vision look wai us imag object detect train improv accuraci",
        "Answer_gpt_summary_original":"the answer suggests that the user is not doing anything wrong and that the metrics are calculated using k-fold cross-validation. to improve accuracy, the user should add more relevant images to their use-case.",
        "Answer_gpt_summary":"answer suggest user wrong metric calcul fold cross valid improv accuraci user add relev imag us case"
    },
    {
        "Question_id":null,
        "Question_title":"Can't pull ACR image in compute instance of Azure Machine Learning Workspace: VNetPLSetupError",
        "Question_body":"When I deployed a training job to a compute instance it fails with the following error:\n\nAzureMLCompute job failed.\nVNetPLSetupError: Failed to pull Docker image xxxxxxxxxxx.azurecr.io\/azureml\/azureml_f306f67d2a96fc883ff0773a2a01394e with authentication mode IdentityToken due to: Docker responded with status code 500: {\"message\":\"Head \\\"https:\/\/xxxxxxxxxx.azurecr.io\/v2\/azureml\/azureml_f306f67d2a96fc883ff0773a2a01394e\/manifests\/latest\\\": Post \\\"https:\/\/xxxxxxxx.azurecr.io\/oauth2\/token\\\": net\/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)\"}\n. This error may be caused by a Deny outgoing rule to Container Registry or AzureFrontDoor service tag, or a missing Allow outgoing rule to the same, which is blocking access to container registry xxxxxxxxxx.azurecr.io\n\nIt suddenly stopped working after it pulled without problems before.\nI can build and push an image, but not pull anymore.\n\nI checked the NSG that is associated with this ML workspace, but it has no outbound rules that could prevent the pull (push works as mentioned). There is no AzureFrontDoor tag configured.\n\nDid anyone come across this 'VNetPLSetupError' error?\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1666098594973,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1052833\/can39t-pull-acr-image-in-compute-instance-of-azure.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-21T13:07:49.777Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nits not enabled. Interestingly the error suddenly disappeared. Not sure what caused it, but I had this error only for a couple of days. There was no change to the infrastructure.\n\nThanks for responding.\n\nBest, Andrej",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pull acr imag comput instanc workspac vnetplsetuperror deploi train job comput instanc fail follow error comput job fail vnetplsetuperror fail pull docker imag azurecr ffdafcffaa authent mode identitytoken docker respond statu code messag head http azurecr ffdafcffaa manifest latest post http azurecr oauth token net http request cancel wait connect client timeout exceed await header error caus deni outgo rule contain registri azurefrontdoor servic tag miss allow outgo rule block access contain registri azurecr suddenli stop work pull problem build push imag pull anymor check nsg associ workspac outbound rule prevent pull push work mention azurefrontdoor tag configur come vnetplsetuperror error thank",
        "Question_preprocessed_content":"pull acr imag comput instanc workspac vnetplsetuperror deploi train job comput instanc fail follow error comput job fail vnetplsetuperror fail pull docker imag authent mode identitytoken docker respond statu code error caus deni outgo rule contain registri azurefrontdoor servic tag miss allow outgo rule block access contain registri suddenli stop work pull problem build push imag pull anymor check nsg associ workspac outbound rule prevent pull azurefrontdoor tag configur come vnetplsetuperror error thank",
        "Answer_original_content":"enabl interestingli error suddenli disappear sure caus error coupl dai chang infrastructur thank respond best andrej",
        "Answer_preprocessed_content":"enabl interestingli error suddenli disappear sure caus error coupl dai chang infrastructur thank respond best andrej",
        "Question_gpt_summary_original":"the user is encountering a challenge with pulling an acr image in a compute instance of their workspace, resulting in a \"vnetplsetuperror\" error, despite having no outbound rules that could prevent the pull and no azure front door tag configured.",
        "Question_gpt_summary":"user encount challeng pull acr imag comput instanc workspac result vnetplsetuperror error despit have outbound rule prevent pull azur door tag configur",
        "Answer_gpt_summary_original":"there are no clear solutions provided in the answer. the user mentions that the error disappeared on its own after a couple of days, but they are unsure of the cause.",
        "Answer_gpt_summary":"clear solut provid answer user mention error disappear coupl dai unsur caus"
    },
    {
        "Question_id":null,
        "Question_title":"Large Data Registry on NAS with multiple DVC and non-DVC users",
        "Question_body":"<p>[I posted this question in the <span class=\"hashtag\">#dvc<\/span> Discord channel, and <a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> suggested that other users can benefit from the discussion, so I\u2019m copying it here]<\/p>\n<p>Some time ago I asked in the forum about using DVC to manage our data on a NAS drive (<a href=\"https:\/\/discuss.dvc.org\/t\/single-cache-or-multiple-caches-in-nas-with-external-data\/1136\" class=\"inline-onebox\">Single cache or multiple caches in NAS with External Data<\/a>). I got useful advice from <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a> , thanks, who suggested setting up a Data Registry. I did, but we are having trouble with users getting permission errors to add new datasets.<\/p>\n<p>The problem is that we have some constraints that go a bit against how DVC seems to be designed for. Namely, we have a folder <code>datasets<\/code> on the NAS with large datasets, both in size and number of files. Multiple users need to be able to save directories\/files to this folder, and then add\/commit\/push them with DVC. We cannot have data duplication either, and the directories\/files in folder <code>datasets<\/code> need to remain there, for non-DVC users to read them.<\/p>\n<p>So, our current solution is that the NAS is mounted on a server at <code>\/nas\/<\/code>, and looks like this:<\/p>\n<pre><code class=\"lang-auto\">nas\n  \u251c\u2500\u2500  dvc_cache\n  \u2514\u2500\u2500  datasets\n           \u251c\u2500\u2500 .dvc\n           \u251c\u2500\u2500 .git\n           \u251c\u2500\u2500 dataset_1\n           \u2514\u2500\u2500 dataset_2\n<\/code><\/pre>\n<p>with a configuration file (hardlinks\/symlinks to avoid data duplication)<\/p>\n<blockquote>\n<p>[cache]<br>\ndir = \/nas\/dvc_cache\/<br>\nshared = group<br>\ntype = \u201chardlink,symlink\u201d<br>\n[core]<br>\nautostage = true<\/p>\n<\/blockquote>\n<p>But an immediate problem is that if user A does <code>dvc add dataset_1<\/code>, and user B does <code>dvc add dataset_2<\/code>, then user A can <code>git commit -a<\/code> and <code>git push<\/code> and commit\/push both datasets.<\/p>\n<p>Another issue we are having is that it looks like if <code>user_A<\/code> adds a dataset, the directory created in the cache, e.g. <code>\/nas\/dvc_cache\/e0<\/code>, instead of having owner <code>user_A<\/code> and group <code>everybody<\/code>, it has both owner and group <code>user_A<\/code>, which blocks for anybody else any dvc operations that involve <code>\/nas\/dvc_cache\/e0<\/code>.<\/p>\n<p>My feeling is that each user should have their clone of the Data Registry on <code>\/nas<\/code>, but I\u2019m not sure this works with the Data Registry idea. Suggestions would be very welcome, thanks!<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1660121162723,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":8.0,
        "Question_view_count":223.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/large-data-registry-on-nas-with-multiple-dvc-and-non-dvc-users\/1294",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-10T08:47:03.954Z",
                "Answer_body":"<p>[<a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> 's reply pasted here]<\/p>\n<ol>\n<li>Hey, <a class=\"mention\" href=\"\/u\/rcasero\">@rcasero<\/a> . It\u2019s a good scenario, let\u2019s try to figure out something. I\u2019m glad to see that you are using this advanced stuff in some interesting way.<\/li>\n<\/ol>\n<blockquote>\n<p>My feeling is that each user should have their clone of the Data Registry on \/nas<\/p>\n<\/blockquote>\n<p>I think that goes into the right direction. I would even say, it should be in their home directories. I don\u2019t see any reason for keeping it on NAS, moreover git and DVC on NAS do not work well usually. You setup should be enough to <code>git clone<\/code> into a personal directory, checkout a specific dataset (via symlinks in this case), add more files and do <code>dvc add<\/code> or <code>dvc commit<\/code> + <code>git push<\/code>.<\/p>\n<blockquote>\n<p>We cannot have data duplication either, and the directories\/files in folder datasets need to remain there, for non-DVC users to read them.<\/p>\n<\/blockquote>\n<p>That\u2019s an interesting requirement, and yes you can keep it then and do <code>git pull<\/code> + <code>dvc pull<\/code> periodically I think. Alternative for end users is to rely on <code>dvc import<\/code> or <code>dvc get<\/code> in their projects. This is the best way to get <code>dataset<\/code> right into their workspace . Multiple users can even get different versions simultaneously w\/o affecting each other. Curious about you thoughts on this workflow. (edited)<\/p>\n<ol start=\"2\">\n<li><em>[<em>19:34<\/em>]<\/em><\/li>\n<\/ol>\n<p><a class=\"mention\" href=\"\/u\/rcasero\">@rcasero<\/a> after this dicussion is done, we should probably update the discussion forum so that other folks can benefit \u2026<\/p>",
                "Answer_score":58.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-10T14:16:54.811Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> Thanks for the suggestions. I have some feedback, and a minor clarification. The clarification is that many datasets in <code>\/nas\/datasets<\/code> won\u2019t be under DVC. It\u2019s only when we need to process a dataset, that we put it under DVC. At that point, we expect that DVC will make that dataset read-only, and although it can still be read by non-DVC users, only DVC users can modify or delete it.<\/p>\n<blockquote>\n<p>I would even say, it should be in their home directories. I don\u2019t see any reason for keeping it on NAS, moreover git and DVC on NAS do not work well usually. You setup should be enough to <code>git clone<\/code> into a personal directory, checkout a specific dataset (via symlinks in this case), add more files and do <code>dvc add<\/code> or <code>dvc commit<\/code> + <code>git push<\/code>.<\/p>\n<\/blockquote>\n<p>The reasons to keep the user\u2019s Data Registry clones on the NAS in our case would be, in my opinion:<\/p>\n<ol>\n<li>There\u2019s not enough space in the home directories for our datasets. If the Data Registry clone is on <code>server:\/home\/user_A\/data_registry\/<\/code>, and the user wants to add new files or a new dataset, they first need to be copied to the home directory, but they won\u2019t fit there.<\/li>\n<li>Even if there were enough space, this would be a bit inefficient. First, a copy needs to be made to the <code>server:\/home\/user_A\/data_registry\/<\/code> filesystem. Then, when the dataset is added to DVC, dvc needs to move the files to the DVC cache on the NAS filesystem, which takes time.<\/li>\n<\/ol>\n<p>So, instead, to me it makes sense to have something like this<\/p>\n<pre><code class=\"lang-auto\">nas\n\u251c\u2500\u2500 data_registry_user_A\n\u251c\u2500\u2500 data_registry_user_B\n\u251c\u2500\u2500 data_registry_user_C\n\u251c\u2500\u2500 datasets\n\u2502   \u251c\u2500\u2500 dataset_1\n\u2502   \u2514\u2500\u2500 dataset_2\n\u2514\u2500\u2500 dvc_cache\n<\/code><\/pre>\n<p>The workflow would be<\/p>\n<ol>\n<li>Data is originally copied to the NAS in <code>\/nas\/datasets<\/code>.<\/li>\n<li>If we need to process a dataset, it can be quickly moved to a user\u2019s data registry, e.g. <code>\/nas\/datasets\/data_registry_user_A<\/code> (<code>mv \/nas\/datasets\/dataset_1 \/nas\/datasets\/data_registry_user_A<\/code>) because both directories are on the same filesystem.<\/li>\n<li>The user then does <code>cd \/nas\/datasets\/data_registry_user_A<\/code> and <code>dvc add dataset_1<\/code> and commits. This should create hardlinks in the cache to the data files. Because hardlinks point to the same inode on the filesystem, it avoids data duplication or moving data around.<\/li>\n<li>The user then goes to the general registry <code>cd \/nas\/datasets<\/code> and then do <code>git pull<\/code> + <code>dvc pull<\/code> as you suggest. This should create more hardlinks, again avoiding duplication, and also allowing non-DVC users to access the data files transparently. (Note: But we expect that dvc will make the files read-only).<\/li>\n<li>Finally, to process the data, the user would create a git project on the server, <code>server:\/home\/user_A\/processing_of_dataset_1\/<\/code>, and then <code>dvc import<\/code> the dataset from the Data Registry, as you suggest. In this case, symlinks are created across filesystems, but that\u2019s fast, and the data doesn\u2019t take up space.<\/li>\n<\/ol>\n<p>Any thoughts? I\u2019m going to give it a try, and report how it goes.<\/p>",
                "Answer_score":57.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-10T18:20:42.097Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/rcasero\">@rcasero<\/a> thanks for moving it here <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/pray.png?v=12\" title=\":pray:\" class=\"emoji\" alt=\":pray:\" loading=\"lazy\" width=\"20\" height=\"20\"> . I hope it\u2019ll be useful for a lot of people.<\/p>\n<aside class=\"quote no-group\" data-username=\"rcasero\" data-post=\"3\" data-topic=\"1294\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/rcasero\/40\/165_2.png\" class=\"avatar\"> rcasero:<\/div>\n<blockquote>\n<p>The reasons to keep the user\u2019s Data Registry clones on the NAS in our case would be, in my opinion:<\/p>\n<\/blockquote>\n<\/aside>\n<p>Let me first try brainstorm with you on this, and then we can discuss the workflow you suggested (that looks like overlaps with what I\u2019m about to write). If you see any discrepancies with what you have in mind, please let me know.<\/p>\n<blockquote>\n<p>There\u2019s not enough space in the home directories for our datasets.<\/p>\n<\/blockquote>\n<p>Yep, but if NAS it attached and DVC cache (<code>.dvc\/cache<\/code> by default) points to NAS, then DVC won\u2019t be creating copies. That\u2019s the beauty of it. It will be manipulating symlinks, data is still staying on NAS.<\/p>\n<p>Does it makes sense? We can discuss I guess some specific commands to make that happen.<\/p>\n<blockquote>\n<p>First, a copy needs to be made to the <code>server:\/home\/user_A\/data_registry\/<\/code> filesystem.<\/p>\n<\/blockquote>\n<p>Same here. DVC can just manipulate symlinks. So you first do <code>git clone<\/code>, then <code>dvc pull dataset1<\/code>. Assuming that DVC is setup correctly (cache points to NAS, symlinks are enabled) this operation should be relatively quick and should not be moving data anywhere. User then can add files into the <code>dataset1<\/code> and do <code>dvc commit<\/code> + <code>git push<\/code> + <code>dvc push<\/code> (if you also use S3 for backup for example).<\/p>\n<p>Same, when they want to use any data - they can do <code>dvc import<\/code> to symlink a specific version of their dataset into their project. Again data is not copied if everything is done correctly.<\/p>\n<blockquote>\n<p>The clarification is that many datasets in <code>\/nas\/datasets<\/code> won\u2019t be under DVC<\/p>\n<\/blockquote>\n<p>That\u2019s fine. Again, on NAS you can keep also a cloned version of the <code>data registry<\/code> + some actual datasets inside it that for now you can gitignore for example.<\/p>\n<hr>\n<p>Once caveat with everything I\u2019ve described so far usually is how to bootstrap this setup initially. How to get the whole dataset on NAS into DVC cache w\/o copying it locally.<\/p>\n<p><img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/exclamation.png?v=12\" title=\":exclamation:\" class=\"emoji\" alt=\":exclamation:\" loading=\"lazy\" width=\"20\" height=\"20\"> Please always have a backup first time you are doing this.<\/p>\n<p>There are two ways. Do this on NAS itself. Like literally:<\/p>\n<pre><code class=\"lang-auto\">cd NAS\/data\ngit init\ndvc init\ndvc config ... # set symlinks\ndvc cache dir ... # set cache location\ndvc add dataset1\n...\ngit commit dataset1.dvc -m \"first version of the first dataset\"\ngit push # to some GH location\n<\/code><\/pre>\n<p>If for some reason Git or DVC do not work well on NAS fs (it happens quite often, since NAS fs can be quite specific). You could use <a href=\"https:\/\/dvc.org\/doc\/command-reference\/add#example-transfer-to-an-external-cache\">example<\/a>.<\/p>",
                "Answer_score":22.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-11T09:40:59.703Z",
                "Answer_body":"<p>Thanks <a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> . I think I have not explained the data workflow too well. So, there isn\u2019t an initial setup on NAS that one person can do, and then users can <code>git clone<\/code> and <code>dvc pull dataset1<\/code>, and maybe add some files, etc.<\/p>\n<p>New datasets will be regularly copied to the NAS drive, by other people, and each dataset will typically be large and with lots of files. So, it\u2019s an ongoing situation, with more and more datasets getting added, and that\u2019s mostly outside of our control. The data <em>will be<\/em> on the NAS, that\u2019s the starting point.<\/p>\n<p>What I\u2019m trying to solve is a situation where multiple users need to now and then process one of those new datasets. So, I want that 1) the user puts the dataset he\/she needs to process under DVC, 2) imports it into their software gitlab project (using DVC to link to the files on the NAS), 3) write their scripts to process the dataset, 4) keep both the dataset and code under version control.<\/p>\n<p>At the same time, those datasets need to be readable by the non-DVC users who put them there (it\u2019s OK if we make the ones under DVC read-only).<\/p>\n<p>Note: I didn\u2019t want to add more complexity to the question, but I actually also want to create new datasets with the processing outputs, and those also live on the NAS, side by side with the raw data.<\/p>\n<p>You are correct that the processing will happen from a (linux) server that mounts the NAS with NFS (let\u2019s assume to <code>\/nas\/datasets<\/code>).<\/p>\n<p>As discussed above, our first attempt at a solution, following <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a> 's suggestions, was to create a Data Registry in <code>\/nas\/datasets<\/code>, to directly <code>dvc add<\/code> datasets as needed. But then we have clashes between users, because they are issuing <code>git<\/code> and <code>dvc<\/code> commands in the same directory.<\/p>\n<p>I think we are both in agreement that 1) it\u2019d be good to have each user have a clone of the data registry and that 2) the folders with the processing scripts and imported datasets should be on the server, 3) we have a shared cache on the NAS for DVC.<\/p>\n<p>But I think that the folder with the processing scripts cannot be used to do the <code>dvc add dataset1<\/code>, because that would require copying the dataset itself to the server home directory (as discussed above, this adds an extra copying of data around, and there\u2019s not enough space for that initial copy of the data that undergoes the <code>dvc add<\/code> anyway).<\/p>\n<p>Maybe it\u2019d simplify the discussion if we think about it this way: \u201cThe data lives on the NAS and cannot leave the NAS\u201d?<\/p>\n<p>Thus, as far as I can tell, the workflow needs to be broken down into two steps:<\/p>\n<ol>\n<li>\n<p>the user has a Data Registry clone on the NAS, and she\/he can <code>dvc add<\/code> the data from there. Then, the user has to <code>dvc pull<\/code> in the common Data Registry, which is the common location where everybody can find all data files. Everything will be hardlinks, so data is not duplicated or actually moved.<\/p>\n<\/li>\n<li>\n<p>the user also has a folder for writing processing scripts on the server, and she\/he can <code>dvc import<\/code> the dataset there, which will happen with softlinks.<\/p>\n<\/li>\n<\/ol>",
                "Answer_score":17.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-11T17:19:49.377Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"rcasero\" data-post=\"5\" data-topic=\"1294\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/rcasero\/40\/165_2.png\" class=\"avatar\"> rcasero:<\/div>\n<blockquote>\n<p>The data lives on the NAS and cannot leave the NAS<\/p>\n<\/blockquote>\n<\/aside>\n<p>I think we are on the same page with this. My point was that it can be done in a way that data always stays on NAS <em>but<\/em> users are doing <code>git clone data-registry<\/code> + <code>dvc add<\/code>, etc in their own home directory. Sorry if that was \/ still not clear from all the answers I gave before. It might be because it\u2019s quite counterintuitive. (or it might be I underestimate, don\u2019t know something - please correct me then).<\/p>\n<blockquote>\n<ol>\n<li>the user also has a folder for writing processing scripts on the server, and she\/he can <code>dvc import<\/code> the dataset there, which will happen with softlinks.<\/li>\n<\/ol>\n<\/blockquote>\n<p>The idea behind updating the data registry is the same. It can be done with soft links. So, let\u2019s imagine the workflow. Let\u2019s say we have a new location on NAS <code>\/nas\/registry\/new-datasets<\/code> and it\u2019s not yet in the data registry. We want to add it (so that it also stays in the NAS, and not data copying happening, etc, etc). We have two options for that.<\/p>\n<p>First is the one you do now (I think). Pretty much <code>\/nas\/registry<\/code> itself is a clone of a data registry. Users can go there, do:<\/p>\n<pre><code class=\"lang-bash\">dvc add new-dataset\ngit commit new-dataset.dvc -m \"add new dataset\"\ngit push\ndvc push # if you have a remote storage for backup\n<\/code><\/pre>\n<p>You concern that multiple users might come simultaneously and try to do this? Can btw describe some exact case when they collide into each other? I would assume this to be rare to be honest.<\/p>\n<p>Second option is to do this:<\/p>\n<pre><code class=\"lang-bash\">cd ~\ngit clone data-registry-repo\ncd  data-registry-repo\n\n# both options below can be setup in the repo config and shared if mount point for NAS is stable\ndvc cache dir \/nas\/registry\/.dvc\/cache\ndvc config cache.type hardlink,symlink \n\n# this command doesn't copy data localy, at least it should not be doing that\ndvc add \/nas\/registry\/new-dataset -o new-dataset\ngit commit new-dataset.dvc -m \"add new dataset\"\ngit push\ndvc push # if you need it\n<\/code><\/pre>\n<p>In the second option, you would still probably want to have <code>\/nas\/registry<\/code> as a Git repo, and you can do <code>git pull<\/code> + <code>dvc checkout --relink new-dataset<\/code> there to also make <code>\/nas\/regiatry\/new-registry<\/code> linked via hardlinks \/ symlinks.<\/p>\n<blockquote>\n<p>Note: I didn\u2019t want to add more complexity to the question, but I actually also want to create new datasets with the processing outputs, and those also live on the NAS, side by side with the raw data.<\/p>\n<\/blockquote>\n<p>That\u2019s interesting and indeed can complicate things. Do you expect all the users to be able to put those processed outputs there? Or some ETL process?<\/p>",
                "Answer_score":57.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-13T02:28:09.373Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/rcasero\">@rcasero<\/a> , I think your problem is similar to our\u2019s, and I\u2019m trying to propose a solution here:<\/p><aside class=\"onebox githubissue\" data-onebox-src=\"https:\/\/github.com\/iterative\/dvc\/issues\/8096\">\n  <header class=\"source\">\n\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/8096\" target=\"_blank\" rel=\"noopener nofollow ugc\">github.com\/iterative\/dvc<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewbox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/8096\" target=\"_blank\" rel=\"noopener nofollow ugc\">Git tracked bare DVC repo (only tracking .dvc file, but don't checkout real file)<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2022-08-05\" data-time=\"05:49:53\" data-timezone=\"UTC\">05:49AM - 05 Aug 22 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/allenyllee\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n          <img alt=\"allenyllee\" src=\"https:\/\/avatars.githubusercontent.com\/u\/3991134?v=4\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          allenyllee\n        <\/a>\n      <\/div>\n    <\/div>\n\n    <div class=\"labels\">\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          awaiting response\n        <\/span>\n        <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">\n          discussion\n        <\/span>\n    <\/div>\n  <\/div>\n<\/div>\n\n  <div class=\"github-row\">\n    <p class=\"github-body-container\"># Background\nWe have a lot of daily generated log file, we want to use dvc to t<span class=\"show-more-container\"><a href=\"\" rel=\"noopener\" class=\"show-more\">\u2026<\/a><\/span><span class=\"excerpt hidden\">racking our daily log.\n\n## Current Method\nIf we want to use dvc to tracking our daily log, for now, we have to:\n1. Create a git repo and `dvc init`\n2. Copy the log files into git repo\n3. `dvc add` those files and `dvc commit` to generate `.dvc` file, and then `dvc push` to transfer files to remote.\n4. `git commit` the generated `.dvc` file, and `git tag` to add a time stamp(or version)\n5. To save local space, remove all the log files, only leave `.dvc` files\n\nWhen new daily logs coming, we need to repeat 2-5 step for tracking.\n\nWhen someone need to analyse log files, they need to: \nClone the git repo, `git checkout` a tagged version, and `dvc checkout` to download files to the local. \n\n## Proposed Method\n1. Provide a single dvc command (something like `dvc init --bare --remote` or a Python API) to create a **\"git tracked bare dvc repo\"** in remote machine\n2. Provide a single dvc command (something like `dvc push --transfer --remote` or a Python API) to directly transfer daily log to the remote, this command has a `--tag` option, it will do the above 2-5 step in the remote machine.\n3. When daily logs coming, just do step 2 to transfer files with version tag. (no need to copy into a local git repo)\n4. When someone need to analyse log files, they can:\nClone the **\"git tracked bare dvc repo\"** with only `.dvc` files, `git checkout` a tagged version, and `dvc checkout` to download files.\n\nFurther, because the **\"git tracked bare dvc repo\"** should only be modified by the data owner, someone can not push their code to the **\"git tracked bare dvc repo\"** remote. Instead, they created a new git repo, and add **\"git tracked bare dvc repo\"** as a another git remote. In the git graph, they can see two parallel line: one for our data repo, one for their code repo. \n\nThey can cherry-pick a commit from data repo, move `.dvc` file into other folder, then do `dvc checkout`, the file will pull from our data repo, downloaded into their folder, then they can start writing their code, commit to their git remote. \n\n## Sum up\nThe **\"git tracked bare dvc repo\"** we can treat it as a combination of `git bare repo` and `dvc cache`, it's a whole structure only for tracking data blob. It can see as a regular git remote, import as a `git submodule`, but can only modified by data owner. For the developer, they just include it, pull the data, do their experiments, push to their own repo without touching the data repo.\n\nAlso, If you don't use git, you can still treat it as a regular dvc cache remote. But with git, you have full power of git!\n\n## Advance\nIf you have multiple data source and want to share a single data repo, one can provide `--source` option in proposed step 2, then the command will create a git branch with provided source name. This newly created branch is parallel to other source branch (with no common commit). From developer's view, they can see many parallel branch resides in data repo, and they just need to pick a branch (a data source) to merge into their local working branch. \n\nIn case the data owner needs to merge two data source into one, it can be as easy as using `git merge` in the data repo, to merge two parallel data source branch into one branch!<\/span><\/p>\n  <\/div>\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>You can join our discussion if you like the proposed idea.<\/p>",
                "Answer_score":47.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-15T08:41:09.396Z",
                "Answer_body":"<p>I like the idea, I\u2019m looking into it, <a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a><\/p>",
                "Answer_score":46.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-21T11:27:31.682Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> ,<\/p>\n<p>I went over your suggestion these last week. Sorry, it takes a bit of time to test these things internally, one has to be careful. I liked it very much, as I said.<\/p>\n<aside class=\"quote no-group\" data-username=\"shcheklein\" data-post=\"6\" data-topic=\"1294\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/shcheklein\/40\/173_2.png\" class=\"avatar\"> shcheklein:<\/div>\n<blockquote>\n<p>You concern that multiple users might come simultaneously and try to do this? Can btw describe some exact case when they collide into each other? I would assume this to be rare to be honest.<\/p>\n<\/blockquote>\n<\/aside>\n<p>This happened as soon as we had a group session to teach dvc. Imagine that you have a directory with the Data Registry, <code>\/nas\/data_registry\/<\/code> (let\u2019s assume it\u2019s configured with autostage, so we don\u2019t need to be doing <code>git add<\/code>)<\/p>\n<pre><code class=\"lang-bash\"># user A\ncd \/nas\/data_registry\/\ndvc add dataset_foo\n\n# user B\ncd \/nas\/data_registry\/\ndvc add dataset_bar\n\n# user A\ngit commit -m \"adding dataset_foo\"\n<\/code><\/pre>\n<p>At this point, user A has committed also <code>dataset_bar<\/code>. And user B will have no idea of what happened.<\/p>\n<p>In addition, there are worse issues, although some of those would probably happen with data registry clones as long as there\u2019s a shared cache. But if we are sharing the same directory, then one user can more easily break it for everyone.<\/p>\n<p>If user A is misconfigured to create files by default with <code>userA<\/code> group, instead of with a common group to all users, suddenly, there are directories\/files in the cache that can only be written by that user. This happens even if dvc is configured with the <code>shared = group<\/code> option. The other users start getting weird \u201cno permission\u201d errors.<\/p>\n<p>Compartmentalising what each user can do in their own data registry clone should prevent at least the first type of problems.<\/p>\n<aside class=\"quote no-group\" data-username=\"shcheklein\" data-post=\"6\" data-topic=\"1294\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/shcheklein\/40\/173_2.png\" class=\"avatar\"> shcheklein:<\/div>\n<blockquote>\n<p>Second option is to do this:<\/p>\n<\/blockquote>\n<\/aside>\n<p>This is the option I\u2019ve tried. It seems to work for me, although I still have to fix some of those datasets. It\u2019s a bit tricky, in the sense that each user needs to remember quite a few steps. Also, it\u2019s fragile in that if you <code>dvc add foo_dataset<\/code> and then again <code>dvc add foo_dataset<\/code>, that creates a <code>foo_dataset\/foo_dataset<\/code>. So, in the end, I\u2019ve written a script so that users have a single simple command to add a dataset with our configuration. The relevant part, based on your suggestions, is<\/p>\n<pre><code class=\"lang-bash\">  # add the dataset to the Data Registry with DVC. Note that we use option \"-o\" to avoid making a\n  # a local copy of the data in this Data Registry clone. Instead, this creates a local directory\n  # with symlinks to the actual data files in the common Data Registry \n  echo_if_verbose \"Putting dataset under DVC\"\n  dvc add \"${dataset_path}\" -o \"${dataset}\"\n\n  # push the changes to the Data Registry project in the Gitlab server. \n  # Note that DVC is configured with autostage, so we don't need to \"git add\" the new dataset.dvc file\n  echo_if_verbose \"Pushing changes to Data Registry project in Gitlab server\"\n  git commit \"${dataset}\".dvc .dvcignore -m \"add dataset ${dataset}\"\n  git push\n\n  # pull the new changes in the common Data Registry\n  echo_if_verbose \"Pulling new changes into common Data Registry: $datasets_dir\"\n  pushd \"$datasets_dir\"\n  git pull\n\n  # relink the dataset files to the cache using hardlinks, so that data is not duplicated\n  echo_if_verbose \"Relinking dataset files to the DVC cache using hardlinks\"\n  dvc checkout --relink \"${dataset}\".dvc\n  popd\n\n  echo_if_verbose \"Dataset successfully added to the Data Registry\"\n<\/code><\/pre>\n<aside class=\"quote no-group\" data-username=\"shcheklein\" data-post=\"6\" data-topic=\"1294\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img loading=\"lazy\" alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/shcheklein\/40\/173_2.png\" class=\"avatar\"> shcheklein:<\/div>\n<blockquote>\n<p>That\u2019s interesting and indeed can complicate things. Do you expect all the users to be able to put those processed outputs there? Or some ETL process?<\/p>\n<\/blockquote>\n<\/aside>\n<p>This is something that I\u2019m still getting my head around. I think I\u2019ll open another thread once I have some positive or negative results with what I\u2019m going to try.<\/p>",
                "Answer_score":66.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"larg data registri na multipl non user post question discord channel shcheklein suggest user benefit discuss copi time ago ask forum manag data na drive singl cach multipl cach na extern data got us advic pmrowla thank suggest set data registri have troubl user get permiss error add new dataset problem constraint bit design folder dataset na larg dataset size number file multipl user need abl save directori file folder add commit push data duplic directori file folder dataset need remain non user read current solut na mount server na look like na cach dataset git dataset dataset configur file hardlink symlink avoid data duplic cach dir na cach share group type hardlink symlink core autostag true immedi problem user add dataset user add dataset user git commit git push commit push dataset issu have look like user add dataset directori creat cach na cach instead have owner user group everybodi owner group user block anybodi oper involv na cach feel user clone data registri na sure work data registri idea suggest welcom thank",
        "Question_preprocessed_content":"larg data registri na multipl non user time ago ask forum manag data na drive got us advic thank suggest set data registri have troubl user get permiss error add new dataset problem constraint bit design folder na larg dataset size number file multipl user need abl save folder data duplic folder need remain non user read current solut na mount server look like configur file cach dir share group type hardlink symlink core autostag true immedi problem user user user dataset issu have look like add dataset directori creat cach instead have owner group owner group block anybodi oper involv feel user clone data registri sure work data registri idea suggest welcom thank",
        "Answer_original_content":"shcheklein repli past hei rcasero good scenario let try figur glad advanc stuff interest wai feel user clone data registri na think goe right direct home directori dont reason keep na git na work usual setup git clone person directori checkout specif dataset symlink case add file add commit git push data duplic directori file folder dataset need remain non user read that interest requir ye git pull pull period think altern end user reli import project best wai dataset right workspac multipl user differ version simultan affect curiou thought workflow edit rcasero dicuss probabl updat discuss forum folk benefit shcheklein thank suggest feedback minor clarif clarif dataset na dataset wont need process dataset point expect dataset read read non user user modifi delet home directori dont reason keep na git na work usual setup git clone person directori checkout specif dataset symlink case add file add commit git push reason user data registri clone na case opinion there space home directori dataset data registri clone server home user data registri user want add new file new dataset need copi home directori wont fit space bit ineffici copi need server home user data registri filesystem dataset ad need file cach na filesystem take time instead make sens like na data registri user data registri user data registri user dataset dataset dataset cach workflow data origin copi na na dataset need process dataset quickli move user data registri na dataset data registri user na dataset dataset na dataset data registri user directori filesystem user na dataset data registri user add dataset commit creat hardlink cach data file hardlink point inod filesystem avoid data duplic move data user goe gener registri na dataset git pull pull suggest creat hardlink avoid duplic allow non user access data file transpar note expect file read final process data user creat git project server server home user process dataset import dataset data registri suggest case symlink creat filesystem that fast data doesnt space thought go try report goe rcasero thank move hope itll us lot peopl rcasero reason user data registri clone na case opinion let try brainstorm discuss workflow suggest look like overlap write discrep mind let know there space home directori dataset yep na attach cach cach default point na wont creat copi that beauti manipul symlink data stai na make sens discuss guess specif command happen copi need server home user data registri filesystem manipul symlink git clone pull dataset assum setup correctli cach point na symlink enabl oper rel quick move data user add file dataset commit git push push us backup exampl want us data import symlink specif version dataset project data copi correctli clarif dataset na dataset wont that fine na clone version data registri actual dataset insid gitignor exampl caveat iv describ far usual bootstrap setup initi dataset na cach copi local backup time wai na like liter na data git init init config set symlink cach dir set cach locat add dataset git commit dataset version dataset git push locat reason git work na happen na specif us exampl thank shcheklein think explain data workflow isnt initi setup na person user git clone pull dataset mayb add file new dataset regularli copi na drive peopl dataset typic larg lot file ongo situat dataset get ad that outsid control data na that start point try solv situat multipl user need process new dataset want user put dataset need process import softwar gitlab project link file na write script process dataset dataset code version control time dataset need readabl non user on read note didnt want add complex question actual want creat new dataset process output live na raw data correct process happen linux server mount na nf let assum na dataset discuss attempt solut follow pmrowla suggest creat data registri na dataset directli add dataset need clash user issu git command directori think agreement itd good user clone data registri folder process script import dataset server share cach na think folder process script add dataset requir copi dataset server home directori discuss add extra copi data there space initi copi data undergo add mayb itd simplifi discuss think wai data live na leav na far tell workflow need broken step user data registri clone na add data user pull common data registri common locat everybodi data file hardlink data duplic actual move user folder write process script server import dataset happen softlink rcasero data live na leav na think page point wai data stai na user git clone data registri add home directori sorri clear answer gave counterintuit underestim dont know correct user folder write process script server import dataset happen softlink idea updat data registri soft link let imagin workflow let new locat na na registri new dataset data registri want add stai na data copi happen option think pretti na registri clone data registri user add new dataset git commit new dataset add new dataset git push push remot storag backup concern multipl user come simultan try btw exact case collid assum rare honest second option git clone data registri repo data registri repo option setup repo config share mount point na stabl cach dir na registri cach config cach type hardlink symlink command copi data locali add na registri new dataset new dataset git commit new dataset add new dataset git push push need second option probabl want na registri git repo git pull checkout relink new dataset na regiatri new registri link hardlink symlink note didnt want add complex question actual want creat new dataset process output live na raw data that interest complic thing expect user abl process output etl process rcasero think problem similar try propos solut github com iter git track bare repo track file checkout real file open aug utc allenylle await respons discuss background lot daili gener log file want us track daili log current method want us track daili log creat git repo init copi log file git repo add file commit gener file push transfer file remot git commit gener file git tag add time stamp version save local space remov log file leav file new daili log come need repeat step track need analys log file need clone git repo git checkout tag version checkout download file local propos method provid singl command like init bare remot python api creat git track bare repo remot machin provid singl command like push transfer remot python api directli transfer daili log remot command tag option step remot machin daili log come step transfer file version tag need copi local git repo need analys log file clone git track bare repo file git checkout tag version checkout download file git track bare repo modifi data owner push code git track bare repo remot instead creat new git repo add git track bare repo git remot git graph parallel line data repo code repo cherri pick commit data repo file folder checkout file pull data repo download folder start write code commit git remot sum git track bare repo treat combin git bare repo cach structur track data blob regular git remot import git submodul modifi data owner develop includ pull data experi push repo touch data repo us git treat regular cach remot git power git advanc multipl data sourc want share singl data repo provid sourc option propos step command creat git branch provid sourc newli creat branch parallel sourc branch common commit develop view parallel branch resid data repo need pick branch data sourc merg local work branch case data owner need merg data sourc easi git merg data repo merg parallel data sourc branch branch join discuss like propos idea like idea look shcheklein shcheklein went suggest week sorri take bit time test thing intern care like said shcheklein concern multipl user come simultan try btw exact case collid assum rare honest happen soon group session teach imagin directori data registri na data registri let assum configur autostag dont need git add user na data registri add dataset foo user na data registri add dataset bar user git commit ad dataset foo point user commit dataset bar user idea happen addit wors issu probabl happen data registri clone long there share cach share directori user easili break user misconfigur creat file default usera group instead common group user suddenli directori file cach written user happen configur share group option user start get weird permiss error compartmentalis user data registri clone prevent type problem shcheklein second option option iv tri work fix dataset bit tricki sens user need rememb step fragil add foo dataset add foo dataset creat foo dataset foo dataset end iv written script user singl simpl command add dataset configur relev base suggest add dataset data registri note us option avoid make local copi data data registri clone instead creat local directori symlink actual data file common data registri echo verbos put dataset add dataset path dataset push chang data registri project gitlab server note configur autostag need git add new dataset file echo verbos push chang data registri project gitlab server git commit dataset ignor add dataset dataset git push pull new chang common data registri echo verbos pull new chang common data registri dataset dir pushd dataset dir git pull relink dataset file cach hardlink data duplic echo verbos relink dataset file cach hardlink checkout relink dataset popd echo verbos dataset successfulli ad data registri shcheklein that interest complic thing expect user abl process output etl process get head think ill open thread posit neg result go try",
        "Answer_preprocessed_content":"hei good scenario let try figur glad advanc stuff interest wai feel user clone data registri think goe right direct home directori dont reason keep na git na work usual setup person directori checkout specif dataset add file data duplic folder dataset need remain non user read that interest requir ye period think altern end user reli project best wai right workspac multipl user differ version simultan affect curiou thought workflow dicuss probabl updat discuss forum folk benefit thank suggest feedback minor clarif clarif dataset wont need process dataset point expect dataset read non user user modifi delet home directori dont reason keep na git na work usual setup person directori checkout specif dataset add file reason user data registri clone na case opinion there space home directori dataset data registri clone user want add new file new dataset need copi home directori wont fit space bit ineffici copi need filesystem dataset ad need file cach na filesystem take time instead make sens like workflow data origin copi na need process dataset quickli move user data registri directori filesystem user commit creat hardlink cach data file hardlink point inod filesystem avoid data duplic move data user goe gener registri suggest creat hardlink avoid duplic allow non user access data file transpar final process data user creat git project server dataset data registri suggest case symlink creat filesystem that fast data doesnt space thought go try report goe thank move hope itll us lot peopl rcasero reason user data registri clone na case opinion let try brainstorm discuss workflow suggest discrep mind let know there space home directori dataset yep na attach cach point na wont creat copi that beauti manipul symlink data stai na make sens discuss guess specif command happen copi need filesystem manipul symlink assum setup correctli oper rel quick move data user add file want us data symlink specif version dataset project data copi correctli clarif dataset wont that fine na clone version actual dataset insid gitignor exampl caveat iv describ far usual bootstrap setup initi dataset na cach copi local backup time wai na like liter reason git work na us exampl thank think explain data workflow isnt initi setup na person user mayb add file new dataset regularli copi na drive peopl dataset typic larg lot file ongo situat dataset get ad that outsid control data na that start point try solv situat multipl user need process new dataset want user put dataset need process import softwar gitlab project write script process dataset dataset code version control time dataset need readabl non user note didnt want add complex question actual want creat new dataset process output live na raw data correct process happen server mount na nf discuss attempt solut follow suggest creat data registri directli dataset need clash user issu command directori think agreement itd good user clone data registri folder process script import dataset server share cach na think folder process script requir copi dataset server home directori mayb itd simplifi discuss think wai data live na leav na far tell workflow need broken step user data registri clone na data user common data registri common locat everybodi data file hardlink data duplic actual move user folder write process script server dataset happen softlink rcasero data live na leav na think page point wai data stai na user home directori sorri clear answer gave counterintuit user folder write process script server dataset happen softlink idea updat data registri soft link let imagin workflow let new locat na data registri want add option pretti clone data registri user concern multipl user come simultan try btw exact case collid assum rare honest second option second option probabl want git repo link hardlink symlink note didnt want add complex question actual want creat new dataset process output live na raw data that interest complic thing expect user abl process output etl process think problem similar try propos solut git track bare repo open aug utc allenylle await respons discuss background lot daili gener log file want us track daili log current method want us track daili log creat git repo copi log file git repo file gener file transfer file remot gener file add time stamp save local space remov log file leav file new daili log come need repeat step track need analys log file need clone git repo tag version download file local propos method provid singl command creat git track bare repo remot machin provid singl command directli transfer daili log remot command option step remot machin daili log come step transfer file version tag need analys log file clone git track bare repo file tag version download file git track bare repo modifi data owner push code git track bare repo remot instead creat new git repo add git track bare repo git remot git graph parallel line data repo code repo commit data repo file folder file pull data repo download folder start write code commit git remot sum git track bare repo treat combin structur track data blob regular git remot import modifi data owner develop includ pull data experi push repo touch data repo us git treat regular cach remot git power git advanc multipl data sourc want share singl data repo provid option propos step command creat git branch provid sourc newli creat branch parallel sourc branch develop view parallel branch resid data repo need pick branch merg local work branch case data owner need merg data sourc easi data repo merg parallel data sourc branch branch join discuss like propos idea like idea look went suggest week sorri take bit time test thing intern care like said shcheklein concern multipl user come simultan try btw exact case collid assum rare honest happen soon group session teach imagin directori data registri point user commit user idea happen addit wors issu probabl happen data registri clone long there share cach share directori user easili break user misconfigur creat file default group instead common group user suddenli cach written user happen configur option user start get weird permiss error compartmentalis user data registri clone prevent type problem shcheklein second option option iv tri work fix dataset bit tricki sens user need rememb step fragil creat end iv written script user singl simpl command add dataset configur relev base suggest shcheklein that interest complic thing expect user abl process output etl process get head think ill open thread posit neg result go try",
        "Question_gpt_summary_original":"the user is encountering challenges with setting up a data registry on a nas drive to allow multiple users to save directories\/files and add\/commit\/push them with git, while avoiding data duplication and allowing non-git users to read the data.",
        "Question_gpt_summary":"user encount challeng set data registri na drive allow multipl user save directori file add commit push git avoid data duplic allow non git user read data",
        "Answer_gpt_summary_original":"possible solutions to the user's challenge of setting up a data registry on a nas drive to allow multiple users to save directories\/files and add\/commit\/push them with git, while avoiding data duplication and allowing non-git users to read the data include manipulating symlinks, using a shared cache on the nas, creating a data registry clone on the nas for each user, and importing datasets with softlinks. additionally, a proposed solution involves creating a \"git tracked bare repo\" on a remote machine to track data blobs and provide a single command to transfer daily logs to the remote with version tags. the \"git tracked bare repo\" can be treated as a combination of a git bare repo and cache and can be imported as a git submodule. it can also be used to merge multiple data sources into one branch.",
        "Answer_gpt_summary":"possibl solut user challeng set data registri na drive allow multipl user save directori file add commit push git avoid data duplic allow non git user read data includ manipul symlink share cach na creat data registri clone na user import dataset softlink addition propos solut involv creat git track bare repo remot machin track data blob provid singl command transfer daili log remot version tag git track bare repo treat combin git bare repo cach import git submodul merg multipl data sourc branch"
    },
    {
        "Question_id":null,
        "Question_title":"RStudio Server stack limit error",
        "Question_body":"I'm using a free account with a default configuration. I'm trying to get through this tutorial https:\/\/docs.microsoft.com\/pl-pl\/azure\/machine-learning\/tutorial-1st-r-experiment.\n\nI encounter the error: C stack usage <big number> is too close to the limit.\n\nBecause of this, I cannot complete the tutorial. I'm wondering how to solve this issue. Is it about free account limitations like memory and\/or CPU?\n\nCan anyone help?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1609858201667,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/219363\/rstudio-server-stack-limit-error.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-11T16:09:10.667Z",
                "Answer_score":0,
                "Answer_body":"Hi, if you're still observing the above error, I found some troubleshooting steps that may be helpful.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"rstudio server stack limit error free account default configur try tutori http doc microsoft com azur machin learn tutori experi encount error stack usag close limit complet tutori wonder solv issu free account limit like memori cpu help",
        "Question_preprocessed_content":"rstudio server stack limit error free account default configur try tutori encount error stack usag close limit complet tutori wonder solv issu free account limit like memori cpu help",
        "Answer_original_content":"observ error troubleshoot step help",
        "Answer_preprocessed_content":"observ error troubleshoot step help",
        "Question_gpt_summary_original":"the user is encountering an error with their free account on rstudio server, which is preventing them from completing a tutorial, and is wondering if it is due to free account limitations.",
        "Question_gpt_summary":"user encount error free account rstudio server prevent complet tutori wonder free account limit",
        "Answer_gpt_summary_original":"the answer does not provide any specific solutions to the error the user is encountering. however, it suggests that there are some troubleshooting steps that may be helpful.",
        "Answer_gpt_summary":"answer provid specif solut error user encount suggest troubleshoot step help"
    },
    {
        "Question_id":null,
        "Question_title":"Using DVC in github codespaces",
        "Question_body":"<p>I\u2019m trying to use dvc in a github codespace and gdrive remote storage. However once I try to \u201cdvc push\u201d<br>\nI get this error<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0ba0f9f145ef2814e08ffd3f4e84be037bd79a34.png\" data-download-href=\"\/uploads\/short-url\/1ES8Pl3nOEFzmQGXs0qEaUQLYRC.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/0ba0f9f145ef2814e08ffd3f4e84be037bd79a34.png\" alt=\"image\" data-base62-sha1=\"1ES8Pl3nOEFzmQGXs0qEaUQLYRC\" width=\"690\" height=\"169\" data-dominant-color=\"282828\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1015\u00d7249 15.3 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1673510308004,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":48.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/using-dvc-in-github-codespaces\/1461",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-12T16:22:28.776Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mouadaqsone\">@mouadAqsone<\/a> ! I haven\u2019t looked in depth, but looks like the authorization method we use for GDrive remotes (<a href=\"https:\/\/github.com\/iterative\/PyDrive2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - iterative\/PyDrive2: Google Drive API Python wrapper library. Maintained fork of PyDrive.<\/a>) doesn\u2019t work inside a GitHub codespace.<\/p>\n<p>As a workaround, I would suggest that you run a first-time <code>dvc push<\/code> on your local machine. This would  create a credentials files in that local machine (<a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#authorization\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">How to Setup a Google Drive DVC Remote<\/a>) and you could use Codespaces secrets (<a href=\"https:\/\/docs.github.com\/en\/codespaces\/managing-your-codespaces\/managing-encrypted-secrets-for-your-codespaces#about-encrypted-secrets-for-github-codespaces\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Managing encrypted secrets for your codespaces - GitHub Docs<\/a>) to expose the contents of that file as an environment variable (<code>GDRIVE_CREDENTIALS_DATA<\/code>) .<\/p>\n<p>I think that would make <code>dvc push<\/code> work inside the codespace<\/p>",
                "Answer_score":66.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-10T11:08:25.145Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>, I\u2019ve finally came back to this as it\u2019s important for our company to work on codespaces. I have made the codespaces secrets with the full content of the credentials files (default.json).<\/p>\n<p>But a dvc pull still just creates a new port and tries to open a new window from which I have to agree to access for the gdrive ( and when I do, i get nothing )<\/p>\n<p>this is my error message<\/p>\n<p>\/usr\/local\/python\/3.10.4\/lib\/python3.10\/site-packages\/oauth2client\/_helpers.py:255: UserWarning: Cannot access \/home\/codespace\/.cache\/pydrive2fs\/710796635688-iivsgbgsb6uv1fap6635dhvuei09o66c.apps.googleusercontent.com\/default.json: No such file or directory<br>\nwarnings.warn(_MISSING_FILE_MESSAGE.format(filename))<br>\nYour browser has been opened to visit:<br>\n\u2026<\/p>\n<p>I\u2019m not very familiar with these types of things, but the logical thing that came to my mind is just copy the default.json file into this folder. Is there a better way than that with codespace Secrets ?<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-11T02:08:41.738Z",
                "Answer_body":"<p>I know it\u2019s super confusing with GDrive, but there are two different credentials files:<\/p>\n<ol>\n<li>\n<p>The one you downloaded from the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#using-a-custom-google-cloud-project-recommended\">Google Cloud console<\/a> (this step is optional though, and I\u2019m not sure if you did it or not).<\/p>\n<\/li>\n<li>\n<p>The one that is generated during the <a href=\"https:\/\/dvc.org\/doc\/user-guide\/how-to\/setup-google-drive-remote#authorization\">Authorization<\/a> process.<\/p>\n<\/li>\n<\/ol>\n<p>You need to get the second one and put it either as an env variable (should be possible to set an env variable in codespaces secrets?) or copy in some location and set up the <code>gdrive_user_credentials_file<\/code> path. Could you please make sure that you are indeed using the second file? What is the structure of the file (field names, etc) that you are copying?<\/p>\n<hr>\n<p>Btw, I definitely recommend taking a look into Dev Containers, and specifically DVC feature <a href=\"https:\/\/github.com\/iterative\/features\" class=\"inline-onebox\">GitHub - iterative\/features: A collection of development container 'features'<\/a> . It simplifies the Codespaces setup with DVC. You can find a bit more sophisticated example here- <a href=\"https:\/\/github.com\/shcheklein\/hackathon\/blob\/main\/.devcontainer.json\" class=\"inline-onebox\">hackathon\/.devcontainer.json at main \u00b7 shcheklein\/hackathon \u00b7 GitHub<\/a><\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"github codespac try us github codespac gdrive remot storag try push error imag",
        "Question_preprocessed_content":"github codespac try us github codespac gdrive remot storag try push error imag",
        "Answer_original_content":"mouadaqson havent look depth look like author method us gdrive remot github iter pydriv googl drive api python wrapper librari maintain fork pydriv doesnt work insid github codespac workaround suggest run time push local machin creat credenti file local machin setup googl drive remot us codespac secret manag encrypt secret codespac github doc expos content file environ variabl gdrive credenti data think push work insid codespac hello daavoo iv final came import compani work codespac codespac secret content credenti file default json pull creat new port tri open new window agre access gdrive error messag usr local python lib python site packag oauthclient helper userwarn access home codespac cach pydrivef iivsgbgsbuvfapdhvueioc app googleusercont com default json file directori warn warn miss file messag format filenam browser open visit familiar type thing logic thing came mind copi default json file folder better wai codespac secret know super confus gdrive differ credenti file download googl cloud consol step option sure gener author process need second env variabl possibl set env variabl codespac secret copi locat set gdrive user credenti file path sure second file structur file field name copi btw definit recommend take look dev contain specif featur github iter featur collect develop contain featur simplifi codespac setup bit sophist exampl hackathon devcontain json main shcheklein hackathon github",
        "Answer_preprocessed_content":"havent look depth look like author method us gdrive remot doesnt work insid github codespac workaround suggest run local machin creat credenti file local machin us codespac secret expos content file environ variabl think work insid codespac hello iv final came import compani work codespac codespac secret content credenti file pull creat new port tri open new window agre access gdrive error messag userwarn access file directori browser open visit familiar type thing logic thing came mind copi file folder better wai codespac secret know super confus gdrive differ credenti file download googl cloud consol gener author process need second env variabl copi locat set path sure second file structur file copi btw definit recommend take look dev contain specif featur github collect develop contain featur simplifi codespac setup bit sophist exampl main github",
        "Question_gpt_summary_original":"the user is encountering an error when attempting to push to github codespace with gdrive remote storage.",
        "Question_gpt_summary":"user encount error attempt push github codespac gdrive remot storag",
        "Answer_gpt_summary_original":"possible solutions to the error encountered when attempting to push to github codespace with gdrive remote storage are:\n\n- run a first-time push on your local machine to create a credentials file and use codespaces secrets to expose the contents of that file as an environment variable.\n- copy the second credentials file into a location and set up the gdrive_user_credentials_file path.\n- use dev containers to simplify the codespaces setup.",
        "Answer_gpt_summary":"possibl solut error encount attempt push github codespac gdrive remot storag run time push local machin creat credenti file us codespac secret expos content file environ variabl copi second credenti file locat set gdrive user credenti file path us dev contain simplifi codespac setup"
    },
    {
        "Question_id":30016116.0,
        "Question_title":"SQL - How to join two tables using values from the other table for missing or null values in either table",
        "Question_body":"<p><strong>Alright StackOverflow, I have a problem:<\/strong><\/p>\n\n<p>I am doing some work with <a href=\"http:\/\/azureml.com\" rel=\"nofollow\" title=\"Azure Machine Learning\">Azure Machine Learning<\/a> and I have reached an impasse. I have two tables, and I need to join them. The tables look like this:<\/p>\n\n<pre><code>   TABLE A          TABLE B   \n+-----------+    +-----------+\n| a | b | c |    | a | b | c |\n+-----------+    +-----------+\n| 1 | 2 |   |    |   | 2 | 3 |\n+-----------+    +-----------+\n<\/code><\/pre>\n\n<p>(those are just examples.)<\/p>\n\n<p>I need to join these tables when columns they share (in this case only b, but could be multiple) are equivalent. I also, however, need to populate missing values. If TABLE A is missing a value for one of its columns, and TABLE B has it for a matching row, they should combine values in the result table. I know that there is a way to do this one way, but it also needs to work in reverse, so that if TABLE B is missing a value, and TABLE A has one, it is populated.<\/p>\n\n<p><strong>EDIT: Desired Result:<\/strong><\/p>\n\n<pre><code>   TABLE C\n+-----------+\n| a | b | c |\n+-----------+\n| 1 | 2 | 3 |\n+-----------+\n<\/code><\/pre>\n\n<p>Some background information:<\/p>\n\n<ul>\n<li>AzureML uses a form of SQLite for their SQL interpretation, so please try and keep your answers in as basic SQL as possible. Thanks! :)<\/li>\n<li>AzureML has a built in join module, for those familiar with AzureML, but I don't think it'll be able to accomplish what is necessary. I'll use the SQL interpretation module.<\/li>\n<\/ul>\n\n<p>Your assistance is appreciated! Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1430669511030,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1430676924820,
        "Question_score":0.0,
        "Question_view_count":90.0,
        "Answer_body":"<p><strong>Answering my own question:<\/strong><\/p>\n\n<p>It turned out the join type I needed was a <em>Full Outer Join.<\/em><\/p>\n\n<p>Background information:<\/p>\n\n<ul>\n<li>For those pursuing AzureML related to this question in the future, I had to enable the functionality to save columns from the 'Right' table input.<\/li>\n<li>I then ran this through a 'Project Columns' module and a 'Metadata' module to rename them to the form I desired.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30016116",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1430677708887,
        "Question_original_content":"sql join tabl valu tabl miss null valu tabl alright stackoverflow problem work reach impass tabl need join tabl look like tabl tabl exampl need join tabl column share case multipl equival need popul miss valu tabl miss valu column tabl match row combin valu result tabl know wai wai need work revers tabl miss valu tabl popul edit desir result tabl background inform us form sqlite sql interpret try answer basic sql possibl thank built join modul familiar think abl accomplish necessari us sql interpret modul assist appreci thank",
        "Question_preprocessed_content":"sql join tabl valu tabl miss null valu tabl alright stackoverflow problem work reach impass tabl need join tabl look like need join tabl column share equival need popul miss valu tabl miss valu column tabl match row combin valu result tabl know wai wai need work revers tabl miss valu tabl popul edit desir result background inform us form sqlite sql interpret try answer basic sql possibl thank built join modul familiar think abl accomplish necessari us sql interpret modul assist appreci thank",
        "Answer_original_content":"answer question turn join type need outer join background inform pursu relat question futur enabl function save column right tabl input ran project column modul metadata modul renam form desir",
        "Answer_preprocessed_content":"answer question turn join type need outer join background inform pursu relat question futur enabl function save column right tabl input ran project column modul metadata modul renam form desir",
        "Question_gpt_summary_original":"the user is encountering a challenge in joining two tables using values from the other table for missing or null values in either table.",
        "Question_gpt_summary":"user encount challeng join tabl valu tabl miss null valu tabl",
        "Answer_gpt_summary_original":"the solution to the challenge of joining two tables using values from the other table for missing or null values is to use a full outer join. the user had to enable the functionality to save columns from the 'right' table input and then run it through a 'project columns' module and a 'metadata' module to rename them to the desired form.",
        "Answer_gpt_summary":"solut challeng join tabl valu tabl miss null valu us outer join user enabl function save column right tabl input run project column modul metadata modul renam desir form"
    },
    {
        "Question_id":null,
        "Question_title":"How to set the environment variable WANDB_IGNORE_GLOBS correctly?",
        "Question_body":"<p>The usage in the <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/environment-variables#optional-environment-variables\">Docs<\/a> is:<\/p>\n<blockquote>\n<p>Set this to a comma separated list of file globs to ignore. These files will not be synced to the cloud<\/p>\n<\/blockquote>\n<p>So, is the below code correct?<\/p>\n<pre><code class=\"lang-python\">os.environ['WANDB_IGNORE_GLOBS'] = '[*.pth, *.npy]'\n<\/code><\/pre>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1668582283551,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":77.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> thank you for writing in! Could you please check if the following would work for you?<\/p>\n<pre><code class=\"lang-auto\">os.environ['WANDB_IGNORE_GLOBS'] = '*.pth,*.npy'\n<\/code><\/pre>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-set-the-environment-variable-wandb-ignore-globs-correctly\/3423",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-17T13:08:01.706Z",
                "Answer_body":"<p>The code seems to be incorrect because the *.npy files still be uploaded (the *.pth files are ignored successfully).<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-17T15:17:40.971Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> thank you for writing in! Could you please check if the following would work for you?<\/p>\n<pre><code class=\"lang-auto\">os.environ['WANDB_IGNORE_GLOBS'] = '*.pth,*.npy'\n<\/code><\/pre>",
                "Answer_score":5.6,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-11-18T03:13:06.203Z",
                "Answer_body":"<p>Thanks for your reply, it works! I also find that it fails if there are spaces after the comma separator.<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-18T15:51:35.267Z",
                "Answer_body":"<p>Great to hear <a class=\"mention\" href=\"\/u\/geyao\">@geyao<\/a> this now works - and thanks for adding the additional information, indeed I confirmed it won\u2019t work with spaces in between. Have a great weekend!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-17T15:51:37.426Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1668698260971,
        "Question_original_content":"set environ variabl ignor glob correctli usag doc set comma separ list file glob ignor file sync cloud code correct environ ignor glob pth npy",
        "Question_preprocessed_content":"set environ variabl correctli usag doc set comma separ list file glob ignor file sync cloud code correct",
        "Answer_original_content":"geyao thank write check follow work environ ignor glob pth npy",
        "Answer_preprocessed_content":"thank write check follow work",
        "Question_gpt_summary_original":"the user is trying to figure out how to correctly set the environment variable _ignore_globs in order to ignore certain files from being synced to the cloud.",
        "Question_gpt_summary":"user try figur correctli set environ variabl ignor glob order ignor certain file sync cloud",
        "Answer_gpt_summary_original":"the solution to ignore certain files from being synced to the cloud is to set the environment variable _ignore_globs to the desired file types, such as '*.pth,*.npy'.",
        "Answer_gpt_summary":"solut ignor certain file sync cloud set environ variabl ignor glob desir file type pth npy"
    },
    {
        "Question_id":67051900.0,
        "Question_title":"How can I generate prediction intervals for Azure AutoML timeseries forecasts?",
        "Question_body":"<p>Is it possible to generate prediction intervals for time series forecasts when using a Azure AutoML trained models? Could we get the training errors out of the process and use them for bootstrapping?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618193869380,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":205.0,
        "Answer_body":"<p>You can generate forecast quantiles. See the following notebook for more details: <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/automated-machine-learning\/forecasting-forecast-function\/auto-ml-forecasting-function.ipynb<\/a><\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67051900",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1618942132740,
        "Question_original_content":"gener predict interv azur automl timeseri forecast possibl gener predict interv time seri forecast azur automl train model train error process us bootstrap",
        "Question_preprocessed_content":"gener predict interv azur automl timeseri forecast possibl gener predict interv time seri forecast azur automl train model train error process us bootstrap",
        "Answer_original_content":"gener forecast quantil follow notebook detail http github com azur machinelearningnotebook blob master us autom machin learn forecast forecast function auto forecast function ipynb",
        "Answer_preprocessed_content":"gener forecast quantil follow notebook detail",
        "Question_gpt_summary_original":"the user is looking to generate prediction intervals for time series forecasts when using an azure automl trained model and is wondering if they can get the training errors out of the process to use for bootstrapping.",
        "Question_gpt_summary":"user look gener predict interv time seri forecast azur automl train model wonder train error process us bootstrap",
        "Answer_gpt_summary_original":"possible solution: the user can generate forecast quantiles using an azure automl trained model. they can refer to a specific notebook for more details on how to do this.",
        "Answer_gpt_summary":"possibl solut user gener forecast quantil azur automl train model refer specif notebook detail"
    },
    {
        "Question_id":null,
        "Question_title":"Can I add OpenAPI specification to a webservice deployed with AzureML in AKS?",
        "Question_body":"I'd like to deploy a machine learning service using AzureML on AKS. I also need to add some OpenAPI specification for it.\n\nFeatures in https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-azure-kubernetes-service?tabs=python are neat, but that of having API docs\/swagger for the webservice seems missing.\n\nHaving some documentation is useful especially if the model takes in input several features of different type.\n\nTo overcome this, I currently get models trained in AzureML and include them in Docker containers that use the python FastAPI library to build the API and OpenAPI\/Swagger specs, and those are deployed on some host.\n\nCan I do something equivalent to this with AKS in AzureML instead? If so, how?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1600897231890,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@DavideFiocco-7346 The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available here. You can print your swagger_uri of the web service and check if it confirms with the specifications you are creating currently.\n\nIf the above response helps, please accept the response as answer. Thanks!!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/105437\/can-i-add-openapi-specification-to-a-webservice-de.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-09-24T06:54:05.547Z",
                "Answer_score":2,
                "Answer_body":"@DavideFiocco-7346 The deployments of Azure ML provide a swagger specification URI that can be used directly. The documentation of this is available here. You can print your swagger_uri of the web service and check if it confirms with the specifications you are creating currently.\n\nIf the above response helps, please accept the response as answer. Thanks!!",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1600930445547,
        "Question_original_content":"add openapi specif webservic deploi ak like deploi machin learn servic ak need add openapi specif featur http doc microsoft com azur machin learn deploi azur kubernet servic tab python neat have api doc swagger webservic miss have document us especi model take input featur differ type overcom current model train includ docker contain us python fastapi librari build api openapi swagger spec deploi host equival ak instead",
        "Question_preprocessed_content":"add openapi specif webservic deploi ak like deploi machin learn servic ak need add openapi specif featur neat have api webservic miss have document us especi model take input featur differ type overcom current model train includ docker contain us python fastapi librari build api spec deploi host equival ak instead",
        "Answer_original_content":"davidefiocco deploy provid swagger specif uri directli document avail print swagger uri web servic check confirm specif creat current respons help accept respons answer thank",
        "Answer_preprocessed_content":"deploy provid swagger specif uri directli document avail print web servic check confirm specif creat current respons help accept respons answer thank",
        "Question_gpt_summary_original":"the user is looking for a way to deploy a machine learning service on aks with openapi\/swagger specifications for the webservice.",
        "Question_gpt_summary":"user look wai deploi machin learn servic ak openapi swagger specif webservic",
        "Answer_gpt_summary_original":"possible solutions: \n- use the swagger specification uri provided by the aks deployment directly.\n- check if the swagger_uri of the web service confirms with the specifications being created.",
        "Answer_gpt_summary":"possibl solut us swagger specif uri provid ak deploy directli check swagger uri web servic confirm specif creat"
    },
    {
        "Question_id":64170759.0,
        "Question_title":"Pyathena is super slow compared to querying from Athena",
        "Question_body":"<p>I run a query from AWS <strong>Athena console<\/strong> and takes 10s.\nThe same query run from <strong>Sagemaker<\/strong> using <strong>PyAthena<\/strong> takes 155s.\nIs PyAthena slowing it down or is the data transfer from Athena to sagemaker so time consuming?<\/p>\n<p>What could I do to speed this up?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1601638106830,
        "Question_favorite_count":null,
        "Question_last_edit_time":1601641259400,
        "Question_score":3.0,
        "Question_view_count":3188.0,
        "Answer_body":"<p>Just figure out a way of boosting the queries:<\/p>\n<p>Before I was trying:<\/p>\n<pre><code>import pandas as pd\nfrom pyathena import connect\n\nconn = connect(s3_staging_dir=STAGIN_DIR,\n             region_name=REGION)\npd.read_sql(QUERY, conn)\n# takes 160s\n<\/code><\/pre>\n<p>Figured out that using a <em>PandasCursor<\/em> instead of a <em>connection<\/em> is way faster<\/p>\n<pre><code>import pandas as pd\npyathena import connect\nfrom pyathena.pandas.cursor import PandasCursor\n\ncursor = connect(s3_staging_dir=STAGIN_DIR,\n                 region_name=REGION,\n                 cursor_class=PandasCursor).cursor()\ndf = cursor.execute(QUERY).as_pandas()\n# takes 12s\n<\/code><\/pre>\n<p>Ref: <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46\" rel=\"noreferrer\">https:\/\/github.com\/laughingman7743\/PyAthena\/issues\/46<\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1629380324768,
        "Answer_score":13.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64170759",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1601639323903,
        "Question_original_content":"pyathena super slow compar queri athena run queri aw athena consol take queri run pyathena take pyathena slow data transfer athena time consum speed",
        "Question_preprocessed_content":"pyathena super slow compar queri athena run queri aw athena consol take queri run pyathena take pyathena slow data transfer athena time consum speed",
        "Answer_original_content":"figur wai boost queri try import panda pyathena import connect conn connect stage dir stagin dir region region read sql queri conn take figur pandascursor instead connect wai faster import panda pyathena import connect pyathena panda cursor import pandascursor cursor connect stage dir stagin dir region region cursor class pandascursor cursor cursor execut queri panda take ref http github com laughingman pyathena issu",
        "Answer_preprocessed_content":"figur wai boost queri try figur pandascursor instead connect wai faster ref",
        "Question_gpt_summary_original":"the user is experiencing a significant slowdown when running a query from pyathena compared to running the same query from the aws athena console, and is looking for ways to speed up the process.",
        "Question_gpt_summary":"user experienc signific slowdown run queri pyathena compar run queri aw athena consol look wai speed process",
        "Answer_gpt_summary_original":"the answer suggests using a pandas cursor instead of a connection to speed up the query process when using pyathena. the user can modify their code to use the pandas cursor and execute the query, which should significantly reduce the query time.",
        "Answer_gpt_summary":"answer suggest panda cursor instead connect speed queri process pyathena user modifi code us panda cursor execut queri significantli reduc queri time"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning: I cannot find experiment's user logs located in logs\/user folder",
        "Question_body":"I am running experiments in Azure Machine Learning using ParallelRunStep, and I cannot get the user folder with logs as defined in readme.txt file with the log folder structure.\nI cannot find log\/user folder with \"Logs generated when loading and running user's scripts.\"\n\nreadme.txt file states:\nParallelRunStep has two major parts:\n1. Scheduling, progress tracking and file concatenation for append_row.\n2. Processing mini batch by calling the entry script.\nThe agent manager on each node start agents.\nAn agent gets mini batch and calls the entry script against the mini batch.\n\n The \"logs\" folder has user, sys and perf sub folders.\n The user folder includes messages from the entry script in processing mini batches.\n The sys folder includes messages from #1 and non-entry script log from #2.\n The perf folder includes periodical checking result of resource usage.\n\n\n\nIn majority case, users can find the processing messages from the user folder.\nUsers need to check sys folder for messages beyond processing mini batches.\nlogs\/\nazureml\/: Logs from azureml dependencies. e.g. azureml.dataprep\nuser\/ : Logs generated when loading and running user's scripts.\nerror\/ : Logs of errors encountered while loading and running entry script.\nstderr\/ : stderr output of user's scripts.\nstdout\/ : stdout output of user's scripts.\nentry_script_log\/ : Logs generated by loggers of EntryScript()\n<node seq> :\nprocessNNN.log.txt : Logs generated by loggers of EntryScript() from each process.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1645621539517,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@CalabriaMonteroSalvadorSGRESEDFPDC-5704 Thanks for the question. Please follow the doc to view and log files for a run. Interactive logging sessions are typically used in notebook environments. The method Experiment.start_logging() starts an interactive logging session. Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics#view-and-download-log-files-for-a-run",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/747549\/azure-machine-learning-i-cannot-find-experiment39s.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-24T02:37:03.417Z",
                "Answer_score":0,
                "Answer_body":"@CalabriaMonteroSalvadorSGRESEDFPDC-5704 Thanks for the question. Please follow the doc to view and log files for a run. Interactive logging sessions are typically used in notebook environments. The method Experiment.start_logging() starts an interactive logging session. Any metrics logged during the session are added to the run record in the experiment. The method run.complete() ends the sessions and marks the run as completed.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-log-view-metrics#view-and-download-log-files-for-a-run",
                "Answer_comment_count":4,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1645670223416,
        "Question_original_content":"experi user log locat log user folder run experi parallelrunstep user folder log defin readm txt file log folder structur log user folder log gener load run user script readm txt file state parallelrunstep major part schedul progress track file concaten append row process mini batch call entri script agent manag node start agent agent get mini batch call entri script mini batch log folder user sy perf sub folder user folder includ messag entri script process mini batch sy folder includ messag non entri script log perf folder includ period check result resourc usag major case user process messag user folder user need check sy folder messag process mini batch log log depend dataprep user log gener load run user script error log error encount load run entri script stderr stderr output user script stdout stdout output user script entri script log log gener logger entryscript process log txt log gener logger entryscript process",
        "Question_preprocessed_content":"experi user log locat folder run experi parallelrunstep user folder log defin file log folder structur folder log gener load run user file state parallelrunstep major part schedul progress track file concaten process mini batch call entri script agent manag node start agent agent get mini batch call entri script mini batch log folder user sy perf sub folder user folder includ messag entri script process mini batch sy folder includ messag script log perf folder includ period check result resourc usag major case user process messag user folder user need check sy folder messag process mini batch log log depend dataprep user log gener load run user script error log error encount load run entri script stderr stderr output user script stdout stdout output user script log gener logger entryscript log gener logger entryscript process",
        "Answer_original_content":"calabriamonterosalvadorsgresedfpdc thank question follow doc view log file run interact log session typic notebook environ method experi start log start interact log session metric log session ad run record experi method run complet end session mark run complet http doc microsoft com azur machin learn log view metric view download log file run",
        "Answer_preprocessed_content":"thank question follow doc view log file run interact log session typic notebook environ method start interact log session metric log session ad run record experi method end session mark run complet",
        "Question_gpt_summary_original":"the user is encountering challenges in locating the user logs located in the logs\/user folder as defined in the readme.txt file.",
        "Question_gpt_summary":"user encount challeng locat user log locat log user folder defin readm txt file",
        "Answer_gpt_summary_original":"the answer suggests following the documentation to view and log files for a run. it also recommends using interactive logging sessions, which can be started with the method experiment.start_logging(). any metrics logged during the session are added to the run record in the experiment. the method run.complete() ends the session and marks the run as completed. the link to the documentation is provided for further guidance.",
        "Answer_gpt_summary":"answer suggest follow document view log file run recommend interact log session start method experi start log metric log session ad run record experi method run complet end session mark run complet link document provid guidanc"
    },
    {
        "Question_id":null,
        "Question_title":"Can the command `dvc add` just add new and changed files?",
        "Question_body":"<p>I have just started playing with dvc to investigate whether it can help me adding version control to large sets of images used for deep learning.<\/p>\n<p>I made my own <em>small<\/em> toy example to try to understand how dvc works.<\/p>\n<p>I have a data folder with 10 images of approx 10 MB each, in total ~100 MB<\/p>\n<p>When I run \u201cdvc add data\u201d I can see that the dvc cache grows with ~100 MB<\/p>\n<p>I now replace one of the images in my data folder with another one, i.e. remove one 10 MB image and add another 10 MB image. The total size is still 100 MB<\/p>\n<p>When I run \u201cdvc add data\u201d again to track the changes, the dvc cache grows by another 100 MB! I was thinking that dvc would automatically add the difference between the data sets, but that does not seems to be the case? Does it <em>copy<\/em> all the content for each \u201cdvc add\u201d command?<\/p>\n<p>If yes, is there any practically convenient way to make dvc detect and store the <em>difference<\/em> between versions of data sets?<\/p>\n<p>Thanks,<br>\nSven<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1575200338304,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":1005.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/can-the-command-dvc-add-just-add-new-and-changed-files\/262",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-12-01T11:58:20.667Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/sven\">@Sven<\/a> !<\/p>\n<p>That is odd, how do you check the size of the cache dir? Dvc should already store all old files plus new file, so it should be 110MB total in the cache, that is a core feature for us.<\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-12-01T18:24:48.191Z",
                "Answer_body":"<p>Hi. Here\u2019s a test I ran with 3 5mb files, which seems correct (no file duplication):<\/p>\n<pre><code class=\"lang-auto\">$ dvc version\nDVC version: 0.71.0\nPython version: 3.7.5\nPlatform: Darwin-19.0.0-x86_64-i386-64bit\nBinary: False\nPackage: brew\nCache: reflink - True, hardlink - True, symlink - True\n\n$ mkdir test &amp;&amp; cd test\n$ dvc init --no-scm\n...\n$ mkdir data\n$ head -c 5242880 \/dev\/urandom &gt; data\/5mbfile1\n$ head -c 5242880 \/dev\/urandom &gt; data\/5mbfile2\n$ ls -lAFh\ndrwxr-xr-x   5 usr  staff   160B Dec  1 12:07 .dvc\/\ndrwxr-xr-x   4 usr  staff   128B Dec  1 12:11 data\/\n$ ls -lAFh data\n-rw-r--r--  1 usr  staff   5.0M Dec  1 12:11 5mbfile1\n-rw-r--r--  1 usr  staff   5.0M Dec  1 12:11 5mbfile2\n$ dvc add data\n100% Add ...\n$ du -sh .dvc\/cache\n 10M\t.dvc\/cache\n$ rm -f data\/5mbfile2\n$ head -c 5242880 \/dev\/urandom &gt; data\/5mbfile3\n$ ls -lAFh data\n-rw-r--r--  1 usr  staff   5.0M Dec  1 12:11 5mbfile1\n-rw-r--r--  1 usr  staff   5.0M Dec  1 12:11 5mbfile3\n$ dvc status\ndata.dvc:                                                                                                                                    \n\tchanged outs:\n\t\tmodified:           data\n$ dvc add data\nWARNING: Output 'data' of 'data.dvc' changed because it is 'modified'                                                                        \n100% Add ...\n$ du -sh .dvc\/cache\n 15M\t.dvc\/cache\n<\/code><\/pre>\n<p>Maybe you can help us reproduce the problem with instructions such as these? Thanks<\/p>",
                "Answer_score":107.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-12-03T14:19:34.958Z",
                "Answer_body":"<p>Thanks for your replies and help!<\/p>\n<p>Having tested the commands supplied by jorgeorpinel, it see that \u201cdu -sh\u201d does not report the same folder size as right clicking on it and selecting \u201cproperties\u201d in Windows explorer.<\/p>\n<p>If I take \u201cdu -sh\u201d as the truth, everything works as expected. I guess that Windows does not does not compute the disk size on disk correctly?<\/p>",
                "Answer_score":52.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-12-03T18:20:44.753Z",
                "Answer_body":"<p>We do indeed trust <code>du -sh<\/code> a lot <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"> Not sure how that is implemented in Windows explorer. Our team member also noted that it might be caused by it not quite understanding the hardlinks we use for some optimisation and counting them twice. Sounds reasonable to me as well <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>",
                "Answer_score":21.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"command add add new chang file start plai investig help ad version control larg set imag deep learn small toi exampl try understand work data folder imag approx total run add data cach grow replac imag data folder remov imag add imag total size run add data track chang cach grow think automat add differ data set case copi content add command ye practic conveni wai detect store differ version data set thank sven",
        "Question_preprocessed_content":"command add new chang file start plai investig help ad version control larg set imag deep learn small toi exampl try understand work data folder imag approx total run add data cach grow replac imag data folder remov imag add imag total size run add data track chang cach grow think automat add differ data set case copi content add command ye practic conveni wai detect store differ version data set thank sven",
        "Answer_original_content":"sven odd check size cach dir store old file plu new file total cach core featur here test ran file correct file duplic version version python version platform darwin bit binari fals packag brew cach reflink true hardlink true symlink true mkdir test test init scm mkdir data head dev urandom data mbfile head dev urandom data mbfile lafh drwxr usr staff dec drwxr usr staff dec data lafh data usr staff dec mbfile usr staff dec mbfile add data add cach cach data mbfile head dev urandom data mbfile lafh data usr staff dec mbfile usr staff dec mbfile statu data chang out modifi data add data warn output data data chang modifi add cach cach mayb help reproduc problem instruct thank thank repli help have test command suppli jorgeorpinel report folder size right click select properti window explor truth work expect guess window comput disk size disk correctli trust lot sure implement window explor team member note caus understand hardlink us optimis count twice sound reason",
        "Answer_preprocessed_content":"odd check size cach dir store old file plu new file total cach core featur here test ran file correct mayb help reproduc problem instruct thank thank repli help have test command suppli jorgeorpinel report folder size right click select properti window explor truth work expect guess window comput disk size disk correctli trust lot sure implement window explor team member note caus understand hardlink us optimis count twice sound reason",
        "Question_gpt_summary_original":"the user is trying to understand how the `add` command works and whether it can help them add version control to large sets of images used for deep learning, but is encountering challenges with the command adding the difference between the data sets.",
        "Question_gpt_summary":"user try understand add command work help add version control larg set imag deep learn encount challeng command ad differ data set",
        "Answer_gpt_summary_original":"there are no clear solutions provided in the answer. the answer is discussing a test run of the `add` command and how it works with a set of files. the user is also discussing a potential issue with the reported folder size in windows explorer versus the size reported by the `du -sh` command.",
        "Answer_gpt_summary":"clear solut provid answer answer discuss test run add command work set file user discuss potenti issu report folder size window explor versu size report command"
    },
    {
        "Question_id":null,
        "Question_title":"Azure AutoML time series model returns strange forecast",
        "Question_body":"I used Azure AutoML to train a time series forecasting model and selected the forecasting horizon to be 6. Each of our data row is one month, so we want to see the forecast for the following 6 months.\n\nHowever, when feeding 2 rows of data to the model, it returns 2 figures, and when feeding 8 rows of data, it returns 8 figures. We expect that as we select the forecasting horizon to be 6, regardless of how many rows of data being fed into the model, it should returns 6 figures.\n\nCould somebody explain why this happens and how to correct it? Thank you.",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1604413410307,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/149896\/azure-automl-time-series-model-returns-strange-for.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-04T05:17:37.92Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI would suggest you double check how you are defining time_column_name. Thanks.\n\nAttached the definition of this class for your reference,\n\n fulldata: pandas.DataFrame           a time series dataset. Needs to contain X and y.\n time_column_name: string             which column (must be in fulldata) is the time axis\n target_column_name: string           which column (must be in fulldata) is to be forecast\n forecast_origin: datetime type       the last time we (pretend to) have target values \n horizon: timedelta                   how far forward, in time units (not periods)\n lookback: timedelta                  how far back does the model look?\n\n\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-26T03:31:04.907Z",
                "Answer_score":0,
                "Answer_body":"I think it will forecast the number of rows you feed into the forecast function (feed datetime with empty column for output). So the behaviour you see is expected.\n\nIdeally you feed in no more than your training forecast horizon. If you feed more it wil become increasingly inaccurate as it starts to use the previously forecast value as input for the \"over the horizon\" forecasts.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"azur automl time seri model return strang forecast azur automl train time seri forecast model select forecast horizon data row month want forecast follow month feed row data model return figur feed row data return figur expect select forecast horizon regardless row data fed model return figur somebodi explain happen correct thank",
        "Question_preprocessed_content":"azur automl time seri model return strang forecast azur automl train time seri forecast model select forecast horizon data row month want forecast follow month feed row data model return figur feed row data return figur expect select forecast horizon regardless row data fed model return figur somebodi explain happen correct thank",
        "Answer_original_content":"hello suggest doubl check defin time column thank attach definit class refer fulldata panda datafram time seri dataset need contain time column string column fulldata time axi target column string column fulldata forecast forecast origin datetim type time pretend target valu horizon timedelta far forward time unit period lookback timedelta far model look regard yutong think forecast number row feed forecast function feed datetim column output behaviour expect ideal feed train forecast horizon feed wil increasingli inaccur start us previous forecast valu input horizon forecast",
        "Answer_preprocessed_content":"hello suggest doubl check defin thank attach definit class refer fulldata time seri dataset need contain string column time axi string column forecast datetim type time target valu horizon timedelta far forward time unit lookback timedelta far model look regard yutong think forecast number row feed forecast function behaviour expect ideal feed train forecast horizon feed wil increasingli inaccur start us previous forecast valu input horizon forecast",
        "Question_gpt_summary_original":"the user encountered an issue with azure automl where their time series forecasting model was returning an unexpected number of figures when fed different amounts of data.",
        "Question_gpt_summary":"user encount issu azur automl time seri forecast model return unexpect number figur fed differ amount data",
        "Answer_gpt_summary_original":"the solution suggested for the issue with azure automl's time series forecasting model is to double check how the time_column_name is defined. the model requires a pandas dataframe with a time series dataset containing x and y, and the time_column_name and target_column_name must be specified. the forecast_origin, horizon, and lookback must also be defined. the behavior of the model is expected to forecast the number of rows fed into the forecast function, and it is recommended to feed in no more than the training forecast horizon to avoid increasing inaccuracies.",
        "Answer_gpt_summary":"solut suggest issu azur automl time seri forecast model doubl check time column defin model requir panda datafram time seri dataset contain time column target column specifi forecast origin horizon lookback defin behavior model expect forecast number row fed forecast function recommend feed train forecast horizon avoid increas inaccuraci"
    },
    {
        "Question_id":null,
        "Question_title":"Deleting data from self-hosted server",
        "Question_body":"<p>I saw <a href=\"https:\/\/community.wandb.ai\/t\/delete-files-from-a-run\/1031\">this<\/a> post, but it doesn\u2019t answer my question. We are running a self-hosted wandb instance. We have somewhat limited space, though we\u2019ve been good about deleting old runs through the interface.<\/p>\n<p>We thought deleting from the interface would also delete the physical files from the hard disk, but that doesn\u2019t appear to be the case. For example, going into the minio folder on the server shows a particular project taking up 130 GB of space, while wandb reports 30 GB. That\u2019s a big difference!<\/p>\n<p>How do we really really delete files from minio that wandb no longer knows anything about?<\/p>\n<p>Suggestion: please have a central <code>\/usage\/<\/code> URL for admins to look through usage from all teams and users rather than having to go through each one individually!<\/p>",
        "Question_answer_count":13,
        "Question_comment_count":0,
        "Question_creation_time":1655303395067,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1072.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/deleting-data-from-self-hosted-server\/2613",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-17T18:27:10.321Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/tkott\">@tkott<\/a>, I\u2019m looking into this but I believe there isn\u2019t a way through the UI to physically delete the files from your hard drive. I\u2019ll look into this more to confirm this and if this is indeed the case, I can put in a feature request for you to make this possible through the UI.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":22.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-17T18:43:33.000Z",
                "Answer_body":"<p>Hi Nate \u2013 so how do you suggest that we free up space generally then? Do we have to note the hash \/ digest of files and look for them on the minio server by hand?<\/p>\n<p>I would expect that if you \u201cdelete\u201d from the server with a big scary \u201cthis is a permanent operation\u201d type warning, that the server will, in fact, remove the files. If it doesn\u2019t remove the files, it isn\u2019t actually a permanent operation since the files can be salvaged (albeit with some work).<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/c068a531a4cb060559186c287c0508c8c7321469.jpeg\" alt=\"image001.jpg\" data-base62-sha1=\"rs7SiFek7tnQtx4wt2yshERms0p\" width=\"79\" height=\"79\"><\/p>",
                "Answer_score":27.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-22T15:10:19.015Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/nathank\">@nathank<\/a> any suggestions for how do it programmatically through wandb in the meantime?<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-12T20:09:02.108Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tkott\">@tkott<\/a>,<br>\nSorry for the delay here.  A member of our team put together a script to do this for you <a href=\"https:\/\/gist.github.com\/venky-wandb\/b95542f107b377fb9e1ad2c811201bbe\" rel=\"noopener nofollow ugc\">here<\/a>.<\/p>\n<p>You can use this by running <code>python file_cleanup.py -d 10<\/code> where the <code>-d<\/code> tag can specify how many days ago a run has to have been deleted from the UI in order for the script to delete it from Minio storage.<\/p>\n<p>Please note that this will only work if you are using our bare-metal Docker container setup. If you have connected an external database this will not work.<\/p>\n<p>Also, we are working towards making this a default part of our local deployment where you can set a retention policy for deleted runs and the server will automatically clean up deleted runs after a certain amount of time .<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":16.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-12T20:57:53.000Z",
                "Answer_body":"<p>Thanks! If I understand that gist right, I first need to use the UI to delete the run. At that point, the run files are still on the server. At that point, I can run this script (or maybe set to run weekly). When the script finds run that were deleted more than 10 days ago (via <code>-d 10<\/code> option), it will look for their references and delete files that live in the parent folder found ( <code> \"\/vol\/minio\/local-files\/{}\/{}\/{}\".format(entity_name, project_name, run_id)<\/code>).<\/p>\n<p>So a couple of questions:<\/p>\n<ol>\n<li>\n<p>What happens to the artifacts associated with runs?<\/p>\n<\/li>\n<li>\n<p>What happens with child folders? (And why isn\u2019t it all files and folders within the parent folder?)<\/p>\n<\/li>\n<li>\n<p>If child directories are present, are they now orphaned with no pointer from the database?<\/p>\n<\/li>\n<\/ol>\n<p>Thanks!<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/c068a531a4cb060559186c287c0508c8c7321469.jpeg\" alt=\"image001.jpg\" data-base62-sha1=\"rs7SiFek7tnQtx4wt2yshERms0p\" width=\"79\" height=\"79\"><\/p>",
                "Answer_score":21.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-21T18:12:48.826Z",
                "Answer_body":"<ol>\n<li>The artifacts are not deleted, if there is an artifacts directory in the object storage we still keep it and delete all the rest of the files.<\/li>\n<li>All child folders are also deleted in the object storage. We don\u2019t delete Artifacts folder because artifacts are not just used by runs but also by other parts of the product so we keep them to avoid breaking things in other parts of the app.<\/li>\n<li>There is no connection with the runs table in the database once the deletion happens but there might be other tables that are still referring to these artifacts.<\/li>\n<\/ol>\n<p>This can be run from outside the container or inside the container. But in either case it expects to have the <code>mysql-connector-python<\/code> python package installed on the container. You can login to the container using <code>docker exec -it wandb-local bash<\/code> and run pip install <a href=\"https:\/\/gist.github.com\/venky-wandb\/b95542f107b377fb9e1ad2c811201bbe#file-file_cleanup-py-L2\">like this<\/a> to install the dependency.<\/p>",
                "Answer_score":26.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-21T18:37:24.000Z",
                "Answer_body":"<p>Thanks for confirming those questions!<\/p>\n<p>Can you also then recommend a way of deleting older versions of artifacts? (which you can do from the UI as well, but presumably this doesn\u2019t affect the underlying store in the same way that deleting runs doesn\u2019t remove them from the store.)<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/c068a531a4cb060559186c287c0508c8c7321469.jpeg\" alt=\"image001.jpg\" data-base62-sha1=\"rs7SiFek7tnQtx4wt2yshERms0p\" width=\"79\" height=\"79\"><\/p>",
                "Answer_score":31.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-04T14:36:12.617Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/a-sh0ts\">@a-sh0ts<\/a>  or <a class=\"mention\" href=\"\/u\/nathank\">@nathank<\/a> \u2013 any suggestions on dealing with artifacts in a similar way?<\/p>",
                "Answer_score":31.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-12T18:00:13.295Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/tkott\">@tkott<\/a> , apologies for the delay here.<br>\nYou can delete artifact versions programmatically via our <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/artifact#delete\">public API<\/a>. Here\u2019s a sample script doing the same:<\/p>\n<pre><code class=\"lang-auto\">api = wandb.Api()\nproject = api.project('project_name')\n\nfor artifact_type in project.artifacts_types():\n    for artifact_collection in artifact_type.collections():        \n        for version in artifact_collection.versions():\n            if artifact_type.type == 'dataset':\n                if len(version.aliases) &gt; 0:\n                    # print out the name of the one we are keeping\n                    print(f'KEEPING {version.name}')\n                else:\n                    print(f'DELETING {version.name}')\n                    if not dry_run:\n                        print('')\n                        version.delete()\n<\/code><\/pre>\n<p>Please let me if this helps.<\/p>",
                "Answer_score":51.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-12T18:18:15.511Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/anmolmann\">@anmolmann<\/a> thanks and I think that\u2019s fine for deleting it from the UI, but my understanding is that this would not delete it from the actual physical drive through the minio interface. How do I do <em>that<\/em>?<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-06T17:33:10.485Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/tkott\">@tkott<\/a> , we identified this as a bug where deleting the artifact versions via API or UI wouldn\u2019t actually delete them from the object storage as well. Our team is working on a fox for this and I\u2019ll keep you posted as soon as i\u2019ve an update on this issue.<br>\nMeanwhile, you can delete these artifact versions via a script similar <a href=\"https:\/\/gist.github.com\/venky-wandb\/b95542f107b377fb9e1ad2c811201bbe\" rel=\"noopener nofollow ugc\">to this one<\/a>. You should remove lines 44 and 45, and update the <code>elif<\/code> in line 41 to <code>elif os.path.isdir(f)<\/code> so that if the folder contains any artifacts then that folder is deleted as well. Also, you should add one more dir path, maybe have a list of dir_paths in line 31 as artifacts are also stored in <code>wandb_artifacts<\/code> in <code>local-files<\/code>. So, your additional dir_path would be <code>dir_path_2 = \"\/vol\/minio\/local-files\/wandb_artifacts\/{}\/{}\".format( idx_1, idx_2)<\/code>, where idx_1 is the artifact index and idx_2 is the artifact version index.<br>\nApologies for the inconvenience caused here as I do acknowledge this workaround would be a hacky way to get around this issue.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/0\/0f65f09b2d5d85af038107eae357ec7bd8e1e33e.png\" data-download-href=\"\/uploads\/short-url\/2cdzanw1auqQjNmN2MsHLYoMSUm.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/0\/0f65f09b2d5d85af038107eae357ec7bd8e1e33e_2_690x365.png\" alt=\"image\" data-base62-sha1=\"2cdzanw1auqQjNmN2MsHLYoMSUm\" width=\"690\" height=\"365\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/0\/0f65f09b2d5d85af038107eae357ec7bd8e1e33e_2_690x365.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/0\/0f65f09b2d5d85af038107eae357ec7bd8e1e33e_2_1035x547.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/0\/0f65f09b2d5d85af038107eae357ec7bd8e1e33e.png 2x\" data-dominant-color=\"F3F3F5\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1174\u00d7622 35.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-06T18:12:51.000Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/anmolmann\">@anmolmann<\/a> Thanks for the suggestion. I\u2019m a little confused because while we\u2019ve used the gist you point out to remove old runs (yay!), if I understand it the artifacts are a separate thing. I don\u2019t know which run to delete that would also (when the changes to the gist are made as you suggest) delete a specific artifact version, and only that specific artifact version. I\u2019m also not sure that we necessarily want to delete the run at the same time as the artifact. Can you elaborate a bit more about the relationship between artifacts and runs and how the gist would handle that?<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/5\/52d5f1c08649212fe6768d2cacdcf8df711810d9.jpeg\" alt=\"~WRD000.jpg\" data-base62-sha1=\"bONwCMToQaefS5MCaw6ckHjvG5z\" width=\"100\" height=\"100\"><\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/c068a531a4cb060559186c287c0508c8c7321469.jpeg\" alt=\"image001.jpg\" data-base62-sha1=\"rs7SiFek7tnQtx4wt2yshERms0p\" width=\"79\" height=\"79\"><\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-05T18:13:10.590Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"delet data self host server saw post doesnt answer question run self host instanc somewhat limit space weve good delet old run interfac thought delet interfac delet physic file hard disk doesnt appear case exampl go minio folder server show particular project take space report that big differ delet file minio longer know suggest central usag url admin look usag team user have individu",
        "Question_preprocessed_content":"delet data server saw post doesnt answer question run instanc somewhat limit space weve good delet old run interfac thought delet interfac delet physic file hard disk doesnt appear case exampl go minio folder server show particular project take space report that big differ delet file minio longer know suggest central url admin look usag team user have individu",
        "Answer_original_content":"tkott look believ isnt wai physic delet file hard drive ill look confirm case featur request possibl thank nate nate suggest free space gener note hash digest file look minio server hand expect delet server big scari perman oper type warn server fact remov file doesnt remov file isnt actual perman oper file salvag albeit work nathank suggest programmat meantim tkott sorri delai member team script us run python file cleanup tag specifi dai ago run delet order script delet minio storag note work bare metal docker contain setup connect extern databas work work make default local deploy set retent polici delet run server automat clean delet run certain time thank nate thank understand gist right need us delet run point run file server point run script mayb set run weekli script find run delet dai ago option look refer delet file live parent folder vol minio local file format entiti project run coupl question happen artifact associ run happen child folder isnt file folder parent folder child directori present orphan pointer databas thank artifact delet artifact directori object storag delet rest file child folder delet object storag dont delet artifact folder artifact run part product avoid break thing part app connect run tabl databas delet happen tabl refer artifact run outsid contain insid contain case expect mysql connector python python packag instal contain login contain docker exec local bash run pip instal like instal depend thank confirm question recommend wai delet older version artifact presum doesnt affect underli store wai delet run doesnt remov store sht nathank suggest deal artifact similar wai hei tkott apolog delai delet artifact version programmat public api here sampl script api api project api project project artifact type project artifact type artifact collect artifact type collect version artifact collect version artifact type type dataset len version alias print keep print keep version print delet version dry run print version delet let help anmolmann thank think that fine delet understand delet actual physic drive minio interfac tkott identifi bug delet artifact version api wouldnt actual delet object storag team work fox ill post soon iv updat issu delet artifact version script similar remov line updat elif line elif path isdir folder contain artifact folder delet add dir path mayb list dir path line artifact store artifact local file addit dir path dir path vol minio local file artifact format idx idx idx artifact index idx artifact version index apolog inconveni caus acknowledg workaround hacki wai issu imag anmolmann thank suggest littl confus weve gist point remov old run yai understand artifact separ thing dont know run delet chang gist suggest delet specif artifact version specif artifact version sure necessarili want delet run time artifact elabor bit relationship artifact run gist handl topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"look believ isnt wai physic delet file hard drive ill look confirm case featur request possibl thank nate nate suggest free space gener note hash digest file look minio server hand expect delet server big scari perman oper type warn server fact remov file doesnt remov file isnt actual perman oper file salvag suggest programmat meantim sorri delai member team script us run tag specifi dai ago run delet order script delet minio storag note work docker contain setup connect extern databas work work make default local deploy set retent polici delet run server automat clean delet run certain time thank nate thank understand gist right need us delet run point run file server point run script script find run delet dai ago look refer delet file live parent folder coupl question happen artifact associ run happen child folder child directori present orphan pointer databas thank artifact delet artifact directori object storag delet rest file child folder delet object storag dont delet artifact folder artifact run part product avoid break thing part app connect run tabl databas delet happen tabl refer artifact run outsid contain insid contain case expect python packag instal contain login contain run pip instal like instal depend thank confirm question recommend wai delet older version artifact suggest deal artifact similar wai hei apolog delai delet artifact version programmat public api here sampl script let help thank think that fine delet understand delet actual physic drive minio interfac identifi bug delet artifact version api wouldnt actual delet object storag team work fox ill post soon iv updat issu delet artifact version script similar remov line updat line folder contain artifact folder delet add dir path mayb list line artifact store addit artifact index artifact version index apolog inconveni caus acknowledg workaround hacki wai issu imag thank suggest littl confus weve gist point remov old run understand artifact separ thing dont know run delet delet specif artifact version specif artifact version sure necessarili want delet run time artifact elabor bit relationship artifact run gist handl topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering a challenge with deleting data from their self-hosted server, as the physical files are not being deleted when they are removed from the interface.",
        "Question_gpt_summary":"user encount challeng delet data self host server physic file delet remov interfac",
        "Answer_gpt_summary_original":"the user is having trouble deleting data from their self-hosted server. the answer suggests that there is no way to physically delete files from the hard drive through the ui, but a feature request can be made for this. in the meantime, a script can be used to delete files programmatically. the script can be run from outside or inside the container and requires the mysql-connector-python package to be installed. artifacts associated with runs are not deleted, but child folders are deleted. to delete older versions of artifacts, a script can be used via the public api. however, deleting artifact versions via api or ui does not delete them from the object storage, and a workaround is suggested to delete them via a script.",
        "Answer_gpt_summary":"user have troubl delet data self host server answer suggest wai physic delet file hard drive featur request meantim script delet file programmat script run outsid insid contain requir mysql connector python packag instal artifact associ run delet child folder delet delet older version artifact script public api delet artifact version api delet object storag workaround suggest delet script"
    },
    {
        "Question_id":null,
        "Question_title":"Compliance - administering data that is under DVC control",
        "Question_body":"<p>Hi there<\/p>\n<p>We are currently looking to implement DVC on our stack.<\/p>\n<p>One thing that isn\u2019t clear from the available documentation, however, is how our data compliance team would go about administering and managing our data store using DVC. What kind of user access control is available to us? Or is this based on our existing Git\/Bitbucket setup?<\/p>\n<p>We\u2019ve been through the tutorials available to us and we understand how to add data to the repository, branch off, update etc, but what we\u2019re looking for clarity on is how we would centrally manage this for multiple users.<\/p>\n<p>Any advice or guidance would be appreciated.<\/p>\n<p>EDIT: To add clarity to the question, our requirement is for all of our incoming data sources to be handled by a specialist compliance team who will act as gatekeepers to the data and make it available to the data scientists within the firm as needed. The intention is for our DS team to pull data as is needed but not be able to update it without approval from the compliance team. Thanks.<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_creation_time":1556203634652,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":8.0,
        "Question_view_count":1482.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/compliance-administering-data-that-is-under-dvc-control\/174",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-04-26T00:24:15.986Z",
                "Answer_body":"<p>Hi!<\/p>\n<p>Great question. DVC does not have any embedded features for access control yet but it relies on the access control models of your code repositories and storages.<\/p>\n<ul>\n<li>code: use Git\\Bitbucket access setting to gain access for projects meta information<\/li>\n<li>storage: use S3\\GCP\\SSH access setting per project (per S3 bucket which is DVC remote)<\/li>\n<\/ul>\n<p>If we assume that you use separate Git repos per project (not a mono repo) and a separate data remote per project (S3 bucket for example) then you can give access by managing these two services - the repository and the S3 bucket.<\/p>\n<p>Your compliance team can create a separate repository per data source (or set of the sources) with the right repository and data bucket setting and provide it to the right data scientists.<\/p>",
                "Answer_score":8.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-26T00:27:35.385Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>!<\/p>\n<p>Would you mind to clarify a few things? Do all data scientists have more or less the same access rights to the data that was released by the compliance team?<\/p>\n<p>How do you enforce these policies now?<\/p>",
                "Answer_score":8.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-26T08:00:56.408Z",
                "Answer_body":"<p>Yeah, the intention is for the data scientists to have the same level of access to read data, create branches and issue pull requests, which the compliance team would then be required to approve\/deny after a review.<\/p>\n<p>Currently we don\u2019t have a process like this yet. As it stands, the data scientists just use a shared folder on a server and there\u2019s no version control on the files they\u2019re accessing, so we have no idea if anyone has changed anything, nor any audit trail or metadata to say what is and isn\u2019t in there. Hence why we\u2019re looking at DVC<\/p>",
                "Answer_score":18.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-26T08:09:01.341Z",
                "Answer_body":"<p>Thank you <a class=\"mention\" href=\"\/u\/dmitry\">@dmitry<\/a>, that\u2019s good to know.<\/p>\n<p>Also great to know we can integrate with our existing Bitbucket service. We won\u2019t be using GCP\/S3 for storage, we have a storage server so it\u2019ll be using SSH to connect to that through bitbucket.<\/p>",
                "Answer_score":58.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-26T18:52:38.460Z",
                "Answer_body":"<p>Makes sense <a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>. There are a few ways to organize this with DVC, but since you are looking for a central place where your data scientists and the compliance team collaborate, I would probably got with one git\/dvc repo with all data sets you want to audit and expose in it.<\/p>\n<p>Each dataset will be represented by the <code>.dvc<\/code> meta-file in this repository. Data scientists will be using these files an entry point to the dataset - to pull it to their project, get a link to the actual data, etc. They will be using DVC commands to make an update to this file which then will have to be committed via PR to the master branch of the central registry. Thus you will have access to the previous versions, you will know who changed what, and will have a way to block a change to the \u201cmaster\u201d version of the dataset.<\/p>\n<p>Btw, it does not mean that you need to create a separate Git repo for this. If you are using a mono-repo where you keep all your data science projects, you can place you datasets here as well.<\/p>\n<p>If it\u2019s all confusing, we can schedule a meeting and help to brainstorm the details of this workflow.<\/p>",
                "Answer_score":12.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-29T12:29:57.524Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/shcheklein\">@shcheklein<\/a> that would be great if you could. Could you PM me and we\u2019ll sort something out?<\/p>",
                "Answer_score":92.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-29T21:30:18.573Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>  We\u2019d be happy to chat! Please shoot me an email - dmitry@iterative.ai<\/p>\n<p>It would be helpful if you can provide your availability. We are based in San Francisco, CA (PST time zone).<\/p>",
                "Answer_score":47.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-05-11T01:00:17.211Z",
                "Answer_body":"<p>Hey,  <a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a>! We are back from the conference and are ready to help you in case your team has any questions, issues. Have you tried to use DVC project as a registry yet?<\/p>",
                "Answer_score":87.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-01T19:42:44.670Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/michaelwdownie\">@michaelwdownie<\/a> if it\u2019s still relevant, please take a look at this <a href=\"https:\/\/dvc.org\/doc\/use-cases\/data-registries\">article<\/a> and the new features we introduced to facilitate Git-like workflow for data. It should be easier to setup and use \u201cdata registries\u201d now. Let\u2019s me know what are your thoughts on this <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"> .<\/p>",
                "Answer_score":61.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"complianc administ data control current look implement stack thing isnt clear avail document data complianc team administ manag data store kind user access control avail base exist git bitbucket setup weve tutori avail understand add data repositori branch updat look clariti central manag multipl user advic guidanc appreci edit add clariti question requir incom data sourc handl specialist complianc team act gatekeep data avail data scientist firm need intent team pull data need abl updat approv complianc team thank",
        "Question_preprocessed_content":"complianc administ data control current look implement stack thing isnt clear avail document data complianc team administ manag data store kind user access control avail base exist setup weve tutori avail understand add data repositori branch updat look clariti central manag multipl user advic guidanc appreci edit add clariti question requir incom data sourc handl specialist complianc team act gatekeep data avail data scientist firm need intent team pull data need abl updat approv complianc team thank",
        "Answer_original_content":"great question embed featur access control reli access control model code repositori storag code us git bitbucket access set gain access project meta inform storag us gcp ssh access set project bucket remot assum us separ git repo project mono repo separ data remot project bucket exampl access manag servic repositori bucket complianc team creat separ repositori data sourc set sourc right repositori data bucket set provid right data scientist michaelwdowni mind clarifi thing data scientist access right data releas complianc team enforc polici yeah intent data scientist level access read data creat branch issu pull request complianc team requir approv deni review current dont process like stand data scientist us share folder server there version control file theyr access idea chang audit trail metadata isnt look thank dmitri that good know great know integr exist bitbucket servic wont gcp storag storag server itll ssh connect bitbucket make sens michaelwdowni wai organ look central place data scientist complianc team collabor probabl got git repo data set want audit expos dataset repres meta file repositori data scientist file entri point dataset pull project link actual data command updat file commit master branch central registri access previou version know chang wai block chang master version dataset btw mean need creat separ git repo mono repo data scienc project place dataset confus schedul meet help brainstorm detail workflow shcheklein great sort michaelwdowni wed happi chat shoot email dmitri iter help provid avail base san francisco pst time zone hei michaelwdowni confer readi help case team question issu tri us project registri michaelwdowni relev look articl new featur introduc facilit git like workflow data easier setup us data registri let know thought",
        "Answer_preprocessed_content":"great question embed featur access control reli access control model code repositori storag code us access set gain access project meta inform storag us access set project assum us separ git repo project separ data remot project access manag servic repositori bucket complianc team creat separ repositori data sourc right repositori data bucket set provid right data scientist mind clarifi thing data scientist access right data releas complianc team enforc polici yeah intent data scientist level access read data creat branch issu pull request complianc team requir review current dont process like stand data scientist us share folder server there version control file theyr access idea chang audit trail metadata isnt look thank that good know great know integr exist bitbucket servic wont storag storag server itll ssh connect bitbucket make sens wai organ look central place data scientist complianc team collabor probabl got git repo data set want audit expos dataset repres repositori data scientist file entri point dataset pull project link actual data command updat file commit master branch central registri access previou version know chang wai block chang master version dataset btw mean need creat separ git repo data scienc project place dataset confus schedul meet help brainstorm detail workflow great sort wed happi chat shoot email help provid avail base san francisco hei confer readi help case team question issu tri us project registri relev look articl new featur introduc facilit workflow data easier setup us data registri let know thought",
        "Question_gpt_summary_original":"the user is looking for clarity on how to centrally manage data compliance for multiple users, with the intention of having a specialist compliance team act as gatekeepers to the data and make it available to the data scientists within the firm as needed.",
        "Question_gpt_summary":"user look clariti central manag data complianc multipl user intent have specialist complianc team act gatekeep data avail data scientist firm need",
        "Answer_gpt_summary_original":"possible solutions for centrally managing data compliance for multiple users include using access control models of code repositories and storages, creating a separate repository per data source with the right repository and data bucket setting, using a central git repo with all data sets to audit and expose, and using a mono-repo where all data science projects are kept. data scientists can use meta-files as an entry point to the dataset and make updates to the file, which will have to be committed via pull request to the master branch of the central registry. this workflow provides access to previous versions, tracks changes, and allows blocking changes to the master version of the dataset. the team can schedule a meeting to brainstorm the details of this workflow, and they can use a project as a registry to facilitate a git-like workflow for data.",
        "Answer_gpt_summary":"possibl solut central manag data complianc multipl user includ access control model code repositori storag creat separ repositori data sourc right repositori data bucket set central git repo data set audit expos mono repo data scienc project kept data scientist us meta file entri point dataset updat file commit pull request master branch central registri workflow provid access previou version track chang allow block chang master version dataset team schedul meet brainstorm detail workflow us project registri facilit git like workflow data"
    },
    {
        "Question_id":63500377.0,
        "Question_title":"memory issues for sparse one hot encoded features",
        "Question_body":"<p>I want to create sparse matrix for one hot encoded features from data frame <code>df<\/code>. But I am getting memory issue for code given below. Shape of <code>sparse_onehot<\/code> is  (450138, 1508)<\/p>\n<pre><code>sp_features = ['id', 'video_id', 'genre']\nsparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features)\nimport scipy\nX = scipy.sparse.csr_matrix(sparse_onehot.values)\n<\/code><\/pre>\n<p>I get memory error as shown below.<\/p>\n<pre><code>MemoryError: Unable to allocate 647. MiB for an array with shape (1508, 450138) and data type uint8\n<\/code><\/pre>\n<p>I have tried <code>scipy.sparse.lil_matrix<\/code> and get same error as above.<\/p>\n<p>Is there any efficient way of handling this?\nThanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1597908911857,
        "Question_favorite_count":null,
        "Question_last_edit_time":1597911320856,
        "Question_score":2.0,
        "Question_view_count":97.0,
        "Answer_body":"<p>Try setting to <code>True<\/code> the <a href=\"https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html\" rel=\"nofollow noreferrer\"><code>sparse<\/code> parameter<\/a>:<\/p>\n<blockquote>\n<p>sparsebool, default False\nWhether the dummy-encoded columns should be backed by a SparseArray (True) or a regular NumPy array (False).<\/p>\n<\/blockquote>\n<pre><code>sparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features, sparse = True)\n<\/code><\/pre>\n<p>This will use a much more memory efficient (but somewhat slower) representation than the default one.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63500377",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1597911539563,
        "Question_original_content":"memori issu spars hot encod featur want creat spars matrix hot encod featur data frame get memori issu code given shape spars onehot featur video genr spars onehot dummi featur column featur import scipi scipi spars csr matrix spars onehot valu memori error shown memoryerror unabl alloc mib arrai shape data type uint tri scipi spars lil matrix error effici wai handl thank advanc",
        "Question_preprocessed_content":"memori issu spars hot encod featur want creat spars matrix hot encod featur data frame get memori issu code given shape memori error shown tri error effici wai handl thank advanc",
        "Answer_original_content":"try set true spars paramet sparsebool default fals dummi encod column back sparsearrai true regular numpi arrai fals spars onehot dummi featur column featur spars true us memori effici somewhat slower represent default",
        "Answer_preprocessed_content":"try set paramet sparsebool default fals column back sparsearrai regular numpi arrai us memori effici represent default",
        "Question_gpt_summary_original":"the user is encountering a memory issue when attempting to create a sparse matrix for one hot encoded features from a data frame, resulting in a memoryerror when trying to allocate an array with shape (1508, 450138).",
        "Question_gpt_summary":"user encount memori issu attempt creat spars matrix hot encod featur data frame result memoryerror try alloc arrai shape",
        "Answer_gpt_summary_original":"the solution to the memory issue when creating a sparse matrix for one hot encoded features from a data frame is to set the sparse parameter to true. this will use a more memory-efficient representation than the default one. the code to implement this solution is: sparse_onehot = pd.get_dummies(df[sp_features], columns = sp_features, sparse = true).",
        "Answer_gpt_summary":"solut memori issu creat spars matrix hot encod featur data frame set spars paramet true us memori effici represent default code implement solut spars onehot dummi featur column featur spars true"
    },
    {
        "Question_id":null,
        "Question_title":"Does wandb charges for data transfer as s3 does(apart from data storage cost)?",
        "Question_body":"<p>The pricing on the wandb website states cost of data storage.Does wandb also charges downloading and uploading of artifacts like S3 does for data transfer?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1638962523480,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":286.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/does-wandb-charges-for-data-transfer-as-s3-does-apart-from-data-storage-cost\/1487",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-08T12:17:21.755Z",
                "Answer_body":"<p>Hey Jasdeep,<\/p>\n<p>Double-checking this with the team. Will get back to you as soon as possible.<\/p>\n<p>Best,<br>\nArman<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-09T17:02:40.287Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/armanharutyunyan\">@armanharutyunyan<\/a>  Any updates?<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-12-14T12:14:01.873Z",
                "Answer_body":"<p>Hey there, sorry about the delay on this. We are only charging for storage, not transfer of data.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-12T12:14:08.550Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"charg data transfer apart data storag cost price websit state cost data storag charg download upload artifact like data transfer",
        "Question_preprocessed_content":"charg data transfer price websit state cost data charg download upload artifact like data transfer",
        "Answer_original_content":"hei jasdeep doubl check team soon possibl best arman armanharutyunyan updat hei sorri delai charg storag transfer data topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hei jasdeep team soon possibl best arman updat hei sorri delai charg storag transfer data topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is inquiring about the cost of data transfer in addition to the cost of data storage on the website.",
        "Question_gpt_summary":"user inquir cost data transfer addit cost data storag websit",
        "Answer_gpt_summary_original":"there is no cost for data transfer on the website, only for data storage.",
        "Answer_gpt_summary":"cost data transfer websit data storag"
    },
    {
        "Question_id":null,
        "Question_title":"Add Metadata after an artifact has been logged",
        "Question_body":"<p>Hi All,<\/p>\n<p>I\u2019m currently using the W&amp;B Metaflow Integration: <a href=\"https:\/\/docs.wandb.ai\/guides\/integrations\/other\/metaflow\">https:\/\/docs.wandb.ai\/guides\/integrations\/other\/metaflow<\/a>. Any instance variables that are Pandas Dataframes or PathLibs are automatically logged as dataset artifacts. Due to this, I\u2019m not able to supply a description for the artifact or any metadata for it when the artifact is created. Is there any way to supply this information after the artifact is created?<\/p>\n<p>Thanks for the help!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1634185251523,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":362.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/add-metadata-after-an-artifact-has-been-logged\/970",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-14T19:25:42.862Z",
                "Answer_body":"<p>I ended up adding a metaflow step that updates the artifact that was just automatically logged, like in this example: <a href=\"https:\/\/docs.wandb.ai\/guides\/artifacts\/api#updating-artifacts\">https:\/\/docs.wandb.ai\/guides\/artifacts\/api#updating-artifacts<\/a>.<\/p>",
                "Answer_score":67.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-15T09:43:32.584Z",
                "Answer_body":"<p>Thank you so much for answering your own question! Feel free to share a code snippet for future readers going down the same path.<br>\n<img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/raised_hands.png?v=10\" title=\":raised_hands:\" class=\"emoji only-emoji\" alt=\":raised_hands:\"><\/p>",
                "Answer_score":2.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-18T12:33:59.760Z",
                "Answer_body":"<p>I\u2019ve updated the docs to reflect this solution <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"add metadata artifact log current metaflow integr http doc guid integr metaflow instanc variabl panda datafram pathlib automat log dataset artifact abl suppli descript artifact metadata artifact creat wai suppli inform artifact creat thank help",
        "Question_preprocessed_content":"add metadata artifact log current metaflow integr instanc variabl panda datafram pathlib automat log dataset artifact abl suppli descript artifact metadata artifact creat wai suppli inform artifact creat thank help",
        "Answer_original_content":"end ad metaflow step updat artifact automat log like exampl http doc guid artifact api updat artifact thank answer question feel free share code snippet futur reader go path iv updat doc reflect solut",
        "Answer_preprocessed_content":"end ad metaflow step updat artifact automat log like exampl thank answer question feel free share code snippet futur reader go path iv updat doc reflect solut",
        "Question_gpt_summary_original":"the user is looking for a way to add metadata to an artifact that has already been logged using the w&b metaflow integration.",
        "Question_gpt_summary":"user look wai add metadata artifact log metaflow integr",
        "Answer_gpt_summary_original":"the solution to adding metadata to an artifact that has already been logged using the w&b metaflow integration is to add a metaflow step that updates the artifact. the answer provides an example and suggests sharing a code snippet for future readers. the documentation has also been updated to reflect this solution.",
        "Answer_gpt_summary":"solut ad metadata artifact log metaflow integr add metaflow step updat artifact answer provid exampl suggest share code snippet futur reader document updat reflect solut"
    },
    {
        "Question_id":66277250.0,
        "Question_title":"Reading multiple CSV files from S3 using Python with Boto3",
        "Question_body":"<p>I am able to read multiple csv files from S3 bucket with boto3 in python and finally combine those files in single dataframe in pandas.However, in some of the folders there are some empty files which results in the error &quot;No columns to parse from file&quot;. Can we skip those empty files in the below codes?<\/p>\n<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('testbucket')\n\nprefix_objs = bucket.objects.filter(Prefix=&quot;extracted\/abc&quot;)\n\n    prefix_df = []\n\nfor obj in prefix_objs:\n    key = obj.key\n    body = obj.get()['Body'].read()\n    temp = pd.read_csv(io.BytesIO(body),header=None, encoding='utf8',sep=',')        \n    prefix_df.append(temp)\n<\/code><\/pre>\n<p>I have used this ans [https:\/\/stackoverflow.com\/questions\/52855221\/reading-multiple-csv-files-from-s3-bucket-with-boto3][1]<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1613736268983,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":3048.0,
        "Answer_body":"<pre><code>s3 = boto3.resource('s3')\nbucket = s3.Bucket('testbucket')\n\nprefix_objs = bucket.objects.filter(Prefix=&quot;extracted\/abc&quot;)\n\nprefix_df = []\n\nfor obj in prefix_objs:\n    try:\n        key = obj.key\n        body = obj.get()['Body'].read()\n        temp = pd.read_csv(io.BytesIO(body),header=None, encoding='utf8',sep=',')        \n        prefix_df.append(temp)\n    except:\n        continue\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66277250",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1613737607800,
        "Question_original_content":"read multipl csv file python boto abl read multipl csv file bucket boto python final combin file singl datafram panda folder file result error column pars file skip file code boto resourc bucket bucket testbucket prefix obj bucket object filter prefix extract abc prefix obj prefix obj kei obj kei bodi obj bodi read temp read csv bytesio bodi header encod utf sep prefix append temp an http stackoverflow com question read multipl csv file bucket boto",
        "Question_preprocessed_content":"read multipl csv file python boto abl read multipl csv file bucket boto python final combin file singl datafram folder file result error column pars file skip file code an",
        "Answer_original_content":"boto resourc bucket bucket testbucket prefix obj bucket object filter prefix extract abc prefix obj prefix obj try kei obj kei bodi obj bodi read temp read csv bytesio bodi header encod utf sep prefix append temp continu",
        "Answer_preprocessed_content":"",
        "Question_gpt_summary_original":"the user is attempting to read multiple csv files from an s3 bucket using boto3 in python and combine them into a single dataframe in pandas, but is encountering an error due to empty files in some of the folders.",
        "Question_gpt_summary":"user attempt read multipl csv file bucket boto python combin singl datafram panda encount error file folder",
        "Answer_gpt_summary_original":"the solution to the problem of encountering an error while attempting to read multiple csv files from an s3 bucket using boto3 in python and combine them into a single dataframe in pandas due to empty files in some of the folders is to use the provided code. the code uses boto3 to access the s3 bucket, filters the objects based on a prefix, reads the csv files using pandas, and appends them to a list. any errors encountered during the process are ignored.",
        "Answer_gpt_summary":"solut problem encount error attempt read multipl csv file bucket boto python combin singl datafram panda file folder us provid code code us boto access bucket filter object base prefix read csv file panda append list error encount process ignor"
    },
    {
        "Question_id":73200116.0,
        "Question_title":"AWS Sagemaker integration with mongodb and lambda",
        "Question_body":"<p>I'm looking for some advice from anyone who's tried aws Sagemaker. I'm very new to this and would appreciate anyone kind enough to help me out.<\/p>\n<p>I have created a basic time series project in a Sagemaker notebook. It trains the model on CSV file data and tests it, with good results.<\/p>\n<p>The data I am using is based on store profits. I am predicting the profit each week.<\/p>\n<p>However, my question is, how can I pass new store sales data into this model each week (only one day a week), and retrain it with the new weeks data (so it can notice any new patterns), then for it to predict the next week profit for each store?<\/p>\n<p>All my store data is synced into mongodb, so I'm presuming I would need a lambda function to get this data and pass it over to the Sagemaker model.<\/p>\n<p>Is it worth retraining the model every week? As I have years worth of store data? Or should I just pass over the old data with the new data added in for it to predict? How do I pass over this data? In a lambda function with a cloud event to make it run automatically every week?<\/p>\n<p>Can I write the predictions back into mongodb in a new table, or are they saved somewhere else first and this would have to be another lambda function?<\/p>\n<p>I have looked at so many tutorials, but none of them seem to explain how I can connect everything up and have the model make predictions automatically and then save them in a dB.<\/p>\n<p>Many thanks in advance to anyone who can explain this to me! Sorry for such a long question!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1659392227390,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":58.0,
        "Answer_body":"<p>Recently I have completed similar use case and here's my answers -<\/p>\n<p><strong>Q1  : Is there need of retraining every week?<\/strong><\/p>\n<p>Ans : Yes, you need to do continuous training and continuous forecasting steps (tie using sagemaker pipeline) in prod to make it work perfectly automated for stable MAE, MAPE etc.<\/p>\n<p><strong>Q2  : How can I pass new data and forecast for next week? How to get input data from mongodb?<\/strong><\/p>\n<p>Ans : You could use Lambda, or Glue job (designed for ETL so better) to drop in S3 bucket. This will could become input raw data bucket for sagemaker pipeline.<\/p>\n<p><strong>Q3  : Can I write the predictions back into mongodb in a new table, or are they saved somewhere else first and this would have to be another lambda function?<\/strong><\/p>\n<p>Ans : Yes you can, both ways.<\/p>\n<p>I would suggest to start small i.e. First drop a csv file to s3 location in say YYMMDD folder. Use this as input and develop completely in one notebook (continuous train, continuous forecast).<\/p>\n<p>Later, learn about pipelines - how to write different steps, pass objects between steps etc and go modify your code to fit in pipeline.<\/p>\n<p>Create a sagemaker pipeline with steps : (Refer links below )<\/p>\n<ol>\n<li>Preprocess ( any transformations, cleansing )<\/li>\n<li>Training ( use prebuilt image or need to build one, depends)<\/li>\n<li>Forecast ( either do batch transform or deploy to endpoint and later delete )<\/li>\n<li>Post processing ( if required )<\/li>\n<\/ol>\n<p>Take the output the from sagemaker pipelines to mongodb. <strong>Sagemaker Pipelines help automate scheduled execution using AWS Event Bridge<\/strong><\/p>\n<p>Some references :<\/p>\n<p><a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-pipelines\/tabular\/abalone_build_train_deploy\/sagemaker-pipelines-preprocess-train-evaluate-batch-transform.ipynb\" rel=\"nofollow noreferrer\">Example Pipelines<\/a> Look here in to know about pipelines<\/p>\n<p><a href=\"https:\/\/github.com\/aws-samples\/amazon-sagemaker-forecasting-air-pollution-with-deepar\/blob\/5e29057a8c9d7f8db6cb1143f7838b6614f44ef1\/01_train_and_evaluate_air_quality_deepar_model.ipynb\" rel=\"nofollow noreferrer\">Example1 DeepAR<\/a><\/p>\n<p><a href=\"https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.html?highlight=deepar\" rel=\"nofollow noreferrer\">Example2 DeepAR<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73200116",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659526594223,
        "Question_original_content":"integr mongodb lambda look advic tri new appreci kind help creat basic time seri project notebook train model csv file data test good result data base store profit predict profit week question pass new store sale data model week dai week retrain new week data notic new pattern predict week profit store store data sync mongodb presum need lambda function data pass model worth retrain model week year worth store data pass old data new data ad predict pass data lambda function cloud event run automat week write predict mongodb new tabl save lambda function look tutori explain connect model predict automat save thank advanc explain sorri long question",
        "Question_preprocessed_content":"integr mongodb lambda look advic tri new appreci kind help creat basic time seri project notebook train model csv file data test good result data base store profit predict profit week question pass new store sale data model week retrain new week data predict week profit store store data sync mongodb presum need lambda function data pass model worth retrain model week year worth store data pass old data new data ad predict pass data lambda function cloud event run automat week write predict mongodb new tabl save lambda function look tutori explain connect model predict automat save thank advanc explain sorri long question",
        "Answer_original_content":"recent complet similar us case answer need retrain week an ye need continu train continu forecast step tie pipelin prod work perfectli autom stabl mae mape pass new data forecast week input data mongodb an us lambda glue job design etl better drop bucket input raw data bucket pipelin write predict mongodb new tabl save lambda function an ye wai suggest start small drop csv file locat yymmdd folder us input develop complet notebook continu train continu forecast later learn pipelin write differ step pass object step modifi code fit pipelin creat pipelin step refer link preprocess transform cleans train us prebuilt imag need build depend forecast batch transform deploi endpoint later delet post process requir output pipelin mongodb pipelin help autom schedul execut aw event bridg refer exampl pipelin look know pipelin exampl deepar exampl deepar",
        "Answer_preprocessed_content":"recent complet similar us case answer need retrain week an ye need continu train continu forecast step prod work perfectli autom stabl mae mape pass new data forecast week input data mongodb an us lambda glue job drop bucket input raw data bucket pipelin write predict mongodb new tabl save lambda function an ye wai suggest start small drop csv file locat yymmdd folder us input develop complet notebook later learn pipelin write differ step pass object step modifi code fit pipelin creat pipelin step preprocess train forecast post process output pipelin mongodb pipelin help autom schedul execut aw event bridg refer exampl pipelin look know pipelin exampl deepar exampl deepar",
        "Question_gpt_summary_original":"the user is looking for advice on how to integrate their time series project with mongodb and lambda in order to automatically retrain the model with new store sales data each week and save the predictions in a database.",
        "Question_gpt_summary":"user look advic integr time seri project mongodb lambda order automat retrain model new store sale data week save predict databas",
        "Answer_gpt_summary_original":"possible solutions from the answer include: \n- continuous training and forecasting using a pipeline in production for stable mae, mape, etc.\n- using lambda or glue job to drop new data in an s3 bucket, which can become the input raw data bucket for the pipeline.\n- writing predictions back into mongodb in a new table or using another lambda function.\n- starting small by dropping a csv file to an s3 location and developing the pipeline in one notebook before modifying the code to fit in the pipeline.\n- creating a pipeline with steps such as preprocessing, training, forecasting, and post-processing.\n- using aws event bridge to automate scheduled execution of the pipeline.",
        "Answer_gpt_summary":"possibl solut answer includ continu train forecast pipelin product stabl mae mape lambda glue job drop new data bucket input raw data bucket pipelin write predict mongodb new tabl lambda function start small drop csv file locat develop pipelin notebook modifi code fit pipelin creat pipelin step preprocess train forecast post process aw event bridg autom schedul execut pipelin"
    },
    {
        "Question_id":null,
        "Question_title":"Month-to-date total cost (VertexA\u0131) keeps increasing even though I delete all my projects and billin",
        "Question_body":" ",
        "Question_answer_count":0,
        "Question_comment_count":0,
        "Question_creation_time":1668131160000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Month-to-date-total-cost-VertexA%C4%B1-keeps-increasing-even-though-I\/td-p\/488385\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-11T01:46:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hello. I am a student and only use GCP for learning. But even though there is no project in my profile and\nI have closed my billing account, the Month-to-date total cost (VertexA\u0131) is constantly increasing. \nI tried everything but couldn't find a solution. I would be very happy if you could help me what to do."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"month date total cost vertexa keep increas delet project billin ",
        "Question_preprocessed_content":"total cost keep increas delet project billin ",
        "Answer_original_content":"hello student us gcp learn project profil close bill account month date total cost vertexa constantli increas tri couldn solut happi help",
        "Answer_preprocessed_content":"hello student us gcp learn project profil close bill account total cost constantli increas tri couldn solut happi help",
        "Question_gpt_summary_original":"the user is experiencing an issue where their month-to-date total cost in vertexa\u0131 keeps increasing even though they have deleted all their projects and billing.",
        "Question_gpt_summary":"user experienc issu month date total cost vertexa keep increas delet project bill",
        "Answer_gpt_summary_original":"possible solutions are not provided in the answer.",
        "Answer_gpt_summary":"possibl solut provid answer"
    },
    {
        "Question_id":71499094.0,
        "Question_title":"Python AzureML Hello world - Can't find module azureml",
        "Question_body":"<p>Python 3.10, Pip install azureml-sdk 1.39.0.<br \/>\nEnvironments: Win10 PS, VS2022, and a docker image- all same results . Pip show shows the azureml-core package.<\/p>\n<p>Simple (I thought) script, but it can't find &quot;azureml.core&quot;   No module named azureml is the error.\nHow do I make it &quot;find&quot; it? I'm new at python so it could be syntax.<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Experiment, Environment, Model,Dataset,Datastore,ScriptRunConfig\n     \n    # check core SDK version number\n    print(&quot;Azure ML SDK Version: &quot;, azureml.core.VERSION)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1647441787870,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":310.0,
        "Answer_body":"<p>azureml python sdk does not support py3.10 yet, AutoML sdk supports py&lt;=3.8.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71499094",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1648177993987,
        "Question_original_content":"python hello world modul python pip instal sdk environ win docker imag result pip show core packag simpl thought script core modul name error new python syntax import core import workspac experi environ model dataset datastor scriptrunconfig check core sdk version number print sdk version core version",
        "Question_preprocessed_content":"python hello world modul python pip instal sdk environ win docker imag result pip show core packag simpl script modul name error new python syntax",
        "Answer_original_content":"python sdk support automl sdk support",
        "Answer_preprocessed_content":"python sdk support automl sdk support",
        "Question_gpt_summary_original":"the user is encountering challenges with a simple python script, where they are unable to make the script \"find\" the \".core\" module, resulting in a \"no module named\" error.",
        "Question_gpt_summary":"user encount challeng simpl python script unabl script core modul result modul name error",
        "Answer_gpt_summary_original":"possible solutions to the user's challenge with a python script that cannot find the \".core\" module are not explicitly mentioned in the answer. however, the answer suggests that the issue may be related to the python version being used. specifically, the python sdk does not support version 3.10 yet, and the automl sdk only supports versions up to 3.8. therefore, the user may need to downgrade their python version to resolve the issue.",
        "Answer_gpt_summary":"possibl solut user challeng python script core modul explicitli mention answer answer suggest issu relat python version specif python sdk support version automl sdk support version user need downgrad python version resolv issu"
    },
    {
        "Question_id":null,
        "Question_title":"What's the training corpus of models behind GCP Natural Language APIs?",
        "Question_body":"Hi, where can I find some information about which datasets are used for training models that power the natural language APIs for sentiment analysis, entity extraction, etc.? Thanks!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1668695760000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":45.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/What-s-the-training-corpus-of-models-behind-GCP-Natural-Language\/td-p\/490614\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-23T08:41:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Natural Language API is trained using different types of datasets.\n\nPublic datasets Examples: Five crowd-flower\u00a0sentiment benchmarks\nEAP customer datasets Examples: Feefo sentiment dataset\nAcademic datasets Examples: Stanford rotten tomatoes sentences, UCI Sentiment Labeled Sentences Data Set. See.\nGoogle datasets Examples: Shopping, Play."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"train corpu model gcp natur languag api inform dataset train model power natur languag api sentiment analysi entiti extract thank",
        "Question_preprocessed_content":"train corpu model gcp natur languag api inform dataset train model power natur languag api sentiment analysi entiti extract thank",
        "Answer_original_content":"natur languag api train differ type dataset public dataset exampl crowd flowersenti benchmark eap custom dataset exampl feefo sentiment dataset academ dataset exampl stanford rotten tomato sentenc uci sentiment label sentenc data set googl dataset exampl shop plai",
        "Answer_preprocessed_content":"natur languag api train differ type dataset public dataset exampl benchmark eap custom dataset exampl feefo sentiment dataset academ dataset exampl stanford rotten tomato sentenc uci sentiment label sentenc data set googl dataset exampl shop plai",
        "Question_gpt_summary_original":"the user is seeking information about the training corpus of models behind google cloud platform's natural language apis.",
        "Question_gpt_summary":"user seek inform train corpu model googl cloud platform natur languag api",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- the natural language api is trained using different types of datasets.\n- public datasets examples include the five crowd-flower sentiment benchmarks.\n- customer datasets examples include the feefo sentiment dataset.\n- academic datasets examples include the stanford rotten tomatoes sentences and the uci sentiment labeled sentences data set.\n- google datasets examples include shopping and play. \n\nin summary, the natural language api is trained using various datasets, including public, customer, academic, and google datasets.",
        "Answer_gpt_summary":"possibl solut extract answer natur languag api train differ type dataset public dataset exampl includ crowd flower sentiment benchmark custom dataset exampl includ feefo sentiment dataset academ dataset exampl includ stanford rotten tomato sentenc uci sentiment label sentenc data set googl dataset exampl includ shop plai summari natur languag api train dataset includ public custom academ googl dataset"
    },
    {
        "Question_id":null,
        "Question_title":"Fail to show images when logging images in a project created in teams mode",
        "Question_body":"<p>We created a team and then added a project. while everything works when working with our personal space separately, in the project we created in the team space, images are not shown. Any ideas on how to fix it?<\/p>\n<p>please let me know what type of information I should provide here.<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1645005003760,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":117.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/fail-to-show-images-when-logging-images-in-a-project-created-in-teams-mode\/1936",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-16T22:52:21.656Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mfazampour\">@mfazampour<\/a>, could you please share a link to your project page so I can take a look? I\u2019m an admin so it can be a private project link.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-17T06:29:57.739Z",
                "Answer_body":"<p>Hi, thanks for replying. This is the link for the team one: <a href=\"https:\/\/wandb.ai\/ebm\/learsim-ebm\/runs\/2fidzop5?workspace=user-mfazampour\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>\n<p>and this one is the logs I have in my personal space: <a href=\"https:\/\/wandb.ai\/mfazampour\/learsim-ebm?workspace=user-mfazampour\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>\n<p><code>pretrain_model.transformation<\/code>, <code>pretrain_model.moving<\/code> and <code>pretrain_model.moving_warped<\/code> should be images. in the wandb page these are shown in a line plot and named <code>image-file<\/code><\/p>\n<p>I also realized that for one of the experiments, the images are actually shown. I don\u2019t know what\u2019s the reason and when it actually fails.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-02T22:26:25.358Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/mfazampour\">@mfazampour<\/a> , it looks like that run was killed after 33s. It\u2019s possible the run didn\u2019t log any images before that happened. Can you try logging an image in a new run to see if that was the issue?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-03T13:22:04.067Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/aidanjd\">@aidanjd<\/a> , This is another one: <a href=\"https:\/\/wandb.ai\/ebm\/learsim-ebm\/runs\/1nrd8saq?workspace=user-mfazampour\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>\n<p>This is how the image data is shown which is super weird considering the same code works in the personal space.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/d6d818201326e4319212e79dacb57053f16932fc.png\" data-download-href=\"\/uploads\/short-url\/uEB9P4N0YhyuLpA9yxgYsshSr2Q.png?dl=1\" title=\"Screen Shot 2022-03-03 at 2.21.09 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d6d818201326e4319212e79dacb57053f16932fc_2_690x274.png\" alt=\"Screen Shot 2022-03-03 at 2.21.09 PM\" data-base62-sha1=\"uEB9P4N0YhyuLpA9yxgYsshSr2Q\" width=\"690\" height=\"274\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d6d818201326e4319212e79dacb57053f16932fc_2_690x274.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/d6d818201326e4319212e79dacb57053f16932fc.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/d6d818201326e4319212e79dacb57053f16932fc.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/d6d818201326e4319212e79dacb57053f16932fc_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-03-03 at 2.21.09 PM<\/span><span class=\"informations\">827\u00d7329 18 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-07T16:38:02.294Z",
                "Answer_body":"<p>Hi Mohammad, I\u2019ll be taking over this ticket. Can you tell me how you are logging your images?<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-10T17:37:07.016Z",
                "Answer_body":"<p>Hi Mohammad, are you still experiencing this issue?<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-14T12:53:46.464Z",
                "Answer_body":"<p>Hi Mohammad, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-02T13:23:00.427Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"fail imag log imag project creat team mode creat team ad project work work person space separ project creat team space imag shown idea fix let know type inform provid",
        "Question_preprocessed_content":"fail imag log imag project creat team mode creat team ad project work work person space separ project creat team space imag shown idea fix let know type inform provid",
        "Answer_original_content":"mfazampour share link project page look admin privat project link thank repli link team log person space pretrain model transform pretrain model move pretrain model move warp imag page shown line plot name imag file realiz experi imag actual shown dont know what reason actual fail hei mfazampour look like run kill possibl run didnt log imag happen try log imag new run issu aidanjd imag data shown super weird consid code work person space screen shot mohammad ill take ticket tell log imag mohammad experienc issu mohammad heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"share link project page look admin privat project link thank repli link team log person space imag page shown line plot name realiz experi imag actual shown dont know what reason actual fail hei look like run kill possibl run didnt log imag happen try log imag new run issu imag data shown super weird consid code work person space screen shot mohammad ill take ticket tell log imag mohammad experienc issu mohammad heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering an issue where images are not being displayed in a project created in teams mode.",
        "Question_gpt_summary":"user encount issu imag displai project creat team mode",
        "Answer_gpt_summary_original":"possible solutions to the issue of images not being displayed in a project created in teams mode are: \n- checking if the run was killed before logging any images\n- trying to log an image in a new run to see if that was the issue\n- checking how the images are being logged and making sure it is done correctly.",
        "Answer_gpt_summary":"possibl solut issu imag displai project creat team mode check run kill log imag try log imag new run issu check imag log make sure correctli"
    },
    {
        "Question_id":null,
        "Question_title":"What is required in HumanLoopInput.InputContent for start_human_loop",
        "Question_body":"I have built a custom key-value extraction workflow that leverages textract Tables and Forms. It does a whole heap of post processing using the output of Textract to extract a small number of highly important fields from documents that are of very poor quality.\n\nMy client would like a human-in-the-loop to make minor changes to the results where certain fields are missing. I think that the sagemaker_a2i_runtime.start_human_loop is the perfect tool for this.\n\nI want to send the human reviewer, the input image and the current Key-Value pairs that I have extracted and have them find any that are missing, or mark them as not there. I have setup and tested the textract.analyse_image workflow with a human reviewer and like the results.\n\nWhat structure and data fields do I need to set in the HumanLoopInput field of sagemaker_a2i_runtime.start_human_loop to get this to work. I assume that it will look something like a dictionary of current K-V pairs and the s3 image file but I cannot find any documentation on how to do this.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645624147204,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":33.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJast4QKSTjCYWe-pcQFyHw\/what-is-required-in-human-loop-input-input-content-for-start-human-loop",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-01T02:18:34.042Z",
                "Answer_score":0,
                "Answer_body":"The HumanLoopInput field accepts JSON.\n\nhttps:\/\/docs.aws.amazon.com\/augmented-ai\/2019-11-07\/APIReference\/API_HumanLoopInput.html\n\nPlease take a look at the following documentation on how to get started with A2I. Specifically, the Create a Human Loop -> Custom Integration section has an example for \"sagemaker_a2i_runtime.start_human_loop\".\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/a2i-get-started-api.html#a2i-get-started-api-create-human-loop",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"requir humanloopinput inputcont start human loop built custom kei valu extract workflow leverag textract tabl form heap post process output textract extract small number highli import field document poor qualiti client like human loop minor chang result certain field miss think runtim start human loop perfect tool want send human review input imag current kei valu pair extract miss mark setup test textract analys imag workflow human review like result structur data field need set humanloopinput field runtim start human loop work assum look like dictionari current pair imag file document",
        "Question_preprocessed_content":"requir built custom extract workflow leverag textract tabl form heap post process output textract extract small number highli import field document poor qualiti client like minor chang result certain field miss think perfect tool want send human review input imag current pair extract miss mark setup test workflow human review like result structur data field need set humanloopinput field work assum look like dictionari current pair imag file document",
        "Answer_original_content":"humanloopinput field accept json http doc aw amazon com augment apirefer api humanloopinput html look follow document start specif creat human loop custom integr section exampl runtim start human loop http doc aw amazon com latest start api html start api creat human loop",
        "Answer_preprocessed_content":"humanloopinput field accept json look follow document start specif creat human loop custom integr section exampl",
        "Question_gpt_summary_original":"the user is looking for guidance on how to structure and populate the humanloopinput field of _a2i_runtime.start_human_loop in order to enable a human-in-the-loop to make minor changes to the results of a custom key-value extraction workflow.",
        "Question_gpt_summary":"user look guidanc structur popul humanloopinput field runtim start human loop order enabl human loop minor chang result custom kei valu extract workflow",
        "Answer_gpt_summary_original":"the solution to structuring and populating the humanloopinput field of _a2i_runtime.start_human_loop is to use json. the user can refer to the documentation on how to get started with a2i, specifically the \"create a human loop -> custom integration\" section which has an example for \"_a2i_runtime.start_human_loop\".",
        "Answer_gpt_summary":"solut structur popul humanloopinput field runtim start human loop us json user refer document start specif creat human loop custom integr section exampl runtim start human loop"
    },
    {
        "Question_id":null,
        "Question_title":"How to log two variables at different increments of timesteps?",
        "Question_body":"<p>Hi there! I was wondering, how do I deal with having multiple variables to log, but one of those variables I only want to log every 100 timesteps? The wandb docs seem to suggest that I need to collect all my metrics into one log function call, but in my scenario above where I want to track one variable every step and another variable every 100 steps, I would need multiple log calls. I saw the docs for the define metrics function, but I\u2019m not quite sure if that\u2019s the way to handle this. How do I approach this in PyTorch? Thanks!<\/p>\n<p>As an example, I currently have this Tensorboard logging that I\u2019m trying to convert to wandb:<\/p>\n<pre><code class=\"lang-auto\">print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\nwriter.add_scalar(\"charts\/episodic_return\", info[\"episode\"][\"r\"], global_step)\n\nif global_step % 100 == 0:\n    writer.add_scalar(\n        \"losses\/qf1_values\", qf1_a_values.mean().item(), global_step\n    )\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1673362179466,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":52.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chulabhaya\">@chulabhaya<\/a> , happy to help. The approach you are considering is correct. You can set a check in place and log a dictionary with the values you want and set the step value.<\/p>\n<pre><code class=\"lang-auto\">for i in range (300):\n    if i%100==0:\n        wandb.log({\"value\": i, \"value\": 100}, step =i)\n    else:\n        wandb.log({\"value\": 100})\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-axes-and-summaries-with-define_metric\">defined metric<\/a> function allows you to have more control over the representation of your x axis and also how that axes is incremented. There are a few examples listed in the linked doc on how it functions. Please let me know if you have any questions.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-log-two-variables-at-different-increments-of-timesteps\/3674",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-13T00:29:00.549Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/chulabhaya\">@chulabhaya<\/a> , happy to help. The approach you are considering is correct. You can set a check in place and log a dictionary with the values you want and set the step value.<\/p>\n<pre><code class=\"lang-auto\">for i in range (300):\n    if i%100==0:\n        wandb.log({\"value\": i, \"value\": 100}, step =i)\n    else:\n        wandb.log({\"value\": 100})\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log#customize-axes-and-summaries-with-define_metric\">defined metric<\/a> function allows you to have more control over the representation of your x axis and also how that axes is incremented. There are a few examples listed in the linked doc on how it functions. Please let me know if you have any questions.<\/p>",
                "Answer_score":20.6,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2023-01-23T11:22:11.010Z",
                "Answer_body":"<p>Thanks so much for confirming my approach! Much appreciated<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1673569740548,
        "Question_original_content":"log variabl differ increment timestep wonder deal have multipl variabl log variabl want log timestep doc suggest need collect metric log function scenario want track variabl step variabl step need multipl log call saw doc defin metric function sure that wai handl approach pytorch thank exampl current tensorboard log try convert print global step global step episod return info episod writer add scalar chart episod return info episod global step global step writer add scalar loss valu valu mean item global step",
        "Question_preprocessed_content":"log variabl differ increment timestep wonder deal have multipl variabl log variabl want log timestep doc suggest need collect metric log function scenario want track variabl step variabl step need multipl log call saw doc defin metric function sure that wai handl approach pytorch thank exampl current tensorboard log try convert",
        "Answer_original_content":"chulabhaya happi help approach consid correct set check place log dictionari valu want set step valu rang log valu valu step log valu defin metric function allow control represent axi ax increment exampl list link doc function let know question",
        "Answer_preprocessed_content":"happi help approach consid correct set check place log dictionari valu want set step valu defin metric function allow control represent axi ax increment exampl list link doc function let know question",
        "Question_gpt_summary_original":"the user is trying to figure out how to log two variables at different increments of timesteps in pytorch.",
        "Question_gpt_summary":"user try figur log variabl differ increment timestep pytorch",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- set a check in place and log a dictionary with the values you want and set the step value.\n- use the defined metric function to have more control over the representation of your x-axis and how that axis is incremented.\n\nin summary, the answer suggests using a check and a defined metric function to log two variables at different increments of timesteps in pytorch.",
        "Answer_gpt_summary":"possibl solut answer set check place log dictionari valu want set step valu us defin metric function control represent axi axi increment summari answer suggest check defin metric function log variabl differ increment timestep pytorch"
    },
    {
        "Question_id":null,
        "Question_title":"Early Stopping",
        "Question_body":"<p>Hello All,<\/p>\n<p>I\u2019m configuring a hyper parameter sweep. I have training, validation, and test set.<\/p>\n<p>I\u2019d like to use the test_loss as the final metric to optimize and val_loss for early stopping.<\/p>\n<p>I don\u2019t see a place to specify a metric for early stopping. Does it default to the same metric specified for overall optimization (of hyper parameters)? If so, how can I change this?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1630705196042,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":13.0,
        "Question_view_count":1487.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/early-stopping\/422",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-04T10:00:58.565Z",
                "Answer_body":"<p>It isn\u2019t possible to have a different metric for hyperband early stopping and search strategy. <a href=\"https:\/\/github.com\/wandb\/sweeps\/blob\/master\/hyperband_stopping.py#L176\">https:\/\/github.com\/wandb\/sweeps\/blob\/master\/hyperband_stopping.py#L176<\/a><\/p>\n<p>One workaround would be to use a search strategy that doesn\u2019t require a metric like <code>random<\/code> or <code>grid<\/code> and then use <code>val_loss<\/code> as your metric for early stopping. You can then easily reconfigure the resulting parameter importance and parallel coordinate plots to show <code>test_loss<\/code> in your dashboard.<\/p>\n<p>If you would would like this feature, you can file a feature request on our client repo issues.<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/github.com\/wandb\/client\">\n  <header class=\"source\">\n      <img src=\"https:\/\/github.githubassets.com\/favicons\/favicon.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/github.com\/wandb\/client\" target=\"_blank\" rel=\"noopener\">GitHub<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/345;\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f17a1f5cd507f0b9925c2434a26722ca80471a98_2_690x345.png\" class=\"thumbnail\" width=\"690\" height=\"345\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f17a1f5cd507f0b9925c2434a26722ca80471a98_2_690x345.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f17a1f5cd507f0b9925c2434a26722ca80471a98_2_1035x517.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/f17a1f5cd507f0b9925c2434a26722ca80471a98.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/f17a1f5cd507f0b9925c2434a26722ca80471a98_2_10x10.png\"><\/div>\n\n<h3><a href=\"https:\/\/github.com\/wandb\/client\" target=\"_blank\" rel=\"noopener\">GitHub - wandb\/client: \ud83d\udd25 A tool for visualizing and tracking your machine...<\/a><\/h3>\n\n  <p>\ud83d\udd25 A tool for visualizing and tracking your machine learning experiments. This repo contains the CLI and Python API. - GitHub - wandb\/client: \ud83d\udd25 A tool for visualizing and tracking your machine learn...<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":103.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-05T01:21:46.052Z",
                "Answer_body":"<p>Welcome to the forum, <a class=\"mention\" href=\"\/u\/max_wasserman\">@max_wasserman<\/a>! Great first question.<\/p>\n<p>While I can see why it might be good for us to add the ability to separate the early-stopping metric from the Bayesian optimization metric, <strong>I would strongly caution against using the test loss in any step of the process<\/strong> \u2013 whether its the optimization of parameters (obviously a  no-no!) or the optimization of hyperparameters. The PyTorch Lightning docs even say that you should only call <code>.test<\/code> <a href=\"https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/common\/trainer.html#testing\">\u201c[o]nly right before publishing your paper or pushing to production\u201d<\/a>.<\/p>\n<p>The purpose of metrics measured on the test set is to reflect, as veridically as possible, the performance of the model on more data drawn from the same distribution, which we in turn hope reflects the performance of the model on data in production. Selecting hyperparameters based on the test set breaks the \u201cinformation wall\u201d (more technically, the conditional independence relation) between the test data and the model\u2019s parameters that make the test set useful for getting unbiased estimates of true generalization performance.<\/p>\n<p>There is at least some indication that the use of fixed validation and test sets has led the ML field as a whole to \u201coverfit\u201d, in the sense of over-estimation of true generalization performance:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/arxiv.org\/abs\/1806.00451\">\n  <header class=\"source\">\n      <img src=\"https:\/\/static.arxiv.org\/static\/browse\/0.3.2.8\/images\/icons\/favicon.ico\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/arxiv.org\/abs\/1806.00451\" target=\"_blank\" rel=\"noopener\">arXiv.org<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c677c3b1d6c1743743356c807ff179c64f4f4f48_2_500x500.png\" class=\"thumbnail onebox-avatar\" width=\"500\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c677c3b1d6c1743743356c807ff179c64f4f4f48_2_500x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c677c3b1d6c1743743356c807ff179c64f4f4f48_2_750x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c677c3b1d6c1743743356c807ff179c64f4f4f48_2_1000x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/c677c3b1d6c1743743356c807ff179c64f4f4f48_2_10x10.png\">\n\n<h3><a href=\"https:\/\/arxiv.org\/abs\/1806.00451\" target=\"_blank\" rel=\"noopener\">Do CIFAR-10 Classifiers Generalize to CIFAR-10?<\/a><\/h3>\n\n  <p>Machine learning is currently dominated by largely experimental work focused\non improvements in a few key tasks. However, the impressive accuracy numbers of\nthe best performing models are questionable because the same test sets have\nbeen used to...<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"http:\/\/proceedings.mlr.press\/v97\/recht19a.html\">\n  <header class=\"source\">\n      <img src=\"https:\/\/proceedings.mlr.press\/v97\/assets\/images\/favicon-pmlr.ico\" class=\"site-icon\" width=\"48\" height=\"48\">\n\n      <a href=\"http:\/\/proceedings.mlr.press\/v97\/recht19a.html\" target=\"_blank\" rel=\"noopener\" title=\"12:00AM - 24 May 2019\">PMLR \u2013 24 May 19<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    \n\n<h3><a href=\"http:\/\/proceedings.mlr.press\/v97\/recht19a.html\" target=\"_blank\" rel=\"noopener\">Do ImageNet Classifiers Generalize to ImageNet?<\/a><\/h3>\n\n  <p>We build new test sets for the CIFAR-10 and ImageNet datasets. Both benchmarks have been the focus of intense research for almost a decade, raising the danger of overfitting to excessively re-used ...<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":136.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-05T19:17:08.278Z",
                "Answer_body":"<aside class=\"quote group-team\" data-username=\"charlesfrye\" data-post=\"3\" data-topic=\"422\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sea2.discourse-cdn.com\/business7\/user_avatar\/community.wandb.ai\/charlesfrye\/40\/52_2.png\" class=\"avatar\"> charlesfrye:<\/div>\n<blockquote>\n<p>electing hyperparameters based on the test set breaks the \u201cinformation wall\u201d (more technically, the conditional independence relation) betw<\/p>\n<\/blockquote>\n<\/aside>\n<p>Thanks so much for the responses.<\/p>\n<p>In this case I am using synthetic data (I can generate a lot it cheaply). I used the names val\/test_loss instead of validation set 1 (for early stopping) and validation set 2 (for bayes optimization) for simplicity.  I will generate more data after this (my true test set) for final unbiased estimation of generalization.<\/p>\n<p>It seems the best solution at the moment is to simply do what <a class=\"mention\" href=\"\/u\/_scott\">@_scott<\/a> recommended: use a random search and log \u2018test_loss\u2019 (actually validation set 2) for viz later.<\/p>\n<p>PS is this the preferred location\/forum where I should post technical questions of this kind? The GitHub page refers to a slack group that appears to be closed.<\/p>",
                "Answer_score":119.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-05T19:34:38.217Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"max_wasserman\" data-post=\"4\" data-topic=\"422\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/7bcc69\/40.png\" class=\"avatar\"> max_wasserman:<\/div>\n<blockquote>\n<p>PS is this the preferred location\/forum where I should post technical questions of this kind? The GitHub page refers to a slack group that appears to be closed.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Yes, this is the place, Thanks for checking!<\/p>\n<p>We really want to make sure our community enjoys the forums so we\u2019re silently moving from slack to discourse and we\u2019ll be making the announcement soon once we\u2019re confident the forums are all setup <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":44.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-05T19:54:39.788Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"max_wasserman\" data-post=\"4\" data-topic=\"422\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/avatars.discourse-cdn.com\/v4\/letter\/m\/7bcc69\/40.png\" class=\"avatar\"> max_wasserman:<\/div>\n<blockquote>\n<p>I will generate more data after this (my true test set) for final unbiased estimation of generalization.<\/p>\n<\/blockquote>\n<\/aside>\n<p>Ah okay, if you\u2019ve got an actual unbiased test set, then you\u2019re golden. I\u2019d be interested to hear more about your project!<\/p>\n<p>And yes, as <a class=\"mention\" href=\"\/u\/_scott\">@_scott<\/a> points out, if you aren\u2019t using Bayesian optimization, the choice of <code>metric<\/code> won\u2019t impact the behavior of your search. <code>random<\/code> is actually a pretty good choice for HPO, competitive with <code>bayes<\/code> in my and others\u2019 experience \u2013 and less prone to error\/misconfiguration. Also BTW, <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#early_terminate\">the <code>early_terminate<\/code> feature uses HyperBand<\/a>, which is more aggressive than the usual early stopping folks learn about in an ML class, based on stopping training when you see increasing validation set error. That style of early stopping is best delegated to the ML framework you\u2019re using.<\/p>\n<p>Thanks for pointing out the issue with the Slack link. As <a class=\"mention\" href=\"\/u\/bhutanisanyam1\">@bhutanisanyam1<\/a> said, we are moving discussion to this forum, but that link should\u2019ve still been in operation anyway. Will fix it shortly.<\/p>",
                "Answer_score":68.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-05T23:48:41.384Z",
                "Answer_body":"<p>I\u2019m doing some graph learning work (inputs are graphs, labels are graphs). Submitting paper soon, so I\u2019ll post it to one of these forums after!<\/p>",
                "Answer_score":113.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:07.313Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"earli stop hello configur hyper paramet sweep train valid test set like us test loss final metric optim val loss earli stop dont place specifi metric earli stop default metric specifi overal optim hyper paramet chang thank",
        "Question_preprocessed_content":"earli stop hello configur hyper paramet sweep train valid test set like us final metric optim earli stop dont place specifi metric earli stop default metric specifi overal optim chang thank",
        "Answer_original_content":"isnt possibl differ metric hyperband earli stop search strategi http github com sweep blob master hyperband stop workaround us search strategi doesnt requir metric like random grid us val loss metric earli stop easili reconfigur result paramet import parallel coordin plot test loss dashboard like featur file featur request client repo issu github github client tool visual track machin tool visual track machin learn experi repo contain cli python api github client tool visual track machin learn welcom forum max wasserman great question good add abil separ earli stop metric bayesian optim metric strongli caution test loss step process optim paramet obvious optim hyperparamet pytorch lightn doc test nly right publish paper push product purpos metric measur test set reflect verid possibl perform model data drawn distribut turn hope reflect perform model data product select hyperparamet base test set break inform wall technic condit independ relat test data model paramet test set us get unbias estim true gener perform indic us fix valid test set led field overfit sens estim true gener perform arxiv org cifar classifi gener cifar machin learn current domin larg experiment work focus improv kei task impress accuraci number best perform model question test set pmlr imagenet classifi gener imagenet build new test set cifar imagenet dataset benchmark focu intens research decad rais danger overfit excess charlesfry elect hyperparamet base test set break inform wall technic condit independ relat betw thank respons case synthet data gener lot cheapli name val test loss instead valid set earli stop valid set bay optim simplic gener data true test set final unbias estim gener best solut moment simpli scott recommend us random search log test loss actual valid set viz later prefer locat forum post technic question kind github page refer slack group appear close max wasserman prefer locat forum post technic question kind github page refer slack group appear close ye place thank check want sure commun enjoi forum silent move slack discours make announc soon confid forum setup max wasserman gener data true test set final unbias estim gener okai youv got actual unbias test set your golden interest hear project ye scott point arent bayesian optim choic metric wont impact behavior search random actual pretti good choic hpo competit bay experi prone error misconfigur btw earli termin featur us hyperband aggress usual earli stop folk learn class base stop train increas valid set error style earli stop best deleg framework your thank point issu slack link bhutanisanyam said move discuss forum link shouldv oper fix shortli graph learn work input graph label graph submit paper soon ill post forum topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"isnt possibl differ metric hyperband earli stop search strategi workaround us search strategi doesnt requir metric like us metric earli stop easili reconfigur result paramet import parallel coordin plot dashboard like featur file featur request client repo issu github github tool visual track tool visual track machin learn experi repo contain cli python api github tool visual track machin welcom forum great question good add abil separ metric bayesian optim metric strongli caution test loss step process optim paramet optim hyperparamet pytorch lightn doc nly right publish paper push product purpos metric measur test set reflect verid possibl perform model data drawn distribut turn hope reflect perform model data product select hyperparamet base test set break inform wall test data model paramet test set us get unbias estim true gener perform indic us fix valid test set led field overfit sens true gener perform classifi gener machin learn current domin larg experiment work focus improv kei task impress accuraci number best perform model question test set pmlr imagenet classifi gener imagenet build new test set imagenet dataset benchmark focu intens research decad rais danger overfit excess charlesfry elect hyperparamet base test set break inform wall betw thank respons case synthet data name instead valid set valid set simplic gener data final unbias estim gener best solut moment simpli recommend us random search log viz later prefer post technic question kind github page refer slack group appear close prefer post technic question kind github page refer slack group appear close ye place thank check want sure commun enjoi forum silent move slack discours make announc soon confid forum setup gener data final unbias estim gener okai youv got actual unbias test set your golden interest hear project ye point arent bayesian optim choic wont impact behavior search actual pretti good choic hpo competit experi prone btw featur us hyperband aggress usual earli stop folk learn class base stop train increas valid set error style earli stop best deleg framework your thank point issu slack link said move discuss forum link shouldv oper fix shortli graph learn work submit paper soon ill post forum topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking for a way to specify a metric for early stopping that is different from the metric used for overall optimization of hyperparameters.",
        "Question_gpt_summary":"user look wai specifi metric earli stop differ metric overal optim hyperparamet",
        "Answer_gpt_summary_original":"the answer suggests that it is not possible to have a different metric for hyperband early stopping and search strategy. however, a possible workaround is to use a search strategy that does not require a metric like random or grid and then use val_loss as the metric for early stopping. the answer also cautions against using the test loss in any step of the process, including the optimization of hyperparameters. instead, it recommends using an actual unbiased test set for final unbiased estimation of generalization. the answer also suggests that random is a good choice for hyperparameter optimization and that the early_terminate feature uses hyperband, which is more aggressive than the usual early stopping. finally, the answer confirms that this is the preferred location\/forum to post technical questions of this kind.",
        "Answer_gpt_summary":"answer suggest possibl differ metric hyperband earli stop search strategi possibl workaround us search strategi requir metric like random grid us val loss metric earli stop answer caution test loss step process includ optim hyperparamet instead recommend actual unbias test set final unbias estim gener answer suggest random good choic hyperparamet optim earli termin featur us hyperband aggress usual earli stop final answer confirm prefer locat forum post technic question kind"
    },
    {
        "Question_id":null,
        "Question_title":"Handle cache to keep the latest version of data",
        "Question_body":"<p>Hi Ruslan,<\/p>\n<p>I would like to ask the best practice on how to handle the following situation with DVC.<br>\nLet\u2019s suppose we have a big raw dataset stored in the cloud.<\/p>\n<ul>\n<li>So, we initialize a project (git + dvc) to work with this dataset. At first we would like to trace how to get the dataset locally. For this we use <code>dvc run<\/code> with commands as <code>wget url<\/code> to download the raw dataset. We commit changes etc.<\/li>\n<li>Next we develop some scripts to preprocess the raw dataset into a dataset we call v0.1.0. Then we use <code>dvc run<\/code> to run the preprocessing on the raw dataset and store the way to obtain the dataset v0.1.0. Thus, we have two folders like <code>raw_data<\/code> and <code>dataset_v0.1.0<\/code>. We commit and push with git and dvc. At this step we would like to keep only <code>dataset_v0.1.0<\/code> in the cache and remove <code>raw_data<\/code> from cache.<\/li>\n<li>We have another developer to work on the project, so he\/she uses git to clone the project and run <code>dvc checkout<\/code> or <code>dvc pull<\/code> to get the latest data version state. After the last command he\/she gets from remote both data: <code>raw_data<\/code> and <code>dataset_v0.1.0<\/code>.<\/li>\n<\/ul>\n<p>Is it possible to handle the cache in such manner that we keep only the latest version without using in-place preprocessing for a given commit ?<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1535720101051,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":631.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/handle-cache-to-keep-the-latest-version-of-data\/87",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-08-31T13:51:18.679Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/vfdev.5\">@vfdev.5<\/a>  !<\/p>\n<p>Great question! There proper way to go about it would be lock(<code>dvc lock preprocessing.dvc<\/code>) your preprocessing stage after you are done with it(i.e. when your dataset_v0.1.0 is ready). By locking your stage you will prevent dvc from looking into dependencies of your stage, so that when you <code>dvc push<\/code> it will not push cache for your <code>wget<\/code> stage to the cloud. When you would like to update your raw_data or dataset, you would simply <code>dvc unlock preprocessing.dvc<\/code>, do what you need to do and then <code>dvc lock<\/code> it back before pushing. Here is how it would look like in commands:<\/p>\n<pre><code class=\"lang-auto\">$ mkdir myrepo\n$ dvc myrepo\n$ git init\n$ dvc init\n$ dvc run -o raw_data wget https:\/\/example.com\/raw_data\n$ dvc run -f preprocess.dvc \\\n          -d preprocess.py -d raw_data \\\n          -o dataset_v0.1.0 \\\n          python preprocess.py raw_data dataset_v0.1.0\n# Locking the stage to prevent dvc from checking its dependencies\n# and also from pushing\/pulling them.\n$ dvc lock preprocess.dvc\n$ git commit -am \"Preprocess\"\n$ git push\n# Will push dataset_v0.1.0 but won't push raw_data\n$ dvc push\n#\n#\n# Now on the colleague's side dvc pull will not pull raw_data\n$ git pull\n$ dvc pull\n<\/code><\/pre>\n<p>Also, if your <code>raw_data<\/code> is stored on one of the supported remotes (external directory\/s3\/gs\/ssh\/hdfs) you can also utilize our <code>external dependency<\/code> feature, that lets you remember the state of the external file and easily <code>dvc repro<\/code> if it is changed(i.e. s3:\/\/mybucket\/raw_data is replaced with a new version). E.g. it would look something like:<\/p>\n<pre><code class=\"lang-auto\"># This will download `s3:\/\/mybucket\/raw_data` into `raw_data`,\n# create `raw_data.dvc` that will have `s3:\/\/mybucket\/raw_data`\n# listed as a dependency(i.e. it will save ETAG) and later when\n# you call `dvc repro` it will automatically check if `s3:\/\/mybucket\/raw_data`\n# had changed and if it did it dvc will automatically download it.\n$ dvc import s3:\/\/mybucket\/raw_data raw_data\n$ dvc run -f preprocess.dvc \\\n          -d preprocess.py -d raw_data \\\n          -o dataset_v0.1.0 \\\n          python preprocess.py raw_data dataset_v0.1.0\n# Now you can utilize dvc lock the same as the example above.\n# But when it is time to check if your original raw_data had been updated\n# you can simply run\n$ dvc repro\n# That will go and check if s3:\/\/mybucket\/raw_data is any different\n# from what you had before and will automatically reproduce the whole\n# pipeline for you\n<\/code><\/pre>\n<p>Here are some useful links:<br>\n<a href=\"https:\/\/dvc.org\/doc\/commands-reference\/lock\"><code>dvc lock<\/code> command reference<\/a><br>\n<a href=\"https:\/\/dvc.org\/doc\/commands-reference\/import\"><code>dvc import<\/code> command reference<\/a><br>\n<a href=\"https:\/\/dvc.org\/doc\/user-guide\/external-dependencies\">some info about external dependencies<\/a><\/p>\n<p>Please feel free to follow up with any questions, we are always happy to help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slightly_smiling_face.png?v=9\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\"><\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":132.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-08-31T14:03:37.539Z",
                "Answer_body":"<p>Ruslan, thanks a lot for your prompt reply !<\/p>",
                "Answer_score":2.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"handl cach latest version data ruslan like ask best practic handl follow situat let suppos big raw dataset store cloud initi project git work dataset like trace dataset local us run command wget url download raw dataset commit chang develop script preprocess raw dataset dataset us run run preprocess raw dataset store wai obtain dataset folder like raw data dataset commit push git step like dataset cach remov raw data cach develop work project us git clone project run checkout pull latest data version state command get remot data raw data dataset possibl handl cach manner latest version place preprocess given commit thank",
        "Question_preprocessed_content":"handl cach latest version data ruslan like ask best practic handl follow situat let suppos big raw dataset store cloud initi project work dataset like trace dataset local us command download raw dataset commit chang develop script preprocess raw dataset dataset us run preprocess raw dataset store wai obtain dataset folder like commit push git step like cach remov cach develop work project us git clone project run latest data version state command get remot data possibl handl cach manner latest version preprocess given commit thank",
        "Answer_original_content":"vfdev great question proper wai lock lock preprocess preprocess stage dataset readi lock stage prevent look depend stage push push cach wget stage cloud like updat raw data dataset simpli unlock preprocess need lock push look like command mkdir myrepo myrepo git init init run raw data wget http exampl com raw data run preprocess preprocess raw data dataset python preprocess raw data dataset lock stage prevent check depend push pull lock preprocess git commit preprocess git push push dataset won push raw data push colleagu pull pull raw data git pull pull raw data store support remot extern directori ssh hdf util extern depend featur let rememb state extern file easili repro chang mybucket raw data replac new version look like download mybucket raw data raw data creat raw data mybucket raw data list depend save etag later repro automat check mybucket raw data chang automat download import mybucket raw data raw data run preprocess preprocess raw data dataset python preprocess raw data dataset util lock exampl time check origin raw data updat simpli run repro check mybucket raw data differ automat reproduc pipelin us link lock command refer import command refer info extern depend feel free follow question happi help thank ruslan ruslan thank lot prompt repli",
        "Answer_preprocessed_content":"great question proper wai lock preprocess stage readi lock stage prevent look depend stage push cach stage cloud like updat dataset simpli need push look like command store support remot util featur let rememb state extern file easili replac new version look like us link command refer command refer info extern depend feel free follow question happi help thank ruslan ruslan thank lot prompt repli",
        "Question_gpt_summary_original":"the user is looking for the best practice to handle a cache to keep the latest version of data while working on a project with git and run.",
        "Question_gpt_summary":"user look best practic handl cach latest version data work project git run",
        "Answer_gpt_summary_original":"the answer suggests locking the preprocessing stage after it's done to prevent pushing cache for the wget stage to the cloud. unlock it when you want to update raw_data or dataset, do what you need to do, and then lock it back before pushing. the answer also mentions utilizing the external dependency feature to remember the state of the external file and easily reproduce it if it is changed.",
        "Answer_gpt_summary":"answer suggest lock preprocess stage prevent push cach wget stage cloud unlock want updat raw data dataset need lock push answer mention util extern depend featur rememb state extern file easili reproduc chang"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning Errors",
        "Question_body":"Hello, I am trying to run some of the sample notebooks from Microsoft Docs for Azure Machine Learning. I am running into the following error and cannot find a workaround though this appears to be a common error that others have also encountered with no workaround. cannot import name 'AzureMLAggregatedException' from 'azureml.exceptions'",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1614300692297,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/289691\/azure-machine-learning-errors.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-05T02:51:15.36Z",
                "Answer_score":0,
                "Answer_body":"Hi,\nIt is the explanation dashboards and the fairness dashboards and it cannot find the Lime library or its methods.\nRegards,\nAmy",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error hello try run sampl notebook microsoft doc run follow error workaround appear common error encount workaround import aggregatedexcept except",
        "Question_preprocessed_content":"error hello try run sampl notebook microsoft doc run follow error workaround appear common error encount workaround import aggregatedexcept",
        "Answer_original_content":"explan dashboard fair dashboard lime librari method regard ami",
        "Answer_preprocessed_content":"explan dashboard fair dashboard lime librari method regard ami",
        "Question_gpt_summary_original":"the user is encountering an error while trying to run sample notebooks from microsoft docs and is unable to find a workaround for the \"aggregatedexception\" import error.",
        "Question_gpt_summary":"user encount error try run sampl notebook microsoft doc unabl workaround aggregatedexcept import error",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer for the \"aggregatedexception\" import error. the answer seems to be addressing a different issue related to the inability to find the lime library or its methods while running explanation and fairness dashboards.",
        "Answer_gpt_summary":"solut provid answer aggregatedexcept import error answer address differ issu relat inabl lime librari method run explan fair dashboard"
    },
    {
        "Question_id":null,
        "Question_title":"503 on translations",
        "Question_body":"I started to see this error on multiple clusters in America. But there is nothing in the status page. I don't think we had any updates to our code.google.api_core.exceptions.ServiceUnavailable: 503 POST https:\/\/translation.googleapis.com\/language\/translate\/v2?prettyPrint=false: The service is unavailable at this time.I guess I need to wait, but posting here just to raise it",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1658295960000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":71.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/503-on-translations\/td-p\/445069\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-25T15:31:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Sometimes this issue happens because the Product is getting an Update.\nOthers it's because you are sending too many requests, and to mitigate this issue the recommendation is to split up the code."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"translat start error multipl cluster america statu page think updat code googl api core except serviceunavail post http translat googleapi com languag translat prettyprint fals servic unavail time guess need wait post rais",
        "Question_preprocessed_content":"translat start error multipl cluster america statu page think updat post servic unavail guess need wait post rais",
        "Answer_original_content":"issu happen product get updat send request mitig issu recommend split code",
        "Answer_preprocessed_content":"issu happen product get updat send request mitig issu recommend split code",
        "Question_gpt_summary_original":"the user is experiencing a 503 error on multiple clusters in america when attempting to use the google translate api, and is unsure of the cause as there is nothing in the status page.",
        "Question_gpt_summary":"user experienc error multipl cluster america attempt us googl translat api unsur caus statu page",
        "Answer_gpt_summary_original":"possible solutions to the 503 error on multiple clusters in america when using the google translate api are to wait for an update to finish, or to split up the code to reduce the number of requests being sent.",
        "Answer_gpt_summary":"possibl solut error multipl cluster america googl translat api wait updat finish split code reduc number request sent"
    },
    {
        "Question_id":65102618.0,
        "Question_title":"struggling to install python package via amazon sagemaker",
        "Question_body":"<p>I am using Amazon Sagemaker and trying to install gaapi4py package via anaconda python3 notebook.<\/p>\n<p>So far I've tried the following commands:<\/p>\n<pre><code>%conda install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>conda install gaapi4py\n\nGot same error:\n\nCollecting package metadata (current_repodata.json): failed\n\nCondaHTTPError: HTTP 000 CONNECTION FAILED for url &lt;https:\/\/conda.anaconda.org\/conda-forge\/linux-64\/current_repodata.json&gt;\nElapsed: -\n\nAn HTTP error occurred when trying to retrieve this URL.\nHTTP errors are often intermittent, and a simple retry will get you on your way.\n'https:\/\/conda.anaconda.org\/conda-forge\/linux-64'\n\n\n\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>Alternatively I've tried the below but it failed as well:<\/p>\n<pre><code>pip install gaapi4py\n<\/code><\/pre>\n<p>Error text:<\/p>\n<pre><code>WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803c50&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c8035f8&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803550&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803400&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('&lt;pip._vendor.urllib3.connection.VerifiedHTTPSConnection object at 0x7f657c803358&gt;: Failed to establish a new connection: [Errno 101] Network is unreachable',)': \/simple\/gaapi4py\/\nERROR: Could not find a version that satisfies the requirement gaapi4py (from versions: none)\nERROR: No matching distribution found for gaapi4py\nWARNING: You are using pip version 20.0.2; however, version 20.3 is available.\nYou should consider upgrading via the '\/home\/ec2-user\/anaconda3\/envs\/python3\/bin\/python -m pip install --upgrade pip' command.\nNote: you may need to restart the kernel to use updated packages.\n<\/code><\/pre>\n<p>What am I doing wrong? All previous packages worked well.<\/p>\n<p>UPD:<\/p>\n<p>Tried also as recommended in amazon book:<\/p>\n<pre><code>import sys\n!{sys.executable} -m pip install gaapi4py\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>import sys\n!conda install -y --prefix {sys.prefix} gaapi4py\n<\/code><\/pre>\n<p>Both didn't work neither, getting same errors as above.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1606886205577,
        "Question_favorite_count":null,
        "Question_last_edit_time":1606887753310,
        "Question_score":1.0,
        "Question_view_count":1148.0,
        "Answer_body":"<p>After talking back-in-forth with our IT department I figured out that custom libraries installation was blocked for security reasons.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65102618",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1644193786212,
        "Question_original_content":"struggl instal python packag try instal gaapipi packag anaconda python notebook far tri follow command conda instal gaapipi conda instal gaapipi got error collect packag metadata current repodata json fail condahttperror http connect fail url elaps http error occur try retriev url http error intermitt simpl retri wai http conda anaconda org conda forg linux note need restart kernel us updat packag altern tri fail pip instal gaapipi error text warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi warn retri retri total connect read redirect statu connect broken newconnectionerror fail establish new connect errno network unreach simpl gaapipi error version satisfi requir gaapipi version error match distribut gaapipi warn pip version version avail consid upgrad home user anaconda env python bin python pip instal upgrad pip command note need restart kernel us updat packag wrong previou packag work upd tri recommend amazon book import sy sy execut pip instal gaapipi import sy conda instal prefix sy prefix gaapipi work get error",
        "Question_preprocessed_content":"struggl instal python packag try instal gaapi packag anaconda python notebook far tri follow command altern tri fail error text wrong previou packag work upd tri recommend amazon book work get error",
        "Answer_original_content":"talk forth depart figur custom librari instal block secur reason",
        "Answer_preprocessed_content":"talk depart figur custom librari instal block secur reason",
        "Question_gpt_summary_original":"the user is struggling to install the gaapi4py package via anaconda python3 notebook, despite trying multiple commands and receiving various errors.",
        "Question_gpt_summary":"user struggl instal gaapipi packag anaconda python notebook despit try multipl command receiv error",
        "Answer_gpt_summary_original":"possible solutions: \n- contact the it department to request permission to install custom libraries.\n- use a different package or library that is already installed and approved for use.",
        "Answer_gpt_summary":"possibl solut contact depart request permiss instal custom librari us differ packag librari instal approv us"
    },
    {
        "Question_id":null,
        "Question_title":"Hypersweep metric with a dictionary hierarchy",
        "Question_body":"<p>I am creating hyper parameter sweeps of a linear regression problem, which has gone without a hitch so far. However, I am working to improve my skills with wand. In particular, I logged dictionaries of dictionaries. Here is an example:<\/p>\n<pre><code class=\"lang-python\"> wandb.log({\n    'epoch': epoch,     \n    'train':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch},                  \n    'valid':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch}\n  } , step=epoch, commit=True)\n<\/code><\/pre>\n<p>The metric should be the training loss. How does one specify it when using dictionary hierarchies? Have I done it correctly below?<\/p>\n<pre><code class=\"lang-python\"># docs: https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration\nsweep_config3 = {\n    'name' : 'broad_sweep', \n    'method' : 'random',\n    'metric' : {\n                'name': {'train': 'loss'},\n                'goal': 'minimize',\n               },\n    'parameters' : {\n        'lr' : {\n            'distribution': 'log_uniform_values',\n            'min': 1.e-3, \n            'max': 1.e-1},\n        'batch_size' : { 'value': 32 },\n        'optim' : { 'value': 'adamw' },\n        'nb_layers' : { 'values': [0, 2, 4] },\n        'pts_layer': { 'values': [5, 10, 30] },\n        'nb_epochs': { 'value': 200},\n    }\n}\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1659466475629,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":54.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hypersweep-metric-with-a-dictionary-hierarchy\/2839",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-04T23:52:12.738Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/erlebacher\">@erlebacher<\/a> , for your initial block of code, you can log dictionary of dictionaries just fine in a single run. However you cannot set a sweep up to optimize for multiple metrics at the same time. This will results in the following error message when attempting to do so:<\/p>\n<pre><code class=\"lang-auto\">wandb: WARNING Malformed sweep config detected! This may cause your sweep to behave in unexpected ways.\nwandb: WARNING To avoid this, please fix the sweep config schema violations below:\nwandb: WARNING   Violation 1. {'loss': None, 'train': None} is not of type 'string'\n<\/code><\/pre>\n<p>You will have to pass a single value in <code>metric<\/code> within your sweep config.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-05T02:40:29.269Z",
                "Answer_body":"<p>Thank you, <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> . I do understand the point you make. But what if in the example above, I wish the metric to be <code>min_loss<\/code>, defined under <code>train<\/code>?<\/p>\n<pre><code class=\"lang-python\"> wandb.log({\n    'epoch': epoch,     \n    'train':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch},                  \n    'valid':{'min_loss': t_min_loss, 'min_loss_epoch': t_min_loss_epoch}\n  } , step=epoch, commit=True)\n<\/code><\/pre>\n<p>Thanks!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-04T02:40:54.441Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"hypersweep metric dictionari hierarchi creat hyper paramet sweep linear regress problem gone hitch far work improv skill wand particular log dictionari dictionari exampl log epoch epoch train min loss min loss min loss epoch min loss epoch valid min loss min loss min loss epoch min loss epoch step epoch commit true metric train loss specifi dictionari hierarchi correctli doc http doc guid sweep configur sweep config broad sweep method random metric train loss goal minim paramet distribut log uniform valu min max batch size valu optim valu adamw layer valu pt layer valu epoch valu",
        "Question_preprocessed_content":"hypersweep metric dictionari hierarchi creat hyper paramet sweep linear regress problem gone hitch far work improv skill wand particular log dictionari dictionari exampl metric train loss specifi dictionari hierarchi correctli",
        "Answer_original_content":"erlebach initi block code log dictionari dictionari fine singl run set sweep optim multipl metric time result follow error messag attempt warn malform sweep config detect caus sweep behav unexpect wai warn avoid fix sweep config schema violat warn violat loss train type string pass singl valu metric sweep config thank mohammadbakir understand point exampl wish metric min loss defin train log epoch epoch train min loss min loss min loss epoch min loss epoch valid min loss min loss min loss epoch min loss epoch step epoch commit true thank topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"initi block code log dictionari dictionari fine singl run set sweep optim multipl metric time result follow error messag attempt pass singl valu sweep config thank understand point exampl wish metric defin thank topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering challenges with specifying a metric when using a dictionary hierarchy in a hyperparameter sweep of a linear regression problem.",
        "Question_gpt_summary":"user encount challeng specifi metric dictionari hierarchi hyperparamet sweep linear regress problem",
        "Answer_gpt_summary_original":"the answer suggests that the user cannot set up a hyperparameter sweep to optimize for multiple metrics at the same time. the solution is to pass a single value in the metric within the sweep config. the user can log a dictionary of dictionaries in a single run, but they need to ensure that the metric is a single value.",
        "Answer_gpt_summary":"answer suggest user set hyperparamet sweep optim multipl metric time solut pass singl valu metric sweep config user log dictionari dictionari singl run need ensur metric singl valu"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker Experiments Deletion Help Needed",
        "Question_body":"Hi Friends,\n\nI have deleted everything in Sagemaker - but support is asking me to delete the experiments that are still in my account : they sent me a link to follow\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/experiments-cleanup.html\n\nbut I have no idea how to complete this task -- does anyone know in terms that someone who has no idea what this means - can follow and achieve this task\n\nyou have no idea how much it would mean to me for any assistance",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1668037196721,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":44.0,
        "Answer_body":"You need to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. If you are already using SageMaker Studio, that should work.\n\nUse the second method. Create a file (Menu File -> New -> Python File). Rename it as cleanup_experiments.py(right click on the file on top and select Rename Python File), then paste the code in the documentation (those three sections, one after another). Save the file and open a terminal (Menu File -> New -> Terminal). Navigate to the directory where you saved the file and execute the command python cleanup_experiments.py",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFMzl26gfQna8sAZcCDJw_Q\/sage-maker-experiments-deletion-help-needed",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-10T09:00:37.516Z",
                "Answer_score":1,
                "Answer_body":"You need to use a computer with Python, the SageMaker SDK installed, and AWS credentials with enough permissions for that account configured. If you are already using SageMaker Studio, that should work.\n\nUse the second method. Create a file (Menu File -> New -> Python File). Rename it as cleanup_experiments.py(right click on the file on top and select Rename Python File), then paste the code in the documentation (those three sections, one after another). Save the file and open a terminal (Menu File -> New -> Terminal). Navigate to the directory where you saved the file and execute the command python cleanup_experiments.py",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1668070837516,
        "Question_original_content":"experi delet help need friend delet support ask delet experi account sent link follow http doc aw amazon com latest experi cleanup html idea complet task know term idea mean follow achiev task idea mean assist",
        "Question_preprocessed_content":"experi delet help need friend delet support ask delet experi account sent link follow idea complet task know term idea mean follow achiev task idea mean assist",
        "Answer_original_content":"need us python sdk instal aw credenti permiss account configur studio work us second method creat file menu file new python file renam cleanup experi right click file select renam python file past code document section save file open termin menu file new termin navig directori save file execut command python cleanup experi",
        "Answer_preprocessed_content":"need us python sdk instal aw credenti permiss account configur studio work us second method creat file renam click file select renam python file past code document save file open termin navig directori save file execut command python",
        "Question_gpt_summary_original":"the user is seeking assistance in deleting experiments from their account, as they have no idea how to complete the task outlined in the provided link.",
        "Question_gpt_summary":"user seek assist delet experi account idea complet task outlin provid link",
        "Answer_gpt_summary_original":"possible solutions to delete experiments from the user's account are:\n\n1. use a computer with python, the sdk installed, and aws credentials with enough permissions for that account configured.\n2. if the user is already using studio, they can use the second method.\n3. create a file (menu file -> new -> python file) and rename it as cleanup_experiments.py.\n4. paste the code in the documentation (those three sections, one after another).\n5. save the file and open a terminal (menu file -> new -> terminal).\n6. navigate to the directory where the file is saved and execute the command python cleanup_experiments.py.",
        "Answer_gpt_summary":"possibl solut delet experi user account us python sdk instal aw credenti permiss account configur user studio us second method creat file menu file new python file renam cleanup experi past code document section save file open termin menu file new termin navig directori file save execut command python cleanup experi"
    },
    {
        "Question_id":48197524.0,
        "Question_title":"How to remove the entire rows if value is NULL in Azure ML studio",
        "Question_body":"<p>I am preparing the data for regression model. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/LFaYl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LFaYl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I want to remove the entire row If all columns have value <code>NULL<\/code>. <\/p>\n\n<p>With Clean Missing Data module seems to me like I only able to remove missing values. But <code>NULL<\/code> is not considers mission value. <\/p>\n\n<p>So are there any other modules that simply can remove the entire row if all values are <code>NULL<\/code>'s<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1515625426463,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1477.0,
        "Answer_body":"<p>you could use \"<strong>Execute Python Script<\/strong>\" or \"<strong>Execute R Script<\/strong>\" to archive that. Or just use \"<strong>Apply SQL Transformation<\/strong>\" -> <code>SELECT * FROM tbl1 where column1 IS NULL AND column2 IS NULL<\/code>.... <\/p>\n\n<p>Greetings,\nStefan<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1526560111352,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48197524",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1516614808532,
        "Question_original_content":"remov entir row valu null studio prepar data regress model want remov entir row column valu null clean miss data modul like abl remov miss valu null consid mission valu modul simpli remov entir row valu null",
        "Question_preprocessed_content":"remov entir row valu null studio prepar data regress model want remov entir row column valu clean miss data modul like abl remov miss valu consid mission valu modul simpli remov entir row valu",
        "Answer_original_content":"us execut python script execut script archiv us appli sql transform select tbl column null column null greet stefan",
        "Answer_preprocessed_content":"us execut python script execut script archiv us appli sql transform greet stefan",
        "Question_gpt_summary_original":"the user is encountering a challenge in studio where they need to remove entire rows if all columns have a value of null.",
        "Question_gpt_summary":"user encount challeng studio need remov entir row column valu null",
        "Answer_gpt_summary_original":"possible solutions to remove entire rows if all columns have a value of null in studio are to use \"execute python script\" or \"execute r script\" or to use \"apply sql transformation\" with the query \"select * from tbl1 where column1 is null and column2 is null\".",
        "Answer_gpt_summary":"possibl solut remov entir row column valu null studio us execut python script execut script us appli sql transform queri select tbl column null column null"
    },
    {
        "Question_id":null,
        "Question_title":"Register Azure ML Model from DatabricksStep",
        "Question_body":"Hi,\n\nI'm calculating a model while executing a DatabricksStep in an Azure ML Pipeline, save it on my Blob Storage as .pkl file and upload it to the current Azure ML Run using Run.upload_file (). All this works without any problems.\n\nBut as soon as I try to register the model to the Azure ML Workspace using Run.register_model (), the script throws the following error:\n\nUserErrorException: UserErrorException:\nMessage:\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:\n1. You are not authorized to access this resource, or directory listing denied.\n2. you may not login your azure service, or use other subscription, you can check your\ndefault account by running azure cli commend:\n'az account list -o table'.\n3. You have multiple objects\/login session opened, please close all session and try again.\n\n InnerException None\n ErrorResponse \n\n{\n\"error\": {\n\"code\": \"UserError\",\n\"message\": \"\\nOperation returned an invalid status code 'Forbidden'. The possible reason could be:\\n1. You are not authorized to access this resource, or directory listing denied.\\n2. you may not login your azure service, or use other subscription, you can check your\\ndefault account by running azure cli commend:\\n'az account list -o table'.\\n3. You have multiple objects\/login session opened, please close all session and try again.\\n \"\n}\n}\n\nwith the following call stack\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/models_client.py in register_model(self, name, tags, properties, description, url, mime_type, framework, framework_version, unpack, experiment_name, run_id, datasets, sample_input_data, sample_output_data, resource_requirements)\n70 return self.\\\n71 _execute_with_workspace_arguments(self._client.ml_models.register, model,\n---> 72 custom_headers=ModelsClient.get_modelmanagement_custom_headers())\n73\n74 @error_with_model_id_handling\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/workspace_client.py in _execute_with_workspace_arguments(self, func, args, *kwargs)\n65\n66 def _execute_with_workspace_arguments(self, func, args, *kwargs):\n---> 67 return self._execute_with_arguments(func, copy.deepcopy(self._workspace_arguments), args, *kwargs)\n68\n69 def get_or_create_experiment(self, experiment_name, is_async=False):\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_with_arguments(self, func, args_list, args, *kwargs)\n536 return self._call_paginated_api(func, args_list, *kwargs)\n537 else:\n--> 538 return self._call_api(func, args_list, *kwargs)\n539 except ErrorResponseException as e:\n540 raise ServiceException(e)\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _call_api(self, func, args, *kwargs)\n234 return AsyncTask(future, _ident=ident, _parent_logger=self._logger)\n235 else:\n--> 236 return self._execute_with_base_arguments(func, args, *kwargs)\n237\n238 def _call_paginated_api(self, func, args, *kwargs):\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_with_base_arguments(self, func, args, *kwargs)\n323 total_retry = 0 if self.retries < 0 else self.retries\n324 return ClientBase._execute_func_internal(\n--> 325 back_off, total_retry, self._logger, func, _noop_reset, args, *kwargs)\n326\n327 @classmethod\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _execute_func_internal(cls, back_off, total_retry, logger, func, reset_func, args, *kwargs)\n343 return func(args, *kwargs)\n344 except Exception as error:\n--> 345 left_retry = cls._handle_retry(back_off, left_retry, total_retry, error, logger, func)\n346\n347 reset_func(args, *kwargs) # reset_func is expected to undo any side effects from a failed func call.\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/azureml\/_restclient\/clientbase.py in _handle_retry(cls, back_off, left_retry, total_retry, error, logger, func)\n384 3. You have multiple objects\/login session opened, please close all session and try again.\n385 \"\"\"\n--> 386 raise_from(UserErrorException(error_msg), error)\n387\n388 elif error.response.status_code == 429:\n\n\/databricks\/python\/lib\/python3.7\/site-packages\/six.py in raise_from(value, from_value)\n\nDid anybody experience the same error and knows what is its cause and how to solve it?\n\nBest,\nJonas",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1605264849177,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/162055\/register-azure-ml-model-from-databricksstep.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-01T05:26:07.107Z",
                "Answer_score":0,
                "Answer_body":"@Jonas-4379 Thanks, We have created bug with the product team, we could see It is n't deterministic error. we would recommend to raise a Azure support desk ticket from Help+Support blade from Azure portal. This will help you to share the details securely and work with an engineer who can provide more insights about the issue that if it can be replicated.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"regist model databricksstep calcul model execut databricksstep pipelin save blob storag pkl file upload current run run upload file work problem soon try regist model workspac run regist model script throw follow error usererrorexcept usererrorexcept messag oper return invalid statu code forbidden possibl reason author access resourc directori list deni login azur servic us subscript check default account run azur cli commend account list tabl multipl object login session open close session try innerexcept errorrespons error code usererror messag noper return invalid statu code forbidden possibl reason author access resourc directori list deni login azur servic us subscript check ndefault account run azur cli commend account list tabl multipl object login session open close session try follow stack databrick python lib python site packag restclient model client regist model self tag properti descript url mime type framework framework version unpack experi run dataset sampl input data sampl output data resourc requir return self execut workspac argument self client model regist model custom header modelscli modelmanag custom header error model handl databrick python lib python site packag restclient workspac client execut workspac argument self func arg kwarg def execut workspac argument self func arg kwarg return self execut argument func copi deepcopi self workspac argument arg kwarg def creat experi self experi async fals databrick python lib python site packag restclient clientbas execut argument self func arg list arg kwarg return self pagin api func arg list kwarg return self api func arg list kwarg errorresponseexcept rais serviceexcept databrick python lib python site packag restclient clientbas api self func arg kwarg return asynctask futur ident ident parent logger self logger return self execut base argument func arg kwarg def pagin api self func arg kwarg databrick python lib python site packag restclient clientbas execut base argument self func arg kwarg total retri self retri total retri self logger func noop reset arg kwarg classmethod databrick python lib python site packag restclient clientbas execut func intern cl total retri logger func reset func arg kwarg return func arg kwarg except error left retri cl handl retri left retri total retri error logger func reset func arg kwarg reset func expect undo effect fail func databrick python lib python site packag restclient clientbas handl retri cl left retri total retri error logger func multipl object login session open close session try rais usererrorexcept error msg error elif error respons statu code databrick python lib python site packag rais valu valu anybodi experi error know caus solv best jona",
        "Question_preprocessed_content":"regist model databricksstep calcul model execut databricksstep pipelin save blob storag pkl file upload current run work problem soon try regist model workspac script throw follow error usererrorexcept usererrorexcept messag oper return invalid statu code forbidden possibl reason author access resourc directori list deni login azur servic us subscript check default account run azur cli commend account list tabl multipl session open close session try innerexcept errorrespons error follow stack tag properti descript url framework unpack dataset return model func arg kwarg def func arg kwarg return arg kwarg def func arg kwarg return kwarg return kwarg errorresponseexcept rais serviceexcept func arg kwarg return asynctask return arg kwarg def func arg kwarg func arg kwarg func arg kwarg logger func arg kwarg return func except error error logger func kwarg expect undo effect fail func error logger func multipl session open close session try error elif anybodi experi error know caus solv best jona",
        "Answer_original_content":"jona thank creat bug product team determinist error recommend rais azur support desk ticket help support blade azur portal help share detail secur work engin provid insight issu replic",
        "Answer_preprocessed_content":"thank creat bug product team determinist error recommend rais azur support desk ticket help support blade azur portal help share detail secur work engin provid insight issu replic",
        "Question_gpt_summary_original":"the user encountered a \"forbidden\" error when attempting to register a model to the workspace using run.register_model() in an azure pipeline.",
        "Question_gpt_summary":"user encount forbidden error attempt regist model workspac run regist model azur pipelin",
        "Answer_gpt_summary_original":"possible solution: the user should raise an azure support desk ticket from the help+support blade in the azure portal. this will allow them to securely share the details of the issue and work with an engineer who can provide more insights about the issue and determine if it can be replicated.",
        "Answer_gpt_summary":"possibl solut user rais azur support desk ticket help support blade azur portal allow secur share detail issu work engin provid insight issu determin replic"
    },
    {
        "Question_id":68083831.0,
        "Question_title":"AWS Sagemaker inference endpoint not utilizing all vCPUs",
        "Question_body":"<p>I have deployed a custom model on sagemaker inference endpoint (single instance) and while I was load testing, I have observed that CPU utilization metric is maxing out at 100% but according to <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-cpu-gpu-utilization-100\/\" rel=\"nofollow noreferrer\">this post<\/a> it should max out at #vCPU*100 %. I have confirmed that the inference endpoint is not using all cores in clowdwatch logs.<\/p>\n<p>So if one prediction call requires one second to be processed to give response, the deployed model is only able to handle one API call per second which could have been increased to 8 calls per second if all vCPUs would have been used.<\/p>\n<p>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/p>\n<p>Or could we use multiprocessing python package inside <code>inference.py<\/code> file while deploying such that each call comes to the default core and from there all calculations\/prediction is done in any other core whichever is empty at that instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1624366085347,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1628443120180,
        "Question_score":3.0,
        "Question_view_count":661.0,
        "Answer_body":"<p>UPDATE<\/p>\n<ul>\n<li><p>Set three environment variables<\/p>\n<ol>\n<li>ENABLE_MULTI_MODEL as &quot;true&quot; (make sure it is string and not bool) and set <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L74\" rel=\"nofollow noreferrer\">SAGEMAKER_HANDLER<\/a> as custom model handler python module path if custom service else dont define it. Also make sure model name <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L94\" rel=\"nofollow noreferrer\">model.mar<\/a>, before compressing it as tar ball and storing in s3<\/li>\n<li>TS_DEFAULT_WORKERS_PER_MODEL as number of vcpus<\/li>\n<li>First environment variable makes sure torch serve env_vars are enabled and second one uses first setting and loads requested number of workers<\/li>\n<li>Setting can be done by passing env dictionary argument to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">PyTorch function<\/a>. Below is explanation as to why it works<\/li>\n<\/ol>\n<\/li>\n<li><p>From the looks of it, sagemaker deployment for pytorch model as given in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">Sagemaker SDK guide<\/a>, uses <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu\" rel=\"nofollow noreferrer\">this dockerfile<\/a>. In this docker, entrypoint is <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> as in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu#L124\" rel=\"nofollow noreferrer\">Dockerfile line#124<\/a>.<\/p>\n<\/li>\n<li><p>This <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> calls <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">serving.main()<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py\" rel=\"nofollow noreferrer\">serving.py<\/a>. Which ends up calling <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py#L34\" rel=\"nofollow noreferrer\">torchserve.start_torchserve(handler_service=HANDLER_SERVICE)<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py\" rel=\"nofollow noreferrer\">torchserve.py<\/a>.<\/p>\n<\/li>\n<li><p><a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L34\" rel=\"nofollow noreferrer\">At line 34 in torchserve.py<\/a> it defines &quot;\/etc\/default-ts.properties&quot; as DEFAULT_TS_CONFIG_FILE. This file is located <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties\" rel=\"nofollow noreferrer\">here<\/a>. In this file <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties#L2\" rel=\"nofollow noreferrer\">enable_envvars_config=true<\/a> is set. It will use this file setting IFF Environment variable &quot;ENABLE_MULTI_MODEL&quot; is set to &quot;false&quot; as refered <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L167\" rel=\"nofollow noreferrer\">here<\/a>. If it is set to &quot;true&quot; then it will use \/etc\/mme-ts.properties<\/p>\n<\/li>\n<\/ul>\n<hr \/>\n<p>As for the question <code>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/code>\nThere are various settings you can use\nFor models you can set <code>default_workers_per_model<\/code> in config.properties <code>TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all)<\/code> in environment variables. Environment variables take top priority.<\/p>\n<p>Other than that for each model, you can set the number of workers by using management API, but sadly it is not possible to curl to management API in sagemaker. SO TS_DEFAULT_WORKERS_PER_MODEL is the best bet.\nSetting this should make sure all cores are used.<\/p>\n<p>But if you are using docker file then in entrypoint you can setup scripts which wait for model loading and curl to it to set number of workers<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># load the model\ncurl -X POST localhost:8081\/models?url=model_1.mar&amp;batch_size=8&amp;max_batch_delay=50\n# after loading the model it is possible to set min_worker, etc\ncurl -v -X PUT http:\/\/localhost:8081\/models\/model_1?min_worker=1\n<\/code><\/pre>\n<p>About the other issue that logs confirm that not all cores are used, I face the same issue and believe that is a problem in the logging system. Please look at this issue <a href=\"https:\/\/github.com\/pytorch\/serve\/issues\/782\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/serve\/issues\/782<\/a>. The community itself agrees that if threads are not set, then by default then it prints 0, even if by default it uses 2*num_cores.<\/p>\n<p><strong>For an exhaustive set of all configs possible<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Reference: https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\n# Variables that can be configured through config.properties and Environment Variables\n# NOTE: Variables which can be configured through environment variables **SHOULD** have a\n# &quot;TS_&quot; prefix\n# debug\ninference_address=http:\/\/0.0.0.0:8080\nmanagement_address=http:\/\/0.0.0.0:8081\nmetrics_address=http:\/\/0.0.0.0:8082\nmodel_store=\/opt\/ml\/model\nload_models=model_1.mar\n# blacklist_env_vars\n# default_workers_per_model\n# default_response_timeout\n# unregister_model_timeout\n# number_of_netty_threads\n# netty_client_threads\n# job_queue_size\n# number_of_gpu\n# async_logging\n# cors_allowed_origin\n# cors_allowed_methods\n# cors_allowed_headers\n# decode_input_request\n# keystore\n# keystore_pass\n# keystore_type\n# certificate_file\n# private_key_file\n# max_request_size\n# max_response_size\n# default_service_handler\n# service_envelope\n# model_server_home\n# snapshot_store\n# prefer_direct_buffer\n# allowed_urls\n# install_py_dep_per_model\n# metrics_format\n# enable_metrics_api\n# initial_worker_port\n\n# Configuration which are not documented or enabled through environment variables\n\n# When below variable is set true, then the variables set in environment have higher precedence.\n# For example, the value of an environment variable overrides both command line arguments and a property in the configuration file. The value of a command line argument overrides a value in the configuration file.\n# When set to false, environment variables are not used at all\n# use_native_io=\n# io_ratio=\n# metric_time_interval=\nenable_envvars_config=true\n# model_snapshot=\n# version=\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1624470584452,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68083831",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1624416005592,
        "Question_original_content":"infer endpoint util vcpu deploi custom model infer endpoint singl instanc load test observ cpu util metric max accord post max vcpu confirm infer endpoint core clowdwatch log predict requir second process respons deploi model abl handl api second increas call second vcpu set deploy us vcpu increas concurr us multiprocess python packag insid infer file deploi come default core calcul predict core whichev instanc",
        "Question_preprocessed_content":"infer endpoint util vcpu deploi custom model infer endpoint load test observ cpu util metric max accord post max vcpu confirm infer endpoint core clowdwatch log predict requir second process respons deploi model abl handl api second increas call second vcpu set deploy us vcpu increas concurr us multiprocess python packag insid file deploi come default core core whichev instanc",
        "Answer_original_content":"updat set environ variabl enabl multi model true sure string bool set handler custom model handler python modul path custom servic dont defin sure model model mar compress tar ball store default worker model number vcpu environ variabl make sure torch serv env var enabl second us set load request number worker set pass env dictionari argument pytorch function explan work look deploy pytorch model given sdk guid us dockerfil docker entrypoint torchserv entrypoint dockerfil line torchserv entrypoint call serv main serv end call torchserv start torchserv handler servic handler servic torchserv line torchserv defin default properti default config file file locat file enabl envvar config true set us file set iff environ variabl enabl multi model set fals refer set true us mme properti question set deploy us vcpu increas concurr set us model set default worker model config properti default worker model nproc environ variabl environ variabl prioriti model set number worker manag api sadli possibl curl manag api default worker model best bet set sure core docker file entrypoint setup script wait model load curl set number worker load model curl post localhost model url model mar batch size max batch delai load model possibl set min worker curl http localhost model model min worker issu log confirm core face issu believ problem log look issu http github com pytorch serv issu commun agre thread set default print default us num core exhaust set config possibl refer http github com pytorch serv blob master doc configur variabl configur config properti environ variabl note variabl configur environ variabl prefix debug infer address http manag address http metric address http model store opt model load model model mar blacklist env var default worker model default respons timeout unregist model timeout number netti thread netti client thread job queue size number gpu async log cor allow origin cor allow method cor allow header decod input request keystor keystor pass keystor type certif file privat kei file max request size max respons size default servic handler servic envelop model server home snapshot store prefer direct buffer allow url instal dep model metric format enabl metric api initi worker port configur document enabl environ variabl variabl set true variabl set environ higher preced exampl valu environ variabl overrid command line argument properti configur file valu command line argument overrid valu configur file set fals environ variabl us nativ ratio metric time interv enabl envvar config true model snapshot version",
        "Answer_preprocessed_content":"updat set environ variabl true set custom model handler python modul path custom servic dont defin sure model compress tar ball store number vcpu environ variabl make sure torch serv enabl second us set load request number worker set pass env dictionari argument pytorch function explan work look deploy pytorch model given sdk guid us dockerfil docker entrypoint dockerfil line call end call line defin file locat file set us file set iff environ variabl set fals refer set true us question set us model set environ variabl environ variabl prioriti model set number worker manag api sadli possibl curl manag api best bet set sure core docker file entrypoint setup script wait model load curl set number worker issu log confirm core face issu believ problem log look issu commun agre thread set default print default us exhaust set config possibl",
        "Question_gpt_summary_original":"the user is encountering a challenge with their inference endpoint not utilizing all vcpus, resulting in a decreased ability to handle api calls per second.",
        "Question_gpt_summary":"user encount challeng infer endpoint util vcpu result decreas abil handl api call second",
        "Answer_gpt_summary_original":"the solution to the challenge of the inference endpoint not utilizing all vcpus is to update three environment variables: enable_multi_model, _handler, and ts_default_workers_per_model. the first environment variable enables torch serve env_vars, the second one loads the requested number of workers, and the third one sets the number of vcpus. the best bet is to set ts_default_workers_per_model to $(nproc --all) in environment variables. additionally, it is possible to set the number of workers for each model using the management api or by setting up scripts in the entrypoint of the docker file. there is a known issue with the logging system that may show 0 threads even if it uses 2*num_cores by default. for an exhaustive set of all configurations possible, refer to the documentation.",
        "Answer_gpt_summary":"solut challeng infer endpoint util vcpu updat environ variabl enabl multi model handler default worker model environ variabl enabl torch serv env var second load request number worker set number vcpu best bet set default worker model nproc environ variabl addition possibl set number worker model manag api set script entrypoint docker file known issu log thread us num core default exhaust set configur possibl refer document"
    },
    {
        "Question_id":null,
        "Question_title":"Custom Vision for Canada region",
        "Question_body":"I used Custom Vision to create a small proof of concept project, and it works super nice; however, now that we are ready for the next steps to see if it is doable to use it, I have got some questions that might be roadblocks if I were to go ahead and work on the business implementation:\n\nThe service cannot be deployed in Canada regions. Is this something that will be considered in the future? This is a huge block because the items classified may contain data that should not leave the Canadian space\n\n\nWhat's the privacy terms of using custom vision or where can I find them to read? As the previous item describes, the items classified could contain compromised data, so it would be unfeasible to use the custom vision service if the data is going to be \"shared\" or \"used\" by Microsoft or other parties for other purposes.\n\nAs a side question, is this service capable of classifying PDF documents as images? And if not, is there a known Azure\/Microsoft service that does so?\n\nThanks a bunch! :)\n\nKenny Perroni",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606512423207,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@KennyPerroni-1017 Thanks for the feedback. We have forwarded this feedback to our product team, You can also raise a user voice request here so the community can vote and provide their feedback, the product team then checks this feedback and implements the same for Canada region. For Region availability please check the following link\n\nFor custom vision service you control over the storage and deletion of any customer data that store as part of the service. Please follow the below for privacy and compliance. As with all of the Cognitive Services, developers using the Custom Vision service should be aware of Microsoft's policies on customer data. See the below Cognitive Services page on the Microsoft Trust Center to learn more.\nhttps:\/\/azure.microsoft.com\/en-us\/support\/legal\/cognitive-services-compliance-and-privacy\/\n\n\n\n\nThe Computer vision Read service support PDF document as images. Here is the link for vision best practice and samples. Also the Form Recognizer supports OCR and PDF documents with AI builder.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/178721\/custom-vision-for-canada-region.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-30T04:09:44.577Z",
                "Answer_score":1,
                "Answer_body":"@KennyPerroni-1017 Thanks for the feedback. We have forwarded this feedback to our product team, You can also raise a user voice request here so the community can vote and provide their feedback, the product team then checks this feedback and implements the same for Canada region. For Region availability please check the following link\n\nFor custom vision service you control over the storage and deletion of any customer data that store as part of the service. Please follow the below for privacy and compliance. As with all of the Cognitive Services, developers using the Custom Vision service should be aware of Microsoft's policies on customer data. See the below Cognitive Services page on the Microsoft Trust Center to learn more.\nhttps:\/\/azure.microsoft.com\/en-us\/support\/legal\/cognitive-services-compliance-and-privacy\/\n\n\n\n\nThe Computer vision Read service support PDF document as images. Here is the link for vision best practice and samples. Also the Form Recognizer supports OCR and PDF documents with AI builder.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1606709384576,
        "Question_original_content":"custom vision canada region custom vision creat small proof concept project work super nice readi step doabl us got question roadblock ahead work busi implement servic deploi canada region consid futur huge block item classifi contain data leav canadian space privaci term custom vision read previou item describ item classifi contain compromis data unfeas us custom vision servic data go share microsoft parti purpos question servic capabl classifi pdf document imag known azur microsoft servic thank bunch kenni perroni",
        "Question_preprocessed_content":"custom vision canada region custom vision creat small proof concept project work super nice readi step doabl us got question roadblock ahead work busi implement servic deploi canada region consid futur huge block item classifi contain data leav canadian space privaci term custom vision read previou item describ item classifi contain compromis data unfeas us custom vision servic data go share microsoft parti purpos question servic capabl classifi pdf document imag known servic thank bunch kenni perroni",
        "Answer_original_content":"kennyperroni thank feedback forward feedback product team rais user voic request commun vote provid feedback product team check feedback implement canada region region avail check follow link custom vision servic control storag delet custom data store servic follow privaci complianc cognit servic develop custom vision servic awar microsoft polici custom data cognit servic page microsoft trust center learn http azur microsoft com support legal cognit servic complianc privaci vision read servic support pdf document imag link vision best practic sampl form recogn support ocr pdf document builder",
        "Answer_preprocessed_content":"thank feedback forward feedback product team rais user voic request commun vote provid feedback product team check feedback implement canada region region avail check follow link custom vision servic control storag delet custom data store servic follow privaci complianc cognit servic develop custom vision servic awar microsoft polici custom data cognit servic page microsoft trust center learn vision read servic support pdf document imag link vision best practic sampl form recogn support ocr pdf document builder",
        "Question_gpt_summary_original":"the user encountered challenges with deploying the custom vision service in canada regions, understanding the privacy terms of using the service, and determining if the service is capable of classifying pdf documents as images.",
        "Question_gpt_summary":"user encount challeng deploi custom vision servic canada region understand privaci term servic determin servic capabl classifi pdf document imag",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. the user can raise a user voice request for the custom vision service to be implemented in canada regions.\n2. the user has control over the storage and deletion of any customer data that is stored as part of the service.\n3. the user should follow the privacy and compliance guidelines provided by microsoft for using cognitive services.\n4. the custom vision service supports pdf documents as images.\n5. the user can check out the best practices and samples for the custom vision service.\n6. the form recognizer supports ocr and pdf documents with ai builder. \n\nin summary, the answer provides possible solutions for the user's challenges with deploying the custom vision service in canada regions, understanding the privacy terms of using the service, and determining if the service is capable of classifying pdf documents as images.",
        "Answer_gpt_summary":"possibl solut answer user rais user voic request custom vision servic implement canada region user control storag delet custom data store servic user follow privaci complianc guidelin provid microsoft cognit servic custom vision servic support pdf document imag user check best practic sampl custom vision servic form recogn support ocr pdf document builder summari answer provid possibl solut user challeng deploi custom vision servic canada region understand privaci term servic determin servic capabl classifi pdf document imag"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ml notebook does not show widgets from ipywidgets",
        "Question_body":"ipywidgets seems to work fine for the simplest usages, i.e. just using a slider. However, when trying to use more complex functionality the notebook does not show \/ display the widgets anymore.\n\nSee in the picture:\n\n\nThe simple usage\n\n widgets.IntSlider()\n\n\n\nworks fine.\nHowever, using ipywidget's interact does not show any widget:\n\n def f(x):\n     return x\n    \n interact(f, x=10)\n\n\n\nWhen I change the editor using the dropdown\n\n\nand use Jupyter or JupyterLab, everything works as expected without flaws.",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_creation_time":1621085408217,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/396317\/azure-ml-notebook-does-not-show-widgets-from-ipywi.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-26T11:37:49.587Z",
                "Answer_score":0,
                "Answer_body":"@imbachb-2223 Thanks for the feedback. This specific type of ipywidget is currently not supported, we have roadmap to support in the future.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"notebook widget ipywidget ipywidget work fine simplest usag slider try us complex function notebook displai widget anymor pictur simpl usag widget intslid work fine ipywidget interact widget def return interact chang editor dropdown us jupyt jupyterlab work expect flaw",
        "Question_preprocessed_content":"notebook widget ipywidget ipywidget work fine simplest usag slider try us complex function notebook displai widget anymor pictur simpl usag work fine ipywidget interact widget def return interact chang editor dropdown us jupyt jupyterlab work expect flaw",
        "Answer_original_content":"imbachb thank feedback specif type ipywidget current support roadmap support futur",
        "Answer_preprocessed_content":"thank feedback specif type ipywidget current support roadmap support futur",
        "Question_gpt_summary_original":"the user encountered challenges with ipywidgets where the notebook does not show\/display the widgets when using more complex functionality.",
        "Question_gpt_summary":"user encount challeng ipywidget notebook displai widget complex function",
        "Answer_gpt_summary_original":"there are currently no solutions for the issue of ipywidgets not displaying when using more complex functionality. however, the developers have a roadmap to support this type of ipywidget in the future.",
        "Answer_gpt_summary":"current solut issu ipywidget displai complex function develop roadmap support type ipywidget futur"
    },
    {
        "Question_id":65830524.0,
        "Question_title":"SQLAlchemy Oracle - InvalidRequestError: could not retrieve isolation level",
        "Question_body":"<p>I am having problems accessing tables in an Oracle database over a SQLAlchemy connection. Specifically, I am using Kedro <code>catalog.load('table_name')<\/code> and getting the error message <code>Table table_name not found<\/code>. So I decided to test my connection using the method listed in this answer: <a href=\"https:\/\/stackoverflow.com\/questions\/41887344\/how-to-verify-sqlalchemy-engine-object\">How to verify SqlAlchemy engine object<\/a>.<\/p>\n<pre><code>from sqlalchemy import create_engine\nengine = create_engine('oracle+cx_oracle:\/\/USER:PASSWORD@HOST:PORT\/?service_name=SERVICE_NAME')\nengine.connect()\n<\/code><\/pre>\n<p>Error: <code>InvalidRequestError: could not retrieve isolation level<\/code><\/p>\n<p>I have tried explicitly adding an isolation level as explained in the <a href=\"https:\/\/docs.sqlalchemy.org\/en\/14\/core\/connections.html#setting-transaction-isolation-levels-including-dbapi-autocommit\" rel=\"nofollow noreferrer\">documentation<\/a> like this:<\/p>\n<pre><code>engine = create_engine('oracle+cx_oracle:\/\/USER:PASSWORD@HOST:PORT\/?service_name=SERVICE_NAME', execution_options={'isolation_level': 'AUTOCOMMIT'})\n<\/code><\/pre>\n<p>and this:<\/p>\n<pre><code>engine.connect().execution_options(isolation_level='AUTOCOMMIT')\n<\/code><\/pre>\n<p>and this:<\/p>\n<pre><code>connection = engine.connect()\nconnection = connection.execution_options(\n    isolation_level=&quot;AUTOCOMMIT&quot;\n)\n<\/code><\/pre>\n<p>but I get the same error in all cases.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1611242774307,
        "Question_favorite_count":null,
        "Question_last_edit_time":1611265283776,
        "Question_score":0.0,
        "Question_view_count":196.0,
        "Answer_body":"<p>Upgrading from SqlAlchemy 1.3.21 to 1.3.22 solved the problem.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65830524",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1611332594540,
        "Question_original_content":"sqlalchemi oracl invalidrequesterror retriev isol level have problem access tabl oracl databas sqlalchemi connect specif catalog load tabl get error messag tabl tabl decid test connect method list answer verifi sqlalchemi engin object sqlalchemi import creat engin engin creat engin oracl oracl user password host port servic servic engin connect error invalidrequesterror retriev isol level tri explicitli ad isol level explain document like engin creat engin oracl oracl user password host port servic servic execut option isol level autocommit engin connect execut option isol level autocommit connect engin connect connect connect execut option isol level autocommit error case",
        "Question_preprocessed_content":"sqlalchemi oracl invalidrequesterror retriev isol level have problem access tabl oracl databas sqlalchemi connect specif get error messag decid test connect method list answer verifi sqlalchemi engin object error tri explicitli ad isol level explain document like error case",
        "Answer_original_content":"upgrad sqlalchemi solv problem",
        "Answer_preprocessed_content":"upgrad sqlalchemi solv problem",
        "Question_gpt_summary_original":"the user is encountering challenges accessing tables in an oracle database over a sqlalchemy connection, and is unable to set the isolation level to resolve the issue.",
        "Question_gpt_summary":"user encount challeng access tabl oracl databas sqlalchemi connect unabl set isol level resolv issu",
        "Answer_gpt_summary_original":"upgrading from sqlalchemy version 1.3.21 to version 1.3.22 could potentially solve the issue of challenges accessing tables in an oracle database over a sqlalchemy connection and being unable to set the isolation level.",
        "Answer_gpt_summary":"upgrad sqlalchemi version version potenti solv issu challeng access tabl oracl databas sqlalchemi connect unabl set isol level"
    },
    {
        "Question_id":null,
        "Question_title":"How to use blob storage file as input to azure ml endpoint",
        "Question_body":"I have big data that I need to pass it to already trained Machine Learning model on Azure and has been deployed as online endpoint, I realize that batch endpoints supports adding a reference to blob file as input, my question is: how to do the same for online enpdoints ?\n\nSo far all examples I see are passing the payload as json (Even in the test tab of the online enpoints) but i don't know how to simply pass a blob storage file's uri as the payload.",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1655374179990,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@MostafaMansour-4203 Thanks for the question. I have checked internally with the product team, Currently Online endpoints are for Realtime synchronous requests.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/891865\/how-to-use-blob-storage-file-as-input-to-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-24T06:20:12.08Z",
                "Answer_score":1,
                "Answer_body":"@MostafaMansour-4203 Thanks for the question. I have checked internally with the product team, Currently Online endpoints are for Realtime synchronous requests.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-06-24T07:11:52.483Z",
                "Answer_score":1,
                "Answer_body":"Answering my own question: real time endpoints doesn't support blob storage file\/folder as input, will have to use batch endpoints",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":21.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1656051612080,
        "Question_original_content":"us blob storag file input endpoint big data need pass train machin learn model azur deploi onlin endpoint realiz batch endpoint support ad refer blob file input question onlin enpdoint far exampl pass payload json test tab onlin enpoint know simpli pass blob storag file uri payload",
        "Question_preprocessed_content":"us blob storag file input endpoint big data need pass train machin learn model azur deploi onlin endpoint realiz batch endpoint support ad refer blob file input question onlin enpdoint far exampl pass payload json know simpli pass blob storag file uri payload",
        "Answer_original_content":"mostafamansour thank question check intern product team current onlin endpoint realtim synchron request answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"thank question check intern product team current onlin endpoint realtim synchron request answer help click upvot help commun member read thread",
        "Question_gpt_summary_original":"the user is trying to figure out how to use a blob storage file as input to an azure online endpoint, as opposed to passing the payload as json.",
        "Question_gpt_summary":"user try figur us blob storag file input azur onlin endpoint oppos pass payload json",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the question of using a blob storage file as input to an azure online endpoint. the answer only clarifies that online endpoints are currently only for real-time synchronous requests.",
        "Answer_gpt_summary":"solut provid answer question blob storag file input azur onlin endpoint answer clarifi onlin endpoint current real time synchron request"
    },
    {
        "Question_id":null,
        "Question_title":"How to pass data to an endpoint",
        "Question_body":"Hello,\nI have followed the DeepAR Chicago Traffic violations notebook example. The Model and Endpoint has been created and the forecasting is working.\n\nhttps:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/deepar_chicago_traffic_violations\/deepar_chicago_traffic_violations.ipynb\n\nHowevr, I haven't deleted the model nor the endpoint in order to use it externally. I have created a Python script on an EC2 that tries to load the endpoint and passes the data to it to get a prediction, and here is what I am doing:\n\nLoading the CSV exactly the way I did it on the notebook\nParsing the CSV the same way I did on the notebook for the \"predictor.predict\" command\nInstead of using the \"predictor.predict\", I am using \"invoke_endpoint\" to load the endpoint and passing the data from the previous point\nInstead of getting the same response I got on the notebook, I am getting the following message:\n\"type: <class 'list'>, valid types: <class 'bytes'>, <class 'bytearray'>, file-like object\"\n\nNot sure what the issue is, seems that it requires a byte data... I guess I cannot send the data as a list to the endpoint and I need to serialize it or to encode it? convert to to JSON? to Bytes?\n\nAny help will be appreciated.\nRegards",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1625081705000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":277.0,
        "Answer_body":"Hello,\n\nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().\n\nIf you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:\n\nfrom sagemaker.serializers import IdentitySerializer\nfrom sagemaker.deserializers import JSONDeserializer\nserializer=IdentitySerializer(content_type=\"application\/json\")\n\nHope this helps!\n\nTo check out the various serializer options that can work for your different use cases check the following link.\nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM\n\nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpamBayk2RT6c6KuNop0DQQ\/how-to-pass-data-to-an-endpoint",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-15T13:25:47.348Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nSo the issue here is the predictor.predict command converts the data to the format necessary for the endpoint to understand, thus you need to serialize or encode the payload by yourself. To do this you can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode().\n\nIf you want to use the predictor class this is taken care of by the serializer option. The serializer encodes\/decodes the data for us and lets you simply call the endpoint through the predictor class. An example of this is the following code snippet:\n\nfrom sagemaker.serializers import IdentitySerializer\nfrom sagemaker.deserializers import JSONDeserializer\nserializer=IdentitySerializer(content_type=\"application\/json\")\n\nHope this helps!\n\nTo check out the various serializer options that can work for your different use cases check the following link.\nSerializers: https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\n\nEdited by: rvegira-aws on Jul 22, 2021 9:22 AM\n\nEdited by: rvegira-aws on Jul 22, 2021 9:24 AM",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-07-22T16:21:57.000Z",
                "Answer_score":0,
                "Answer_body":"Thanks rvegira-aws,\n\nI changed the approach, instead of using the \"invoke_endpoint\" method, I have used the predictor class as you suggested and this has fixed the issue.\n\nRegards.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-17T16:50:37.000Z",
                "Answer_score":0,
                "Answer_body":"I faced the exact problem when building models for my website, thanks for the question",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639574747348,
        "Question_original_content":"pass data endpoint hello follow deepar chicago traffic violat notebook exampl model endpoint creat forecast work http github com aw amazon exampl blob master introduct appli machin learn deepar chicago traffic violat deepar chicago traffic violat ipynb howevr haven delet model endpoint order us extern creat python script tri load endpoint pass data predict load csv exactli wai notebook pars csv wai notebook predictor predict command instead predictor predict invok endpoint load endpoint pass data previou point instead get respons got notebook get follow messag type valid type file like object sure issu requir byte data guess send data list endpoint need serial encod convert json byte help appreci regard",
        "Question_preprocessed_content":"pass data endpoint hello follow deepar chicago traffic violat notebook exampl model endpoint creat forecast work howevr haven delet model endpoint order us extern creat python script tri load endpoint pass data predict load csv exactli wai notebook pars csv wai notebook command instead load endpoint pass data previou point instead get respons got notebook get follow messag type valid type object sure issu requir byte guess send data list endpoint need serial encod convert json byte help appreci regard",
        "Answer_original_content":"hello issu predictor predict command convert data format necessari endpoint understand need serial encod payload work like json dump payload byte arrai json dump payload encod want us predictor class taken care serial option serial encod decod data let simpli endpoint predictor class exampl follow code snippet serial import identityseri deseri import jsondeseri serial identityseri content type applic json hope help check serial option work differ us case check follow link serial http readthedoc stabl api infer serial html edit rvegira aw jul edit rvegira aw jul",
        "Answer_preprocessed_content":"hello issu command convert data format necessari endpoint understand need serial encod payload work like byte arrai want us predictor class taken care serial option serial data let simpli endpoint predictor class exampl follow code snippet serial import identityseri deseri import jsondeseri hope help check serial option work differ us case check follow link serial edit jul edit jul",
        "Question_gpt_summary_original":"the user is encountering challenges in passing data to an endpoint created from a deepar chicago traffic violations notebook example, and is receiving an error message indicating that the data must be in a byte format.",
        "Question_gpt_summary":"user encount challeng pass data endpoint creat deepar chicago traffic violat notebook exampl receiv error messag indic data byte format",
        "Answer_gpt_summary_original":"the solution to the user's challenge in passing data to an endpoint created from a deepar chicago traffic violations notebook example is to serialize or encode the payload by themselves. they can work with something like json.dumps(payload) or for a byte array json.dumps(payload).encode(). if they want to use the predictor class, the serializer option takes care of encoding\/decoding the data for them and lets them simply call the endpoint through the predictor class. they can check out the various serializer options that can work for their different use cases at the serializers link provided.",
        "Answer_gpt_summary":"solut user challeng pass data endpoint creat deepar chicago traffic violat notebook exampl serial encod payload work like json dump payload byte arrai json dump payload encod want us predictor class serial option take care encod decod data let simpli endpoint predictor class check serial option work differ us case serial link provid"
    },
    {
        "Question_id":69076270.0,
        "Question_title":".txt altered after save leads to CSV reader seeing too many fields",
        "Question_body":"<p>I am running a <code>JupyterLab<\/code> on <code>AWS SageMaker<\/code>. Kernel: <code>conda_amazonei_mxnet_p27<\/code><\/p>\n<p>The number of fields found: <code>saw 9<\/code> increments by 1, each run.<\/p>\n<p><strong>Error:<\/strong> <code>ParserError: Error tokenizing data. C error: Expected 2 fields in line 50, saw 9<\/code><\/p>\n<hr \/>\n<h3>Code:<\/h3>\n<p>Invocation (Error doesn't appear when running all cells before this but does when this is ran):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>train = open('train_textcorrupted.csv', 'a')\nval = open('val.csv', 'a')\nclasses = open('classes.txt', 'a')\nuni_label = 'Organisation\\tUniversity'\nn_pad = 4\nfor i in range(len(unis)-n_pad):\n    record = ' '.join(unis[i:(i+n_pad)])\n    full_record = f'{uni_label}\\t{record}\\n'\n    if random.random() &gt; 0.9:\n        val.write(full_record)\n    else:\n        train.write(full_record) \n\nclasses.write(uni_label)\nclasses.close() \nval.close()\ntrain.close()                      \n<\/code><\/pre>\n<h3>Traceback:<\/h3>\n<pre class=\"lang-sh prettyprint-override\"><code>---------------------------------------------------------------------------\nParserError                               Traceback (most recent call last)\n&lt;ipython-input-8-89b1728bd5a6&gt; in &lt;module&gt;\n      7       --gpus 1\n      8     &quot;&quot;&quot;.split()\n----&gt; 9 run_training(args)\n&lt;ipython-input-5-091daf2638a1&gt; in run_training(input)\n     55     csv_logger = pl.loggers.CSVLogger(save_dir=f'{args.modeldir}\/csv_logs')\n     56     loggers = [logger, csv_logger]\n---&gt; 57     dm = OntologyTaggerDataModule.from_argparse_args(args)\n     58     if args.model_uri:\n     59         local_model_uri = os.environ.get('SM_CHANNEL_MODEL', '.')\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pytorch_lightning\/core\/datamodule.py in from_argparse_args(cls, args, **kwargs)\n    324         datamodule_kwargs.update(**kwargs)\n    325 \n--&gt; 326         return cls(**datamodule_kwargs)\n    327 \n    328     @classmethod\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pytorch_lightning\/core\/datamodule.py in __call__(cls, *args, **kwargs)\n     47 \n     48         # Get instance of LightningDataModule by mocking its __init__ via __call__\n---&gt; 49         obj = type.__call__(cls, *args, **kwargs)\n     50 \n     51         return obj\n&lt;ipython-input-3-66ee2be72e78&gt; in __init__(self, traindir, train_file, validate_file, model_name, labels, batch_size)\n     30         print('tokenizer', tokenizer)\n     31         print('labels_file', labels_file)\n---&gt; 32         label_mapper = LabelMapper(labels_file)\n     33         self.batch_size = batch_size\n     34         self.num_classes = label_mapper.num_classes\n&lt;ipython-input-3-66ee2be72e78&gt; in __init__(self, classes_file)\n    102 \n    103     def __init__(self, classes_file):\n--&gt; 104         self._raw_labels = pd.read_csv(classes_file, header=None, sep='\\t')\n    105 \n    106         self._map = []\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read_csv(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\n    686     )\n    687 \n--&gt; 688     return _read(filepath_or_buffer, kwds)\n    689 \n    690 \n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in _read(filepath_or_buffer, kwds)\n    458 \n    459     try:\n--&gt; 460         data = parser.read(nrows)\n    461     finally:\n    462         parser.close()\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   1196     def read(self, nrows=None):\n   1197         nrows = _validate_integer(&quot;nrows&quot;, nrows)\n-&gt; 1198         ret = self._engine.read(nrows)\n   1199 \n   1200         # May alter columns \/ col_dict\n~\/anaconda3\/envs\/pytorch_latest_p36\/lib\/python3.6\/site-packages\/pandas\/io\/parsers.py in read(self, nrows)\n   2155     def read(self, nrows=None):\n   2156         try:\n-&gt; 2157             data = self._reader.read(nrows)\n   2158         except StopIteration:\n   2159             if self._first_chunk:\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader.read()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._read_low_memory()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._read_rows()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.TextReader._tokenize_rows()\npandas\/_libs\/parsers.pyx in pandas._libs.parsers.raise_parser_error()\nParserError: Error tokenizing data. C error: Expected 2 fields in line 50, saw 9\n<\/code><\/pre>\n<hr \/>\n<p><code>classes.txt<\/code> (tab-separated) Before runtime<\/p>\n<pre><code>Activity    Event\nActor   Person\nAgent   Person\nAlbum   Product\nAnimal  Object\nArchitecturalStructure  Location\nArtist  Person\nAthlete Person\nAutomobileEngine    Product\nAward   Object\nBiomolecule Object\nBird    Object\nBodyOfWater Location\nBuilding    Location\nChemicalSubstance   Object\nCompany Organisation\nCompetition Event\nDevice  Product\nDisease Object\nDistrict    Location\nEukaryote   Object\nEvent   Event\nFilm    Object\nFood    Object\nLanguage    Object\nLocation    Location\nMeanOfTransportation    Product\nMotorsportSeason    Event\nMunicipality    Location\nMusicalWork Product\nOrganisation    Organisation\nPainter Person\nPeriodicalLiterature    Product\nPerson  Person\nPersonFunction  Person\nPlant   Object\nPoet    Person\nPolitician  Person\nRiver   Location\nSchool  Organisation\nSettlement  Location\nSoftware    Product\nSong    Product\nSpecies Object\nSportsSeason    Event\nStation Location\nTown    Location\nVillage Location\nWriter  Person\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\nOrganisation    University\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1630938895957,
        "Question_favorite_count":null,
        "Question_last_edit_time":1631000955160,
        "Question_score":0.0,
        "Question_view_count":87.0,
        "Answer_body":"<p>Problem Found:<\/p>\n<p>So no fault of my own, I keep ensuring these fields are on their own lines in <code>classes.txt<\/code> and <code>Ctrl+S<\/code>. Then when I reopen the file, <strong>after runtime<\/strong>, it'll have fields be on the same line again.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/UbRTm.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/UbRTm.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To fix this, on line <code>classes.write(uni_label)<\/code>.<\/p>\n<p>I replaced it with <code>classes.write('\\n'+uni_label)<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1631001042847,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69076270",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1631000840376,
        "Question_original_content":"txt alter save lead csv reader see field run jupyterlab kernel conda amazonei mxnet number field saw increment run error parsererror error token data error expect field line saw code invoc error appear run cell ran train open train textcorrupt csv val open val csv class open class txt uni label organis tunivers pad rang len uni pad record join uni pad record uni label record random random val write record train write record class write uni label class close val close train close traceback parsererror traceback recent gpu split run train arg run train input csv logger logger csvlogger save dir arg modeldir csv log logger logger csv logger ontologytaggerdatamodul argpars arg arg arg model uri local model uri environ channel model anaconda env pytorch latest lib python site packag pytorch lightn core datamodul argpars arg cl arg kwarg datamodul kwarg updat kwarg return cl datamodul kwarg classmethod anaconda env pytorch latest lib python site packag pytorch lightn core datamodul cl arg kwarg instanc lightningdatamodul mock init obj type cl arg kwarg return obj init self traindir train file valid file model label batch size print token token print label file label file label mapper labelmapp label file self batch size batch size self num class label mapper num class init self class file def init self class file self raw label read csv class file header sep self map anaconda env pytorch latest lib python site packag panda parser read csv filepath buffer sep delimit header name index col usecol squeez prefix mangl dupe col dtype engin convert true valu fals valu skipinitialspac skiprow skipfoot nrow valu default filter verbos skip blank line pars date infer datetim format date col date parser dayfirst cach date iter chunksiz compress thousand decim linetermin quotechar quot doublequot escapechar comment encod dialect error bad line warn bad line delim whitespac low memori memori map float precis return read filepath buffer kwd anaconda env pytorch latest lib python site packag panda parser read filepath buffer kwd try data parser read nrow final parser close anaconda env pytorch latest lib python site packag panda parser read self nrow def read self nrow nrow valid integ nrow nrow ret self engin read nrow alter column col dict anaconda env pytorch latest lib python site packag panda parser read self nrow def read self nrow try data self reader read nrow stopiter self chunk panda lib parser pyx panda lib parser textread read panda lib parser pyx panda lib parser textread read low memori panda lib parser pyx panda lib parser textread read row panda lib parser pyx panda lib parser textread token row panda lib parser pyx panda lib parser rais parser error parsererror error token data error expect field line saw class txt tab separ runtim activ event actor person agent person album product anim object architecturalstructur locat artist person athlet person automobileengin product award object biomolecul object bird object bodyofwat locat build locat chemicalsubst object compani organis competit event devic product diseas object district locat eukaryot object event event film object food object languag object locat locat meanoftransport product motorsportseason event municip locat musicalwork product organis organis painter person periodicalliteratur product person person personfunct person plant object poet person politician person river locat school organis settlement locat softwar product song product speci object sportsseason event station locat town locat villag locat writer person organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers organis univers",
        "Question_preprocessed_content":"txt alter save lead csv reader see field run kernel number field increment run error code invoc traceback runtim",
        "Answer_original_content":"problem fault ensur field line class txt ctrl reopen file runtim field line fix line class write uni label replac class write uni label",
        "Answer_preprocessed_content":"problem fault ensur field line reopen file runtim field line fix line replac",
        "Question_gpt_summary_original":"the user encountered a challenge where a .txt file was altered after saving, leading to the csv reader seeing too many fields and resulting in an error.",
        "Question_gpt_summary":"user encount challeng txt file alter save lead csv reader see field result error",
        "Answer_gpt_summary_original":"the solution to the challenge of a .txt file being altered after saving and causing an error in the csv reader is to add a new line character before writing each label in the file. this can be done by replacing \"classes.write(uni_label)\" with \"classes.write('\\n'+uni_label)\" in the code.",
        "Answer_gpt_summary":"solut challeng txt file alter save caus error csv reader add new line charact write label file replac class write uni label class write uni label code"
    },
    {
        "Question_id":56046428.0,
        "Question_title":"What is \"[0]#011train-merror:0.17074#011validation-merror:0.1664\" error when running xgb_model.fit() in AWS Sagemaker?",
        "Question_body":"<p>I'm running through the official sagemaker tutorial <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>And although training completes, I'm getting errors like below periodically during training, <code>xgb_model.fit(inputs=data_channels,  logs=True)<\/code>.<\/p>\n\n<p>I have no experience with xgboost or sagemaker at this point.<\/p>\n\n<pre><code>[0]#011train-merror:0.17074#011validation-merror:0.1664\n<\/code><\/pre>\n\n<p><strong>Full logs:<\/strong><\/p>\n\n<pre><code>2019-05-08 17:04:32 Starting - Starting the training job...\n2019-05-08 17:04:33 Starting - Launching requested ML instances.........\n2019-05-08 17:06:10 Starting - Preparing the instances for training......\n2019-05-08 17:07:06 Downloading - Downloading input data...\n2019-05-08 17:07:50 Training - Training image download completed. Training in progress.\nArguments: train\n[2019-05-08:17:07:51:INFO] Running standalone xgboost training.\n[2019-05-08:17:07:51:INFO] File size need to be processed in the node: 1122.95mb. Available memory size in the node: 152390.7mb\n[2019-05-08:17:07:51:INFO] Determined delimiter of CSV input is ','\n[17:07:51] S3DistributionType set as FullyReplicated\n[17:07:55] 50000x784 matrix with 39200000 entries loaded from \/opt\/ml\/input\/data\/train?format=csv&amp;label_column=0&amp;delimiter=,\n[2019-05-08:17:07:55:INFO] Determined delimiter of CSV input is ','\n[17:07:55] S3DistributionType set as FullyReplicated\n[17:07:56] 10000x784 matrix with 7840000 entries loaded from \/opt\/ml\/input\/data\/validation?format=csv&amp;label_column=0&amp;delimiter=,\n[17:07:56] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[0]#011train-merror:0.17074#011validation-merror:0.1664\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 14 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:57] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 62 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 12 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 12 pruned nodes, max_depth=5\n[1]#011train-merror:0.12624#011validation-merror:0.1273\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 10 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 20 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:58] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\n[2]#011train-merror:0.11272#011validation-merror:0.1143\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 20 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 4 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:07:59] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 12 pruned nodes, max_depth=5\n[3]#011train-merror:0.10072#011validation-merror:0.1052\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 34 extra nodes, 22 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 8 pruned nodes, max_depth=5\n[4]#011train-merror:0.09216#011validation-merror:0.097\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 30 extra nodes, 22 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:00] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 8 pruned nodes, max_depth=5\n[5]#011train-merror:0.08544#011validation-merror:0.0904\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 0 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 2 pruned nodes, max_depth=5\n\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:01] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 36 extra nodes, 16 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 42 extra nodes, 12 pruned nodes, max_depth=5\n[6]#011train-merror:0.08064#011validation-merror:0.0864\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 40 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 4 pruned nodes, max_depth=5\n[7]#011train-merror:0.0769#011validation-merror:0.0821\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 46 extra nodes, 16 pruned nodes, max_depth=5\n[17:08:02] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 20 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 44 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 54 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 58 extra nodes, 4 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 8 pruned nodes, max_depth=5\n[8]#011train-merror:0.0731#011validation-merror:0.0809\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 12 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 32 extra nodes, 24 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 56 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 10 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:03] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 48 extra nodes, 8 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 52 extra nodes, 6 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 38 extra nodes, 14 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 60 extra nodes, 2 pruned nodes, max_depth=5\n[17:08:04] src\/tree\/updater_prune.cc:74: tree pruning end, 1 roots, 50 extra nodes, 6 pruned nodes, max_depth=5\n[9]#011train-merror:0.06942#011validation-merror:0.0773\n\n2019-05-08 17:08:12 Uploading - Uploading generated training model\n2019-05-08 17:08:12 Completed - Training job completed\nBillable seconds: 66\n\n<\/code><\/pre>\n\n<p>Is this something I should be concerned about?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1557337671520,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":302.0,
        "Answer_body":"<p>No need to worry. These are not errors in your code. These are info messages that are calculating the error of the model on the training data (train-error) and on the validation data (validation-error), and these values should get smaller as the training progress. <\/p>\n\n<p>In time, these values will be more meaningful for you. You will be able to compare different algorithms and hyper-parameters based on which is the smaller error, or you will be able to see that your model is overfitting, when the error values of the training is very different from the validation error. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56046428",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1557581566207,
        "Question_original_content":"train merror valid merror error run xgb model fit run offici tutori train complet get error like period train xgb model fit input data channel log true experi xgboost point train merror valid merror log start start train job start launch request instanc start prepar instanc train download download input data train train imag download complet train progress argument train info run standalon xgboost train info file size need process node avail memori size node info determin delimit csv input sdistributiontyp set fullyrepl matrix entri load opt input data train format csv label column delimit info determin delimit csv input sdistributiontyp set fullyrepl matrix entri load opt input data valid format csv label column delimit src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth train merror valid merror src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth train merror valid merror src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth train merror valid merror src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth train merror valid merror src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth train merror valid merror src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth train merror valid merror src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth train merror valid merror src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth train merror valid merror src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth train merror valid merror src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth src tree updat prune tree prune end root extra node prune node max depth train merror valid merror upload upload gener train model complet train job complet billabl second concern",
        "Question_preprocessed_content":"error run run offici tutori train complet get error like period train experi xgboost point log concern",
        "Answer_original_content":"need worri error code info messag calcul error model train data train error valid data valid error valu smaller train progress time valu meaning abl compar differ algorithm hyper paramet base smaller error abl model overfit error valu train differ valid error",
        "Answer_preprocessed_content":"need worri error code info messag calcul error model train data valid data valu smaller train progress time valu meaning abl compar differ algorithm base smaller error abl model overfit error valu train differ valid error",
        "Question_gpt_summary_original":"the user is encountering errors during training with xgboost and is unsure if they should be concerned.",
        "Question_gpt_summary":"user encount error train xgboost unsur concern",
        "Answer_gpt_summary_original":"the answer suggests that the errors encountered during training with xgboost are not a cause for concern. these errors are actually info messages that calculate the error of the model on the training and validation data. as the training progresses, these values should get smaller and become more meaningful. the user can compare different algorithms and hyper-parameters based on which has the smaller error or determine if the model is overfitting by comparing the training error to the validation error.",
        "Answer_gpt_summary":"answer suggest error encount train xgboost caus concern error actual info messag calcul error model train valid data train progress valu smaller meaning user compar differ algorithm hyper paramet base smaller error determin model overfit compar train error valid error"
    },
    {
        "Question_id":null,
        "Question_title":"Span filtering prodigy datasets using W&B Tables",
        "Question_body":"<p>Hi! I am currently testing the new prodigy integration to visualize NER datasets and it works great, thank you! But I have some problems.<br>\nI would like to filter  <code>row[\"spans\"][\"label\"] <\/code> by the special entities and I have problems here. My use case:<\/p>\n<ul>\n<li>Which texts contain only people (PERSON)? Which organisations (ORG) or locations (LOCATION)? How many people do I find in total?<\/li>\n<li>I would like to create new rows\/columns to visualize the results.<\/li>\n<\/ul>\n<p>My problem: I can\u2019t manage to filter according to the entities. What am I doing wrong?<br>\nYour example here is also ok for the test:<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/kshen\/prodigy\/reports\/Visualizing-Prodigy-Datasets-Using-W-B-Tables--Vmlldzo5NDE2MTc\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/7a7a7077833cb4ec4be6e63ad7c2db322d3e15a6.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/kshen\/prodigy\/reports\/Visualizing-Prodigy-Datasets-Using-W-B-Tables--Vmlldzo5NDE2MTc\" target=\"_blank\" rel=\"noopener\" title=\"02:05PM - 17 August 2021\">W&amp;B \u2013 17 Aug 21<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/6f5cc42f5be5838eaa024b6166848b71312afb4e.png\" class=\"thumbnail onebox-avatar\" width=\"300\" height=\"300\">\n\n<h3><a href=\"https:\/\/wandb.ai\/kshen\/prodigy\/reports\/Visualizing-Prodigy-Datasets-Using-W-B-Tables--Vmlldzo5NDE2MTc\" target=\"_blank\" rel=\"noopener\">Visualizing Prodigy Datasets Using W&amp;B Tables<\/a><\/h3>\n\n  <p>Use the W&amp;B\/Prodigy integration to upload your Prodigy annotated datasets to W&amp;B for easier visualization. Made by Kevin Shen using Weights &amp; Biases<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0.png\" data-download-href=\"\/uploads\/short-url\/quowLbwH5KL7ZM97JjbPxcjnfNe.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_517x186.png\" alt=\"image\" data-base62-sha1=\"quowLbwH5KL7ZM97JjbPxcjnfNe\" width=\"517\" height=\"186\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_517x186.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_775x279.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_1034x372.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/b9a81ed4d714e6b33f97a5dcdc6f3ca749dfe8e0_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1762\u00d7637 64 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Thank you<br>\nAlfred<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1631795821780,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":290.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/span-filtering-prodigy-datasets-using-w-b-tables\/646",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-18T20:59:51.751Z",
                "Answer_body":"<p>Glad you\u2019re enjoying the prodigy integration!<\/p>\n<p>I\u2019ll try get an answer for this for you ASAP.<\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T10:20:07.557Z",
                "Answer_body":"<p>I don\u2019t know what I did wrong last time, but it works perfectly. Thanks!<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/65110a2cb18407338e0a0ea30428977c2e81e590.png\" alt=\"image\" data-base62-sha1=\"eq4JnNYDJ8fJExgVYAeDukhCpjO\" width=\"447\" height=\"320\"><\/p>",
                "Answer_score":66.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-24T11:30:29.718Z",
                "Answer_body":"<p>Nice! Glad you got it working <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"span filter prodigi dataset tabl current test new prodigi integr visual ner dataset work great thank problem like filter row span label special entiti problem us case text contain peopl person organis org locat locat peopl total like creat new row column visual result problem manag filter accord entiti wrong exampl test aug visual prodigi dataset tabl us prodigi integr upload prodigi annot dataset easier visual kevin shen imag thank alfr",
        "Question_preprocessed_content":"span filter prodigi dataset tabl current test new prodigi integr visual ner dataset work great thank problem like filter special entiti problem us case text contain peopl organis locat peopl total like creat new visual result problem manag filter accord entiti wrong exampl test aug visual prodigi dataset tabl us integr upload prodigi annot dataset easier visual kevin shen imag thank alfr",
        "Answer_original_content":"glad your enjoi prodigi integr ill try answer asap dont know wrong time work perfectli thank nice glad got work",
        "Answer_preprocessed_content":"glad your enjoi prodigi integr ill try answer asap dont know wrong time work perfectli thank nice glad got work",
        "Question_gpt_summary_original":"the user is encountering challenges with filtering prodigy datasets using w&b tables to visualize ner datasets and create new rows\/columns to filter according to entities.",
        "Question_gpt_summary":"user encount challeng filter prodigi dataset tabl visual ner dataset creat new row column filter accord entiti",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer to the question. the responder is acknowledging the user's positive feedback and promising to provide an answer to the question as soon as possible.",
        "Answer_gpt_summary":"solut provid answer question respond acknowledg user posit feedback promis provid answer question soon possibl"
    },
    {
        "Question_id":null,
        "Question_title":"Customvision run trained tensorflow model in Python: Placeholder:0 refers to a non existing tensor in image classification",
        "Question_body":"Hi all,\n\nI have trained an image classifier with the customvision service, which worked as charm. Now I would like to run the model inference locally with a python script on my PC. Therefore I have been following the tutorial on https:\/\/docs.microsoft.com\/en-us\/azure\/cognitive-services\/custom-vision-service\/export-model-python\n\nI am having troubles with sess.graph.get_tensor_by_name('Placeholder:0').shape.as_list()\n\nCould you please provide some information on the system requirements and the python package versions? An openCV 4.5.1 C++ code snippet on how to consume the downloaded model would be also great if possible.\n\nI am using Python 3.8.5\n\nThank you",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1621939072900,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Thank you, will do.\n\nI have solved the issue with using C++ openCV instead and WinML also helps with rapid prototyping.\n\nThis was a particularly good example I have found:\n\nhttps:\/\/github.com\/Azure-Samples\/cognitive-services-onnx-customvision-sample\n\nWould be great to have more of those.\n\nBest.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/408585\/customvision-run-trained-tensorflow-model-in-pytho.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-05-26T07:34:21.963Z",
                "Answer_score":0,
                "Answer_body":"Thank you, will do.\n\nI have solved the issue with using C++ openCV instead and WinML also helps with rapid prototyping.\n\nThis was a particularly good example I have found:\n\nhttps:\/\/github.com\/Azure-Samples\/cognitive-services-onnx-customvision-sample\n\nWould be great to have more of those.\n\nBest.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1622014461963,
        "Question_original_content":"customvis run train tensorflow model python placehold refer non exist tensor imag classif train imag classifi customvis servic work charm like run model infer local python script follow tutori http doc microsoft com azur cognit servic custom vision servic export model python have troubl sess graph tensor placehold shape list provid inform requir python packag version opencv code snippet consum download model great possibl python thank",
        "Question_preprocessed_content":"customvis run train tensorflow model python placehold refer non exist tensor imag classif train imag classifi customvis servic work charm like run model infer local python script follow tutori have troubl provid inform requir python packag version opencv code snippet consum download model great possibl python thank",
        "Answer_original_content":"thank solv issu opencv instead winml help rapid prototyp particularli good exampl http github com azur sampl cognit servic onnx customvis sampl great best",
        "Answer_preprocessed_content":"thank solv issu opencv instead winml help rapid prototyp particularli good exampl great best",
        "Question_gpt_summary_original":"the user is encountering challenges running an image classifier trained with the customvision service locally with a python script, specifically with the system requirements and the python package versions.",
        "Question_gpt_summary":"user encount challeng run imag classifi train customvis servic local python script specif requir python packag version",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer are: using c++ opencv instead of python, using winml for rapid prototyping, and referring to the provided github link for a good example.",
        "Answer_gpt_summary":"possibl solut mention answer opencv instead python winml rapid prototyp refer provid github link good exampl"
    },
    {
        "Question_id":null,
        "Question_title":"Custom Tooltip",
        "Question_body":"<p>Hello!<\/p>\n<p>I am trying to plot a ROC curve and is working nicely! Now I want to modify the tooltip to add the Threshold value. I think that can be done with Weave and the Table of Thresholds, False and True Positive Rates but I don\u2019t have too much knowledge about Weave.<\/p>\n<p>Example run with Plot and Table: <a href=\"https:\/\/wandb.ai\/marioparreno\/personal-test\/runs\/n0nlj2l6?workspace=user-marioparreno\" class=\"inline-onebox\">Weights &amp; Biases<\/a><\/p>\n<p>Weave expression here (at Tooltip field)?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/205eb72f31cdc9af99a39aee3af3e0072dcc9005.png\" data-download-href=\"\/uploads\/short-url\/4CmcMGALP0orfkQwaBtzXnvcRlH.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/2\/205eb72f31cdc9af99a39aee3af3e0072dcc9005_2_353x500.png\" alt=\"image\" data-base62-sha1=\"4CmcMGALP0orfkQwaBtzXnvcRlH\" width=\"353\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/2\/205eb72f31cdc9af99a39aee3af3e0072dcc9005_2_353x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/205eb72f31cdc9af99a39aee3af3e0072dcc9005.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/2\/205eb72f31cdc9af99a39aee3af3e0072dcc9005.png 2x\" data-dominant-color=\"F9F9F9\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">387\u00d7547 23.1 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1674664389701,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":61.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/custom-tooltip\/3750",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-25T17:34:44.042Z",
                "Answer_body":"<p>The code I used to generate the tables:<\/p>\n<pre><code class=\"lang-python\">def roc_curve_multilabel(\n    y_true=None, y_probas=None, labels=None, title=None\n):\n    y_true = np.array(y_true)\n    y_probas = np.array(y_probas)\n    \n    classes = np.arange(0, y_true.shape[1])\n    \n    fpr, tpr, thr = dict(), dict(), dict()\n    \n    for c in classes:\n        if labels is not None and (\n            isinstance(classes[c], int) or isinstance(classes[0], np.integer)\n        ):\n            class_label = labels[classes[c]]\n        else:\n            class_label = classes[c]\n        fpr[class_label], tpr[class_label], thr[class_label] = sklearn_metrics.roc_curve(\n            y_true[..., c], y_probas[..., c]\n        )\n    \n    df = pd.DataFrame(\n        {\n            \"class\": np.hstack([[k] * len(v) for k, v in fpr.items()]),\n            \"fpr\": np.hstack(list(fpr.values())),\n            \"tpr\": np.hstack(list(tpr.values())),\n            \"thr\": np.hstack(list(thr.values())),\n        }\n    )\n    \n    df = df.round(3)\n\n    if len(df) &gt; wandb.Table.MAX_ROWS:\n        wandb.termwarn(\n            \"wandb uses only %d data points to create the plots.\" % wandb.Table.MAX_ROWS\n        )\n        # different sampling could be applied, possibly to ensure endpoints are kept\n        df = sklearn_utils.resample(\n            df,\n            replace=False,\n            n_samples=wandb.Table.MAX_ROWS,\n            random_state=42,\n            stratify=df[\"class\"],\n        ).sort_values([\"fpr\", \"tpr\", \"class\"])\n\n    table = wandb.Table(dataframe=df)\n    title = title or \"ROC\"\n    return wandb.plot_table(\n        \"wandb\/area-under-curve\/v0\",\n        table,\n        {\"x\": \"fpr\", \"y\": \"tpr\", \"class\": \"class\"},\n        {\n            \"title\": title,\n            \"x-axis-title\": \"False positive rate\",\n            \"y-axis-title\": \"True positive rate\",\n        },\n    )\n<\/code><\/pre>",
                "Answer_score":46.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-27T22:48:54.037Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/marioparreno\">@marioparreno<\/a>, it looks like this is a bug affecting just line plots in Weave. The tooltip expression should be <code>row[\"thr\"]<\/code> but this isn\u2019t currently working with line plots. If you change the mark to \u201cpoint\u201d you will get the expected behavior. I\u2019ve reported this to our engineering team so we can get a fix on this.<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/b\/bc0b60e829efe3943c642c37d57d61e2ccc25e81.png\" alt=\"Screen Shot 2023-01-27 at 3.47.59 PM\" data-base62-sha1=\"qPw8R9obwJu3b5epYhIx9aWVG25\" width=\"280\" height=\"139\"><\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/f\/f75a22414abe87b79e1b925246cb879eef97b7cf.png\" alt=\"Screen Shot 2023-01-27 at 3.47.48 PM\" data-base62-sha1=\"zib2QO2tasOxusEECbtUCL99bQz\" width=\"462\" height=\"102\"><\/p>",
                "Answer_score":11.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-28T08:22:55.955Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/nathank\">@nathank<\/a> , thanks! In the meantime could you help me with something related?<\/p>\n<p>At the tooltip I would like to put more info, as the fpr, tpr and class. I can add expressions simply with \u2018+\u2019, but I want to add new lines. I tried \u2018\\n\u2019 and \u2018<br>\u2019 but not works. Examples:<\/p>\n<pre><code class=\"lang-auto\">row[\"fpr\"].toString.prepend(\"FPR: \") + \"\\n\" + row[\"thr\"].toString.prepend(\"Threshold: \")\n<\/code><\/pre>\n<p>I understand the \u2018<br>\u2019 is not treated as HTML but with \u2018\\n\u2019 something strange occurs too, the tooltip is just \u2018-\u2019.<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/e\/e7c0a2bea9d2acac4e72a60c38346208007b19a9.png\" alt=\"err\" data-base62-sha1=\"x4b0YKAyprj2ZKOmUESfpx58Y09\" width=\"250\" height=\"170\"><\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"custom tooltip hello try plot roc curv work nice want modifi tooltip add threshold valu think weav tabl threshold fals true posit rate dont knowledg weav exampl run plot tabl weav express tooltip field imag",
        "Question_preprocessed_content":"custom tooltip hello try plot roc curv work nice want modifi tooltip add threshold valu think weav tabl threshold fals true posit rate dont knowledg weav exampl run plot tabl weav express imag",
        "Answer_original_content":"code gener tabl def roc curv multilabel true proba label titl true arrai true proba arrai proba class arang true shape fpr tpr thr dict dict dict class label isinst class int isinst class integ class label label class class label class fpr class label tpr class label thr class label sklearn metric roc curv true proba datafram class hstack len fpr item fpr hstack list fpr valu tpr hstack list tpr valu thr hstack list thr valu round len tabl max row termwarn us data point creat plot tabl max row differ sampl appli possibl ensur endpoint kept sklearn util resampl replac fals sampl tabl max row random state stratifi class sort valu fpr tpr class tabl tabl datafram titl titl roc return plot tabl area curv tabl fpr tpr class class titl titl axi titl fals posit rate axi titl true posit rate marioparreno look like bug affect line plot weav tooltip express row thr isnt current work line plot chang mark point expect behavior iv report engin team fix nathank thank meantim help relat tooltip like info fpr tpr class add express simpli want add new line tri work exampl row fpr tostr prepend fpr row thr tostr prepend threshold understand treat html strang occur tooltip",
        "Answer_preprocessed_content":"code gener tabl look like bug affect line plot weav tooltip express isnt current work line plot chang mark point expect behavior iv report engin team fix thank meantim help relat tooltip like info fpr tpr class add express simpli want add new line tri work exampl understand treat html strang occur tooltip",
        "Question_gpt_summary_original":"the user is attempting to modify the tooltip of a roc curve plot to include the threshold value, but is having difficulty due to a lack of knowledge about weave.",
        "Question_gpt_summary":"user attempt modifi tooltip roc curv plot includ threshold valu have difficulti lack knowledg weav",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the user is reporting a bug in weave and asking for help in modifying the tooltip of a roc curve plot to include more information. the user has tried adding expressions with +, but is having difficulty adding new lines to the tooltip.",
        "Answer_gpt_summary":"solut provid answer user report bug weav ask help modifi tooltip roc curv plot includ inform user tri ad express have difficulti ad new line tooltip"
    },
    {
        "Question_id":73471486.0,
        "Question_title":"How to prevent storing data in Jupyter project tree when writing data from Sagemaker to S3",
        "Question_body":"<p>I am new to AWS Sagemaker and I wrote data to my S3 bucket.\nBut these datasets also appear in the working tree of my jupyter instance.<\/p>\n<p>How can I move data directly to S3 without saving it &quot;locally&quot;?<\/p>\n<p>My code:<\/p>\n<pre><code>import os\nimport pandas as pd\n\nimport sagemaker, boto3\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\n\n# please provide your own bucket and folder path of your bucket here\nbucket = &quot;test-bucket2342343&quot;\nsm_sess = sagemaker.Session(default_bucket=bucket)\nfile_path = &quot;Use Cases\/Sagemaker Demo\/xgboost&quot;\n\n# data \ndf_train = pd.DataFrame({'X':[0,100,200,400,450,  550,600,800,1600],\n                         'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\ndf_test = pd.DataFrame({'X':[10,90,240,459,120,  650,700,1800,1300],\n                        'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\n# move to S3 \ndf_train[['y','X']].to_csv('train.csv', header=False, index=False)\n\ndf_val = df_test.copy()\ndf_val[['y','X']].to_csv('val.csv', header=False, index=False)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;train.csv&quot;)).upload_file(&quot;train.csv&quot;)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;val.csv&quot;)).upload_file(&quot;val.csv&quot;)\n\n<\/code><\/pre>\n<p>It successfully appears in my S3 bucket.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But it also appears here:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661336683943,
        "Question_favorite_count":null,
        "Question_last_edit_time":1661337201248,
        "Question_score":0.0,
        "Question_view_count":17.0,
        "Answer_body":"<p>with Pandas you can save to S3 directly (<a href=\"https:\/\/stackoverflow.com\/a\/56275519\/121956\">relevant answer<\/a>). For example:<\/p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame( [ [1, 1, 1], [2, 2, 2] ], columns=['a', 'b', 'c'])\ndf.to_csv('s3:\/\/test-bucket2342343\/\/tmp.csv', index=False)\n<\/code><\/pre>\n<p>Or, use what you currently do and delete the local files:<\/p>\n<pre><code>import os\nos.remove('train.csv')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73471486",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1661447960212,
        "Question_original_content":"prevent store data jupyt project tree write data new wrote data bucket dataset appear work tree jupyt instanc data directli save local code import import panda import boto import execut role input import traininginput serial import csvserial provid bucket folder path bucket bucket test bucket sess session default bucket bucket file path us case demo xgboost data train datafram test datafram train csv train csv header fals index fals val test copi val csv val csv header fals index fals boto session resourc bucket bucket object path join file path train csv upload file train csv boto session resourc bucket bucket object path join file path val csv upload file val csv successfulli appear bucket appear",
        "Question_preprocessed_content":"prevent store data jupyt project tree write data new wrote data bucket dataset appear work tree jupyt instanc data directli save local code successfulli appear bucket appear",
        "Answer_original_content":"panda save directli relev answer exampl import panda datafram column csv test bucket tmp csv index fals us current delet local file import remov train csv",
        "Answer_preprocessed_content":"panda save directli exampl us current delet local file",
        "Question_gpt_summary_original":"the user is encountering a challenge of preventing data from being stored in the working tree of their jupyter instance when writing data to an s3 bucket.",
        "Question_gpt_summary":"user encount challeng prevent data store work tree jupyt instanc write data bucket",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions to prevent data from being stored in the working tree of a jupyter instance when writing data to an s3 bucket. the first solution is to use pandas to save data directly to s3. the second solution is to delete the local files after writing them to s3 using the current method.",
        "Answer_gpt_summary":"answer suggest possibl solut prevent data store work tree jupyt instanc write data bucket solut us panda save data directli second solut delet local file write current method"
    },
    {
        "Question_id":72096297.0,
        "Question_title":"Hyperparameter tuning job In Sagemaker with cross valdiation",
        "Question_body":"<p>I managed to get something <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/automatic-model-tuning-ex-tuning-job.html\" rel=\"nofollow noreferrer\">along those lines<\/a> to work. This is great but to be more on the save side (i.e. not rely too much on the train validation split) one should really use cross validation. I am curious, if this can also be achieved via Sagemaker hyperparameter tuning jobs? I googled extensively ...<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1651564321897,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":70.0,
        "Answer_body":"<p>It is not possible through HPO.<\/p>\n<p>You need to add additional step in your workflow to achieve cross-validation.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72096297",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651629955790,
        "Question_original_content":"hyperparamet tune job cross valdiat manag line work great save reli train valid split us cross valid curiou achiev hyperparamet tune job googl extens",
        "Question_preprocessed_content":"hyperparamet tune job cross valdiat manag line work great save us cross valid curiou achiev hyperparamet tune job googl extens",
        "Answer_original_content":"possibl hpo need add addit step workflow achiev cross valid",
        "Answer_preprocessed_content":"possibl hpo need add addit step workflow achiev",
        "Question_gpt_summary_original":"the user is attempting to use hyperparameter tuning jobs to achieve cross validation.",
        "Question_gpt_summary":"user attempt us hyperparamet tune job achiev cross valid",
        "Answer_gpt_summary_original":"possible solutions: \n- add an additional step in the workflow to achieve cross-validation. \n- hyperparameter tuning jobs alone cannot achieve cross-validation.",
        "Answer_gpt_summary":"possibl solut add addit step workflow achiev cross valid hyperparamet tune job achiev cross valid"
    },
    {
        "Question_id":null,
        "Question_title":"Deletion of storage",
        "Question_body":"<p>Hello.<br>\nI have created a toy project with google drive storage. I created 3 versions of the dataset and then deleted the drive folder to create a new storage folder. Then create 3 more dataset versions. When I want to download the first version it throws me an error:<\/p>\n<blockquote>\n<p>Some of the cache files do not exist neither locally nor on remote. Missing cache files:<br>\nname: None, md5: 176fc4479b1573160fd05222c20976d2.dir<\/p>\n<\/blockquote>\n<p>Is there a way to recover or rebuild the versions before the storage change?<\/p>\n<p>Thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1666479337651,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/deletion-of-storage\/1370",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-27T11:51:39.358Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/aaron\">@aaron<\/a>!<br>\nIf you want to retreive the first version, the one that you deleted you would need to try to recover data on drive itself - <a href=\"https:\/\/support.google.com\/drive\/answer\/2375102?hl=en&amp;co=GENIE.Platform%3DDesktop\" class=\"inline-onebox\">Delete and restore files in Google Drive - Computer - Google Drive Help<\/a><br>\nThey might be in Trash folder, but that holds only for 30 days after \u201cdeletion\u201d. You would need to retrieve it  and merge with the \u201cnew\u201d folder.<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"delet storag hello creat toi project googl drive storag creat version dataset delet drive folder creat new storag folder creat dataset version want download version throw error cach file exist local remot miss cach file fcbfdcd dir wai recov rebuild version storag chang thank advanc",
        "Question_preprocessed_content":"delet storag hello creat toi project googl drive storag creat version dataset delet drive folder creat new storag folder creat dataset version want download version throw error cach file exist local remot miss cach file wai recov rebuild version storag chang thank advanc",
        "Answer_original_content":"hello aaron want retreiv version delet need try recov data drive delet restor file googl drive googl drive help trash folder hold dai delet need retriev merg new folder",
        "Answer_preprocessed_content":"hello want retreiv version delet need try recov data drive delet restor file googl drive googl drive help trash folder hold dai delet need retriev merg new folder",
        "Question_gpt_summary_original":"the user encountered a challenge when attempting to download a version of their dataset from google drive storage after deleting the folder and creating a new storage folder, as they received an error indicating that some of the cache files were missing.",
        "Question_gpt_summary":"user encount challeng attempt download version dataset googl drive storag delet folder creat new storag folder receiv error indic cach file miss",
        "Answer_gpt_summary_original":"the possible solution to retrieve the deleted version of the dataset is to try to recover data on google drive itself by deleting and restoring files. the deleted version might be in the trash folder, but it only holds for 30 days after deletion. once retrieved, the deleted version needs to be merged with the new folder.",
        "Answer_gpt_summary":"possibl solut retriev delet version dataset try recov data googl drive delet restor file delet version trash folder hold dai delet retriev delet version need merg new folder"
    },
    {
        "Question_id":null,
        "Question_title":"Failure to upload data after run",
        "Question_body":"<p>Hi,<\/p>\n<p>I have several runs that upon termination are getting stuck on the following error (unable to copy as text due to the rotating \u201c|\u201d sign):<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/ae27b992b80ee351ee93649b5d64e097b02af05a.png\" data-download-href=\"\/uploads\/short-url\/oQEbMyZj2EOLrZlOJ30i5nPRdlE.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae27b992b80ee351ee93649b5d64e097b02af05a_2_690x53.png\" alt=\"image\" data-base62-sha1=\"oQEbMyZj2EOLrZlOJ30i5nPRdlE\" width=\"690\" height=\"53\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/ae27b992b80ee351ee93649b5d64e097b02af05a_2_690x53.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/ae27b992b80ee351ee93649b5d64e097b02af05a.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/ae27b992b80ee351ee93649b5d64e097b02af05a.png 2x\" data-dominant-color=\"1A2015\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">843\u00d765 14 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Any ideas on how to fix without stopping my run?<\/p>\n<p>Edit: I checked the <code>debug-internal.log<\/code> and it seems the below error message pops up.<br>\nAdditional details -<br>\n(1) The server uploading the data is in a private network (.technion.ac.il)<br>\n(2) Unlike other runs that do succeed in the meantime, here I upload a video using:<\/p>\n<pre><code class=\"lang-auto\">wandb.log({\"video\": wandb.Video(all_images, fps=4, format=\"mp4\")})\n<\/code><\/pre>\n<p>Error:<\/p>\n<p>2022-12-28 13:30:28,582 ERROR   Thread-17 :3928098 [internal_api.py:upload_file():1875] upload_file exception <a href=\"https:\/\/storage.googleapis.com\/wandb-production.appspot.com\/tomjur\/\" rel=\"noopener nofollow ugc\">https:\/\/storage.googleapis.com\/wandb-production.appspot.com\/tomjur\/<\/a>[my proj]\/[current run]\/wandb-metadata.json?Expires=1672313428&amp;GoogleAccessId=wandb-production%<a href=\"http:\/\/40appspot.gserviceaccount.com\" rel=\"noopener nofollow ugc\">40appspot.gserviceaccount.com<\/a>&amp;Signature=UKkwGiiPdbchVt3D9Np7HXU91ioIwnWZa5EqapeDR0UFZbjElm2TU5XOF93P2z7%2BnvCTcAEeSkV0tVg3ln0aMTUw%2BYTVSiyBBHncYCZkJcHWS2MDgcA5v9LzjnQ3c5kJoUA5dJwMWNVLebZ8LWkUDKdBSeafEfogP3xE%2FLIDdPtHolpISNJIvEY3JDPOrTJUh9Ge%2Bw3%2BgLgvep9LbG2qEtmj%2FIeeW%2FXqi9wIpfWdDjZ6ZlkGkohYjJPdSGA6GM9RLrglaDVnwmJQXTgyLEIpD9%2FsuKJeBN3U0v6qz1aE2OBoDCK%2Fs4lDEspXy%2FgOqVS6FEHrwQ813rkabFR6dvC1tA%3D%3D: HTTPSConnectionPool(host=\u2018<a href=\"http:\/\/storage.googleapis.com\" rel=\"noopener nofollow ugc\">storage.googleapis.com<\/a>\u2019, port=443): Max retries exceeded with url: \/wandb-production.appspot.com\/tomjur\/[my proj]\/[current run]\/wandb-metadata.json?Expires=1672313428&amp;GoogleAccessId=wandb-production%<a href=\"http:\/\/40appspot.gserviceaccount.com\" rel=\"noopener nofollow ugc\">40appspot.gserviceaccount.com<\/a>&amp;Signature=UKkwGiiPdbchVt3D9Np7HXU91ioIwnWZa5EqapeDR0UFZbjElm2TU5XOF93P2z7%2BnvCTcAEeSkV0tVg3ln0aMTUw%2BYTVSiyBBHncYCZkJcHWS2MDgcA5v9LzjnQ3c5kJoUA5dJwMWNVLebZ8LWkUDKdBSeafEfogP3xE%2FLIDdPtHolpISNJIvEY3JDPOrTJUh9Ge%2Bw3%2BgLgvep9LbG2qEtmj%2FIeeW%2FXqi9wIpfWdDjZ6ZlkGkohYjJPdSGA6GM9RLrglaDVnwmJQXTgyLEIpD9%2FsuKJeBN3U0v6qz1aE2OBoDCK%2Fs4lDEspXy%2FgOqVS6FEHrwQ813rkabFR6dvC1tA%3D%3D (Caused by SSLError(CertificateError(\u201chostname \u2018<a href=\"http:\/\/storage.googleapis.com\" rel=\"noopener nofollow ugc\">storage.googleapis.com<\/a>\u2019 doesn\u2019t match either of \u2018*.technion.ac.il\u2019, \u2018technion.ac.il\u2019\u201d)))<\/p>\n<p>Thanks,<br>\nTom<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1672226208345,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":150.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/failure-to-upload-data-after-run\/3612",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-28T23:31:11.886Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tomjur\">@tomjur<\/a> , happy to help. From the error message it appears you are encountering and SSL certification error, <code>SSLError(CertificateError(\u201c....\")<\/code>, most likely attributed to your private networks firewall. Please refer to our <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\/troubleshooting#how-do-i-deal-with-network-issues\">guide here<\/a> on addressing network related issues. Do let me know if you still encounter issues after addressing the SSL error.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-03T18:44:25.426Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tomjur\">@tomjur<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"failur upload data run run termin get stuck follow error unabl copi text rotat sign imag idea fix stop run edit check debug intern log error messag pop addit detail server upload data privat network technion unlik run succe meantim upload video log video video imag fp format error error thread intern api upload file upload file except http storag googleapi com product appspot com tomjur proj current run metadata json expir googleaccessid product appspot gserviceaccount com signatur ukkwgiipdbchvtdnphxuioiwnwzaeqapedrufzbjelmtuxofpz bnvctcaeeskvtvglnamtuw bytvsiybbhncyczkjchwsmdgcavlzjnqckjouadjwmwnvlebzlwkudkdbseafefogpx fliddptholpisnjiveyjdportjuhg bglgveplbgqetmj fieew fxqiwipfwddjzzlkgkohyjjpdsgagmrlrgladvnwmjqxtgyleipd fsukjebnuvqzaeobodck fsldespxi fgoqvsfehrwqrkabfrta httpsconnectionpool host storag googleapi com port max retri exceed url product appspot com tomjur proj current run metadata json expir googleaccessid product appspot gserviceaccount com signatur ukkwgiipdbchvtdnphxuioiwnwzaeqapedrufzbjelmtuxofpz bnvctcaeeskvtvglnamtuw bytvsiybbhncyczkjchwsmdgcavlzjnqckjouadjwmwnvlebzlwkudkdbseafefogpx fliddptholpisnjiveyjdportjuhg bglgveplbgqetmj fieew fxqiwipfwddjzzlkgkohyjjpdsgagmrlrgladvnwmjqxtgyleipd fsukjebnuvqzaeobodck fsldespxi fgoqvsfehrwqrkabfrta caus sslerror certificateerror hostnam storag googleapi com doesnt match technion technion thank tom",
        "Question_preprocessed_content":"failur upload data run run termin get stuck follow error imag idea fix stop run edit check error messag pop addit detail server upload data privat network unlik run succe meantim upload video error error except port max retri exceed url thank tom",
        "Answer_original_content":"tomjur happi help error messag appear encount ssl certif error sslerror certificateerror like attribut privat network firewal refer guid address network relat issu let know encount issu address ssl error tomjur heard go close request like open convers let know",
        "Answer_preprocessed_content":"happi help error messag appear encount ssl certif error like attribut privat network firewal refer guid address network relat issu let know encount issu address ssl error heard go close request like convers let know",
        "Question_gpt_summary_original":"the user is encountering an error when attempting to upload data after a run, due to a mismatch between the hostname and the server's private network domain.",
        "Question_gpt_summary":"user encount error attempt upload data run mismatch hostnam server privat network domain",
        "Answer_gpt_summary_original":"possible solutions from the answer include checking the private network's firewall settings and addressing the ssl certification error. the guide provided in the answer can be referred to for addressing network-related issues. if the issue persists, the user can reopen the conversation with the support team.",
        "Answer_gpt_summary":"possibl solut answer includ check privat network firewal set address ssl certif error guid provid answer refer address network relat issu issu persist user reopen convers support team"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning - error during the creation Create a control script",
        "Question_body":"Hello, I am reproducing this tutorial https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world \/ Create a control script. The next observations appear in the console.\n\nI will thank you if some ideas are shared with me to face this issue.\n\nRegards",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643753509637,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@Anth0nyCamp0s I believe the error is because in the config command you are using the script parameter hello.py which is in .\/src directory but because you are already in .\/src directory on the terminal and the source_directory parameter also mentions to use .\/src as the path to the file the following error is indicated in the message.\n\n\n\n\nIf you navigate back to get-started directory in your terminal and run the script run-hello.py your experiment should be created successfully.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/718873\/azure-machine-learning-error-during-the-creation-c.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-02T08:04:16.33Z",
                "Answer_score":0,
                "Answer_body":"@Anth0nyCamp0s I believe the error is because in the config command you are using the script parameter hello.py which is in .\/src directory but because you are already in .\/src directory on the terminal and the source_directory parameter also mentions to use .\/src as the path to the file the following error is indicated in the message.\n\n\n\n\nIf you navigate back to get-started directory in your terminal and run the script run-hello.py your experiment should be created successfully.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643789056328,
        "Question_original_content":"error creation creat control script hello reproduc tutori http doc microsoft com azur machin learn tutori experi hello world creat control script observ appear consol thank idea share face issu regard",
        "Question_preprocessed_content":"error creation creat control script hello reproduc tutori creat control script observ appear consol thank idea share face issu regard",
        "Answer_original_content":"anthnycamp believ error config command script paramet hello src directori src directori termin sourc directori paramet mention us src path file follow error indic messag navig start directori termin run script run hello experi creat successfulli answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"believ error config command script paramet directori directori termin paramet mention us path file follow error indic messag navig directori termin run script experi creat successfulli answer help click upvot help commun member read thread",
        "Question_gpt_summary_original":"the user encountered an error while attempting to create a control script while reproducing a tutorial.",
        "Question_gpt_summary":"user encount error attempt creat control script reproduc tutori",
        "Answer_gpt_summary_original":"the possible solution to the error encountered while creating a control script is to navigate back to the get-started directory in the terminal and run the script run-hello.py. this should create the experiment successfully.",
        "Answer_gpt_summary":"possibl solut error encount creat control script navig start directori termin run script run hello creat experi successfulli"
    },
    {
        "Question_id":null,
        "Question_title":"Training Auto ML models from a Python script via Azure Functions is failing with MemoryError",
        "Question_body":"Hi,\n\nI'm trying to do the following:\n- I want to run an Auto ML training script in Azure Functions via a Python script.\n- I have configured a remote compute and a dataset in Azure ML that I'm using. The dataset is a 22 KB csv file.\n- When I start the experiment the script fails with a memory error complaining about the system running out of memory (see below)\n- I get the error regardless of what app service plan tier I'm using; the script isn't actually running out of memory based on the metrics\n\n 2021-02-08T11:11:09.395 [Error] Executed 'Functions.AutomatedAutoMLTrainerWorker' (Failed, Id=1857c033-8199-444f-8deb-b2de73d44741, Duration=73313ms)\n Result: FailureException: MemoryError: Engine process terminated. This is most likely due to system running out of memory. Please retry with increased memory. |session_id=c2ed81b1-66b1-4dda-82b6-056b335eaec0\n Stack:   \n File \"\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py\", line 357, in _handle__invocation_requestself.__run_sync_func, invocation_id, fi.func, args)\n File \"\/usr\/local\/lib\/python3.7\/concurrent\/futures\/thread.py\", line 57, in runresult = self.fn(*self.args, **self.kwargs)File \"\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py\", line 542, in __run_sync_funcreturn func(**params)\n File \"\/home\/site\/wwwroot\/AutomatedAutoMLTrainerWorker\/__init__.py\", line 64, in mainrun = experiment.submit(automl_config, show_output=True)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/core\/experiment.py\", line 220, in submitrun = submit_func(config, self.workspace, self.name, **kwargs)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 102, in _automl_static_submitshow_output)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 214, in _start_executionautoml_run = _default_execution(experiment, settings_obj, fit_params, False, show_output)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/train\/automl\/automlconfig.py\", line 127, in _default_executionreturn automl_estimator.fit(**fit_params)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/train\/automl\/_azureautomlclient.py\", line 218, in fittest_data\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/automl\/core\/dataset_utilities.py\", line 90, in convert_inputs_datasetreturn tuple([_convert_to_trackable_definition(dataset) for dataset in datasets])\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/automl\/core\/dataset_utilities.py\", line 90, in <listcomp>return tuple([_convert_to_trackable_definition(dataset) for dataset in datasets])\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/automl\/core\/dataset_utilities.py\", line 162, in _convert_to_trackable_definitiondefinition, trackable = _reference_dataset(dataset)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/automl\/core\/dataset_utilities.py\", line 178, in _reference_datasetreturn dataset._dataflow, False\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/_loggerfactory.py\", line 129, in wrapperreturn func(*args, **kwargs)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/abstract_dataset.py\", line 208, in _dataflowdataprep().api._datastore_helper._set_auth_type(self._registration.workspace)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py\", line 143, in _set_auth_typeget_engine_api().set_aml_auth(SetAmlAuthMessageArgument(AuthType.DERIVED, json.dumps(auth)))\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py\", line 19, in get_engine_api_engine_api = EngineAPI()\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py\", line 69, in __init__connect_to_requests_channel()\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py\", line 56, in connect_to_requests_channelself._engine_server_secret = self.sync_host_secret(self.requests_channel.host_secret)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_aml_helper.py\", line 38, in wrapperreturn send_message_func(op_code, message, cancellation_token)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py\", line 260, in sync_host_secretresponse = self._message_channel.send_message('Engine.SyncHostSecret', message_args, cancellation_token)\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py\", line 275, in send_messageraise message['error']\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py\", line 223, in process_responsesresponse = self._read_response(caller='MultiThreadMessageChannel.process_responses')\n File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py\", line 148, in _read_responseraise error\n\n\n\nMy requirements.txt for the function app looks like this:\n\n azure-functions==1.6.0\n azureml-core==1.21.0\n azureml-train-automl-client==1.21.0\n azureml-contrib-functions==1.21.0\n azure-storage-queue==12.1.5\n ruamel.yaml>=0.16.5\n PyJWT==1.7.1\n\n\n\n\nMy training script looks like this:\n\n import logging\n    \n import azure.functions as func\n import os\n import tempfile\n    \n import azureml.core\n from azureml.core import Experiment, Workspace\n from azureml.core.environment import Environment\n from azureml.core.dataset import Dataset\n from azureml.core.compute import ComputeTarget, AmlCompute\n from azureml.core.authentication import MsiAuthentication\n from azureml.train.automl import AutoMLConfig\n    \n    \n def main(msg: func.QueueMessage) -> None:\n     tenant_id = os.getenv(\"TenantId\")\n     resource_group = os.getenv(\"AzureMLResourceGroup\")\n     workspace_name = os.getenv(\"AzureMLWorkspace\")\n     compute_name = os.getenv(\"AzureMLComputeName\")\n     dataset_name = os.getenv(\"AzureMLDataSetName\")\n     dataset_label_column = os.getenv(\"AzureMLDataSetLabelColumn\")\n     experiment_name = os.getenv(\"AzureMLExperimentName\")\n    \n     msi_auth = MsiAuthentication()\n     ws = Workspace(subscription_id=tenant_id,\n                resource_group=resource_group,\n                workspace_name=workspace_name,\n                auth=msi_auth)\n    \n     compute_target = ws.compute_targets[compute_name]\n    \n     dataset = Dataset.get_by_name(workspace=ws, name=dataset_name)\n    \n     tempFilePath = tempfile.gettempdir() + '\/debug.log'\n     automl_config = AutoMLConfig(task='regression',\n                                    experiment_timeout_minutes=60,\n                                    primary_metric='normalized_root_mean_squared_error',\n                                    training_data=dataset,\n                                    compute_target=compute_target,\n                                    label_column_name=dataset_label_column,\n                                    debug_log=tempFilePath)\n    \n     experiment = Experiment(ws, experiment_name)\n    \n     run = experiment.submit(automl_config, show_output=True)\n     run.wait_for_completion()\n\n\n\n\nDoes anyone have a clue what might be going wrong? :)",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1612783690643,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/263618\/training-auto-ml-models-from-a-python-script-via-a.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-12T04:14:33.33Z",
                "Answer_score":0,
                "Answer_body":"@Joonasijl-7350 Thanks, are you able to increase the allocated memory for the container where the training is happening?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"train auto model python script azur function fail memoryerror try follow want run auto train script azur function python script configur remot comput dataset dataset csv file start experi script fail memori error complain run memori error regardless app servic plan tier script isn actual run memori base metric error execut function automatedautomltrainerwork fail deb bded durat result failureexcept memoryerror engin process termin like run memori retri increas memori session cedb dda beaec stack file azur function host worker python linux azur function worker dispatch line handl invoc requestself run sync func invoc func arg file usr local lib python concurr futur thread line runresult self self arg self kwarg file azur function host worker python linux azur function worker dispatch line run sync funcreturn func param file home site root automatedautomltrainerwork init line mainrun experi submit automl config output true file home site root python packag lib site packag core experi line submitrun submit func config self workspac self kwarg file home site root python packag lib site packag train automl automlconfig line automl static submitshow output file home site root python packag lib site packag train automl automlconfig line start executionautoml run default execut experi set obj fit param fals output file home site root python packag lib site packag train automl automlconfig line default executionreturn automl estim fit fit param file home site root python packag lib site packag train automl azureautomlcli line fittest data file home site root python packag lib site packag automl core dataset util line convert input datasetreturn tupl convert trackabl definit dataset dataset dataset file home site root python packag lib site packag automl core dataset util line return tupl convert trackabl definit dataset dataset dataset file home site root python packag lib site packag automl core dataset util line convert trackabl definitiondefinit trackabl refer dataset dataset file home site root python packag lib site packag automl core dataset util line refer datasetreturn dataset dataflow fals file home site root python packag lib site packag data loggerfactori line wrapperreturn func arg kwarg file home site root python packag lib site packag data abstract dataset line dataflowdataprep api datastor helper set auth type self registr workspac file home site root python packag lib site packag dataprep api datastor helper line set auth typeget engin api set aml auth setamlauthmessageargu authtyp deriv json dump auth file home site root python packag lib site packag dataprep api engineapi api line engin api engin api engineapi file home site root python packag lib site packag dataprep api engineapi api line init connect request channel file home site root python packag lib site packag dataprep api engineapi api line connect request channelself engin server secret self sync host secret self request channel host secret file home site root python packag lib site packag dataprep api aml helper line wrapperreturn send messag func code messag cancel token file home site root python packag lib site packag dataprep api engineapi api line sync host secretrespons self messag channel send messag engin synchostsecret messag arg cancel token file home site root python packag lib site packag dataprep api engineapi engin line send messagerais messag error file home site root python packag lib site packag dataprep api engineapi engin line process responsesrespons self read respons caller multithreadmessagechannel process respons file home site root python packag lib site packag dataprep api engineapi engin line read responserais error requir txt function app look like azur function core train automl client contrib function azur storag queue ruamel yaml pyjwt train script look like import log import azur function func import import tempfil import core core import experi workspac core environ import environ core dataset import dataset core comput import computetarget amlcomput core authent import msiauthent train automl import automlconfig def main msg func queuemessag tenant getenv tenantid resourc group getenv resourcegroup workspac getenv workspac comput getenv computenam dataset getenv datasetnam dataset label column getenv datasetlabelcolumn experi getenv experimentnam msi auth msiauthent workspac subscript tenant resourc group resourc group workspac workspac auth msi auth comput target comput target comput dataset dataset workspac dataset tempfilepath tempfil gettempdir debug log automl config automlconfig task regress experi timeout minut primari metric normal root mean squar error train data dataset comput target comput target label column dataset label column debug log tempfilepath experi experi experi run experi submit automl config output true run wait complet clue go wrong",
        "Question_preprocessed_content":"train auto model python script azur function fail memoryerror try follow want run auto train script azur function python script configur remot comput dataset dataset csv file start experi script fail memori error complain run memori error regardless app servic plan tier script isn actual run memori base metric execut result failureexcept memoryerror engin process termin like run memori retri increas memori stack file line arg file line runresult line func file line mainrun file line submitrun kwarg file line file line fals file line file line file line dataset dataset file line return dataset dataset file line trackabl file line fals file line wrapperreturn func file line file line file line engineapi file line file line file line wrapperreturn messag file line file line messag file line file line error function app look like train script look like import log import func import import tempfil import core core import experi workspac import environ import dataset import computetarget amlcomput import msiauthent import automlconfig def main msiauthent dataset tempfilepath automlconfig experi experi run clue go wrong",
        "Answer_original_content":"joonasijl thank abl increas alloc memori contain train happen",
        "Answer_preprocessed_content":"thank abl increas alloc memori contain train happen",
        "Question_gpt_summary_original":"the user is encountering a memoryerror when attempting to train an auto ml model from a python script via azure functions.",
        "Question_gpt_summary":"user encount memoryerror attempt train auto model python script azur function",
        "Answer_gpt_summary_original":"increasing the allocated memory for the container where the training is happening could be a possible solution to the memoryerror encountered when training an auto ml model from a python script via azure functions.",
        "Answer_gpt_summary":"increas alloc memori contain train happen possibl solut memoryerror encount train auto model python script azur function"
    },
    {
        "Question_id":null,
        "Question_title":"Question: distinction between Modules and Models in Designer",
        "Question_body":"In Azure ML, under designer, there are 3 categories:\n\nDatasets\n\n\nModules\n\n\nModels\n\nDatasets are pretty straightforward, but I don't understand the distinction between Modules and Models. As an ML researcher, when I think of a \"model\", I think of something like linear regression or SVM. However, those are listed under Modules -> Machine Learning Algorithms. So what exactly qualifies as a Model?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1594756919963,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi,\n\nThanks for reaching out.\n\nModel is a concept of Machine Learning itself. A machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data. Once you have trained the model, you can use it to reason over data that it hasn't seen before, and make predictions about those data. For example, let's say you want to build an application that can recognize a user's emotions based on their facial expressions. You can train a model by providing it with images of faces that are each tagged with a certain emotion, and then you can use that model in an application that can recognize any user's emotion.\n\nModules is one of the concept of Azure Machine Learning Designer. Each module represents a set of code that can run independently and perform a machine learning task, given the required inputs. A module might contain a particular algorithm, or perform a task that is important in machine learning, such as missing value replacement, or statistical analysis.\n\nLet me know if you have any question.\n\nRegards,\nYutong",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/46991\/question-distinction-between-modules-and-models-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-14T20:39:44.477Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nThanks for reaching out.\n\nModel is a concept of Machine Learning itself. A machine learning model is a file that has been trained to recognize certain types of patterns. You train a model over a set of data, providing it an algorithm that it can use to reason over and learn from those data. Once you have trained the model, you can use it to reason over data that it hasn't seen before, and make predictions about those data. For example, let's say you want to build an application that can recognize a user's emotions based on their facial expressions. You can train a model by providing it with images of faces that are each tagged with a certain emotion, and then you can use that model in an application that can recognize any user's emotion.\n\nModules is one of the concept of Azure Machine Learning Designer. Each module represents a set of code that can run independently and perform a machine learning task, given the required inputs. A module might contain a particular algorithm, or perform a task that is important in machine learning, such as missing value replacement, or statistical analysis.\n\nLet me know if you have any question.\n\nRegards,\nYutong",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":38.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1594759184476,
        "Question_original_content":"question distinct modul model design design categori dataset modul model dataset pretti straightforward understand distinct modul model research think model think like linear regress svm list modul machin learn algorithm exactli qualifi model",
        "Question_preprocessed_content":"question distinct modul model design design categori dataset modul model dataset pretti straightforward understand distinct modul model research think model think like linear regress svm list modul machin learn algorithm exactli qualifi model",
        "Answer_original_content":"thank reach model concept machin learn machin learn model file train recogn certain type pattern train model set data provid algorithm us reason learn data train model us reason data hasn seen predict data exampl let want build applic recogn user emot base facial express train model provid imag face tag certain emot us model applic recogn user emot modul concept design modul repres set code run independ perform machin learn task given requir input modul contain particular algorithm perform task import machin learn miss valu replac statist analysi let know question regard yutong",
        "Answer_preprocessed_content":"thank reach model concept machin learn machin learn model file train recogn certain type pattern train model set data provid algorithm us reason learn data train model us reason data hasn seen predict data exampl let want build applic recogn user emot base facial express train model provid imag face tag certain emot us model applic recogn user emot modul concept design modul repres set code run independ perform machin learn task given requir input modul contain particular algorithm perform task import machin learn miss valu replac statist analysi let know question regard yutong",
        "Question_gpt_summary_original":"the user is confused about the distinction between modules and models in designer, as they are used to thinking of models as linear regression or svm, which are listed under modules -> machine learning algorithms.",
        "Question_gpt_summary":"user confus distinct modul model design think model linear regress svm list modul machin learn algorithm",
        "Answer_gpt_summary_original":"the answer explains that a model is a file that has been trained to recognize certain types of patterns, and that modules are sets of code that can run independently and perform a machine learning task. the possible solution is to understand that models are a concept of machine learning itself, while modules are a concept of designer that contain a particular algorithm or perform a task important in machine learning.",
        "Answer_gpt_summary":"answer explain model file train recogn certain type pattern modul set code run independ perform machin learn task possibl solut understand model concept machin learn modul concept design contain particular algorithm perform task import machin learn"
    },
    {
        "Question_id":63405080.0,
        "Question_title":"SageMaker in local Jupyter notebook: cannot use AWS hosted XGBoost container (\"KeyError: 'S3DistributionType'\" and \"Failed to run: ['docker-compose'\")",
        "Question_body":"<p>Running SageMaker within a local Jupyter notebook (using VS Code) works without issue, except that attempting to train an XGBoost model using the AWS hosted container results in errors (container name: <code>246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1-cpu-py3<\/code>).<\/p>\n<h2>Jupyter Notebook<\/h2>\n<pre class=\"lang-py prettyprint-override\"><code>import sagemaker\n\nsession = sagemaker.LocalSession()\n\n# Load and prepare the training and validation data\n...\n\n# Upload the training and validation data to S3\ntest_location = session.upload_data(os.path.join(data_dir, 'test.csv'), key_prefix=prefix)\nval_location = session.upload_data(os.path.join(data_dir, 'validation.csv'), key_prefix=prefix)\ntrain_location = session.upload_data(os.path.join(data_dir, 'train.csv'), key_prefix=prefix)\n\nregion = session.boto_region_name\ninstance_type = 'ml.m4.xlarge'\ncontainer = sagemaker.image_uris.retrieve('xgboost', region, '1.0-1', 'py3', instance_type=instance_type)\n\nrole = 'arn:aws:iam::&lt;USER ID #&gt;:role\/service-role\/AmazonSageMaker-ExecutionRole-&lt;ROLE ID #&gt;'\n\nxgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n\nxgb_estimator.set_hyperparameters(max_depth=5, eta=0.2, gamma=4, min_child_weight=6,\n                                  subsample=0.8, objective='reg:squarederror', early_stopping_rounds=10,\n                                  num_round=200)\n\ns3_input_train = sagemaker.inputs.TrainingInput(s3_data=train_location, content_type='csv')\ns3_input_validation = sagemaker.inputs.TrainingInput(s3_data=val_location, content_type='csv')\n\nxgb_estimator.fit({'train': s3_input_train, 'validation': s3_input_validation})\n<\/code><\/pre>\n<h2>Docker Container KeyError<\/h2>\n<pre><code>algo-1-tfcvc_1  | ERROR:sagemaker-containers:Reporting training FAILURE\nalgo-1-tfcvc_1  | ERROR:sagemaker-containers:framework error: \nalgo-1-tfcvc_1  | Traceback (most recent call last):\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_containers\/_trainer.py&quot;, line 84, in train\nalgo-1-tfcvc_1  |     entrypoint()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 94, in main\nalgo-1-tfcvc_1  |     train(framework.training_env())\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 90, in train\nalgo-1-tfcvc_1  |     run_algorithm_mode()\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/training.py&quot;, line 68, in run_algorithm_mode\nalgo-1-tfcvc_1  |     checkpoint_config=checkpoint_config\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_xgboost_container\/algorithm_mode\/train.py&quot;, line 115, in sagemaker_train\nalgo-1-tfcvc_1  |     validated_data_config = channels.validate(data_config)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 106, in validate\nalgo-1-tfcvc_1  |     channel_obj.validate(value)\nalgo-1-tfcvc_1  |   File &quot;\/miniconda3\/lib\/python3.6\/site-packages\/sagemaker_algorithm_toolkit\/channel_validation.py&quot;, line 52, in validate\nalgo-1-tfcvc_1  |     if (value[CONTENT_TYPE], value[TRAINING_INPUT_MODE], value[S3_DIST_TYPE]) not in self.supported:\nalgo-1-tfcvc_1  | KeyError: 'S3DistributionType'\n\n<\/code><\/pre>\n<h2>Local PC Runtime Error<\/h2>\n<pre><code>RuntimeError: Failed to run: ['docker-compose', '-f', '\/tmp\/tmp71tx0fop\/docker-compose.yaml', 'up', '--build', '--abort-on-container-exit'], Process exited with code: 1\n<\/code><\/pre>\n<p>If the Jupyter notebook is run using the Amazon cloud SageMaker environment (rather than on the local PC), there are no errors. Note that when running on the cloud notebook, the session is initialized as:<\/p>\n<pre><code>session = sagemaker.Session()\n<\/code><\/pre>\n<p>It appears that there is an issue with how the <code>LocalSession()<\/code> works with the hosted docker container.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1597366468133,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1174.0,
        "Answer_body":"<p>When running SageMaker in a local Jupyter notebook, it expects the Docker container to be running on the local machine as well.<\/p>\n<p>The key to ensuring that SageMaker (running in a local notebook) uses the AWS hosted docker container, is to omit the <code>LocalSession<\/code> object when initializing the <code>Estimator<\/code>.<\/p>\n<h2>Wrong<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output', sagemaker_session=session)\n<\/code><\/pre>\n<h2>Correct<\/h2>\n<pre><code>xgb_estimator = sagemaker.estimator.Estimator(\n    container, role, train_instance_count=1, train_instance_type=instance_type,\n    output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n<\/code><\/pre>\n<p>\u00a0\u00a0<\/p>\n<h2>Additional info<\/h2>\n<p>The SageMaker Python SDK source code provides the following helpful hints:<\/p>\n<h1>File: <em>sagemaker\/local\/local_session.py<\/em><\/h1>\n<pre><code>class LocalSagemakerClient(object):\n    &quot;&quot;&quot;A SageMakerClient that implements the API calls locally.\n\n    Used for doing local training and hosting local endpoints. It still needs access to\n    a boto client to interact with S3 but it won't perform any SageMaker call.\n    ...\n<\/code><\/pre>\n<h1>File: <em>sagemaker\/estimator.py<\/em><\/h1>\n<pre><code>class EstimatorBase(with_metaclass(ABCMeta, object)):\n    &quot;&quot;&quot;Handle end-to-end Amazon SageMaker training and deployment tasks.\n\n    For introduction to model training and deployment, see\n    http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-training.html\n\n    Subclasses must define a way to determine what image to use for training,\n    what hyperparameters to use, and how to create an appropriate predictor instance.\n    &quot;&quot;&quot;\n\n    def __init__(self, role, train_instance_count, train_instance_type,\n                 train_volume_size=30, train_max_run=24 * 60 * 60, input_mode='File',\n                 output_path=None, output_kms_key=None, base_job_name=None, sagemaker_session=None, tags=None):\n        &quot;&quot;&quot;Initialize an ``EstimatorBase`` instance.\n\n        Args:\n            role (str): An AWS IAM role (either name or full ARN). ...\n            \n        ...\n\n            sagemaker_session (sagemaker.session.Session): Session object which manages interactions with\n                Amazon SageMaker APIs and any other AWS services needed. If not specified, the estimator creates one\n                using the default AWS configuration chain.\n        &quot;&quot;&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1597367070867,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63405080",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1597366468132,
        "Question_original_content":"local jupyt notebook us aw host xgboost contain keyerror sdistributiontyp fail run docker compos run local jupyt notebook code work issu attempt train xgboost model aw host contain result error contain dkr ecr west amazonaw com xgboost cpu jupyt notebook import session localsess load prepar train valid data upload train valid data test locat session upload data path join data dir test csv kei prefix prefix val locat session upload data path join data dir valid csv kei prefix prefix train locat session upload data path join data dir train csv kei prefix prefix region session boto region instanc type xlarg contain imag uri retriev xgboost region instanc type instanc type role arn aw iam role servic role amazon executionrol xgb estim estim estim contain role train instanc count train instanc type instanc type output path session default bucket prefix output session session xgb estim set hyperparamet max depth eta gamma min child weight subsampl object reg squarederror earli stop round num round input train input traininginput data train locat content type csv input valid input traininginput data val locat content type csv xgb estim fit train input train valid input valid docker contain keyerror algo tfcvc error contain report train failur algo tfcvc error contain framework error algo tfcvc traceback recent algo tfcvc file miniconda lib python site packag contain trainer line train algo tfcvc entrypoint algo tfcvc file miniconda lib python site packag xgboost contain train line main algo tfcvc train framework train env algo tfcvc file miniconda lib python site packag xgboost contain train line train algo tfcvc run algorithm mode algo tfcvc file miniconda lib python site packag xgboost contain train line run algorithm mode algo tfcvc checkpoint config checkpoint config algo tfcvc file miniconda lib python site packag xgboost contain algorithm mode train line train algo tfcvc valid data config channel valid data config algo tfcvc file miniconda lib python site packag algorithm toolkit channel valid line valid algo tfcvc channel obj valid valu algo tfcvc file miniconda lib python site packag algorithm toolkit channel valid line valid algo tfcvc valu content type valu train input mode valu dist type self support algo tfcvc keyerror sdistributiontyp local runtim error runtimeerror fail run docker compos tmp tmptxfop docker compos yaml build abort contain exit process exit code jupyt notebook run amazon cloud environ local error note run cloud notebook session initi session session appear issu localsess work host docker contain",
        "Question_preprocessed_content":"local jupyt notebook us aw host xgboost contain run local jupyt notebook work issu attempt train xgboost model aw host contain result error jupyt notebook docker contain keyerror local runtim error jupyt notebook run amazon cloud environ error note run cloud notebook session initi appear issu work host docker contain",
        "Answer_original_content":"run local jupyt notebook expect docker contain run local machin kei ensur run local notebook us aw host docker contain omit localsess object initi estim wrong xgb estim estim estim contain role train instanc count train instanc type instanc type output path session default bucket prefix output session session correct xgb estim estim estim contain role train instanc count train instanc type instanc type output path session default bucket prefix output addit info python sdk sourc code provid follow help hint file local local session class localcli object client implement api call local local train host local endpoint need access boto client interact won perform file estim class estimatorbas metaclass abcmeta object handl end end train deploy task introduct model train deploy http doc aw amazon com latest work train html subclass defin wai determin imag us train hyperparamet us creat appropri predictor instanc def init self role train instanc count train instanc type train volum size train max run input mode file output path output km kei base job session tag initi estimatorbas instanc arg role str aw iam role arn session session session session object manag interact api aw servic need specifi estim creat default aw configur chain",
        "Answer_preprocessed_content":"run local jupyt notebook expect docker contain run local machin kei ensur us aw host docker contain omit object initi wrong correct addit info python sdk sourc code provid follow help hint file file",
        "Question_gpt_summary_original":"the user encountered challenges when attempting to train an xgboost model using the aws hosted container in a local jupyter notebook, resulting in errors such as \"keyerror: 's3distributiontype'\" and \"failed to run: ['docker-compose'])\".",
        "Question_gpt_summary":"user encount challeng attempt train xgboost model aw host contain local jupyt notebook result error keyerror sdistributiontyp fail run docker compos",
        "Answer_gpt_summary_original":"to resolve errors when attempting to train an xgboost model using the aws hosted container in a local jupyter notebook, the user should omit the localsession object when initializing the estimator. the correct code should be: \n\n```\nxgb_estimator = estimator.estimator(container, role, train_instance_count=1, train_instance_type=instance_type, output_path=f's3:\/\/{session.default_bucket()}\/{prefix}\/output')\n```\n\nthe python sdk source code provides additional helpful hints for local training and deployment tasks.",
        "Answer_gpt_summary":"resolv error attempt train xgboost model aw host contain local jupyt notebook user omit localsess object initi estim correct code xgb estim estim estim contain role train instanc count train instanc type instanc type output path session default bucket prefix output python sdk sourc code provid addit help hint local train deploy task"
    },
    {
        "Question_id":62422682.0,
        "Question_title":"sagemaker notebook instance Elastic Inference tensorflow model local deployment",
        "Question_body":"<p>I am trying to replicate <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_using_elastic_inference_with_your_own_model\/tensorflow_serving_pretrained_model_elastic_inference.ipynb<\/a><\/p>\n\n<p>My elastic inference accelerator is attached to notebook instance. I am using conda_amazonei_tensorflow_p36 kernel. According to documentation I made the changes for local EI:<\/p>\n\n<pre><code>%%time\nimport boto3\n\nregion = boto3.Session().region_name\nsaved_model = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/model\/resnet\/resnet_50_v2_fp32_NCHW.tar.gz'.format(region)\n\nimport sagemaker\nfrom sagemaker.tensorflow.serving import Model\n\nrole = sagemaker.get_execution_role()\n\ntensorflow_model = Model(model_data=saved_model,\nrole=role,\nframework_version='1.14')\ntf_predictor = tensorflow_model.deploy(initial_instance_count=1,\ninstance_type='local',\naccelerator_type='local_sagemaker_notebook')\n<\/code><\/pre>\n\n<p>I am getting following log in the notebook:<\/p>\n\n<pre><code>Attaching to tmp6uqys1el_algo-1-7ynb1_1\nalgo-1-7ynb1_1 | INFO:main:starting services\nalgo-1-7ynb1_1 | INFO:main:using default model name: Servo\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving model config:\nalgo-1-7ynb1_1 | model_config_list: {\nalgo-1-7ynb1_1 | config: {\nalgo-1-7ynb1_1 | name: \"Servo\",\nalgo-1-7ynb1_1 | base_path: \"\/opt\/ml\/model\/export\/Servo\",\nalgo-1-7ynb1_1 | model_platform: \"tensorflow\"\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:nginx config:\nalgo-1-7ynb1_1 | load_module modules\/ngx_http_js_module.so;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_processes auto;\nalgo-1-7ynb1_1 | daemon off;\nalgo-1-7ynb1_1 | pid \/tmp\/nginx.pid;\nalgo-1-7ynb1_1 | error_log \/dev\/stderr error;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | worker_rlimit_nofile 4096;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | events {\nalgo-1-7ynb1_1 | worker_connections 2048;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | http {\nalgo-1-7ynb1_1 | include \/etc\/nginx\/mime.types;\nalgo-1-7ynb1_1 | default_type application\/json;\nalgo-1-7ynb1_1 | access_log \/dev\/stdout combined;\nalgo-1-7ynb1_1 | js_include tensorflow-serving.js;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream tfs_upstream {\nalgo-1-7ynb1_1 | server localhost:8501;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | upstream gunicorn_upstream {\nalgo-1-7ynb1_1 | server unix:\/tmp\/gunicorn.sock fail_timeout=1;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | server {\nalgo-1-7ynb1_1 | listen 8080 deferred;\nalgo-1-7ynb1_1 | client_max_body_size 0;\nalgo-1-7ynb1_1 | client_body_buffer_size 100m;\nalgo-1-7ynb1_1 | subrequest_output_buffer_size 100m;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | set $tfs_version 1.14;\nalgo-1-7ynb1_1 | set $default_tfs_model Servo;\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/tfs {\nalgo-1-7ynb1_1 | rewrite ^\/tfs\/(.) \/$1 break;\nalgo-1-7ynb1_1 | proxy_redirect off;\nalgo-1-7ynb1_1 | proxy_pass_request_headers off;\nalgo-1-7ynb1_1 | proxy_set_header Content-Type 'application\/json';\nalgo-1-7ynb1_1 | proxy_set_header Accept 'application\/json';\nalgo-1-7ynb1_1 | proxy_pass http:\/\/tfs_upstream;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ping {\nalgo-1-7ynb1_1 | js_content ping;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/invocations {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location ~ ^\/models\/(.)\/invoke {\nalgo-1-7ynb1_1 | js_content invocations;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/models {\nalgo-1-7ynb1_1 | proxy_pass http:\/\/gunicorn_upstream\/models;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | location \/ {\nalgo-1-7ynb1_1 | return 404 '{\"error\": \"Not Found\"}';\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | keepalive_timeout 3;\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 | }\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 |\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 8)\nalgo-1-7ynb1_1 | INFO:main:nginx version info:\nalgo-1-7ynb1_1 | nginx version: nginx\/1.16.1\nalgo-1-7ynb1_1 | built by gcc 5.4.0 20160609 (Ubuntu 5.4.0-6ubuntu1~16.04.11)\nalgo-1-7ynb1_1 | built with OpenSSL 1.0.2g 1 Mar 2016\nalgo-1-7ynb1_1 | TLS SNI support enabled\nalgo-1-7ynb1_1 | configure arguments: --prefix=\/etc\/nginx --sbin-path=\/usr\/sbin\/nginx --modules-path=\/usr\/lib\/nginx\/modules --conf-path=\/etc\/nginx\/nginx.conf --error-log-path=\/var\/log\/nginx\/error.log --http-log-path=\/var\/log\/nginx\/access.log --pid-path=\/var\/run\/nginx.pid --lock-path=\/var\/run\/nginx.lock --http-client-body-temp-path=\/var\/cache\/nginx\/client_temp --http-proxy-temp-path=\/var\/cache\/nginx\/proxy_temp --http-fastcgi-temp-path=\/var\/cache\/nginx\/fastcgi_temp --http-uwsgi-temp-path=\/var\/cache\/nginx\/uwsgi_temp --http-scgi-temp-path=\/var\/cache\/nginx\/scgi_temp --user=nginx --group=nginx --with-compat --with-file-aio --with-threads --with-http_addition_module --with-http_auth_request_module --with-http_dav_module --with-http_flv_module --with-http_gunzip_module --with-http_gzip_static_module --with-http_mp4_module --with-http_random_index_module --with-http_realip_module --with-http_secure_link_module --with-http_slice_module --with-http_ssl_module --with-http_stub_status_module --with-http_sub_module --with-http_v2_module --with-mail --with-mail_ssl_module --with-stream --with-stream_realip_module --with-stream_ssl_module --with-stream_ssl_preread_module --with-cc-opt='-g -O2 -fPIE -fstack-protector-strong -Wformat -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fPIC' --with-ld-opt='-Wl,-Bsymbolic-functions -fPIE -pie -Wl,-z,relro -Wl,-z,now -Wl,--as-needed -pie'\nalgo-1-7ynb1_1 | INFO:main:started nginx (pid: 10)\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888114: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.888186: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988623: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988688: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988728: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988762: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:08.988783: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.001922: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.082734: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:09.613725: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\nalgo-1-7ynb1_1 |\n!algo-1-7ynb1_1 | 172.18.0.1 - - [17\/Jun\/2020:05:02:10 +0000] \"GET \/ping HTTP\/1.1\" 200 3 \"-\" \"-\"\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662569us] [Execution Engine] Error getting application context for [TensorFlow][2]\nalgo-1-7ynb1_1 | [Wed Jun 17 05:02:11 2020, 662722us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-7ynb1_1 | EI Error Code: [3, 16, 8]\nalgo-1-7ynb1_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-7ynb1_1 | EI Request ID: TF-D66B9810-D81A-448F-ACE2-703FFFA0F194 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | EI Client Version: 1.5.3\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.668412: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-7ynb1_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-7ynb1_1 | INFO:main:tensorflow version info:\nalgo-1-7ynb1_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-7ynb1_1 | TensorFlow Library: 1.14.0\nalgo-1-7ynb1_1 | EI Version: EI-1.4\nalgo-1-7ynb1_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-7ynb1_1 | INFO:main:started tensorflow serving (pid: 38)`enter code here`\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759706: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.759783: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860242: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860309: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860333: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860365: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.860382: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.873381: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nalgo-1-7ynb1_1 | 2020-06-17 05:02:11.949421: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-7ynb1_1 | 2020-06-17 05:02:12.512935: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-7ynb1_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\nalgo-1-7ynb1_1 | Number of Elastic Inference Accelerators Available: 1\nalgo-1-7ynb1_1 | Elastic Inference Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Type: eia2.medium\nalgo-1-7ynb1_1 | Elastic Inference Accelerator Ordinal: 0\n`\n<\/code><\/pre>\n\n<p>The log never stops in notebook. It keeps throwing in notebook cells. I am not sure whether the model is deployed correctly.<\/p>\n\n<p>I can see the docker of the model running\n<a href=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/YRXKL.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I try to infer\/predict from that model, I get error:<\/p>\n\n<pre><code>algo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761607us] [Execution Engine] Error getting application context for [TensorFlow][2]\n\nalgo-1-iikpj_1 | [Wed Jun 17 05:29:47 2020, 761691us] [Execution Engine][TensorFlow][2] Failed - Last Error:\nalgo-1-iikpj_1 | EI Error Code: [3, 16, 8]\nalgo-1-iikpj_1 | EI Error Description: Unable to authenticate with accelerator\nalgo-1-iikpj_1 | EI Request ID: TF-ADECD8EF-7138-4B5F-9C37-ADFDC8122DF1 -- EI Accelerator ID: eia-813285f77ceb448c849e2331116f251b\nalgo-1-iikpj_1 | EI Client Version: 1.5.3\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.768249: F external\/org_tensorflow\/tensorflow\/contrib\/ei\/session\/eia_session.cc:1219] Non-OK-status: SwapExStateWithEI(tmp_inputs, tmp_outputs, tmp_freeze) status: Internal: Failed to get the initial operator whitelist from server.\nalgo-1-iikpj_1 | WARNING:main:unexpected tensorflow serving exit (status: 6). restarting.\nalgo-1-iikpj_1 | INFO:main:tensorflow version info:\nalgo-1-iikpj_1 | TensorFlow ModelServer: 1.14.0-rc0+dev.sha.34d9e85\nalgo-1-iikpj_1 | TensorFlow Library: 1.14.0\nalgo-1-iikpj_1 | EI Version: EI-1.4\nalgo-1-iikpj_1 | INFO:main:tensorflow serving command: tensorflow_model_server --port=9000 --rest_api_port=8501 --model_config_file=\/sagemaker\/model-config.cfg\nalgo-1-iikpj_1 | INFO:main:started tensorflow serving (pid: 1052)\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854331: I tensorflow_serving\/model_servers\/server_core.cc:462] Adding\/updating models.\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.854405: I tensorflow_serving\/model_servers\/server_core.cc:561] (Re-)adding model: Servo\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 2020\/06\/17 05:29:47 [error] 11#11: *2 connect() failed (111: Connection refused) while connecting to upstream, client: 172.18.0.1, server: , request: \"POST \/invocations HTTP\/1.1\", subrequest: \"\/v1\/models\/Servo:predict\", upstream: \"http:\/\/127.0.0.1:8501\/v1\/models\/Servo:predict\", host: \"localhost:8080\"\nalgo-1-iikpj_1 | 172.18.0.1 - - [17\/Jun\/2020:05:29:47 +0000] \"POST \/invocations HTTP\/1.1\" 502 157 \"-\" \"-\"\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954825: I tensorflow_serving\/core\/basic_manager.cc:739] Successfully reserved resources to load servable {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.954887: I tensorflow_serving\/core\/loader_harness.cc:66] Approving load for servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955448: I tensorflow_serving\/core\/loader_harness.cc:74] Loading servable version {name: Servo version: 1527887769}\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955494: I external\/org_tensorflow\/tensorflow\/contrib\/session_bundle\/bundle_shim.cc:363] Attempting to load native SavedModelBundle in bundle-shim from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.955859: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:31] Reading SavedModel from: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | 2020-06-17 05:29:47.969511: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/reader.cc:54] Reading meta graph with tags { serve }\nJSONDecodeError Traceback (most recent call last)\nin ()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/tensorflow\/serving.py in predict(self, data, initial_args)\n116 args[\"CustomAttributes\"] = self._model_attributes\n117\n--&gt; 118 return super(Predictor, self).predict(data, args)\n119\n120\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in predict(self, data, initial_args, target_model)\n109 request_args = self._create_request_args(data, initial_args, target_model)\n110 response = self.sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\n--&gt; 111 return self._handle_response(response)\n112\n113 def _handle_response(self, response):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in _handle_response(self, response)\n119 if self.deserializer is not None:\n120 # It's the deserializer's responsibility to close the stream\n--&gt; 121 return self.deserializer(response_body, response[\"ContentType\"])\n122 data = response_body.read()\n123 response_body.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/site-packages\/sagemaker\/predictor.py in call(self, stream, content_type)\n578 \"\"\"\n579 try:\n--&gt; 580 return json.load(codecs.getreader(\"utf-8\")(stream))\n581 finally:\n582 stream.close()\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in load(fp, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n297 cls=cls, object_hook=object_hook,\n298 parse_float=parse_float, parse_int=parse_int,\n--&gt; 299 parse_constant=parse_constant, object_pairs_hook=object_pairs_hook, **kw)\n300\n301\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/init.py in loads(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\n352 parse_int is None and parse_float is None and\n353 parse_constant is None and object_pairs_hook is None and not kw):\n--&gt; 354 return _default_decoder.decode(s)\n355 if cls is None:\n356 cls = JSONDecoder\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in decode(self, s, _w)\n337\n338 \"\"\"\n--&gt; 339 obj, end = self.raw_decode(s, idx=_w(s, 0).end())\n340 end = _w(s, end).end()\n341 if end != len(s):\n\n~\/anaconda3\/envs\/amazonei_tensorflow_p36\/lib\/python3.6\/json\/decoder.py in raw_decode(self, s, idx)\n355 obj, end = self.scan_once(s, idx)\n356 except StopIteration as err:\n--&gt; 357 raise JSONDecodeError(\"Expecting value\", s, err.value) from None\n358 return obj, end\n\nJSONDecodeError: Expecting value: line 1 column 1 (char 0)\n\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.047106: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:202] Restoring SavedModel bundle.\nalgo-1-iikpj_1 | 2020-06-17 05:29:48.564452: I external\/org_tensorflow\/tensorflow\/cc\/saved_model\/loader.cc:151] Running initialization op on SavedModel bundle at path: \/opt\/ml\/model\/export\/Servo\/1527887769\nalgo-1-iikpj_1 | Using Amazon Elastic Inference Client Library Version: 1.5.3\n<\/code><\/pre>\n\n<p>I tried several ways to solve JSONDecodeError: Expecting value: line 1 column 1 (char 0) using json.loads, json.dumps etc but nothing helps.\nI also tried Rest API post to docker deployed model:<\/p>\n\n<pre><code>curl -v -X POST \\ -H 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/Servo:predict\nbut still getting error:\n[![enter image description here][1]][1]\n<\/code><\/pre>\n\n<p>Please help me to resolve the issue. Initially, I was trying to use my tensorflow serving model and getting the same errors. Then I thought of following with the same model which was used in AWS example notebook (resnet_50_v2_fp32_NCHW.tar.gz'). So, the above experiment is using AWS example notebook with model provided by sagemaker-sample-data.<\/p>\n\n<p>Please help me out. Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1592375129670,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":350.0,
        "Answer_body":"<p>Solved it. The error I was getting is due to roles\/permission of elastic inference attached to notebook. Once fixed these permissions by our devops team. It worked as expected.  See <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/issues\/142<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":-1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62422682",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1592876274340,
        "Question_original_content":"notebook instanc elast infer tensorflow model local deploy try replic http github com awslab amazon exampl blob master python sdk tensorflow serv elast infer model tensorflow serv pretrain model elast infer ipynb elast infer acceler attach notebook instanc conda amazonei tensorflow kernel accord document chang local time import boto region boto session region save model sampl data tensorflow model resnet resnet nchw tar format region import tensorflow serv import model role execut role tensorflow model model model data save model role role framework version predictor tensorflow model deploi initi instanc count instanc type local acceler type local notebook get follow log notebook attach tmpuqysel algo ynb algo ynb info main start servic algo ynb info main default model servo algo ynb info main tensorflow serv model config algo ynb model config list algo ynb config algo ynb servo algo ynb base path opt model export servo algo ynb model platform tensorflow algo ynb algo ynb algo ynb algo ynb algo ynb info main nginx config algo ynb load modul modul ngx http modul algo ynb algo ynb worker process auto algo ynb daemon algo ynb pid tmp nginx pid algo ynb error log dev stderr error algo ynb algo ynb worker rlimit nofil algo ynb algo ynb event algo ynb worker connect algo ynb algo ynb algo ynb http algo ynb includ nginx mime type algo ynb default type applic json algo ynb access log dev stdout combin algo ynb includ tensorflow serv algo ynb algo ynb upstream tf upstream algo ynb server localhost algo ynb algo ynb algo ynb upstream gunicorn upstream algo ynb server unix tmp gunicorn sock fail timeout algo ynb algo ynb algo ynb server algo ynb listen defer algo ynb client max bodi size algo ynb client bodi buffer size algo ynb subrequest output buffer size algo ynb algo ynb set tf version algo ynb set default tf model servo algo ynb algo ynb locat tf algo ynb rewrit tf break algo ynb proxi redirect algo ynb proxi pass request header algo ynb proxi set header content type applic json algo ynb proxi set header accept applic json algo ynb proxi pass http tf upstream algo ynb algo ynb algo ynb locat ping algo ynb content ping algo ynb algo ynb algo ynb locat invoc algo ynb content invoc algo ynb algo ynb algo ynb locat model invok algo ynb content invoc algo ynb algo ynb algo ynb locat model algo ynb proxi pass http gunicorn upstream model algo ynb algo ynb algo ynb locat algo ynb return error algo ynb algo ynb algo ynb keepal timeout algo ynb algo ynb algo ynb algo ynb algo ynb info main tensorflow version info algo ynb tensorflow modelserv dev sha algo ynb tensorflow librari algo ynb version algo ynb info main tensorflow serv command tensorflow model server port rest api port model config file model config cfg algo ynb info main start tensorflow serv pid algo ynb info main nginx version info algo ynb nginx version nginx algo ynb built gcc ubuntu ubuntu algo ynb built openssl mar algo ynb tl sni support enabl algo ynb configur argument prefix nginx sbin path usr sbin nginx modul path usr lib nginx modul conf path nginx nginx conf error log path var log nginx error log http log path var log nginx access log pid path var run nginx pid lock path var run nginx lock http client bodi temp path var cach nginx client temp http proxi temp path var cach nginx proxi temp http fastcgi temp path var cach nginx fastcgi temp http uwsgi temp path var cach nginx uwsgi temp http scgi temp path var cach nginx scgi temp user nginx group nginx compat file aio thread http addit modul http auth request modul http dav modul http flv modul http gunzip modul http gzip static modul http modul http random index modul http realip modul http secur link modul http slice modul http ssl modul http stub statu modul http sub modul http modul mail mail ssl modul stream stream realip modul stream ssl modul stream ssl preread modul opt fpie fstack protector strong wformat werror format secur fortifi sourc fpic opt bsymbol function fpie pie relro need pie algo ynb info main start nginx pid algo ynb tensorflow serv model server server core ad updat model algo ynb tensorflow serv model server server core ad model servo algo ynb tensorflow serv core basic manag successfulli reserv resourc load servabl servo version algo ynb tensorflow serv core loader har approv load servabl version servo version algo ynb tensorflow serv core loader har load servabl version servo version algo ynb extern org tensorflow tensorflow contrib session bundl bundl shim attempt load nativ savedmodelbundl bundl shim opt model export servo algo ynb extern org tensorflow tensorflow save model reader read savedmodel opt model export servo algo ynb extern org tensorflow tensorflow save model reader read meta graph tag serv algo ynb extern org tensorflow tensorflow save model loader restor savedmodel bundl algo ynb extern org tensorflow tensorflow save model loader run initi savedmodel bundl path opt model export servo algo ynb amazon elast infer client librari version algo ynb number elast infer acceler avail algo ynb elast infer acceler eia fcebcefb algo ynb elast infer acceler type eia medium algo ynb elast infer acceler ordin algo ynb algo ynb jun ping http algo ynb wed jun execut engin error get applic context tensorflow algo ynb wed jun execut engin tensorflow fail error algo ynb error code algo ynb error descript unabl authent acceler algo ynb request ac acceler eia fcebcefb algo ynb client version algo ynb extern org tensorflow tensorflow contrib session eia session non statu swapexstatewithei tmp input tmp output tmp freez statu intern fail initi oper whitelist server algo ynb warn main unexpect tensorflow serv exit statu restart algo ynb info main tensorflow version info algo ynb tensorflow modelserv dev sha algo ynb tensorflow librari algo ynb version algo ynb info main tensorflow serv command tensorflow model server port rest api port model config file model config cfg algo ynb info main start tensorflow serv pid enter code algo ynb tensorflow serv model server server core ad updat model algo ynb tensorflow serv model server server core ad model servo algo ynb tensorflow serv core basic manag successfulli reserv resourc load servabl servo version algo ynb tensorflow serv core loader har approv load servabl version servo version algo ynb tensorflow serv core loader har load servabl version servo version algo ynb extern org tensorflow tensorflow contrib session bundl bundl shim attempt load nativ savedmodelbundl bundl shim opt model export servo algo ynb extern org tensorflow tensorflow save model reader read savedmodel opt model export servo algo ynb extern org tensorflow tensorflow save model reader read meta graph tag serv algo ynb extern org tensorflow tensorflow save model loader restor savedmodel bundl algo ynb extern org tensorflow tensorflow save model loader run initi savedmodel bundl path opt model export servo algo ynb amazon elast infer client librari version algo ynb number elast infer acceler avail algo ynb elast infer acceler eia fcebcefb algo ynb elast infer acceler type eia medium algo ynb elast infer acceler ordin log stop notebook keep throw notebook cell sure model deploi correctli docker model run try infer predict model error algo iikpj wed jun execut engin error get applic context tensorflow algo iikpj wed jun execut engin tensorflow fail error algo iikpj error code algo iikpj error descript unabl authent acceler algo iikpj request adecdef adfdcdf acceler eia fcebcefb algo iikpj client version algo iikpj extern org tensorflow tensorflow contrib session eia session non statu swapexstatewithei tmp input tmp output tmp freez statu intern fail initi oper whitelist server algo iikpj warn main unexpect tensorflow serv exit statu restart algo iikpj info main tensorflow version info algo iikpj tensorflow modelserv dev sha algo iikpj tensorflow librari algo iikpj version algo iikpj info main tensorflow serv command tensorflow model server port rest api port model config file model config cfg algo iikpj info main start tensorflow serv pid algo iikpj tensorflow serv model server server core ad updat model algo iikpj tensorflow serv model server server core ad model servo algo iikpj error connect fail connect refus connect upstream client server request post invoc http subrequest model servo predict upstream http model servo predict host localhost algo iikpj error connect fail connect refus connect upstream client server request post invoc http subrequest model servo predict upstream http model servo predict host localhost algo iikpj jun post invoc http algo iikpj tensorflow serv core basic manag successfulli reserv resourc load servabl servo version algo iikpj tensorflow serv core loader har approv load servabl version servo version algo iikpj tensorflow serv core loader har load servabl version servo version algo iikpj extern org tensorflow tensorflow contrib session bundl bundl shim attempt load nativ savedmodelbundl bundl shim opt model export servo algo iikpj extern org tensorflow tensorflow save model reader read savedmodel opt model export servo algo iikpj extern org tensorflow tensorflow save model reader read meta graph tag serv jsondecodeerror traceback recent anaconda env amazonei tensorflow lib python site packag tensorflow serv predict self data initi arg arg customattribut self model attribut return super predictor self predict data arg anaconda env amazonei tensorflow lib python site packag predictor predict self data initi arg target model request arg self creat request arg data initi arg target model respons self session runtim client invok endpoint request arg return self handl respons respons def handl respons self respons anaconda env amazonei tensorflow lib python site packag predictor handl respons self respons self deseri deseri respons close stream return self deseri respons bodi respons contenttyp data respons bodi read respons bodi close anaconda env amazonei tensorflow lib python site packag predictor self stream content type try return json load codec getread utf stream final stream close anaconda env amazonei tensorflow lib python json init load cl object hook pars float pars int pars constant object pair hook cl cl object hook object hook pars float pars float pars int pars int pars constant pars constant object pair hook object pair hook anaconda env amazonei tensorflow lib python json init load encod cl object hook pars float pars int pars constant object pair hook pars int pars float pars constant object pair hook return default decod decod cl cl jsondecod anaconda env amazonei tensorflow lib python json decod decod self obj end self raw decod idx end end end end end len anaconda env amazonei tensorflow lib python json decod raw decod self idx obj end self scan idx stopiter err rais jsondecodeerror expect valu err valu return obj end jsondecodeerror expect valu line column char algo iikpj extern org tensorflow tensorflow save model loader restor savedmodel bundl algo iikpj extern org tensorflow tensorflow save model loader run initi savedmodel bundl path opt model export servo algo iikpj amazon elast infer client librari version tri wai solv jsondecodeerror expect valu line column char json load json dump help tri rest api post docker deploi model curl post content type applic json data input http model servo predict get error enter imag descript help resolv issu initi try us tensorflow serv model get error thought follow model aw exampl notebook resnet nchw tar experi aw exampl notebook model provid sampl data help thank",
        "Question_preprocessed_content":"notebook instanc elast infer tensorflow model local deploy try replic elast infer acceler attach notebook instanc kernel accord document chang local get follow log notebook log stop notebook keep throw notebook cell sure model deploi correctli docker model run try model error tri wai solv jsondecodeerror expect valu line column help tri rest api post docker deploi model help resolv issu initi try us tensorflow serv model get error thought follow model aw exampl notebook experi aw exampl notebook model provid help thank",
        "Answer_original_content":"solv error get role permiss elast infer attach notebook fix permiss devop team work expect http github com aw tensorflow serv contain issu",
        "Answer_preprocessed_content":"solv error get elast infer attach notebook fix permiss devop team work expect",
        "Question_gpt_summary_original":"the user is encountering challenges with deploying a tensorflow model with elastic inference on a notebook instance, resulting in errors such as \"jsondecodeerror: expecting value: line 1 column 1 (char 0)\" and \"curl -v -x post \\ -h 'content-type:application\/json' \\ -d '{\"data\": {\"inputs\": [[[[0.13075708159043742, 0.048010725848070535, 0.9012465727287071], [0.1643217202482622, 0.7392467524276859, 0.5618572640643519], [0.7697097217983989, 0.9829998452540657, 0.08567413146192027]]]]} }' \\ http:\/\/127.0.0.1:8080\/v1\/models\/servo",
        "Question_gpt_summary":"user encount challeng deploi tensorflow model elast infer notebook instanc result error jsondecodeerror expect valu line column char curl post content type applic json data input http model servo",
        "Answer_gpt_summary_original":"the solution to the error encountered while deploying a tensorflow model with elastic inference on a notebook instance is to fix the roles\/permissions of elastic inference attached to the notebook. the user's devops team fixed these permissions, and the deployment worked as expected.",
        "Answer_gpt_summary":"solut error encount deploi tensorflow model elast infer notebook instanc fix role permiss elast infer attach notebook user devop team fix permiss deploy work expect"
    },
    {
        "Question_id":null,
        "Question_title":"Retrieve Notebooks Azure Files",
        "Question_body":"Is it possible to retrieve notebooks that were hosted on notebooks.azure.com? If so, how? The service is now discontinued but I would like to retrieve files that were hosted on the service.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615959743450,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@sean-9375 I am afraid that the option to retrieve this data is not possible. Please refer this thread for information and the options that were available before the last day to migrate them. Thanks!!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/317875\/retrieve-notebooks-azure-files.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-17T10:07:29.02Z",
                "Answer_score":1,
                "Answer_body":"@sean-9375 I am afraid that the option to retrieve this data is not possible. Please refer this thread for information and the options that were available before the last day to migrate them. Thanks!!",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1615975649020,
        "Question_original_content":"retriev notebook azur file possibl retriev notebook host notebook azur com servic discontinu like retriev file host servic",
        "Question_preprocessed_content":"retriev notebook azur file possibl retriev notebook host servic discontinu like retriev file host servic",
        "Answer_original_content":"sean afraid option retriev data possibl refer thread inform option avail dai migrat thank",
        "Answer_preprocessed_content":"afraid option retriev data possibl refer thread inform option avail dai migrat thank",
        "Question_gpt_summary_original":"the user is looking to retrieve notebooks that were hosted on notebooks.azure.com, which is now discontinued, and is wondering if it is possible and how to do so.",
        "Question_gpt_summary":"user look retriev notebook host notebook azur com discontinu wonder possibl",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer for retrieving notebooks that were hosted on notebooks.azure.com. the user is directed to refer to a thread for information and options that were available before the last day to migrate them.",
        "Answer_gpt_summary":"solut provid answer retriev notebook host notebook azur com user direct refer thread inform option avail dai migrat"
    },
    {
        "Question_id":null,
        "Question_title":"Can we append data to an existing csv file stored in Azure blob storage?",
        "Question_body":"I have a machine learning model deployed in azure designer studio. I need to retrain it everyday with new data through python code. I need to keep the existing csv data in the blob storage and also add some more data to the existing csv and retrain it. If I retrain the model with only the new data, the old data is lost so I need to retrain the model by appending new data to existing data. Is there any way to do it through python coding?\n\nI have also researched about append blob but they add only in the end of the blob. In the documentation, they have mentioned we cannot update or add to an existing blob.\n\nAny help is appreciated. Thanks a lot.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1615437989000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/308980\/can-we-append-data-to-an-existing-csv-file-stored.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-11T10:11:06.027Z",
                "Answer_score":0,
                "Answer_body":"@SenthilMuruganRAMACHANDRAN-7389 The best practice with respect to Azure Machine learning is to register your dataset and version it if you would like to retrain it to create a new model. You can infact have multiple csv files in your storage and create a single tabular dataset from the files. For example:\n\nHere we are using files from a blob container which are placed at different times and registering the dataset with versioning. If you would like to add more file, you can simply add more csv files to the web path and then register a new version or use the older versions again if required.\n\n # create a TabularDataset from Titanic training data\n web_paths = ['https:\/\/dprepdata.blob.core.windows.net\/demo\/Titanic.csv',\n              'https:\/\/dprepdata.blob.core.windows.net\/demo\/Titanic2.csv']\n titanic_ds = Dataset.Tabular.from_delimited_files(path=web_paths)\n    \n # create a new version of titanic_ds\n titanic_ds = titanic_ds.register(workspace = workspace,\n                                  name = 'titanic_ds',\n                                  description = 'new titanic training data',\n                                  create_new_version = True)",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"append data exist csv file store azur blob storag machin learn model deploi azur design studio need retrain everydai new data python code need exist csv data blob storag add data exist csv retrain retrain model new data old data lost need retrain model append new data exist data wai python code research append blob add end blob document mention updat add exist blob help appreci thank lot",
        "Question_preprocessed_content":"append data exist csv file store azur blob storag machin learn model deploi azur design studio need retrain everydai new data python code need exist csv data blob storag add data exist csv retrain retrain model new data old data lost need retrain model append new data exist data wai python code research append blob add end blob document mention updat add exist blob help appreci thank lot",
        "Answer_original_content":"senthilmuruganramachandran best practic respect regist dataset version like retrain creat new model infact multipl csv file storag creat singl tabular dataset file exampl file blob contain place differ time regist dataset version like add file simpli add csv file web path regist new version us older version requir creat tabulardataset titan train data web path http dprepdata blob core window net demo titan csv http dprepdata blob core window net demo titan csv titan dataset tabular delimit file path web path creat new version titan titan titan regist workspac workspac titan descript new titan train data creat new version true",
        "Answer_preprocessed_content":"best practic respect regist dataset version like retrain creat new model infact multipl csv file storag creat singl tabular dataset file exampl file blob contain place differ time regist dataset version like add file simpli add csv file web path regist new version us older version requir creat tabulardataset titan train data creat new version workspac descript new titan train data true",
        "Question_gpt_summary_original":"the user needs to retrain a machine learning model deployed in azure designer studio with new data every day, while also preserving the existing data stored in a csv file in azure blob storage, and is looking for a way to do this through python coding.",
        "Question_gpt_summary":"user need retrain machin learn model deploi azur design studio new data dai preserv exist data store csv file azur blob storag look wai python code",
        "Answer_gpt_summary_original":"the answer suggests that the best practice for retraining a machine learning model with new data in azure designer studio is to register the dataset and version it. multiple csv files can be stored in azure blob storage and a single tabular dataset can be created from them. the code provided shows how to create a tabular dataset from csv files and register it with versioning. new csv files can be added to the web path and a new version of the dataset can be created or older versions can be used again if required.",
        "Answer_gpt_summary":"answer suggest best practic retrain machin learn model new data azur design studio regist dataset version multipl csv file store azur blob storag singl tabular dataset creat code provid show creat tabular dataset csv file regist version new csv file ad web path new version dataset creat older version requir"
    },
    {
        "Question_id":null,
        "Question_title":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Question_body":"Does SageMaker Multi-Model Endpoint support SageMaker Model Monitor?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1590501108000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":130.0,
        "Answer_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints . https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq2z-BEt7TnmZ8vFYs-Hu7g\/does-sage-maker-multi-model-endpoint-support-sage-maker-model-monitor",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-05-26T13:58:28.000Z",
                "Answer_score":0,
                "Answer_body":"Amazon SageMaker Model Monitor currently supports only endpoints that host a single model and does not support monitoring multi-model endpoints. For information on using multi-model endpoints, see Host Multiple Models with Multi-Model Endpoints . https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-monitor.html",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1590501508000,
        "Question_original_content":"multi model endpoint support model monitor multi model endpoint support model monitor",
        "Question_preprocessed_content":"endpoint support model monitor endpoint support model monitor",
        "Answer_original_content":"model monitor current support endpoint host singl model support monitor multi model endpoint inform multi model endpoint host multipl model multi model endpoint http doc aw amazon com latest model monitor html",
        "Answer_preprocessed_content":"model monitor current support endpoint host singl model support monitor endpoint inform endpoint host multipl model endpoint",
        "Question_gpt_summary_original":"the user is inquiring about whether a multi-model endpoint supports model monitoring.",
        "Question_gpt_summary":"user inquir multi model endpoint support model monitor",
        "Answer_gpt_summary_original":"the answer states that the model monitor currently does not support monitoring multi-model endpoints. however, it suggests checking out the documentation on how to host multiple models with multi-model endpoints. therefore, the possible solution is to use multi-model endpoints to host multiple models and monitor them separately.",
        "Answer_gpt_summary":"answer state model monitor current support monitor multi model endpoint suggest check document host multipl model multi model endpoint possibl solut us multi model endpoint host multipl model monitor separ"
    },
    {
        "Question_id":null,
        "Question_title":"ERROR: unexpected error - 'retries'",
        "Question_body":"<p>Hi, i run dvc from inside a docker container ,i call dvc  commands from python script , inside the container ,everything is configured ,repo ,remote storage and remote credentials, when i run the script without the container it works perfectly ,but when using the container it raise this error<br>\nwhen it comes to \u201cdvc push\u201d in the script \u201cERROR: unexpected error - \u2018retries\u2019\u201d ,the error obviously not informative ,so please need a help for that<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1644477401580,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":284.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-unexpected-error-retries\/1051",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-10T10:20:51.556Z",
                "Answer_body":"<p>Could you please share the output of <code>dvc push -v<\/code>? It\u2019ll have the error traceback and more logging information. Thanks.<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-10T10:47:33.719Z",
                "Answer_body":"<p>here is the complete trace from command line when running the docker container<\/p>\n<p>dvc pushing \u2026<br>\ndvc started pushing the data\u2026<\/p>\n<pre><code class=\"lang-auto\">2022-02-10 10:42:18,096 DEBUG: Adding '\/usr\/src\/app\/.dvc\/config.local' to gitignore file.\n2022-02-10 10:42:18,112 DEBUG: Adding '\/usr\/src\/app\/.dvc\/tmp' to gitignore file.\n2022-02-10 10:42:18,115 DEBUG: Adding '\/usr\/src\/app\/.dvc\/cache' to gitignore file.\n2022-02-10 10:43:01,608 DEBUG: Preparing to transfer data from '\/usr\/src\/app\/.dvc\/cache' to 'drayd-pipeline-dev\/Data'\n2022-02-10 10:43:01,609 DEBUG: Preparing to collect status from 'drayd-pipeline-dev\/Data'\n2022-02-10 10:43:01,609 DEBUG: Collecting status from 'drayd-pipeline-dev\/Data'\n2022-02-10 10:43:02,716 ERROR: unexpected error - 'retries'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/main.py\", line 55, in main\n    ret = cmd.do_run()\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/command\/base.py\", line 45, in do_run\n    return self.run()\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/command\/data_sync.py\", line 67, in run\n    glob=self.args.glob,\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/repo\/__init__.py\", line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/repo\/push.py\", line 57, in push\n    obj_ids, jobs, remote=remote, odb=odb or dest_odb\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/data_cloud.py\", line 91, in push\n    cache_odb=self.repo.odb.local,\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/transfer.py\", line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/status.py\", line 159, in compare_status\n    dest, obj_ids, index=dest_index, **kwargs\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/status.py\", line 131, in status\n    exists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 474, in hashes_exist\n    remote_size, remote_hashes = self._estimate_remote_size(hashes, name)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 271, in _estimate_remote_size\n    remote_hashes = set(hashes)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 225, in _hashes_with_limit\n    for hash_ in self._list_hashes(prefix, progress_callback):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 215, in _list_hashes\n    for path in self._list_paths(prefix, progress_callback):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 195, in _list_paths\n    for file_info in self.fs.find(fs_path, prefix=prefix):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 179, in find\n    files = self.fs.find(with_prefix, prefix=self.path.parts(path)[-1])\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/fsspec\/asyn.py\", line 91, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/fsspec\/asyn.py\", line 71, in sync\n    raise return_result\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/fsspec\/asyn.py\", line 25, in _runner\n    result[0] = await coro\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/core.py\", line 665, in _find\n    out = await self._lsdir(path, delimiter=\"\", prefix=prefix)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/core.py\", line 579, in _lsdir\n    s3 = await self.get_s3(bucket)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/core.py\", line 247, in get_s3\n    return await self._s3creator.get_bucket_client(bucket)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/utils.py\", line 65, in get_bucket_client\n    response = await general_client.head_bucket(Bucket=bucket_name)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/client.py\", line 174, in _make_api_call\n    operation_model, request_dict, request_context)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/client.py\", line 193, in _make_request\n    return await self._endpoint.make_request(operation_model, request_dict)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/endpoint.py\", line 83, in _send_request\n    exception):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/endpoint.py\", line 216, in _needs_retry\n    caught_exception=caught_exception, request_dict=request_dict)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/hooks.py\", line 29, in _emit\n    response = handler(**kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/botocore\/retryhandler.py\", line 189, in __call__\n    retries_context = kwargs['request_dict']['context']['retries']\nKeyError: 'retries'\n------------------------------------------------------------\n2022-02-10 10:43:03,070 DEBUG: Adding '\/usr\/src\/app\/.dvc\/config.local' to gitignore file.\n2022-02-10 10:43:03,086 DEBUG: Adding '\/usr\/src\/app\/.dvc\/tmp' to gitignore file.\n2022-02-10 10:43:03,091 DEBUG: Adding '\/usr\/src\/app\/.dvc\/cache' to gitignore file.\n2022-02-10 10:43:03,115 DEBUG: [Errno 95] no more link types left to try out: [Errno 95] 'reflink' is not supported by &lt;class 'dvc.fs.local.LocalFileSystem'&gt;: [Errno 18] Invalid cross-device link\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/main.py\", line 55, in main\n    ret = cmd.do_run()\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/command\/base.py\", line 45, in do_run\n    return self.run()\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/command\/data_sync.py\", line 67, in run\n    glob=self.args.glob,\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/repo\/__init__.py\", line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/repo\/push.py\", line 57, in push\n    obj_ids, jobs, remote=remote, odb=odb or dest_odb\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/data_cloud.py\", line 91, in push\n    cache_odb=self.repo.odb.local,\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/transfer.py\", line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/status.py\", line 159, in compare_status\n    dest, obj_ids, index=dest_index, **kwargs\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/status.py\", line 131, in status\n    exists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 474, in hashes_exist\n    remote_size, remote_hashes = self._estimate_remote_size(hashes, name)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 271, in _estimate_remote_size\n    remote_hashes = set(hashes)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 225, in _hashes_with_limit\n    for hash_ in self._list_hashes(prefix, progress_callback):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 215, in _list_hashes\n    for path in self._list_paths(prefix, progress_callback):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 195, in _list_paths\n    for file_info in self.fs.find(fs_path, prefix=prefix):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 179, in find\n    files = self.fs.find(with_prefix, prefix=self.path.parts(path)[-1])\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/fsspec\/asyn.py\", line 91, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/fsspec\/asyn.py\", line 71, in sync\n    raise return_result\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/fsspec\/asyn.py\", line 25, in _runner\n    result[0] = await coro\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/core.py\", line 665, in _find\n    out = await self._lsdir(path, delimiter=\"\", prefix=prefix)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/core.py\", line 579, in _lsdir\n    s3 = await self.get_s3(bucket)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/core.py\", line 247, in get_s3\n    return await self._s3creator.get_bucket_client(bucket)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/utils.py\", line 65, in get_bucket_client\n    response = await general_client.head_bucket(Bucket=bucket_name)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/client.py\", line 174, in _make_api_call\n    operation_model, request_dict, request_context)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/client.py\", line 193, in _make_request\n    return await self._endpoint.make_request(operation_model, request_dict)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/endpoint.py\", line 83, in _send_request\n    exception):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/endpoint.py\", line 216, in _needs_retry\n    caught_exception=caught_exception, request_dict=request_dict)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/hooks.py\", line 29, in _emit\n    response = handler(**kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/botocore\/retryhandler.py\", line 189, in __call__\n    retries_context = kwargs['request_dict']['context']['retries']\nKeyError: 'retries'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/utils.py\", line 28, in _link\n    func(from_path, to_path)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/local.py\", line 148, in reflink\n    System.reflink(from_info, to_info)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/system.py\", line 112, in reflink\n    System._reflink_linux(source, link_name)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/system.py\", line 96, in _reflink_linux\n    fcntl.ioctl(d.fileno(), FICLONE, s.fileno())\nOSError: [Errno 18] Invalid cross-device link\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/utils.py\", line 69, in _try_links\n    return _link(link, from_fs, from_path, to_fs, to_path)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/utils.py\", line 34, in _link\n    ) from exc\nOSError: [Errno 95] 'reflink' is not supported by &lt;class 'dvc.fs.local.LocalFileSystem'&gt;\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/utils.py\", line 124, in _test_link\n    _try_links([link], from_fs, from_file, to_fs, to_file)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/utils.py\", line 79, in _try_links\n    ) from error\nOSError: [Errno 95] no more link types left to try out\n------------------------------------------------------------\n2022-02-10 10:43:03,118 DEBUG: Removing '\/usr\/src\/.6xKEDniecvveNNhe48SRTv.tmp'\n2022-02-10 10:43:03,126 DEBUG: [Errno 95] no more link types left to try out: [Errno 95] 'hardlink' is not supported by &lt;class 'dvc.fs.local.LocalFileSystem'&gt;: [Errno 18] Invalid cross-device link: '\/usr\/src\/app\/.dvc\/cache\/.Yo26z2kNKRkG4gowQEitca.tmp' -&gt; '\/usr\/src\/.6xKEDniecvveNNhe48SRTv.tmp'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/main.py\", line 55, in main\n    ret = cmd.do_run()\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/command\/base.py\", line 45, in do_run\n    return self.run()\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/command\/data_sync.py\", line 67, in run\n    glob=self.args.glob,\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/repo\/__init__.py\", line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/repo\/push.py\", line 57, in push\n    obj_ids, jobs, remote=remote, odb=odb or dest_odb\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/data_cloud.py\", line 91, in push\n    cache_odb=self.repo.odb.local,\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/transfer.py\", line 153, in transfer\n    status = compare_status(src, dest, obj_ids, check_deleted=False, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/status.py\", line 159, in compare_status\n    dest, obj_ids, index=dest_index, **kwargs\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/status.py\", line 131, in status\n    exists.update(odb.hashes_exist(hashes, name=odb.fs_path, **kwargs))\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 474, in hashes_exist\n    remote_size, remote_hashes = self._estimate_remote_size(hashes, name)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 271, in _estimate_remote_size\n    remote_hashes = set(hashes)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 225, in _hashes_with_limit\n    for hash_ in self._list_hashes(prefix, progress_callback):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 215, in _list_hashes\n    for path in self._list_paths(prefix, progress_callback):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/objects\/db\/base.py\", line 195, in _list_paths\n    for file_info in self.fs.find(fs_path, prefix=prefix):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/fsspec_wrapper.py\", line 179, in find\n    files = self.fs.find(with_prefix, prefix=self.path.parts(path)[-1])\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/fsspec\/asyn.py\", line 91, in wrapper\n    return sync(self.loop, func, *args, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/fsspec\/asyn.py\", line 71, in sync\n    raise return_result\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/fsspec\/asyn.py\", line 25, in _runner\n    result[0] = await coro\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/core.py\", line 665, in _find\n    out = await self._lsdir(path, delimiter=\"\", prefix=prefix)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/core.py\", line 579, in _lsdir\n    s3 = await self.get_s3(bucket)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/core.py\", line 247, in get_s3\n    return await self._s3creator.get_bucket_client(bucket)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/s3fs\/utils.py\", line 65, in get_bucket_client\n    response = await general_client.head_bucket(Bucket=bucket_name)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/client.py\", line 174, in _make_api_call\n    operation_model, request_dict, request_context)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/client.py\", line 193, in _make_request\n    return await self._endpoint.make_request(operation_model, request_dict)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/endpoint.py\", line 83, in _send_request\n    exception):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/endpoint.py\", line 216, in _needs_retry\n    caught_exception=caught_exception, request_dict=request_dict)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/aiobotocore\/hooks.py\", line 29, in _emit\n    response = handler(**kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/botocore\/retryhandler.py\", line 189, in __call__\n    retries_context = kwargs['request_dict']['context']['retries']\nKeyError: 'retries'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/utils.py\", line 28, in _link\n    func(from_path, to_path)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/local.py\", line 141, in hardlink\n    System.hardlink(from_info, to_info)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/system.py\", line 39, in hardlink\n    os.link(src, link_name)\nOSError: [Errno 18] Invalid cross-device link: '\/usr\/src\/app\/.dvc\/cache\/.Yo26z2kNKRkG4gowQEitca.tmp' -&gt; '\/usr\/src\/.6xKEDniecvveNNhe48SRTv.tmp'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/utils.py\", line 69, in _try_links\n    return _link(link, from_fs, from_path, to_fs, to_path)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/utils.py\", line 34, in _link\n    ) from exc\nOSError: [Errno 95] 'hardlink' is not supported by &lt;class 'dvc.fs.local.LocalFileSystem'&gt;\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/utils.py\", line 124, in _test_link\n    _try_links([link], from_fs, from_file, to_fs, to_file)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/dvc\/fs\/utils.py\", line 79, in _try_links\n    ) from error\nOSError: [Errno 95] no more link types left to try out\n------------------------------------------------------------\n2022-02-10 10:43:03,127 DEBUG: Removing '\/usr\/src\/.6xKEDniecvveNNhe48SRTv.tmp'\n2022-02-10 10:43:03,128 DEBUG: Removing '\/usr\/src\/.6xKEDniecvveNNhe48SRTv.tmp'\n2022-02-10 10:43:03,131 DEBUG: Removing '\/usr\/src\/app\/.dvc\/cache\/.Yo26z2kNKRkG4gowQEitca.tmp'\n2022-02-10 10:43:03,177 DEBUG: Version info for developers:\nDVC version: 2.9.3 (pip)\n---------------------------------\nPlatform: Python 3.7.4 on Linux-5.10.16.3-microsoft-standard-WSL2-x86_64-with-debian-10.1\nSupports:\n        webhdfs (fsspec = 2022.1.0),\n        http (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n        https (aiohttp = 3.8.1, aiohttp-retry = 2.4.6),\n        s3 (s3fs = 2022.1.0, boto3 = 1.20.49)\nCache types: symlink\nCache directory: 9p on C:\\\nCaches: local\nRemotes: s3\nWorkspace directory: 9p on C:\\\nRepo: dvc, git\n\nHaving any troubles? Hit us up at https:\/\/dvc.org\/support, we are always happy to help!\n2022-02-10 10:43:03,185 DEBUG: Analytics is enabled.\n2022-02-10 10:43:03,276 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmpfbhas6pl']'\n2022-02-10 10:43:03,277 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmpfbhas6pl']'\n<\/code><\/pre>\n<p>dvc finished pushing \u2026<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error unexpect error retri run insid docker contain command python script insid contain configur repo remot storag remot credenti run script contain work perfectli contain rais error come push script error unexpect error retri error obvious inform need help",
        "Question_preprocessed_content":"error unexpect error retri run insid docker contain command python script insid contain configur repo remot storag remot credenti run script contain work perfectli contain rais error come push script error unexpect error retri error obvious inform need help",
        "Answer_original_content":"share output push itll error traceback log inform thank complet trace command line run docker contain push start push data debug ad usr src app config local gitignor file debug ad usr src app tmp gitignor file debug ad usr src app cach gitignor file debug prepar transfer data usr src app cach drayd pipelin dev data debug prepar collect statu drayd pipelin dev data debug collect statu drayd pipelin dev data error unexpect error retri traceback recent file usr local lib python site packag main line main ret cmd run file usr local lib python site packag command base line run return self run file usr local lib python site packag command data sync line run glob self arg glob file usr local lib python site packag repo init line wrapper return repo arg kwarg file usr local lib python site packag repo push line push obj id job remot remot odb odb dest odb file usr local lib python site packag data cloud line push cach odb self repo odb local file usr local lib python site packag object transfer line transfer statu compar statu src dest obj id check delet fals kwarg file usr local lib python site packag object statu line compar statu dest obj id index dest index kwarg file usr local lib python site packag object statu line statu exist updat odb hash exist hash odb path kwarg file usr local lib python site packag object base line hash exist remot size remot hash self estim remot size hash file usr local lib python site packag object base line estim remot size remot hash set hash file usr local lib python site packag object base line hash limit hash self list hash prefix progress callback file usr local lib python site packag object base line list hash path self list path prefix progress callback file usr local lib python site packag object base line list path file info self path prefix prefix file usr local lib python site packag fsspec wrapper line file self prefix prefix self path part path file usr local lib python site packag fsspec asyn line wrapper return sync self loop func arg kwarg file usr local lib python site packag fsspec asyn line sync rais return result file usr local lib python site packag fsspec asyn line runner result await coro file usr local lib python site packag sf core line await self lsdir path delimit prefix prefix file usr local lib python site packag sf core line lsdir await self bucket file usr local lib python site packag sf core line return await self screator bucket client bucket file usr local lib python site packag sf util line bucket client respons await gener client head bucket bucket bucket file usr local lib python site packag aiobotocor client line api oper model request dict request context file usr local lib python site packag aiobotocor client line request return await self endpoint request oper model request dict file usr local lib python site packag aiobotocor endpoint line send request except file usr local lib python site packag aiobotocor endpoint line need retri caught except caught except request dict request dict file usr local lib python site packag aiobotocor hook line emit respons handler kwarg file usr local lib python site packag botocor retryhandl line retri context kwarg request dict context retri keyerror retri debug ad usr src app config local gitignor file debug ad usr src app tmp gitignor file debug ad usr src app cach gitignor file debug errno link type left try errno reflink support errno invalid cross devic link traceback recent file usr local lib python site packag main line main ret cmd run file usr local lib python site packag command base line run return self run file usr local lib python site packag command data sync line run glob self arg glob file usr local lib python site packag repo init line wrapper return repo arg kwarg file usr local lib python site packag repo push line push obj id job remot remot odb odb dest odb file usr local lib python site packag data cloud line push cach odb self repo odb local file usr local lib python site packag object transfer line transfer statu compar statu src dest obj id check delet fals kwarg file usr local lib python site packag object statu line compar statu dest obj id index dest index kwarg file usr local lib python site packag object statu line statu exist updat odb hash exist hash odb path kwarg file usr local lib python site packag object base line hash exist remot size remot hash self estim remot size hash file usr local lib python site packag object base line estim remot size remot hash set hash file usr local lib python site packag object base line hash limit hash self list hash prefix progress callback file usr local lib python site packag object base line list hash path self list path prefix progress callback file usr local lib python site packag object base line list path file info self path prefix prefix file usr local lib python site packag fsspec wrapper line file self prefix prefix self path part path file usr local lib python site packag fsspec asyn line wrapper return sync self loop func arg kwarg file usr local lib python site packag fsspec asyn line sync rais return result file usr local lib python site packag fsspec asyn line runner result await coro file usr local lib python site packag sf core line await self lsdir path delimit prefix prefix file usr local lib python site packag sf core line lsdir await self bucket file usr local lib python site packag sf core line return await self screator bucket client bucket file usr local lib python site packag sf util line bucket client respons await gener client head bucket bucket bucket file usr local lib python site packag aiobotocor client line api oper model request dict request context file usr local lib python site packag aiobotocor client line request return await self endpoint request oper model request dict file usr local lib python site packag aiobotocor endpoint line send request except file usr local lib python site packag aiobotocor endpoint line need retri caught except caught except request dict request dict file usr local lib python site packag aiobotocor hook line emit respons handler kwarg file usr local lib python site packag botocor retryhandl line retri context kwarg request dict context retri keyerror retri handl except except occur traceback recent file usr local lib python site packag util line link func path path file usr local lib python site packag local line reflink reflink info info file usr local lib python site packag line reflink reflink linux sourc link file usr local lib python site packag line reflink linux fcntl ioctl fileno ficlon fileno oserror errno invalid cross devic link except direct caus follow except traceback recent file usr local lib python site packag util line try link return link link path path file usr local lib python site packag util line link exc oserror errno reflink support except direct caus follow except traceback recent file usr local lib python site packag util line test link try link link file file file usr local lib python site packag util line try link error oserror errno link type left try debug remov usr src xkedniecvvennhesrtv tmp debug errno link type left try errno hardlink support errno invalid cross devic link usr src app cach yozknkrkggowqeitca tmp usr src xkedniecvvennhesrtv tmp traceback recent file usr local lib python site packag main line main ret cmd run file usr local lib python site packag command base line run return self run file usr local lib python site packag command data sync line run glob self arg glob file usr local lib python site packag repo init line wrapper return repo arg kwarg file usr local lib python site packag repo push line push obj id job remot remot odb odb dest odb file usr local lib python site packag data cloud line push cach odb self repo odb local file usr local lib python site packag object transfer line transfer statu compar statu src dest obj id check delet fals kwarg file usr local lib python site packag object statu line compar statu dest obj id index dest index kwarg file usr local lib python site packag object statu line statu exist updat odb hash exist hash odb path kwarg file usr local lib python site packag object base line hash exist remot size remot hash self estim remot size hash file usr local lib python site packag object base line estim remot size remot hash set hash file usr local lib python site packag object base line hash limit hash self list hash prefix progress callback file usr local lib python site packag object base line list hash path self list path prefix progress callback file usr local lib python site packag object base line list path file info self path prefix prefix file usr local lib python site packag fsspec wrapper line file self prefix prefix self path part path file usr local lib python site packag fsspec asyn line wrapper return sync self loop func arg kwarg file usr local lib python site packag fsspec asyn line sync rais return result file usr local lib python site packag fsspec asyn line runner result await coro file usr local lib python site packag sf core line await self lsdir path delimit prefix prefix file usr local lib python site packag sf core line lsdir await self bucket file usr local lib python site packag sf core line return await self screator bucket client bucket file usr local lib python site packag sf util line bucket client respons await gener client head bucket bucket bucket file usr local lib python site packag aiobotocor client line api oper model request dict request context file usr local lib python site packag aiobotocor client line request return await self endpoint request oper model request dict file usr local lib python site packag aiobotocor endpoint line send request except file usr local lib python site packag aiobotocor endpoint line need retri caught except caught except request dict request dict file usr local lib python site packag aiobotocor hook line emit respons handler kwarg file usr local lib python site packag botocor retryhandl line retri context kwarg request dict context retri keyerror retri handl except except occur traceback recent file usr local lib python site packag util line link func path path file usr local lib python site packag local line hardlink hardlink info info file usr local lib python site packag line hardlink link src link oserror errno invalid cross devic link usr src app cach yozknkrkggowqeitca tmp usr src xkedniecvvennhesrtv tmp except direct caus follow except traceback recent file usr local lib python site packag util line try link return link link path path file usr local lib python site packag util line link exc oserror errno hardlink support except direct caus follow except traceback recent file usr local lib python site packag util line test link try link link file file file usr local lib python site packag util line try link error oserror errno link type left try debug remov usr src xkedniecvvennhesrtv tmp debug remov usr src xkedniecvvennhesrtv tmp debug remov usr src app cach yozknkrkggowqeitca tmp debug version info develop version pip platform python linux microsoft standard wsl debian support webhdf fsspec http aiohttp aiohttp retri http aiohttp aiohttp retri sf boto cach type symlink cach directori cach local remot workspac directori repo git have troubl hit http org support happi help debug analyt enabl debug try spawn daemon analyt tmp tmpfbhaspl debug spawn daemon analyt tmp tmpfbhaspl finish push",
        "Answer_preprocessed_content":"share output itll error traceback log inform thank complet trace command line run docker contain push start push data finish push",
        "Question_gpt_summary_original":"the user encountered an unexpected error - 'retries' - when running a python script from inside a docker container, which worked perfectly when run outside the container.",
        "Question_gpt_summary":"user encount unexpect error retri run python script insid docker contain work perfectli run outsid contain",
        "Answer_gpt_summary_original":"the possible solution to the unexpected error encountered when running a python script from inside a docker container is to share the output of push -v. this will provide more logging information and the error traceback, which can help in identifying the root cause of the issue.",
        "Answer_gpt_summary":"possibl solut unexpect error encount run python script insid docker contain share output push provid log inform error traceback help identifi root caus issu"
    },
    {
        "Question_id":null,
        "Question_title":"Dvc and S3 permissions management",
        "Question_body":"<p>Hello dvc people!<\/p>\n<p>At my team we are experimenting with dvc as an interface to work with trained models and datasets. So far it has been working nicely, but we found a mayor roadblock when it comes to permissions management.<\/p>\n<p>In our ideal scenario, when a teammate gets access to one of our repositories in GitHub that person will only have access to the data from that particular repository. This permission should be given automatically by the fact that the person has access to the repository, with no need to modify roles in AWS.<\/p>\n<p>However, this seems challenging to achieve through S3 + DVC. Our considered solution right now is to have a single S3 bucket with all our DVC repositories. If we grant access to all teammates to the whole bucket it would mean that they will have access to all the company\u2019s data, which is less than ideal. So we were considering having a \u201cJunior\u201d role which we will have to give access manually to whatever specific S3 folders they need and a \u201cSenior\u201d role with access to everything.<\/p>\n<p>This solution is suboptimal as we will need to handle different permissions for GitHub and for S3, which adds overhead.<\/p>\n<p>Is there any other pattern we are missing here? Is there any easy way for a teammate to be given access to ONLY the dvc remote from the GitHub repository automatically?<\/p>\n<p>Thanks for the help <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/relaxed.png?v=10\" title=\":relaxed:\" class=\"emoji\" alt=\":relaxed:\"><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1631267338282,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":484.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dvc-and-s3-permissions-management\/887",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-10T17:48:06.234Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/javi\">@Javi<\/a>, thank you for asking! This is a very good question and two possible solutions:<\/p>\n<ol>\n<li>\n<strong>Workflow level<\/strong>. You can allow <code>write<\/code>\/<code>push<\/code> access to the bucket only for bots\/virtual-users that will push data to the bucket through CI\/CD only after approved pull requests in GitHub. While all the users will have <code>read<\/code>\/<code>pull<\/code> access from the bucket.<\/li>\n<li>\n<strong>User level<\/strong>. The way you describe it. We are in the design stage for this feature. It would be great if you can help us with the design and requirements.<\/li>\n<\/ol>\n<p>I\u2019d be happy to help you and go deeper into the solutions. Please let me know if you are open to a chat - please shoot me a Hi email to my-first-name at iterative.ai<\/p>",
                "Answer_score":62.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"permiss manag hello peopl team experi interfac work train model dataset far work nice mayor roadblock come permiss manag ideal scenario teammat get access repositori github person access data particular repositori permiss given automat fact person access repositori need modifi role aw challeng achiev consid solut right singl bucket repositori grant access teammat bucket mean access compani data ideal consid have junior role access manual specif folder need senior role access solut suboptim need handl differ permiss github add overhead pattern miss easi wai teammat given access remot github repositori automat thank help",
        "Question_preprocessed_content":"permiss manag hello peopl team experi interfac work train model dataset far work nice mayor roadblock come permiss manag ideal scenario teammat get access repositori github person access data particular repositori permiss given automat fact person access repositori need modifi role aw challeng achiev consid solut right singl bucket repositori grant access teammat bucket mean access compani data ideal consid have junior role access manual specif folder need senior role access solut suboptim need handl differ permiss github add overhead pattern miss easi wai teammat given access remot github repositori automat thank help",
        "Answer_original_content":"javi thank ask good question possibl solut workflow level allow write push access bucket bot virtual user push data bucket approv pull request github user read pull access bucket user level wai design stage featur great help design requir happi help deeper solut let know open chat shoot email iter",
        "Answer_preprocessed_content":"thank ask good question possibl solut workflow level allow access bucket push data bucket approv pull request github user access bucket user level wai design stage featur great help design requir happi help deeper solut let know open chat shoot email",
        "Question_gpt_summary_original":"the user is facing a challenge in managing permissions for their team's use of s3 in conjunction with github, as they need to find a way to automatically grant access to only the data from a particular repository when a teammate is given access to it.",
        "Question_gpt_summary":"user face challeng manag permiss team us conjunct github need wai automat grant access data particular repositori teammat given access",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions to the challenge of managing permissions for s3 and github integration. the first solution involves allowing write\/push access to the bucket only for bots\/virtual-users that will push data to the bucket through ci\/cd only after approved pull requests in github. the second solution is still in the design stage, and the user is invited to help with the design and requirements.",
        "Answer_gpt_summary":"answer suggest possibl solut challeng manag permiss github integr solut involv allow write push access bucket bot virtual user push data bucket approv pull request github second solut design stage user invit help design requir"
    },
    {
        "Question_id":null,
        "Question_title":"Getting ConnectTimeout in offline mode when trying to log an image",
        "Question_body":"<p>I am running wandb in offline mode since I don\u2019t have an internet connection on the compute nodes that I use for my experiments.<br>\nThis works fine when I\u2019m logging training loss and other things.<br>\nWhen I try to log images, however, I get the following warning <code>wandb: Network error (ConnectTimeout), entering retry loop.<\/code>  and the run waits forever.<\/p>\n<p>The logging happens through:<\/p>\n<pre><code class=\"lang-python\">images = []\nfor i in range(10):\n    images.append(wandb.Image(image[i], caption=f\"{caption}.{i}\"))\nwandb.log({category: images})\n<\/code><\/pre>\n<p>I can even see that the offline mode is active since I get the following output when I stop the run:<\/p>\n<pre><code class=\"lang-auto\">wandb: You can sync this run to the cloud by running:\nwandb: wandb sync \/scratch_emmy\/outputs\/wandb\/offline-run-20230208_142336-9685bcf5ea8d5d35ccc9d93b2d035832\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1675863242911,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/getting-connecttimeout-in-offline-mode-when-trying-to-log-an-image\/3844",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-10T19:43:42.600Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/arnenix\">@arnenix<\/a> , happy to look into this for you. I ran a test on my end and did not run into the same issue you face. I was successful in logging i<a href=\"https:\/\/wandb.ai\/mohammadbakir\/jira-offline-imgs\/runs\/1fae84wm\/overview?workspace=user-mohammadbakir\">mages offline then syncing the run to wandb<\/a> using the same code example you provided.<\/p>\n<p>Which version of wandb are you using? If not <a href=\"https:\/\/github.com\/wandb\/wandb\/releases\" rel=\"noopener nofollow ugc\">our latest<\/a>,  upgrade and try again.  If the problem persists, provide me a copy of your <code>debug.log<\/code> and <code>debug-internal.log<\/code> files of the runs that are failing. These are located in the run\/logs folder under the wandb folder of your working directory. Thanks<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-15T23:38:10.521Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/arnenix\">@arnenix<\/a> , since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":5.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-21T07:37:56.683Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/mohammadbakir\">@mohammadbakir<\/a> ,<\/p>\n<p>sorry for not responding earlier.<br>\nI also opened an issue on github and they were able to reproduce my issue and are working on a fix.<br>\nFor further info see: <a href=\"https:\/\/github.com\/wandb\/wandb\/issues\/4946\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">[CLI]: Getting ConnectTimeout in offline mode when trying to log an image \u00b7 Issue #4946 \u00b7 wandb\/wandb \u00b7 GitHub<\/a><\/p>\n<p>I think this thread can be closed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get connecttimeout offlin mode try log imag run offlin mode dont internet connect comput node us experi work fine log train loss thing try log imag follow warn network error connecttimeout enter retri loop run wait forev log happen imag rang imag append imag imag caption caption log categori imag offlin mode activ follow output stop run sync run cloud run sync scratch emmi output offlin run bcfeadddbd",
        "Question_preprocessed_content":"get connecttimeout offlin mode try log imag run offlin mode dont internet connect comput node us experi work fine log train loss thing try log imag follow warn run wait forev log happen offlin mode activ follow output stop run",
        "Answer_original_content":"arnenix happi look ran test end run issu face success log imag offlin sync run code exampl provid version latest upgrad try problem persist provid copi debug log debug intern log file run fail locat run log folder folder work directori thank arnenix heard go close request like open convers let know mohammadbakir sorri respond earlier open issu github abl reproduc issu work fix info cli get connecttimeout offlin mode try log imag issu github think thread close",
        "Answer_preprocessed_content":"happi look ran test end run issu face success log imag offlin sync run code exampl provid version latest upgrad try problem persist provid copi file run fail locat folder folder work directori thank heard go close request like convers let know sorri respond earlier open issu github abl reproduc issu work fix info get connecttimeout offlin mode try log imag issu github think thread close",
        "Question_gpt_summary_original":"the user is encountering a connecttimeout error when attempting to log images while running in offline mode.",
        "Question_gpt_summary":"user encount connecttimeout error attempt log imag run offlin mode",
        "Answer_gpt_summary_original":"possible solutions to the connecttimeout error when attempting to log images while running in offline mode are: upgrading to the latest version, providing debug.log and debug-internal.log files of the failing runs, and checking the issue on github for further information.",
        "Answer_gpt_summary":"possibl solut connecttimeout error attempt log imag run offlin mode upgrad latest version provid debug log debug intern log file fail run check issu github inform"
    },
    {
        "Question_id":null,
        "Question_title":"Invalid artifact",
        "Question_body":"<p>When trying to add a reference to an object stored on s3 I got a :<\/p>\n<pre><code class=\"lang-auto\">CommError: Invalid artifact path: s3:\/\/my_bucket_names\/my_lang_folder\/my_subfolder\/my_file.json\n<\/code><\/pre>\n<p>I used :<\/p>\n<pre><code class=\"lang-auto\"> self.run = wandb.init(\n     project=\"my_project\",\n     entity=\"my_group_name\",\n      dir=self.temp_dir,\n      resume=bool(config.previous_model),\n )\n\n for dataset in config.datasets:\n    self.run.use_artifact(\n        f\"s3:\/\/my_bucket_name\/{make_path(dataset['lang'],dataset['type'],[],dataset['name'],'json')}\",\n        type=\"train_dataset\",\n    )\n<\/code><\/pre>\n<p>I\u2019m sure the dataset exists in this bucket. However, it\u2019s a private bucket but I have access to it with my credentials.<\/p>\n<p>Thanks in advance for any help.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1635863488014,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":322.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/invalid-artifact\/1198",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-02T14:47:32.463Z",
                "Answer_body":"<p>Note for the posterity:<\/p>\n<p>I managed to make it work by firstly creating an Artifact like so :<\/p>\n<pre><code class=\"lang-auto\">artifact = wandb.Artifact(\"dataset_name\", type=\"train_dataset\")\nartifact.add_reference(f\"s3:\/\/my_bucket\/{make_path(dataset_metas['lang'],dataset_metas['type'],[],dataset_metas['name'],'json')}\", name=\"dataset_name\")\nself.run.use_artifact(artifact)\n<\/code><\/pre>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-02T14:59:36.564Z",
                "Answer_body":"<p>I\u2019m glad you were able to solve your problem. Have a good rest of the day<\/p>",
                "Answer_score":16.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-01T14:48:30.705Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"invalid artifact try add refer object store got commerror invalid artifact path bucket name lang folder subfold file json self run init project project entiti group dir self temp dir resum bool config previou model dataset config dataset self run us artifact bucket path dataset lang dataset type dataset json type train dataset sure dataset exist bucket privat bucket access credenti thank advanc help",
        "Question_preprocessed_content":"invalid artifact try add refer object store got sure dataset exist bucket privat bucket access credenti thank advanc help",
        "Answer_original_content":"note poster manag work firstli creat artifact like artifact artifact dataset type train dataset artifact add refer bucket path dataset meta lang dataset meta type dataset meta json dataset self run us artifact artifact glad abl solv problem good rest dai topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"note poster manag work firstli creat artifact like glad abl solv problem good rest dai topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user encountered a challenge when trying to add a reference to an object stored on s3, resulting in an \"invalid artifact path\" error.",
        "Question_gpt_summary":"user encount challeng try add refer object store result invalid artifact path error",
        "Answer_gpt_summary_original":"the solution to the \"invalid artifact path\" error when trying to add a reference to an object stored on s3 is to create an artifact and add a reference to the s3 object using the correct path. the code snippet provided in the answer shows how to create an artifact and add a reference to the s3 object.",
        "Answer_gpt_summary":"solut invalid artifact path error try add refer object store creat artifact add refer object correct path code snippet provid answer show creat artifact add refer object"
    },
    {
        "Question_id":null,
        "Question_title":"Error on trying to run dvc pipeline",
        "Question_body":"<p>I\u2019m trying to run the training stage of a machine learning pipeline on dvc with the following command(s):<\/p>\n<pre><code class=\"lang-auto\">dvc run -n train \\ \n-d data\/census_clean.csv -d starter\/train_model.py \\ \n-o model\/model.joblib -o model\/encoder.joblib -o model\/lb.joblib \\\npython starter\/train_model.py\n\n<\/code><\/pre>\n<p>But this throws up the following error everytime and the run fails to start.<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/06e6383b7e3e4f59a41f78bd245925f207064081.png\" alt=\"Screenshot 2021-11-29 at 10.13.50 AM\" data-base62-sha1=\"Z26HMFZc92veBXN62ytKqjdtTP\" width=\"474\" height=\"142\"><\/p>\n<p>I\u2019m not sure where the mistake on my part is - the same error repeats if I try to run without the output commands, and not specifying the params since hyerparameter tuning is not required at this stage. Please let me know what I\u2019m missing.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1638161185366,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":364.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-on-trying-to-run-dvc-pipeline\/989",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-29T10:28:54.292Z",
                "Answer_body":"<p>Hi. Looks like you might be using python 3.9.8, which has a bug in argparser. The solution is to simply use some other python version. Or you could use one of our binary packages (e.g. brew, snap, conda, exe, osxpkg, etc). Please show <code>dvc doctor<\/code> output so we could confirm that this is indeed what is going on.<\/p>",
                "Answer_score":10.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-29T16:16:42.208Z",
                "Answer_body":"<p>Hi<\/p>\n<p>Yes you\u2019re right about the Python version<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/daa681664d0f6abe3293f56486b4e125fd4a04de.png\" data-download-href=\"\/uploads\/short-url\/vcgP8P5WZEMhMAio9WKnBLIAOrc.png?dl=1\" title=\"Screenshot 2021-11-29 at 9.45.04 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/daa681664d0f6abe3293f56486b4e125fd4a04de_2_690x475.png\" alt=\"Screenshot 2021-11-29 at 9.45.04 PM\" data-base62-sha1=\"vcgP8P5WZEMhMAio9WKnBLIAOrc\" width=\"690\" height=\"475\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/daa681664d0f6abe3293f56486b4e125fd4a04de_2_690x475.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/daa681664d0f6abe3293f56486b4e125fd4a04de_2_1035x712.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/daa681664d0f6abe3293f56486b4e125fd4a04de.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/daa681664d0f6abe3293f56486b4e125fd4a04de_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2021-11-29 at 9.45.04 PM<\/span><span class=\"informations\">1328\u00d7916 112 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-29T16:51:46.604Z",
                "Answer_body":"<p>And switching to a different Python version worked, thanks a lot for the help.<\/p>",
                "Answer_score":45.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error try run pipelin try run train stage machin learn pipelin follow command run train data censu clean csv starter train model model model joblib model encod joblib model joblib python starter train model throw follow error everytim run fail start sure mistak error repeat try run output command specifi param hyerparamet tune requir stage let know miss thank",
        "Question_preprocessed_content":"error try run pipelin try run train stage machin learn pipelin follow command throw follow error everytim run fail start sure mistak error repeat try run output command specifi param hyerparamet tune requir stage let know miss thank",
        "Answer_original_content":"look like python bug argpars solut simpli us python version us binari packag brew snap conda ex osxpkg doctor output confirm go ye your right python version screenshot switch differ python version work thank lot help",
        "Answer_preprocessed_content":"look like python bug argpars solut simpli us python version us binari packag output confirm go ye your right python version screenshot switch differ python version work thank lot help",
        "Question_gpt_summary_original":"the user is encountering an error when trying to run a machine learning pipeline and is unsure of the cause.",
        "Question_gpt_summary":"user encount error try run machin learn pipelin unsur caus",
        "Answer_gpt_summary_original":"possible solutions to the error encountered when running a machine learning pipeline include switching to a different version of python or using one of the available binary packages such as brew, snap, conda, exe, or osxpkg. the user should also provide the doctor output to confirm that the issue is related to the python version.",
        "Answer_gpt_summary":"possibl solut error encount run machin learn pipelin includ switch differ version python avail binari packag brew snap conda ex osxpkg user provid doctor output confirm issu relat python version"
    },
    {
        "Question_id":58035744.0,
        "Question_title":"AML run.log() and run.log_list() fail without error",
        "Question_body":"<p>I have a Pipeline with DatabricksSteps each containing:<\/p>\n\n<pre><code>from azureml.core.run import Run\nrun = Run.get_context()\n#do stuff\nrun.log(name, val, desc)\nrun.log_list(name, vals, desc)\nrun.log_image(title, fig, desc)\n<\/code><\/pre>\n\n<p>Only <code>log_image()<\/code> seems to work.  The image appears in the \"images\" section of the AML experiment workspace as expected, but the \"tracked metrics\" and \"charts\" areas are blank.  In an interactive job, <code>run.log()<\/code> and <code>run.log_list()<\/code> work as expected.  I tested that there is no problem with the arguments by using <code>print()<\/code> instead of <code>run.log()<\/code>.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1569018204223,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":121.0,
        "Answer_body":"<p>Add run.flush() at the end of the script.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58035744",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1569427861030,
        "Question_original_content":"aml run log run log list fail error pipelin databricksstep contain core run import run run run context stuff run log val desc run log list val desc run log imag titl fig desc log imag work imag appear imag section aml experi workspac expect track metric chart area blank interact job run log run log list work expect test problem argument print instead run log",
        "Question_preprocessed_content":"aml fail error pipelin databricksstep contain work imag appear imag section aml experi workspac expect track metric chart area blank interact job work expect test problem argument instead",
        "Answer_original_content":"add run flush end script",
        "Answer_preprocessed_content":"add end script",
        "Question_gpt_summary_original":"the user is encountering challenges with the aml run.log() and run.log_list() functions, which are not producing any errors but are not logging the expected metrics and charts in the aml experiment workspace.",
        "Question_gpt_summary":"user encount challeng aml run log run log list function produc error log expect metric chart aml experi workspac",
        "Answer_gpt_summary_original":"the solution to the challenge of the aml run.log() and run.log_list() functions not logging expected metrics and charts in the aml experiment workspace is to add run.flush() at the end of the script.",
        "Answer_gpt_summary":"solut challeng aml run log run log list function log expect metric chart aml experi workspac add run flush end script"
    },
    {
        "Question_id":null,
        "Question_title":"What factors affect Sagemaker endpoint response time?",
        "Question_body":"I'm using Sagemaker as part of a planned deployment of a XGBoost model to production where it will be called by a customer facing mobile app (via another back end service that we also have hosted in AWS).\n\nI would like to understand how to improve response times. I have tested the response time of my model both when it resides locally on my own dev machine, as well as when its running in Sagemaker.\n\nThe wall time for local atomic predictions takes about 1 ms at 50p and 7 ms at 99p.\n\nThe wall time for atomic predictions (using the Python client SDK in a sagemaker notebook) takes about 20 ms at 50p and 25 ms at 99p. However, there are outliers that take as long as ~300ms.\n\nI am curious to know what factors affect the performance of Sagemaker calls (other than the complexity of the model itself). And I would be very grateful for any tips to get our outliers lower (preferably around 50 ms if possible).",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1550614607000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":319.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUraUOZYBITMGxNh339cctOA\/what-factors-affect-sagemaker-endpoint-response-time",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-02-19T23:48:45.000Z",
                "Answer_score":0,
                "Answer_body":"Hi bradmitchell,\nSageMaker Endpoints are managed hosted solution which has layered routing internally. You could gain some additional insights into the system with the OverheadLatency metric. You will also see the ModelLatency metric which will show what the time taken by customer model itself -- https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/monitoring-cloudwatch.html.\n\nWhat kind of TPS are you driving against your Endpoint? (In a low request rate situation, it is possible that the caches on our side are not kept warm and you might observe increased latencies). I believe the above metrics will help you with more details.\n\nThank you,\nArun",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-02-20T15:40:25.000Z",
                "Answer_score":0,
                "Answer_body":"Hi Arun,\n\nThank you for suggesting the Cloudwatch metrics. I just checked the Overhead Latency and it lines up pretty closely with the average timings I got in a Sagemaker Jupyter Notebook using the Python client SDK to invoke the endpoint.\n\nCurrently, I am expecting about 2.5 transactions per second during peak usage and 0.5 TPS during low traffic hours. The peak number will hopefully go up to around 10 TPS later this year.\n\nThe cache explanation makes a lot of sense. I've experimented with a few different TPS settings. There is pretty consistently some spiking of timings to start off but then it levels off to around 20 ms for higher TPS. In lower TPS experiments I've noticed that the timings remain a little unpredictable.\n\nThank you again for your help!\n\nBrad",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"factor affect endpoint respons time plan deploy xgboost model product call custom face mobil app end servic host aw like understand improv respons time test respons time model resid local dev machin run wall time local atom predict take wall time atom predict python client sdk notebook take outlier long curiou know factor affect perform call complex model grate tip outlier lower prefer possibl",
        "Question_preprocessed_content":"factor affect endpoint respons time plan deploy xgboost model product call custom face mobil app like understand improv respons time test respons time model resid local dev machin run wall time local atom predict take wall time atom predict take outlier long curiou know factor affect perform call grate tip outlier lower",
        "Answer_original_content":"bradmitchel endpoint manag host solut layer rout intern gain addit insight overheadlat metric modellat metric time taken custom model http doc aw amazon com latest monitor cloudwatch html kind tp drive endpoint low request rate situat possibl cach kept warm observ increas latenc believ metric help detail thank arun arun thank suggest cloudwatch metric check overhead latenc line pretti close averag time got jupyt notebook python client sdk invok endpoint current expect transact second peak usag tp low traffic hour peak number hopefulli tp later year cach explan make lot sens experi differ tp set pretti consist spike time start level higher tp lower tp experi notic time remain littl unpredict thank help brad",
        "Answer_preprocessed_content":"bradmitchel endpoint manag host solut layer rout intern gain addit insight overheadlat metric modellat metric time taken custom model kind tp drive endpoint believ metric help detail thank arun arun thank suggest cloudwatch metric check overhead latenc line pretti close averag time got jupyt notebook python client sdk invok endpoint current expect transact second peak usag tp low traffic hour peak number hopefulli tp later year cach explan make lot sens experi differ tp set pretti consist spike time start level higher tp lower tp experi notic time remain littl unpredict thank help brad",
        "Question_gpt_summary_original":"the user is looking to understand what factors affect the performance of endpoint calls to their deployed xgboost model, and is seeking tips to reduce outliers to around 50 ms.",
        "Question_gpt_summary":"user look understand factor affect perform endpoint call deploi xgboost model seek tip reduc outlier",
        "Answer_gpt_summary_original":"possible solutions to improve endpoint call performance and reduce outliers to around 50 ms include monitoring the overheadlatency and modellatency metrics using cloudwatch, driving a higher request rate to keep caches warm, and experimenting with different tps settings to find the optimal balance between latency and predictability.",
        "Answer_gpt_summary":"possibl solut improv endpoint perform reduc outlier includ monitor overheadlat modellat metric cloudwatch drive higher request rate cach warm experi differ tp set optim balanc latenc predict"
    },
    {
        "Question_id":null,
        "Question_title":"Install and load Tidymodels package in AML",
        "Question_body":"I'm trying install and load some R packages in the Execute R Script in Azure Machine Learning for to run models, such as tidymodels, timetk, modeltime, modeltime.ensemble.\n\n library(forecast)\n library(tidyverse)\n library(lubridate)\n install.packages(\"quantdates\",repos = \"https:\/\/cloud.r-project.org\")\n install.packages(\"tidymodels\",repos = \"https:\/\/cloud.r-project.org\")\n library(quantdates)\n library(tidymodels) \n library(timetk) \n library(modeltime) \n library(modeltime.resample) \n library(modeltime.ensemble)\n\n\n\nHowever I get the following error:\n\n Error: package or namespace load failed for \u2018tidymodels\u2019 in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\n namespace \u2018rlang\u2019 0.4.5 is already loaded, but >= 1.0.2 is required\n azureml_main(input_dataframe_1), library(tidymodels), tryCatch({\n     attr(package, \"LibPath\") <- which.lib.loc\n     ns <- loadNamespace(package, lib.loc)\n     env <- attachNamespace(ns, pos = pos, deps)\n }, error = function(e) {\n     P <- if (!is.null(cc <- conditionCall(e))) \n         paste(\" in\", deparse(cc)[1])\n     else \"\"\n     msg <- gettextf(\"package or namespace load failed for %s%s:\\n %s\", sQuote(package), P, conditionMessage(e))\n     if (logical.return) \n         message(paste(\"Error:\", msg), domain = NA)\n     else stop(msg, call. = FALSE, domain = NA)\n }), tryCatchList(expr, classes, parentenv, handlers), tryCatchOne(expr, names, parentenv, handlers[[1]]), value[[3]](cond), stop(msg, call. = FALSE, domain = NA), .handleSimpleError(function (e) \n {\n     error_msg <<- paste(toString(e), toString(sys.calls()[-c(1:3)]), sep = \"\\n\")\n     stop(e)\n }, \"package or namespace load failed for \u2018tidymodels\u2019 in loadNamespace(i, c(lib.loc, .libPaths()), versionCheck = vI[[i]]):\\n namespace \u2018rlang\u2019 0.4.5 is already loaded, but >= 1.0.2 is required\", quote(NULL)), h(simpleError(msg, call))\n '.\n ---------- End of error message from R  interpreter  ----------\n\n\n\nI have also tried with devtools package for install a particular version but I keep getting the same error with the rlang package. Sometimes, I get the same error with the cli package.\n\nIn my local machine, the R code runs fine. I have the R version 4.1.3 and the Azure Machine Learning has the R version 3.5.1.\n\nDoes anyone know how I can solve this problem?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1653509127043,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/864244\/install-and-load-tidymodels-package-in-aml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-26T10:49:40.953Z",
                "Answer_score":0,
                "Answer_body":"@JeanPaulPiedrahitaGarca-3958 I think the reason that tidymodels failed to install is because it is built on windows as per the Description file of the package.\n\n Built: R 4.2.0; ; 2022-05-24 05:31:20 UTC; windows\n\n\n\nThe note and warning section of Installing R packages from documentation details the way to check if a package can be installed with the Execute R script module.\n\nDo not install packages which are pre-built on\/for Windows, since the designer components are running on Ubuntu. To check whether a package is pre-built on windows, you could go to CRAN and search your package, download one binary file according to your OS, and check Built: part in the DESCRIPTION file.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-27T13:25:28.763Z",
                "Answer_score":1,
                "Answer_body":"@romungi-MSFT After everything I've managed to consult, I think the problem is in the R version 3.5.1: tidymodels package require a rlang version package greater than o equal to 1.0.2. However, the rlang version 1.0.2 only works for R versions starting from 4.0.0:\n\n        Package  Version  Published\n 96           R    3.5.1 2018-07-02\n 97  tidymodels    0.0.1 2018-07-27\n 98       rlang    0.2.2 2018-08-16\n 99       rlang    0.3.0 2018-10-22\n 100      rlang  0.3.0.1 2018-10-25\n 101 tidymodels    0.0.2 2018-11-27\n 102          R    3.5.2 2018-12-20\n 103      rlang    0.3.1 2019-01-08\n 104          R    3.5.3 2019-03-11\n 105      rlang    0.3.2 2019-03-21\n 106      rlang    0.3.3 2019-03-29\n 107      rlang    0.3.4 2019-04-07\n 108          R    3.6.0 2019-04-26\n 109      rlang    0.4.0 2019-06-25\n 110          R    3.6.1 2019-07-05\n 111 tidymodels    0.0.3 2019-10-04\n 112      rlang    0.4.1 2019-10-24\n 113      rlang    0.4.2 2019-11-23\n 114          R    3.6.2 2019-12-12\n 115      rlang    0.4.3 2020-01-24\n 116      rlang    0.4.4 2020-01-28\n 117 tidymodels    0.1.0 2020-02-16\n 118          R    3.6.3 2020-02-29\n 119      rlang    0.4.5 2020-03-01\n 120          R    4.0.0 2020-04-24\n 121      rlang    0.4.6 2020-05-02\n 122          R    4.0.1 2020-06-06\n 123          R    4.0.2 2020-06-22\n 124      rlang    0.4.7 2020-07-09\n 125 tidymodels    0.1.1 2020-07-14\n 126      rlang    0.4.8 2020-10-08\n 127          R    4.0.3 2020-10-10\n 128 tidymodels    0.1.2 2020-11-22\n 129      rlang    0.4.9 2020-11-26\n 130      rlang   0.4.10 2020-12-30\n 131          R    4.0.4 2021-02-15\n 132          R    4.0.5 2021-03-31\n 133 tidymodels    0.1.3 2021-04-19\n 134      rlang   0.4.11 2021-04-30\n 135          R    4.1.0 2021-05-18\n 136          R    4.1.1 2021-08-10\n 137 tidymodels    0.1.4 2021-10-01\n 138      rlang   0.4.12 2021-10-18\n 139      rlang    1.0.0 2022-01-26\n 140      rlang    1.0.1 2022-02-03\n 141      rlang    1.0.2 2022-03-04\n 142 tidymodels    0.2.0 2022-03-19\n\n\n\nFor this reason, in the AML only loaded (or manages to load) the rlang version 0.4.5, which the last compatible version for R 3.0.0. The rlang version 1.0.2 only is compatible for R 4.0.0. I wish there was a possibility that AML had a more recent version of R. Thank you again.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"instal load tidymodel packag aml try instal load packag execut script run model tidymodel timetk modeltim modeltim ensembl librari forecast librari tidyvers librari lubrid instal packag quantdat repo http cloud project org instal packag tidymodel repo http cloud project org librari quantdat librari tidymodel librari timetk librari modeltim librari modeltim resampl librari modeltim ensembl follow error error packag namespac load fail tidymodel loadnamespac lib loc libpath versioncheck namespac rlang load requir main input datafram librari tidymodel trycatch attr packag libpath requir quot null simpleerror msg end error messag interpret tri devtool packag instal particular version get error rlang packag error cli packag local machin code run fine version version know solv problem",
        "Question_preprocessed_content":"instal load tidymodel packag aml try instal load packag execut script run model tidymodel timetk modeltim librari librari librari librari librari librari librari follow error error packag namespac load fail tidymodel loadnamespac versioncheck namespac rlang load requir librari trycatch requir quot end error messag interpret tri devtool packag instal particular version get error rlang packag error cli packag local machin code run fine version version know solv problem",
        "Answer_original_content":"jeanpaulpiedrahitagarca think reason tidymodel fail instal built window descript file packag built utc window note warn section instal packag document detail wai check packag instal execut script modul instal packag pre built window design compon run ubuntu check packag pre built window cran search packag download binari file accord check built descript file answer help click upvot help commun member read thread romungi msft manag consult think problem version tidymodel packag requir rlang version packag greater equal rlang version work version start packag version publish tidymodel rlang rlang rlang tidymodel rlang rlang rlang rlang rlang tidymodel rlang rlang rlang rlang tidymodel rlang rlang rlang tidymodel rlang tidymodel rlang rlang tidymodel rlang tidymodel rlang rlang rlang rlang tidymodel reason aml load manag load rlang version compat version rlang version compat wish possibl aml recent version thank",
        "Answer_preprocessed_content":"think reason tidymodel fail instal built window descript file packag built utc window note warn section instal packag document detail wai check packag instal execut script modul instal packag window design compon run ubuntu check packag window cran search packag download binari file accord check built descript file answer help click upvot help commun member read thread manag consult think problem version tidymodel packag requir rlang version packag greater equal rlang version work version start packag version publish tidymodel rlang rlang rlang tidymodel rlang rlang rlang rlang rlang tidymodel rlang rlang rlang rlang tidymodel rlang rlang rlang tidymodel rlang tidymodel rlang rlang tidymodel rlang tidymodel rlang rlang rlang rlang tidymodel reason aml load rlang version compat version rlang version compat wish possibl aml recent version thank",
        "Question_gpt_summary_original":"the user is encountering challenges with installing and loading the tidymodels package in , as they are receiving an error related to the rlang package.",
        "Question_gpt_summary":"user encount challeng instal load tidymodel packag receiv error relat rlang packag",
        "Answer_gpt_summary_original":"possible solutions to the challenge of installing and loading the tidymodels package in  include checking if the package is pre-built on windows, avoiding the installation of packages pre-built on\/for windows, and upgrading the r version to at least 4.0.0 to be compatible with the required rlang version 1.0.2. the last compatible version of rlang for r 3.0.0 is 0.4.5, which is the version that aml can load. however, it would be helpful if aml had a more recent version of r.",
        "Answer_gpt_summary":"possibl solut challeng instal load tidymodel packag includ check packag pre built window avoid instal packag pre built window upgrad version compat requir rlang version compat version rlang version aml load help aml recent version"
    },
    {
        "Question_id":null,
        "Question_title":"Getting error while translating document using Azure translator - \"Cannot access source document location with the current permissions\" InvalidRequest",
        "Question_body":"I am using translator resource to translate the one document at a time from the source container through python code.\n\nFollowing the documentation, generated the container level SAS for existing containers.\n\nTried different combinations of request body formats to meet the requirement of translating the single file from source container using \"Container level SAS\".\n\nI was able to translate various documents but suddenly the python code is failing with the error -\n{ 'status': 'ValidationFailed', 'error': {'code': 'InvalidRequest', 'message': 'Cannot access source document location with the current permissions.', 'target': 'Operation', 'innerError': {'code': 'InvalidDocumentAccessLevel', 'message': 'Cannot access source document location with the current permissions.'}\u200b}, 'summary': {'total': 0, 'failed': 0, 'success': 0, 'inProgress': 0, 'notYetStarted': 0, 'cancelled': 0, 'totalCharacterCharged': 0}\u200b}\n\nWhen same request bodies are tried for newly created containers, documents are translating successfully.\nDoes existing containers gives this issue sometimes and we should always use the newly created containers?\n\nOld container's SAS urls are generated with appropriate permissions i.e. Read,list for source and Write.List for Target container.\n\nPlease suggest me the correct way.\n\nAlso attaching the different request body formats I am using to take single file at a time using \"Container level SAS\"\n\nSpecifying the file name in source url and target url\n\n    {\n                 \"inputs\": [\n                     {\n                         \"storageType\": \"File\",\n                         \"source\": {\n                             \"sourceUrl\": \"https:\/\/myblob.blob.core.windows.net\/src_container\/file.docx?<SAS>\"\n                         },\n                         \"targets\": [\n                             {   \n                        \"targetUrl\":\"https:\/myblob.blob.core.windows.net\/container\/<target_blob_name_without_any_extension>? \n                            <SAS>\",\n                                 \"language\": \"fr\"\n                             }\n                         ]\n                     }\n                 ]\n             }\n\n\n\nSpecifying the file name as suffix in the source\n\n {\n         \"inputs\": [\n             {\n                \n                 \"source\": {\n                     \"sourceUrl\": \"https:\/\/myblob.blob.core.windows.net\/src_container\/file.docx?<SAS>\",\n \"suffix\":\"file.docx\"\n                 },\n                 \"targets\": [\n                     {\n                            \n                \"targetUrl\":\"https:\/myblob.blob.core.windows.net\/container\/<target_blob_name_without_any_extension>? \n                    <SAS>\",\n                         \"language\": \"fr\"\n                     }\n                 ]\n             }\n         ]\n     }\n\n\n\nWhich is the correct way of achieving the use case using \"Container level SAS\".\nAm I doing something wrong in request body ?\nQuick help is needed. Can anyone guide me please?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1635709097990,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/610532\/getting-error-while-translating-document-using-azu.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-01T22:15:30.413Z",
                "Answer_score":1,
                "Answer_body":"Hi, please follow the instructions for Document Translation using the Python Client Library. Here's another sample. Based on the error message, I recommend that you generate a new SAS token and pay close attention to the Start and Expiry date and time.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"get error translat document azur translat access sourc document locat current permiss invalidrequest translat resourc translat document time sourc contain python code follow document gener contain level sa exist contain tri differ combin request bodi format meet requir translat singl file sourc contain contain level sa abl translat document suddenli python code fail error statu validationfail error code invalidrequest messag access sourc document locat current permiss target oper innererror code invaliddocumentaccesslevel messag access sourc document locat current permiss summari total fail success inprogress notyetstart cancel totalcharactercharg request bodi tri newli creat contain document translat successfulli exist contain give issu us newli creat contain old contain sa url gener appropri permiss read list sourc write list target contain suggest correct wai attach differ request bodi format singl file time contain level sa specifi file sourc url target url input storagetyp file sourc sourceurl http myblob blob core window net src contain file docx target targeturl http myblob blob core window net contain languag specifi file suffix sourc input sourc sourceurl http myblob blob core window net src contain file docx suffix file docx target targeturl http myblob blob core window net contain languag correct wai achiev us case contain level sa wrong request bodi quick help need guid",
        "Question_preprocessed_content":"get error translat document azur translat access sourc document locat current permiss invalidrequest translat resourc translat document time sourc contain python code follow document gener contain level sa exist contain tri differ combin request bodi format meet requir translat singl file sourc contain contain level sa abl translat document suddenli python code fail error statu validationfail error summari request bodi tri newli creat contain document translat successfulli exist contain give issu us newli creat contain old contain sa url gener appropri permiss read list sourc target contain suggest correct wai attach differ request bodi format singl file time contain level sa specifi file sourc url target url target specifi file suffix sourc target correct wai achiev us case contain level sa wrong request bodi quick help need guid",
        "Answer_original_content":"follow instruct document translat python client librari sampl base error messag recommend gener new sa token pai close attent start expiri date time kindli accept answer inform help thank",
        "Answer_preprocessed_content":"follow instruct document translat python client librari sampl base error messag recommend gener new sa token pai close attent start expiri date time kindli accept answer inform help thank",
        "Question_gpt_summary_original":"the user is encountering an error while attempting to translate a document using azure translator, with the error message \"cannot access source document location with the current permissions.\" invalidrequest.",
        "Question_gpt_summary":"user encount error attempt translat document azur translat error messag access sourc document locat current permiss invalidrequest",
        "Answer_gpt_summary_original":"possible solutions to the error message \"cannot access source document location with the current permissions\" while using azure translator are to follow the instructions for document translation using the python client library and generate a new sas token while paying close attention to the start and expiry date and time.",
        "Answer_gpt_summary":"possibl solut error messag access sourc document locat current permiss azur translat follow instruct document translat python client librari gener new sa token pai close attent start expiri date time"
    },
    {
        "Question_id":null,
        "Question_title":"Public read-only s3 remote",
        "Question_body":"<p>Hey folks,<\/p>\n<p>The DVC <a href=\"https:\/\/github.com\/iterative\/example-get-started\" rel=\"nofollow noopener\">getting started example<\/a> has a read-only HTTP remote.<\/p>\n<p>Is it possible to do the same with S3 so the public can have read-only permission? I believe I\u2019ve set my bucket permissions correctly* (I can download w\/o AWS creds), but when I try <code>dvc pull<\/code> w\/o creds I get an error:<\/p>\n<pre><code class=\"lang-auto\">$ dvc pull\nERROR: unexpected error - Unable to locate credentials \n<\/code><\/pre>\n<p>It looks like <code>dvc pull<\/code> is checking for credentials regardless.<\/p>\n<ul>\n<li>\n<ul>\n<li>allowing s3:GetObject and s3:GetObject<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>If this isn\u2019t possible, does anyone have a suggestion on the easiest approach to allow read-only access to DVC data analogous to a public GitHub repo?<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1587508457422,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":8.0,
        "Question_view_count":1718.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/public-read-only-s3-remote\/355",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-04-21T22:38:37.028Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/dlite\">@dlite<\/a> !<\/p>\n<p>Indeed, aws requires you to have at least some creds to access even public buckets through s3 protocol. But, you could do that another way: by accessing it through HTTP. For example, in our core repo we use public s3 bucket to store some images, but we use it as an HTTP remote for <code>dvc pull<\/code>ing. See <a href=\"https:\/\/github.com\/iterative\/dvc\/blob\/master\/.dvc\/config#L2\">https:\/\/github.com\/iterative\/dvc\/blob\/master\/.dvc\/config#L2<\/a> (we do route it through our own domain, but it doesn\u2019t matter, you could do the same with regular <code>https:\/\/&lt;bucket&gt;.s3.amazonaws.com\/..<\/code> URL for your bucket.<\/p>",
                "Answer_score":99.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-21T22:42:24.669Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/dlite\">@dlite<\/a> So just to make it clear, you could configure two remotes in your dvc project: one to access your bucket through s3 to push stuff to and one http one to allow anyone to download from. E.g.<\/p>\n<pre><code class=\"lang-auto\">dvc remote add mys3 s3:\/\/bucket\/path\n# note -d, which makes it the default one\ndvc remote add -d mys3http https:\/\/bucket.s3.amazonaws.com\/path \n# now you could\ndvc push -r mys3\n# or you could set it as a default one for your machine using --local flag,\n# which uses .dvc\/config.local which is .gitignored and will only stay on your machine.\ndvc remote default mys3 --local\n<\/code><\/pre>",
                "Answer_score":74.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-21T22:45:24.939Z",
                "Answer_body":"<p>Interesting! I\u2019ll give this a shot tomorrow.<\/p>\n<p>Thanks for the quick reply <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a>!<\/p>",
                "Answer_score":99.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-22T12:32:40.309Z",
                "Answer_body":"<p>Thanks again <a class=\"mention\" href=\"\/u\/kupruser\">@kupruser<\/a> - the process you outlined above is working well for me.<\/p>\n<pre><code class=\"lang-auto\">dvc remote default mys3 --local\n<\/code><\/pre>\n<p>I like this as it makes the flow very similar to working on a public GitHub repo (I can push by default and others can read by default).<\/p>",
                "Answer_score":134.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"public read remot hei folk get start exampl read http remot possibl public read permiss believ iv set bucket permiss correctli download aw cred try pull cred error pull error unexpect error unabl locat credenti look like pull check credenti regardless allow getobject getobject isnt possibl suggest easiest approach allow read access data analog public github repo thank",
        "Question_preprocessed_content":"public remot hei folk get start exampl http remot possibl public permiss believ iv set bucket permiss correctli try cred error look like check credenti regardless allow getobject getobject isnt possibl suggest easiest approach allow access data analog public github repo thank",
        "Answer_original_content":"dlite aw requir cred access public bucket protocol wai access http exampl core repo us public bucket store imag us http remot pull http github com iter blob master config rout domain doesnt matter regular http amazonaw com url bucket dlite clear configur remot project access bucket push stuff http allow download remot add my bucket path note make default remot add myshttp http bucket amazonaw com path push my set default machin local flag us config local gitignor stai machin remot default my local interest ill shot tomorrow thank quick repli kuprus thank kuprus process outlin work remot default my local like make flow similar work public github repo push default read default",
        "Answer_preprocessed_content":"aw requir cred access public bucket protocol wai access http exampl core repo us public bucket store imag us http remot ing",
        "Question_gpt_summary_original":"the user is encountering challenges in setting up a public read-only s3 remote, as pull is checking for credentials regardless of the bucket permissions.",
        "Question_gpt_summary":"user encount challeng set public read remot pull check credenti regardless bucket permiss",
        "Answer_gpt_summary_original":"the answer suggests that the user can access the public s3 bucket through http instead of s3 protocol. the solution involves configuring two remotes in the project, one for pushing to the bucket through s3 and another for allowing anyone to download from it through http. the user can set the http remote as the default one for their machine using the --local flag.",
        "Answer_gpt_summary":"answer suggest user access public bucket http instead protocol solut involv configur remot project push bucket allow download http user set http remot default machin local flag"
    },
    {
        "Question_id":null,
        "Question_title":"Vertex AI Notebook deleting cells",
        "Question_body":"Hello everyone, I have been facing an issue for the past few months where on occasion my vertex AI notebooks will completely wipe and delete all the cells in a .ipynb file. This happens at what appears to be random times. Context to reproduce: N96 High Memory instance 624 gb of ramIdle time: 1440single user only notebook What happens: notebook with shutdown in the midst of running. Once the notebook is back up and running all the cells in the ipynb file are gone. There is no error message If anyone has faced this issue in the past and knows how to resolve I would really appreciate and information! Thank you ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661849640000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":41.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Notebook-deleting-cells\/td-p\/461480\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-09T14:02:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"This seems that it\u2019s an issue that you are facing only applies to your use case, so what I would recommend to you is that you raise a support ticket to receive better support."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"notebook delet cell hello face issu past month occas notebook complet wipe delet cell ipynb file happen appear random time context reproduc high memori instanc ramidl time singl user notebook happen notebook shutdown midst run notebook run cell ipynb file gone error messag face issu past know resolv appreci inform thank",
        "Question_preprocessed_content":"notebook delet cell hello face issu past month occas notebook complet wipe delet cell ipynb file happen appear random time context reproduc high memori instanc ramidl time singl user notebook happen notebook shutdown midst run notebook run cell ipynb file gone error messag face issu past know resolv appreci inform thank",
        "Answer_original_content":"issu face appli us case recommend rais support ticket receiv better support",
        "Answer_preprocessed_content":"issu face appli us case recommend rais support ticket receiv better support",
        "Question_gpt_summary_original":"the user is encountering an issue where their notebook will randomly delete all the cells in a .ipynb file, with no error message, on an n96 high memory instance with 624 gb of ram and idle time of 1440.",
        "Question_gpt_summary":"user encount issu notebook randomli delet cell ipynb file error messag high memori instanc ram idl time",
        "Answer_gpt_summary_original":"possible solution: the user should raise a support ticket to receive better support for their issue.",
        "Answer_gpt_summary":"possibl solut user rais support ticket receiv better support issu"
    },
    {
        "Question_id":null,
        "Question_title":"Access company's fileshare from Azure ML Compute",
        "Question_body":"I have been doing some searching around and can't seem to find anything particularly related to this question. Is it possible to connect to a local fileshare system from an Azure ML instance? Basically, I have a large amount of data in the form of images stored on a local drive and would like to use ml.azure to train and make predictions on this data.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1633621586400,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/581725\/access-company39s-fileshare-from-azure-ml-compute.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-08T03:43:32.46Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nI am not pretty sure what's \"company fileshare\" you are mentioning. But Azure Machine Learning Studio does support upload data from local\/ datastore\/ Web URL\/ public dataset. You can create your dataset for training.\n\nReference:https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-connect-data-ui#create-datasets\n\nBesides Studio, you can also connect to datastore and storage.\n\nThere are two dataset types, based on how users consume them in training; FileDatasets and TabularDatasets. Both types can be used in Azure Machine Learning training workflows involving, estimators, AutoML, hyperDrive and pipelines.\n\nA FileDataset references single or multiple files in your datastores or public URLs. If your data is already cleansed, and ready to use in training experiments, you can download or mount the files to your compute as a FileDataset object.\n\nA TabularDataset represents data in a tabular format by parsing the provided file or list of files. This provides you with the ability to materialize the data into a pandas or Spark DataFrame so you can work with familiar data preparation and training libraries without having to leave your notebook. You can create a TabularDataset object from .csv, .tsv, .parquet, .jsonl files, and from SQL query results.\n\nReference: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets#dataset-types\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"access compani fileshar comput search particularli relat question possibl connect local fileshar instanc basic larg data form imag store local drive like us azur train predict data",
        "Question_preprocessed_content":"access compani fileshar comput search particularli relat question possibl connect local fileshar instanc basic larg data form imag store local drive like us train predict data",
        "Answer_original_content":"hello pretti sure compani fileshar mention studio support upload data local datastor web url public dataset creat dataset train refer http doc microsoft com azur machin learn connect data creat dataset studio connect datastor storag dataset type base user consum train filedataset tabulardataset type train workflow involv estim automl hyperdr pipelin filedataset refer singl multipl file datastor public url data cleans readi us train experi download mount file comput filedataset object tabulardataset repres data tabular format pars provid file list file provid abil materi data panda spark datafram work familiar data prepar train librari have leav notebook creat tabulardataset object csv tsv parquet jsonl file sql queri result refer http doc microsoft com azur machin learn creat regist dataset dataset type hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"hello pretti sure compani fileshar mention studio support upload data local datastor web url public dataset creat dataset train studio connect datastor storag dataset type base user consum train filedataset tabulardataset type train workflow involv estim automl hyperdr pipelin filedataset refer singl multipl file datastor public url data cleans readi us train experi download mount file comput filedataset object tabulardataset repres data tabular format pars provid file list file provid abil materi data panda spark datafram work familiar data prepar train librari have leav notebook creat tabulardataset object csv tsv parquet jsonl file sql queri result refer hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Question_gpt_summary_original":"the user is trying to access a company's fileshare from an instance in order to use ml.azure to train and make predictions on a large amount of image data stored on a local drive.",
        "Question_gpt_summary":"user try access compani fileshar instanc order us azur train predict larg imag data store local drive",
        "Answer_gpt_summary_original":"possible solutions from the answer include using  studio to upload data from local, datastore, web url, or public dataset to create a dataset for training. additionally, users can connect to datastore and storage and use filedatasets or tabulardatasets in training workflows involving estimators, automl, hyperdrive, and pipelines. a filedataset references single or multiple files in datastores or public urls, while a tabulardataset represents data in a tabular format by parsing the provided file or list of files. users can create a tabulardataset object from .csv, .tsv, .parquet, .jsonl files, and from sql query results.",
        "Answer_gpt_summary":"possibl solut answer includ studio upload data local datastor web url public dataset creat dataset train addition user connect datastor storag us filedataset tabulardataset train workflow involv estim automl hyperdr pipelin filedataset refer singl multipl file datastor public url tabulardataset repres data tabular format pars provid file list file user creat tabulardataset object csv tsv parquet jsonl file sql queri result"
    },
    {
        "Question_id":null,
        "Question_title":"How does one save a plot in wandb with wandb.log?",
        "Question_body":"<p>I\u2019m trying to save a plot with wandb.log. Their <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/log\/plots\">docs<\/a> say to do:<\/p>\n<pre><code class=\"lang-auto\">    wandb.log({\"chart\": plt})\n<\/code><\/pre>\n<p>but this fails for me.<\/p>\n<p>I get two errors, 1st error (when I do NOT do <code>plt.show()<\/code> before trying to do wand.log):<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/Applications\/PyCharm.app\/Contents\/plugins\/python\/helpers\/pydev\/_pydevd_bundle\/pydevd_exec2.py\", line 3, in Exec\n    exec(exp, global_vars, local_vars)\n  File \"&lt;input&gt;\", line 1, in &lt;module&gt;\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 256, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 222, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1548, in log\n    self._log(data=data, step=step, commit=commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1339, in _log\n    self._partial_history_callback(data, step, commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1228, in _partial_history_callback\n    self._backend.interface.publish_partial_history(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 541, in publish_partial_history\n    data = history_dict_to_json(run, data, step=user_step, ignore_copy_err=True)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 54, in history_dict_to_json\n    payload[key] = val_to_json(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 82, in val_to_json\n    val = Plotly.make_plot_media(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/plotly.py\", line 48, in make_plot_media\n    val = util.matplotlib_to_plotly(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/util.py\", line 560, in matplotlib_to_plotly\n    return tools.mpl_to_plotly(obj)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/tools.py\", line 112, in mpl_to_plotly\n    matplotlylib.Exporter(renderer).run(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 53, in run\n    self.crawl_fig(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 124, in crawl_fig\n    self.crawl_ax(ax)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 146, in crawl_ax\n    self.draw_collection(ax, collection)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 289, in draw_collection\n    offset_order = offset_dict[collection.get_offset_position()]\nAttributeError: 'LineCollection' object has no attribute 'get_offset_position'\n<\/code><\/pre>\n<p>I get two errors, 2nd error (when I DO <code>plt.show()<\/code> before trying to do wand.log):<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/Applications\/PyCharm.app\/Contents\/plugins\/python\/helpers\/pydev\/_pydevd_bundle\/pydevd_exec2.py\", line 3, in Exec\n    exec(exp, global_vars, local_vars)\n  File \"&lt;input&gt;\", line 1, in &lt;module&gt;\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 256, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 222, in wrapper\n    return func(self, *args, **kwargs)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1548, in log\n    self._log(data=data, step=step, commit=commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1339, in _log\n    self._partial_history_callback(data, step, commit)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/wandb_run.py\", line 1228, in _partial_history_callback\n    self._backend.interface.publish_partial_history(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 541, in publish_partial_history\n    data = history_dict_to_json(run, data, step=user_step, ignore_copy_err=True)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 54, in history_dict_to_json\n    payload[key] = val_to_json(\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/utils.py\", line 82, in val_to_json\n    val = Plotly.make_plot_media(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/sdk\/data_types\/plotly.py\", line 48, in make_plot_media\n    val = util.matplotlib_to_plotly(val)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/wandb\/util.py\", line 560, in matplotlib_to_plotly\n    return tools.mpl_to_plotly(obj)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/tools.py\", line 112, in mpl_to_plotly\n    matplotlylib.Exporter(renderer).run(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 53, in run\n    self.crawl_fig(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/exporter.py\", line 122, in crawl_fig\n    with self.renderer.draw_figure(fig=fig, props=utils.get_figure_properties(fig)):\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/contextlib.py\", line 119, in __enter__\n    return next(self.gen)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mplexporter\/renderers\/base.py\", line 45, in draw_figure\n    self.open_figure(fig=fig, props=props)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/renderer.py\", line 90, in open_figure\n    self.mpl_x_bounds, self.mpl_y_bounds = mpltools.get_axes_bounds(fig)\n  File \"\/Users\/brandomiranda\/opt\/anaconda3\/envs\/meta_learning\/lib\/python3.9\/site-packages\/plotly\/matplotlylib\/mpltools.py\", line 265, in get_axes_bounds\n    x_min, y_min, x_max, y_max = min(x_min), min(y_min), max(x_max), max(y_max)\nValueError: min() arg is an empty sequence\n<\/code><\/pre>\n<p>Note that their trivial example DOES work:<\/p>\n<pre><code class=\"lang-auto\">import matplotlib.pyplot as plt\n\nplt.plot([1, 2, 3, 4])\nplt.ylabel(\"some interesting numbers\")\nwandb.log({\"chart\": plt})\n<\/code><\/pre>\n<p>for me.<\/p>\n<hr>\n<p>cross posted: <a href=\"https:\/\/stackoverflow.com\/questions\/72134168\/how-does-one-save-a-plot-in-wandb-with-wandb-log\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">python - How does one save a plot in wandb with wandb.log? - Stack Overflow<\/a><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1651788927254,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":188.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-does-one-save-a-plot-in-wandb-with-wandb-log\/2373",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-09T21:30:52.813Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/brando\">@brando<\/a>,<\/p>\n<p>I\u2019m sorry you are facing this. Could you share the code snippet you were using to generate this chart? I\u2019ll test this on my end and see how this can be resolved.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-12T22:18:46.528Z",
                "Answer_body":"<p>Hi Brando,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-08T21:31:40.625Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save plot log try save plot log doc log chart plt fail error error plt try wand log traceback recent file applic pycharm app content plugin python helper pydev pydevd bundl pydevd exec line exec exec exp global var local var file line file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self log data data step step commit commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self partial histori callback data step commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line partial histori callback self backend interfac publish partial histori file user brandomiranda opt anaconda env meta learn lib python site packag sdk interfac interfac line publish partial histori data histori dict json run data step user step ignor copi err true file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line histori dict json payload kei val json file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line val json val plotli plot media val file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type plotli line plot media val util matplotlib plotli val file user brandomiranda opt anaconda env meta learn lib python site packag util line matplotlib plotli return tool mpl plotli obj file user brandomiranda opt anaconda env meta learn lib python site packag plotli tool line mpl plotli matplotlylib export render run fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line run self crawl fig fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line crawl fig self crawl file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line crawl self draw collect collect file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line draw collect offset order offset dict collect offset posit attributeerror linecollect object attribut offset posit error error plt try wand log traceback recent file applic pycharm app content plugin python helper pydev pydevd bundl pydevd exec line exec exec exp global var local var file line file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line wrapper return func self arg kwarg file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self log data data step step commit commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line log self partial histori callback data step commit file user brandomiranda opt anaconda env meta learn lib python site packag sdk run line partial histori callback self backend interfac publish partial histori file user brandomiranda opt anaconda env meta learn lib python site packag sdk interfac interfac line publish partial histori data histori dict json run data step user step ignor copi err true file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line histori dict json payload kei val json file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type util line val json val plotli plot media val file user brandomiranda opt anaconda env meta learn lib python site packag sdk data type plotli line plot media val util matplotlib plotli val file user brandomiranda opt anaconda env meta learn lib python site packag util line matplotlib plotli return tool mpl plotli obj file user brandomiranda opt anaconda env meta learn lib python site packag plotli tool line mpl plotli matplotlylib export render run fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line run self crawl fig fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport export line crawl fig self render draw figur fig fig prop util figur properti fig file user brandomiranda opt anaconda env meta learn lib python contextlib line enter return self gen file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mplexport render base line draw figur self open figur fig fig prop prop file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib render line open figur self mpl bound self mpl bound mpltool ax bound fig file user brandomiranda opt anaconda env meta learn lib python site packag plotli matplotlylib mpltool line ax bound min min max max min min min min max max max max valueerror min arg sequenc note trivial exampl work import matplotlib pyplot plt plt plot plt ylabel interest number log chart plt cross post python save plot log stack overflow",
        "Question_preprocessed_content":"save plot log try save plot log doc fail error error error error note trivial exampl work cross post python save plot log stack overflow",
        "Answer_original_content":"brando sorri face share code snippet gener chart ill test end resolv thank ramit brando want follow support request heard let know assist issu resolv best topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"sorri face share code snippet gener chart ill test end resolv thank ramit brando want follow support request heard let know assist issu resolv best topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering challenges when attempting to save a plot with .log, resulting in two errors when trying to do so.",
        "Question_gpt_summary":"user encount challeng attempt save plot log result error try",
        "Answer_gpt_summary_original":"there are no solutions provided in the answer. the responder is asking for more information to help resolve the issue and is following up with the user to see if they still need assistance.",
        "Answer_gpt_summary":"solut provid answer respond ask inform help resolv issu follow user need assist"
    },
    {
        "Question_id":null,
        "Question_title":"Academic email not registered anymore?",
        "Question_body":"<p>Hey!<\/p>\n<p>I cannot create an academic team anymore (last year, it worked, but I just deleted the group). In my email settings, there is no academic tag. If I try to upgrade my plan to academic again, I\u2019m forwarded to my project page. Am I missing something here? Thanks <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1673957040347,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":90.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/academic-email-not-registered-anymore\/3706",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-18T18:18:32.502Z",
                "Answer_body":"<p>Hey Lukas,<\/p>\n<p>Can you send me your username and your academic email?<\/p>\n<p>Cheers!<br>\nArtsiom<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-19T16:00:57.777Z",
                "Answer_body":"<p>Hey Artsiom, thanks for your reply! I guess I cannot send you a PM? My username is lura and my academic mail lukas.rauch[at]<a href=\"http:\/\/uni-kassel.de\" rel=\"noopener nofollow ugc\">uni-kassel.de<\/a><\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-19T21:56:41.492Z",
                "Answer_body":"<p>I readded your academic domain into our system. Could you check if it works for you now? <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-23T08:48:03.512Z",
                "Answer_body":"<p>Hey, thanks, but unfortunately, no. I still cannot create an academic team or verify my mail <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/confused.png?v=12\" title=\":confused:\" class=\"emoji\" alt=\":confused:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-23T23:19:54.104Z",
                "Answer_body":"<p>Could you try one more time? I was told we just changed your status in our database directly <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-24T12:19:15.555Z",
                "Answer_body":"<p>Now it works. Thank you very much <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-29T06:27:11.275Z",
                "Answer_body":"<p>Hi Lukas!<\/p>\n<p>I\u2019m glad I could help!<\/p>\n<p>Cheers,<br>\nArtsiom<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"academ email regist anymor hei creat academ team anymor year work delet group email set academ tag try upgrad plan academ forward project page miss thank",
        "Question_preprocessed_content":"academ email regist anymor hei creat academ team anymor email set academ tag try upgrad plan academ forward project page miss thank",
        "Answer_original_content":"hei luka send usernam academ email cheer artsiom hei artsiom thank repli guess send usernam lura academ mail luka rauch uni kassel read academ domain check work hei thank unfortun creat academ team verifi mail try time told chang statu databas directli work thank luka glad help cheer artsiom",
        "Answer_preprocessed_content":"hei luka send usernam academ email cheer artsiom hei artsiom thank repli guess send usernam lura academ mail read academ domain check work hei thank unfortun creat academ team verifi mail try time told chang statu databas directli work thank luka glad help cheer artsiom",
        "Question_gpt_summary_original":"the user is unable to create an academic team after deleting the group and is unable to upgrade their plan to academic due to being forwarded to their project page.",
        "Question_gpt_summary":"user unabl creat academ team delet group unabl upgrad plan academ forward project page",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n1. re-adding the academic domain into the system.\n2. changing the user's status in the database directly.\n\nthe summary of the solutions is that the user's issue was resolved by re-adding the academic domain and changing their status in the database directly.",
        "Answer_gpt_summary":"possibl solut extract answer ad academ domain chang user statu databas directli summari solut user issu resolv ad academ domain chang statu databas directli"
    },
    {
        "Question_id":null,
        "Question_title":"Loading pickle object in entry script in Azure ML",
        "Question_body":"I have an entry script that loads a pickled tokenizer object from Tensorflow and the model itself. When I try to deploy, locally or otherwise, I get an error saying something broke in the init function in the score.py script. Commenting out the tokenizer and the deployment works so I'm sure it's because of it. This is how I define the function:\n\n def init():\n     global tokenizer, model\n     tokenizer_path = os.path.join('.\/objs', 'tokenizer.pkl') # tried absolute path as well, didn't work\n     tokenizer = pickle.load(tokenizer_path)\n     # tokenizer = pickle.load(open(tokenizer_path, 'rb')) # also tried this, didn't work\n     model = tf.keras.models.load_model(os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model.h5'))\n\n\n\nIs that the correct way to load a pickle object in the entry script? Any tips would be appreciated.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1633278181287,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/575512\/loading-pickle-object-in-entry-script-in-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-04T13:34:19.55Z",
                "Answer_score":0,
                "Answer_body":"@2JK I think this should help.\n\nfrom sklearn.externals import joblib\n\n tokenizer_path = os.path.join('.\/objs', 'tokenizer.pkl') # tried absolute path as well, didn't work\n tokenizer = joblib.load(tokenizer_path)\n\n\n\n\nDid you also try the absolute path in your tokenizer_path?",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T17:22:23.093Z",
                "Answer_score":0,
                "Answer_body":"I have the same problem . did you get a solution ?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"load pickl object entri script entri script load pickl token object tensorflow model try deploi local error sai broke init function score script comment token deploy work sure defin function def init global token model token path path join obj token pkl tri absolut path work token pickl load token path token pickl load open token path tri work model kera model load model path join getenv model dir model correct wai load pickl object entri script tip appreci",
        "Question_preprocessed_content":"load pickl object entri script entri script load pickl token object tensorflow model try deploi local error sai broke init function script comment token deploy work sure defin function def init global token model tri absolut path work token token tri work model correct wai load pickl object entri script tip appreci",
        "Answer_original_content":"think help sklearn extern import joblib token path path join obj token pkl tri absolut path work token joblib load token path try absolut path token path problem solut",
        "Answer_preprocessed_content":"think help import joblib tri absolut path work token try absolut path problem solut",
        "Question_gpt_summary_original":"the user is encountering an error when trying to deploy a pickled tokenizer object from tensorflow and the model itself, and is unsure if the correct way to load the pickle object in the entry script has been used.",
        "Question_gpt_summary":"user encount error try deploi pickl token object tensorflow model unsur correct wai load pickl object entri script",
        "Answer_gpt_summary_original":"the solution suggested is to use the joblib library to load the pickled tokenizer object and to ensure that the correct path to the object is used, including trying an absolute path if necessary.",
        "Answer_gpt_summary":"solut suggest us joblib librari load pickl token object ensur correct path object includ try absolut path necessari"
    },
    {
        "Question_id":null,
        "Question_title":"Detectron2 Visualization",
        "Question_body":"<p>Hi,<\/p>\n<p>Has anyone already implemented a visulalization of segmentation or object detection tasks using the detectron2 library?<br>\nThere is already a GitHub issue on this: <a href=\"https:\/\/github.com\/facebookresearch\/detectron2\/issues\/3404\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Visualizations on W&amp;B \u00b7 Issue #3404 \u00b7 facebookresearch\/detectron2 \u00b7 GitHub<\/a> but I was wondering if anyone might already has a somewhat usable code to start with.<\/p>\n<p>Thanks<br>\nBen<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1635154546674,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":502.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/detectron2-visualization\/1086",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-25T12:45:30.157Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/ben-spex\">@ben-spex<\/a> Hi. I\u2019m AyushExel from that github thread. I\u2019m working on a detectron2 integration. detection and instance segmentation is supported but I\u2019m refining it further to support more use cases.<br>\nHere\u2019s my branch - <a href=\"https:\/\/github.com\/AyushExel\/detectron2\/tree\/wandb\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">GitHub - AyushExel\/detectron2 at wandb<\/a><br>\nHere are the changes I made - <a href=\"https:\/\/github.com\/AyushExel\/detectron2\/pull\/2\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">W&amp;B integration by AyushExel \u00b7 Pull Request #2 \u00b7 AyushExel\/detectron2 \u00b7 GitHub<\/a><\/p>\n<p>I\u2019ll add more instructions for using this integration by the end of the week. Stay tuned!<\/p>",
                "Answer_score":21.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-25T13:11:22.055Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cayush\">@cayush<\/a><br>\nah perfect awesome. I just had a quick glance and it looks like exactly what I was hoping for.<\/p>\n<p>I will not have time this week, except for maybe the weekend. But I will definitely give it a try next week.<\/p>\n<p>Thanks a lot for all the effort.<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:06.781Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"detectron visual implement visul segment object detect task detectron librari github issu visual issu facebookresearch detectron github wonder somewhat usabl code start thank ben",
        "Question_preprocessed_content":"detectron visual implement visul segment object detect task detectron librari github issu visual issu github wonder somewhat usabl code start thank ben",
        "Answer_original_content":"ben spex ayushexel github thread work detectron integr detect instanc segment support refin support us case here branch github ayushexel detectron chang integr ayushexel pull request ayushexel detectron github ill add instruct integr end week stai tune cayush perfect awesom quick glanc look like exactli hope time week mayb weekend definit try week thank lot effort topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"ayushexel github thread work detectron integr detect instanc segment support refin support us case here branch github chang integr ayushexel pull request github ill add instruct integr end week stai tune perfect awesom quick glanc look like exactli hope time week mayb weekend definit try week thank lot effort topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking for a usable code to visualize segmentation or object detection tasks using the detectron2 library.",
        "Question_gpt_summary":"user look usabl code visual segment object detect task detectron librari",
        "Answer_gpt_summary_original":"the answer provides a link to a branch on github that supports detection and instance segmentation using the detectron2 library. the user also mentions that they are refining it further to support more use cases and will add more instructions for using the integration by the end of the week. the user who asked the question expresses gratitude and plans to try it out next week.",
        "Answer_gpt_summary":"answer provid link branch github support detect instanc segment detectron librari user mention refin support us case add instruct integr end week user ask question express gratitud plan try week"
    },
    {
        "Question_id":65362133.0,
        "Question_title":"What happens when I add\/remove parameters dynamically during an Optuna study?",
        "Question_body":"<p>Optuna's FAQ has a <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/faq.html#id10\" rel=\"nofollow noreferrer\">clear answer<\/a> when it comes to dynamically adjusting the range of parameter during a study: it poses no problem since each sampler is defined individually.<\/p>\n<p>But what about adding and\/or removing parameters? Is Optuna able to handle such adjustments?<\/p>\n<p>One thing I noticed when doing this is that in the results dataframe these parameters get <code>nan<\/code> entries for other trials. Would there be any benefit to being able to set these <code>nan<\/code>s to their (default) value that they had when not being sampled? Is the study still sound with all these unknown values?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1608315900387,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":227.0,
        "Answer_body":"<p>Question was answered <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2141\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<blockquote>\n<p>Thanks for the question. Optuna internally supports two types of sampling: <code>optuna.samplers.BaseSampler.sample_independent<\/code> and <code>optuna.samplers.BaseSampler.sample_relative<\/code>.<\/p>\n<p>The former <code>optuna.samplers.BaseSampler.sample_independent<\/code> is a method that samples independently on each parameter, and is not affected by the addition or removal of parameters. The added parameters are taken into account from the timing when they are added.<\/p>\n<p>The latter <code>optuna.samplers.BaseSampler.sample_relative<\/code> is a method that samples by considering the correlation of parameters and is affected by the addition or removal of parameters. Optuna's default search space for correlation is the product set of the domains of the parameters that exist from the beginning of the hyperparameter tuning to the present. Developers who implement samplers can implement their own search space calculation method <code>optuna.samplers.BaseSampler.infer_relative_search_space<\/code>. This may allow correlations to be considered for hyperparameters that have been added or removed, but this depends on the sampling algorithm, so there is no API for normal users to modify.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65362133",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1609649865303,
        "Question_original_content":"happen add remov paramet dynam studi faq clear answer come dynam adjust rang paramet studi pose problem sampler defin individu ad remov paramet abl handl adjust thing notic result datafram paramet nan entri trial benefit abl set nan default valu sampl studi sound unknown valu",
        "Question_preprocessed_content":"happen paramet dynam studi faq clear answer come dynam adjust rang paramet studi pose problem sampler defin individu ad remov paramet abl handl adjust thing notic result datafram paramet entri trial benefit abl set valu sampl studi sound unknown valu",
        "Answer_original_content":"question answer thank question intern support type sampl sampler basesampl sampl independ sampler basesampl sampl rel sampler basesampl sampl independ method sampl independ paramet affect addit remov paramet ad paramet taken account time ad sampler basesampl sampl rel method sampl consid correl paramet affect addit remov paramet default search space correl product set domain paramet exist begin hyperparamet tune present develop implement sampler implement search space calcul method sampler basesampl infer rel search space allow correl consid hyperparamet ad remov depend sampl algorithm api normal user modifi",
        "Answer_preprocessed_content":"question answer thank question intern support type sampl method sampl independ paramet affect addit remov paramet ad paramet taken account time ad method sampl consid correl paramet affect addit remov paramet default search space correl product set domain paramet exist begin hyperparamet tune present develop implement sampler implement search space calcul method allow correl consid hyperparamet ad remov depend sampl algorithm api normal user modifi",
        "Question_gpt_summary_original":"The user encountered challenges when adding and\/or removing parameters dynamically during an Optuna study, resulting in <code>nan<\/code> entries for other trials in the results dataframe and uncertainty about the soundness of the study with all these unknown values.",
        "Question_gpt_summary":"user encount challeng ad remov paramet dynam studi result nan entri trial result datafram uncertainti sound studi unknown valu",
        "Answer_gpt_summary_original":"The answer suggests that Optuna internally supports two types of sampling: independent and relative. Independent sampling is not affected by the addition or removal of parameters, while relative sampling considers the correlation of parameters and is affected by the addition or removal of parameters. Optuna's default search space for correlation is the product set of the domains of the parameters that exist from the beginning of the hyperparameter tuning to the present. Developers who implement samplers can implement their own search space calculation method.",
        "Answer_gpt_summary":"answer suggest intern support type sampl independ rel independ sampl affect addit remov paramet rel sampl consid correl paramet affect addit remov paramet default search space correl product set domain paramet exist begin hyperparamet tune present develop implement sampler implement search space calcul method"
    },
    {
        "Question_id":71505415.0,
        "Question_title":"\"The timestamp column must have valid timestamp entries.\" error when using `timestamp_split_column_name` arg in `AutoMLTabularTrainingJob.run`",
        "Question_body":"<p>From <a href=\"https:\/\/googleapis.dev\/python\/aiplatform\/latest\/aiplatform.html#google.cloud.aiplatform.AutoMLTabularTrainingJob.run\" rel=\"nofollow noreferrer\">the docs<\/a> it says that<\/p>\n<blockquote>\n<p>The value of the key values of the key (the values in the column) must be in RFC 3339 date-time format, where time-offset = \u201cZ\u201d (e.g. 1985-04-12T23:20:50.52Z)<\/p>\n<\/blockquote>\n<p>The dataset that I'm pointing to is a CSV in cloud storage, where the data is in the format suggested by the docs:<\/p>\n<pre><code>$ gsutil cat gs:\/\/my-data.csv | head | xsv select TS_SPLIT_COL\nTS_SPLIT_COL\n2021-01-18T00:00:00.00Z\n2021-01-18T00:00:00.00Z\n2021-01-04T00:00:00.00Z\n2021-03-06T00:00:00.00Z\n2021-01-15T00:00:00.00Z\n2021-02-11T00:00:00.00Z\n2021-02-05T00:00:00.00Z\n2021-05-20T00:00:00.00Z\n2021-01-05T00:00:00.00Z\n<\/code><\/pre>\n<p>But I receive a <code>Training pipeline failed with error message: The timestamp column must have valid timestamp entries.<\/code> error when I try to run a training job<\/p>\n<p>EDIT: this should hopefully make it more reproducible<\/p>\n<p>data: <a href=\"https:\/\/pastebin.com\/qEDqvzX6\" rel=\"nofollow noreferrer\">https:\/\/pastebin.com\/qEDqvzX6<\/a><\/p>\n<p>Code I'm running:<\/p>\n<pre><code>from google.cloud import aiplatform\n\nPROJECT = &quot;my-project&quot;\nDATASET_ID = &quot;dataset-id&quot;  # points to CSV \n\naiplatform.init(project=PROJECT)\n\ndataset = aiplatform.TabularDataset(DATASET_ID)\n\njob = aiplatform.AutoMLTabularTrainingJob(\n    display_name=&quot;so-58454722&quot;,\n    optimization_prediction_type=&quot;classification&quot;,\n    optimization_objective=&quot;maximize-au-roc&quot;,\n)\n\nmodel = job.run(\n    dataset=dataset,\n    model_display_name=&quot;so-58454722&quot;,\n    target_column=&quot;Y&quot;,\n    training_fraction_split=0.8,\n    validation_fraction_split=0.1,\n    test_fraction_split=0.1,\n    timestamp_split_column_name=&quot;TS_SPLIT_COL&quot;,\n)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1647473614997,
        "Question_favorite_count":null,
        "Question_last_edit_time":1647534964220,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":"<p>Try this timestamp format instead:<\/p>\n<p><code>2022-03-18T01:23:45.123456+00:00<\/code><\/p>\n<p>It uses <code>+00:00<\/code> instead of <code>Z<\/code> to specify timezone.<\/p>\n<p>This change will eliminate the &quot;The timestamp column must have valid timestamp entries.&quot; error<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71505415",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1647602028112,
        "Question_original_content":"timestamp column valid timestamp entri error timestamp split column arg automltabulartrainingjob run doc sai valu kei valu kei valu column rfc date time format time offset dataset point csv cloud storag data format suggest doc gsutil cat data csv head xsv select split col split col receiv train pipelin fail error messag timestamp column valid timestamp entri error try run train job edit hopefulli reproduc data http pastebin com qedqvzx code run googl cloud import aiplatform project project dataset dataset point csv aiplatform init project project dataset aiplatform tabulardataset dataset job aiplatform automltabulartrainingjob displai optim predict type classif optim object maxim roc model job run dataset dataset model displai target column train fraction split valid fraction split test fraction split timestamp split column split col",
        "Question_preprocessed_content":"timestamp column valid timestamp error arg doc sai valu kei valu kei rfc format dataset point csv cloud storag data format suggest doc receiv error try run train job edit hopefulli reproduc data code run",
        "Answer_original_content":"try timestamp format instead us instead specifi timezon chang elimin timestamp column valid timestamp entri error",
        "Answer_preprocessed_content":"try timestamp format instead us instead specifi timezon chang elimin timestamp column valid timestamp error",
        "Question_gpt_summary_original":"the user is encountering an error when using the `timestamp_split_column_name` arg in `automltabulartrainingjob.run` despite the data being in the correct format as specified in the documentation.",
        "Question_gpt_summary":"user encount error timestamp split column arg automltabulartrainingjob run despit data correct format specifi document",
        "Answer_gpt_summary_original":"the solution to the error encountered when using the `timestamp_split_column_name` arg in `automltabulartrainingjob.run` is to use a different timestamp format that specifies the timezone using `+00:00` instead of `z`. this change will eliminate the error message \"the timestamp column must have valid timestamp entries.\"",
        "Answer_gpt_summary":"solut error encount timestamp split column arg automltabulartrainingjob run us differ timestamp format specifi timezon instead chang elimin error messag timestamp column valid timestamp entri"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Jupyter notebook",
        "Question_body":"What are the advantages of using SageMaker jupyter instance instead of running it locally? Is there a special integration with SageMaker that we lose it if we do not use Sagemaker jupyer instance?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606707121000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Answer_body":"Some useful points:\n\nThe typical arguments of cloud vs local will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\nSageMaker notebooks already run in an explicit IAM context (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run sagemaker.get_execution_role()\nPre-built environments for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\nLinux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\nIf you started using SageMaker Studio, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUIzWlfNVTSIWIqkVsIaNv2A\/sagemaker-jupyter-notebook",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-30T04:05:24.000Z",
                "Answer_score":0,
                "Answer_body":"Some useful points:\n\nThe typical arguments of cloud vs local will apply (as with e.g. Cloud9, Workspaces, etc): Can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, right-size the environment for what workloads you need to do on a given day, etc.\nSageMaker notebooks already run in an explicit IAM context (via assigned execution role) - so you don't need to log in e.g. as you would through the CLI on local machine... Can just run sagemaker.get_execution_role()\nPre-built environments for a range of use-cases (e.g. generic data science, TensorFlow, PyTorch, MXNet, etc) with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance - no more \"environment soup\" on your local laptop.\nLinux-based environments, which typically makes for a shorter path to production code than Mac\/Windows.\nIf you started using SageMaker Studio, then yes there are some native integrations such as the UIs for experiment tracking and endpoint management\/monitoring; easy sharing of notebook snapshots; and whatever else might be announced over the next couple of weeks.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1606709124000,
        "Question_original_content":"jupyt notebook advantag jupyt instanc instead run local special integr lose us jupyer instanc",
        "Question_preprocessed_content":"jupyt notebook advantag jupyt instanc instead run local special integr lose us jupyer instanc",
        "Answer_original_content":"us point typic argument cloud local appli cloud workspac coupl work lifetim laptop thing run local machin shut right size environ workload need given dai notebook run explicit iam context assign execut role need log cli local machin run execut role pre built environ rang us case gener data scienc tensorflow pytorch mxnet librari instal easi wipe reset environ stop start instanc environ soup local laptop linux base environ typic make shorter path product code mac window start studio ye nativ integr ui experi track endpoint manag monitor easi share notebook snapshot announc coupl week",
        "Answer_preprocessed_content":"us point typic argument cloud local appli work lifetim laptop thing run local machin shut environ workload need given dai notebook run explicit iam context need log cli local run environ rang librari instal easi environ stop start instanc environ soup local laptop environ typic make shorter path product code start studio ye nativ integr ui experi track endpoint easi share notebook snapshot announc coupl week",
        "Question_gpt_summary_original":"the user is questioning the advantages of using a jupyter instance instead of running it locally, and whether there is a special integration with jupyter that would be lost if they do not use the instance.",
        "Question_gpt_summary":"user question advantag jupyt instanc instead run local special integr jupyt lost us instanc",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer are: \n- using a jupyter instance in the cloud can de-couple your work from the lifetime of your laptop, keep things running when your local machine is shut down, and right-size the environment for what workloads you need to do on a given day.\n- notebooks already run in an explicit iam context, so you don't need to log in as you would through the cli on a local machine.\n- pre-built environments for a range of use-cases with libraries already installed, and easy wiping\/reset of the environment by stopping & starting the instance.\n- linux-based environments, which typically makes for a shorter path to production code than mac\/windows.\n- if you started using studio, then there are some native integrations such as the uis for experiment tracking and endpoint management\/monitoring, easy sharing of notebook snapshots, and whatever else might be announced over the next couple of weeks.",
        "Answer_gpt_summary":"possibl solut mention answer jupyt instanc cloud coupl work lifetim laptop thing run local machin shut right size environ workload need given dai notebook run explicit iam context need log cli local machin pre built environ rang us case librari instal easi wipe reset environ stop start instanc linux base environ typic make shorter path product code mac window start studio nativ integr ui experi track endpoint manag monitor easi share notebook snapshot announc coupl week"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML How to retrain published ML WebSerice (end point) using Jupyter Notebooks",
        "Question_body":"Hi Azure ML\n\nWe are using Azure Notebooks to train ML models. We are able to publish Web services then.(https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-deploy-models-with-aml)\n\nOut data is stored in Azure Blob Storage. My qustions are:\n\n1- How can i remotely (as from a web point) update the data at Blob Storage?\n\n2- How Can I retrain model on that new data that is already publiched? Here I need some kind separate rest end point to be able to lunch retraining remotely.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1595284669333,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/49994\/azure-ml-how-to-retrain-published-ml-webserice-end.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-21T01:55:42.433Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nThe machine learning model workflow generally follows this sequence:\n\nTrain\n\nDevelop machine learning training scripts in Python, R, or with the visual designer.\nCreate and configure a compute target.\nSubmit the scripts to a configured compute target to run in that environment. During training, the scripts can read from or write to datastores. The logs and output produced during training are saved as runs in the workspace and grouped under experiments.\nPackage - After a satisfactory run is found, register the persisted model in the model registry.\n\nValidate - Query the experiment for logged metrics from the current and past runs. If the metrics don't indicate a desired outcome, loop back to step 1 and iterate on your scripts.\n\nDeploy - Develop a scoring script that uses the model and Deploy the model as a web service in Azure, or to an IoT Edge device.\n\nMonitor - Monitor for data drift between the training dataset and inference data of a deployed model. When necessary, loop back to step 1 to retrain the model with new training data.\n\n\n\n\nFor your scenario, I would highly recommend you try Azure Machine Learning Designer, which works with Azure Notebook well and easy to use.\n\nIF you still want to stick with Notebook, I think creating pipelines will be good to you. https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-your-first-pipeline?view=azure-devops\n\nThanks,\nYutong",
                "Answer_comment_count":2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":37.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"retrain publish webseric end point jupyt notebook azur notebook train model abl publish web servic http doc microsoft com azur machin learn tutori deploi model aml data store azur blob storag qustion remot web point updat data blob storag retrain model new data publich need kind separ rest end point abl lunch retrain remot",
        "Question_preprocessed_content":"retrain publish webseric jupyt notebook azur notebook train model abl publish web servic data store azur blob storag qustion remot updat data blob storag retrain model new data publich need kind separ rest end point abl lunch retrain remot",
        "Answer_original_content":"machin learn model workflow gener follow sequenc train develop machin learn train script python visual design creat configur comput target submit script configur comput target run environ train script read write datastor log output produc train save run workspac group experi packag satisfactori run regist persist model model registri valid queri experi log metric current past run metric indic desir outcom loop step iter script deploi develop score script us model deploi model web servic azur iot edg devic monitor monitor data drift train dataset infer data deploi model necessari loop step retrain model new train data scenario highli recommend try design work azur notebook easi us want stick notebook think creat pipelin good http doc microsoft com azur machin learn creat pipelin view azur devop thank yutong",
        "Answer_preprocessed_content":"machin learn model workflow gener follow sequenc train develop machin learn train script python visual design creat configur comput target submit script configur comput target run environ train script read write datastor log output produc train save run workspac group experi packag satisfactori run regist persist model model registri valid queri experi log metric current past run metric indic desir outcom loop step iter script deploi develop score script us model deploi model web servic azur iot edg devic monitor monitor data drift train dataset infer data deploi model necessari loop step retrain model new train data scenario highli recommend try design work azur notebook easi us want stick notebook think creat pipelin good thank yutong",
        "Question_gpt_summary_original":"the user is looking for a way to remotely update data stored in azure blob storage and retrain a model that has already been published as a web service.",
        "Question_gpt_summary":"user look wai remot updat data store azur blob storag retrain model publish web servic",
        "Answer_gpt_summary_original":"the answer provides a general workflow for machine learning model development and deployment. it suggests using azure designer or creating pipelines in azure notebook to remotely update data stored in azure blob storage and retrain a model that has already been published as a web service. the answer also emphasizes the importance of monitoring for data drift and retraining the model with new training data when necessary.",
        "Answer_gpt_summary":"answer provid gener workflow machin learn model develop deploy suggest azur design creat pipelin azur notebook remot updat data store azur blob storag retrain model publish web servic answer emphas import monitor data drift retrain model new train data necessari"
    },
    {
        "Question_id":57819173.0,
        "Question_title":"SageMaker AWS Binary Text Classification",
        "Question_body":"<p>Can AWS SageMaker handle binary classification using TFidf vectorized text as prediction base?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1567761634067,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":109.0,
        "Answer_body":"<p>You would have to use inference pipeline for your use case. What that means is that you will need to use a pre-processing step to featurize your text into tfidf and then feed into Sagemaker classification. Here's a <a href=\"https:\/\/stackoverflow.com\/questions\/57767899\/how-to-create-a-pipeline-in-sagemaker-with-pytorch\">SO answer<\/a> with more details around this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57819173",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1567815655550,
        "Question_original_content":"aw binari text classif handl binari classif tfidf vector text predict base",
        "Question_preprocessed_content":"aw binari text classif handl binari classif tfidf vector text predict base",
        "Answer_original_content":"us infer pipelin us case mean need us pre process step featur text tfidf feed classif answer detail",
        "Answer_preprocessed_content":"us infer pipelin us case mean need us step featur text tfidf feed classif answer detail",
        "Question_gpt_summary_original":"the user is looking to use tfidf vectorized text as a prediction base for binary classification on aws.",
        "Question_gpt_summary":"user look us tfidf vector text predict base binari classif aw",
        "Answer_gpt_summary_original":"to use tfidf vectorized text as a prediction base for binary classification on aws, the user needs to use an inference pipeline. this involves a pre-processing step to featurize the text into tfidf and then feed it into classification. more details on this process can be found in a stack overflow answer.",
        "Answer_gpt_summary":"us tfidf vector text predict base binari classif aw user need us infer pipelin involv pre process step featur text tfidf feed classif detail process stack overflow answer"
    },
    {
        "Question_id":null,
        "Question_title":"Pipline with many parallel stages",
        "Question_body":"<p>I would like to use DVC for project that\u2019s not ML but a sensitivity analysis for a bunch of a parameters of a simulation model.<\/p>\n<p>The DAG of my pipline looks basically like the diagram below. There\u2019s a single \u201ccommand\u201d file (box at the top) in which all simulations are defined (a csv file with one row for each simulation). Each column in the 3x4 blocks represents the set of steps for one simulation (preprocessing, running, postprocessing). Finally, combined results are created from groups of simulations (the 3x4 blocks).<\/p>\n<p>I want to make sure that, if a change is made to the command file for one simulation, only the stuff for that simulation is redone. It would be OK if the first step (the arrows from the command file to the first row) is always run for all simulations (this is a minor step). But for the following steps should be done only for the relevant simulations.<br>\nI could create a stage for every simulation and every step but this is hassle (in reality there are hundreds of simulations). Also I want to make it easy to add more simulations, by adding more lines to the command csv file.<\/p>\n<p>Is there functionality in DVC to handle this kind of situation? Is that a way to restructure my project to make things easier?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838.png\" data-download-href=\"\/uploads\/short-url\/hNYqo5HDfYLp8U1HKRw7uFANtxK.png?dl=1\" title=\"dvc_dag_example\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838_2_690x426.png\" alt=\"dvc_dag_example\" data-base62-sha1=\"hNYqo5HDfYLp8U1HKRw7uFANtxK\" width=\"690\" height=\"426\" srcset=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838_2_690x426.png, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838_2_1035x639.png 1.5x, https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/original\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard17\/uploads\/dataversioncontrol\/optimized\/1X\/7ccb231abb031bb8d5d06e05a6d363eaaff35838_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">dvc_dag_example<\/span><span class=\"informations\">1083\u00d7670 39.8 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1632401878212,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":304.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/pipline-with-many-parallel-stages\/904",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-23T15:01:49.893Z",
                "Answer_body":"<blockquote>\n<p>I want to make sure that, if a change is made to the command file for one simulation, only the stuff for that simulation is redone.<\/p>\n<\/blockquote>\n<p>How DVC pipelines work is that, if any dependencies of any stage you are targetting on the workspace we start from them and reproduce every stage they are connected through dependency\/output pair relations. You can create a dependency against the command file, which would allow you to re-run that stage (and all bound stages, if there are any) automatically.<\/p>\n<blockquote>\n<p>I could create a stage for every simulation and every step but this is hassle (in reality there are hundreds of simulations). Also I want to make it easy to add more simulations, by adding more lines to the command csv file.<\/p>\n<\/blockquote>\n<p>There are 2 options that I can see,<br>\na) Use foreach loops: they are relatively limited, so might not support your use case but if you want to repeat a stage with the same shape over different commands, then they should work<br>\nb) Generate your own DVC files: you can write a script that takes all the arguments you need and outputs a dvc.yaml file with the format you want (or can use <code>dvc stage add<\/code> instead of manually generating a YAML template).<\/p>\n<p>They both have their pluses\/minuses (esp regarding complexity), but might be worth to give a shot.<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-23T15:39:20.610Z",
                "Answer_body":"<p>One more thing to consider is what to do with your initial CSV file based on the options mentioned above:<br>\na) If you use foreach loops, you may want to consider restructuring your CSV file into a supported parameters file format: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#parameter-dependencies\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#parameter-dependencies<\/a>. This would allow you to have individual stages only depend on the specified parameters from that file.<br>\nb) If you take the suggested approach of generating your own dvc.yaml files, you can probably read in the CSV file to generate your pipeline without making it a dependency within the pipeline itself.<\/p>",
                "Answer_score":6.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-09-27T15:30:54.767Z",
                "Answer_body":"<p>Thanks. After reading up on parameter files and foreach I believe a combination of the two may be what I need.<\/p>",
                "Answer_score":16.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"piplin parallel stage like us project that sensit analysi bunch paramet simul model dag piplin look basic like diagram there singl command file box simul defin csv file row simul column block repres set step simul preprocess run postprocess final combin result creat group simul block want sure chang command file simul stuff simul redon step arrow command file row run simul minor step follow step relev simul creat stage simul step hassl realiti hundr simul want easi add simul ad line command csv file function handl kind situat wai restructur project thing easier dag exampl",
        "Question_preprocessed_content":"piplin parallel stage like us project that sensit analysi bunch paramet simul model dag piplin look basic like diagram there singl command file simul defin column block repres set step simul final combin result creat group simul want sure chang command file simul stuff simul redon step run simul follow step relev simul creat stage simul step hassl want easi add simul ad line command csv file function handl kind situat wai restructur project thing easier",
        "Answer_original_content":"want sure chang command file simul stuff simul redon pipelin work depend stage target workspac start reproduc stage connect depend output pair relat creat depend command file allow run stage bound stage automat creat stage simul step hassl realiti hundr simul want easi add simul ad line command csv file option us foreach loop rel limit support us case want repeat stage shape differ command work gener file write script take argument need output yaml file format want us stage add instead manual gener yaml templat pluse minus esp complex worth shot thing consid initi csv file base option mention us foreach loop want consid restructur csv file support paramet file format http org doc user guid project structur pipelin file paramet depend allow individu stage depend specifi paramet file suggest approach gener yaml file probabl read csv file gener pipelin make depend pipelin thank read paramet file foreach believ combin need",
        "Answer_preprocessed_content":"want sure chang command file simul stuff simul redon pipelin work depend stage target workspac start reproduc stage connect pair relat creat depend command file allow stage automat creat stage simul step hassl want easi add simul ad line command csv file option us foreach loop rel limit support us case want repeat stage shape differ command work gener file write script take argument need output yaml file format want worth shot thing consid initi csv file base option mention us foreach loop want consid restructur csv file support paramet file format allow individu stage depend specifi paramet file suggest approach gener yaml file probabl read csv file gener pipelin make depend pipelin thank read paramet file foreach believ combin need",
        "Question_gpt_summary_original":"the user is facing a challenge of creating a pipeline with many parallel stages for a sensitivity analysis of a simulation model, while ensuring that changes to the command file for one simulation only redo the relevant steps.",
        "Question_gpt_summary":"user face challeng creat pipelin parallel stage sensit analysi simul model ensur chang command file simul redo relev step",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions for creating a pipeline with many parallel stages for a sensitivity analysis of a simulation model, while ensuring that changes to the command file for one simulation only redo the relevant steps. the first solution is to use foreach loops, which are relatively limited but can work if you want to repeat a stage with the same shape over different commands. the second solution is to generate your own files by writing a script that takes all the arguments you need and outputs a .yaml file with the format you want. the answer also suggests restructuring the csv file into a supported parameters file format if using foreach loops and reading in the csv file to generate the pipeline without making it a dependency within the pipeline itself if generating your own .yaml files.",
        "Answer_gpt_summary":"answer suggest possibl solut creat pipelin parallel stage sensit analysi simul model ensur chang command file simul redo relev step solut us foreach loop rel limit work want repeat stage shape differ command second solut gener file write script take argument need output yaml file format want answer suggest restructur csv file support paramet file format foreach loop read csv file gener pipelin make depend pipelin gener yaml file"
    },
    {
        "Question_id":null,
        "Question_title":"Working with a small subset of remote data",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m trying to use DVC for managing data produced from simulations.  Each simulation produces a large number of small files, and a few very large files.  I have worked out how to set up a git repository with DVC and add the data (one .dvc file per simulation), and also how to push it to remote storage.  There are too many files to have one .dvc file per file. I think this is known as the \u201cdata registry\u201d use case.<\/p>\n<p>I then want to start a new project and work on, say, one of the simulations.<br>\nI think what I want to do is to \u201cdvc import\u201d the simulation data directory into the new project.  This works, and I can import just a single simulation.  However, I would like to be able to get just a small subset of the files within a simulation, without downloading the whole simulation.<\/p>\n<p>I think what I want to be able to do is to \u201cdvc import\u201d but tell it <em>not<\/em> to actually fetch the data, and then be able to tell it which files under the simulation I want to fetch.<\/p>\n<p>Is such a thing possible?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1604100899517,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":957.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/working-with-a-small-subset-of-remote-data\/541",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-31T00:33:07.337Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ianhinder\">@ianhinder<\/a><\/p>\n<p>Very interesting use case! How do you see this command? Will something like <code>--no-pull<\/code> option work for you:<\/p>\n<pre><code class=\"lang-auto\">dvc import --no-fetch https:\/\/github.com\/iterative\/dataset-registry use-cases\/cats-dogs\/\ndvc pull \\\n    cats-dogs\/data\/train\/dogs\/dog.100.jpg \\\n    cats-dogs\/data\/train\/dogs\/dog.101.jpg \\\n    cats-dogs\/data\/train\/dogs\/dog.102.jpg\n<\/code><\/pre>\n<p>Please let me know what other options you have in mind.<\/p>",
                "Answer_score":13.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-31T00:48:35.037Z",
                "Answer_body":"<aside class=\"quote no-group\" data-username=\"dmitry\" data-post=\"2\" data-topic=\"541\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard17\/user_avatar\/discuss.dvc.org\/dmitry\/40\/7_2.png\" class=\"avatar\"> dmitry:<\/div>\n<blockquote>\n<pre><code class=\"lang-auto\">dvc import --no-fetch https:\/\/github.com\/iterative\/dataset-registry use-cases\/cats-dogs\/\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>Yes, that\u2019s the sort of thing I had in mind.  It would also be good to somehow specify what I want to download beyond just a list of filenames.  For example, including wildcards, or maybe more advanced brace-expansion, such as<\/p>\n<pre><code class=\"lang-auto\">dvc pull cats-dogs\/data\/train\/{dogs,cats}\/**\/*.img\n<\/code><\/pre>\n<p>though I appreciate that if this is not yet somewhere in the code, it\u2019s quite a big project to design and add.  Possibly something like rsync\u2019s filter rules?  Again quite big to add.  But being able to specify the subsets without having to know exactly the filenames to download would be good, if there is a way to do that, so at least simple wildcards.<\/p>\n<p>With your idea, would we be able to have the simulation committed as a directory (i.e. one .dvc file), and then pull different parts under that?  I had the impression that if you try to do this at the moment, it pulls the whole directory.<\/p>",
                "Answer_score":38.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-31T05:06:39.239Z",
                "Answer_body":"<p><code>--no-fetch<\/code> - should be easy to implement and a partial pull (per file) can be used. However, the default pull (<code>dvc pull<\/code> or <code>dvc pull cats-dogs<\/code>) will still be downloading all the files. We need to implement a proper pattern mechanism to solve it. Below are the action points.<\/p>\n<h2>Implement --no-fetch<\/h2>\n<p><code>--no-fetch<\/code> might be useful for this scenario with a following partial, per-file pull or for a repository metadata change without downloading. I created an issue <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/4815\">https:\/\/github.com\/iterative\/dvc\/issues\/4815<\/a><\/p>\n<h2>Workaround<\/h2>\n<p>For now, as a workaround, you can create the dvc-file manually. The only tricky part here is the checksums that you will need to find and copy from the original repo: <code>rev_lock<\/code> and <code>outs.md5<\/code><\/p>\n<pre><code class=\"lang-auto\">md5: ''\nfrozen: true\ndeps:\n- path: use-cases\/cats-dogs\n  repo:\n    url: https:\/\/github.com\/iterative\/dataset-registry\n    rev_lock: f31f5c4cdae787b4bdeb97a717687d44667d9e62 # use master HEAD checksum from the original repo\nouts:\n- md5: 22e3f61e52c0ba45334d973244efc155.dir # find it in the dvc file in the original repo\n  path: cats-dogs\n<\/code><\/pre>\n<h2>Implement import\/pull by pattern<\/h2>\n<p>A proper solution will require to define a pattern in <code>import<\/code>. <code>dvc pull<\/code> should also support this pattern. The wildcard is one pattern.<\/p>\n<p>I\u2019d also include more advance patters link date <code>dvc pull users\/%Y\/%m\/%d\/users.csv?startdata=2020-09-01,enddate=now,ignoremissing<\/code><\/p>\n<p>Created another ticket: <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/4816\">https:\/\/github.com\/iterative\/dvc\/issues\/4816<\/a><\/p>",
                "Answer_score":93.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"work small subset remot data try us manag data produc simul simul produc larg number small file larg file work set git repositori add data file simul push remot storag file file file think known data registri us case want start new project work simul think want import simul data directori new project work import singl simul like abl small subset file simul download simul think want abl import tell actual fetch data abl tell file simul want fetch thing possibl",
        "Question_preprocessed_content":"work small subset remot data try us manag data produc simul simul produc larg number small file larg file work set git repositori add data push remot storag file file file think known data registri us case want start new project work simul think want import simul data directori new project work import singl simul like abl small subset file simul download simul think want abl import tell actual fetch data abl tell file simul want fetch thing possibl",
        "Answer_original_content":"ianhind interest us case command like pull option work import fetch http github com iter dataset registri us case cat dog pull cat dog data train dog dog jpg cat dog data train dog dog jpg cat dog data train dog dog jpg let know option mind dmitri import fetch http github com iter dataset registri us case cat dog ye that sort thing mind good specifi want download list filenam exampl includ wildcard mayb advanc brace expans pull cat dog data train dog cat img appreci code big project design add possibl like rsync filter rule big add abl specifi subset have know exactli filenam download good wai simpl wildcard idea abl simul commit directori file pull differ part impress try moment pull directori fetch easi implement partial pull file default pull pull pull cat dog download file need implement proper pattern mechan solv action point implement fetch fetch us scenario follow partial file pull repositori metadata chang download creat issu http github com iter issu workaround workaround creat file manual tricki checksum need copi origin repo rev lock out frozen true dep path us case cat dog repo url http github com iter dataset registri rev lock ffccdaebbdebadd us master head checksum origin repo out efecbadefc dir file origin repo path cat dog implement import pull pattern proper solut requir defin pattern import pull support pattern wildcard pattern includ advanc patter link date pull user user csv startdata enddat ignoremiss creat ticket http github com iter issu",
        "Answer_preprocessed_content":"interest us case command like option work let know option mind dmitri ye that sort thing mind good specifi want download list filenam exampl includ wildcard mayb advanc appreci code big project design add possibl like rsync filter rule big add abl specifi subset have know exactli filenam download good wai simpl wildcard idea abl simul commit directori pull differ part impress try moment pull directori easi implement partial pull default pull download file need implement proper pattern mechan solv action point implement us scenario follow partial pull repositori metadata chang download creat issu workaround workaround creat file manual tricki checksum need copi origin repo implement pattern proper solut requir defin pattern support pattern wildcard pattern includ advanc patter link date creat ticket",
        "Question_gpt_summary_original":"the user is encountering challenges with working with a small subset of remote data, specifically trying to import a single simulation and then be able to tell it which files under the simulation they want to fetch without downloading the whole simulation.",
        "Question_gpt_summary":"user encount challeng work small subset remot data specif try import singl simul abl tell file simul want fetch download simul",
        "Answer_gpt_summary_original":"the user is trying to import a single simulation and then be able to tell it which files under the simulation they want to fetch without downloading the whole simulation. the solution proposed is to use the --no-fetch option and a partial, per-file pull. however, the default pull will still download all the files. a proper solution would require defining a pattern in import and pull should also support this pattern. the wildcard is one pattern, and more advanced patterns like date pull can also be included. workarounds include creating the file manually and finding and copying the checksums from the original repo. two issues have been created to implement these solutions.",
        "Answer_gpt_summary":"user try import singl simul abl tell file simul want fetch download simul solut propos us fetch option partial file pull default pull download file proper solut requir defin pattern import pull support pattern wildcard pattern advanc pattern like date pull includ workaround includ creat file manual find copi checksum origin repo issu creat implement solut"
    },
    {
        "Question_id":66817781.0,
        "Question_title":"What are the differences between AWS sagemaker and sagemaker_pyspark?",
        "Question_body":"<p>I'm currently running a quick Machine Learning proof of concept on AWS with SageMaker, and I've come across two libraries: <code>sagemaker<\/code> and <code>sagemaker_pyspark<\/code>. I would like to work with distributed data. My questions are:<\/p>\n<ol>\n<li><p>Is using <code>sagemaker<\/code> the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented <code>sagemaker_pyspark<\/code>? Based on this assumption, I do not understand what it would offer regarding using <code>scikit-learn<\/code> on a SageMaker notebook (in terms of computing capabilities).<\/p>\n<\/li>\n<li><p>Is it normal for something like <code>model = xgboost_estimator.fit(training_data)<\/code> to take 4 minutes to run with <code>sagemaker_pyspark<\/code> for a small set of test data? I see that what it does below is to train the model and also create an Endpoint to be able to offer its predictive services, and I assume that this endpoint is deployed on an EC2 instance that is created and started at the moment. Correct me if I'm wrong. I assume this from how the estimator is defined:<\/p>\n<\/li>\n<\/ol>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker_pyspark.algorithms import XGBoostSageMakerEstimator\n\n\nxgboost_estimator = XGBoostSageMakerEstimator (\n    trainingInstanceType = &quot;ml.m4.xlarge&quot;,\n    trainingInstanceCount = 1,\n    endpointInstanceType = &quot;ml.m4.xlarge&quot;,\n    endpointInitialInstanceCount = 1,\n    sagemakerRole = IAMRole(get_execution_role())\n)\n\nxgboost_estimator.setNumRound(1)\n<\/code><\/pre>\n<p>If so, is there a way to reuse the same endpoint with different training jobs so that I don't have to wait for a new endpoint to be created each time?<\/p>\n<ol start=\"3\">\n<li><p>Does <code>sagemaker_pyspark<\/code> support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/p>\n<\/li>\n<li><p>Do you know if <code>sagemaker_pyspark<\/code> can perform hyperparameter optimization? From what I see, <code>sagemaker<\/code> offers the <code>HyperparameterTuner<\/code> class, but I can't find anything like it in <code>sagemaker_pyspark<\/code>. I suppose it is a more recent library and there is still a lot of functionality to implement.<\/p>\n<\/li>\n<li><p>I am a bit confused about the concept of <code>entry_point<\/code> and <code>container<\/code>\/<code>image_name<\/code> (both possible input arguments for the <code>Estimator<\/code> object from the <code>sagemaker<\/code> library): can you deploy models with and without containers? why would you use model containers? Do you always need to define the model externally with the <code>entry_point<\/code> script? It is also confusing that the class <code>AlgorithmEstimator<\/code> allows the input argument <code>algorithm_arn<\/code>; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/li>\n<li><p>I see the <code>sagemaker<\/code> library offers SageMaker Pipelines, which seem to be very handy for deploying properly structured ML workflows. However, I don't think this is available with <code>sagemaker_pyspark<\/code>, so in that case, I would rather create my workflows with a combination of Step Functions (to orchestrate the entire thing), Glue processes (for ETL, preprocessing and feature\/target engineering) and SageMaker processes using <code>sagemaker_pyspark<\/code>.<\/p>\n<\/li>\n<li><p>I also found out that <code>sagemaker<\/code> has the <code>sagemaker.sparkml.model.SparkMLModel<\/code> object. What is the difference between this and what <code>sagemaker_pyspark<\/code> offers?<\/p>\n<\/li>\n<\/ol>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1616764992750,
        "Question_favorite_count":null,
        "Question_last_edit_time":1618398204670,
        "Question_score":1.0,
        "Question_view_count":242.0,
        "Answer_body":"<p><code>sagemaker<\/code> is the SageMaker Python SDK. It calls SageMaker-related AWS service APIs on your behalf. You don't need to use it, but it can make life easier<\/p>\n<blockquote>\n<ol>\n<li>Is using sagemaker the equivalent of running a training job without taking advantage of the distributed computing capabilities of AWS? I assume it is, if not, why have they implemented sagemaker_pyspark?<\/li>\n<\/ol>\n<\/blockquote>\n<p>No. You can run distributed training jobs using <code>sagemaker<\/code> (see <code>instance_count<\/code> parameter)<\/p>\n<p><code>sagemaker_pyspark<\/code> facilitates calling SageMaker-related AWS service APIs from Spark. Use it if you want to use SageMaker services from Spark<\/p>\n<blockquote>\n<ol start=\"2\">\n<li>Is it normal for something like model = xgboost_estimator.fit(training_data) to take 4 minutes to run with sagemaker_pyspark for a small set of test data?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, it takes a few minutes for an EC2 instance to spin-up. Use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#local-mode\" rel=\"nofollow noreferrer\">Local Mode<\/a> if you want to iterate more quickly locally. Note: Local Mode won't work with SageMaker built-in algorithms, but you can prototype with (non AWS) XGBoost\/SciKit-Learn<\/p>\n<blockquote>\n<ol start=\"3\">\n<li>Does sagemaker_pyspark support custom algorithms? Or does it only allow you to use the predefined ones in the library?<\/li>\n<\/ol>\n<\/blockquote>\n<p>Yes, but you'd probably want to extend <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/api.html#sagemakerestimator\" rel=\"nofollow noreferrer\">SageMakerEstimator<\/a>. Here you can provide the <code>trainingImage<\/code> URI<\/p>\n<blockquote>\n<ol start=\"4\">\n<li>Do you know if sagemaker_pyspark can perform hyperparameter optimization?<\/li>\n<\/ol>\n<\/blockquote>\n<p>It does not appear so. It'd probably be easier just to do this from SageMaker itself though<\/p>\n<blockquote>\n<p>can you deploy models with and without containers?<\/p>\n<\/blockquote>\n<p>You can certainly host your own models any way you want. But if you want to use SageMaker model inference hosting, then containers are required<\/p>\n<blockquote>\n<p>why would you use model containers?<\/p>\n<\/blockquote>\n<blockquote>\n<p>Do you always need to define the model externally with the entry_point script?<\/p>\n<\/blockquote>\n<p>The whole Docker thing makes bundling dependencies easier, and also makes things language\/runtime-neutral. SageMaker doesn't care if your algorithm is in Python or Java or Fortran. But it needs to know how to &quot;run&quot; it, so you tell it a working directory and a command to run. This is the entry point<\/p>\n<blockquote>\n<p>It is also confusing that the class AlgorithmEstimator allows the input argument algorithm_arn; I see there are three different ways of passing a model as input, why? which one is better?<\/p>\n<\/blockquote>\n<p>Please clarify which &quot;three&quot; you are referring to<\/p>\n<p>6 is not a question, so no answer required :)<\/p>\n<blockquote>\n<ol start=\"7\">\n<li>What is the difference between this and what sagemaker_pyspark offers?<\/li>\n<\/ol>\n<\/blockquote>\n<p>sagemaker_pyspark lets you call SageMaker services from Spark, whereas SparkML Serving lets you use Spark ML services from SageMaker<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1634230982412,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66817781",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1617119678127,
        "Question_original_content":"differ pyspark current run quick machin learn proof concept aw come librari pyspark like work distribut data question equival run train job take advantag distribut comput capabl aw assum implement pyspark base assumpt understand offer scikit learn notebook term comput capabl normal like model xgboost estim fit train data minut run pyspark small set test data train model creat endpoint abl offer predict servic assum endpoint deploi instanc creat start moment correct wrong assum estim defin import execut role pyspark algorithm import xgboostestim xgboost estim xgboostestim traininginstancetyp xlarg traininginstancecount endpointinstancetyp xlarg endpointinitialinstancecount role iamrol execut role xgboost estim setnumround wai reus endpoint differ train job wait new endpoint creat time pyspark support custom algorithm allow us predefin on librari know pyspark perform hyperparamet optim offer hyperparametertun class like pyspark suppos recent librari lot function implement bit confus concept entri point contain imag possibl input argument estim object librari deploi model contain us model contain need defin model extern entri point script confus class algorithmestim allow input argument algorithm arn differ wai pass model input better librari offer pipelin handi deploi properli structur workflow think avail pyspark case creat workflow combin step function orchestr entir thing glue process etl preprocess featur target engin process pyspark sparkml model sparkmlmodel object differ pyspark offer",
        "Question_preprocessed_content":"differ current run quick machin learn proof concept aw come librari like work distribut data question equival run train job take advantag distribut comput capabl aw assum implement base assumpt understand offer notebook normal like minut run small set test data train model creat endpoint abl offer predict servic assum endpoint deploi instanc creat start moment correct wrong assum estim defin wai reus endpoint differ train job wait new endpoint creat time support custom algorithm allow us predefin on librari know perform hyperparamet optim offer class like suppos recent librari lot function implement bit confus concept deploi model contain us model contain need defin model extern script confus class allow input argument differ wai pass model input better librari offer pipelin handi deploi properli structur workflow think avail case creat workflow combin step function glue process process object differ offer",
        "Answer_original_content":"python sdk call relat aw servic api behalf need us life easier equival run train job take advantag distribut comput capabl aw assum implement pyspark run distribut train job instanc count paramet pyspark facilit call relat aw servic api spark us want us servic spark normal like model xgboost estim fit train data minut run pyspark small set test data ye take minut instanc spin us local mode want iter quickli local note local mode won work built algorithm prototyp non aw xgboost scikit learn pyspark support custom algorithm allow us predefin on librari ye probabl want extend estim provid trainingimag uri know pyspark perform hyperparamet optim appear probabl easier deploi model contain certainli host model wai want want us model infer host contain requir us model contain need defin model extern entri point script docker thing make bundl depend easier make thing languag runtim neutral care algorithm python java fortran need know run tell work directori command run entri point confus class algorithmestim allow input argument algorithm arn differ wai pass model input better clarifi refer question answer requir differ pyspark offer pyspark let servic spark sparkml serv let us spark servic",
        "Answer_preprocessed_content":"python sdk call relat aw servic api behalf need us life easier equival run train job take advantag distribut comput capabl aw assum implement run distribut train job facilit call relat aw servic api spark us want us servic spark normal like model minut run small set test data ye take minut instanc us local mode want iter quickli local note local mode won work algorithm prototyp support custom algorithm allow us predefin on librari ye probabl want extend estim provid uri know perform hyperparamet optim appear probabl easier deploi model contain certainli host model wai want want us model infer host contain requir us model contain need defin model extern script docker thing make bundl depend easier make thing care algorithm python java fortran need know run tell work directori command run entri point confus class algorithmestim allow input argument differ wai pass model input better clarifi refer question answer requir differ offer let servic spark sparkml serv let us spark servic",
        "Question_gpt_summary_original":"the user is trying to understand the differences between and _pyspark, and is encountering challenges related to distributed computing capabilities, model deployment, custom algorithms, hyperparameter optimization, entry points, containers, and pipelines.",
        "Question_gpt_summary":"user try understand differ pyspark encount challeng relat distribut comput capabl model deploy custom algorithm hyperparamet optim entri point contain pipelin",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n\n- use _pyspark to run distributed training jobs and call -related aws service apis from spark.\n- use local mode if you want to iterate more quickly locally, but note that it won't work with built-in algorithms.\n- extend estimator to support custom algorithms and provide the training image uri.\n- use model containers for model inference hosting, as they make bundling dependencies easier and language\/runtime-neutral.\n- define the model externally with the entry_point script, which specifies the working directory and command to run.\n- use sparkml serving to use spark ml services from spark.",
        "Answer_gpt_summary":"possibl solut extract answer us pyspark run distribut train job relat aw servic api spark us local mode want iter quickli local note won work built algorithm extend estim support custom algorithm provid train imag uri us model contain model infer host bundl depend easier languag runtim neutral defin model extern entri point script specifi work directori command run us sparkml serv us spark servic spark"
    },
    {
        "Question_id":null,
        "Question_title":"Run Python Script in Azure Machine Learning pipeline",
        "Question_body":"Hi\n\nI have a python script which will take parameters from Azure Data Factory and automatically register Datasets in Azure ML Workspace. I have already prepared this script. And I have run in the Notebooks and is working fine.\n\nI want to create a Pipeline in AML and call this python Script which will not have any input datasets and call this pipeline from ADF using PipelineID.\n\nBut when I am creating a Pipeline, system is showing default function azureml_main. I do not want to use this. How to solve this issue",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1652785260630,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/852593\/run-python-script-in-azure-machine-learning-pipeli.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-31T09:35:36.767Z",
                "Answer_score":0,
                "Answer_body":"Python SDK is more flexible obviously. I was suffering from the complicated UI then back to SDK. You can publish model in SDk then back to studio.\n\nBut they do integrate useful thing.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"run python script pipelin python script paramet azur data factori automat regist dataset workspac prepar script run notebook work fine want creat pipelin aml python script input dataset pipelin adf pipelineid creat pipelin show default function main want us solv issu",
        "Question_preprocessed_content":"run python script pipelin python script paramet azur data factori automat regist dataset workspac prepar script run notebook work fine want creat pipelin aml python script input dataset pipelin adf pipelineid creat pipelin show default function want us solv issu",
        "Answer_original_content":"python sdk flexibl obvious suffer complic sdk publish model sdk studio integr us thing",
        "Answer_preprocessed_content":"python sdk flexibl obvious suffer complic sdk publish model sdk studio integr us thing",
        "Question_gpt_summary_original":"the user is trying to create a pipeline in  to call a python script which will take parameters from azure data factory and automatically register datasets in a workspace, but is encountering an issue with the system showing a default function _main.",
        "Question_gpt_summary":"user try creat pipelin python script paramet azur data factori automat regist dataset workspac encount issu show default function main",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n- use python sdk instead of the complicated ui to create the pipeline in .\n- publish the model using sdk and then switch back to the studio.\n-  integrates useful features.",
        "Answer_gpt_summary":"possibl solut answer us python sdk instead complic creat pipelin publish model sdk switch studio integr us featur"
    },
    {
        "Question_id":null,
        "Question_title":"Does dvc have a model sharing feature?",
        "Question_body":"<p>Data versioning of models is possible using DVC, but is there a way to centralize all models so that users can share and resuse models?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1671113441042,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":41.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/does-dvc-have-a-model-sharing-feature\/1418",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-15T17:31:12.403Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/upasana\">@upasana<\/a> !<\/p>\n<p>You might find this blog post useful <a href=\"https:\/\/iterative.ai\/blog\/gto-model-registry\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Building a GitOps ML Model Registry with DVC and GTO<\/a><\/p>",
                "Answer_score":45.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"model share featur data version model possibl wai central model user share resus model",
        "Question_preprocessed_content":"model share featur data version model possibl wai central model user share resus model",
        "Answer_original_content":"upasana blog post us build gitop model registri gto",
        "Answer_preprocessed_content":"blog post us build gitop model registri gto",
        "Question_gpt_summary_original":"the user is inquiring about a model sharing feature in order to centralize and share models for data versioning.",
        "Question_gpt_summary":"user inquir model share featur order central share model data version",
        "Answer_gpt_summary_original":"possible solution: the answer suggests using a gitops ml model registry with gto as a way to centralize and share models for data versioning.",
        "Answer_gpt_summary":"possibl solut answer suggest gitop model registri gto wai central share model data version"
    },
    {
        "Question_id":null,
        "Question_title":"Pipeline stops at train model stage",
        "Question_body":"Hello there,\n\nWhen I am running the following steps of the pipeline, I'm getting this error at the \"train model\" stage.\n\nCan anyone explain why I'm getting this error?",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1642031139267,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/693063\/pipeline-stops-at-train-model-stage.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-13T23:43:46.163Z",
                "Answer_score":0,
                "Answer_body":"After your thoughtful suggestion this is what I got in std_log.txt after clicking failed module ( train model ) :\n\nDo I need to fix the error for all the following steps ?\n\nFile \"\/azureml-envs\/azureml_27f4babfbdfccc3e0926823cfdde349d\/lib\/python3.6\/site-packages\/azureml\/studio\/modules\/datatransform\/common\/named_encoder.py\", line 105, in validate_series\ncolumn_name=series.name)\n\nFile \"\/azureml-envs\/azureml_27f4babfbdfccc3e0926823cfdde349d\/lib\/python3.6\/site-packages\/azureml\/studio\/modules\/datatransform\/common\/named_encoder.py\", line 157, in _check_too_many_unique_values\ntroubleshoot_hint=\"Find the explanation and resolution in https:\/\/docs.microsoft.com\/en-us\/\"\n\nFile \"\/azureml-envs\/azureml_27f4babfbdfccc3e0926823cfdde349d\/lib\/python3.6\/site-packages\/azureml\/studio\/common\/error.py\", line 835, in throw\nraise err\n> err = ColumnUniqueValuesExceededError('Number of unique values in column: \"TotalCharges\" is greater than allowed. Find the explanation and resolution in https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/designer-error-codes#error-0014',)",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-14T03:38:27.7Z",
                "Answer_score":1,
                "Answer_body":"@Dexter-9539 Yes, The error needs to be resolved. It looks like a data error and if you follow the link mentioned in the error the steps indicate possible ways to remove the unique values that are not required to be passed for training. I would recommend to follow this suggestion based on the error.\n\nFor ID columns which is not meaningful features during training a model, you can use Edit Metadata to mark that column as Clear feature and it will not be used during training a model.\n\nYou can mark TotalCharges as clear feature in this case on the edit metadata module and check if it works. Thanks!!\n\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pipelin stop train model stage hello run follow step pipelin get error train model stage explain get error",
        "Question_preprocessed_content":"pipelin stop train model stage hello run follow step pipelin get error train model stage explain get error",
        "Answer_original_content":"thought suggest got std log txt click fail modul train model need fix error follow step file env fbabfbdfecfd lib python site packag studio modul datatransform common name encod line valid seri column seri file env fbabfbdfecfd lib python site packag studio modul datatransform common name encod line check uniqu valu troubleshoot hint explan resolut http doc microsoft com file env fbabfbdfecfd lib python site packag studio common error line throw rais err err columnuniquevaluesexceedederror number uniqu valu column totalcharg greater allow explan resolut http doc microsoft com azur machin learn algorithm modul refer design error code error dexter ye error need resolv look like data error follow link mention error step indic possibl wai remov uniqu valu requir pass train recommend follow suggest base error column meaning featur train model us edit metadata mark column clear featur train model mark totalcharg clear featur case edit metadata modul check work thank answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"thought suggest got click fail modul need fix error follow step file line file line explan resolut file line throw rais err err columnuniquevaluesexceedederror ye error need resolv look like data error follow link mention error step indic possibl wai remov uniqu valu requir pass train recommend follow suggest base error column meaning featur train model us edit metadata mark column clear featur train model mark totalcharg clear featur case edit metadata modul check work thank answer help click upvot help commun member read thread",
        "Question_gpt_summary_original":"the user is encountering an error at the \"train model\" stage of their pipeline.",
        "Question_gpt_summary":"user encount error train model stage pipelin",
        "Answer_gpt_summary_original":"the user needs to fix the error encountered during the \"train model\" stage of their pipeline. the error seems to be related to the number of unique values in a column, and the solution involves following the link provided in the error message to remove the unnecessary unique values. additionally, the user can mark certain columns as clear features using the edit metadata module to exclude them from the training process.",
        "Answer_gpt_summary":"user need fix error encount train model stage pipelin error relat number uniqu valu column solut involv follow link provid error messag remov unnecessari uniqu valu addition user mark certain column clear featur edit metadata modul exclud train process"
    },
    {
        "Question_id":65889143.0,
        "Question_title":"upload image dataset to S3 sagemaker",
        "Question_body":"<p>my dataset is 3 folders (train, validation and test) of images. each folder has two subfolders (cat1 and cat2). I am using AWS sage maker to preprocess my data and train my model. we all know that we have to upload the training data to S3 bucket before starting the &quot;.fit&quot; process.\nI want to know how to upload my data set to S3<\/p>\n<pre><code># general prefix\nprefix='chest-xray'\n#unique train\/test prefixes\ntrain_prefix   = '{}\/{}'.format(prefix, 'train')\nval_prefix   = '{}\/{}'.format(prefix, 'validation')\ntest_prefix    = '{}\/{}'.format(prefix, 'test')\n\n# uploading data to S3, and saving locations\ntrain_path  = sagemaker_session.upload_data(train_data, bucket=bucket, key_prefix=train_prefix)\n<\/code><\/pre>\n<p>what the train_data parameters should look like<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611594075077,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":444.0,
        "Answer_body":"<p>According to the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/session.html#sagemaker.session.Session.upload_data\" rel=\"nofollow noreferrer\">documentation<\/a> <code>train_data<\/code> is the local path of the file to upload to S3, so you need this file locally where you are launching the training job. If you are using a notebook this is not the way to do. You have instead to manually upload your dataset in a S3 bucket. I suggest to preprocess your dataset in a single file (tfrecord for example if you are using TF) and upload that file to S3. You can do it using the AWS web console or using the AWS-CLI with the <code>aws s3 cp yourfile s3:\/\/your-bucket <\/code>command.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65889143",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1611678605710,
        "Question_original_content":"upload imag dataset dataset folder train valid test imag folder subfold cat cat aw sage maker preprocess data train model know upload train data bucket start fit process want know upload data set gener prefix prefix chest xrai uniqu train test prefix train prefix format prefix train val prefix format prefix valid test prefix format prefix test upload data save locat train path session upload data train data bucket bucket kei prefix train prefix train data paramet look like",
        "Question_preprocessed_content":"upload imag dataset dataset folder imag folder subfold aw sage maker preprocess data train model know upload train data bucket start process want know upload data set paramet look like",
        "Answer_original_content":"accord document train data local path file upload need file local launch train job notebook wai instead manual upload dataset bucket suggest preprocess dataset singl file tfrecord exampl upload file aw web consol aw cli aw yourfil bucket command",
        "Answer_preprocessed_content":"accord document local path file upload need file local launch train job notebook wai instead manual upload dataset bucket suggest preprocess dataset singl file upload file aw web consol command",
        "Question_gpt_summary_original":"the user is facing a challenge in uploading their image dataset to an s3 bucket in order to use aws sage maker to preprocess and train their model.",
        "Question_gpt_summary":"user face challeng upload imag dataset bucket order us aw sage maker preprocess train model",
        "Answer_gpt_summary_original":"the answer suggests that the user needs to have the file locally where they are launching the training job. if they are using a notebook, they need to manually upload their dataset to an s3 bucket. the answer recommends preprocessing the dataset in a single file, such as tfrecord, and uploading that file to s3 using either the aws web console or the aws cli.",
        "Answer_gpt_summary":"answer suggest user need file local launch train job notebook need manual upload dataset bucket answer recommend preprocess dataset singl file tfrecord upload file aw web consol aw cli"
    },
    {
        "Question_id":null,
        "Question_title":"Can Sagemaker RStudio mount an NFS share?",
        "Question_body":"Our existing Python code currently accesses data files residing on a file system. It navigates through folders to reach individual files and reads them.\n\nWe would now like to migrate the code to notebooks hosted in Sagemaker Studio. The data files will be hosted in S3 buckets. However, we do not want to modify the code to use S3 API to access files. Instead, we would like to continue using a filesystem view of the files. What is the best way to make the S3 objects visible to the Sagemaker Studio as a filesystem? Please note that we would like to mount in Sagemaker studio and not on Sagemaker notebook.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1665409392840,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":25.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqjjos-zZQhCU4vxTSFSDLQ\/can-sagemaker-r-studio-mount-an-nfs-share",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-12T16:58:30.223Z",
                "Answer_score":0,
                "Answer_body":"SageMaker Studio uses EFS for storage. So you could potentially do an aws s3 sync and sync your S3 files\/folders to EFS and use them as a filesystem. I'd also recommend to be cautious of the file sizes since EFS is more expensive.\n\nStudio now supports JupyterLab3 and you can now install popular extensions. So if you're just looking at viewing the S3 files from Studio, you can use the S3 file browser extension.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"rstudio mount nf share exist python code current access data file resid file navig folder reach individu file read like migrat code notebook host studio data file host bucket want modifi code us api access file instead like continu filesystem view file best wai object visibl studio filesystem note like mount studio notebook",
        "Question_preprocessed_content":"rstudio mount nf share exist python code current access data file resid file navig folder reach individu file read like migrat code notebook host studio data file host bucket want modifi code us api access file instead like continu filesystem view file best wai object visibl studio filesystem note like mount studio notebook",
        "Answer_original_content":"studio us ef storag potenti aw sync sync file folder ef us filesystem recommend cautiou file size ef expens studio support jupyterlab instal popular extens look view file studio us file browser extens",
        "Answer_preprocessed_content":"studio us ef storag potenti aw sync sync ef us filesystem recommend cautiou file size ef expens studio support jupyterlab instal popular extens look view file studio us file browser extens",
        "Question_gpt_summary_original":"the user is looking for a way to make s3 objects visible to rstudio as a filesystem, while avoiding the need to modify existing code to use s3 api.",
        "Question_gpt_summary":"user look wai object visibl rstudio filesystem avoid need modifi exist code us api",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. sync s3 files\/folders to efs and use them as a filesystem in rstudio.\n2. be cautious of file sizes since efs is more expensive.\n3. use the s3 file browser extension if only viewing s3 files from rstudio.",
        "Answer_gpt_summary":"possibl solut answer sync file folder ef us filesystem rstudio cautiou file size ef expens us file browser extens view file rstudio"
    },
    {
        "Question_id":null,
        "Question_title":"Machine Learning Model Deployment",
        "Question_body":"I am new to ML model and am researching using Azure Databricks and MLFlow to train a model. My question is once the model is created, is there a way to host the model that can be downloaded and inferenced remotely ? I am looking for options other than deploying it as a REST endpoint.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593620777527,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/41755\/machine-learning-model-deployment.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-07T08:37:52.993Z",
                "Answer_score":0,
                "Answer_body":"@MaheshSivan-3675 I think this video from MS build 2019 will help you understand on the integration of databricks with mlflow and Azure ML. Some of the UI or ML portal seen in demo might have changed but the functionality remains the same.\n\n\n\n\nWhile using Azure ML you can register and download your model which can be deployed to different targets other than a REST API.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":1.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"machin learn model deploy new model research azur databrick train model question model creat wai host model download inferenc remot look option deploi rest endpoint",
        "Question_preprocessed_content":"machin learn model deploy new model research azur databrick train model question model creat wai host model download inferenc remot look option deploi rest endpoint",
        "Answer_original_content":"maheshsivan think video build help understand integr databrick portal seen demo chang function remain regist download model deploi differ target rest api",
        "Answer_preprocessed_content":"think video build help understand integr databrick portal seen demo chang function remain regist download model deploi differ target rest api",
        "Question_gpt_summary_original":"the user is looking for options to host a machine learning model created with azure databricks and , other than deploying it as a rest endpoint.",
        "Question_gpt_summary":"user look option host machin learn model creat azur databrick deploi rest endpoint",
        "Answer_gpt_summary_original":"the answer suggests that the user can register and download their machine learning model created with azure databricks and , and deploy it to different targets other than a rest api. the video from ms build 2019 is recommended for understanding the integration of databricks with .",
        "Answer_gpt_summary":"answer suggest user regist download machin learn model creat azur databrick deploi differ target rest api video build recommend understand integr databrick"
    },
    {
        "Question_id":71752458.0,
        "Question_title":"MLflow Experiments Tracking : local (dev tools - vscode) to databricks workspace",
        "Question_body":"<p>I had configured my databricks workspace in local using,<\/p>\n<p><code>databricks configure --profile &lt;profile_name&gt; --token<\/code><\/p>\n<p>by which I am able to list the clusters and create secret scope.<\/p>\n<p>But I am unable to create mlflow experiments. I had set the tracking uri to &quot;databricks&quot; and also tested with &quot;databricks\/&lt;profile_name&quot; and tested but i am unable to create or track any experiments on my databricks workspace.<\/p>\n<p>I get this following error;<\/p>\n<p><code>from mlflow.tracking import MlflowClient client = MlflowClient() mlflow.set_tracking_uri(&quot;databricks&quot;) experiment =  client.get_experiment_by_name('\/Shared\/test')<\/code><\/p>\n<p>MlflowException: API request to endpoint was successful but the response body was not in a valid JSON format. Response body: '&lt;!doctype html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=&quot;utf-8&quot;\/&gt;&lt;meta http-equiv=&quot;Content-Language&quot; content=&quot;en&quot;\/&gt;&lt;title&gt;Databricks - Sign In&lt;\/title&gt;&lt;meta name=&quot;viewport&quot; content=&quot;width=960&quot;\/&gt;&lt;link rel=&quot;icon&quot; type=&quot;image\/png&quot; href=&quot;\/favicon.ico&quot;\/&gt;&lt;meta http-equiv=&quot;content-type&quot; content=&quot;text\/html; charset=UTF8&quot;\/&gt;&lt;link rel=&quot;icon&quot; href=&quot;\/favicon.ico&quot;&gt;&lt;script defer=&quot;defer&quot; src=&quot;\/login\/login.0ceb14c0.js&quot;&gt;&lt;\/script&gt;&lt;\/head&gt;&lt;body class=&quot;light-mode&quot;&gt;&lt;uses-legacy-bootstrap&gt;&lt;div id=&quot;login-page&quot;&gt;&lt;\/div&gt;&lt;\/uses-legacy-bootstrap&gt;&lt;\/body&gt;&lt;\/html&gt;'<\/p>\n<p>Could someone help me on what I am missing here?<\/p>\n<p>I am expecting to create\/track mlflow experiements in databricks workspace via dev-tools(vscode).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649164485623,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":220.0,
        "Answer_body":"<p>I had the same problem while trying to load a model from model registry with mismatching versions (client 1.22.0).<\/p>\n<p>I had to downgrade the client version to make it work.<\/p>\n<p>Downgraded first the client to 1.21 and then server to 1.20<\/p>\n<p>Refer - <a href=\"https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage\" rel=\"nofollow noreferrer\">https:\/\/docs.databricks.com\/dev-tools\/api\/latest\/mlflow.html#operation\/transition-model-version-stage<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71752458",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649225844710,
        "Question_original_content":"experi track local dev tool vscode databrick workspac configur databrick workspac local databrick configur profil token abl list cluster creat secret scope unabl creat experi set track uri databrick test databrick databrick sign help miss expect creat track experi databrick workspac dev tool vscode",
        "Question_preprocessed_content":"experi track local databrick workspac configur databrick workspac local abl list cluster creat secret scope unabl creat experi set track uri databrick test test unabl creat track experi databrick workspac follow error except api request endpoint success respons bodi valid json format respons bodi databrick sign viewport rel icon rel icon defer defer help miss expect experi databrick workspac",
        "Answer_original_content":"problem try load model model registri mismatch version client downgrad client version work downgrad client server refer http doc databrick com dev tool api latest html oper transit model version stage",
        "Answer_preprocessed_content":"problem try load model model registri mismatch version downgrad client version work downgrad client server refer",
        "Question_gpt_summary_original":"the user is encountering challenges with configuring their databricks workspace in local using dev tools (vscode) to create and track experiments.",
        "Question_gpt_summary":"user encount challeng configur databrick workspac local dev tool vscode creat track experi",
        "Answer_gpt_summary_original":"possible solution: downgrade the client version to 1.21 and then the server to 1.20 to resolve the issue of loading a model from the model registry with mismatching versions. refer to the documentation for more information.",
        "Answer_gpt_summary":"possibl solut downgrad client version server resolv issu load model model registri mismatch version refer document inform"
    },
    {
        "Question_id":null,
        "Question_title":"Get stuck at uploading at the end",
        "Question_body":"<p>My program finished before a few hours, but wandb get stuck at uploading files until now. I asked my friend and he said he had the same problem. So there are any problem for wandb server in the past  few hours and how to fix it?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/8\/8fd37530d06301653d3129bbe42f02a5a1061cdf.jpeg\" data-download-href=\"\/uploads\/short-url\/kwlk7giIWlj5N5PsCmnxPYQd8d1.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/8fd37530d06301653d3129bbe42f02a5a1061cdf_2_690x83.jpeg\" alt=\"image\" data-base62-sha1=\"kwlk7giIWlj5N5PsCmnxPYQd8d1\" width=\"690\" height=\"83\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/8fd37530d06301653d3129bbe42f02a5a1061cdf_2_690x83.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/8fd37530d06301653d3129bbe42f02a5a1061cdf_2_1035x124.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/8fd37530d06301653d3129bbe42f02a5a1061cdf_2_1380x166.jpeg 2x\" data-dominant-color=\"181717\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">2122\u00d7256 120 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1669253840661,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":576.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/get-stuck-at-uploading-at-the-end\/3453",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-29T01:15:31.935Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/yueyang\">@yueyang<\/a> , happy to look into this for you. We did not have any network related issues last week that would have prevented users from logging experiments. Are you still experiencing this issue? If yes, could you please:<\/p>\n<ul>\n<li>Expand on the type of experiment you were running and approximately how many files and their size you were attempting to upload.<\/li>\n<li>Share the <code>debug.log<\/code> and <code>debug-internal.log<\/code> files of the hanging runs. These files are located in the working directory under<code> wandb\/<\/code> folder<\/li>\n<li>Link to your private workspace for us to review<\/li>\n<\/ul>\n<p>Thank you<\/p>",
                "Answer_score":1.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-01T23:18:35.210Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/yueyang\">@yueyang<\/a>  since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-30T23:19:17.434Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"stuck upload end program finish hour stuck upload file ask friend said problem problem server past hour fix imag",
        "Question_preprocessed_content":"stuck upload end program finish hour stuck upload file ask friend said problem problem server past hour fix imag",
        "Answer_original_content":"yueyang happi look network relat issu week prevent user log experi experienc issu ye expand type experi run approxim file size attempt upload share debug log debug intern log file hang run file locat work directori folder link privat workspac review thank yueyang heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"happi look network relat issu week prevent user log experi experienc issu ye expand type experi run approxim file size attempt upload share file hang run file locat work directori folder link privat workspac review thank heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is experiencing difficulty uploading files after their program has finished running, and is seeking a solution to the server issue.",
        "Question_gpt_summary":"user experienc difficulti upload file program finish run seek solut server issu",
        "Answer_gpt_summary_original":"there are no clear solutions provided in the answer. the responder is asking for more information about the issue, such as the type and size of files being uploaded, and requesting debug logs to review. the responder also notes that if they do not hear back from the user, the request will be closed.",
        "Answer_gpt_summary":"clear solut provid answer respond ask inform issu type size file upload request debug log review respond note hear user request close"
    },
    {
        "Question_id":null,
        "Question_title":"Parallel computing with Python SDK V2",
        "Question_body":"Hello :)\n\nDo you have any kind of idea when Azure Machine Learning Python SDK V2 could support parallel computing? We are testing things out with the machine learning studio and we are in a bit confusing stage that should we go with the SDK V1 or V2, but seemingly the V2 is not yet supporting multiple nodes in compute clusters.\n\nBest regards,\nTuomas",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1657198500433,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @TuomasPartanen-9618\n\nI have a good news for you, we are testing Parallel Run Step NOW in private preview of V2.\n\nFor your scenario, v1 is stable and serving all production customers. v2 (through DPv2) is still in private preview, and there are some dependency on new dataset\/mltable implementation. So if you want to seriously put some production traffic, I suggest guide to v1; but if you just want to have some prototypes, v2 may be better, as v2 is growing but v1 will not. Also, V2 will have the feature you want - Parallel.\n\nThe estimate time is not confirmed but should be around October.\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/918129\/parallel-computing-with-python-sdk-v2.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-14T04:53:23.743Z",
                "Answer_score":0,
                "Answer_body":"Hello @TuomasPartanen-9618\n\nI have a good news for you, we are testing Parallel Run Step NOW in private preview of V2.\n\nFor your scenario, v1 is stable and serving all production customers. v2 (through DPv2) is still in private preview, and there are some dependency on new dataset\/mltable implementation. So if you want to seriously put some production traffic, I suggest guide to v1; but if you just want to have some prototypes, v2 may be better, as v2 is growing but v1 will not. Also, V2 will have the feature you want - Parallel.\n\nThe estimate time is not confirmed but should be around October.\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1657774403743,
        "Question_original_content":"parallel comput python sdk hello kind idea python sdk support parallel comput test thing machin learn studio bit confus stage sdk seemingli support multipl node comput cluster best regard tuoma",
        "Question_preprocessed_content":"parallel comput python sdk hello kind idea python sdk support parallel comput test thing machin learn studio bit confus stage sdk seemingli support multipl node comput cluster best regard tuoma",
        "Answer_original_content":"hello tuomaspartanen good new test parallel run step privat preview scenario stabl serv product custom dpv privat preview depend new dataset mltabl implement want serious product traffic suggest guid want prototyp better grow featur want parallel estim time confirm octob hope help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_preprocessed_content":"hello good new test parallel run step privat preview scenario stabl serv product custom privat preview depend new implement want serious product traffic suggest guid want prototyp better grow featur want parallel estim time confirm octob hope help regard yutong kindli accept answer feel help support commun thank lot",
        "Question_gpt_summary_original":"the user tuomas is facing a challenge in deciding whether to use python sdk v1 or v2 for parallel computing, as v2 does not yet support multiple nodes in compute clusters.",
        "Question_gpt_summary":"user tuoma face challeng decid us python sdk parallel comput support multipl node comput cluster",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- if tuomas wants to seriously put some production traffic, it is suggested to guide him to use v1 as it is stable and serving all production customers.\n- if tuomas just wants to have some prototypes, v2 may be better as it is growing but v1 will not.\n- v2 will have the feature tuomas wants - parallel, but it is still in private preview and the estimated time for release is around october.",
        "Answer_gpt_summary":"possibl solut answer tuoma want serious product traffic suggest guid us stabl serv product custom tuoma want prototyp better grow featur tuoma want parallel privat preview estim time releas octob"
    },
    {
        "Question_id":null,
        "Question_title":"How to pass null\/Nan values into the dataframe passed into batch transform",
        "Question_body":"I am trying to make inference using my Xgboost model on a dataset which has NaN values, now inherently Xgboost handles the NaN values, it does not throw any error while training with NaN values whereas the batch transform job gives the error ''could not convert string to float'' when it encounters NaN values into the dataset that is to be transformed.\nCan anyone help me as to how can I pass NaN values into my input dataset for the batch transform job?\nThanks a ton!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1549531218000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUtKq2ZetyTUmvDwS7i3ot2Q\/how-to-pass-null-nan-values-into-the-dataframe-passed-into-batch-transform",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-03-12T17:50:37.000Z",
                "Answer_score":0,
                "Answer_body":"Hi bharat-patidar,\n\nThanks for your interest in Amazon SageMaker XGBoost. We recently made a change to address empty values in the dataset. Now you should be able to pass empty strings or \"NaN\" strings with CSV data. Can you please verify and let us know if the change addresses the issue at your end?\n\nThanks!\nRegards,\nAmazon SageMaker Team",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pass null nan valu datafram pass batch transform try infer xgboost model dataset nan valu inher xgboost handl nan valu throw error train nan valu batch transform job give error convert string float encount nan valu dataset transform help pass nan valu input dataset batch transform job thank ton",
        "Question_preprocessed_content":"pass valu datafram pass batch transform try infer xgboost model dataset nan valu inher xgboost handl nan valu throw error train nan valu batch transform job give error convert string float encount nan valu dataset transform help pass nan valu input dataset batch transform job thank ton",
        "Answer_original_content":"bharat patidar thank xgboost recent chang address valu dataset abl pass string nan string csv data verifi let know chang address issu end thank regard team",
        "Answer_preprocessed_content":"thank xgboost recent chang address valu dataset abl pass string nan string csv data verifi let know chang address issu end thank regard team",
        "Question_gpt_summary_original":"the user is encountering an issue with passing null\/nan values into the dataframe passed into batch transform for an xgboost model.",
        "Question_gpt_summary":"user encount issu pass null nan valu datafram pass batch transform xgboost model",
        "Answer_gpt_summary_original":"possible solutions: the xgboost team recently made a change to address empty values in the dataset. now, users should be able to pass empty strings or \"nan\" strings with csv data. the user is asked to verify if the change addresses the issue at their end.",
        "Answer_gpt_summary":"possibl solut xgboost team recent chang address valu dataset user abl pass string nan string csv data user ask verifi chang address issu end"
    },
    {
        "Question_id":null,
        "Question_title":"Hp-sweep with function with arguments",
        "Question_body":"<p>Hi<\/p>\n<p>Im wondering if its possible to have hp-sweep and instantiating wandb agent (using wandb.agent api) , such that the function will have arguments, in all of the example I\u2019ve seen that the passed functions to wandb.agent has no args at all.<\/p>\n<p>My training script looks as follows<\/p>\n<pre><code class=\"lang-auto\">\ndef trainer(input1, input2, input3):\n  # construct cfg file given the inputs\n  # passing the cfg file to wandb.init() \n  # receiving the returned config from wandb.config (with the decided params by the wandb Params controler)\n  # Continue training as usual, given the latest cfg.\n<\/code><\/pre>\n<p>I will be happy to get some inspiration about other ways to implement this<br>\nHope this is helpful<\/p>\n<p>Dor<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1670934400394,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":182.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hp-sweep-with-function-with-arguments\/3539",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-14T12:46:58.632Z",
                "Answer_body":"<p>Hi Dor,<\/p>\n<p>Thanks for writing in! You can pass arguments to the function in the agent in the normal way <code>function=main(args)<\/code>, is this raising you any error?<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-19T15:21:42.859Z",
                "Answer_body":"<p>Hi Dor,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-11T12:26:56.570Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"sweep function argument wonder possibl sweep instanti agent agent api function argument exampl iv seen pass function agent arg train script look follow def trainer input input input construct cfg file given input pass cfg file init receiv return config config decid param param control continu train usual given latest cfg happi inspir wai implement hope help dor",
        "Question_preprocessed_content":"function argument wonder possibl instanti agent function argument exampl iv seen pass function agent arg train script look follow happi inspir wai implement hope help dor",
        "Answer_original_content":"dor thank write pass argument function agent normal wai function main arg rais error best lui dor want follow support request heard let know assist issu resolv best lui topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"dor thank write pass argument function agent normal wai rais error best lui dor want follow support request heard let know assist issu resolv best lui topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking for a way to use hp-sweep with a function that has arguments in order to construct a configuration file, receive the returned configuration from the parameter controller, and continue training as usual.",
        "Question_gpt_summary":"user look wai us sweep function argument order construct configur file receiv return configur paramet control continu train usual",
        "Answer_gpt_summary_original":"there are no possible solutions provided in the answer. the answer is a follow-up message from the support team asking if the user needs further assistance or if their issue has been resolved.",
        "Answer_gpt_summary":"possibl solut provid answer answer follow messag support team ask user need assist issu resolv"
    },
    {
        "Question_id":null,
        "Question_title":"Reading dataset after uploading to storage",
        "Question_body":"hi,\n\nI created various datasets but within my python notebook, how do i read it?\n\nSo currently if this is what I have:\n\nx_train_df = pd.read_csv('data_reviews\/x_train.csv')\n\nwhat should I replace it with?",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1650301669787,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/816330\/reading-dataset-after-uploading-to-storage.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-19T13:17:17.23Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. I assume you registered a datastore and uploaded datasets to storage. To access the data from storage depends on whether it's structured or unstructured. For unstructured data you can access using FileDataset and for structured data you can access using TabularDataset. The following code snippet shows how to create dataset from datastore. For FileDataset, review Mount vs Download and sample notebook.\n\n from azureml.core import Workspace, Datastore, Dataset\n    \n datastore_name = 'your datastore name'\n    \n # get existing workspace\n workspace = Workspace.from_config()\n        \n # retrieve an existing datastore in the workspace by name\n datastore = Datastore.get(workspace, datastore_name)\n    \n # create a TabularDataset from 3 file paths in datastore\n datastore_paths = [(datastore, 'weather\/2018\/11.csv'),\n                    (datastore, 'weather\/2018\/12.csv'),\n                    (datastore, 'weather\/2019\/*.csv')]\n    \n weather_ds = Dataset.Tabular.from_delimited_files(path=datastore_paths)\n\n\n\n\n\n--please don't forget to Accept Answer if the reply is helpful. Thanks.--",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"read dataset upload storag creat dataset python notebook read current train read csv data review train csv replac",
        "Question_preprocessed_content":"read dataset upload storag creat dataset python notebook read current replac",
        "Answer_original_content":"thank reach assum regist datastor upload dataset storag access data storag depend structur unstructur unstructur data access filedataset structur data access tabulardataset follow code snippet show creat dataset datastor filedataset review mount download sampl notebook core import workspac datastor dataset datastor datastor exist workspac workspac workspac config retriev exist datastor workspac datastor datastor workspac datastor creat tabulardataset file path datastor datastor path datastor weather csv datastor weather csv datastor weather csv weather dataset tabular delimit file path datastor path forget accept answer repli help thank",
        "Answer_preprocessed_content":"thank reach assum regist datastor upload dataset storag access data storag depend structur unstructur unstructur data access filedataset structur data access tabulardataset follow code snippet show creat dataset datastor filedataset review mount download sampl notebook core import workspac datastor dataset datastor exist workspac workspac retriev exist datastor workspac datastor creat tabulardataset file path datastor forget accept answer repli help",
        "Question_gpt_summary_original":"the user is struggling to read a dataset they have uploaded to storage within their python notebook.",
        "Question_gpt_summary":"user struggl read dataset upload storag python notebook",
        "Answer_gpt_summary_original":"possible solutions to accessing data from storage within a python notebook depend on whether the data is structured or unstructured. for unstructured data, the user can access it using filedataset, while for structured data, they can access it using tabulardataset. the code snippet provided shows how to create a dataset from a datastore. the user is advised to review mount vs download and sample notebook for filedataset.",
        "Answer_gpt_summary":"possibl solut access data storag python notebook depend data structur unstructur unstructur data user access filedataset structur data access tabulardataset code snippet provid show creat dataset datastor user advis review mount download sampl notebook filedataset"
    },
    {
        "Question_id":null,
        "Question_title":"Why can't I see web app bot in the market place?",
        "Question_body":"I've tried to look everywhere in the marketplace, however, I can't seem to find a web app bot.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1640039500510,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/670277\/why-can39t-i-see-web-app-bot-in-the-market-place.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-20T23:24:34.567Z",
                "Answer_score":1,
                "Answer_body":"Sorry, Web App Bot and Bot Channels Registration are deprecated but existing resources will continue to work. You should use Azure Bot, instead.\n\nQuick start for Azure Bot: https:\/\/docs.microsoft.com\/en-us\/azure\/bot-service\/abs-quickstart?view=azure-bot-service-4.0&tabs=userassigned\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":16.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"web app bot market place tri look marketplac web app bot",
        "Question_preprocessed_content":"web app bot market place tri look marketplac web app bot",
        "Answer_original_content":"sorri web app bot bot channel registr deprec exist resourc continu work us azur bot instead quick start azur bot http doc microsoft com azur bot servic ab quickstart view azur bot servic tab userassign hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"sorri web app bot bot channel registr deprec exist resourc continu work us azur bot instead quick start azur bot hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Question_gpt_summary_original":"the user is unable to locate a web app bot in the marketplace.",
        "Question_gpt_summary":"user unabl locat web app bot marketplac",
        "Answer_gpt_summary_original":"possible solutions: \n\n- the web app bot and bot channels registration are deprecated, so the user should use azure bot instead. \n- the user can refer to the quick start guide for azure bot to get started. \n\nsummary: the web app bot and bot channels registration are no longer available, and the user should use azure bot instead. the user can refer to the quick start guide for azure bot to get started.",
        "Answer_gpt_summary":"possibl solut web app bot bot channel registr deprec user us azur bot instead user refer quick start guid azur bot start summari web app bot bot channel registr longer avail user us azur bot instead user refer quick start guid azur bot start"
    },
    {
        "Question_id":null,
        "Question_title":"Custom Dockerfile on Azure Environment with python poetry",
        "Question_body":"I am new to docker and environments. This could be basics but i have been trying to install packages in my pyproject.toml file in Dockerfile without success.\n\nI have tried using poetry to export requirements.txt file and using it with the\nEnvironment.from_pip_requirements('requirements.txt') function and a Dockerfile.\n\nBut could there be any elegant solution to use toml file directly for creating a custom environment ?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1638821054557,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Thanks for the response, @Ram-msft\nUsing the Dockerfile :\n\n FROM python:3.8-slim-buster\n ENV PYTHONUNBUFFERED=1 \\\n     PYTHONDONTWRITEBYTECODE=1 \\\n     PIP_NO_CACHE_DIR=1 \\\n     PIP_DISABLE_PIP_VERSION_CHECK=1 \\\n     POETRY_VERSION=1.1.7 \\\n     PYLINT_VERSION=2.9.4\n    \n RUN pip install pylint==$PYLINT_VERSION \\\n     && pip install \"poetry==$POETRY_VERSION\" \n    \n COPY pyproject.toml .\/\n RUN poetry config virtualenvs.create false",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/653688\/custom-dockerfile-on-azure-environment-with-python.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-08T16:31:16.963Z",
                "Answer_score":1,
                "Answer_body":"Thanks for the response, @Ram-msft\nUsing the Dockerfile :\n\n FROM python:3.8-slim-buster\n ENV PYTHONUNBUFFERED=1 \\\n     PYTHONDONTWRITEBYTECODE=1 \\\n     PIP_NO_CACHE_DIR=1 \\\n     PIP_DISABLE_PIP_VERSION_CHECK=1 \\\n     POETRY_VERSION=1.1.7 \\\n     PYLINT_VERSION=2.9.4\n    \n RUN pip install pylint==$PYLINT_VERSION \\\n     && pip install \"poetry==$POETRY_VERSION\" \n    \n COPY pyproject.toml .\/\n RUN poetry config virtualenvs.create false",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-12-07T15:37:01.407Z",
                "Answer_score":0,
                "Answer_body":"@AntaraDas-4298 Thanks for the question. Can you please share the sample\/document that you are trying. I would recommend to use yml file that is relatively easy to get from pip requirements file\n\nfrom azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nenv = Environment(\u201cmyenv\u201d)\nenv.python.conda_dependencies = CondaDependencies(\u201cmy_yaml_file\u201d)\n\n\n\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-custom-image",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1638981076963,
        "Question_original_content":"custom dockerfil azur environ python poetri new docker environ basic try instal packag pyproject toml file dockerfil success tri poetri export requir txt file environ pip requir requir txt function dockerfil eleg solut us toml file directli creat custom environ",
        "Question_preprocessed_content":"custom dockerfil azur environ python poetri new docker environ basic try instal packag file dockerfil success tri poetri export file function dockerfil eleg solut us toml file directli creat custom environ",
        "Answer_original_content":"thank respons ram msft dockerfil python slim buster env pythonunbuff pythondontwritebytecod pip cach dir pip disabl pip version check poetri version pylint version run pip instal pylint pylint version pip instal poetri poetri version copi pyproject toml run poetri config virtualenv creat fals",
        "Answer_preprocessed_content":"thank respons dockerfil env pythonunbuff pythondontwritebytecod run pip instal pip instal copi run poetri config fals",
        "Question_gpt_summary_original":"the user is trying to create a custom dockerfile on an azure environment with python poetry, but is having difficulty installing packages in their pyproject.toml file.",
        "Question_gpt_summary":"user try creat custom dockerfil azur environ python poetri have difficulti instal packag pyproject toml file",
        "Answer_gpt_summary_original":"the answer suggests using a specific dockerfile with environment variables and installing the required packages using pip. it also includes copying the pyproject.toml file and disabling virtual environment creation.",
        "Answer_gpt_summary":"answer suggest specif dockerfil environ variabl instal requir packag pip includ copi pyproject toml file disabl virtual environ creation"
    },
    {
        "Question_id":67639665.0,
        "Question_title":"Azure ML not able to create conda environment (exit code: -15)",
        "Question_body":"<p>When I try to run the experiment defined in <a href=\"https:\/\/github.com\/MicrosoftLearning\/mslearn-dp100\/blob\/main\/06%20-%20Work%20with%20Data.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> in  notebook, I encountered an error when it is creating the conda env. The error occurs when the below cell is executed:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Experiment, ScriptRunConfig, Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.widgets import RunDetails\n\n\n# Create a Python environment for the experiment\nsklearn_env = Environment(&quot;sklearn-env&quot;)\n\n# Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\nsklearn_env.python.conda_dependencies = packages\n\n# Get the training dataset\ndiabetes_ds = ws.datasets.get(&quot;diabetes dataset&quot;)\n\n# Create a script config\nscript_config = ScriptRunConfig(source_directory=experiment_folder,\n                              script='diabetes_training.py',\n                              arguments = ['--regularization', 0.1, # Regularizaton rate parameter\n                                           '--input-data', diabetes_ds.as_named_input('training_data')], # Reference to dataset\n                              environment=sklearn_env)\n\n# submit the experiment\nexperiment_name = 'mslearn-train-diabetes'\nexperiment = Experiment(workspace=ws, name=experiment_name)\nrun = experiment.submit(config=script_config)\nRunDetails(run).show()\nrun.wait_for_completion() \n<\/code><\/pre>\n<p>Everytime I run this, I always faced the issue of creating the conda env as below:<\/p>\n<pre><code>Creating conda environment...\nRunning: ['conda', 'env', 'create', '-p', '\/home\/azureuser\/.azureml\/envs\/azureml_000000000000', '-f', 'azureml-environment-setup\/mutated_conda_dependencies.yml']\nCollecting package metadata (repodata.json): ...working... done\nSolving environment: ...working... done\nPreparing transaction: ...working... done\nVerifying transaction: ...working... done\nExecuting transaction: ...working... done\n\nInstalling pip dependencies: ...working... \n\nAttempting to clean up partially built conda environment: \/home\/azureuser\/.azureml\/envs\/azureml_000000000000\nRemove all packages in environment \/home\/azureuser\/.azureml\/envs\/azureml_000000000000:\nCreating conda environment failed with exit code: -15\n<\/code><\/pre>\n<p>I could not find anything useful on the internet and this is not the only script where it fail. When I am try to run other experiments I have sometimes faced this issue. One solution which worked in the above case is I moved the pandas from pip to conda and it was able to create the coonda env. Example below:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep[pandas]'])\n\n<\/code><\/pre>\n<pre class=\"lang-py prettyprint-override\"><code># Ensure the required packages are installed (we need scikit-learn, Azure ML defaults, and Azure ML dataprep)\npackages = CondaDependencies.create(conda_packages=['scikit-learn','pip','pandas'],\n                                    pip_packages=['azureml-defaults','azureml-dataprep'])\n\n<\/code><\/pre>\n\n<p>The error message (or the logs from Azure) is also not much help. Would apprecite if a proper solution is available.<\/p>\n<p>Edit: I have recently started learning to use Azure for Machine learning and so if I am not sure if I am missing something? I assume the example notebooks should work as is hence raised this question.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1621610368967,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1621615281776,
        "Question_score":4.0,
        "Question_view_count":2373.0,
        "Answer_body":"<h2>short answer<\/h2>\n<p>Totally been in your shoes before. This code sample seems a smidge out of date. Using <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/train-with-datasets\/train-with-datasets.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> as a reference, can you try the following?<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>packages = CondaDependencies.create(\n    pip_packages=['azureml-defaults','scikit-learn']\n)\n<\/code><\/pre>\n<h2>longer  answer<\/h2>\n<p><a href=\"https:\/\/www.anaconda.com\/blog\/using-pip-in-a-conda-environment\" rel=\"nofollow noreferrer\">Using pip with Conda<\/a> is not always smooth sailing. In this instance, conda isn't reporting up the issue that pip is having. The solution is to create and test this environment locally where we can get more information, which will at least will give you a more informative error message.<\/p>\n<ol>\n<li>Install anaconda  or miniconda (or use an Azure ML Compute Instance which has conda pre-installed)<\/li>\n<li>Make a  file called environment.yml that looks like this<\/li>\n<\/ol>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: aml_env\ndependencies:\n - python=3.8\n - pip=21.0.1\n - pip:\n    - azureml-defaults\n    - azureml-dataprep[pandas]\n    - scikit-learn==0.24.1\n<\/code><\/pre>\n<ol start=\"3\">\n<li>Create this environment with the command <code>conda env create -f environment.yml<\/code>.<\/li>\n<li>respond to any discovered error message<\/li>\n<li>If there' no error, use this new <code>environment.yml<\/code> with Azure ML like so<\/li>\n<\/ol>\n<pre class=\"lang-py prettyprint-override\"><code>sklearn_env = Environment.from_conda_specification(name = 'sklearn-env', file_path = '.\/environment.yml')\n<\/code><\/pre>\n<h2>more context<\/h2>\n<p>the error I'm guessing that's happening is when you reference a pip requirements file from a conda environment file. In this scenario, conda calls <code>pip install -r  requirements.txt<\/code> and if that command errors out, conda can't report the error.<\/p>\n<h3><code>requirements.txt<\/code><\/h3>\n<pre><code>scikit-learn==0.24.1\nazureml-dataprep[pandas]\n<\/code><\/pre>\n<h3><code>environment.yml<\/code><\/h3>\n<pre><code>name: aml_env\ndependencies:\n - python=3.8\n - pip=21.0.1\n - pip:\n    - -rrequirements.txt\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1621619024732,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67639665",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1621615054950,
        "Question_original_content":"abl creat conda environ exit code try run experi defin notebook notebook encount error creat conda env error occur cell execut core import experi scriptrunconfig environ core conda depend import condadepend widget import rundetail creat python environ experi sklearn env environ sklearn env ensur requir packag instal need scikit learn default dataprep packag condadepend creat conda packag scikit learn pip pip packag default dataprep panda sklearn env python conda depend packag train dataset diabet dataset diabet dataset creat script config script config scriptrunconfig sourc directori experi folder script diabet train argument regular regularizaton rate paramet input data diabet name input train data refer dataset environ sklearn env submit experi experi mslearn train diabet experi experi workspac experi run experi submit config script config rundetail run run wait complet everytim run face issu creat conda env creat conda environ run conda env creat home azureus env environ setup mutat conda depend yml collect packag metadata repodata json work solv environ work prepar transact work verifi transact work execut transact work instal pip depend work attempt clean partial built conda environ home azureus env remov packag environ home azureus env creat conda environ fail exit code us internet script fail try run experi face issu solut work case move panda pip conda abl creat coonda env exampl ensur requir packag instal need scikit learn default dataprep packag condadepend creat conda packag scikit learn pip pip packag default dataprep panda ensur requir packag instal need scikit learn default dataprep packag condadepend creat conda packag scikit learn pip panda pip packag default dataprep error messag log azur help apprecit proper solut avail edit recent start learn us azur machin learn sure miss assum exampl notebook work rais question",
        "Question_preprocessed_content":"abl creat conda environ try run experi defin notebook notebook encount error creat conda env error occur cell execut everytim run face issu creat conda env us internet script fail try run experi face issu solut work case move panda pip conda abl creat coonda env exampl error messag help apprecit proper solut avail edit recent start learn us azur machin learn sure miss assum exampl notebook work rais question",
        "Answer_original_content":"short answer total shoe code sampl smidg date notebook refer try follow packag condadepend creat pip packag default scikit learn longer answer pip conda smooth sail instanc conda isn report issu pip have solut creat test environ local inform inform error messag instal anaconda miniconda us comput instanc conda pre instal file call environ yml look like aml env depend python pip pip default dataprep panda scikit learn creat environ command conda env creat environ yml respond discov error messag error us new environ yml like sklearn env environ conda specif sklearn env file path environ yml context error guess happen refer pip requir file conda environ file scenario conda call pip instal requir txt command error conda report error requir txt scikit learn dataprep panda environ yml aml env depend python pip pip rrequir txt",
        "Answer_preprocessed_content":"short answer total shoe code sampl smidg date notebook refer try follow longer answer pip conda smooth sail instanc conda isn report issu pip have solut creat test environ local inform inform error messag instal anaconda miniconda file call look like creat environ command respond discov error messag error us new like context error guess happen refer pip requir file conda environ file scenario conda call command error conda report error",
        "Question_gpt_summary_original":"the user encountered an error when creating a conda environment for an experiment defined in a notebook, and was unable to find a solution despite attempting to move the pandas package from pip to conda.",
        "Question_gpt_summary":"user encount error creat conda environ experi defin notebook unabl solut despit attempt panda packag pip conda",
        "Answer_gpt_summary_original":"the answer suggests creating and testing the environment locally to get more information about the error message. the solution involves installing anaconda or miniconda, creating an environment.yml file, and using the command \"conda env create -f environment.yml\" to create the environment. the user should respond to any discovered error message, and if there is no error, use the new environment.yml file. the error may be happening when referencing a pip requirements file from a conda environment file.",
        "Answer_gpt_summary":"answer suggest creat test environ local inform error messag solut involv instal anaconda miniconda creat environ yml file command conda env creat environ yml creat environ user respond discov error messag error us new environ yml file error happen referenc pip requir file conda environ file"
    },
    {
        "Question_id":71308112.0,
        "Question_title":"How to return a float with 'text\/csv' as \"Content-Type\" from SageMaker endpoint that uses custom inference code?",
        "Question_body":"<p>I am trying to return output(or predictions) from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as &quot;Content-Type&quot;. I have tried multiple ways, but the sagemaker always returns with 'text\/html; charset=utf-8' as the &quot;Content-Type&quot;, and I would like SageMaker to return 'text\/csv' or 'text\/csv; charset=utf-8'.<\/p>\n<p>Here's the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#process-output\" rel=\"nofollow noreferrer\"><code>output_fn<\/code><\/a> from my inference-code:<\/p>\n<pre><code>** my other code **\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return output_float\n<\/code><\/pre>\n<p>above function returns number with float data-type and I got error(in cloudwatch logs) that  this function should only be returning string, tuple, dict or Respoonse instance.<\/p>\n<p>So, here are all the different ways I have tried to have SageMaker return my number with 'text\/csv' but only receives 'text\/html; charset=utf-8'<\/p>\n<ol>\n<li><code>return json.dumps(output_float)<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li>by using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\"><code>sagemaker.serializers.CSVSerializer<\/code><\/a> like this:\n<pre><code>from sagemaker.serializers import CSVSerializer\ncsv_serialiser = CSVSerializer(content_type='text\/csv')\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return csv_serialiser.serialize(output_float)\n<\/code><\/pre>\nI got <code>'NoneType' object has no attribute 'startswith'<\/code> error with this.<\/li>\n<li>as a tuple: <code>return (output_float,)<\/code>. I haven't noted down what this did, but it sure didn't return the number with 'text\/csv' as &quot;Content-Type&quot;.<\/li>\n<li>made changes in my model object to return a float on calling <code>.predict_proba<\/code> on my model-object and deployed it from sagemaker studio without using any custom inference code and deployed this from SageMaker studio. but got this error after sending a request to the endpoint: <code>'NoneType' object has no attribute 'startswith'<\/code>, but at my side, when I pass proper inputs to the unpicked model and call .predict_proba, i get float as expected.<\/li>\n<li>by returning <a href=\"https:\/\/tedboy.github.io\/flask\/generated\/generated\/flask.Response.html#flask.Response\" rel=\"nofollow noreferrer\"><code>flask.Response<\/code><\/a> like this:\n<pre><code>from flask import Response\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return Response(response=output_float, status=200, headers={'Content-Type':'text\/csv; charset=utf-8'})\n<\/code><\/pre>\nbut, I got some IndexError with this(I didn't notedown the traceback.)<\/li>\n<\/ol>\n<p>Some other info:<\/p>\n<ol>\n<li>Model I am using is completely outside of SageMaker, not from a training job or anything like that.<\/li>\n<li>All of the aforementioned endpoints have been deployed entirely from aws-cli with relevant .json files, except the one in point-8 above.<\/li>\n<\/ol>\n<p>How do I return number with &quot;text\/csv&quot; as content-type from sagemaker? I need my output in &quot;text\/csv&quot; content-type specifically for Model-Quality-Monitor. How do I do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1646135687137,
        "Question_favorite_count":null,
        "Question_last_edit_time":1646652069630,
        "Question_score":0.0,
        "Question_view_count":461.0,
        "Answer_body":"<p>From the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L88\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example, I suggest testing by setting the Response as follows:<\/p>\n<pre><code>return flask.Response(response= output_float, status=200, mimetype=&quot;text\/csv&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71308112",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1646685903080,
        "Question_original_content":"return float text csv content type endpoint us custom infer code try return output predict endpoint text csv text csv charset utf content type tri multipl wai return text html charset utf content type like return text csv text csv charset utf output infer code code def output predict content type text csv code return output float function return number float data type got error cloudwatch log function return string tupl dict respoons instanc differ wai tri return number text csv receiv text html charset utf return json dump output float sent text html charset utf return output float sent text html charset utf return output float sent text html charset utf return output float sent text html charset utf return output float sent text html charset utf serial csvserial like serial import csvserial csv serialis csvserial content type text csv def output predict content type text csv code return csv serialis serial output float got nonetyp object attribut startswith error tupl return output float haven note sure return number text csv content type chang model object return float call predict proba model object deploi studio custom infer code deploi studio got error send request endpoint nonetyp object attribut startswith pass proper input unpick model predict proba float expect return flask respons like flask import respons def output predict content type text csv code return respons respons output float statu header content type text csv charset utf got indexerror notedown traceback info model complet outsid train job like aforement endpoint deploi entir aw cli relev json file point return number text csv content type need output text csv content type specif model qualiti monitor",
        "Question_preprocessed_content":"return float endpoint us custom infer code try return output endpoint tri multipl wai return like return function return number float got error function return string tupl dict respoons instanc differ wai tri return number receiv sent sent sent sent sent like got error tupl haven note sure return number chang model object return float call deploi studio custom infer code deploi studio got error send request endpoint pass proper input unpick model float expect return like got indexerror info model complet outsid train job like aforement endpoint deploi entir relev json file return number need output specif",
        "Answer_original_content":"scikit bring exampl suggest test set respons follow return flask respons respons output float statu mimetyp text csv",
        "Answer_preprocessed_content":"exampl suggest test set respons follow",
        "Question_gpt_summary_original":"the user is encountering challenges in returning a float with 'text\/csv' as \"content-type\" from an endpoint that uses custom inference code.",
        "Question_gpt_summary":"user encount challeng return float text csv content type endpoint us custom infer code",
        "Answer_gpt_summary_original":"the solution suggested for returning a float with 'text\/csv' as \"content-type\" from an endpoint that uses custom inference code is to set the response as follows: return flask.response(response= output_float, status=200, mimetype=\"text\/csv\").",
        "Answer_gpt_summary":"solut suggest return float text csv content type endpoint us custom infer code set respons follow return flask respons respons output float statu mimetyp text csv"
    },
    {
        "Question_id":null,
        "Question_title":"ERROR: failed to reproduce \u2018dvc.yaml\u2019: [Errno 13] Permission denied",
        "Question_body":"<p>Dear community,<\/p>\n<p>please help me what I can do with the following\u2026<br>\nI am running <code>dvc repro<\/code> and get the following error:<\/p>\n<p><strong>ERROR: failed to reproduce \u2018dvc.yaml\u2019: [Errno 13] Permission denied: '\/mnt\/c\/Users\/michael\/projects\/deeplearing\/.dvc\/cache\/57\/01bf0661ad5b16b5165d9c6042a0cd597\u2019<\/strong><\/p>\n<p>notes:<\/p>\n<ul>\n<li>I was running the <code>dvc repro<\/code> in a console window of VSCode within a WSL environment<\/li>\n<li>I am using a conda environment with dvc installed<\/li>\n<li>I already tried deleting the .cache directory<\/li>\n<li>it looks like this is not a DVC problem, but an OS problem\u2026 still hoping for suggestions because the first stage is running successfully (almost identical to the second stage except that the amount of processed files is smaller)<\/li>\n<\/ul>\n<p>Maybe somebody has a clue what I can try?<\/p>\n<p>My <code>dvc.yaml<\/code> looks like this:<\/p>\n<pre><code class=\"lang-auto\">stages:\n  create_first_dataset:\n    cmd: python create_first_dataset.py\n    deps:\n    - original_dataset\n    - create_first_dataset.py\n    outs:\n    - first_dataset\n  create_second_dataset:\n    cmd: python create_second_dataset.py\n    deps:\n    - original_dataset\n    - create_second_dataset.py\n    outs:\n    - second_dataset\n<\/code><\/pre>\n<p>dvc doctor output:<\/p>\n<pre><code class=\"lang-auto\">DVC version: 2.5.4 (pip)\n---------------------------------\nPlatform: Python 3.8.10 on Linux-5.4.72-microsoft-standard-WSL2-x86_64-with-glibc2.17\nSupports:\n        http (requests = 2.26.0),\n        https (requests = 2.26.0),\n        ssh (paramiko = 2.7.2)\nCache types: hardlink, symlink\nCache directory: 9p on C:\\\nCaches: local\nRemotes: ssh\nWorkspace directory: 9p on C:\\\nRepo: dvc, git\n<\/code><\/pre>\n<p>Thanks all!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1627673145405,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1239.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/error-failed-to-reproduce-dvc-yaml-errno-13-permission-denied\/835",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-31T01:28:34.013Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/zigex\">@zigex<\/a><\/p>\n<p>Can you also share the contents of dvc.lock (at least the entry with <code>md5: 5701bf0661ad5b16b5165d9c6042a0cd597<\/code>) and the full output of <code>dvc repro -v<\/code>?<\/p>\n<p>One thing you can try for now is to check your user owns all the files and dirs in .dvc\/cache and that files have <code>775<\/code> permissions, and folders <code>664<\/code> (or whatever is appropriate for your setup).<\/p>\n<p>Thanks<\/p>",
                "Answer_score":42.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-01T16:22:28.446Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/jorgeorpinel\">@jorgeorpinel<\/a><\/p>\n<p>thank you very much for trying to help me and replying.<\/p>\n<p>Upon your suggestion, I tried to run <code>dvc repro -v<\/code> and it suddenly worked. I tried on Friday a couple of hours without any success and did not change anything in my code.<\/p>\n<p>If the error message appears again, I will provide the outputs you suggested.<\/p>\n<p>Kind regards!<\/p>",
                "Answer_score":52.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-02T20:11:56.438Z",
                "Answer_body":"<p>Sounds good. Maybe another process is meddling with your file permissions. Anyway, glad it\u2019s working now. Best<\/p>",
                "Answer_score":21.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error fail reproduc yaml errno permiss deni dear commun help follow run repro follow error error fail reproduc yaml errno permiss deni mnt user michael project deeplear cach bfadbbdcacd note run repro consol window vscode wsl environ conda environ instal tri delet cach directori look like problem problem hope suggest stage run successfulli ident second stage process file smaller mayb somebodi clue try yaml look like stage creat dataset cmd python creat dataset dep origin dataset creat dataset out dataset creat second dataset cmd python creat second dataset dep origin dataset creat second dataset out second dataset doctor output version pip platform python linux microsoft standard wsl glibc support http request http request ssh paramiko cach type hardlink symlink cach directori cach local remot ssh workspac directori repo git thank",
        "Question_preprocessed_content":"error fail reproduc yaml permiss deni dear commun help follow run follow error error fail reproduc yaml permiss deni note run consol window vscode wsl environ conda environ instal tri delet cach directori look like problem problem hope suggest stage run successfulli mayb somebodi clue try look like doctor output thank",
        "Answer_original_content":"zigex share content lock entri bfadbbdcacd output repro thing try check user own file dir cach file permiss folder appropri setup thank hello jorgeorpinel thank try help repli suggest tri run repro suddenli work tri fridai coupl hour success chang code error messag appear provid output suggest kind regard sound good mayb process meddl file permiss glad work best",
        "Answer_preprocessed_content":"share content lock output thing try check user own file dir file permiss folder thank hello thank try help repli suggest tri run suddenli work tri fridai coupl hour success chang code error messag appear provid output suggest kind regard sound good mayb process meddl file permiss glad work best",
        "Question_gpt_summary_original":"the user is encountering a permission denied error while running repro in a console window of vscode within a wsl environment.",
        "Question_gpt_summary":"user encount permiss deni error run repro consol window vscode wsl environ",
        "Answer_gpt_summary_original":"possible solutions to the permission denied error while running repro in a console window of vscode within a wsl environment are to check if the user owns all the files and directories in .\/cache and that files have 775 permissions, and folders 664 (or whatever is appropriate for the setup). additionally, the user can try running repro -v to see if it resolves the issue. it is also suggested to provide the contents of .lock and the full output of repro -v if the error message appears again.",
        "Answer_gpt_summary":"possibl solut permiss deni error run repro consol window vscode wsl environ check user own file directori cach file permiss folder appropri setup addition user try run repro resolv issu suggest provid content lock output repro error messag appear"
    },
    {
        "Question_id":null,
        "Question_title":"Using wandb sweep with torch.distributed.launch",
        "Question_body":"<p>Hello<\/p>\n<p>I am using wandb sweep to perform hyperparameter tuning.<\/p>\n<p>Basically when I launch wandb agent with \u201cwandb agent &lt;USERNAME\/PROJECTNAME\/SWEEPID&gt;\u201d,<\/p>\n<p>It will automatically run  \u201c\/usr\/bin\/env python train.py --param1=value1 --param2=value2\u201d according to the configurations.<\/p>\n<p>However my code is based on torch distributed data parallel and it has to be launched with torch.distributed.launch   train.py  rather than just  python train.py.<\/p>\n<p>How can I tackle this problem?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_creation_time":1653309050747,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":148.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/using-wandb-sweep-with-torch-distributed-launch\/2483",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-24T21:56:22.573Z",
                "Answer_body":"<p>Hi <span class=\"mention\">@rash<\/span>!<\/p>\n<p>Thanks for writing in. You can change the command that the agent runs by specifying the command structure in your sweep config. Specifically, you can change the <code>interpreter<\/code> variable to switch to <code>torch.distributed.launch<\/code>. <a href=\"https:\/\/docs.wandb.ai\/guides\/sweeps\/configuration#command\">Here<\/a> is a link to our docs regarding how this can be done.<\/p>\n<p>Please let me know if I can be of further assistance.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-25T02:26:21.509Z",
                "Answer_body":"<p>Thanks Ramit<\/p>\n<p>I have  followed what you suggested but I am still unable to run with torch.distributed.launch.<\/p>\n<p>Below is my configuration yaml file.<\/p>\n<p>\u2018\u2019\u2019\u2019\u2019<br>\nmethod: random<\/p>\n<p>program: rae_wandb.py<\/p>\n<p>metric:<\/p>\n<p>name: total_mean_rank_sum<\/p>\n<p>goal: minimize<\/p>\n<p>command:<\/p>\n<ul>\n<li>\n<p>${env}<\/p>\n<\/li>\n<li>\n<p>torch.distributed.launch<\/p>\n<\/li>\n<li>\n<p>${program}<\/p>\n<\/li>\n<li>\n<p>${args}<\/p>\n<\/li>\n<\/ul>\n<p><span class=\"hashtag\">#command:<\/span><\/p>\n<p><span class=\"hashtag\">#-<\/span> python raw_wandb.py<\/p>\n<p><span class=\"hashtag\">#-<\/span> python -m torch.distributed.launch --nproc_per_node=4 rae_wandb.py  -m torch.distributed.launch --nproc_per_node=4<\/p>\n<p>parameters:<\/p>\n<p>lr:<\/p>\n<pre><code>min: 0.0\n\nmax: 0.01\n<\/code><\/pre>\n<p>coef_lr:<\/p>\n<pre><code>min: 0.0\n\nmax: 0.01\n<\/code><\/pre>\n<p>sim_header:<\/p>\n<pre><code>values: [\"meanP\", \"seqLSTM\", \"seqTransf\"]\n<\/code><\/pre>\n<p>\u2018\u2019\u2019\u2019\u2019\u2019\u2019\u2019<\/p>\n<p>when I launch an agent , it runs       \/usr\/bin\/env torch.distributed.launch rae_wandb.py --coef_lr=0.0068455254534794605 --lr=0.008759887226936639 --sim_header=seqTransf<\/p>\n<p>what I really need is \/usr\/bin\/env python -m torch.distributed.launch rae_wandb.py --coef_lr=0.0068455254534794605 --lr=0.008759887226936639 --sim_header=seqTransf<\/p>\n<p>I would like to find some descriptive examples.<\/p>\n<p>Many thanks!<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-25T16:13:23.113Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/raeh\">@raeh<\/a>,<\/p>\n<p>The following should work in this case then:<\/p>\n<pre data-code-wrap=\"yaml\"><code class=\"lang-nohighlight\">command:\n    - ${env}\n    - ${interpreter}\n    - \"-m\"\n    - \"torch.distributed.launch\"\n    - ${program}\n    - ${args}\n<\/code><\/pre>\n<p>Please let me know if this solves the issue for you.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-02T17:50:52.062Z",
                "Answer_body":"<p>Hi Ray,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-08T20:04:43.437Z",
                "Answer_body":"<p>Hi Ray, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-24T16:13:42.652Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"sweep torch distribut launch hello sweep perform hyperparamet tune basic launch agent agent automat run usr bin env python train param valu param valu accord configur code base torch distribut data parallel launch torch distribut launch train python train tackl problem thank advanc",
        "Question_preprocessed_content":"sweep hello sweep perform hyperparamet tune basic launch agent agent automat run python accord configur code base torch distribut data parallel launch python tackl problem thank advanc",
        "Answer_original_content":"rash thank write chang command agent run specifi command structur sweep config specif chang interpret variabl switch torch distribut launch link doc let know assist thank ramit thank ramit follow suggest unabl run torch distribut launch configur yaml file method random program rae metric total mean rank sum goal minim command env torch distribut launch program arg command python raw python torch distribut launch nproc node rae torch distribut launch nproc node paramet min max coef min max sim header valu meanp seqlstm seqtransf launch agent run usr bin env torch distribut launch rae coef sim header seqtransf need usr bin env python torch distribut launch rae coef sim header seqtransf like descript exampl thank hei raeh follow work case command env interpret torch distribut launch program arg let know solv issu thank ramit rai want follow support request heard let know assist issu resolv best rai heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thank write chang command agent run specifi command structur sweep config specif chang variabl switch link doc let know assist thank ramit thank ramit follow suggest unabl run configur yaml file method random program metric goal minim command command python python paramet launch agent run need python like descript exampl thank hei follow work case let know solv issu thank ramit rai want follow support request heard let know assist issu resolv best rai heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering a challenge of using sweep with torch.distributed.launch to perform hyperparameter tuning.",
        "Question_gpt_summary":"user encount challeng sweep torch distribut launch perform hyperparamet tune",
        "Answer_gpt_summary_original":"the user is having trouble using sweep with torch.distributed.launch for hyperparameter tuning. the solution suggested is to change the command structure in the sweep config by specifying the interpreter variable to switch to torch.distributed.launch. the answer provides a link to the documentation on how to do this. additionally, the answer suggests a specific command structure that should work in this case. if the issue persists, the user is encouraged to follow up with the support team.",
        "Answer_gpt_summary":"user have troubl sweep torch distribut launch hyperparamet tune solut suggest chang command structur sweep config specifi interpret variabl switch torch distribut launch answer provid link document addition answer suggest specif command structur work case issu persist user encourag follow support team"
    },
    {
        "Question_id":57147396.0,
        "Question_title":"Amazon Sage Maker: How to authenticate AWS SageMaker End-Point Request",
        "Question_body":"<p>I have an aws sagemaker end-point which need to be called from .Net core client, I have used the AWS SDK that deals with SageMaker and provided the required credentials however, always it keeps saying : <\/p>\n\n<p>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.<\/p>\n\n<p>var requestBody = \"{'url':'\"+\"<a href=\"https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg\" rel=\"nofollow noreferrer\">https:\/\/cdn.pixabay.com\/photo\/2018\/05\/28\/22\/11\/message-in-a-bottle-3437294_960_720.jpg<\/a>\" + \"'}\";<\/p>\n\n<pre><code>        var request = new Amazon.SageMakerRuntime.Model.InvokeEndpointRequest()\n        {\n            EndpointName = \"CG-model-v1-endpoint\",\n            ContentType = \"application\/json;utf-8\",\n            Body = new MemoryStream(Encoding.ASCII.GetBytes(JsonConvert.SerializeObject(requestBody)))\n\n        };\n\n\n        var awsClient = new AmazonSageMakerRuntimeClient(awsAccessKeyId: \"XXXX\", awsSecretAccessKey: \"XXX\", region: RegionEndpoint.EUCentral1);\n\n        try\n        {\n            var resposnse = await awsClient.InvokeEndpointAsync(request);\n\n        }\n        catch (Exception ex)\n        {\n\n            return ApiResponse&lt;bool&gt;.Create(false);\n        }\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1563803017410,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":222.0,
        "Answer_body":"<p>I found the error , it was simply because of the request content-type,it had to be application\/json instead of application\/json;utf-8<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57147396",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1563872105436,
        "Question_original_content":"amazon sage maker authent end point request end point need call net core client aw sdk deal provid requir credenti keep sai request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail var requestbodi url http cdn pixabai com photo messag bottl jpg var request new amazon runtim model invokeendpointrequest endpointnam model endpoint contenttyp applic json utf bodi new memorystream encod ascii getbyt jsonconvert serializeobject requestbodi var awsclient new amazonruntimecli awsaccesskeyid awssecretaccesskei region regionendpoint eucentr try var resposns await awsclient invokeendpointasync request catch except return apirespons creat fals",
        "Question_preprocessed_content":"amazon sage maker authent request need call net core client aw sdk deal provid requir credenti keep sai request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail var requestbodi",
        "Answer_original_content":"error simpli request content type applic json instead applic json utf",
        "Answer_preprocessed_content":"error simpli request instead",
        "Question_gpt_summary_original":"the user is encountering a challenge with authenticating an end-point request using the aws sdk and provided credentials, resulting in an error message.",
        "Question_gpt_summary":"user encount challeng authent end point request aw sdk provid credenti result error messag",
        "Answer_gpt_summary_original":"solution: the error encountered while authenticating an end-point request using the aws sdk and provided credentials was due to the request content-type being set to \"application\/json;utf-8\" instead of \"application\/json\". changing the content-type to \"application\/json\" should resolve the issue.",
        "Answer_gpt_summary":"solut error encount authent end point request aw sdk provid credenti request content type set applic json utf instead applic json chang content type applic json resolv issu"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML workspace blob structure \/ Can I safely delete these blobs?",
        "Question_body":"Hello,\n\nI am trying to figure out the folder structure of Azure ML workspace in my storage account.\nI want to be able to delete old pipeline runs and experiments that have piled up in my workspace directly from Azure Storage Explorer without breaking the system.\nMy datastores and folder structure are as follows:\n\nDatastore: workspaceartifactstore\nBlob container: azureml\nFolder structure:\n\u251c\u2500 ComputeRecord\n\u251c\u2500 Dataset\n\u251c\u2500 ExperimentRun\n\u251c\u2500 LocalUpload\n\nDatastore: workspaceblobstore (Default)\nBlob container: azureml-blobstore-(a series of numbers)\nFolder structure:\n\u251c\u2500 azureml\n\u2502 \u251c\u2500\u2500 (a series of numbers)-setup\n\u2502 \u2502 \u251c\u2500\u2500 _tracer.py\n\u2502 \u2502 \u251c\u2500\u2500 azureml_globals.py\n\u2502 \u2502 \u251c\u2500\u2500 context_managers.py\n\u2502 \u2502 \u251c\u2500\u2500 job_prep.py\n\u2502 \u2502 \u251c\u2500\u2500 log_history_status.py\n\u2502 \u2502 \u251c\u2500\u2500 request_utilities.py\n\u2502 \u2502 \u251c\u2500\u2500 run_token_provider.py\n\u2502 \u2502 \u251c\u2500\u2500 utility_context_managers.py\n\u2502 \u251c\u2500\u2500 (another series of numbers)-setup\n\u2502 \u2502 \u251c\u2500\u2500 sames files as above\n\nIt would help if I understood what does each of these containers actually store.\nI already tried to delete all blobs stored in 'workspaceblobstore', but it didn't remove any pipeline or experiment from ML Studio.\nI have a few datasets registered in my workspace, and I don't want to delete them (nor unregister them).\n\nCan I set a data retention policy on both containers in order to delete old blobs?\nCan I safely delete the blobs (folders) stored in 'workspaceartifactstore' too? Will they be recreated automatically when I run a new experiment?\nWhy are there two separate 'azureml' and 'azureml-blobstore-(a series of numbers)' containers? Is it possible to merge them?\n\nThanks.\n\nThank you.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1647500925610,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. I've worked on a similar inquiry and the advise is to not delete data stored in default datastore to avoid weird errors. The option to easily delete experiment runs is on the roadmap. Here's a similar thread. Feel free to raise and track feature request on ideas portal.\n\nAccording to documentation, when you create a workspace, an Azure blob container and an Azure file share are automatically registered as datastores to the workspace. They're named workspaceblobstore and workspacefilestore, respectively. The workspaceblobstore is used to store workspace artifacts and your machine learning experiment logs. It's also set as the default datastore and can't be deleted from the workspace. The workspacefilestore is used to store notebooks and R scripts authorized via compute instance.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/775834\/azure-ml-workspace-blob-structure-can-i-safely-del.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-17T14:52:56.467Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. I've worked on a similar inquiry and the advise is to not delete data stored in default datastore to avoid weird errors. The option to easily delete experiment runs is on the roadmap. Here's a similar thread. Feel free to raise and track feature request on ideas portal.\n\nAccording to documentation, when you create a workspace, an Azure blob container and an Azure file share are automatically registered as datastores to the workspace. They're named workspaceblobstore and workspacefilestore, respectively. The workspaceblobstore is used to store workspace artifacts and your machine learning experiment logs. It's also set as the default datastore and can't be deleted from the workspace. The workspacefilestore is used to store notebooks and R scripts authorized via compute instance.",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1647528776467,
        "Question_original_content":"workspac blob structur safe delet blob hello try figur folder structur workspac storag account want abl delet old pipelin run experi pile workspac directli azur storag explor break datastor folder structur follow datastor workspaceartifactstor blob contain folder structur computerecord dataset experimentrun localupload datastor workspaceblobstor default blob contain blobstor seri number folder structur seri number setup tracer global context manag job prep log histori statu request util run token provid util context manag seri number setup same file help understood contain actual store tri delet blob store workspaceblobstor remov pipelin experi studio dataset regist workspac want delet unregist set data retent polici contain order delet old blob safe delet blob folder store workspaceartifactstor recreat automat run new experi separ blobstor seri number contain possibl merg thank thank",
        "Question_preprocessed_content":"workspac blob structur safe delet blob hello try figur folder structur workspac storag account want abl delet old pipelin run experi pile workspac directli azur storag explor break datastor folder structur follow datastor workspaceartifactstor blob contain folder structur computerecord dataset experimentrun localupload datastor workspaceblobstor blob contain seri number folder structur setup setup same file help understood contain actual store tri delet blob store workspaceblobstor remov pipelin experi studio dataset regist workspac want delet set data retent polici contain order delet old blob safe delet blob store workspaceartifactstor recreat automat run new experi separ seri number contain possibl merg thank thank",
        "Answer_original_content":"thank reach work similar inquiri advis delet data store default datastor avoid weird error option easili delet experi run roadmap similar thread feel free rais track featur request idea portal accord document creat workspac azur blob contain azur file share automat regist datastor workspac name workspaceblobstor workspacefilestor respect workspaceblobstor store workspac artifact machin learn experi log set default datastor delet workspac workspacefilestor store notebook script author comput instanc",
        "Answer_preprocessed_content":"thank reach work similar inquiri advis delet data store default datastor avoid weird error option easili delet experi run roadmap similar thread feel free rais track featur request idea portal accord document creat workspac azur blob contain azur file share automat regist datastor workspac name workspaceblobstor workspacefilestor respect workspaceblobstor store workspac artifact machin learn experi log set default datastor delet workspac workspacefilestor store notebook script author comput instanc",
        "Question_gpt_summary_original":"the user is trying to understand the folder structure of their workspace in their storage account, and is looking for a way to delete old pipeline runs and experiments without breaking the system, as well as setting a data retention policy and merging the two separate containers.",
        "Question_gpt_summary":"user try understand folder structur workspac storag account look wai delet old pipelin run experi break set data retent polici merg separ contain",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n\n- avoid deleting data stored in the default datastore to prevent errors.\n- the option to easily delete experiment runs is on the roadmap.\n- raise and track feature requests on the ideas portal.\n- when creating a workspace, an azure blob container and an azure file share are automatically registered as datastores to the workspace.\n- the workspaceblobstore is used to store workspace artifacts and machine learning experiment logs, and it's set as the default datastore and cannot be deleted from the workspace.\n- the workspacefilestore is used to store notebooks and r scripts authorized via compute instance. \n\nin summary, the answer provides some advice on managing the folder structure of a workspace in a storage account, including avoiding deleting data from the default datastore, and information on the two datastores automatically registered when creating a workspace.",
        "Answer_gpt_summary":"possibl solut answer avoid delet data store default datastor prevent error option easili delet experi run roadmap rais track featur request idea portal creat workspac azur blob contain azur file share automat regist datastor workspac workspaceblobstor store workspac artifact machin learn experi log set default datastor delet workspac workspacefilestor store notebook script author comput instanc summari answer provid advic manag folder structur workspac storag account includ avoid delet data default datastor inform datastor automat regist creat workspac"
    },
    {
        "Question_id":null,
        "Question_title":"Error when using azure trained model on yolov5",
        "Question_body":"Hi, I trained a model with Azure Machine Learning studio for an objects detection tasks using yolov5 as the model. I created the experiment using the notebook and it ran successfully for a little less than 30 epochs. The experiment status tell me that it completed successfully and I have access to the file \"model.pt\" in the output folder of the child run.\n\nNow I would like to use that trained model and test it with the github version of yolov5 installed on my local computer, but when I use it, it simply doesn't work and display an error (it works with my own trained model from my local computer), the error is the following:\n\nTraceback (most recent call last):\nFile \"C:\\Users\\Username\\Desktop\\yolov5\\detect.py\", line 261, in <module>\nmain(opt)\nFile \"C:\\Users\\Username\\Desktop\\yolov5\\detect.py\", line 256, in main\nrun(**vars(opt))\nFile \"C:\\Users\\Username\\Desktop\\yolov5\\lib\\site-packages\\torch\\autograd\\grad_mode.py\", line 27, in decorate_context\nreturn func(args, *kwargs)\nFile \"C:\\Users\\Username\\Desktop\\yolov5\\detect.py\", line 92, in run\nmodel = DetectMultiBackend(weights, device=device, dnn=dnn, data=data)\nFile \"C:\\Users\\Username\\Desktop\\yolov5\\models\\common.py\", line 305, in init\nmodel = attempt_load(weights if isinstance(weights, list) else w, map_location=device)\nFile \"C:\\Users\\Username\\Desktop\\yolov5\\models\\experimental.py\", line 98, in attempt_load\nmodel.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval()) # FP32 model\nKeyError: 'model'\n\nSo is there an explanation, or does the model simply doesn't work with classic yolov5 installations ? Either way I would appreciate the help. Thank you !",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1650115703107,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/814600\/error-when-using-trained-model-on-yolov5.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-18T07:15:39.15Z",
                "Answer_score":0,
                "Answer_body":"@EricNeliz-1395 Are you able to deploy the model as a realtime endpoint directly on Azure? I couldn't really find any direct reference to using the model out of the box on local installations for yolo. One project does help to document steps to use yolov3 with keras locally and on azure. Is this an option to convert your model to a format like keras?\n\n\n\n\nRef: Micheleen Harris",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"error azur train model yolov train model studio object detect task yolov model creat experi notebook ran successfulli littl epoch experi statu tell complet successfulli access file model output folder child run like us train model test github version yolov instal local us simpli work displai error work train model local error follow traceback recent file user usernam desktop yolov detect line main opt file user usernam desktop yolov detect line main run var opt file user usernam desktop yolov lib site packag torch autograd grad mode line decor context return func arg kwarg file user usernam desktop yolov detect line run model detectmultibackend weight devic devic dnn dnn data data file user usernam desktop yolov model common line init model attempt load weight isinst weight list map locat devic file user usernam desktop yolov model experiment line attempt load model append ckpt ema ckpt ema model float fuse eval model keyerror model explan model simpli work classic yolov instal wai appreci help thank",
        "Question_preprocessed_content":"error azur train model yolov train model studio object detect task yolov model creat experi notebook ran successfulli littl epoch experi statu tell complet successfulli access file output folder child run like us train model test github version yolov instal local us simpli work displai error error follow traceback file line main file line main run file line return func file line run model detectmultibackend file line init model isinst file line model keyerror model explan model simpli work classic yolov instal wai appreci help thank",
        "Answer_original_content":"ericneliz abl deploi model realtim endpoint directli azur couldn direct refer model box local instal yolo project help document step us yolov kera local azur option convert model format like kera ref micheleen harri",
        "Answer_preprocessed_content":"abl deploi model realtim endpoint directli azur couldn direct refer model box local instal yolo project help document step us yolov kera local azur option convert model format like kera ref micheleen harri",
        "Question_gpt_summary_original":"the user is encountering an error when attempting to use an azure-trained model with the github version of yolov5 installed on their local computer.",
        "Question_gpt_summary":"user encount error attempt us azur train model github version yolov instal local",
        "Answer_gpt_summary_original":"possible solutions from the answer include deploying the model as a realtime endpoint directly on azure and converting the model to a format like keras.",
        "Answer_gpt_summary":"possibl solut answer includ deploi model realtim endpoint directli azur convert model format like kera"
    },
    {
        "Question_id":51430645.0,
        "Question_title":"split dataframe column header and values into multiple columns",
        "Question_body":"<p>I've uploaded my <code>csv<\/code> file on Azure, but for some reason it became like this<\/p>\n\n<pre><code> nominal;data;curs;cdx         Column 1\n0          1;21.06.2000;28  2300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n1          1;22.06.2000;28  2200;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n2          1;23.06.2000;28  1900;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n3          1;24.06.2000;28  1700;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n4          1;27.06.2000;28  1300;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n5          1;28.06.2000;28  1100;\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd \u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\u00ef\u00bf\u00bd\n<\/code><\/pre>\n\n<p>Basically instead of four columns <code>nominal<\/code>, <code>data<\/code>, <code>curs<\/code>, <code>cdx<\/code> I got two columns with one having all the values and the last one (it is empty or something because the last column has encoding issue) - no idea what.<\/p>\n\n<p>I have deleted the column <code>Column 1<\/code> like this<\/p>\n\n<pre><code>import pandas as pd\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    dataframe1.drop(['Column 1'], axis = 1, inplace = True)\n    print('Input pandas.DataFrame #1:\\r\\n\\r\\n{0}'.format(dataframe1))\n    return dataframe1,\n<\/code><\/pre>\n\n<p>How to split the first column into multiple now? To get 4 separate columns<\/p>\n\n<p>I am using pandas 0.18<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":9,
        "Question_creation_time":1532029428013,
        "Question_favorite_count":null,
        "Question_last_edit_time":1532029712492,
        "Question_score":0.0,
        "Question_view_count":1086.0,
        "Answer_body":"<p>You need to split the column with:<\/p>\n\n<pre><code>dataframe1['nominal;data;curs;cdx'].str.split(';',expand=True)\n<\/code><\/pre>\n\n<p>Then change the headers with:<\/p>\n\n<pre><code>dataframe1.columns = 'nominal;data;curs;cdx'.split(';')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51430645",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1532031297043,
        "Question_original_content":"split datafram column header valu multipl column upload csv file azur reason like nomin data cur cdx column basic instead column nomin data cur cdx got column have valu column encod issu idea delet column column like import panda def main datafram datafram datafram drop column axi inplac true print input panda datafram format datafram return datafram split column multipl separ column panda",
        "Question_preprocessed_content":"split datafram column header valu multipl column upload file azur reason like basic instead column got column have valu idea delet column like split column multipl separ column panda",
        "Answer_original_content":"need split column datafram nomin data cur cdx str split expand true chang header datafram column nomin data cur cdx split",
        "Answer_preprocessed_content":"need split column chang header",
        "Question_gpt_summary_original":"the user is encountering a challenge of splitting a dataframe column header and values into multiple columns using pandas 0.18.",
        "Question_gpt_summary":"user encount challeng split datafram column header valu multipl column panda",
        "Answer_gpt_summary_original":"to split a dataframe column header and values into multiple columns using pandas 0.18, you can use the following solutions: \n1. split the column using `dataframe1['nominal;data;curs;cdx'].str.split(';',expand=true)`\n2. change the headers using `dataframe1.columns = 'nominal;data;curs;cdx'.split(';')`",
        "Answer_gpt_summary":"split datafram column header valu multipl column panda us follow solut split column datafram nomin data cur cdx str split expand true chang header datafram column nomin data cur cdx split"
    },
    {
        "Question_id":null,
        "Question_title":"D365 Demand Forecasting - Can we connect to new Azure ML Service instead of a classic studio service?",
        "Question_body":"There is a D365 forecasting option that allows to connect to a azure ml classic studio service. Can we connect from D365 to the new Azure ML Service? I couldnt find any documentation about this, any pointers please.Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1606959642257,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/183990\/d365-demand-forecasting-can-we-connect-to-new-azur.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-04T08:31:11.253Z",
                "Answer_score":0,
                "Answer_body":"@SriramNarayanan-6939 you can deploy a real-time endpoint in Azure Machine Learning designer and get the REST endpoint\/token by following this doc: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-designer-automobile-price-deploy",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"demand forecast connect new servic instead classic studio servic forecast option allow connect classic studio servic connect new servic document pointer thank",
        "Question_preprocessed_content":"demand forecast connect new servic instead classic studio servic forecast option allow connect classic studio servic connect new servic document pointer",
        "Answer_original_content":"sriramnarayanan deploi real time endpoint design rest endpoint token follow doc http doc microsoft com azur machin learn tutori design automobil price deploi",
        "Answer_preprocessed_content":"deploi endpoint design rest follow doc",
        "Question_gpt_summary_original":"the user is looking for a way to connect their d365 forecasting option to a new service instead of a classic studio service, but has been unable to find any documentation on the matter.",
        "Question_gpt_summary":"user look wai connect forecast option new servic instead classic studio servic unabl document matter",
        "Answer_gpt_summary_original":"possible solution: the user can deploy a real-time endpoint in designer and get the rest endpoint\/token by following the documentation provided in the answer.",
        "Answer_gpt_summary":"possibl solut user deploi real time endpoint design rest endpoint token follow document provid answer"
    },
    {
        "Question_id":null,
        "Question_title":"How to increase the number of parameters logged in MLFlow (current max limit is 100)",
        "Question_body":"I'm using AzureML to train some networks and the recommended MLFlow to log parameters and metrics.\nHowever, I have reached the maximum limit of 100 parameters (mostly configuration options I pass to the trainer but also real model parameters). And I get the following error:\n\nmlflow.exceptions.RestException: BAD_REQUEST: Response: {'Error': {'Code': 'UserError', 'Severity': None, 'Message': 'A field of the entity is over the size limit. FieldName=Parameters, Limit=100, Size=101. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.', 'MessageFormat': None, 'MessageParameters': None, 'ReferenceCode': None, 'DetailsUri': None, 'Target': None, 'Details': [], 'InnerError': None, 'DebugInfo': None, 'AdditionalInfo': None}, 'Correlation': {'operation': 'a9544317f5dd99458be006824948b38d', 'request': '0b131227514b3b4b'}, 'Environment': 'eastus', 'Location': 'eastus', 'ComponentName': 'mlflow', 'error_code': 'BAD_REQUEST'}\n\nIs there a way to increase this limit? If not, are there any other workarounds?\n\nThank you!",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1623455284870,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/433155\/how-to-increase-the-number-of-parameters-logged-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-14T06:55:54.64Z",
                "Answer_score":0,
                "Answer_body":"@JS-0758 Unfortunately, it looks like we only allow logging 100 parameters per run id. Are you using autolog or manually logging more than 100 params?",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"increas number paramet log current max limit train network recommend log paramet metric reach maximum limit paramet configur option pass trainer real model paramet follow error except restexcept bad request respons error code usererror sever messag field entiti size limit fieldnam paramet limit size http aka azur machin learn limit servic limit document messageformat messageparamet referencecod detailsuri target detail innererror debuginfo additionalinfo correl oper afddbebd request bbbb environ eastu locat eastu componentnam error code bad request wai increas limit workaround thank",
        "Question_preprocessed_content":"increas number paramet log train network recommend log paramet metric reach maximum limit paramet follow error respons correl environ eastu locat eastu componentnam wai increas limit workaround thank",
        "Answer_original_content":"unfortun look like allow log paramet run autolog manual log param",
        "Answer_preprocessed_content":"unfortun look like allow log paramet run autolog manual log param",
        "Question_gpt_summary_original":"the user has encountered a challenge of reaching the maximum limit of 100 parameters when using  to log parameters and metrics, and is looking for a way to increase this limit or any other workarounds.",
        "Question_gpt_summary":"user encount challeng reach maximum limit paramet log paramet metric look wai increas limit workaround",
        "Answer_gpt_summary_original":"there is no way to increase the limit of 100 parameters when using  to log parameters and metrics. the user should check if they are using autolog or manually logging more than 100 parameters.",
        "Answer_gpt_summary":"wai increas limit paramet log paramet metric user check autolog manual log paramet"
    },
    {
        "Question_id":null,
        "Question_title":"Parallel coordinate plot doesn't work with nested groupings",
        "Question_body":"<p>Hi!<\/p>\n<p>I have tried to use a parallel coordinate plot with a nested group, and it only shows the top-level group, whereas line plots correctly show multiple level groups. See screenshot below for example\u2014there are 3 lines on the parallel coordinates, but 6 on the line plot. Is this correct behaviour?<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda.png\" data-download-href=\"\/uploads\/short-url\/oRjaIBHNsSCjMDTUurlQOHfluci.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda_2_503x500.png\" alt=\"image\" data-base62-sha1=\"oRjaIBHNsSCjMDTUurlQOHfluci\" width=\"503\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda_2_503x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda_2_754x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda_2_1006x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/ae3ada7ecf86fcbad869ea1404ea0ac792733cda_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1042\u00d71034 127 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1663059079631,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/parallel-coordinate-plot-doesnt-work-with-nested-groupings\/3102",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-14T20:39:57.043Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/olipinski\">@olipinski<\/a>!<\/p>\n<p>Thank you for reaching out to us. This is a known issue which is planned to be resolved in the future, I\u2019ll keep you posted with updates on this as movement is made!<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":5.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-15T10:18:45.814Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a> !<\/p>\n<p>Thanks for your quick reply! I will keep a lookout for when it\u2019s fixed.<\/p>\n<p>Thanks,<br>\nOlaf<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-14T10:19:22.767Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"parallel coordin plot work nest group tri us parallel coordin plot nest group show level group line plot correctli multipl level group screenshot examplether line parallel coordin line plot correct behaviour imag",
        "Question_preprocessed_content":"parallel coordin plot work nest group tri us parallel coordin plot nest group show group line plot correctli multipl level group screenshot examplether line parallel coordin line plot correct behaviour imag",
        "Answer_original_content":"olipinski thank reach known issu plan resolv futur ill post updat movement thank ramit ramit goolri thank quick repli lookout fix thank olaf topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thank reach known issu plan resolv futur ill post updat movement thank ramit thank quick repli lookout fix thank olaf topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user encountered a challenge where a parallel coordinate plot does not correctly display nested groupings, while a line plot does.",
        "Question_gpt_summary":"user encount challeng parallel coordin plot correctli displai nest group line plot",
        "Answer_gpt_summary_original":"there is a known issue with parallel coordinate plots not displaying nested groupings correctly, but it is planned to be resolved in the future. the user should keep a lookout for updates on this issue.",
        "Answer_gpt_summary":"known issu parallel coordin plot displai nest group correctli plan resolv futur user lookout updat issu"
    },
    {
        "Question_id":null,
        "Question_title":"Log_code not working with hydra",
        "Question_body":"<p>Hello to all,<\/p>\n<p>I am new here.  I would like to  save my files to the wandb experiment .<br>\nThis was also working before I used hydra. Since Hydra is changing the run dir.<br>\nI also adapted the  wandb.run.log_code(root=) to the where files are.<br>\nStill not working.<br>\nHas someone an Idea How to fix it<\/p>\n<p>Thank you very much<br>\nBest regards<br>\nChris<\/p>",
        "Question_answer_count":14,
        "Question_comment_count":0,
        "Question_creation_time":1666091221890,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":820.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/log-code-not-working-with-hydra\/3276",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-18T12:20:30.169Z",
                "Answer_body":"<p>Hi Chris,<\/p>\n<p>Thanks for writing in! I was wondering if you could send me a code snippet and so I can see how are you trying to save your files. Also, here is our documentation on how to integrate Hydra with wandb, may it help you?<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-18T14:38:48.604Z",
                "Answer_body":"<blockquote>\n<p>Blockquote<br>\nrun_name = cfg.details.run_name + f\"{cfg.name}_{cfg.details.job_type}<em>seed<\/em>{cfg.seed}\"<br>\nimport wandb<br>\nwandb.init(<br>\nproject=\u201cmaster_lab_irl\u201d,<br>\nname=run_name,<br>\nsync_tensorboard=True,<br>\nmonitor_gym=True,<br>\nsave_code=True,<br>\nsettings=wandb.Settings(code_dir=\u201c\/home\/l-oktober\/lab_master\/src\/inverse_rl\/\u201d),<br>\njob_type=cfg.details.job_type,<br>\n)<\/p>\n<\/blockquote>\n<pre><code>    wandb.run.log_code(root= \"\/home\/l-\/oktober\/lab_master\/src\/inverse_rl\/\")\n<\/code><\/pre>\n<p>Without Hydra I did  wandb.run.log_code((\u201c.\u201d)  and it worked<br>\nI had a look at the documentation but havent found something<\/p>\n<p>Thank you for your help<\/p>",
                "Answer_score":6.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-20T10:12:06.568Z",
                "Answer_body":"<p>Do you need more Information ?<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-20T16:56:29.603Z",
                "Answer_body":"<p>Hi Chris,<\/p>\n<p>Sorry for my late response! I am having some troubles trying to reproduce this issue. I have found a more detail guide here and a complete hydra example here. Please let me know if this would be useful. I will keep trying to reproduce this issue. Thanks!<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":11.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-20T18:58:38.395Z",
                "Answer_body":"<p>Hi Luis,<\/p>\n<p>where can I find this example ?<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-20T19:36:52.970Z",
                "Answer_body":"<p>I found some but all are without saving the code (all .py files)<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-21T16:09:10.643Z",
                "Answer_body":"<p>Hi Chris,<\/p>\n<p>Thanks for your patience! I\u2019ve been able to investigate this in depth and reproduce the issue. Just to confirm that you are having the same behaviour:<\/p>\n<ul>\n<li>First I have run a simple code and <code>run.log_code()<\/code> is saving the <code>.py<\/code> file in the desired folder and uploading an artifact<\/li>\n<li>When I have integrated hydra, the <code>.py<\/code> file is not saved in the <code>root<\/code> argument directory and the artifact is not uploaded to W&amp;B.<br>\nCould you confirm me if you are experiencing the same issue and so I will report this?<\/li>\n<\/ul>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":16.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-23T15:53:41.997Z",
                "Answer_body":"<p>yes it seems that wandb save the files in the   wandb.root or similar<br>\nand this is by hydra a different dir then the  dir the .py is run .<br>\nif I do wandb.run.log_code(root= ) and change it to the orginal dir where the .py files are its still not working.<br>\nIt seems that is because wandb gets the wandb.root with os  current dir and this false.<br>\nAnd I tried to change it but its not working<br>\nMaybe you can help me ( and are able to follow me)<br>\nthanks<\/p>",
                "Answer_score":11.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-26T12:35:32.772Z",
                "Answer_body":"<p>yes so its not possible to make a quick  fix ?<\/p>",
                "Answer_score":20.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-27T11:50:03.653Z",
                "Answer_body":"<p>Hi Chris,<\/p>\n<p>Thanks for answering back! I have gone through our entire conversation again and I think I have been able to solve this issue. I am running a code like this in <code>main.py<\/code> file:<\/p>\n<pre><code>import hydraimport omegaconfimport wandb@hydra.main(config_path=\"configs\/\", config_name=\"defaults\")def run_experiment(cfg: omegaconf.DictConfig) -&gt; None: with wandb.init(entity=cfg.wandb.entity, project=cfg.wandb.project) as run: for i in range(5): run.log({'accuracy':i}) run.log_code(root=cfg.wandb.dir)run_experiment()\n<\/code><\/pre>\n<p>And my <code>defaults.yaml<\/code> config file is like:<\/p>\n<pre><code>---wandb: project: project_name entity: entity_name dir: &lt;folder where main.py is&gt;\n<\/code><\/pre>\n<p>And I can see inside my artifacts tab, in the files page, the <code>main.py<\/code> file. Could you share is there is something different in your workflow? Also, if this is still not working for you, a workaround may be creating a new artifact and adding your <code>.py<\/code> files with <code>add_file<\/code> (docs here). Please let me know if this would be useful. Thanks!<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-02T09:25:02.096Z",
                "Answer_body":"<p>Hi Chris,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-04T10:10:19.112Z",
                "Answer_body":"<p>sorry I did somehow did not see that you answered me<br>\nThank you very much for your answer and yes it works<br>\nThe code its not where it was used to be but it a qiuck fix<\/p>\n<p>THank you very much and<br>\nPlease keep up your great work<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-04T10:19:27.773Z",
                "Answer_body":"<p>Hi Chris,<\/p>\n<p>No worries at all! Great to know that this code is working for you snd thank you very much for the kind words. May I help you in any other way?<\/p>\n<p>Best,<br>\nLuis<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-03T10:11:08.380Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"log code work hydra hello new like save file experi work hydra hydra chang run dir adapt run log code root file work idea fix thank best regard chri",
        "Question_preprocessed_content":"work hydra hello new like save file experi work hydra hydra chang run dir adapt file work idea fix thank best regard chri",
        "Answer_original_content":"chri thank write wonder send code snippet try save file document integr hydra help best lui blockquot run cfg detail run cfg cfg detail job type seed cfg seed import init project master lab irl run sync tensorboard true monitor gym true save code true set set code dir home oktob lab master src invers job type cfg detail job type run log code root home oktob lab master src invers hydra run log code work look document havent thank help need inform chri sorri late respons have troubl try reproduc issu guid complet hydra exampl let know us try reproduc issu thank best lui lui exampl save code file chri thank patienc iv abl investig depth reproduc issu confirm have behaviour run simpl code run log code save file desir folder upload artifact integr hydra file save root argument directori artifact upload confirm experienc issu report best lui ye save file root similar hydra differ dir dir run run log code root chang orgin dir file work get root current dir fals tri chang work mayb help abl follow thank ye possibl quick fix chri thank answer gone entir convers think abl solv issu run code like main file import hydraimport omegaconfimport hydra main config path config config default def run experi cfg omegaconf dictconfig init entiti cfg entiti project cfg project run rang run log accuraci run log code root cfg dir run experi default yaml config file like project project entiti entiti dir insid artifact tab file page main file share differ workflow work workaround creat new artifact ad file add file doc let know us thank best lui chri want follow support request heard let know assist issu resolv best lui sorri answer thank answer ye work code qiuck fix thank great work chri worri great know code work snd thank kind word help wai best lui topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"chri thank write wonder send code snippet try save file document integr hydra help best lui blockquot import init hydra work look document havent thank help need inform chri sorri late respons have troubl try reproduc issu guid complet hydra exampl let know us try reproduc issu thank best lui lui exampl save code chri thank patienc iv abl investig depth reproduc issu confirm have behaviour run simpl code save file desir folder upload artifact integr hydra file save argument directori artifact upload confirm experienc issu report best lui ye save file root similar hydra differ dir dir run chang orgin dir file work get root current dir fals tri chang work mayb help thank ye possibl quick fix chri thank answer gone entir convers think abl solv issu run code like file config file like insid artifact tab file page file share differ workflow work workaround creat new artifact ad file let know us thank best lui chri want follow support request heard let know assist issu resolv best lui sorri answer thank answer ye work code qiuck fix thank great work chri worri great know code work snd thank kind word help wai best lui topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user chris is encountering a challenge with the log_code not working with hydra, and is looking for a solution.",
        "Question_gpt_summary":"user chri encount challeng log code work hydra look solut",
        "Answer_gpt_summary_original":"the user chris is encountering an issue with the log_code not working with hydra. the possible solutions include sending a code snippet to see how the files are being saved, checking the documentation on how to integrate hydra, and using a workaround by creating a new artifact and adding the .py files with add_file. the issue was resolved by changing the root directory in the code and using a quick fix. the support team also provided additional resources and offered further assistance.",
        "Answer_gpt_summary":"user chri encount issu log code work hydra possibl solut includ send code snippet file save check document integr hydra workaround creat new artifact ad file add file issu resolv chang root directori code quick fix support team provid addit resourc offer assist"
    },
    {
        "Question_id":37702759.0,
        "Question_title":"How to visualize charts in Visual Studio and Azure ML through R script?",
        "Question_body":"<p>I have seen on various examples (even in Azure ML) that you are able to create appealing charts using R in Visual Studio (not R Studio!), but I have no clue how they did it. I am experienced with R, but if someone could point me in the right direction of how to visualize data sets in Visual Studio and Azure ML; I would really appreciate it.\nHere is an example I would like to duplicate (in both Azure ML and Visual Studio): <a href=\"http:\/\/i.stack.imgur.com\/2aoGB.jpg\" rel=\"nofollow\">Visual studio chart<\/a><\/p>\n\n<p>Image source: <a href=\"https:\/\/regmedia.co.uk\/2016\/03\/09\/r_vis_studio_plot.jpg?x=648&amp;y=348&amp;crop=1\" rel=\"nofollow\">https:\/\/regmedia.co.uk\/2016\/03\/09\/r_vis_studio_plot.jpg?x=648&amp;y=348&amp;crop=1<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1465389537790,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1485875676343,
        "Question_score":2.0,
        "Question_view_count":1548.0,
        "Answer_body":"<p>You can install ggplot2 in your solution in the Visual Studio extension Open R (<a href=\"https:\/\/www.visualstudio.com\/en-us\/features\/rtvs-vs.aspx\" rel=\"nofollow noreferrer\">https:\/\/www.visualstudio.com\/en-us\/features\/rtvs-vs.aspx<\/a>) through this line of code and visualize it within the R Plot window in Visual Studio after creating your R-project: <\/p>\n\n<pre><code>install.packages('ggplot2', dep = TRUE)\n\nlibrary(ggplot2)\n<\/code><\/pre>\n\n<p>The reason I have \u00ablibrary(ggplot2)\u00bb is to check if the package got successfully installed, else you would get an error like this: <strong>Error in library(ggplot2) : there is no package called \u2018ggplot2\u2019<\/strong><\/p>\n\n<p>So if you don\u2019t get that error; you should be good to go.<\/p>\n\n<p>For your question about how to output charts; you simply have to populate the ggplot2 charts from a datasource, like in my example below (csv-file):<\/p>\n\n<pre><code>dataset1 &lt;- read.csv(\"Adult Census Income Binary Classification dataset.csv\", header = TRUE, sep = \",\", quote = \"\", fill = TRUE, comment.char = \"\")\n\nhead(dataset1)\n\ninstall.packages('ggplot2', dep = TRUE)\n\nlibrary(ggplot2)\n\nnames(dataset1) &lt;- sub(pattern = ',', replacement = '.', x = names(dataset1))\n\nfoo = qplot(age, data = dataset1, geom = \"histogram\", fill = income, position = \"dodge\");\n\nprint(foo)\n\nbar = qplot(age, data = dataset1, geom = \"density\", alpha = 1, fill = income);\n\nprint(bar)\n<\/code><\/pre>\n\n<p>Here you can see that I create two charts, one histogram and one density-chart.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/sLxMN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/sLxMN.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>In Azure ML, the same charts (this time I included a histogram for Relationships as well), would look like this:<\/p>\n\n<pre><code>\/\/ Map 1-based optional input ports to variables\n\ndataset1 &lt;- maml.mapInputPort(1) # class: data.frame\n\nlibrary(ggplot2)\n\nlibrary(data.table)\n\nnames(dataset1) &lt;- sub(pattern=',', replacement='.', x=names(dataset1))\n\n\/\/ This time we need to specify the X to be sex; which we didn\u2019t need in Visual Studio\n\nfoo = qplot(x=sex, data=dataset1, geom=\"histogram\", fill=income, position=\"dodge\");\n\nprint(foo)\n\nfoo = qplot(x=relationship, data=dataset1, geom=\"histogram\", fill=income, position=\"dodge\");\n\nprint(foo)\n\nfoo = qplot(x=age, data=dataset1, geom=\"density\", alpha=0.5, fill=income);\n\nprint(foo)\n\n\/\/ Select data.frame to be sent to the output Dataset port maml.mapOutputPort(\"dataset1\");\n<\/code><\/pre>\n\n<p>Remember to put all of this in a \u201cExecute R Script module\u201d in order to run it correctly. After that, you can right lick the module and visualize the result.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/2vTlD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/2vTlD.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/37702759",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1465396070168,
        "Question_original_content":"visual chart visual studio script seen exampl abl creat appeal chart visual studio studio clue experienc point right direct visual data set visual studio appreci exampl like duplic visual studio visual studio chart imag sourc http regmedia vi studio plot jpg crop",
        "Question_preprocessed_content":"visual chart visual studio script seen exampl abl creat appeal chart visual studio clue experienc point right direct visual data set visual studio appreci exampl like duplic visual studio chart imag sourc",
        "Answer_original_content":"instal ggplot solut visual studio extens open http visualstudio com featur rtv aspx line code visual plot window visual studio creat project instal packag ggplot dep true librari ggplot reason librari ggplot check packag got successfulli instal error like error librari ggplot packag call ggplot dont error good question output chart simpli popul ggplot chart datasourc like exampl csv file dataset read csv adult censu incom binari classif dataset csv header true sep quot true comment char head dataset instal packag ggplot dep true librari ggplot name dataset sub pattern replac name dataset foo qplot ag data dataset geom histogram incom posit dodg print foo bar qplot ag data dataset geom densiti alpha incom print bar creat chart histogram densiti chart chart time includ histogram relationship look like map base option input port variabl dataset maml mapinputport class data frame librari ggplot librari data tabl name dataset sub pattern replac name dataset time need specifi sex didnt need visual studio foo qplot sex data dataset geom histogram incom posit dodg print foo foo qplot relationship data dataset geom histogram incom posit dodg print foo foo qplot ag data dataset geom densiti alpha incom print foo select data frame sent output dataset port maml mapoutputport dataset rememb execut script modul order run correctli right lick modul visual result",
        "Answer_preprocessed_content":"instal ggplot solut visual studio extens open line code visual plot window visual studio creat reason librari check packag got successfulli instal error like error librari packag call ggplot dont error good question output chart simpli popul ggplot chart datasourc like exampl creat chart histogram chart look like rememb execut script modul order run correctli right lick modul visual result",
        "Question_gpt_summary_original":"the user is seeking guidance on how to create appealing charts using r in visual studio, not r studio.",
        "Question_gpt_summary":"user seek guidanc creat appeal chart visual studio studio",
        "Answer_gpt_summary_original":"to create appealing charts using r in visual studio, you can install ggplot2 in your solution in the visual studio extension open r through a line of code and visualize it within the r plot window in visual studio after creating your r-project. you can output charts by populating the ggplot2 charts from a datasource. the example given shows creating two charts, one histogram and one density-chart. you can create the same charts in visual studio by specifying the x-axis and using the same code. finally, put all of this in an execute r script module to run it correctly and visualize the result by right-clicking the module.",
        "Answer_gpt_summary":"creat appeal chart visual studio instal ggplot solut visual studio extens open line code visual plot window visual studio creat project output chart popul ggplot chart datasourc exampl given show creat chart histogram densiti chart creat chart visual studio specifi axi code final execut script modul run correctli visual result right click modul"
    },
    {
        "Question_id":null,
        "Question_title":"Export grouped runs individually as CSV",
        "Question_body":"<p>Hi,<\/p>\n<p>I am new to using wandb. I have a project that contain many groups, each group has multiple runs. I want to export the metrics of each single run in a group to a CSV file but it only allows me to export the statistics of  the group.<\/p>\n<p>How can export each run from a group to CSV?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1671459688409,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":157.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/export-grouped-runs-individually-as-csv\/3571",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-12-19T18:41:50.161Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ahmedramly\">@ahmedramly<\/a>!<\/p>\n<p>Just to confirm your intent here - you would like each individual group as a new CSV file, correct?<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":10.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-19T19:07:33.548Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a><\/p>\n<p>Not exactly, I would like to export a single group or multiple groups but instead of exporting a single metric \u201caccuracy for example\u201d for the group, I want to export the metrics of all the runs included in the group. For some further statistical analysis for instance.<\/p>\n<p>Thanks in advance<\/p>",
                "Answer_score":10.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-05T09:14:26.566Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ahmedramly\">@ahmedramly<\/a>,<\/p>\n<p>Got it. The best way to do this would be to use our API to import data: <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\" class=\"inline-onebox\">Import &amp; Export API - Documentation<\/a>.<\/p>\n<p>Specifically, the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/public-api\/api#runs\">runs<\/a> method of our API can be used to filter runs based on your requirements and then move that data to a  Pandas dataframe to export as a CSV<\/p>",
                "Answer_score":5.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-10T13:32:12.756Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ahmedramly\">@ahmedramly<\/a>,<br>\nJust wanted to check if you have had a chance to try out the API solution I suggested? If you have any further questions, please don\u2019t hesitate to reach out.<br>\nBest,<br>\nRamit<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-11T15:00:39.619Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ahmedramly\">@ahmedramly<\/a>,<br>\nIt seems like you may have moved on from this issue, so I\u2019m going to go ahead and close this thread. If you have any further questions or run into any other issues, please don\u2019t hesitate to reach out.<br>\nThanks again for your time,<br>\nRamit<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"export group run individu csv new project contain group group multipl run want export metric singl run group csv file allow export statist group export run group csv thank",
        "Question_preprocessed_content":"export group run individu csv new project contain group group multipl run want export metric singl run group csv file allow export statist group export run group csv thank",
        "Answer_original_content":"ahmedramli confirm intent like individu group new csv file correct thank ramit ramit goolri exactli like export singl group multipl group instead export singl metric accuraci exampl group want export metric run includ group statist analysi instanc thank advanc ahmedramli got best wai us api import data import export api document specif run method api filter run base requir data panda datafram export csv ahmedramli want check chanc try api solut suggest question dont hesit reach best ramit ahmedramli like move issu go ahead close thread question run issu dont hesit reach thank time ramit",
        "Answer_preprocessed_content":"confirm intent like individu group new csv file correct thank ramit exactli like export singl group multipl group instead export singl metric accuraci exampl group want export metric run includ group statist analysi instanc thank advanc got best wai us api import data import export api document specif run method api filter run base requir data panda datafram export csv want check chanc try api solut suggest question dont hesit reach best ramit like move issu go ahead close thread question run issu dont hesit reach thank time ramit",
        "Question_gpt_summary_original":"the user is looking for a way to export the metrics of each individual run in a group to a csv file, but is only able to export the statistics of the group.",
        "Question_gpt_summary":"user look wai export metric individu run group csv file abl export statist group",
        "Answer_gpt_summary_original":"the solution to export the metrics of all the runs included in a group to a csv file is to use the import and export api. the runs method of the api can be used to filter runs based on requirements and then move that data to a pandas dataframe to export as a csv.",
        "Answer_gpt_summary":"solut export metric run includ group csv file us import export api run method api filter run base requir data panda datafram export csv"
    },
    {
        "Question_id":null,
        "Question_title":"How can I add a table to a run after it has completed via the API?",
        "Question_body":"<p>I would like to log a table to a wandb run, like shown here: <a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/tables-quickstart\">https:\/\/docs.wandb.ai\/guides\/data-vis\/tables-quickstart<\/a><\/p>\n<p>The table will contain information about the performance of an RL agent in environments which differ from its training environment. I want to add the table to the wandb created during the training of the RL agent. Is this possible?<\/p>\n<p>Thanks.<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1651088864988,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":119.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-can-i-add-a-table-to-a-run-after-it-has-completed-via-the-api\/2334",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-29T19:42:20.636Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jcoholich\">@jcoholich<\/a>,<\/p>\n<p>You can <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/resuming?q=resume\">resume<\/a> a run and log your table through that way.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-04T04:36:06.496Z",
                "Answer_body":"<p>Hi <strong><a class=\"mention\" href=\"\/u\/jcoholich\">@jcoholich<\/a><\/strong>,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-10T19:13:04.965Z",
                "Answer_body":"<p>Hi Jeremiah, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-28T19:42:42.352Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"add tabl run complet api like log tabl run like shown http doc guid data vi tabl quickstart tabl contain inform perform agent environ differ train environ want add tabl creat train agent possibl thank",
        "Question_preprocessed_content":"add tabl run complet api like log tabl run like shown tabl contain inform perform agent environ differ train environ want add tabl creat train agent possibl thank",
        "Answer_original_content":"jcoholich resum run log tabl wai thank ramit jcoholich want follow support request heard let know assist issu resolv best jeremiah heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"resum run log tabl wai thank ramit want follow support request heard let know assist issu resolv best jeremiah heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking to add a table to a run after it has completed via the api, containing information about the performance of an rl agent in environments which differ from its training environment.",
        "Question_gpt_summary":"user look add tabl run complet api contain inform perform agent environ differ train environ",
        "Answer_gpt_summary_original":"there are no possible solutions provided in the answer as it seems to be a customer support response that is not related to the question asked.",
        "Answer_gpt_summary":"possibl solut provid answer custom support respons relat question ask"
    },
    {
        "Question_id":56863907.0,
        "Question_title":"how to set path of bucket in amazonsagemaker jupyter notebook?",
        "Question_body":"<p>I'm new to the aws how to set path of my bucket and access file of that bucket?<\/p>\n\n<p>Is there anything i need to change with prefix ?<\/p>\n\n<pre><code>import os\nimport boto3\nimport re\nimport copy\nimport time\nfrom time import gmtime, strftime\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\nregion = boto3.Session().region_name\n\nbucket='ltfs1' # Replace with your s3 bucket name\nprefix = 'sagemaker\/ltfs1' # Used as part of the path in the bucket where you store data\n# bucket_path = 'https:\/\/s3-{}.amazonaws.com\/{}'.format(region,bucket) # The URL to access the bucket\n<\/code><\/pre>\n\n<p>I'm using the above code but it's showing file not found error<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1562134154127,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":852.0,
        "Answer_body":"<p>If the file you are accessing is in the root directory of your s3 bucket, you can access the file like this:<\/p>\n\n<pre><code>import pandas as pd\n\nbucket='ltfs1'\ndata_key = 'data.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ntraining_data = pd.read_csv(data_location)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56863907",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1562166882852,
        "Question_original_content":"set path bucket amazon jupyt notebook new aw set path bucket access file bucket need chang prefix import import boto import import copi import time time import gmtime strftime import execut role role execut role region boto session region bucket ltf replac bucket prefix ltf path bucket store data bucket path http amazonaw com format region bucket url access bucket code show file error",
        "Question_preprocessed_content":"set path bucket amazon jupyt notebook new aw set path bucket access file bucket need chang prefix code show file error",
        "Answer_original_content":"file access root directori bucket access file like import panda bucket ltf data kei data csv data locat format bucket data kei train data read csv data locat",
        "Answer_preprocessed_content":"file access root directori bucket access file like",
        "Question_gpt_summary_original":"the user is encountering challenges setting the path of their bucket and accessing files in the bucket in an amazon jupyter notebook.",
        "Question_gpt_summary":"user encount challeng set path bucket access file bucket amazon jupyt notebook",
        "Answer_gpt_summary_original":"possible solution: if the file is in the root directory of the s3 bucket, the user can access it by importing pandas and using the code provided in the answer to set the path and read the file.",
        "Answer_gpt_summary":"possibl solut file root directori bucket user access import panda code provid answer set path read file"
    },
    {
        "Question_id":null,
        "Question_title":"Azure job requires interactive authentication every time",
        "Question_body":"I'm running a neural network on azure and it's working except it requires me to sign in with a device code for every job that runs. This is an issue for when I want to run long studies with multiple jobs and can't authenticate every job.\n\nI've tried to use the azure CLI authentication with the following code:\n\n cli_auth = AzureCliAuthentication()\n ws = Workspace(subscription_id=\"id\",\n                resource_group=\"rsgp\",\n                workspace_name=\"ws\", auth=cli_auth)\n\n\n\nWhen I do that, I recieve the error: \"Could not retrieve user token. Please run 'az login'\"\nThis happens when I login using az login before a run as well.\n\nI've also tried using the default credential before calling the workspace instead of using CLI, using this code:\n\n credential = DefaultAzureCredential()\n    \n    \n client = ResourceManagementClient(\n     credential=credential,\n     subscription_id=\"id\"\n )\n\n\n\nThis is functional, but I still need to log in manually for every job.\n\nAny help would be appreciated! Thanks!",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1647457870313,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/775236\/azure-job-requires-interactive-authentication-ever.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-17T07:29:42.533Z",
                "Answer_score":0,
                "Answer_body":"@SydneyD-4380 I think you should be using service principal authentication for automated processes. Detailed steps to create a service principal are documented in this notebook along with other available options.\n\nTo summarize, you set a service principal with the required access role and then use the same while defining your workspace.\n\n import os\n from azureml.core.authentication import ServicePrincipalAuthentication\n    \n svc_pr_password = os.environ.get(\"AZUREML_PASSWORD\")\n    \n svc_pr = ServicePrincipalAuthentication(\n     tenant_id=\"my-tenant-id\",\n     service_principal_id=\"my-application-id\",\n     service_principal_password=svc_pr_password)\n    \n    \n ws = Workspace(\n     subscription_id=\"my-subscription-id\",\n     resource_group=\"my-ml-rg\",\n     workspace_name=\"my-ml-workspace\",\n     auth=svc_pr\n     )\n    \n print(\"Found workspace {} at location {}\".format(ws.name, ws.location))\n\n\n\nService principal password can be set as environment variable if it is for local testing or can be retried using Azure keyvault or a secret variable in Azure DevOps.\nHope this helps!!\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"azur job requir interact authent time run neural network azur work requir sign devic code job run issu want run long studi multipl job authent job tri us azur cli authent follow code cli auth azurecliauthent workspac subscript resourc group rsgp workspac auth cli auth reciev error retriev user token run login happen login login run tri default credenti call workspac instead cli code credenti defaultazurecredenti client resourcemanagementcli credenti credenti subscript function need log manual job help appreci thank",
        "Question_preprocessed_content":"azur job requir interact authent time run neural network azur work requir sign devic code job run issu want run long studi multipl job authent job tri us azur cli authent follow code azurecliauthent reciev error retriev user token run login happen login login run tri default credenti call workspac instead cli code credenti defaultazurecredenti client resourcemanagementcli function need log manual job help appreci thank",
        "Answer_original_content":"sydneyd think servic princip authent autom process detail step creat servic princip document notebook avail option summar set servic princip requir access role us defin workspac import core authent import serviceprincipalauthent svc password environ password svc serviceprincipalauthent tenant tenant servic princip applic servic princip password svc password workspac subscript subscript resourc group workspac workspac auth svc print workspac locat format locat servic princip password set environ variabl local test retri azur keyvault secret variabl azur devop hope help answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"think servic princip authent autom process detail step creat servic princip document notebook avail option summar set servic princip requir access role us defin workspac import import serviceprincipalauthent serviceprincipalauthent workspac print servic princip password set environ variabl local test retri azur keyvault secret variabl azur devop hope help answer help click upvot help commun member read thread",
        "Question_gpt_summary_original":"the user is encountering challenges with running a neural network on azure, as they are required to sign in with a device code for every job that runs, and have tried using the azure cli authentication and the default credential, but still need to log in manually for every job.",
        "Question_gpt_summary":"user encount challeng run neural network azur requir sign devic code job run tri azur cli authent default credenti need log manual job",
        "Answer_gpt_summary_original":"possible solutions to the challenge of having to manually sign in with a device code for every job that runs on azure include using service principal authentication for automated processes. the steps to create a service principal are documented in a notebook, and involve setting a service principal with the required access role and then using it while defining the workspace. the service principal password can be set as an environment variable or retrieved using azure keyvault or a secret variable in azure devops.",
        "Answer_gpt_summary":"possibl solut challeng have manual sign devic code job run azur includ servic princip authent autom process step creat servic princip document notebook involv set servic princip requir access role defin workspac servic princip password set environ variabl retriev azur keyvault secret variabl azur devop"
    },
    {
        "Question_id":46222606.0,
        "Question_title":"Updating pandas to version 0.19 in Azure ML Studio",
        "Question_body":"<p>I would really like to get access to some of the updated functions in pandas 0.19, but Azure ML studio uses pandas 0.18 as part of the Anaconda 4.0 bundle. Is there a way to update the version that is used within the \"Execute Python Script\" components?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_creation_time":1505401641617,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":1903.0,
        "Answer_body":"<p>I offer the below steps for you to show how to update the version of pandas  library in <code>Execute Python Script<\/code>.<\/p>\n\n<p><strong><em>Step 1<\/em><\/strong> : Use the <code>virtualenv<\/code> component to create an independent python runtime environment in your system.Please install it first with command <code>pip install virtualenv<\/code> if you don't have it.<\/p>\n\n<p>If you installed it successfully ,you could see it in your python\/Scripts file.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ZFI2t.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step2<\/em><\/strong> : Run the commad to create independent python runtime environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/nzDqz.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 3<\/em><\/strong> : Then go into the created directory's Scripts folder and activate it (this step is important , don't miss it)<\/p>\n\n<p>Please don't close this command window and use <code>pip install pandas==0.19<\/code> to download external libraries in this command window.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Wj857.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Wj857.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 4<\/em><\/strong> : Compress all of the files in the Lib\/site-packages folder into a zip package (I'm calling it pandas - package here)<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ch9Oo.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 5<\/em><\/strong> \uff1aUpload the zip package into the Azure Machine Learning WorkSpace DataSet.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/efRkK.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/efRkK.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>specific steps please refer to the <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/cdb56f95-7f4c-404d-bde7-5bb972e6f232\/\" rel=\"noreferrer\">Technical Notes<\/a>.<\/p>\n\n<p>After success, you will see the uploaded package in the DataSet List<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/ngGCu.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong><em>Step 6<\/em><\/strong> \uff1a Before the defination of method <code>azureml_main<\/code> in the Execute Python Script module, you need to remove the old <code>pandas<\/code> modules &amp; its dependencies, then to import <code>pandas<\/code> again, as the code below.<\/p>\n\n<pre><code>import sys\nimport pandas as pd\nprint(pd.__version__)\ndel sys.modules['pandas']\ndel sys.modules['numpy']\ndel sys.modules['pytz']\ndel sys.modules['six']\ndel sys.modules['dateutil']\nsys.path.insert(0, '.\\\\Script Bundle')\nfor td in [m for m in sys.modules if m.startswith('pandas.') or m.startswith('numpy.') or m.startswith('pytz.') or m.startswith('dateutil.') or m.startswith('six.')]:\n    del sys.modules[td]\nimport pandas as pd\nprint(pd.__version__)\n# The entry point function can contain up to two input arguments:\n#   Param&lt;dataframe1&gt;: a pandas.DataFrame\n#   Param&lt;dataframe2&gt;: a pandas.DataFrame\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n<\/code><\/pre>\n\n<p>Then you can see the result from logs as below, first print the old version <code>0.14.0<\/code>, then print the new version <code>0.19.0<\/code> from the uploaded zip file.<\/p>\n\n<pre><code>[Information]         0.14.0\n[Information]         0.19.0\n<\/code><\/pre>\n\n<p>You could also refer to these threads: <a href=\"https:\/\/stackoverflow.com\/questions\/45749479\/access-blob-file-using-time-stamp-in-azure\/45814318#45814318\">Access blob file using time stamp in Azure<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/12669546\/reload-with-reset\">reload with reset<\/a>.<\/p>\n\n<p>Hope it helps you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1505805193896,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46222606",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1505456513227,
        "Question_original_content":"updat panda version studio like access updat function panda studio us panda anaconda bundl wai updat version execut python script compon",
        "Question_preprocessed_content":"updat panda version studio like access updat function panda studio us panda anaconda bundl wai updat version execut python script compon",
        "Answer_original_content":"offer step updat version panda librari execut python script step us virtualenv compon creat independ python runtim environ instal command pip instal virtualenv instal successfulli python script file step run commad creat independ python runtim environ step creat directori script folder activ step import miss close command window us pip instal panda download extern librari command window step compress file lib site packag folder zip packag call panda packag step upload zip packag workspac dataset specif step refer technic note success upload packag dataset list step defin method main execut python script modul need remov old panda modul depend import panda code import sy import panda print version del sy modul panda del sy modul numpi del sy modul pytz del sy modul del sy modul dateutil sy path insert script bundl sy modul startswith panda startswith numpi startswith pytz startswith dateutil startswith del sy modul import panda print version entri point function contain input argument param panda datafram param panda datafram def main datafram datafram result log print old version print new version upload zip file inform inform refer thread access blob file time stamp azur reload reset hope help",
        "Answer_preprocessed_content":"offer step updat version panda librari step us compon creat independ python runtim environ instal command instal successfulli file step run commad creat independ python runtim environ step creat directori script folder activ close command window us download extern librari command window step compress file folder zip packag step upload zip packag workspac dataset specif step refer technic note success upload packag dataset list step defin method execut python script modul need remov old modul depend import code result log print old version print new version upload zip file refer thread access blob file time stamp azur reload reset hope help",
        "Question_gpt_summary_original":"the user is looking for a way to update the version of pandas used in studio from 0.18 to 0.19 in order to access new functions.",
        "Question_gpt_summary":"user look wai updat version panda studio order access new function",
        "Answer_gpt_summary_original":"to update the version of pandas library in execute python script, the user can use the virtualenv component to create an independent python runtime environment in their system. they can then activate it and use pip install pandas==0.19 to download external libraries in this command window. after compressing all of the files in the lib\/site-packages folder into a zip package, they can upload the zip package into the workspace dataset. before the definition of method _main in the execute python script module, the user needs to remove the old pandas modules and its dependencies, then import pandas again. the entry point function can contain up to two input arguments: param<dataframe1>: a pandas.dataframe and param<dataframe2>: a pandas.dataframe.",
        "Answer_gpt_summary":"updat version panda librari execut python script user us virtualenv compon creat independ python runtim environ activ us pip instal panda download extern librari command window compress file lib site packag folder zip packag upload zip packag workspac dataset definit method main execut python script modul user need remov old panda modul depend import panda entri point function contain input argument param panda datafram param panda datafram"
    },
    {
        "Question_id":null,
        "Question_title":"UNZIP large zip file in azure machine learning",
        "Question_body":"I have a big zip file, I need to unzip it in order to use the files in my notebook\n\nI used this script in my notebook :\n\nimport os\nimport zipfile\nlocal_zip = 'Caltech101.zip'\nzip_ref = zipfile.ZipFile(local_zip, 'r')\nzip_ref.extractall('Caltech101')\nzip_ref.close()\n\nit succeeded (green v in the notebook), but only the first two folders were unzipped.\nthis file is only 130 MB\nI also need to unzip 3 GB zip file",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1612431352160,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/258610\/unzip-large-zip-file-in-azure-machine-learning.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-05T18:20:48.12Z",
                "Answer_score":0,
                "Answer_body":"Thanks for following up. Can you please try available public datasets in this dataset repository to determine if you experience the same issue? If you're able to upload other files to Azure ML Notebook and unzip them successfully, then the issue is most likely related to your file content\/format.",
                "Answer_comment_count":6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"unzip larg zip file big zip file need unzip order us file notebook script notebook import import zipfil local zip caltech zip zip ref zipfil zipfil local zip zip ref extractal caltech zip ref close succeed green notebook folder unzip file need unzip zip file",
        "Question_preprocessed_content":"unzip larg zip file big zip file need unzip order us file notebook script notebook import import zipfil succeed folder unzip file need unzip zip file",
        "Answer_original_content":"thank follow try avail public dataset dataset repositori determin experi issu abl upload file notebook unzip successfulli issu like relat file content format",
        "Answer_preprocessed_content":"thank follow try avail public dataset dataset repositori determin experi issu abl upload file notebook unzip successfulli issu like relat file",
        "Question_gpt_summary_original":"the user encountered a challenge in attempting to unzip a large zip file (130 mb and 3 gb) in their notebook, only succeeding in unzipping the first two folders.",
        "Question_gpt_summary":"user encount challeng attempt unzip larg zip file notebook succeed unzip folder",
        "Answer_gpt_summary_original":"possible solutions:\n- try available public datasets in the dataset repository to determine if the issue is specific to the user's file.\n- check if other files can be uploaded and unzipped successfully in the notebook to determine if the issue is related to the file content\/format. \n\nsummary:\nthe user can try using other public datasets to determine if the issue is specific to their file or check if other files can be uploaded and unzipped successfully to determine if the issue is related to the file content\/format.",
        "Answer_gpt_summary":"possibl solut try avail public dataset dataset repositori determin issu specif user file check file upload unzip successfulli notebook determin issu relat file content format summari user try public dataset determin issu specif file check file upload unzip successfulli determin issu relat file content format"
    },
    {
        "Question_id":null,
        "Question_title":"About the end of Machine Learning Studio (classic)",
        "Question_body":"hello.\n\nI am currently using Machine Learning Studio (classic).\n\n'From now through 31 August 2024, you can continue to use the existing Machine Learning Studio (classic) experiments and web services. Beginning 1 December 2021, new creation of Machine Learning Studio (classic) resources will not be available.'\n\nAs mentioned above, what do resources mean?\nIs it possible to continue creating experiments and web sales, and using APIs from outside until 2024?",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1635690920907,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @gatsby53-6558 ,\n\nThanks for reaching out to us here. There are several important dates.\n\nBeginning 1 December 2021, you will not be able to create new Machine Learning Studio (classic) resources. You can still work on your existing resource from 1 December 2021 to 31 August 2024.\n\nSupport for Machine Learning Studio (classic) will end on 31 August 2024. We recommend you transition to Azure Machine Learning by that date.\n\nPlease refer to this guidance for how to migrate your project for better experience.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/610432\/about-the-end-of-machine-learning-studio-classic.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-01T02:29:26.197Z",
                "Answer_score":0,
                "Answer_body":"Hello @gatsby53-6558 ,\n\nThanks for reaching out to us here. There are several important dates.\n\nBeginning 1 December 2021, you will not be able to create new Machine Learning Studio (classic) resources. You can still work on your existing resource from 1 December 2021 to 31 August 2024.\n\nSupport for Machine Learning Studio (classic) will end on 31 August 2024. We recommend you transition to Azure Machine Learning by that date.\n\nPlease refer to this guidance for how to migrate your project for better experience.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/migrate-overview\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1635733766196,
        "Question_original_content":"end machin learn studio classic hello current machin learn studio classic august continu us exist machin learn studio classic experi web servic begin decemb new creation machin learn studio classic resourc avail mention resourc mean possibl continu creat experi web sale api outsid",
        "Question_preprocessed_content":"end machin learn studio hello current machin learn studio august continu us exist machin learn studio experi web servic begin decemb new creation machin learn studio resourc mention resourc mean possibl continu creat experi web sale api outsid",
        "Answer_original_content":"hello gatsbi thank reach import date begin decemb abl creat new machin learn studio classic resourc work exist resourc decemb august support machin learn studio classic end august recommend transit date refer guidanc migrat project better experi http doc microsoft com azur machin learn migrat overview hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"hello thank reach import date begin decemb abl creat new machin learn studio resourc work exist resourc decemb august support machin learn studio end august recommend transit date refer guidanc migrat project better experi hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Question_gpt_summary_original":"the user is inquiring about the resources available in machine learning studio (classic) and whether they can continue to create experiments and web services, and use apis from outside until 2024.",
        "Question_gpt_summary":"user inquir resourc avail machin learn studio classic continu creat experi web servic us api outsid",
        "Answer_gpt_summary_original":"the answer states that users can continue to work on their existing resources in machine learning studio (classic) until august 31, 2024, but they will not be able to create new resources after december 1, 2021. the answer recommends transitioning to a different platform by august 31, 2024, and provides a link to guidance on how to migrate projects.",
        "Answer_gpt_summary":"answer state user continu work exist resourc machin learn studio classic august abl creat new resourc decemb answer recommend transit differ platform august provid link guidanc migrat project"
    },
    {
        "Question_id":60405600.0,
        "Question_title":"How to make inference on local PC with the model trained on AWS SageMaker by using the built-in algorithm Semantic Segmentation?",
        "Question_body":"<p>Similar to the issue of <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/200\" rel=\"nofollow noreferrer\">The trained model can be deployed on the other platform without dependency of sagemaker or aws service?<\/a>.<\/p>\n\n<p>I have trained a model on AWS SageMaker by using the built-in algorithm <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/semantic-segmentation.html\" rel=\"nofollow noreferrer\">Semantic Segmentation<\/a>. This trained model named as <code>model.tar.gz<\/code> is stored on S3. So I want to download this file from S3 and then use it to make inference on my local PC without using AWS SageMaker anymore. Since the built-in algorithm Semantic Segmentation is built using the <a href=\"https:\/\/github.com\/dmlc\/gluon-cv\" rel=\"nofollow noreferrer\">MXNet Gluon framework and the Gluon CV toolkit<\/a>, so I try to refer the documentation of <a href=\"https:\/\/mxnet.apache.org\/\" rel=\"nofollow noreferrer\">mxnet<\/a> and <a href=\"https:\/\/gluon-cv.mxnet.io\/\" rel=\"nofollow noreferrer\">gluon-cv<\/a> to make inference on local PC.<\/p>\n\n<p>It's easy to download this file from S3, and then I unzip this file to get three files:<\/p>\n\n<ol>\n<li><strong>hyperparams.json<\/strong>: includes the parameters for network architecture, data inputs, and training. Refer to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/segmentation-hyperparameters.html\" rel=\"nofollow noreferrer\">Semantic Segmentation Hyperparameters<\/a>.<\/li>\n<li><strong>model_algo-1<\/strong><\/li>\n<li><strong>model_best.params<\/strong><\/li>\n<\/ol>\n\n<p>Both <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong> are the trained models, and I think it's the output from <code>net.save_parameters<\/code> (Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/4-train.html\" rel=\"nofollow noreferrer\">Train the neural network<\/a>). I can also load them with the function <code>mxnet.ndarray.load<\/code>.<\/p>\n\n<p>Refer to <a href=\"https:\/\/beta.mxnet.io\/guide\/getting-started\/crash-course\/5-predict.html\" rel=\"nofollow noreferrer\">Predict with a pre-trained model<\/a>. I found there are two necessary things:<\/p>\n\n<ol>\n<li>Reconstruct the network for making inference.<\/li>\n<li>Load the trained parameters.<\/li>\n<\/ol>\n\n<p>As for reconstructing the network for making inference, since I have used PSPNet from training, so I can use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network. And I know how to use some services of AWS SageMaker, for example batch transform jobs, to make inference. I want to reproduce it on my local PC. If I use the class <code>gluoncv.model_zoo.PSPNet<\/code> to reconstruct the network, I can't make sure whether the parameters for this network are same those used on AWS SageMaker while making inference. Because I can't see the image <code>501404015308.dkr.ecr.ap-northeast-1.amazonaws.com\/semantic-segmentation:latest<\/code> in detail. <\/p>\n\n<p>As for loading the trained parameters, I can use the <code>load_parameters<\/code>. But as for <strong>model_algo-1<\/strong> and <strong>model_best.params<\/strong>, I don't know which one I should use.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1582681249783,
        "Question_favorite_count":null,
        "Question_last_edit_time":1582681944470,
        "Question_score":0.0,
        "Question_view_count":351.0,
        "Answer_body":"<p>The following code works well for me.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import mxnet as mx\nfrom mxnet import image\nfrom gluoncv.data.transforms.presets.segmentation import test_transform\nimport gluoncv\n\n# use cpu\nctx = mx.cpu(0)\n\n# load test image\nimg = image.imread('.\/img\/IMG_4015.jpg')\nimg = test_transform(img, ctx)\nimg = img.astype('float32')\n\n# reconstruct the PSP network model\nmodel = gluoncv.model_zoo.PSPNet(2)\n\n# load the trained model\nmodel.load_parameters('.\/model\/model_algo-1')\n\n# make inference\noutput = model.predict(img)\npredict = mx.nd.squeeze(mx.nd.argmax(output, 1)).asnumpy()\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60405600",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1583126137776,
        "Question_original_content":"infer local model train built algorithm semant segment similar issu train model deploi platform depend aw servic train model built algorithm semant segment train model name model tar store want download file us infer local anymor built algorithm semant segment built mxnet gluon framework gluon toolkit try refer document mxnet gluon infer local easi download file unzip file file hyperparam json includ paramet network architectur data input train refer semant segment hyperparamet model algo model best param model algo model best param train model think output net save paramet refer train neural network load function mxnet ndarrai load refer predict pre train model necessari thing reconstruct network make infer load train paramet reconstruct network make infer pspnet train us class gluoncv model zoo pspnet reconstruct network know us servic exampl batch transform job infer want reproduc local us class gluoncv model zoo pspnet reconstruct network sure paramet network make infer imag dkr ecr northeast amazonaw com semant segment latest load train paramet us load paramet model algo model best param know us",
        "Question_preprocessed_content":"infer local model train algorithm semant segment similar issu train model deploi platform depend aw servic train model algorithm semant segment train model name store want download file us infer local anymor algorithm semant segment built mxnet gluon framework gluon toolkit try refer document mxnet infer local easi download file unzip file file includ paramet network architectur data input train refer semant segment hyperparamet train model think output load function refer predict model necessari thing reconstruct network make infer load train paramet reconstruct network make infer pspnet train us class reconstruct network know us servic exampl batch transform job infer want reproduc local us class reconstruct network sure paramet network make infer imag load train paramet us know us",
        "Answer_original_content":"follow code work import mxnet mxnet import imag gluoncv data transform preset segment import test transform import gluoncv us cpu ctx cpu load test imag img imag imread img img jpg img test transform img ctx img img astyp float reconstruct psp network model model gluoncv model zoo pspnet load train model model load paramet model model algo infer output model predict img predict squeez argmax output asnumpi",
        "Answer_preprocessed_content":"follow code work",
        "Question_gpt_summary_original":"the user is facing challenges in downloading a trained model from s3, reconstructing the network for making inference, and loading the trained parameters in order to make inference on their local pc without using any aws services.",
        "Question_gpt_summary":"user face challeng download train model reconstruct network make infer load train paramet order infer local aw servic",
        "Answer_gpt_summary_original":"the answer provides a code snippet that can be used to download a trained model from s3, reconstruct the network for making inference, and load the trained parameters in order to make inference on a local pc without using any aws services. the code uses the mxnet library and the gluoncv toolkit.",
        "Answer_gpt_summary":"answer provid code snippet download train model reconstruct network make infer load train paramet order infer local aw servic code us mxnet librari gluoncv toolkit"
    },
    {
        "Question_id":56771758.0,
        "Question_title":"On-Premises Hosting of Trained DeepAR Model built on AWS SageMaker",
        "Question_body":"<p>I have started working with <strong>AWS SageMaker<\/strong> recently with the examples provided by AWS. I used this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/deepar_electricity\/DeepAR-Electricity.ipynb\" rel=\"nofollow noreferrer\">example<\/a> (<strong>DeepAR<\/strong> Model) in order to forecast a time series. After training, a model artifacts file has been created in my S3 bucket. <\/p>\n\n<p><strong>My question:<\/strong> Is there a way to host that trained model in a own hosting environment? (client premises)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1561548485583,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":379.0,
        "Answer_body":"<p>Except SageMaker XGBoost, SageMaker built-in algorithms are not designed to be used out of Amazon. That does not mean that it's impossible, for example you can find here and there snippets peeking inside model artifacts (eg for <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">Factorization Machines<\/a> and <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_applying_machine_learning\/ntm_20newsgroups_topic_modeling\/ntm_20newsgroups_topic_model.ipynb\" rel=\"nofollow noreferrer\">Neural Topic Model<\/a>) but these things can be hacky and are usually not part of official service features. Regarding DeepAR specifically, the model was open-sourced couple weeks ago as part of <code>gluon-ts<\/code> python package (<a href=\"https:\/\/aws.amazon.com\/blogs\/opensource\/gluon-time-series-open-source-time-series-modeling-toolkit\/\" rel=\"nofollow noreferrer\">blog post<\/a>, <a href=\"https:\/\/gluon-ts.mxnet.io\/\" rel=\"nofollow noreferrer\">code<\/a>) so if you develop a model specifically for your own hosting environment I'd recommend to use that gluon-ts code in the MXNet container, so that you'll be able to open and read the artifact out of SageMaker.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56771758",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1561921541280,
        "Question_original_content":"premis host train deepar model built start work recent exampl provid aw exampl deepar model order forecast time seri train model artifact file creat bucket question wai host train model host environ client premis",
        "Question_preprocessed_content":"host train deepar model built start work recent exampl provid aw exampl order forecast time seri train model artifact file creat bucket question wai host train model host environ",
        "Answer_original_content":"xgboost built algorithm design amazon mean imposs exampl snippet peek insid model artifact factor machin neural topic model thing hacki usual offici servic featur deepar specif model open sourc coupl week ago gluon python packag blog post code develop model specif host environ recommend us gluon code mxnet contain abl open read artifact",
        "Answer_preprocessed_content":"xgboost algorithm design amazon mean imposs exampl snippet peek insid model artifact thing hacki usual offici servic featur deepar specif model coupl week ago python packag develop model specif host environ recommend us code mxnet contain abl open read artifact",
        "Question_gpt_summary_original":"the user is facing a challenge of hosting a trained deepar model built on  in their own hosting environment.",
        "Question_gpt_summary":"user face challeng host train deepar model built host environ",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n- xgboost can be used out of amazon.\n- snippets can be found to peek inside model artifacts for some built-in algorithms, but they can be hacky and unofficial.\n- for hosting a trained deepar model specifically, the gluon-ts python package can be used in the mxnet container to open and read the artifact.",
        "Answer_gpt_summary":"possibl solut answer xgboost amazon snippet peek insid model artifact built algorithm hacki unoffici host train deepar model specif gluon python packag mxnet contain open read artifact"
    },
    {
        "Question_id":null,
        "Question_title":"Free tier usage Sagemaker",
        "Question_body":"Hi, I have a question about the free tier usage of AWS Sagemaker.\nIt says on the free tier page:\n\nAmazon SageMaker\n250 Hours\nper month of t2.medium notebook usage for the first two months\n\nDoes this mean that you have to start using Sagemaker right away when you start your account or your two months will be void? Or does this mean that the first two months you're using Sagemaker, up to 250 hours per month of t2.medium notebook usage is free?\n\nI really just want to test something but I have to know if I can still do this for free. I've had my account since september 2018 so >2 months.\n\nThe FAQ page that's referred to about expiring offers FREE TRIAL (What's the difference between expiring and non-expiring offers? VIEW FAQs >> https:\/\/aws.amazon.com\/free\/faqs\/?ft=n) doesn't give an answer to my question.\n\nThanks in advance.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1548335489000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":116.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUWzc5HuoeQZOuo79E8SQwxg\/free-tier-usage-sagemaker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-01-28T22:33:22.000Z",
                "Answer_score":0,
                "Answer_body":"Hello?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-01-28T08:59:09.000Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nSorry for any confusion caused!\n\nThe SageMaker trial begins after your first use of SageMaker - if you have not yet launched any SageMaker resources you will still be eligible for the 2 month trial period.\n\nWe can take a look at your account and advise directly if you create a support case: http:\/\/amzn.to\/aws-support-center.\n\nI hope this helps!\n\nWarmest Regards,\n-RachaelAWS",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"free tier usag question free tier usag sai free tier page hour month medium notebook usag month mean start right awai start account month void mean month hour month medium notebook usag free want test know free account septemb month faq page refer expir offer free trial differ expir non expir offer view faq http aw amazon com free faq answer question thank advanc",
        "Question_preprocessed_content":"free tier usag question free tier usag sai free tier page hour month notebook usag month mean start right awai start account month void mean month hour month notebook usag free want test know free account septemb month faq page refer expir offer free trial answer question thank advanc",
        "Answer_original_content":"hello hello sorri confus caus trial begin us launch resourc elig month trial period look account advis directli creat support case http amzn aw support center hope help warmest regard rachaelaw",
        "Answer_preprocessed_content":"hello hello sorri confus caus trial begin us launch resourc elig month trial period look account advis directli creat support case hope help warmest regard rachaelaw",
        "Question_gpt_summary_original":"the user is questioning the free tier usage of aws and is unsure if they can still use it for free after having their account since september 2018.",
        "Question_gpt_summary":"user question free tier usag aw unsur us free have account septemb",
        "Answer_gpt_summary_original":"if the user has not launched any resources, they are still eligible for the 2-month trial period. they can create a support case to get advice on their account.",
        "Answer_gpt_summary":"user launch resourc elig month trial period creat support case advic account"
    },
    {
        "Question_id":null,
        "Question_title":"dataset.to_pandas_dataframe() throws a DatabaseConnectionException",
        "Question_body":"Hello,\n\nI try to access a Azure ML Dataset in python and get the following error message:\n\n Execution failed in operation 'to_pandas_dataframe' for Dataset(id='[data.id]', name='[data.name]', version=1, error_code=ScriptExecution.DatabaseConnection.Unexpected,error_message=ScriptExecutionException was caused by DatabaseConnectionException.\\n  DatabaseConnectionException was caused by UnexpectedException.\\n    'MSSQL' encountered unexpected exception of type 'AggregateException' with HResult 'x80131500' while opening connection to server ([REDACTED]), database ([REDACTED]).\\n      Failed due to inner exception of type: AggregateException\\n| session_id=[session-id]) ErrorCode: ScriptExecution.DatabaseConnection.Unexpected\n\n\n\nIt is a Tabular Dataset created from a Azure SQL database datasource.\nFor the access from the studio to the database a service principal is used.\nI can access all the resources mentioned above in the standard ui.\n\nThe \"sample usage\" in the \"consume\" tab of the dataset was used for accessing the dataset in python.\nRegarding environments i tried python 3.6 and 3.8 locally and in a compute instance of the ML studio.\nHowever the same error keeps coming.\n\nI also tried to use sync-keys as described in this question:\nhttps:\/\/docs.microsoft.com\/en-us\/answers\/questions\/644562\/datasetto-pandas-dataframe-throws-a-scriptexecutio.html\n\n\n\n\nBest Regards,\nGerhard",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1646912554290,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/767054\/datasetto-pandas-dataframe-throws-a-databaseconnec.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-11T09:21:44.827Z",
                "Answer_score":1,
                "Answer_body":"A DevOps-Colleague could find the cause of the problem. The service principal didn't have the correct rights for my use case.\n\n@romungi-MSFT : I'll try your suggestion since I can create a snapshot of the table.\n\nThanks for your time and support!\n\nCheers,\nGerhard",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"dataset panda datafram throw databaseconnectionexcept hello try access dataset python follow error messag execut fail oper panda datafram dataset data data version error code scriptexecut databaseconnect unexpect error messag scriptexecutionexcept caus databaseconnectionexcept databaseconnectionexcept caus unexpectedexcept mssql encount unexpect except type aggregateexcept hresult open connect server redact databas redact fail inner except type aggregateexcept session session errorcod scriptexecut databaseconnect unexpect tabular dataset creat azur sql databas datasourc access studio databas servic princip access resourc mention standard sampl usag consum tab dataset access dataset python environ tri python local comput instanc studio error keep come tri us sync kei describ question http doc microsoft com answer question datasetto panda datafram throw scriptexecutio html best regard gerhard",
        "Question_preprocessed_content":"throw databaseconnectionexcept hello try access dataset python follow error messag execut fail oper version caus databaseconnectionexcept caus mssql encount unexpect except type aggregateexcept hresult open connect server databas fail inner except type errorcod tabular dataset creat azur sql databas datasourc access studio databas servic princip access resourc mention standard sampl usag consum tab dataset access dataset python environ tri python local comput instanc studio error keep come tri us describ question best regard gerhard",
        "Answer_original_content":"devop colleagu caus problem servic princip correct right us case romungi msft try suggest creat snapshot tabl thank time support cheer gerhard",
        "Answer_preprocessed_content":"caus problem servic princip correct right us case try suggest creat snapshot tabl thank time support cheer gerhard",
        "Question_gpt_summary_original":"the user is encountering an error when attempting to access a dataset in python using the dataset.to_pandas_dataframe() method, resulting in a databaseconnectionexception.",
        "Question_gpt_summary":"user encount error attempt access dataset python dataset panda datafram method result databaseconnectionexcept",
        "Answer_gpt_summary_original":"possible solutions to the error encountered when accessing a dataset in python using the dataset.to_pandas_dataframe() method are to have a devops-colleague find the cause of the problem and to ensure that the service principal has the correct rights for the use case. another possible solution is to create a snapshot of the table.",
        "Answer_gpt_summary":"possibl solut error encount access dataset python dataset panda datafram method devop colleagu caus problem ensur servic princip correct right us case possibl solut creat snapshot tabl"
    },
    {
        "Question_id":49689216.0,
        "Question_title":"How can I print the Canonical String which aws-requests-auth sends?",
        "Question_body":"<p>I want to have a lambda calling a Sagemaker instance in another region. If both are in the same region, everything works fine. If they are not, I get the following error:<\/p>\n\n<pre><code>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\n\nThe Canonical String for this request should have been\n'POST\n\/endpoints\/foo-endpoint\/invocations\n\nhost:runtime.sagemaker.us-east-1.amazonaws.com\nx-amz-date:20180406T082536Z\n\nhost;x-amz-date\n1234567890foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo'\n\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20180406T082536Z\n20180406\/us-east-1\/sagemaker\/aws4_request\n987654321abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf'\n<\/code><\/pre>\n\n<p>I use <a href=\"https:\/\/github.com\/DavidMuller\/aws-requests-auth\" rel=\"nofollow noreferrer\"><code>aws-requests-auth<\/code><\/a> (0.4.1) with boto3 (1.5.15 - updating to 1.7.1 didn't change anything, <a href=\"https:\/\/github.com\/boto\/boto3\/blob\/develop\/CHANGELOG.rst\" rel=\"nofollow noreferrer\">changelog<\/a>) like this:<\/p>\n\n<pre><code>import requests\nfrom aws_requests_auth.aws_auth import AWSRequestsAuth\nauth = AWSRequestsAuth(aws_access_key=config['AWS']['ACCESS_KEY'],\n                       aws_secret_access_key=(\n                           config['AWS']['SECRET_ACCESS_KEY']),\n                       aws_host=config['AWS']['HOST'],\n                       aws_region=config['AWS']['REGION'],\n                       aws_service=config['AWS']['SERVICE'])\n\npayload = {'foo': 'bar'}\nresponse = requests.post(post_url,\n                         data=json.dumps(payload),\n                         headers={'content-type': 'application\/json'},\n                         auth=auth)\n<\/code><\/pre>\n\n<p>printing <code>auth<\/code> only gives <code>&lt;aws_requests_auth.aws_auth.AWSRequestsAuth object at 0x7f9d00c98390&gt;<\/code>.<\/p>\n\n<p>Is there a way to print the \"Canonical String\" mentioned in the error message?<\/p>\n\n<p>(Any other ideas how to fix this are appreciated as well)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1523005580303,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1531211963023,
        "Question_score":3.0,
        "Question_view_count":827.0,
        "Answer_body":"<p>A work-around for the asked question:<\/p>\n\n<pre><code>req = requests.request('POST', 'http:\/\/httpbin.org\/get')\nreq.body = b''\nreq.method = ''\nprint(auth.get_aws_request_headers(req,\n                                   aws_access_key=auth.aws_access_key,\n                                   aws_secret_access_key=auth.aws_secret_access_key,\n                                   aws_token=auth.aws_token))\n<\/code><\/pre>\n\n<p>The problem is not solved, though. And now I wonder what the first argument of <code>auth.get_aws_request_headers<\/code> is.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49689216",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1523012141480,
        "Question_original_content":"print canon string aw request auth send want lambda call instanc region region work fine follow error request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail canon string request post endpoint foo endpoint invoc host runtim east amazonaw com amz date host amz date foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo string sign aw hmac sha east aw request abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf us aw request auth boto updat chang changelog like import request aw request auth aw auth import awsrequestsauth auth awsrequestsauth aw access kei config aw access kei aw secret access kei config aw secret access kei aw host config aw host aw region config aw region aw servic config aw servic payload foo bar respons request post post url data json dump payload header content type applic json auth auth print auth give wai print canon string mention error messag idea fix appreci",
        "Question_preprocessed_content":"print canon string send want lambda call instanc region region work fine follow error us boto like print give wai print canon string mention error messag idea fix appreci",
        "Answer_original_content":"work ask question req request request post http httpbin org req bodi req method print auth aw request header req aw access kei auth aw access kei aw secret access kei auth aw secret access kei aw token auth aw token problem solv wonder argument auth aw request header",
        "Answer_preprocessed_content":"ask question problem solv wonder argument",
        "Question_gpt_summary_original":"the user is encountering a challenge in printing the canonical string which aws-requests-auth sends when attempting to call a lambda instance in another region.",
        "Question_gpt_summary":"user encount challeng print canon string aw request auth send attempt lambda instanc region",
        "Answer_gpt_summary_original":"the answer provides a work-around for the challenge of printing the canonical string which aws-requests-auth sends when calling a lambda instance in another region. the work-around involves setting the request body to an empty byte string, setting the request method to an empty string, and then printing the aws request headers using the auth.get_aws_request_headers() function. however, the problem is not fully solved and the answer raises a new question about the first argument of the auth.get_aws_request_headers() function.",
        "Answer_gpt_summary":"answer provid work challeng print canon string aw request auth send call lambda instanc region work involv set request bodi byte string set request method string print aw request header auth aw request header function problem fulli solv answer rais new question argument auth aw request header function"
    },
    {
        "Question_id":null,
        "Question_title":"Register Data Sets in Azure ML",
        "Question_body":"Hello dear Microsoft Q&A Team,\n\nI'm new to Azure ML and have the following problem: I can't create data assets using a datastore. I did the following:\n\nI created a datastore which points on my adls gen2 datalake using a service principal. I assigend all Storage Blob Data rolls (of the ressource group of the datalake) to this service principal. But when I try to create a dataset using this datastore the storage path does not load:\n\nAny idea what I'm doing wrong?",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1665561633607,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1044629\/register-data-sets-in-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-21T07:48:19.023Z",
                "Answer_score":0,
                "Answer_body":"Hello @romungi-MSFT ,\n\nno worries. I was able to create a data set, but had to enter the file path manually:\n\n\n\n\n\nFor me it is fine, but I think it still not working the way it should be.\n\nCheers\n\nLukas",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"regist data set hello dear microsoft team new follow problem creat data asset datastor follow creat datastor point adl gen datalak servic princip assigend storag blob data roll ressourc group datalak servic princip try creat dataset datastor storag path load idea wrong",
        "Question_preprocessed_content":"regist data set hello dear microsoft team new follow problem creat data asset datastor follow creat datastor point adl gen datalak servic princip assigend storag blob data roll servic princip try creat dataset datastor storag path load idea wrong",
        "Answer_original_content":"hello romungi msft worri abl creat data set enter file path manual fine think work wai cheer luka",
        "Answer_preprocessed_content":"hello worri abl creat data set enter file path manual fine think work wai cheer luka",
        "Question_gpt_summary_original":"the user is encountering challenges with creating data assets using a datastore in , and is unable to load the storage path.",
        "Question_gpt_summary":"user encount challeng creat data asset datastor unabl load storag path",
        "Answer_gpt_summary_original":"possible solutions: \n- manually enter the file path to create a data set.",
        "Answer_gpt_summary":"possibl solut manual enter file path creat data set"
    },
    {
        "Question_id":null,
        "Question_title":"Rearranging columns in guild compare",
        "Question_body":"<p>Hi there!<\/p>\n<p>First off, thanks a lot for making Guild!<\/p>\n<p>I was wondering if there is a way to rearrange the columns in <code>guild compare<\/code>? It seems like by default the scalars come after the flags whereas I\u2019d like for it to be the other way round.<\/p>\n<p>Second, I\u2019m noticing that if I use <code>guild compare -cc<\/code> followed by a comma-separated list of column names which include both flags and scalars, only the scalars get outputted correctly; the values for the flag columns are empty.<\/p>\n<p>FWIW I\u2019m on Guild version 0.7.1:<\/p>\n<pre><code>% pip freeze | grep guildai                                                                                                                               20-12-08 - 14:26:45\nguildai==0.7.1\n<\/code><\/pre>\n<p>EDIT: here is a quick example to reproduce things.<\/p>\n<p>Here\u2019s my toy <code>guild.yml<\/code> file:<\/p>\n<pre><code>- operations:\n    quick_example:\n      description: \"Quick example\"\n      exec: \"echo \\\"Loss: 0.123\\\"\"\n      flags:\n        log_file:\n          type: string\n          default: \"example.log\"\n      output-scalars:\n        - loss: 'Loss: (\\value)'\n<\/code><\/pre>\n<p>If I run <code>guild run quick_example<\/code> and then <code>guild_compare<\/code>, I see<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8.png\" data-download-href=\"\/uploads\/short-url\/vezbGnqgINBlCyo0RrLJKtjFQLK.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_517x42.png\" alt=\"image\" data-base62-sha1=\"vezbGnqgINBlCyo0RrLJKtjFQLK\" width=\"517\" height=\"42\" srcset=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_517x42.png, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_775x63.png 1.5x, https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_1034x84.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/optimized\/1X\/dae8f403971fd56cf3ac5e4b506538faffed2da8_2_10x10.png\"><div class=\"meta\"><svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1350\u00d7110 5.76 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg><\/div><\/a><\/div><\/p>\n<p>However, if I run <code>guild compare -cc log_file,loss<\/code> I get<\/p>\n<p><img src=\"https:\/\/global.discourse-cdn.com\/standard11\/uploads\/guild\/original\/1X\/30bf182e19fbc50a4b89c31a42ed85cc964b4039.png\" alt=\"image\" data-base62-sha1=\"6XekRJDBHwfTbaXShdnSESkYNMR\" width=\"377\" height=\"99\"><\/p>\n<p>Thanks a lot in advance!<br>\njks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607455711413,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":240.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/rearranging-columns-in-guild-compare\/496",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-08T21:02:37.145Z",
                "Answer_body":"<p>To show flags in a column listing you need to prefix the name with <code>=<\/code>. E.g. <code>guild compare -cc loss,=log_file<\/code>.<\/p>\n<p>In your Guild file, specify the columns you want in order using the operation <code>compare<\/code> attribute like this:<\/p>\n<pre><code class=\"lang-yaml\"># guild.yml\n\nquick_example:\n  ...\n  compare:\n    - last loss step as step\n    - loss\n    - =log_file\n<\/code><\/pre>\n<p>Unfortunately there\u2019s no way to generalize the order to place flags after scalars. This configuration is all per operation.<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-08T21:33:25.270Z",
                "Answer_body":"<p>Thank you <a class=\"mention\" href=\"\/u\/garrett\">@garrett<\/a>! This seems to be what I needed \u2013 per-operation configuation should be just fine.<\/p>",
                "Answer_score":46.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"rearrang column compar thank lot make wonder wai rearrang column compar like default scalar come flag like wai round second notic us compar follow comma separ list column name includ flag scalar scalar output correctli valu flag column fwiw version pip freez grep edit quick exampl reproduc thing here toi yml file oper quick exampl descript quick exampl exec echo loss flag log file type string default exampl log output scalar loss loss valu run run quick exampl compar imag run compar log file loss thank lot advanc jk",
        "Question_preprocessed_content":"rearrang column compar thank lot make wonder wai rearrang column like default scalar come flag like wai round second notic us follow list column name includ flag scalar scalar output correctli valu flag column fwiw version edit quick exampl reproduc thing here toi file run imag run thank lot advanc jk",
        "Answer_original_content":"flag column list need prefix compar loss log file file specifi column want order oper compar attribut like yml quick exampl compar loss step step loss log file unfortun there wai gener order place flag scalar configur oper thank garrett need oper configu fine",
        "Answer_preprocessed_content":"flag column list need prefix file specifi column want order oper attribut like unfortun there wai gener order place flag scalar configur oper thank need configu fine",
        "Question_gpt_summary_original":"the user is encountering challenges with rearranging columns in guild compare and outputting both flags and scalars correctly.",
        "Question_gpt_summary":"user encount challeng rearrang column compar output flag scalar correctli",
        "Answer_gpt_summary_original":"the solution to showing flags in a column listing is to prefix the name with =. to specify the columns in order, use the operation compare attribute in the guild file. unfortunately, there is no way to generalize the order to place flags after scalars, as this configuration is all per operation.",
        "Answer_gpt_summary":"solut show flag column list prefix specifi column order us oper compar attribut file unfortun wai gener order place flag scalar configur oper"
    },
    {
        "Question_id":null,
        "Question_title":"Auzre ML realtime endpoint deployment stuck after source folder upload",
        "Question_body":"Hi,\n\nUsing either Azure CLI or Python SDK V2, realtime endpoint deployment gets stuck forever after source folder is uploaded. Any suggestions? Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1658139184840,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/930850\/auzre-ml-realtime-endpoint-deployment-stuck-after.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-21T03:48:41.097Z",
                "Answer_score":1,
                "Answer_body":"@romungi-MSFT It looks like that using CLI or Python SDK, the deployment process always gets stuck after the upload of large files (model or other assets). Only when the model is uploaded and registered beforehand using Azure ML studio UI, the deployment process can succeed whether via CLI, Python SDK or the studio UI.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"auzr realtim endpoint deploy stuck sourc folder upload azur cli python sdk realtim endpoint deploy get stuck forev sourc folder upload suggest thank",
        "Question_preprocessed_content":"auzr realtim endpoint deploy stuck sourc folder upload azur cli python sdk realtim endpoint deploy get stuck forev sourc folder upload suggest thank",
        "Answer_original_content":"romungi msft look like cli python sdk deploy process get stuck upload larg file model asset model upload regist studio deploy process succe cli python sdk studio",
        "Answer_preprocessed_content":"look like cli python sdk deploy process get stuck upload larg file model upload regist studio deploy process succe cli python sdk studio",
        "Question_gpt_summary_original":"the user is encountering an issue with deploying a realtime endpoint on , where the process gets stuck after the source folder is uploaded.",
        "Question_gpt_summary":"user encount issu deploi realtim endpoint process get stuck sourc folder upload",
        "Answer_gpt_summary_original":"possible solutions to the issue of a stuck deployment process after uploading large files on  are to use the studio ui to upload and register the model beforehand, and then proceed with the deployment process via cli, python sdk, or the studio ui.",
        "Answer_gpt_summary":"possibl solut issu stuck deploy process upload larg file us studio upload regist model proce deploy process cli python sdk studio"
    },
    {
        "Question_id":null,
        "Question_title":"Ground Truth - Error loading Image for Task",
        "Question_body":"Hi,\n\nI am creating a custom labeling job using boto3. It creates the job and the user is able to access the job using the front end tool but it does not load the images. I am using the default pre labeling lambda functions (arn:aws:lambda:us-east-1:432418664414:function:PRE-SemanticSegmentation) and the default for the annotation consolidation lambda (arn:aws:lambda:us-east-1:432418664414:function:ACS-SemanticSegmentation). I think the issue is with the name I am using for the LabelAttributeName (custom-ref) but I am not sure.\n\nFull error: There was an error loading the image for the task\nThis task was created incorrectly and cannot be completed at this time. Please return\/stop this task.\n\nAny help would be appreciated.\n\nThanks,\nHumberto",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1552494750000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":82.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUfa0f2oL7SY2hRFJ05dRC6Q\/ground-truth-error-loading-image-for-task",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-03-13T18:15:54.000Z",
                "Answer_score":0,
                "Answer_body":"The template I was using was missing the grant_read_access to the task.input.taskObject.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"ground truth error load imag task creat custom label job boto creat job user abl access job end tool load imag default pre label lambda function arn aw lambda east function pre semanticsegment default annot consolid lambda arn aw lambda east function ac semanticsegment think issu labelattributenam custom ref sure error error load imag task task creat incorrectli complet time return stop task help appreci thank humberto",
        "Question_preprocessed_content":"ground truth error load imag task creat custom label job boto creat job user abl access job end tool load imag default pre label lambda function default annot consolid lambda think issu labelattributenam sure error error load imag task task creat incorrectli complet time task help appreci thank humberto",
        "Answer_original_content":"templat miss grant read access task input taskobject",
        "Answer_preprocessed_content":"templat miss",
        "Question_gpt_summary_original":"the user encountered a challenge with loading images for a custom labeling job using boto3, resulting in an error message indicating the task was created incorrectly and could not be completed.",
        "Question_gpt_summary":"user encount challeng load imag custom label job boto result error messag indic task creat incorrectli complet",
        "Answer_gpt_summary_original":"the solution to the challenge of loading images for a custom labeling job using boto3 is to add the grant_read_access to the task.input.taskobject in the template being used.",
        "Answer_gpt_summary":"solut challeng load imag custom label job boto add grant read access task input taskobject templat"
    },
    {
        "Question_id":64974572.0,
        "Question_title":"RecordIO: \"The header of the MXNet RecordIO record...does not start with a valid magic number\"",
        "Question_body":"<p>Using Linear Learner in Sagemaker with MXNet RecordIO, I get <code>&quot;The header of the MXNet RecordIO record at position 5,089,840 in the dataset does not start with a valid magic number&quot;<\/code> after <code>fit()<\/code> has been running 38 minutes.<\/p>\n<p>The file was generated using this code. Note that I tried two ways to upload to S3. I also tried direct upload of the <code>BytesIO<\/code> as well as upload of a file, shown here.<\/p>\n<pre><code>train_file = 'linear_train.data'\n\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\nf.seek(0)\n \n# Write the stuff\nwith open(train_file, &quot;wb&quot;) as fl:\n    fl.write(f.getvalue())\n\n# Alternative for upload\n# boto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)\nboto3.client('s3').upload_file(train_file,\n                                  Bucket=bucket,\n                                  Key=os.path.join(prefix, 'train', train_file),\n                                  ExtraArgs={'ACL': 'bucket-owner-full-control'})\n<\/code><\/pre>\n<p>To check that the file was non-corrupt, I downloaded it from S3 and simply read it as follows.<\/p>\n<pre><code>   record = MXRecordIO(fl, 'r')\n   while True:\n      item = record.read()\n      # Here we print the item, break when we reach the end, etc. This confirms that the RecordIO is valid.\n<\/code><\/pre>\n<p>So, the file seems OK.<\/p>\n<p>How can I run Linear Learner?<\/p>\n<p>Here is the error message:<\/p>\n<pre><code>Failure reason if any: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError)\n\nCaused by: [17:04:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.3446.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 5,089,840 in the dataset does not start with a valid magic number.\n\nStack trace returned 10 entries:\n[bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xbca0) [0x7f337885cca0]\n[bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xbffa) [0x7f337885cffa]\n[bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7f33788675e6]\n[bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7f3367272141]\n[bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7f3378893958]\n[bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7f33\nFailed 39.0 min; Failure reason if any: ClientError: Unable to read data channel 'train'. Requested content-type is 'application\/x-recordio-protobuf'. Please verify the data matches the requested content-type. (caused by MXNetError)\n\nCaused by: [17:04:49] \/opt\/brazil-pkg-cache\/packages\/AIAlgorithmsCppLibs\/AIAlgorithmsCppLibs-2.0.3446.0\/AL2012\/generic-flavor\/src\/src\/aialgs\/io\/iterator_base.cpp:100: (Input Error) The header of the MXNet RecordIO record at position 5,089,840 in the dataset does not start with a valid magic number.\n\nStack trace returned 10 entries:\n[bt] (0) \/opt\/amazon\/lib\/libaialgs.so(+0xbca0) [0x7f337885cca0]\n[bt] (1) \/opt\/amazon\/lib\/libaialgs.so(+0xbffa) [0x7f337885cffa]\n[bt] (2) \/opt\/amazon\/lib\/libaialgs.so(aialgs::iterator_base::Next()+0x4a6) [0x7f33788675e6]\n[bt] (3) \/opt\/amazon\/lib\/libmxnet.so(MXDataIterNext+0x21) [0x7f3367272141]\n[bt] (4) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call_unix64+0x4c) [0x7f3378893958]\n[bt] (5) \/opt\/amazon\/python2.7\/lib\/python2.7\/lib-dynload\/_ctypes.so(ffi_call+0x15f) [0x7f33\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606157531417,
        "Question_favorite_count":null,
        "Question_last_edit_time":1606158102500,
        "Question_score":0.0,
        "Question_view_count":178.0,
        "Answer_body":"<p>Is was because of CSV files in the same S3 folder as the RecordIO.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64974572",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1606206060190,
        "Question_original_content":"recordio header mxnet recordio record start valid magic number linear learner mxnet recordio header mxnet recordio record posit dataset start valid magic number fit run minut file gener code note tri wai upload tri direct upload bytesio upload file shown train file linear train data bytesio smac write numpi dens tensor train astyp float train astyp float seek write stuff open train file write getvalu altern upload boto session resourc bucket bucket object path join prefix train train file upload fileobj boto client upload file train file bucket bucket kei path join prefix train train file extraarg acl bucket owner control check file non corrupt download simpli read follow record mxrecordio true item record read print item break reach end confirm recordio valid file run linear learner error messag failur reason clienterror unabl read data channel train request content type applic recordio protobuf verifi data match request content type caus mxneterror caus opt brazil pkg cach packag aialgorithmscpplib aialgorithmscpplib gener flavor src src aialg iter base cpp input error header mxnet recordio record posit dataset start valid magic number stack trace return entri opt amazon lib libaialg xbca xfcca opt amazon lib libaialg xbffa xfcffa opt amazon lib libaialg aialg iter base xfe opt amazon lib libmxnet mxdataiternext opt amazon python lib python lib dynload ctype ffi unix opt amazon python lib python lib dynload ctype ffi fail min failur reason clienterror unabl read data channel train request content type applic recordio protobuf verifi data match request content type caus mxneterror caus opt brazil pkg cach packag aialgorithmscpplib aialgorithmscpplib gener flavor src src aialg iter base cpp input error header mxnet recordio record posit dataset start valid magic number stack trace return entri opt amazon lib libaialg xbca xfcca opt amazon lib libaialg xbffa xfcffa opt amazon lib libaialg aialg iter base xfe opt amazon lib libmxnet mxdataiternext opt amazon python lib python lib dynload ctype ffi unix opt amazon python lib python lib dynload ctype ffi",
        "Question_preprocessed_content":"recordio header mxnet recordio start valid magic number linear learner mxnet recordio run minut file gener code note tri wai upload tri direct upload upload file shown check file download simpli read follow file run linear learner error messag",
        "Answer_original_content":"csv file folder recordio",
        "Answer_preprocessed_content":"csv file folder recordio",
        "Question_gpt_summary_original":"the user is encountering a challenge with recordio when using linear learner in mxnet, resulting in an error message indicating that the header of the mxnet recordio record does not start with a valid magic number.",
        "Question_gpt_summary":"user encount challeng recordio linear learner mxnet result error messag indic header mxnet recordio record start valid magic number",
        "Answer_gpt_summary_original":"possible solution: remove the csv files from the same s3 folder as the recordio files.",
        "Answer_gpt_summary":"possibl solut remov csv file folder recordio file"
    },
    {
        "Question_id":58801976.0,
        "Question_title":"Evaluating payload provided when calling invoke endpoint operation",
        "Question_body":"<p>I'm working on a credit fraud dataset on sagemaker. I'm using a linear learner binary classification algorithm. I divided the data into training and test sets and got the results for test set. When I tried to evaluate model performance characteristics on training set, I'm getting the following error<\/p>\n\n<pre><code> An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error \n(400) from model with message \"unable to evaluate payload provided\".\n<\/code><\/pre>\n\n<p>I mentioned the code below<\/p>\n\n<pre><code>train_data = 's3:\/\/{}\/{}\/{}'.format(bucket, prefix, 'train\/examples') #making train_data\n\n#making test_data\ntest_key = \"{}\/test\/examples\".format(prefix)\ns3.Bucket(bucket).download_file(test_key, 'test_data')\n\n#preparing train channels for training the data\ntrain_channel = sagemaker.session.s3_input(train_data, content_type='text\/csv')\n\n#training the data\nlinear.fit(inputs=train_channel,  logs=True)\n\n#creating the endpoint\nlinear_predictor = linear.deploy(initial_instance_count=1,\n                             instance_type='ml.m4.xlarge')\n\n#getting the results on test_data\nl = []\nwith open('test_data', 'r') as f:\nfor j in range(0,56962):\n    single_test = f.readline()\n    result = linear_predictor.predict(single_test)\n    l.append(result)\n    if j%10000 ==0 :\n        print(j)\nprint(l[0:10])\n\n#getting the results on train_data\n#THE CODE BELOW IS THROWING THE ABOVE MENTIONED ERROR\nq =[]\nwith open('train_data', 'r') as f:\nfor j in range(0,56962):\n    single_test = f.readline()\n    result = linear_predictor.predict(single_test)\n    q.append(result)\n    if j%10000 ==0 :\n        print(j)\nprint(q[0:10])\n<\/code><\/pre>\n\n<p>I'm getting the results on test data. I stored it in list l. For getting the results on the training set, I followed the similar procedure, but I'm getting the above mentioned error. Can someone please offer a resolution for this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1573477497977,
        "Question_favorite_count":null,
        "Question_last_edit_time":1573548102688,
        "Question_score":0.0,
        "Question_view_count":1769.0,
        "Answer_body":"<p>\"unable to evaluate payload provided\" occurs only when the input data format is not compatible with the ML model you created. In this case, to get results on training set, we need to remove the last column(label column) before passing it to endpoint<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58801976",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1574162320510,
        "Question_original_content":"evalu payload provid call invok endpoint oper work credit fraud dataset linear learner binari classif algorithm divid data train test set got result test set tri evalu model perform characterist train set get follow error error occur modelerror call invokeendpoint oper receiv client error model messag unabl evalu payload provid mention code train data format bucket prefix train exampl make train data make test data test kei test exampl format prefix bucket bucket download file test kei test data prepar train channel train data train channel session input train data content type text csv train data linear fit input train channel log true creat endpoint linear predictor linear deploi initi instanc count instanc type xlarg get result test data open test data rang singl test readlin result linear predictor predict singl test append result print print get result train data code throw mention error open train data rang singl test readlin result linear predictor predict singl test append result print print get result test data store list get result train set follow similar procedur get mention error offer resolut",
        "Question_preprocessed_content":"evalu payload provid call invok endpoint oper work credit fraud dataset linear learner binari classif algorithm divid data train test set got result test set tri evalu model perform characterist train set get follow error mention code get result test data store list get result train set follow similar procedur get mention error offer resolut",
        "Answer_original_content":"unabl evalu payload provid occur input data format compat model creat case result train set need remov column label column pass endpoint",
        "Answer_preprocessed_content":"unabl evalu payload provid occur input data format compat model creat case result train set need remov column pass endpoint",
        "Question_gpt_summary_original":"the user is encountering an error when attempting to evaluate model performance characteristics on a training set using a linear learner binary classification algorithm.",
        "Question_gpt_summary":"user encount error attempt evalu model perform characterist train set linear learner binari classif algorithm",
        "Answer_gpt_summary_original":"solution: to evaluate model performance characteristics on a training set using a linear learner binary classification algorithm, the user needs to remove the last column (label column) from the input data before passing it to the endpoint. the error \"unable to evaluate payload provided\" occurs only when the input data format is not compatible with the machine learning model created.",
        "Answer_gpt_summary":"solut evalu model perform characterist train set linear learner binari classif algorithm user need remov column label column input data pass endpoint error unabl evalu payload provid occur input data format compat machin learn model creat"
    },
    {
        "Question_id":73062370.0,
        "Question_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Question_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1658389162900,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":58.0,
        "Answer_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1658393400156,
        "Question_original_content":"graph best perform run sweep ner model us sweep hyperparamet search grid search run meaning graph figur creat graph show best run term score know",
        "Question_preprocessed_content":"graph best perform run sweep ner model us sweep hyperparamet search grid search run meaning graph figur creat graph show best run term know",
        "Answer_original_content":"sweep view filter run certain criteria click button add filter run score accuraci metric log higher certain valu cours won filter best run run accuraci higher exampl pictur",
        "Answer_preprocessed_content":"sweep view filter run certain criteria click button add filter run score accuraci metric log higher certain valu cours won filter best run run accuraci higher",
        "Question_gpt_summary_original":"the user is trying to figure out how to create a graph that shows the best 10 runs in terms of f-score from a grid search with about 100 runs using  sweeps for hyperparameter search.",
        "Question_gpt_summary":"user try figur creat graph show best run term score grid search run sweep hyperparamet search",
        "Answer_gpt_summary_original":"the answer suggests using the filter option in the sweep view to only show runs with a certain metric value higher than a certain threshold. however, it does not provide a direct solution for selecting the top 10 runs based on f-score.",
        "Answer_gpt_summary":"answer suggest filter option sweep view run certain metric valu higher certain threshold provid direct solut select run base score"
    },
    {
        "Question_id":null,
        "Question_title":"Tracking Data Provenance with DVC",
        "Question_body":"<p>I recently discovered DVC and am looking to replace my current shell script-based approach for downloading source datasets and building derived datasets with DVC. In my current process, I have a clear record of data provenance as my scripted pipelines begin with downloads of the source datasets from the web.<\/p>\n<p>The question I have is the following: does DVC provide functionality that allows me to capture data provenance somehow? Can I record the URL from which the data was originally sourced and bind that to metadata associated with the data file(s)? Or will I need to maintain scripts that allow me to easily reacquire the data from the web? In the documentation, it seems like the story begins with the source data already in hand.<\/p>\n<p>I would love to have functionality that allows me to easily reacquire the source data from the web if needed and verify that indeed the dataset is equivalent to the original form that was used in the original pipeline development (via hash comparison).<\/p>\n<p>As of yet, I\u2019m not quite seeing how I would accomplish this with existing DVC commands. Any pointers would be greatly appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1652228110629,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":170.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/tracking-data-provenance-with-dvc\/1186",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-18T10:50:48.093Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/diehl\">@diehl<\/a> , sorry for late response.<\/p>\n<p>If I understand your use case correctly, it looks like you might be interested in using <code>dvc import<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/import\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/import<\/a>) and\/or <code>dvc import-url<\/code> (<a href=\"https:\/\/dvc.org\/doc\/command-reference\/import-url\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/command-reference\/import-url<\/a>)<\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-05-21T18:22:52.060Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/daavoo\">@daavoo<\/a>, No worries!<\/p>\n<p>That sounds on point. Thank you!<\/p>",
                "Answer_score":11.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"track data proven recent discov look replac current shell script base approach download sourc dataset build deriv dataset current process clear record data proven script pipelin begin download sourc dataset web question follow provid function allow captur data proven record url data origin sourc bind metadata associ data file need maintain script allow easili reacquir data web document like stori begin sourc data hand love function allow easili reacquir sourc data web need verifi dataset equival origin form origin pipelin develop hash comparison see accomplish exist command pointer greatli appreci",
        "Question_preprocessed_content":"track data proven recent discov look replac current shell approach download sourc dataset build deriv dataset current process clear record data proven script pipelin begin download sourc dataset web question follow provid function allow captur data proven record url data origin sourc bind metadata associ data file need maintain script allow easili reacquir data web document like stori begin sourc data hand love function allow easili reacquir sourc data web need verifi dataset equival origin form origin pipelin develop see accomplish exist command pointer greatli appreci",
        "Answer_original_content":"diehl sorri late respons understand us case correctli look like interest import http org doc command refer import import url http org doc command refer import url daavoo worri sound point thank",
        "Answer_preprocessed_content":"sorri late respons understand us case correctli look like interest worri sound point thank",
        "Question_gpt_summary_original":"the user is looking for a way to track data provenance with , including the ability to record the url from which the data was originally sourced and bind that to metadata associated with the data file(s).",
        "Question_gpt_summary":"user look wai track data proven includ abil record url data origin sourc bind metadata associ data file",
        "Answer_gpt_summary_original":"possible solutions to track data provenance with the ability to record the url from which the data was originally sourced and bind that to metadata associated with the data file(s) are using the import and\/or import-url commands.",
        "Answer_gpt_summary":"possibl solut track data proven abil record url data origin sourc bind metadata associ data file import import url command"
    },
    {
        "Question_id":null,
        "Question_title":"I am trying to use google vision api in c++ to read local images",
        "Question_body":"I have tried using c++ to use google vision api to read local files, but I have no experience with google, and I am mainly a c++ developer, so changing languages is not that of an option. Do any one knows how to do it in this language. Furthermore, any written documentation or tutorial in my language(c++) will be extremely helpful.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660317840000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/I-am-trying-to-use-google-vision-api-in-c-to-read-local-images\/td-p\/453466\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"There is a library of Vision API in C++ you can use google-cloud-cpp quickstart."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"try us googl vision api read local imag tri us googl vision api read local file experi googl mainli develop chang languag option know languag furthermor written document tutori languag extrem help",
        "Question_preprocessed_content":"try us googl vision api read local imag tri us googl vision api read local file experi googl mainli develop chang languag option know languag furthermor written document tutori languag extrem help",
        "Answer_original_content":"librari vision api us googl cloud cpp quickstart",
        "Answer_preprocessed_content":"librari vision api us quickstart",
        "Question_gpt_summary_original":"the user is a c++ developer with no experience in google vision api, and is looking for a way to read local images using the api in c++.",
        "Question_gpt_summary":"user develop experi googl vision api look wai read local imag api",
        "Answer_gpt_summary_original":"possible solution: the user can use the google cloud c++ quickstart library for the vision api to read local images in c++.",
        "Answer_gpt_summary":"possibl solut user us googl cloud quickstart librari vision api read local imag"
    },
    {
        "Question_id":70835006.0,
        "Question_title":"How to integrate spark.ml pipeline fitting and hyperparameter optimisation in AWS Sagemaker?",
        "Question_body":"<p>Here is a high-level picture of what I am trying to achieve: I want to train a LightGBM model with spark as a compute <a href=\"https:\/\/microsoft.github.io\/SynapseML\/docs\/features\/lightgbm\/about\/\" rel=\"nofollow noreferrer\">backend<\/a>, all in SageMaker using their Training Job api.\nTo clarify:<\/p>\n<ol>\n<li>I have to use LightGBM in general, there is no option here.<\/li>\n<li>The reason I need to use spark compute backend is because the training with the current dataset does not fit in memory anymore.<\/li>\n<li>I want to use SageMaker Training job setting so I could use SM Hyperparameter optimisation job to find the best hyperparameters for LightGBM. While LightGBM spark interface itself does offer some hyperparameter tuning capabilities, it does not offer Bayesian HP tuning.<\/li>\n<\/ol>\n<p>Now, I know the general approach to running custom training in SM: build a container in a certain way, and then just pull it from ECR and kick-off a training job\/hyperparameter tuning job through <code>sagemaker.Estimator<\/code> <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/estimators.html\" rel=\"nofollow noreferrer\">API<\/a>. Now, in this case SM would handle resource provisioning for you, would create an instance and so on. What I am confused about is that essentially, to use spark compute backend, I would need to have an EMR cluster running, so the SDK would have to handle that as well. However, I do not see how this is possible with the API above.<\/p>\n<p>Now, there is also that thing called <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">Sagemaker Pyspark SDK<\/a>. However, the provided <code>SageMakerEstimator<\/code> API from that package does not support on-the-fly cluster configuration either.<\/p>\n<p>Does anyone know a way how to run a Sagemaker training job that would use an EMR cluster so that later the same job could be used for hyperparameter tuning activities?<\/p>\n<p>One way I see is to run an EMR cluster in the background, and then just create a regular SM estimator job that would connect to the EMR cluster and do the training, essentially running a spark driver program in SM Estimator job.<\/p>\n<p>Has anyone done anything similar in the past?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643032642193,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":196.0,
        "Answer_body":"<p>Thanks for your questions. Here are answers:<\/p>\n<ul>\n<li><p><strong>SageMaker PySpark SDK<\/strong> <a href=\"https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/\" rel=\"nofollow noreferrer\">https:\/\/sagemaker-pyspark.readthedocs.io\/en\/latest\/<\/a> does the opposite of what you want: being able to call a non-spark (or spark) SageMaker job from a Spark environment. Not sure that's what you need here.<\/p>\n<\/li>\n<li><p><strong>Running Spark in SageMaker jobs<\/strong>. While you can use SageMaker Notebooks to connect to a remote EMR cluster for interactive coding, you do not need EMR to run Spark in SageMaker jobs (Training and Processing). You have 2 options:<\/p>\n<ul>\n<li><p><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/amazon_sagemaker_processing.html#pysparkprocessor\" rel=\"nofollow noreferrer\">SageMaker Processing has a built-in Spark Container<\/a>, which is easy to use but unfortunately not connected to SageMaker Model Tuning (that works with Training only). If you use this, you will have to find and use a third-party, external parameter search library ; for example <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/run-distributed-hyperparameter-and-neural-architecture-tuning-jobs-with-syne-tune\/\" rel=\"nofollow noreferrer\">Syne Tune<\/a> from AWS itself (that supports bayesian optimization)<\/p>\n<\/li>\n<li><p>SageMaker Training can run custom docker-based jobs, on one or multiple machines. If you can fit your Spark code within SageMaker Training spec, then you will be able to use SageMaker Model Tuning to tune your Spark code. However there is no framework container for Spark on SageMaker Training, so you would have to build your own, and I am not aware of any examples. Maybe you could get inspiration from the <a href=\"https:\/\/github.com\/aws\/sagemaker-spark-container\" rel=\"nofollow noreferrer\">Processing container code here<\/a> to build a custom Training container<\/p>\n<\/li>\n<\/ul>\n<\/li>\n<\/ul>\n<p>Your idea of using the Training job as a client to launch an EMR cluster is good and should work (if SM has the right permissions), and will indeed allow you to use SM Model Tuning. I'd recommend:<\/p>\n<ul>\n<li>each SM job to create a new transient cluster (auto-terminate after step) to keep costs low and avoid tuning results to be polluted by inter-job contention that could arise if running everything on the same cluster.<\/li>\n<li>use the cheapest possible instance type for the SM estimator, because it will need to stay up during all duration of your EMR experiment to collect and print your final metric (accuracy, duration, cost...)<\/li>\n<\/ul>\n<p>In the same spirit, I once used SageMaker Training myself to launch Batch Transform jobs for the sole purpose of leveraging the bayesian search API to find an inference configuration that minimizes cost.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70835006",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1645793823447,
        "Question_original_content":"integr spark pipelin fit hyperparamet optimis high level pictur try achiev want train lightgbm model spark comput backend train job api clarifi us lightgbm gener option reason need us spark comput backend train current dataset fit memori anymor want us train job set us hyperparamet optimis job best hyperparamet lightgbm lightgbm spark interfac offer hyperparamet tune capabl offer bayesian tune know gener approach run custom train build contain certain wai pull ecr kick train job hyperparamet tune job estim api case handl resourc provis creat instanc confus essenti us spark comput backend need emr cluster run sdk handl possibl api thing call pyspark sdk provid estim api packag support fly cluster configur know wai run train job us emr cluster later job hyperparamet tune activ wai run emr cluster background creat regular estim job connect emr cluster train essenti run spark driver program estim job similar past thank",
        "Question_preprocessed_content":"integr pipelin fit hyperparamet optimis pictur try achiev want train lightgbm model spark comput backend train job api clarifi us lightgbm gener option reason need us spark comput backend train current dataset fit memori anymor want us train job set us hyperparamet optimis job best hyperparamet lightgbm lightgbm spark interfac offer hyperparamet tune capabl offer bayesian tune know gener approach run custom train build contain certain wai pull ecr train tune job api case handl resourc provis creat instanc confus essenti us spark comput backend need emr cluster run sdk handl possibl api thing call pyspark sdk provid api packag support cluster configur know wai run train job us emr cluster later job hyperparamet tune activ wai run emr cluster background creat regular estim job connect emr cluster train essenti run spark driver program estim job similar past thank",
        "Answer_original_content":"thank question answer pyspark sdk http pyspark readthedoc latest opposit want abl non spark spark job spark environ sure need run spark job us notebook connect remot emr cluster interact code need emr run spark job train process option process built spark contain easi us unfortun connect model tune work train us us parti extern paramet search librari exampl syne tune aw support bayesian optim train run custom docker base job multipl machin fit spark code train spec abl us model tune tune spark code framework contain spark train build awar exampl mayb inspir process contain code build custom train contain idea train job client launch emr cluster good work right permiss allow us model tune recommend job creat new transient cluster auto termin step cost low avoid tune result pollut inter job content aris run cluster us cheapest possibl instanc type estim need stai durat emr experi collect print final metric accuraci durat cost spirit train launch batch transform job sole purpos leverag bayesian search api infer configur minim cost",
        "Answer_preprocessed_content":"thank question answer pyspark sdk opposit want abl job spark environ sure need run spark job us notebook connect remot emr cluster interact code need emr run spark job option process spark contain easi us unfortun connect model tune us us extern paramet search librari exampl syne tune aw train run custom job multipl machin fit spark code train spec abl us model tune tune spark code framework contain spark train build awar exampl mayb inspir process contain code build custom train contain idea train job client launch emr cluster good work allow us model tune recommend job creat new transient cluster cost low avoid tune result pollut content aris run cluster us cheapest possibl instanc type estim need stai durat emr experi collect print final metric spirit train launch batch transform job sole purpos leverag bayesian search api infer configur minim cost",
        "Question_gpt_summary_original":"the user is trying to use spark as a compute backend to train a lightgbm model and use sm's hyperparameter optimisation job to find the best hyperparameters, but is unsure of how to integrate spark.ml pipeline fitting and hyperparameter optimisation in order to do so.",
        "Question_gpt_summary":"user try us spark comput backend train lightgbm model us hyperparamet optimis job best hyperparamet unsur integr spark pipelin fit hyperparamet optimis order",
        "Answer_gpt_summary_original":"possible solutions from the answer include using a third-party, external parameter search library such as syne tune from aws, building a custom training container, or using sm job to launch a transient emr cluster. it is recommended to use the cheapest possible instance type for the sm estimator and to create a new transient cluster for each sm job to avoid inter-job contention.",
        "Answer_gpt_summary":"possibl solut answer includ parti extern paramet search librari syne tune aw build custom train contain job launch transient emr cluster recommend us cheapest possibl instanc type estim creat new transient cluster job avoid inter job content"
    },
    {
        "Question_id":71185505.0,
        "Question_title":"XGBClassifer, when de-serialized, gives 'XGBModel' object has no attribute 'enable_categorical'",
        "Question_body":"<p>I have a serialized XGBClassifier object, trained and generated using xgboost=1.5.2.<\/p>\n<pre><code>XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=0.5,\n              colsample_bynode=1, colsample_bytree=0.30140958911801474,\n              eval_metric='logloss', gamma=0.1203484640861413, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.1, max_bin=368, max_delta_step=0, max_depth=6,\n              min_child_weight=1, missing=nan, monotone_constraints='()',\n              n_estimators=100, n_jobs=6, num_parallel_tree=1, random_state=42,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n              single_precision_histogram=True, subsample=0.976171515775659,\n              tree_method='gpu_hist', use_label_encoder=False,\n              validate_parameters=1, verbosity=None)\n<\/code><\/pre>\n<p>I load the object using:<\/p>\n<pre><code>clf_model = joblib.load(model_path)\n<\/code><\/pre>\n<p>I want to use the object to predict on some data I am using Azure environment which also has xgboost=1.5.2. but it gives error:<\/p>\n<pre><code>File &quot;score.py&quot;, line 78, in score_execution\n[stderr]    clf_preds = clf_model.predict(clf_data_transformed)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 1284, in predict\n[stderr]    class_probs = super().predict(\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 879, in predict\n[stderr]    if self._can_use_inplace_predict():\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 811, in _can_use_inplace_predict\n[stderr]    predictor = self.get_params().get(&quot;predictor&quot;, None)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 505, in get_params\n[stderr]    params.update(cp.__class__.get_params(cp, deep))\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/xgboost\/sklearn.py&quot;, line 502, in get_params\n[stderr]    params = super().get_params(deep)\n[stderr]  File &quot;\/opt\/miniconda\/lib\/python3.8\/site-packages\/sklearn\/base.py&quot;, line 210, in get_params\n[stderr]    value = getattr(self, key)\n[stderr]AttributeError: 'XGBModel' object has no attribute 'enable_categorical'\n<\/code><\/pre>\n<p>We have same version in pipelines that produce\/serialize the model and in the pipeline that deserialize the model to predict on new data.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1645277798680,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":358.0,
        "Answer_body":"<p>Here are some possible solutions :<\/p>\n<ul>\n<li>Save the model in some other way, e.g. the JSON specified here <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a><\/li>\n<li>Limit the allowed range of xgboost versions to those that are known to work with our model. This could lead to issues in the future, for example if the aging version of xgboost we require is no longer supported by newer versions of Python.<\/li>\n<li>Using <code>save_model<\/code> to save in JSON is worth a shot to try.<\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71185505",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1645427212643,
        "Question_original_content":"xgbclassif serial give xgbmodel object attribut enabl categor serial xgbclassifi object train gener xgboost xgbclassifi base score booster gbtree colsampl bylevel colsampl bynod colsampl bytre eval metric logloss gamma gpu import type gain interact constraint learn rate max bin max delta step max depth min child weight miss nan monoton constraint estim job num parallel tree random state reg alpha reg lambda scale po weight singl precis histogram true subsampl tree method gpu hist us label encod fals valid paramet verbos load object clf model joblib load model path want us object predict data azur environ xgboost give error file score line score execut stderr clf pred clf model predict clf data transform stderr file opt miniconda lib python site packag xgboost sklearn line predict stderr class prob super predict stderr file opt miniconda lib python site packag xgboost sklearn line predict stderr self us inplac predict stderr file opt miniconda lib python site packag xgboost sklearn line us inplac predict stderr predictor self param predictor stderr file opt miniconda lib python site packag xgboost sklearn line param stderr param updat class param deep stderr file opt miniconda lib python site packag xgboost sklearn line param stderr param super param deep stderr file opt miniconda lib python site packag sklearn base line param stderr valu getattr self kei stderr attributeerror xgbmodel object attribut enabl categor version pipelin produc serial model pipelin deseri model predict new data",
        "Question_preprocessed_content":"xgbclassif give xgbmodel object attribut serial xgbclassifi object train gener load object want us object predict data azur environ give error version pipelin model pipelin deseri model predict new data",
        "Answer_original_content":"possibl solut save model wai json specifi http xgboost readthedoc latest tutori save model html limit allow rang xgboost version known work model lead issu futur exampl ag version xgboost requir longer support newer version python save model save json worth shot try",
        "Answer_preprocessed_content":"possibl solut save model wai json specifi limit allow rang xgboost version known work model lead issu futur exampl ag version xgboost requir longer support newer version python save json worth shot try",
        "Question_gpt_summary_original":"the user encountered a challenge when attempting to de-serialize an xgbclassifier object, trained and generated using xgboost=1.5.2, which resulted in an 'xgbmodel' object having no attribute 'enable_categorical'.",
        "Question_gpt_summary":"user encount challeng attempt serial xgbclassifi object train gener xgboost result xgbmodel object have attribut enabl categor",
        "Answer_gpt_summary_original":"possible solutions to the challenge of de-serializing an xgbclassifier object include saving the model in a different way, such as using the json format specified in the xgboost documentation, or limiting the allowed range of xgboost versions to those known to work with the model. another solution is to try using the save_model function to save the model in json format.",
        "Answer_gpt_summary":"possibl solut challeng serial xgbclassifi object includ save model differ wai json format specifi xgboost document limit allow rang xgboost version known work model solut try save model function save model json format"
    },
    {
        "Question_id":null,
        "Question_title":"Register a dataset as model in azure",
        "Question_body":"How is it possible to register a dataset or blob container directly as a model?\n\nAt the moment I have built a pipeline that outputs my trained models and registers them as datasets. I want to perform inference by deploying the models to for example AKS.\n\nAt the moment it appears the only way to register a model is by first downloading the dataset. Note I cannot register the model based on the runID because my model output is saved directly to blob storage through OutputFileDatasetConfig.\n\nThe motivation here is to avoid having to download and then upload large models.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607946981470,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/197584\/register-a-dataset-as-model-in-azure.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-15T08:22:28.227Z",
                "Answer_score":0,
                "Answer_body":"@ReubenDenleyZotzWilson-3139 You can try to register your model\/file which is in the output directory of the run by using the azure ML cli command? Once you setup the configuration of your Azure account or workspace you can simply use this command with the run id and path of the output to register the model.\n\nExample:\naz ml model register -n sklearn_mnist --asset-path outputs\/sklearn_mnist_model.pkl --experiment-name myexperiment --run-id myrunid --tag area=mnist",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"regist dataset model azur possibl regist dataset blob contain directli model moment built pipelin output train model regist dataset want perform infer deploi model exampl ak moment appear wai regist model download dataset note regist model base runid model output save directli blob storag outputfiledatasetconfig motiv avoid have download upload larg model",
        "Question_preprocessed_content":"regist dataset model azur possibl regist dataset blob contain directli model moment built pipelin output train model regist dataset want perform infer deploi model exampl ak moment appear wai regist model download dataset note regist model base runid model output save directli blob storag outputfiledatasetconfig motiv avoid have download upload larg model",
        "Answer_original_content":"reubendenleyzotzwilson try regist model file output directori run cli command setup configur azur account workspac simpli us command run path output regist model exampl model regist sklearn mnist asset path output sklearn mnist model pkl experi myexperi run myrunid tag area mnist",
        "Answer_preprocessed_content":"try regist output directori run cli command setup configur azur account workspac simpli us command run path output regist model exampl model regist myexperi myrunid area mnist",
        "Question_gpt_summary_original":"the user is looking for a way to register a dataset or blob container directly as a model in azure, without having to download and then upload large models.",
        "Question_gpt_summary":"user look wai regist dataset blob contain directli model azur have download upload larg model",
        "Answer_gpt_summary_original":"the answer suggests using the azure cli command to register a model or file directly from the output directory of a run. the user needs to set up the configuration of their azure account or workspace and use the cli command with the run id and path of the output to register the model. an example command is provided in the answer.",
        "Answer_gpt_summary":"answer suggest azur cli command regist model file directli output directori run user need set configur azur account workspac us cli command run path output regist model exampl command provid answer"
    },
    {
        "Question_id":69071684.0,
        "Question_title":"How to optimize for multiple metrics in Optuna",
        "Question_body":"<p>How do I optimize for multiple metrics simultaneously inside the <code>objective<\/code> function of Optuna. For example, I am training an LGBM classifier and want to find the best hyperparameter set for all common classification metrics like F1, precision, recall, accuracy, AUC, etc.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    # Train\n    gbm = lgb.train(param, dtrain)\n\n    preds = gbm.predict(X_test)\n    pred_labels = np.rint(preds)\n    # Calculate metrics\n    accuracy = sklearn.metrics.accuracy_score(y_test, pred_labels)\n    recall = metrics.recall_score(pred_labels, y_test)\n    precision = metrics.precision_score(pred_labels, y_test)\n    f1 = metrics.f1_score(pred_labels, y_test, pos_label=1)\n\n    ...\n<\/code><\/pre>\n<p>How do I do it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630917852487,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1630917952870,
        "Question_score":2.0,
        "Question_view_count":1887.0,
        "Answer_body":"<p>After defining the grid and fitting the model with these params and generate predictions, calculate all metrics you want to optimize for:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    param_grid = {&quot;n_estimators&quot;: trial.suggest_int(&quot;n_estimators&quot;, 2000, 10000, step=200)}\n    clf = lgbm.LGBMClassifier(objective='binary', **param_grid)\n    clf.fit(X_train, y_train)\n    preds = clf.predict(X_valid)\n    probs = clf.predict_proba(X_valid)\n \n    # Metrics\n    f1 = sklearn.metrics.f1_score(y_valid, press)\n    accuracy = ...\n    precision = ...\n    recall = ...\n    logloss = ...\n<\/code><\/pre>\n<p>and return them in the order you want:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def objective(trial):\n    ...\n\n    return f1, logloss, accuracy, precision, recall\n<\/code><\/pre>\n<p>Then, in the study object, specify whether you want to minimize or maximize each metric to <code>directions<\/code> like so:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>study = optuna.create_study(directions=['maximize', 'minimize', 'maximize', 'maximize', 'maximize'])\n\nstudy.optimize(objective, n_trials=100)\n<\/code><\/pre>\n<p>For more details, see <a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/002_multi_objective.html#sphx-glr-tutorial-20-recipes-002-multi-objective-py\" rel=\"nofollow noreferrer\">Multi-objective Optimization with Optuna<\/a> in the documentation.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1659853446112,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69071684",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1630917852487,
        "Question_original_content":"optim multipl metric optim multipl metric simultan insid object function exampl train lgbm classifi want best hyperparamet set common classif metric like precis recal accuraci auc def object trial train gbm lgb train param dtrain pred gbm predict test pred label rint pred calcul metric accuraci sklearn metric accuraci score test pred label recal metric recal score pred label test precis metric precis score pred label test metric score pred label test po label",
        "Question_preprocessed_content":"optim multipl metric optim multipl metric simultan insid function exampl train lgbm classifi want best hyperparamet set common classif metric like precis recal accuraci auc",
        "Answer_original_content":"defin grid fit model param gener predict calcul metric want optim def object trial param grid estim trial suggest int estim step clf lgbm lgbmclassifi object binari param grid clf fit train train pred clf predict valid prob clf predict proba valid metric sklearn metric score valid press accuraci precis recal logloss return order want def object trial return logloss accuraci precis recal studi object specifi want minim maxim metric direct like studi creat studi direct maxim minim maxim maxim maxim studi optim object trial detail multi object optim document",
        "Answer_preprocessed_content":"defin grid fit model param gener predict calcul metric want optim return order want studi object specifi want minim maxim metric like detail optim document",
        "Question_gpt_summary_original":"The user is trying to optimize for multiple metrics simultaneously inside the objective function of Optuna for a classification problem.",
        "Question_gpt_summary":"user try optim multipl metric simultan insid object function classif problem",
        "Answer_gpt_summary_original":"Possible solutions from the answer are:\n\n- Define a grid and fit the model with the parameters.\n- Generate predictions and calculate all metrics to optimize for.\n- Return the metrics in the order you want.\n- Specify whether to minimize or maximize each metric in the study object.\n\nThe answer provides a code example using Optuna library for multi-objective optimization.",
        "Answer_gpt_summary":"possibl solut answer defin grid fit model paramet gener predict calcul metric optim return metric order want specifi minim maxim metric studi object answer provid code exampl librari multi object optim"
    },
    {
        "Question_id":null,
        "Question_title":"Problem exporting data to azure ml scored dataset to sql database from designer",
        "Question_body":"I am trying to export scored data from azure ML pipeline to azure sql database but its running for 6 hours and still not creating any table. I am not sure how to approach to this problem.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1626948657997,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/485734\/problem-exporting-data-to-azure-ml-scored-dataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-07-22T21:43:58.777Z",
                "Answer_score":0,
                "Answer_body":"Hi, thanks for reaching out. I wasn't able to reproduce this issue. I created an experiment using the sample provided in AML Designer. I was able to export the data to Azure SQL Database successfully. If issue persists, can you share your dataset and steps to reproduce this issue? Thanks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"problem export data score dataset sql databas design try export score data pipelin azur sql databas run hour creat tabl sure approach problem",
        "Question_preprocessed_content":"problem export data score dataset sql databas design try export score data pipelin azur sql databas run hour creat tabl sure approach problem",
        "Answer_original_content":"thank reach wasn abl reproduc issu creat experi sampl provid aml design abl export data azur sql databas successfulli issu persist share dataset step reproduc issu thank",
        "Answer_preprocessed_content":"thank reach wasn abl reproduc issu creat experi sampl provid aml design abl export data azur sql databas successfulli issu persist share dataset step reproduc issu thank",
        "Question_gpt_summary_original":"the user is encountering a challenge in exporting scored data from a pipeline to an azure sql database, which has been running for 6 hours without creating any tables.",
        "Question_gpt_summary":"user encount challeng export score data pipelin azur sql databas run hour creat tabl",
        "Answer_gpt_summary_original":"possible solutions: \n- check if the experiment is using the correct sample provided in aml designer.\n- share the dataset and steps to reproduce the issue with the support team. \n\nsummary: the support team suggests checking the experiment and dataset, and sharing the steps to reproduce the issue if it persists.",
        "Answer_gpt_summary":"possibl solut check experi correct sampl provid aml design share dataset step reproduc issu support team summari support team suggest check experi dataset share step reproduc issu persist"
    },
    {
        "Question_id":null,
        "Question_title":"Comparing different metric\/param\/plot files",
        "Question_body":"<p>Is it possible to perform a diff on 2 distinct metric\/param\/plot files or only the same file can be compared across branch\/commit? The scenario is if instead of having my experiments on different branches I have them on different folders.<br>\nThanks!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1607582857470,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":263.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/comparing-different-metric-param-plot-files\/582",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-10T08:06:30.403Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/quentin\">@quentin<\/a> that a good scenario! We discussed this with <a class=\"mention\" href=\"\/u\/paffciu\">@Paffciu<\/a> briefly but I\u2019m not sure the discussion was reflected in any issues.<\/p>\n<p>Could you please add a comment to this one <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/4446\">https:\/\/github.com\/iterative\/dvc\/issues\/4446<\/a>? I can do this by myself but a comment from a user gives more value to an issue <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":7.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-12-10T10:20:36.358Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/quentin\">@quentin<\/a>!<br>\n<a class=\"mention\" href=\"\/u\/dmitry\">@dmitry<\/a> is right, we have been discussing it, but did not create issue. I did one now:<br>\n<\/p><aside class=\"onebox githubissue\">\n  <header class=\"source\">\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5074\" target=\"_blank\" rel=\"noopener\">github.com\/iterative\/dvc<\/a>\n  <\/header>\n  <article class=\"onebox-body\">\n    <div class=\"github-row\">\n  <div class=\"github-icon-container\" title=\"Issue\">\n\t  <svg width=\"60\" height=\"60\" class=\"github-icon\" viewBox=\"0 0 14 16\" aria-hidden=\"true\"><path d=\"M7 2.3c3.14 0 5.7 2.56 5.7 5.7s-2.56 5.7-5.7 5.7A5.71 5.71 0 0 1 1.3 8c0-3.14 2.56-5.7 5.7-5.7zM7 1C3.14 1 0 4.14 0 8s3.14 7 7 7 7-3.14 7-7-3.14-7-7-7zm1 3H6v5h2V4zm0 6H6v2h2v-2z\"><\/path><\/svg>\n  <\/div>\n\n  <div class=\"github-info-container\">\n    <h4>\n      <a href=\"https:\/\/github.com\/iterative\/dvc\/issues\/5074\" target=\"_blank\" rel=\"noopener\">plots\/metrics\/params: allow comparing between different paths<\/a>\n    <\/h4>\n\n    <div class=\"github-info\">\n      <div class=\"date\">\n        opened <span class=\"discourse-local-date\" data-format=\"ll\" data-date=\"2020-12-10\" data-time=\"10:18:36\" data-timezone=\"UTC\">10:18AM - 10 Dec 20 UTC<\/span>\n      <\/div>\n\n\n      <div class=\"user\">\n        <a href=\"https:\/\/github.com\/pared\" target=\"_blank\" rel=\"noopener\">\n          <img alt=\"pared\" src=\"https:\/\/avatars0.githubusercontent.com\/u\/10195618?v=4\" class=\"onebox-avatar-inline\" width=\"20\" height=\"20\">\n          pared\n        <\/a>\n      <\/div>\n    <\/div>\n  <\/div>\n<\/div>\n\n<div class=\"github-row\">\n  <p class=\"github-content\">In some use cases\nuser might want to compare different metrics in different paths.\nWe need to discuss if and how we are...<\/p>\n<\/div>\n\n<div class=\"labels\">\n    <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">feature request<\/span>\n    <span style=\"display:inline-block;margin-top:2px;background-color: #B8B8B8;padding: 2px;border-radius: 4px;color: #fff;margin-left: 3px;\">ui<\/span>\n<\/div>\n\n  <\/article>\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n\n<p>Could I ask you to chime in and describe your use case?<br>\nFor example, is it possible that you might want to compare <code>metric1.json<\/code> from <code>branch1<\/code> with <code>metric2.json<\/code> from <code>branch2<\/code>? Or is comparison happening only on the same revision?<\/p>",
                "Answer_score":16.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"compar differ metric param plot file possibl perform diff distinct metric param plot file file compar branch commit scenario instead have experi differ branch differ folder thank",
        "Question_preprocessed_content":"compar differ file possibl perform diff distinct file file compar scenario instead have experi differ branch differ folder thank",
        "Answer_original_content":"quentin good scenario discuss paffciu briefli sure discuss reflect issu add comment http github com iter issu comment user give valu issu quentin dmitri right discuss creat issu github com iter plot metric param allow compar differ path open dec utc pare us case user want compar differ metric differ path need discuss featur request ask chime us case exampl possibl want compar metric json branch metric json branch comparison happen revis",
        "Answer_preprocessed_content":"good scenario discuss briefli sure discuss reflect issu add comment comment user give valu issu right discuss creat issu allow compar differ path open dec utc pare us case user want compar differ metric differ path need discuss featur request ask chime us case exampl possibl want compar comparison happen revis",
        "Question_gpt_summary_original":"the user is looking for a way to compare different metric\/param\/plot files across different branches\/commits and folders.",
        "Question_gpt_summary":"user look wai compar differ metric param plot file differ branch commit folder",
        "Answer_gpt_summary_original":"the answer suggests creating an issue on github to allow comparing different metric\/param\/plot files across different paths. the user is encouraged to add a comment to the issue to provide more value. the feature request is to discuss if and how to allow comparing different metrics in different paths. the ui is requested to describe their use case, such as comparing metric files from different branches or only on the same revision.",
        "Answer_gpt_summary":"answer suggest creat issu github allow compar differ metric param plot file differ path user encourag add comment issu provid valu featur request discuss allow compar differ metric differ path request us case compar metric file differ branch revis"
    },
    {
        "Question_id":null,
        "Question_title":"How to determin which GCP VM do I need for ML",
        "Question_body":"Hi to allIm trying to run a procedure looking to reduce the number of features for a model.The first try was with google Colab pro+ but it keep crashing and nver run the entire process, then I got a VM n1-highmem-8 that has: GPUs1 x NVIDIA Tesla V100  +  n1-highmem-8 (vCPUs: 8, RAM: 52GB)and still not getting the process done.The question is how to determin which type of machine should I use? Can I get any metric from the cell that is runing in colab and be able to determin the Type of VM that I need?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658842140000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":73.0,
        "Answer_body":"There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs.\u00a0\u00a0\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-determin-which-GCP-VM-do-I-need-for-ML\/td-p\/447075\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-01T10:31:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs.\u00a0\u00a0\n\nView solution in original post"
            },
            {
                "Answer_creation_time":"2022-08-01T10:31:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs."
            },
            {
                "Answer_creation_time":"2022-08-03T06:39:00",
                "Answer_has_accepted":true,
                "Answer_score":0,
                "Answer_body":"I sincerely appreciate your response, you cannot imagine how important and valuable your help has been"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659349860000,
        "Question_original_content":"determin gcp need allim try run procedur look reduc number featur model try googl colab pro crash nver run entir process got highmem gpu nvidia tesla highmem vcpu ram get process question determin type machin us metric cell rune colab abl determin type need",
        "Question_preprocessed_content":"determin gcp need allim try run procedur look reduc number featur try googl colab pro crash nver run entir process got gpu nvidia tesla get process question determin type machin us metric cell rune colab abl determin type need",
        "Answer_original_content":"thing consider instal necessari driver gpu complet guid follow python wrapper cuda code wai specifi us gpu specif task wrapper cpu instead task keep crash convert code cuda version trivial task involv deeper knowledg gpu work hurri try pycuda github project strongli recommend take look get start blog view solut origin post",
        "Answer_preprocessed_content":"thing consider instal necessari driver gpu complet guid follow python wrapper cuda code wai specifi us gpu specif task wrapper cpu instead task keep crash convert code cuda version trivial task involv deeper knowledg gpu work hurri try cuda github project strongli recommend take look get start blog view solut origin post",
        "Question_gpt_summary_original":"the user is encountering challenges in determining which google cloud platform (gcp) virtual machine (vm) is needed to run a machine learning (ml) procedure to reduce the number of features for a model.",
        "Question_gpt_summary":"user encount challeng determin googl cloud platform gcp virtual machin need run machin learn procedur reduc number featur model",
        "Answer_gpt_summary_original":"the answer does not provide any solutions to the user's challenge of determining which gcp virtual machine is needed to run a machine learning procedure to reduce the number of features for a model. instead, the answer provides guidance on installing necessary drivers for the gpu and using a python wrapper for cuda in the code to specify when to use the gpu for specific tasks. the answer also suggests that converting the code to a cuda version is not a trivial task and recommends taking a look at getting started blogs. additionally, the answer mentions the py2cuda github project as a potential solution for those in a hurry.",
        "Answer_gpt_summary":"answer provid solut user challeng determin gcp virtual machin need run machin learn procedur reduc number featur model instead answer provid guidanc instal necessari driver gpu python wrapper cuda code specifi us gpu specif task answer suggest convert code cuda version trivial task recommend take look get start blog addition answer mention pycuda github project potenti solut hurri"
    },
    {
        "Question_id":56558552.0,
        "Question_title":"How to save your fitted transformer into blob, so your prediction pipeline can use it in AML Service?",
        "Question_body":"<p>I am building a data transformation and training pipeline on Azure Machine Leaning Service. I'd like to save my fitted transformer (e.g. tf-idf) to the blob, so my prediction pipeline can later access it. <\/p>\n\n<pre><code>transformed_data = PipelineData(\"transformed_data\", \n                               datastore = default_datastore,\n                               output_path_on_compute=\"my_project\/tfidf\")\n\nstep_tfidf = PythonScriptStep(name = \"tfidf_step\",\n                              script_name = \"transform.py\",\n                              arguments = ['--input_data', blob_train_data, \n                                           '--output_folder', transformed_data],\n                              inputs = [blob_train_data],\n                              outputs = [transformed_data],\n                              compute_target = aml_compute,\n                              source_directory = project_folder,\n                              runconfig = run_config,\n                              allow_reuse = False)\n\n<\/code><\/pre>\n\n<p>The above code saves the transformer to a current run's folder, which is dynamically generated during each run. <\/p>\n\n<p>I want to save the transformer to a fixed location on blob, so I can access it later, when calling a prediction pipeline.<\/p>\n\n<p>I tried to use an instance of <code>DataReference<\/code> class as <code>PythonScriptStep<\/code> output, but it results in an error: \n<code>ValueError: Unexpected output type: &lt;class 'azureml.data.data_reference.DataReference'&gt;<\/code> <\/p>\n\n<p>It's because <code>PythonScriptStep<\/code> only accepts <code>PipelineData<\/code> or <code>OutputPortBinding<\/code> objects as outputs.<\/p>\n\n<p>How could I save my fitted transformer so it's later accessible by any aribitraly process (e.g. my prediction pipeline)?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1560330676657,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1056.0,
        "Answer_body":"<p>Another solution is to pass <code>DataReference<\/code> as an input to your <code>PythonScriptStep<\/code>. <\/p>\n\n<p>Then inside <code>transform.py<\/code> you're able to read this <code>DataReference<\/code> as a command line argument. <\/p>\n\n<p>You can parse it and use it just as any regular path to save your vectorizer to.<\/p>\n\n<p>E.g. you can:<\/p>\n\n<pre><code>step_tfidf = PythonScriptStep(name = \"tfidf_step\",\n                              script_name = \"transform.py\",\n                              arguments = ['--input_data', blob_train_data, \n                                           '--output_folder', transformed_data,\n                                           '--transformer_path', trained_transformer_path],\n                              inputs = [blob_train_data, trained_transformer_path],\n                              outputs = [transformed_data],\n                              compute_target = aml_compute,\n                              source_directory = project_folder,\n                              runconfig = run_config,\n                              allow_reuse = False)\n<\/code><\/pre>\n\n<p>Then inside your script (<code>transform.py<\/code> in the example above) you can e.g.:<\/p>\n\n<pre><code>import argparse\nimport joblib as jbl\nimport os\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--transformer_path', dest=\"transformer_path\", required=True)\nargs = parser.parse_args()\n\ntfidf = ### HERE CREATE AND TRAIN YOUR VECTORIZER ###\n\nvect_filename = os.path.join(args.transformer_path, 'my_vectorizer.jbl')\n\n<\/code><\/pre>\n\n<hr>\n\n<p>EXTRA: The third way would be to just register the vectorizer as another model in your workspace. You can then use it exactly as any other registered model. (Though this option does not involve explicit writing to blob - as specified in the question above)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1578692352196,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56558552",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1562586040652,
        "Question_original_content":"save fit transform blob predict pipelin us aml servic build data transform train pipelin azur machin lean servic like save fit transform idf blob predict pipelin later access transform data pipelinedata transform data datastor default datastor output path comput project tfidf step tfidf pythonscriptstep tfidf step script transform argument input data blob train data output folder transform data input blob train data output transform data comput target aml comput sourc directori project folder runconfig run config allow reus fals code save transform current run folder dynam gener run want save transform fix locat blob access later call predict pipelin tri us instanc datarefer class pythonscriptstep output result error valueerror unexpect output type pythonscriptstep accept pipelinedata outputportbind object output save fit transform later access aribitrali process predict pipelin",
        "Question_preprocessed_content":"save fit transform blob predict pipelin us aml servic build data transform train pipelin azur machin lean servic like save fit transform blob predict pipelin later access code save transform current run folder dynam gener run want save transform fix locat blob access later call predict pipelin tri us instanc class output result error accept object output save fit transform later access aribitrali process",
        "Answer_original_content":"solut pass datarefer input pythonscriptstep insid transform abl read datarefer command line argument pars us regular path save vector step tfidf pythonscriptstep tfidf step script transform argument input data blob train data output folder transform data transform path train transform path input blob train data train transform path output transform data comput target aml comput sourc directori project folder runconfig run config allow reus fals insid script transform exampl import argpars import joblib jbl import sklearn featur extract text import tfidfvector parser argpars argumentpars parser add argument transform path dest transform path requir true arg parser pars arg tfidf creat train vector vect filenam path join arg transform path vector jbl extra wai regist vector model workspac us exactli regist model option involv explicit write blob specifi question",
        "Answer_preprocessed_content":"solut pass input insid abl read command line argument pars us regular path save vector insid script extra wai regist vector model workspac us exactli regist model",
        "Question_gpt_summary_original":"the user is encountering challenges in saving their fitted transformer into a blob so that their prediction pipeline can access it in  service.",
        "Question_gpt_summary":"user encount challeng save fit transform blob predict pipelin access servic",
        "Answer_gpt_summary_original":"the answer suggests three possible solutions to the challenge of saving a fitted transformer into a blob for use in  service. one solution is to pass datareference as an input to the pythonscriptstep and read it as a command line argument in transform.py. another solution is to register the vectorizer as another model in the workspace. the third solution is to use the tfidfvectorizer and save it as a regular path.",
        "Answer_gpt_summary":"answer suggest possibl solut challeng save fit transform blob us servic solut pass datarefer input pythonscriptstep read command line argument transform solut regist vector model workspac solut us tfidfvector save regular path"
    },
    {
        "Question_id":67993641.0,
        "Question_title":"DevOps pipeline running python script error on import azureml.core",
        "Question_body":"<p>I'm trying to run a Python script in a DevOps pipeline upon check in. A basic 'hellow world' script works, but when I import azureml.core, it errors out with ModuleNotFoundError: No module named 'azureml'.<\/p>\n<p>That makes sense, since I don't know how it was going to find azureml.core. My question is: how do I get the Python script to find the module? Do I need to check it in as part of my code base in DevOps? Or is there some way to reference it via a hyperlink?<\/p>\n<p>Here is my YML file:<\/p>\n<pre><code>trigger:\n- master\n\npool:\n  vmImage: ubuntu-latest\n\nsteps:\n- task: PythonScript@0\n  inputs:\n    scriptSource: 'filepath'\n    scriptPath: test.py\n<\/code><\/pre>\n<p>And here is my python script:<\/p>\n<pre><code>print('hello world')\n\nimport azureml.core\nfrom azureml.core import Workspace\n\n# Load the workspace from the saved config file\nws = Workspace.from_config()\nprint('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1623792015740,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":410.0,
        "Answer_body":"<p>You have to install the lost package into your python, the default python does not have that. Please use the below yml:<\/p>\n<pre><code>trigger:\n- master\n\npool:\n  vmImage: 'ubuntu-latest'\n\nvariables:\n  solution: '**\/*.sln'\n  buildPlatform: 'Any CPU'\n  buildConfiguration: 'Release'\n\nsteps:\n- task: UsePythonVersion@0\n  displayName: 'Use Python 3.8'\n  inputs:\n    versionSpec: 3.8\n\n- script: python3 -m pip install --upgrade pip\n  displayName: 'upgrade pip'\n\n- script: python3 -m pip install azureml.core\n  displayName: 'Install azureml.core'\n\n\n\n- task: PythonScript@0\n  inputs: \n    scriptSource: 'filepath'\n    scriptPath: test.py\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67993641",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1623837729747,
        "Question_original_content":"devop pipelin run python script error import core try run python script devop pipelin check basic hellow world script work import core error modulenotfounderror modul name make sens know go core question python script modul need check code base devop wai refer hyperlink yml file trigger master pool vmimag ubuntu latest step task pythonscript input scriptsourc filepath scriptpath test python script print hello world import core core import workspac load workspac save config file workspac config print readi us work format core version",
        "Question_preprocessed_content":"devop pipelin run python script error import core try run python script devop pipelin check basic hellow world script work import core error modulenotfounderror modul name make sens know go core question python script modul need check code base devop wai refer hyperlink yml file python script",
        "Answer_original_content":"instal lost packag python default python us yml trigger master pool vmimag ubuntu latest variabl solut sln buildplatform cpu buildconfigur releas step task usepythonvers displaynam us python input versionspec script python pip instal upgrad pip displaynam upgrad pip script python pip instal core displaynam instal core task pythonscript input scriptsourc filepath scriptpath test",
        "Answer_preprocessed_content":"instal lost packag python default python us yml",
        "Question_gpt_summary_original":"the user is encountering a challenge with running a python script in a devops pipeline, where the script errors out with a modulenotfounderror when attempting to import .core, and is unsure of how to get the python script to find the module.",
        "Question_gpt_summary":"user encount challeng run python script devop pipelin script error modulenotfounderror attempt import core unsur python script modul",
        "Answer_gpt_summary_original":"the solution to the challenge of encountering a modulenotfounderror when running a python script in a devops pipeline is to install the missing package into the python environment. this can be done by using a yaml file that specifies the necessary steps, including upgrading pip and installing the missing package. the yaml file also includes a task to run the python script.",
        "Answer_gpt_summary":"solut challeng encount modulenotfounderror run python script devop pipelin instal miss packag python environ yaml file specifi necessari step includ upgrad pip instal miss packag yaml file includ task run python script"
    },
    {
        "Question_id":73567221.0,
        "Question_title":"reading hdf5 file from s3 to sagemaker, is the whole file transferred?",
        "Question_body":"<p>I'm reading a file from my S3 bucket in a notebook in sagemaker studio (same account) using the following code:<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nh5_file = h5py.File(s3.open(s3url,'rb'), 'r')\ndata = h5_file.get(dataset_path_in_h5)\n<\/code><\/pre>\n<p>But I don't know what actually append behind the scene, does the whole h5 file is being transferred  ? that's seems unlikely as the code is executed quite fast while the whole file is 20GB. Or is just the dataset in dataset_path_in_h5 is transferred ?\nI suppose that if the whole file is transferred at each call it could cost me a lot.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662025046283,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":18.0,
        "Answer_body":"<p>When you open the file, a file object is created. It has a tiny memory footprint. The dataset values aren't read into memory until you access them.<\/p>\n<p>You are returning <code>data<\/code> as a NumPy array. That loads the entire dataset into memory. (NOTE: the <code>.get()<\/code> method you are using is deprecated. Current syntax is provided in the example.)<\/p>\n<p>As an alternative to returning an array, you can create a dataset object (which also has a small memory foorprint). When you do, the data is read into memory as you need it. Dataset objects behave like NumPy arrays. (Use of a dataset object vs NumPy array depends on downstream usage. Frequently you don't need an array, but sometimes they are required.) Also, if chunked I\/O was enabled when the dataset was created, datasets are read in chunks.<\/p>\n<p>Differences shown below. Note, I used Python's file context manager to open the file. It avoids problems if the file isn't closed properly (you forget or the program exits prematurely).<\/p>\n<pre><code>dataset_path_in_h5=&quot;\/Mode1\/SingleFault\/SimulationCompleted\/IDV2\/Mode1_IDVInfo_2_100\/Run1\/processdata&quot;\ns3 = s3fs.S3FileSystem()\nwith h5py.File(s3.open(s3url,'rb'), 'r') as h5_file:\n     # your way to get a numpy array -- .get() is depreciated:\n     data = h5_file.get(dataset_path_in_h5)\n     # this is the preferred syntax to return an array:\n     data_arr = h5_file[dataset_path_in_h5][()]\n     # this returns a h5py dataset object:\n     data_ds = h5_file[dataset_path_in_h5]  # deleted [()] \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73567221",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662041978820,
        "Question_original_content":"read hdf file file transfer read file bucket notebook studio account follow code dataset path mode singlefault simulationcomplet idv mode idvinfo run processdata sf sfilesystem file hpy file open surl data file dataset path know actual append scene file transfer unlik code execut fast file dataset dataset path transfer suppos file transfer cost lot",
        "Question_preprocessed_content":"read hdf file file transfer read file bucket notebook studio follow code know actual append scene file transfer unlik code execut fast file dataset transfer suppos file transfer cost lot",
        "Answer_original_content":"open file file object creat tini memori footprint dataset valu aren read memori access return data numpi arrai load entir dataset memori note method deprec current syntax provid exampl altern return arrai creat dataset object small memori foorprint data read memori need dataset object behav like numpi arrai us dataset object numpi arrai depend downstream usag frequent need arrai requir chunk enabl dataset creat dataset read chunk differ shown note python file context manag open file avoid problem file isn close properli forget program exit prematur dataset path mode singlefault simulationcomplet idv mode idvinfo run processdata sf sfilesystem hpy file open surl file wai numpi arrai depreci data file dataset path prefer syntax return arrai data arr file dataset path return hpy dataset object data file dataset path delet",
        "Answer_preprocessed_content":"open file file object creat tini memori footprint dataset valu aren read memori access return numpi arrai load entir dataset memori altern return arrai creat dataset object data read memori need dataset object behav like numpi arrai chunk enabl dataset creat dataset read chunk differ shown note python file context manag open file avoid problem file isn close properli",
        "Question_gpt_summary_original":"the user is trying to understand if the whole hdf5 file is being transferred when they use the code to read the file from their s3 bucket in a notebook in studio.",
        "Question_gpt_summary":"user try understand hdf file transfer us code read file bucket notebook studio",
        "Answer_gpt_summary_original":"possible solutions to avoid loading the entire hdf5 file into memory when reading it from an s3 bucket in a notebook in studio are: creating a dataset object instead of returning a numpy array, using the current syntax to return an array, and enabling chunked i\/o if the dataset was created with it. additionally, using python's file context manager to open the file can avoid problems if the file isn't closed properly.",
        "Answer_gpt_summary":"possibl solut avoid load entir hdf file memori read bucket notebook studio creat dataset object instead return numpi arrai current syntax return arrai enabl chunk dataset creat addition python file context manag open file avoid problem file isn close properli"
    },
    {
        "Question_id":null,
        "Question_title":"How to verify that checkpoints work for SageMaker Spot Training?",
        "Question_body":"Hi,\n\nHow can we know that checkpoint works before launching a sagemaker spot training job? Is there a way to force a regular checkpoint to s3 instead of waiting for the SIGTERM?\n\ncheers",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1584346040000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":51.0,
        "Answer_body":"Hi olivier, If you enable Sagemaker checkpointing , it periodically saves a copy of the artifacts into S3. I have used this in pytorch and it works by checkpointing periodically and the blog on Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs also mentions the same\n\nTo avoid restarting a training job from scratch should it be interrupted, we strongly recommend that you implement checkpointing, a technique that saves the model in training at periodic intervals",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUbvA_lGXgQ3CdPuEoImWQVw\/how-to-verify-that-checkpoints-work-for-sage-maker-spot-training",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-03-16T08:20:40.000Z",
                "Answer_score":0,
                "Answer_body":"Hi olivier, If you enable Sagemaker checkpointing , it periodically saves a copy of the artifacts into S3. I have used this in pytorch and it works by checkpointing periodically and the blog on Managed Spot Training: Save Up to 90% On Your Amazon SageMaker Training Jobs also mentions the same\n\nTo avoid restarting a training job from scratch should it be interrupted, we strongly recommend that you implement checkpointing, a technique that saves the model in training at periodic intervals",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1584346840000,
        "Question_original_content":"verifi checkpoint work spot train know checkpoint work launch spot train job wai forc regular checkpoint instead wait sigterm cheer",
        "Question_preprocessed_content":"verifi checkpoint work spot train know checkpoint work launch spot train job wai forc regular checkpoint instead wait sigterm cheer",
        "Answer_original_content":"olivi enabl checkpoint period save copi artifact pytorch work checkpoint period blog manag spot train save train job mention avoid restart train job scratch interrupt strongli recommend implement checkpoint techniqu save model train period interv",
        "Answer_preprocessed_content":"olivi enabl checkpoint period save copi artifact pytorch work checkpoint period blog manag spot train save train job mention avoid restart train job scratch interrupt strongli recommend implement checkpoint techniqu save model train period interv",
        "Question_gpt_summary_original":"the user is looking for a way to verify that checkpoints work for spot training and to force a regular checkpoint to s3 instead of waiting for a sigterm.",
        "Question_gpt_summary":"user look wai verifi checkpoint work spot train forc regular checkpoint instead wait sigterm",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- enable checkpointing to periodically save a copy of the artifacts into s3.\n- implement checkpointing to save the model in training at periodic intervals and avoid restarting a training job from scratch if it is interrupted. \n\nin summary, to verify that checkpoints work for spot training and to force a regular checkpoint to s3, the user can enable and implement checkpointing.",
        "Answer_gpt_summary":"possibl solut answer enabl checkpoint period save copi artifact implement checkpoint save model train period interv avoid restart train job scratch interrupt summari verifi checkpoint work spot train forc regular checkpoint user enabl implement checkpoint"
    },
    {
        "Question_id":67498965.0,
        "Question_title":"Pre-existing Compute Resource necessary for running a scheduled Azure ML pipeline?",
        "Question_body":"<p>I have been exploring Azure ML Pipeline. I am referring to <a href=\"https:\/\/github.com\/MicrosoftLearning\/DP100\/blob\/master\/06A%20-%20Creating%20a%20Pipeline.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a> for the below code:<\/p>\n<p>Here is a small snippet from a MS Repo:<\/p>\n<pre><code>train_step = PythonScriptStep(name = &quot;Prepare Data&quot;,\nsource_directory = experiment_folder,\nscript_name = &quot;prep_diabetes.py&quot;,\narguments = ['--input-data', diabetes_ds.as_named_input('raw_data'),\n'--prepped-data', prepped_data_folder],\noutputs=[prepped_data_folder],\ncompute_target = pipeline_cluster,\nrunconfig = pipeline_run_config,\nallow_reuse = True)\n<\/code><\/pre>\n<p>This suggests that while defining a pipeline, we must provide it a compute resource(pipeline_cluster). This obviously makes sense, since specific compute might be required for a specific step.<\/p>\n<p>But do we need to have this compute resource up and running always, so that whenever a pipeline is triggered, it can find the compute resource?<\/p>\n<p>Also, i figured we can probably keep a cluster with Zero minimum nodes, in which cases cluster is resized whenever pipeline is triggered. But i think there is a minimal cost incurrent in probably container registry regularly in such a setup. Is this the recommended way to deploy ML pipelines or some more efficient approach is possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1620804113940,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":29.0,
        "Answer_body":"<p>Yep you're right -- create a <code>ComputeTarget<\/code> with a minimum of zero nodes. The <a href=\"https:\/\/azure.microsoft.com\/en-us\/pricing\/details\/container-registry\/#pricing\" rel=\"nofollow noreferrer\">container registry costs<\/a> are ~$0.16 USD\/day and, IIRC, that cost is bundled in with Azure Machine learning.<\/p>\n<p>This is what our team does for our published pipelines in production.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67498965",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1620823042460,
        "Question_original_content":"pre exist comput resourc necessari run schedul pipelin explor pipelin refer notebook code small snippet repo train step pythonscriptstep prepar data sourc directori experi folder script prep diabet argument input data diabet name input raw data prep data prep data folder output prep data folder comput target pipelin cluster runconfig pipelin run config allow reus true suggest defin pipelin provid comput resourc pipelin cluster obvious make sens specif comput requir specif step need comput resourc run pipelin trigger comput resourc figur probabl cluster zero minimum node case cluster resiz pipelin trigger think minim cost incurr probabl contain registri regularli setup recommend wai deploi pipelin effici approach possibl",
        "Question_preprocessed_content":"comput resourc necessari run schedul pipelin explor pipelin refer notebook code small snippet repo suggest defin pipelin provid comput obvious make sens specif comput requir specif step need comput resourc run pipelin trigger comput resourc figur probabl cluster zero minimum node case cluster resiz pipelin trigger think minim cost incurr probabl contain registri regularli setup recommend wai deploi pipelin effici approach possibl",
        "Answer_original_content":"yep right creat computetarget minimum zero node contain registri cost usd dai iirc cost bundl team publish pipelin product",
        "Answer_preprocessed_content":"yep right creat minimum zero node contain registri cost iirc cost bundl team publish pipelin product",
        "Question_gpt_summary_original":"the user is encountering challenges with understanding the necessity of a pre-existing compute resource for running a scheduled pipeline, and is looking for an efficient approach to deploy ml pipelines.",
        "Question_gpt_summary":"user encount challeng understand necess pre exist comput resourc run schedul pipelin look effici approach deploi pipelin",
        "Answer_gpt_summary_original":"solution: the answer suggests creating a compute target with a minimum of zero nodes to address the challenge of understanding the necessity of a pre-existing compute resource for running a scheduled pipeline. additionally, the answer mentions that the container registry costs are approximately $0.16 usd\/day and are bundled in with the solution. this is what the team does for their published pipelines in production.",
        "Answer_gpt_summary":"solut answer suggest creat comput target minimum zero node address challeng understand necess pre exist comput resourc run schedul pipelin addition answer mention contain registri cost approxim usd dai bundl solut team publish pipelin product"
    },
    {
        "Question_id":60072981.0,
        "Question_title":"How to open a model tarfile stored in S3 bucket in sagemaker notebook?",
        "Question_body":"<p>I know that loading a .csv file into sagemaker notebook from S3 bucket is pretty straightforward but I want to load a model.tar.gz file stored in S3 bucket. I tried to do the following<\/p>\n\n<pre><code>import botocore \nimport sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.predictor import csv_serializer\nimport boto3\n\nsm_client = boto3.client(service_name='sagemaker')\nruntime_sm_client = boto3.client(service_name='sagemaker-runtime')\n\ns3 = boto3.resource('s3')\ns3_client = boto3.client('s3')\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nACCOUNT_ID  = boto3.client('sts').get_caller_identity()['Account']\nREGION      = boto3.Session().region_name\nBUCKET      = 'sagemaker.prismade.net'\ndata_key    = 'DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\nloc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\nprint(loc)\nwith tarfile.open(loc) as tar:\n    tar.extractall(path='.')\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>--------------------------------------------------------------------------\nFileNotFoundError                         Traceback (most recent call last)\n&lt;ipython-input-215-bfdddac71b95&gt; in &lt;module&gt;()\n     20 loc = 's3:\/\/{}\/{}'.format(BUCKET, data_key)\n     21 print(loc)\n---&gt; 22 with tarfile.open(loc) as tar:\n     23     tar.extractall(path='.')\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in open(cls, name, mode, fileobj, bufsize, **kwargs)\n   1567                     saved_pos = fileobj.tell()\n   1568                 try:\n-&gt; 1569                     return func(name, \"r\", fileobj, **kwargs)\n   1570                 except (ReadError, CompressionError):\n   1571                     if fileobj is not None:\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/tarfile.py in gzopen(cls, name, mode, fileobj, compresslevel, **kwargs)\n   1632 \n   1633         try:\n-&gt; 1634             fileobj = gzip.GzipFile(name, mode + \"b\", compresslevel, fileobj)\n   1635         except OSError:\n   1636             if fileobj is not None and mode == 'r':\n\n~\/anaconda3\/envs\/python3\/lib\/python3.6\/gzip.py in __init__(self, filename, mode, compresslevel, fileobj, mtime)\n    161             mode += 'b'\n    162         if fileobj is None:\n--&gt; 163             fileobj = self.myfileobj = builtins.open(filename, mode or 'rb')\n    164         if filename is None:\n    165             filename = getattr(fileobj, 'name', '')\n\nFileNotFoundError: [Errno 2] No such file or directory: 's3:\/\/sagemaker.prismade.net\/DEMO_MME_ANN\/multi_model_artifacts\/axel.tar.gz'\n<\/code><\/pre>\n\n<p>What is the mistake here and how can I accomplish this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1580895955387,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":2984.0,
        "Answer_body":"<p>Not every python library that is designed to work with a file system (tarfile.open, in this example) knows how to read an object from S3 as a file. <\/p>\n\n<p>The simple way to solve it is to first copy the object into the local file system as a file.<\/p>\n\n<pre><code>import boto3\n\ns3 = boto3.client('s3')\ns3.download_file('BUCKET_NAME', 'OBJECT_NAME', 'FILE_NAME')\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60072981",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1581004771980,
        "Question_original_content":"open model tarfil store bucket notebook know load csv file notebook bucket pretti straightforward want load model tar file store bucket tri follow import botocor import import execut role predictor import csv serial import boto client boto client servic runtim client boto client servic runtim boto resourc client boto client session session role execut role account boto client st caller ident account region boto session region bucket prismad net data kei demo mme ann multi model artifact axel tar loc format bucket data kei print loc tarfil open loc tar tar extractal path follow error filenotfounderror traceback recent loc format bucket data kei print loc tarfil open loc tar tar extractal path anaconda env python lib python tarfil open cl mode fileobj bufsiz kwarg save po fileobj tell try return func fileobj kwarg readerror compressionerror fileobj anaconda env python lib python tarfil gzopen cl mode fileobj compresslevel kwarg try fileobj gzip gzipfil mode compresslevel fileobj oserror fileobj mode anaconda env python lib python gzip init self filenam mode compresslevel fileobj mtime mode fileobj fileobj self myfileobj builtin open filenam mode filenam filenam getattr fileobj filenotfounderror errno file directori prismad net demo mme ann multi model artifact axel tar mistak accomplish",
        "Question_preprocessed_content":"open model tarfil store bucket notebook know load csv file notebook bucket pretti straightforward want load file store bucket tri follow follow error mistak accomplish",
        "Answer_original_content":"python librari design work file tarfil open exampl know read object file simpl wai solv copi object local file file import boto boto client download file bucket object file",
        "Answer_preprocessed_content":"python librari design work file know read object file simpl wai solv copi object local file file",
        "Question_gpt_summary_original":"the user is encountering challenges in opening a model tarfile stored in an s3 bucket in a notebook.",
        "Question_gpt_summary":"user encount challeng open model tarfil store bucket notebook",
        "Answer_gpt_summary_original":"the solution to the challenge of opening a model tarfile stored in an s3 bucket in a notebook is to first copy the object into the local file system as a file using the boto3 library. this is because not every python library that is designed to work with a file system knows how to read an object from s3 as a file.",
        "Answer_gpt_summary":"solut challeng open model tarfil store bucket notebook copi object local file file boto librari python librari design work file know read object file"
    },
    {
        "Question_id":null,
        "Question_title":"OSError: [Errno 28] No space left on device -- PyTorch, CNN, estimator",
        "Question_body":"hey there, I'm training a convolutional neural network (CNN) on a large dataset (10k images - 50 GB) stored on S3 bucket using estimator(sagemaker infrastructure) . everything works well when I work with 2000 images which has the total size of almost 5-10 GB. however, I get an error when I increase number of images to 3000 or more. the error indicates that there is no space left on device. I also attached the estimator setup to this message, as you can see I am using ml.g4dn.12xlarge instance which has 192 GB of memory!! I also increased the volume size to 900 GB. I still don't know why I am getting space\/storage error!! I know that error is related the function \"_get_train_data_loader\" in which it is trying to download the images!! I was reading somewhere that EFS (elastic file system) might help with this issue, if so, I don't know how to specify it in the estimator. estimator = PyTorch( entry_point=\"pbdl_sm.py\", role=role, framework_version=\"1.4.0\", py_version=\"py3\", instance_count=1, instance_type=\"ml.g4dn.12xlarge\", volume_size = 900, hyperparameters={\"epochs\": 6, \"backend\": \"gloo\",\"lr\": 0.001,\"train_size\":2900,\"n_realz\":3000}, )",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1657555011984,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":404.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuRZhu6ZpTPSmVO-4P2GDmQ\/os-error-errno-28-no-space-left-on-device-py-torch-cnn-estimator",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-13T19:49:21.485Z",
                "Answer_score":0,
                "Answer_body":"Hi\n\nThank you for reaching out to us.\n\nIn general the no space left on device error occurs when there is a high disk utilization, Requesting you to review the instance metrics and cloudwatch metrics\/logs of the training job for more detailed information.\n\nIt also depends on the various factors like learning rate, number epochs and configuration of the training job and the estimator, However I would recommend you to try distributed training with the pytorch using the smdistributed [1] on multiple instances , Currently, the following are supported: distributed training with parameter servers, SageMaker Distributed (SMD) Data and Model Parallelism, and MPI. SMD Model Parallelism can only be used with MPI.\n\nTo enable the SageMaker distributed data parallelism: { \"smdistributed\": { \"dataparallel\": { \"enabled\": True } } }\n\nI would also recommend to try the PIPE mode, With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first. This means that your training jobs start sooner, finish quicker, and need less disk space.\n\nIf you are facing any issues and require further investigation on the issue, I would encourage you to open a case with the premium support along with the training job ARN and the cloudwatch logs of the job.Due to security reason, this post is not suitable for sharing customer's resource.\n\nReference:\n\n[1] https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/sagemaker.pytorch.html\n\n[2] https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\n\n[3] https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/80df7d61a4bf14a11f0442020e2003a7c1f78115\/advanced_functionality\/pipe_bring_your_own\/train.py\n\n[4] https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/training\/distributed_training\/pytorch\/data_parallel\/maskrcnn\/pytorch_smdataparallel_maskrcnn_demo.html",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"oserror errno space left devic pytorch cnn estim hei train convolut neural network cnn larg dataset imag store bucket estim infrastructur work work imag total size error increas number imag error indic space left devic attach estim setup messag gdn xlarg instanc memori increas volum size know get space storag error know error relat function train data loader try download imag read ef elast file help issu know specifi estim estim pytorch entri point pbdl role role framework version version instanc count instanc type gdn xlarg volum size hyperparamet epoch backend gloo train size realz",
        "Question_preprocessed_content":"oserror space left devic pytorch cnn estim hei train convolut neural network larg dataset store bucket estim work work imag total size error increas number imag error indic space left devic attach estim setup messag instanc memori increas volum size know get error know error relat function try download imag read ef help issu know specifi estim estim pytorch",
        "Answer_original_content":"thank reach gener space left devic error occur high disk util request review instanc metric cloudwatch metric log train job detail inform depend factor like learn rate number epoch configur train job estim recommend try distribut train pytorch smdistribut multipl instanc current follow support distribut train paramet server distribut smd data model parallel mpi smd model parallel mpi enabl distribut data parallel smdistribut dataparallel enabl true recommend try pipe mode pipe input mode dataset stream directli train instanc instead download mean train job start sooner finish quicker need disk space face issu requir investig issu encourag open case premium support train job arn cloudwatch log job secur reason post suitabl share custom resourc refer http readthedoc stabl framework pytorch pytorch html http aw amazon com blog machin learn pipe input mode amazon algorithm http github com aw amazon exampl blob dfdabfafeacf advanc function pipe bring train http exampl readthedoc latest train distribut train pytorch data parallel maskrcnn pytorch smdataparallel maskrcnn demo html",
        "Answer_preprocessed_content":"thank reach gener space left devic error occur high disk util request review instanc metric cloudwatch train job detail inform depend factor like learn rate number epoch configur train job estim recommend try distribut train pytorch smdistribut multipl instanc current follow support distribut train paramet server distribut data model parallel mpi smd model parallel mpi enabl distribut data parallel recommend try pipe mode pipe input mode dataset stream directli train instanc instead download mean train job start sooner finish quicker need disk space face issu requir investig issu encourag open case premium support train job arn cloudwatch log secur reason post suitabl share custom resourc refer",
        "Question_gpt_summary_original":"the user is encountering an oserror: [errno 28] no space left on device when attempting to train a convolutional neural network (cnn) on a large dataset (10k images - 50 gb) stored on an s3 bucket using an estimator infrastructure.",
        "Question_gpt_summary":"user encount oserror errno space left devic attempt train convolut neural network cnn larg dataset imag store bucket estim infrastructur",
        "Answer_gpt_summary_original":"possible solutions to the oserror: [errno 28] no space left on device error when training a cnn on a large dataset stored on an s3 bucket using an estimator infrastructure include reviewing instance metrics and cloudwatch metrics\/logs of the training job for more detailed information, trying distributed training with pytorch using the smdistributed on multiple instances, enabling distributed data parallelism, and using pipe input mode to stream the dataset directly to the training instances. if further investigation is required, opening a case with premium support along with the training job arn and the cloudwatch logs of the job is recommended.",
        "Answer_gpt_summary":"possibl solut oserror errno space left devic error train cnn larg dataset store bucket estim infrastructur includ review instanc metric cloudwatch metric log train job detail inform try distribut train pytorch smdistribut multipl instanc enabl distribut data parallel pipe input mode stream dataset directli train instanc investig requir open case premium support train job arn cloudwatch log job recommend"
    },
    {
        "Question_id":null,
        "Question_title":"Can Google Cloud Vision work as fast as Google lens for OCR?",
        "Question_body":"Hello, I am using Google Cloud Vision for text recognition, but the processing speed is quite slow (5 to 15 seconds). I would like to know how does Google lens work so fast and if there's any way to make Google Vision as fast.Edit: My photos that go through Vision are stored in Firebase Storage. (As I've read in some posts this is the quickest way to process them). Thanks in advance!",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1640779500000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":474.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Can-Google-Cloud-Vision-work-as-fast-as-Google-lens-for-OCR\/td-p\/181569\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-12-30T11:55:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"=> For uploading\u00a0 images to Google Cloud\u00a0 Vision you can use one of the following options:\n\n1) Directly upload the image as binary (is the slowest)\n\n2) Upload the image as base64 encoded (is ~25% faster)\n\n3) Use a pre-uploaded image stored on Google Cloud Storage [1] (is the fastest)\n\n- Please refer to the documentation [2] for best practices for Cloud Vision API\n\n- Also, the response time of Vision API depends on the resource status or network latency.\u00a0 (network IO, file transfer to Vision API, etc).\u00a0\n\n[1] https:\/\/cloud.google.com\/storage\/\n\n\u00a0[2] https:\/\/cloud.google.com\/vision\/docs\/supported-files"
            },
            {
                "Answer_creation_time":"2022-01-03T02:21:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Google Cloud Vision API work? Google image recognition API\u00a0will identify images from pre-trained models on large datasets of images and then it classifies the images into thousands of categories to detect the objects, places, people and faces in the images.\n\n\u00a0\n\nMyAccountAccess"
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"googl cloud vision work fast googl len ocr hello googl cloud vision text recognit process speed slow second like know googl len work fast wai googl vision fast edit photo vision store firebas storag read post quickest wai process thank advanc",
        "Question_preprocessed_content":"googl cloud vision work fast googl len ocr hello googl cloud vision text recognit process speed slow like know googl len work fast wai googl vision photo vision store firebas storag thank advanc",
        "Answer_original_content":"upload imag googl cloud vision us follow option directli upload imag binari slowest upload imag base encod faster us pre upload imag store googl cloud storag fastest refer document best practic cloud vision api respons time vision api depend resourc statu network latenc network file transfer vision api http cloud googl com storag http cloud googl com vision doc support file googl cloud vision api work googl imag recognit apiwil identifi imag pre train model larg dataset imag classifi imag thousand categori detect object place peopl face imag myaccountaccess",
        "Answer_preprocessed_content":"upload imag googl cloud vision us follow option directli upload imag binari upload imag base encod us imag store googl cloud storag refer document best practic cloud vision api respons time vision api depend resourc statu network latenc googl cloud vision api work googl imag recognit apiwil identifi imag model larg dataset imag classifi imag thousand categori detect object place peopl face imag myaccountaccess",
        "Question_gpt_summary_original":"the user is trying to use google cloud vision for text recognition, but is experiencing slow processing speeds and is wondering if there is a way to make google vision as fast as google lens.",
        "Question_gpt_summary":"user try us googl cloud vision text recognit experienc slow process speed wonder wai googl vision fast googl len",
        "Answer_gpt_summary_original":"possible solutions to improve the processing speed of google cloud vision for text recognition are: directly uploading the image as binary, uploading the image as base64 encoded, or using a pre-uploaded image stored on google cloud storage. the response time of the vision api depends on the resource status or network latency. it is recommended to refer to the documentation for best practices for the cloud vision api. the api identifies images from pre-trained models on large datasets of images and classifies them into thousands of categories to detect objects, places, people, and faces in the images.",
        "Answer_gpt_summary":"possibl solut improv process speed googl cloud vision text recognit directli upload imag binari upload imag base encod pre upload imag store googl cloud storag respons time vision api depend resourc statu network latenc recommend refer document best practic cloud vision api api identifi imag pre train model larg dataset imag classifi thousand categori detect object place peopl face imag"
    },
    {
        "Question_id":null,
        "Question_title":"Real-time endpoint response is empty",
        "Question_body":"Hi, I've created and deployed NLP pipeline as a real-time endpoint, but the response that I get is an empty result ( {\"Results\":{}} ) with Status 200 OK.\n\n\n\n\nI have the Owner role, so i think that the permission is not a problem. I've also tried to deploy the preset experiment \"Sample 1: Regression - Automobile Price Prediction (Basic)\" using the same resources and it returned the results without a problem.\n\nIn the training pipeline, I've used the train and validation datasets. After creating the real-time inference pipeline, it automatically created two Web Service inputs the one linked to the train dataset input is irrelevant in the real-time inference pipeline as it's used only to create the Vocabulary for the Extract N-Gram Features from Text module linked to the validation dataset.\nAfter that, I've deleted the Web Service input connected to the training dataset and tried to deploy the real-time endpoint.\n\n\n\n\n\n\nAfter submitting the training pipeline, I've saved the Result Vocabulary from the Extract N-Gram Features from Text module that is connected to the training dataset and used it as an input vocabulary for the N-Gram module (ReadOnly vocabulary mode) in the real-time inference pipeline and deployed that as a real-time endpoint.\n\n\n\n\nIn both cases above, I didn't get any errors, scoring and evaluation of a model are correct but the response from the endpoint is empty.\nIt's worth mentioning, that if I leave the real-time pipeline as is (with two inputs) it doesn't work because of the parameter settings in the Extract N-Gram Features from Text module where training dataset is connected.",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1592313268633,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/36610\/real-time-endpoint-response-is-empty.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-01T07:58:21.297Z",
                "Answer_score":0,
                "Answer_body":"@AntonioAli-4907 Is it possible to share details of your experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue? This option is available from the top right hand corner of the portal by clicking the smiley face, Please select the option Microsoft can email you about the feedback along with a screen shot so our service team can lookup and advise through email.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"real time endpoint respons creat deploi nlp pipelin real time endpoint respons result result statu owner role think permiss problem tri deploi preset experi sampl regress automobil price predict basic resourc return result problem train pipelin train valid dataset creat real time infer pipelin automat creat web servic input link train dataset input irrelev real time infer pipelin creat vocabulari extract gram featur text modul link valid dataset delet web servic input connect train dataset tri deploi real time endpoint submit train pipelin save result vocabulari extract gram featur text modul connect train dataset input vocabulari gram modul readonli vocabulari mode real time infer pipelin deploi real time endpoint case error score evalu model correct respons endpoint worth mention leav real time pipelin input work paramet set extract gram featur text modul train dataset connect",
        "Question_preprocessed_content":"endpoint respons creat deploi nlp pipelin endpoint respons result statu owner role think permiss problem tri deploi preset experi sampl regress automobil price predict resourc return result problem train pipelin train valid dataset creat infer pipelin automat creat web servic input link train dataset input irrelev infer pipelin creat vocabulari extract featur text modul link valid dataset delet web servic input connect train dataset tri deploi endpoint submit train pipelin save result vocabulari extract featur text modul connect train dataset input vocabulari modul infer pipelin deploi endpoint case error score evalu model correct respons endpoint worth mention leav pipelin work paramet set extract featur text modul train dataset connect",
        "Answer_original_content":"antonioali possibl share detail experi issu azur com portal servic engin lookup issu option avail right hand corner portal click smilei face select option microsoft email feedback screen shot servic team lookup advis email",
        "Answer_preprocessed_content":"possibl share detail experi issu portal servic engin lookup issu option avail right hand corner portal click smilei face select option microsoft email feedback screen shot servic team lookup advis email",
        "Question_gpt_summary_original":"the user encountered challenges with deploying a real-time endpoint, resulting in an empty response despite the scoring and evaluation of the model being correct.",
        "Question_gpt_summary":"user encount challeng deploi real time endpoint result respons despit score evalu model correct",
        "Answer_gpt_summary_original":"the possible solution to the challenge of an empty response from a real-time endpoint is to share the details of the experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue. this can be done by clicking the smiley face on the top right-hand corner of the portal and selecting the option for microsoft to email the user about the feedback along with a screenshot. the service team can then advise through email.",
        "Answer_gpt_summary":"possibl solut challeng respons real time endpoint share detail experi issu azur com portal servic engin lookup issu click smilei face right hand corner portal select option microsoft email user feedback screenshot servic team advis email"
    },
    {
        "Question_id":null,
        "Question_title":"ConnectionResetError appearing while i am training due to wandb run",
        "Question_body":"<p>Hi all,<br>\nI am facing a problem while I am training my model.<br>\nthe training crashed due to wandb run.<br>\ncould anyone explain why this error happened and how can I avoid it.<br>\n<img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/0\/0169513adb7250af5ca174ac97243b8fdc0e5c6a.jpeg\" alt=\"Capture.PNG\" data-base62-sha1=\"cu7muubyTNVdJjnHUpHxiUChdM\" width=\"690\" height=\"360\"><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1664662498353,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":121.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/connectionreseterror-appearing-while-i-am-training-due-to-wandb-run\/3210",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-01T22:18:53.675Z",
                "Answer_body":"<pre><code class=\"lang-auto\">ConnectionreSerror: [Winerror 10054] An existing connection had to be closed by the remote host\nException in Thread Netstatatthr:\nTradeback (Most recent Call Last):\n<\/code><\/pre>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-03T23:12:09.544Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/asekhri\">@asekhri<\/a> , happy to help. We\u2019ve seen this issue and it could be directly attributed to proxy configuration when users attempt to log to wandb from inside a fire-walled corporate network or they might be connected to a VPN. Are any of those cases applicable to you?<\/p>\n<p>Please provide the <code>debug.log<\/code> and <code>debug-internal.log<\/code> files for the crashed run, as the files may provide additional clues. Additionally, are you using wandb cloud or local? and which version are you using. Thanks<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-06T22:54:39.810Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/asekhri\">@asekhri<\/a> since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-05T22:54:52.631Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"connectionreseterror appear train run face problem train model train crash run explain error happen avoid",
        "Question_preprocessed_content":"connectionreseterror appear train run face problem train model train crash run explain error happen avoid",
        "Answer_original_content":"connectionreserror winerror exist connect close remot host except thread netstatatthr tradeback recent asekhri happi help weve seen issu directli attribut proxi configur user attempt log insid wall corpor network connect vpn case applic provid debug log debug intern log file crash run file provid addit clue addition cloud local version thank asekhri heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"happi help weve seen issu directli attribut proxi configur user attempt log insid corpor network connect vpn case applic provid file crash run file provid addit clue addition cloud local version thank heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering a \"connectionreseterror\" while training their model, and is seeking an explanation for why this error occurred and how to avoid it in the future.",
        "Question_gpt_summary":"user encount connectionreseterror train model seek explan error occur avoid futur",
        "Answer_gpt_summary_original":"possible solutions to avoid the \"connectionreseterror\" while training a model include checking the proxy configuration, ensuring that the user is not connected to a vpn, and providing the debug.log and debug-internal.log files for the crashed run. additionally, the user should specify whether they are using cloud or local and which version they are using.",
        "Answer_gpt_summary":"possibl solut avoid connectionreseterror train model includ check proxi configur ensur user connect vpn provid debug log debug intern log file crash run addition user specifi cloud local version"
    },
    {
        "Question_id":null,
        "Question_title":"Cancel all child runs in Azure ML",
        "Question_body":"How to I properly cancel all child runs in an Azure ML experiment? When I use the code below as expected from documentation, I get an error. \"RunConfigurationException:\nMessage: Error in deserialization. dict fields don't have list element type information. field=output_data, list_element_type=<class 'azureml.core.runconfig.OutputData'>...} with exception init() missing 2 required positional arguments: 'datastore_name' and 'relative_path'\"\n\nrun = Run.get(ws, 'run-id-123456789')\n\nfor child in run.get_children():\nprint(child.get_details())\ntry:\nchild.cancel()\nexcept Exception as e:\nprint(e)\ncontinue\n\nThe datasets and runs were configured properly because they run just fine.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1649253299717,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"You should cancel all the children run by canceling the parent.\n\nAny benefit to cancel child once a time? Just curious",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/802549\/cancel-all-child-runs-in-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-02T15:52:35.547Z",
                "Answer_score":0,
                "Answer_body":"You should cancel all the children run by canceling the parent.\n\nAny benefit to cancel child once a time? Just curious",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651506755547,
        "Question_original_content":"cancel child run properli cancel child run experi us code expect document error runconfigurationexcept messag error deseri dict field list element type inform field output data list element type except init miss requir posit argument datastor rel path run run run child run children print child detail try child cancel except print continu dataset run configur properli run fine",
        "Question_preprocessed_content":"cancel child run properli cancel child run experi us code expect document error runconfigurationexcept messag error deseri dict field list element type inform except init miss requir posit argument run child try except print continu dataset run configur properli run fine",
        "Answer_original_content":"cancel children run cancel parent benefit cancel child time curiou",
        "Answer_preprocessed_content":"cancel children run cancel parent benefit cancel child time curiou",
        "Question_gpt_summary_original":"the user is encountering an error when attempting to cancel all child runs in an experiment, and is unable to find a solution despite the datasets and runs being configured properly.",
        "Question_gpt_summary":"user encount error attempt cancel child run experi unabl solut despit dataset run configur properli",
        "Answer_gpt_summary_original":"possible solution: to cancel all child runs in an experiment, the user should cancel the parent run. it is not necessary to cancel each child run individually.",
        "Answer_gpt_summary":"possibl solut cancel child run experi user cancel parent run necessari cancel child run individu"
    },
    {
        "Question_id":null,
        "Question_title":"Tensorflow and Azure machine learning",
        "Question_body":"Is azure working well with Tensorflow framework? I don\u2019t see any document about it. Any help is good.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1653989511207,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @Chungsun-1776\n\nWelcome to the Microsoft Q&A Platform,\n\nTensorFlow is supported on Azure Machine Learning:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-tensorflow\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-machine-learning-tensorflow?tabs=bash\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-keras\n\nI hope this helps!\n\nPlease don\u2019t forget to \"Accept the answer\" and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/871068\/tensorflow-and-azure-machine-learning.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-31T10:26:09.017Z",
                "Answer_score":1,
                "Answer_body":"Hello @Chungsun-1776\n\nWelcome to the Microsoft Q&A Platform,\n\nTensorFlow is supported on Azure Machine Learning:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-tensorflow\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-machine-learning-tensorflow?tabs=bash\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-keras\n\nI hope this helps!\n\nPlease don\u2019t forget to \"Accept the answer\" and \u201cup-vote\u201d wherever the information provided helps you, this can be beneficial to other community members.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1653992769016,
        "Question_original_content":"tensorflow azur work tensorflow framework dont document help good",
        "Question_preprocessed_content":"tensorflow azur work tensorflow framework dont document help good",
        "Answer_original_content":"hello chungsun welcom microsoft platform tensorflow support http doc microsoft com azur machin learn train tensorflow http doc microsoft com azur azur function function machin learn tensorflow tab bash http doc microsoft com azur machin learn train kera hope help dont forget accept answer vote inform provid help benefici commun member",
        "Answer_preprocessed_content":"hello welcom microsoft platform tensorflow support hope help dont forget accept answer inform provid help benefici commun member",
        "Question_gpt_summary_original":"the user is seeking assistance in determining if azure is compatible with the tensorflow framework.",
        "Question_gpt_summary":"user seek assist determin azur compat tensorflow framework",
        "Answer_gpt_summary_original":"possible solutions to determine if azure is compatible with the tensorflow framework are provided in the answer. the answer includes links to microsoft documentation that show tensorflow is supported on azure for machine learning and azure functions. the answer also includes a link to documentation on how to train keras on azure.",
        "Answer_gpt_summary":"possibl solut determin azur compat tensorflow framework provid answer answer includ link microsoft document tensorflow support azur machin learn azur function answer includ link document train kera azur"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Service",
        "Question_body":"I am using AWS Sagemaker services and delete any instances that belong to the services. However, i still get charged per day even though I dont use the service anymore. I sent the case to the customer support center but no clear explanation yet. Please any kind of help for my account to stop the cost.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651712288852,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":42.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJXLrO6tMRFm6MCHQf39nbg\/sagemaker-service",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-05T21:51:06.787Z",
                "Answer_score":1,
                "Answer_body":"Your Cost explorer can help you find what exactly are you being charged for. You can filter by SageMaker, and then group by 'Usage type'. In general, make sure -\n\nNo notebook instances are running\nNo endpoints are being hosted\nIf you're using SageMaker Studio, all the apps are shut down for every user profile. There might still be EFS charges if you have data over the free tier. See this documentation for shutting down resources - https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-run-and-manage-shut-down.html\n\nAnd, make sure to check all regions, if you've used multiple regions. This should stop recurring costs (if you had training jobs, processing jobs etc - you only pay for the billable seconds, so you won't see recurring costs).",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"servic servic delet instanc belong servic charg dai dont us servic anymor sent case custom support center clear explan kind help account stop cost",
        "Question_preprocessed_content":"servic servic delet instanc belong servic charg dai dont us servic anymor sent case custom support center clear explan kind help account stop cost",
        "Answer_original_content":"cost explor help exactli charg filter group usag type gener sure notebook instanc run endpoint host studio app shut user profil ef charg data free tier document shut resourc http doc aw amazon com latest notebook run manag shut html sure check region multipl region stop recur cost train job process job pai billabl second won recur cost",
        "Answer_preprocessed_content":"cost explor help exactli charg filter group usag type gener sure notebook instanc run endpoint host studio app shut user profil ef charg data free tier document shut resourc sure check region multipl region stop recur cost",
        "Question_gpt_summary_original":"the user is facing an issue with their service provider, where they are being charged for services they have already deleted, despite contacting customer support for assistance.",
        "Question_gpt_summary":"user face issu servic provid charg servic delet despit contact custom support assist",
        "Answer_gpt_summary_original":"possible solutions to the issue of being charged for deleted services despite contacting customer support include using the cost explorer to filter and group charges by usage type, ensuring that no notebook instances or endpoints are running, shutting down all apps for every user profile, checking for efs charges, and checking all regions if multiple regions were used. these steps should help stop recurring costs.",
        "Answer_gpt_summary":"possibl solut issu charg delet servic despit contact custom support includ cost explor filter group charg usag type ensur notebook instanc endpoint run shut app user profil check ef charg check region multipl region step help stop recur cost"
    },
    {
        "Question_id":57488706.0,
        "Question_title":"Deploying custom model on Azure ML Studio",
        "Question_body":"<p>In Azure ML Studio, we have the option of choosing a number of inbuilt ML models like Classification, Regression, etc. , which we can drag and drop to our workflow.<\/p>\n\n<p>My question is, can I upload a custom ML model that I have built locally on my system in Python, and add it to the workflow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1565761282107,
        "Question_favorite_count":null,
        "Question_last_edit_time":1565767277470,
        "Question_score":1.0,
        "Question_view_count":1187.0,
        "Answer_body":"<ol>\n<li>Take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio. Click the \u201cNew\u201d icon in the bottom left:\n<a href=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" alt=\"\"><\/a><\/li>\n<li>In the pane that comes up, click on dataset, and then \u201cFrom Local File\u201d:\n<a href=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" alt=\"\"><\/a><\/li>\n<li>Select the zip file where you stored your serialized model and click the tick. You expirement should look like this:\n<a href=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" alt=\"\"><\/a><\/li>\n<li>Put the following code to run your classification experiment:<\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]])\n<\/code><\/pre>\n\n<p><strong>Update<\/strong> \nIf you want to declare this experiment as an API you need to add web input and output to the Python script module.\n<a href=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":1566565696980,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57488706",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1566202471556,
        "Question_original_content":"deploi custom model studio studio option choos number inbuilt model like classif regress drag drop workflow question upload custom model built local python add workflow",
        "Question_preprocessed_content":"deploi custom model studio studio option choos number inbuilt model like classif regress drag drop workflow question upload custom model built local python add workflow",
        "Answer_original_content":"model pkl file zip upload studio click new icon left pane come click dataset local file select zip file store serial model click tick expir look like follow code run classif experi import panda import sy import pickl def main datafram datafram sy path insert script bundl model pickl load open script bundl model pkl pred model predict datafram return datafram pred updat want declar experi api need add web input output python script modul",
        "Answer_preprocessed_content":"file zip upload studio click new icon left pane come click dataset local file select zip file store serial model click tick expir look like follow code run classif experi updat want declar experi api need add web input output python script modul",
        "Question_gpt_summary_original":"the user is looking to deploy a custom ml model built locally on their system in python to their workflow in studio.",
        "Question_gpt_summary":"user look deploi custom model built local python workflow studio",
        "Answer_gpt_summary_original":"to deploy a custom ml model built locally on a system in python to a workflow in studio, the user can take the model.pkl file, zip it, and upload it into studio. then, they can click the new icon in the bottom left, select dataset, and choose the zip file where the serialized model is stored. after that, the user can run the classification experiment by importing pandas, sys, and pickle, and adding the necessary code. if the user wants to declare this experiment as an api, they need to add web input and output to the python script module.",
        "Answer_gpt_summary":"deploi custom model built local python workflow studio user model pkl file zip upload studio click new icon left select dataset choos zip file serial model store user run classif experi import panda sy pickl ad necessari code user want declar experi api need add web input output python script modul"
    },
    {
        "Question_id":null,
        "Question_title":"What is the difference between uri_file and uri_folder in components?",
        "Question_body":"What is the difference between uri_file and uri_folder in components?\n\nNo matter I specify uri_file or uri_folder in a component input\/output type, in Azure ML Studio jobs it is displayed as uri_folder and I still need to manually append a file name to the path derefernced by uri_file to access a single file. Is there any convenience or difference to specify uri_file if I only intend to access a single file?",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1655434518570,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Thanks.\nWe are planning for some smart deduction or inheriting the type from component to job runtime, we still have some open questions that need to close.\nAlso, we will add doc\/sample explicitly call out the default type in job level.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/892897\/what-is-the-difference-between-uri-file-and-uri-fo.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-22T01:30:38.64Z",
                "Answer_score":1,
                "Answer_body":"Thanks.\nWe are planning for some smart deduction or inheriting the type from component to job runtime, we still have some open questions that need to close.\nAlso, we will add doc\/sample explicitly call out the default type in job level.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-06-20T01:10:08.487Z",
                "Answer_score":1,
                "Answer_body":"Hi @ramr-msft , I am using the YML spec as posted in #893075 Specifying input type as number in ComponentCommand and registing command with Python SDK v2 causes error:\n\nname: stock_pred_data_prep\ndisplay_name: Preprocess data for training\ndescription: reads raw price data, normalize and split the data\n# version: 1 # Not specifying a version will automatically update the version\ntype: command\ninputs:\n  data: {type: uri_folder}\n  test_ratio: {type: number}\n  window: {type: number}\noutputs:\n  scaler: {type: uri_file}\n  train_data_x: {type: uri_file}\n  train_data_y: {type: uri_file}\n  test_data_x: {type: uri_file}\n  test_data_y: {type: uri_file}\ncode: ..\/..\nenvironment:\n  azureml:tensorflow_sklean_cpu:1.0\ncommand: >-\n  PYTHONPATH=$PYTHONPATH:$(pwd) \n  PYTHONPATH=$PYTHONPATH:$(pwd)  python azure_pipeline\/preproc_data\/preproc_data.py \n      --data=${\n                 {inputs.data}} --test_ratio=${\n                 {inputs.test_ratio}} \n      --window=${\n                 {inputs.window}} \n      --scaler=${\n                 {outputs.scaler}} \n      --train_data_x=${\n                 {outputs.train_data_x}} --train_data_y=${\n                 {outputs.train_data_y}} \n      --test_data_x=${\n                 {outputs.test_data_x}} --test_data_y=${\n                 {outputs.test_data_y}}\n\n\n\n\n\nIn Azure ML Studtio, it is displayed correctly as uri_file:\n\n\nHowever, in Data asset, it is displayed as uri_folder, even though uri_file is said to be supported:\n\n\nAnd I need to save data by appending a filename to the input path like this, since the input path is considered a folder by Python:\n\nnp.save(os.path.join(args.train_data_x, 'x_train.npy'), xtrain)",
                "Answer_comment_count":4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1655861438640,
        "Question_original_content":"differ uri file uri folder compon differ uri file uri folder compon matter specifi uri file uri folder compon input output type studio job displai uri folder need manual append file path derefernc uri file access singl file conveni differ specifi uri file intend access singl file",
        "Question_preprocessed_content":"differ compon differ compon matter specifi compon type studio job displai need manual append file path derefernc access singl file conveni differ specifi intend access singl file",
        "Answer_original_content":"thank plan smart deduct inherit type compon job runtim open question need close add doc sampl explicitli default type job level",
        "Answer_preprocessed_content":"thank plan smart deduct inherit type compon job runtim open question need close add explicitli default type job level",
        "Question_gpt_summary_original":"the user is trying to understand the difference between uri_file and uri_folder in components, and why they need to manually append a file name to the path dereferenced by uri_file to access a single file.",
        "Question_gpt_summary":"user try understand differ uri file uri folder compon need manual append file path dereferenc uri file access singl file",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer are:\n- planning for smart deduction or inheriting the type from component to job runtime.\n- adding doc\/sample to explicitly call out the default type in job level.",
        "Answer_gpt_summary":"possibl solut mention answer plan smart deduct inherit type compon job runtim ad doc sampl explicitli default type job level"
    },
    {
        "Question_id":null,
        "Question_title":"Mutually exclusive parameters for sweeps",
        "Question_body":"<p>Can I pass exclusive parameters for a sweep? E.g. for a particular pre-trained model, I want to try learning rate values of [0.1, 0.2]. For another model I want to use [0.3, 0.4].  if I use the sweep configuration below, then grid search will try all the four learning rate values for each model. However, for model 1 - I want to use a learning rate of 0.1, 0.2 whereas, for model2, I want to use 0.3, 0.4.<\/p>\n<p>project: my_project<br>\nprogram: main.py<br>\nname: grid_search<br>\nmethod: grid<br>\nparameters:<br>\nlearning_rate:<br>\nvalues: [0.1, 0.2, 0.3, 0.4]<br>\narch:<br>\nvalues: [\u2018model1\u2019, \u2018model2\u2019]<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1658961338889,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":192.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/mutually-exclusive-parameters-for-sweeps\/2808",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-01T19:28:37.298Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/hsingh-utd\">@hsingh-utd<\/a>,<br>\nAre you hoping for <code>model1<\/code> to always receive [0.1, 0.2] or would you like the sweep to pair [0.3, 0.4] with <code>model1<\/code> at some point during the sweep?<\/p>\n<p>For instance, you could say <code>values:{(0.1, 0.2), (0.3, 0.4)]<\/code> to give <code>model1<\/code> either one of the sets at any point.<\/p>\n<p>Or if you would like try different combinations of the 2 different learning rates you could use<\/p>\n<pre><code class=\"lang-auto\">learning_rate_1:\nvalues: [0.1, 0.2, 0.3, 0.4]\nlearning_rate_2:\nvalues: [0.1, 0.2, 0.3, 0.4]\n<\/code><\/pre>\n<p>Feel free to clarify your goal if I\u2019m not pointing you in the right direction.<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-08T13:23:42.847Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/hsingh-utd\">@hsingh-utd<\/a>, I wanted to follow up and see if you were still looking for help with this?<\/p>\n<p>Thank you,<br>\nNate<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-07T13:24:31.482Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"mutual exclus paramet sweep pass exclus paramet sweep particular pre train model want try learn rate valu model want us us sweep configur grid search try learn rate valu model model want us learn rate model want us project project program main grid search method grid paramet learn rate valu arch valu model model",
        "Question_preprocessed_content":"mutual exclus paramet sweep pass exclus paramet sweep particular model want try learn rate valu model want us us sweep configur grid search try learn rate valu model model want us learn rate model want us project program method grid paramet valu arch valu",
        "Answer_original_content":"hsingh utd hope model receiv like sweep pair model point sweep instanc valu model set point like try differ combin differ learn rate us learn rate valu learn rate valu feel free clarifi goal point right direct thank nate hsingh utd want follow look help thank nate topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"hope receiv like sweep pair point sweep instanc set point like try differ combin differ learn rate us feel free clarifi goal point right direct thank nate want follow look help thank nate topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is facing a challenge of passing mutually exclusive parameters for a sweep in order to use different learning rate values for different models.",
        "Question_gpt_summary":"user face challeng pass mutual exclus paramet sweep order us differ learn rate valu differ model",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions to the challenge of passing mutually exclusive parameters for a sweep in order to use different learning rate values for different models. the first solution is to use values:{(0.1, 0.2), (0.3, 0.4)] to give model1 either one of the sets at any point. the second solution is to use learning_rate_1: values: [0.1, 0.2, 0.3, 0.4] learning_rate_2: values: [0.1, 0.2, 0.3, 0.4] to try different combinations of the 2 different learning rates.",
        "Answer_gpt_summary":"answer suggest possibl solut challeng pass mutual exclus paramet sweep order us differ learn rate valu differ model solut us valu model set point second solut us learn rate valu learn rate valu try differ combin differ learn rate"
    },
    {
        "Question_id":null,
        "Question_title":"Sagemaker Studio is not opening after deleting lifecycle configuration",
        "Question_body":"I was working on a sagemaker studio for ML work, I attached Lifecycle Configuration with it, which was creating problem. Then I deleted the lifecycle configuration without detaching it, and this problem is happening. Can't start sagemaker studio notebook and this is shown.\n\nAny suggestion to fix this ?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1667756083758,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":23.0,
        "Answer_body":"You can try detaching the LCC script using the CLI. You can use the CloudShell from console, since your console role is able to perform updates on the domain.\n\nUse the update-domain CLI call, and provide an empty configuration for the default user settings, something like-\n\naws sagemaker update-domain --domain-id d-abc123 \\\n--default-user-settings '{\n\"JupyterServerAppSettings\": {\n  \"DefaultResourceSpec\": {\n    \"InstanceType\": \"system\"\n   },\n}}'",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU91ywEwTsRRqmHKZJ1yVrrA\/sagemaker-studio-is-not-opening-after-deleting-lifecycle-configuration",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-07T01:49:47.445Z",
                "Answer_score":1,
                "Answer_body":"You can try detaching the LCC script using the CLI. You can use the CloudShell from console, since your console role is able to perform updates on the domain.\n\nUse the update-domain CLI call, and provide an empty configuration for the default user settings, something like-\n\naws sagemaker update-domain --domain-id d-abc123 \\\n--default-user-settings '{\n\"JupyterServerAppSettings\": {\n  \"DefaultResourceSpec\": {\n    \"InstanceType\": \"system\"\n   },\n}}'",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1667785787444,
        "Question_original_content":"studio open delet lifecycl configur work studio work attach lifecycl configur creat problem delet lifecycl configur detach problem happen start studio notebook shown suggest fix",
        "Question_preprocessed_content":"studio open delet lifecycl configur work studio work attach lifecycl configur creat problem delet lifecycl configur detach problem happen start studio notebook shown suggest fix",
        "Answer_original_content":"try detach lcc script cli us cloudshel consol consol role abl perform updat domain us updat domain cli provid configur default user set like updat domain domain abc default user set jupyterserverappset defaultresourcespec instancetyp",
        "Answer_preprocessed_content":"try detach lcc script cli us cloudshel consol consol role abl perform updat domain us cli provid configur default user set like",
        "Question_gpt_summary_original":"the user is encountering a challenge with their studio not opening after deleting the lifecycle configuration without detaching it.",
        "Question_gpt_summary":"user encount challeng studio open delet lifecycl configur detach",
        "Answer_gpt_summary_original":"possible solutions to the challenge of the studio not opening after deleting the lifecycle configuration without detaching it are to detach the lcc script using the cli and to use the cloudshell from console. the user can perform updates on the domain using the console role and the update-domain cli call. an empty configuration for the default user settings can be provided using the command: update-domain --domain-id d-abc123 --default-user-settings '{ \"jupyterserverappsettings\": { \"defaultresourcespec\": { \"instancetype\": \"system\" }, }}'.",
        "Answer_gpt_summary":"possibl solut challeng studio open delet lifecycl configur detach detach lcc script cli us cloudshel consol user perform updat domain consol role updat domain cli configur default user set provid command updat domain domain abc default user set jupyterserverappset defaultresourcespec instancetyp"
    },
    {
        "Question_id":null,
        "Question_title":"Dataset-level access control for data registry",
        "Question_body":"<p>Hello!<br>\nI would like to organize data registry with multiple datasets and multiple users (data scientists from different teams). Since some datasets contain sensitive information, it is important to have access control, both read and write. It would be great to have as flexible setup as possible. In the simplest case there could be just public\/private datasets, but RBAC\/ABAC or the possibility to provide dataset access to specific user directly (create many-to-many access control record) would be much better.<\/p>\n<p>DVC does not provide such features, but there are some workarounds, e.g <a href=\"https:\/\/discuss.dvc.org\/t\/restrict-access-to-dvc-repo\/553\">here<\/a> and <a href=\"https:\/\/discuss.dvc.org\/t\/compliance-administering-data-that-is-under-dvc-control\/174\">there<\/a>. As far as I understood one could create several remotes and to simulate different groups\/roles and provide access to different users respectively. For me it seems to be pretty easy to break involuntary such workaround: e.g. having two remote \u2018public\u2019 and \u2018private\u2019 someone with appropriate access can accidentally push \u2018private\u2019 dataset to \u2018public\u2019 remote, making it available for anybody with \u2018public\u2019 remote access.<br>\nThere could also be two separate repos (or dvc registries) with public and private, it would be less probable to violate policy accidentally, but creation of e.g. 4 or 5 independent access groups (\u2018private_for_project_A\u2019, \u2018private_for_project_B\u2019) would be messy.<\/p>\n<p>I would be very thankful if anybody could share ideas how to organize more reliable\/convenient access control.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1617550888575,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":279.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/dataset-level-access-control-for-data-registry\/712",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-04T22:25:48.594Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/zimka\">@zimka<\/a> it is a great question. There are some ways of organizing this - you\u2019ve already provided the links to the workarounds.<\/p>\n<p>Also, we are thinking of creating special ACL features in DVC to provide a more flexible way of doing it. Now we collect requirements for this project.<\/p>\n<p>It would be great to set up a chat with you and talk about an ideal scenario and the requirements that you have. Please let me know what is the best time this week for the chat. My email - <strong>my name<\/strong> at iterative.ai,<\/p>",
                "Answer_score":12.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-04-05T13:32:33.407Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/dmitry\">@dmitry<\/a> I have sent you an email, thank you!<\/p>",
                "Answer_score":2.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"dataset level access control data registri hello like organ data registri multipl dataset multipl user data scientist differ team dataset contain sensit inform import access control read write great flexibl setup possibl simplest case public privat dataset rbac abac possibl provid dataset access specif user directli creat access control record better provid featur workaround far understood creat remot simul differ group role provid access differ user respect pretti easi break involuntari workaround have remot public privat appropri access accident push privat dataset public remot make avail anybodi public remot access separ repo registri public privat probabl violat polici accident creation independ access group privat project privat project messi thank anybodi share idea organ reliabl conveni access control",
        "Question_preprocessed_content":"access control data registri hello like organ data registri multipl dataset multipl user dataset contain sensit inform import access control read write great flexibl setup possibl simplest case dataset possibl provid dataset access specif user directli better provid featur workaround far understood creat remot simul differ provid access differ user respect pretti easi break involuntari workaround have remot public privat appropri access accident push privat dataset public remot make avail anybodi public remot access separ repo public privat probabl violat polici accident creation independ access group messi thank anybodi share idea organ access control",
        "Answer_original_content":"zimka great question wai organ youv provid link workaround think creat special acl featur provid flexibl wai collect requir project great set chat talk ideal scenario requir let know best time week chat email iter dmitri sent email thank",
        "Answer_preprocessed_content":"great question wai organ youv provid link workaround think creat special acl featur provid flexibl wai collect requir project great set chat talk ideal scenario requir let know best time week chat email sent email thank",
        "Question_gpt_summary_original":"the user is facing the challenge of implementing dataset-level access control for their data registry, with the need for flexible setup and reliable\/convenient access control.",
        "Question_gpt_summary":"user face challeng implement dataset level access control data registri need flexibl setup reliabl conveni access control",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer include using workarounds that have already been provided, and the possibility of creating special acl features to provide a more flexible way of implementing dataset-level access control. the answer also suggests setting up a chat to discuss the user's ideal scenario and requirements.",
        "Answer_gpt_summary":"possibl solut mention answer includ workaround provid possibl creat special acl featur provid flexibl wai implement dataset level access control answer suggest set chat discuss user ideal scenario requir"
    },
    {
        "Question_id":71509160.0,
        "Question_title":"AzureML list huge amount of files",
        "Question_body":"<p>I have directory in AzureML notebook in which I have 300k files and need to list their names.\nApproach below works but takes 1.5h to execute:<\/p>\n<pre><code>from os import listdir\nfrom os.path import isfile, join\nmypath = &quot;.\/temp\/&quot;\ndocsOnDisk = [f for f in listdir(mypath) if isfile(join(mypath, f))]\n<\/code><\/pre>\n<p>What is the azure way to quickly list those files? (both notebook and this directory is in FileShare).<\/p>\n<p>I am also aware that the approach below will give some gain, but still it is not the azure way to do this.<\/p>\n<pre><code>docsOnDisk = [f.name for f in scandir(mypath) ] # shall be 2-20x faster\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1647505772703,
        "Question_favorite_count":null,
        "Question_last_edit_time":1647506520003,
        "Question_score":1.0,
        "Question_view_count":102.0,
        "Answer_body":"<p>Try using glob module and filter method instead of list comprehension.<\/p>\n<pre><code>import glob\nfrom os.path import isfile\nmypath = &quot;.\/temp\/*&quot;\ndocsOnDisk = glob.glob(mypath)\nverified_docsOnDisk = list(filter(lambda x:isfile(x), docsOnDisk))\n<\/code><\/pre>\n<p>glob should give only existing files. Its not needed to verify them by using isfile(). But still if you need to try it out then you can use filter method instead of list comprehension. To skip verification, you can comment last line.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71509160",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1648651666183,
        "Question_original_content":"list huge file directori notebook file need list name approach work take execut import listdir path import isfil join mypath temp docsondisk listdir mypath isfil join mypath azur wai quickli list file notebook directori fileshar awar approach gain azur wai docsondisk scandir mypath shall faster",
        "Question_preprocessed_content":"list huge file directori notebook file need list name approach work take execut azur wai quickli list file awar approach gain azur wai",
        "Answer_original_content":"try glob modul filter method instead list comprehens import glob path import isfil mypath temp docsondisk glob glob mypath verifi docsondisk list filter lambda isfil docsondisk glob exist file need verifi isfil need try us filter method instead list comprehens skip verif comment line",
        "Answer_preprocessed_content":"try glob modul filter method instead list comprehens glob exist file need verifi isfil need try us filter method instead list comprehens skip verif comment line",
        "Question_gpt_summary_original":"the user is facing a challenge of needing to list the names of 300k files in a directory on their notebook, and is looking for an azure-based solution to do this quickly.",
        "Question_gpt_summary":"user face challeng need list name file directori notebook look azur base solut quickli",
        "Answer_gpt_summary_original":"the answer suggests using the glob module and filter method instead of list comprehension to quickly list the names of 300k files in a directory on their notebook. the glob module should give only existing files, so it's not necessary to verify them using isfile(). however, if verification is needed, the user can use the filter method instead of list comprehension. to skip verification, the user can comment out the last line.",
        "Answer_gpt_summary":"answer suggest glob modul filter method instead list comprehens quickli list name file directori notebook glob modul exist file necessari verifi isfil verif need user us filter method instead list comprehens skip verif user comment line"
    },
    {
        "Question_id":62343056.0,
        "Question_title":"How to log a confusion matrix to azureml platform using python",
        "Question_body":"<p>Hello Stackoverflowers,<\/p>\n\n<p>I'm using azureml and I'm wondering if it is possible to log a confusion matrix of the xgboost model I'm training, together with the other metrics I'm already logging. Here's a sample of the code I'm using:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nimport json\n\nwith open('.\/azureml.config', 'r') as f:\n    config = json.load(f)\n\nsvc_pr = ServicePrincipalAuthentication(\n   tenant_id=config['tenant_id'],\n   service_principal_id=config['svc_pr_id'],\n   service_principal_password=config['svc_pr_password'])\n\n\nws = Workspace(workspace_name=config['workspace_name'],\n                        subscription_id=config['subscription_id'],\n                        resource_group=config['resource_group'],\n                        auth=svc_pr)\n\ny_pred = model.predict(dtest)\n\nacc = metrics.accuracy_score(y_test, (y_pred&gt;.5).astype(int))\nrun.log(\"accuracy\",  acc)\nf1 = metrics.f1_score(y_test, (y_pred&gt;.5).astype(int), average='binary')\nrun.log(\"f1 score\",  f1)\n\n\ncmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\nrun.log_confusion_matrix('Confusion matrix', cmtx)\n<\/code><\/pre>\n\n<p>The above code raises this kind of error:<\/p>\n\n<pre><code>TypeError: Object of type ndarray is not JSON serializable\n<\/code><\/pre>\n\n<p>I already tried to transform the matrix in a simpler one, but another error occurred as before I logged a \"manual\" version of it (<code>cmtx = [[30000, 50],[40, 2000]]<\/code>).<\/p>\n\n<pre><code>run.log_confusion_matrix('Confusion matrix', [list([int(y) for y in x]) for x in cmtx])\n\nAzureMLException: AzureMLException:\n    Message: UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-    c5103b205379\/Confusion matrix already exists.\n    InnerException None\n    ErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-c5103b205379\/Confusion matrix already exists.\"\n    }\n}\n<\/code><\/pre>\n\n<p>This makes me think that I'm not properly handling the command <code>run.log_confusion_matrix()<\/code>. So, again, which is the best way I can log a confusion matrix to my azureml experiments?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591960207257,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1418.0,
        "Answer_body":"<p>I eventually found a solution thanks to colleague of mine. I'm hence answering myself, in order to close the question and, maybe, help somebody else.<\/p>\n<p>You can find the proper function in this link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----<\/a>.<\/p>\n<p>Anyway, you also have to consider that, apparently, Azure doesn't work with the standard confusion matrix format returned by sklearn. It accepts indeed ONLY list of list, instead of numpy array, populated with numpy.int64 elements. So you also have to transform the matrix in a simpler format (for the sake of simplicity I used the nested list comprehension in the command below:<\/p>\n<pre><code>cmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\ncmtx = {\n\n&quot;schema_type&quot;: &quot;confusion_matrix&quot;,\n&quot;parameters&quot;: params,\n &quot;data&quot;: {&quot;class_labels&quot;: [&quot;0&quot;, &quot;1&quot;],\n          &quot;matrix&quot;: [[int(y) for y in x] for x in cmtx]}\n}\nrun.log_confusion_matrix('Confusion matrix - error rate', cmtx)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62343056",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1593519812260,
        "Question_original_content":"log confus matrix platform python hello stackoverflow wonder possibl log confus matrix xgboost model train metric log sampl code core model import model core import workspac core experi import experi core authent import serviceprincipalauthent import json open config config json load svc serviceprincipalauthent tenant config tenant servic princip config svc servic princip password config svc password workspac workspac config workspac subscript config subscript resourc group config resourc group auth svc pred model predict dtest acc metric accuraci score test pred astyp int run log accuraci acc metric score test pred astyp int averag binari run log score cmtx metric confus matrix test pred astyp int run log confus matrix confus matrix cmtx code rais kind error typeerror object type ndarrai json serializ tri transform matrix simpler error occur log manual version cmtx run log confus matrix confus matrix list int cmtx except except messag usererror resourc conflict artifactid experimentrun dcid confus matrix exist innerexcept errorrespons error messag usererror resourc conflict artifactid experimentrun dcid confus matrix exist make think properli handl command run log confus matrix best wai log confus matrix experi",
        "Question_preprocessed_content":"log confus matrix platform python hello stackoverflow wonder possibl log confus matrix xgboost model train metric log sampl code code rais kind error tri transform matrix simpler error occur log manual version make think properli handl command best wai log confus matrix experi",
        "Answer_original_content":"eventu solut thank colleagu answer order close question mayb help somebodi proper function link http doc microsoft com python api core core run run view azur log confus matrix valu descript consid appar azur work standard confus matrix format return sklearn accept list list instead numpi arrai popul numpi int element transform matrix simpler format sake simplic nest list comprehens command cmtx metric confus matrix test pred astyp int cmtx schema type confus matrix paramet param data class label matrix int cmtx run log confus matrix confus matrix error rate cmtx",
        "Answer_preprocessed_content":"eventu solut thank colleagu answer order close question mayb help somebodi proper function link consid appar azur work standard confus matrix format return sklearn accept list list instead numpi arrai popul element transform matrix simpler format sake simplic nest list comprehens command",
        "Question_gpt_summary_original":"the user is encountering challenges with logging a confusion matrix to their platform using python, resulting in errors such as \"object of type ndarray is not json serializable\" and \"resource conflict: artifactid experimentrun\/dcid.3196bf92-4952-4850-9a8a-c5103b205379\/confusion matrix already exists.\".",
        "Question_gpt_summary":"user encount challeng log confus matrix platform python result error object type ndarrai json serializ resourc conflict artifactid experimentrun dcid confus matrix exist",
        "Answer_gpt_summary_original":"the solution to the problem of logging a confusion matrix to a platform using python involves using a specific function found in a provided link. additionally, the confusion matrix format returned by sklearn needs to be transformed into a simpler format that is accepted by azure, which involves converting it into a list of lists populated with numpy.int64 elements. the provided code snippet demonstrates how to do this transformation.",
        "Answer_gpt_summary":"solut problem log confus matrix platform python involv specif function provid link addition confus matrix format return sklearn need transform simpler format accept azur involv convert list list popul numpi int element provid code snippet demonstr transform"
    },
    {
        "Question_id":null,
        "Question_title":"Change SSH remote",
        "Question_body":"<p>Hi there, I had a configuration with SSH remote working fine.<br>\nThen, I had to change to other IP so I changed the IP in the configuration file. Then new server is a copy from the previous one.<br>\nNow, I get the following error when I try to push:<\/p>\n<p>ERROR: failed to transfer \u2018md5: 52ad4e880291989c4ac4419cc4d85376\u2019 - Permission denied<\/p>\n<p>Can be something related to the cache?<br>\nfailed to upload \u2018.dvc\/cache\/ca\/217e1b1f3a0c37a2b0c490558d7ddf.dir\u2019 to 'ssh:\/\/user@IP\/path\/data\/ca\/217e1b1f3a0c37a2b0c490558d7ddf<\/p>\n<p>Does anyone know why? Am I missing any step?<br>\nThanks in advance<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1654850566708,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":152.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/change-ssh-remote\/1208",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-06-11T14:55:11.303Z",
                "Answer_body":"<p>Hey. Does <code>ssh user@IP<\/code> work? If so, can you check if the path to specified in the configuration is readable\/writable by your user?<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-22T09:05:41.482Z",
                "Answer_body":"<p>Yes! That was the problem, the folders were not owned by the user.<br>\nThanks<\/p>",
                "Answer_score":46.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"chang ssh remot configur ssh remot work fine chang chang configur file new server copi previou follow error try push error fail transfer adecacccd permiss deni relat cach fail upload cach ebfacabcdddf dir ssh user path data ebfacabcdddf know miss step thank advanc",
        "Question_preprocessed_content":"chang ssh remot configur ssh remot work fine chang chang configur file new server copi previou follow error try push error fail transfer permiss deni relat cach fail upload know miss step thank advanc",
        "Answer_original_content":"hei ssh user work check path specifi configur readabl writabl user ye problem folder own user thank",
        "Answer_preprocessed_content":"hei work check path specifi configur user ye problem folder own user thank",
        "Question_gpt_summary_original":"the user is encountering an error when attempting to push to a new ssh remote after changing the ip in the configuration file, and is wondering if it is related to the cache or if they are missing any steps.",
        "Question_gpt_summary":"user encount error attempt push new ssh remot chang configur file wonder relat cach miss step",
        "Answer_gpt_summary_original":"solution: check if ssh connection to the new ip is working and verify if the path specified in the configuration file is readable\/writable by the user. in this case, the problem was that the folders were not owned by the user.",
        "Answer_gpt_summary":"solut check ssh connect new work verifi path specifi configur file readabl writabl user case problem folder own user"
    },
    {
        "Question_id":null,
        "Question_title":"Async Inference not able to process later requests",
        "Question_body":"Hi there, hope all of you are fine.\n\nI am trying to deploy a train-on-inference type model. I am done with BYOC, and it is working completely fine with real-time inference endpoints. Also, I am able to make it work with Async inference, and concurrent requests on the same instance are also being handled. But, the later requests, never get processed, without any logical error. Also once the endpoint gets scaled down to 0 instance, it fails to scales up.\n\nThese are some of error and warning messages which I get intermittently:\n\n\n\ndata-log:\n2022-03-23T11:23:17.723:[sagemaker logs] [5ea751c9-9271-4533-bc09-c117791e1372] Received server error (500) from primary with message \"<!DOCTYPE HTML PUBLIC \"-\/\/W3C\/\/DTD HTML 3.2 Final\/\/EN\">\n\n\n\nwarnings:\n\/usr\/local\/lib\/python3.8\/dist-packages\/numpy\/core\/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n  setattr(self, word, getattr(machar, word).flat[0])\n\n\nKindly help me with this. Thanks.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1648126119561,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":172.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUqAl1qUyYRK-cbY3DGH-X9g\/async-inference-not-able-to-process-later-requests",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-29T06:30:15.076Z",
                "Answer_score":1,
                "Answer_body":"Hello, I'm running into the exact same issue. I used the same guide and the async endpoint doesn't scale up or down.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-14T14:12:50.944Z",
                "Answer_score":0,
                "Answer_body":"Hi, hope you are fine. Thanks for getting back to me. This is what I am using:\n\n\n# Configure Autoscaling on asynchronous endpoint down to zero instances\nresponse = client.register_scalable_target(\n    ServiceNamespace=\"sagemaker\",\n    ResourceId=resource_id,\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",\n    MinCapacity=0,\n    MaxCapacity=4,\n)\n\nresponse = client.put_scaling_policy(\n    PolicyName=\"Invocations-ScalingPolicy\",\n    ServiceNamespace=\"sagemaker\",  # The namespace of the AWS service that provides the resource.\n    ResourceId=resource_id,  # Endpoint name\n    ScalableDimension=\"sagemaker:variant:DesiredInstanceCount\",  # SageMaker supports only Instance Count\n    PolicyType=\"TargetTrackingScaling\",  # 'StepScaling'|'TargetTrackingScaling'\n    TargetTrackingScalingPolicyConfiguration={\n        \"TargetValue\": 2.0,  # The target value for the metric. - here the metric is - SageMakerVariantInvocationsPerInstance\n        \"CustomizedMetricSpecification\": {\n            \"MetricName\": \"ApproximateBacklogSizePerInstance\",\n            \"Namespace\": \"AWS\/SageMaker\",\n            \"Dimensions\": [{\"Name\": \"EndpointName\", \"Value\": endpoint_name}],\n            \"Statistic\": \"Average\",\n        },\n        \"ScaleInCooldown\": 300,  # The cooldown period helps you prevent your Auto Scaling group from launching or terminating\n        # additional instances before the effects of previous activities are visible.\n        # You can configure the length of time based on your instance startup time or other application needs.\n        # ScaleInCooldown - The amount of time, in seconds, after a scale in activity completes before another scale in activity can start.\n        \"ScaleOutCooldown\": 300  # ScaleOutCooldown - The amount of time, in seconds, after a scale out activity completes before another scale out activity can start.\n        # 'DisableScaleIn': True|False - ndicates whether scale in by the target tracking policy is disabled.\n        # If the value is true , scale in is disabled and the target tracking policy won't remove capacity from the scalable resource.\n    },\n)",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"async infer abl process later request hope fine try deploi train infer type model byoc work complet fine real time infer endpoint abl work async infer concurr request instanc handl later request process logic error endpoint get scale instanc fail scale error warn messag intermitt data log log eac receiv server error primari messag warn usr local lib python dist packag numpi core getlimit userwarn valu smallest subnorm type zero setattr self word getattr machar word flat kindli help thank",
        "Question_preprocessed_content":"async infer abl process later request hope fine try deploi type model byoc work complet fine infer endpoint abl work async infer concurr request instanc handl later request process logic error endpoint get scale instanc fail scale error warn messag intermitt log receiv server error primari messag warn userwarn valu smallest subnorm type zero setattr flat kindli help thank",
        "Answer_original_content":"hello run exact issu guid async endpoint scale hope fine thank get configur autosc asynchron endpoint zero instanc respons client regist scalabl target servicenamespac resourceid resourc scalabledimens variant desiredinstancecount mincapac maxcapac respons client scale polici policynam invoc scalingpolici servicenamespac namespac aw servic provid resourc resourceid resourc endpoint scalabledimens variant desiredinstancecount support instanc count policytyp targettrackingsc stepscal targettrackingsc targettrackingscalingpolicyconfigur targetvalu target valu metric metric variantinvocationsperinst customizedmetricspecif metricnam approximatebacklogsizeperinst namespac aw dimens endpointnam valu endpoint statist averag scaleincooldown cooldown period help prevent auto scale group launch termin addit instanc effect previou activ visibl configur length time base instanc startup time applic need scaleincooldown time second scale activ complet scale activ start scaleoutcooldown scaleoutcooldown time second scale activ complet scale activ start disablescalein true fals ndicat scale target track polici disabl valu true scale disabl target track polici won remov capac scalabl resourc",
        "Answer_preprocessed_content":"hello run exact issu guid async endpoint scale hope fine thank get configur autosc asynchron endpoint zero instanc respons servicenamespac scalabledimens variant desiredinstancecount mincapac maxcapac respons servicenamespac namespac aw servic provid resourc endpoint scalabledimens variant desiredinstancecount support instanc count policytyp targettrackingsc stepscal targettrackingsc targettrackingscalingpolicyconfigur targetvalu target valu metric metric variantinvocationsperinst customizedmetricspecif statist averag scaleincooldown cooldown period help prevent auto scale group launch termin addit instanc effect previou activ visibl configur length time base instanc startup time applic need scaleincooldown time second scale activ complet scale activ start scaleoutcooldown scaleoutcooldown time second scale activ complet scale activ start disablescalein true fals ndicat scale target track polici disabl valu true scale disabl target track polici won remov capac scalabl resourc",
        "Question_gpt_summary_original":"the user is encountering challenges with async inference not being able to process later requests, as well as scaling up issues when the endpoint is scaled down to 0 instance, with intermittent errors and warnings.",
        "Question_gpt_summary":"user encount challeng async infer abl process later request scale issu endpoint scale instanc intermitt error warn",
        "Answer_gpt_summary_original":"the answer provides a possible solution to the challenges faced by the user with async inference not being able to process later requests and scaling up issues when the endpoint is scaled down to 0 instance. the solution involves configuring autoscaling on asynchronous endpoint down to zero instances using aws services. the solution includes setting up a scaling policy, defining the target value for the metric, and configuring the cooldown period to prevent launching or terminating additional instances before the effects of previous activities are visible.",
        "Answer_gpt_summary":"answer provid possibl solut challeng face user async infer abl process later request scale issu endpoint scale instanc solut involv configur autosc asynchron endpoint zero instanc aw servic solut includ set scale polici defin target valu metric configur cooldown period prevent launch termin addit instanc effect previou activ visibl"
    },
    {
        "Question_id":50032795.0,
        "Question_title":"prevent access to s3 buckets for sagemaker users",
        "Question_body":"<p>I'm trying to add an IAM user for using sagemaker. I used the <code>AmazonSageMakerFullAccess<\/code> policy. But when I log in as this user I can see all of the s3 buckets of the root account and download files from them.<\/p>\n<p>The <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sagemaker-roles.html\" rel=\"nofollow noreferrer\">sagemaker documentation<\/a> states<\/p>\n<blockquote>\n<p>When attaching the AmazonSageMakerFullAccess policy to a role, you must do one of the following to allow Amazon SageMaker to access your S3 bucket:<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the name of the bucket where you store training data, or the model artifacts resulting from model training, or both.<\/p>\n<p>Include the string &quot;SageMaker&quot; or &quot;sagemaker&quot; in the object name of the training data object(s).<\/p>\n<p>Tag the S3 object with &quot;sagemaker=true&quot;. The key and value are case sensitive. For more information, see Object Tagging in the Amazon Simple Storage Service Developer Guide.<\/p>\n<p>Add a bucket policy that allows access for the execution role. For more information, see Using Bucket Policies and User Policies in the Amazon Simple Storage Service Developer Guide.<\/p>\n<\/blockquote>\n<p>This seems to be inaccurate the user can access s3 buckets lacking <code>sagemaker<\/code> in the name. How do I limit the access?<\/p>\n<p>the full policy is below<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;sagemaker:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;,\n                &quot;ecr:GetDownloadUrlForLayer&quot;,\n                &quot;ecr:BatchGetImage&quot;,\n                &quot;ecr:BatchCheckLayerAvailability&quot;,\n                &quot;cloudwatch:PutMetricData&quot;,\n                &quot;cloudwatch:PutMetricAlarm&quot;,\n                &quot;cloudwatch:DescribeAlarms&quot;,\n                &quot;cloudwatch:DeleteAlarms&quot;,\n                &quot;ec2:CreateNetworkInterface&quot;,\n                &quot;ec2:CreateNetworkInterfacePermission&quot;,\n                &quot;ec2:DeleteNetworkInterface&quot;,\n                &quot;ec2:DeleteNetworkInterfacePermission&quot;,\n                &quot;ec2:DescribeNetworkInterfaces&quot;,\n                &quot;ec2:DescribeVpcs&quot;,\n                &quot;ec2:DescribeDhcpOptions&quot;,\n                &quot;ec2:DescribeSubnets&quot;,\n                &quot;ec2:DescribeSecurityGroups&quot;,\n                &quot;application-autoscaling:DeleteScalingPolicy&quot;,\n                &quot;application-autoscaling:DeleteScheduledAction&quot;,\n                &quot;application-autoscaling:DeregisterScalableTarget&quot;,\n                &quot;application-autoscaling:DescribeScalableTargets&quot;,\n                &quot;application-autoscaling:DescribeScalingActivities&quot;,\n                &quot;application-autoscaling:DescribeScalingPolicies&quot;,\n                &quot;application-autoscaling:DescribeScheduledActions&quot;,\n                &quot;application-autoscaling:PutScalingPolicy&quot;,\n                &quot;application-autoscaling:PutScheduledAction&quot;,\n                &quot;application-autoscaling:RegisterScalableTarget&quot;,\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:GetLogEvents&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;,\n                &quot;s3:PutObject&quot;,\n                &quot;s3:DeleteObject&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:s3:::*SageMaker*&quot;,\n                &quot;arn:aws:s3:::*Sagemaker*&quot;,\n                &quot;arn:aws:s3:::*sagemaker*&quot;\n            ]\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:CreateBucket&quot;,\n                &quot;s3:GetBucketLocation&quot;,\n                &quot;s3:ListBucket&quot;,\n                &quot;s3:ListAllMyBuckets&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;s3:GetObject&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEqualsIgnoreCase&quot;: {\n                    &quot;s3:ExistingObjectTag\/SageMaker&quot;: &quot;true&quot;\n                }\n            }\n        },\n        {\n            &quot;Action&quot;: &quot;iam:CreateServiceLinkedRole&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:iam::*:role\/aws-service-role\/sagemaker.application-autoscaling.amazonaws.com\/AWSServiceRoleForApplicationAutoScaling_SageMakerEndpoint&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringLike&quot;: {\n                    &quot;iam:AWSServiceName&quot;: &quot;sagemaker.application-autoscaling.amazonaws.com&quot;\n                }\n            }\n        },\n        {\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Condition&quot;: {\n                &quot;StringEquals&quot;: {\n                    &quot;iam:PassedToService&quot;: &quot;sagemaker.amazonaws.com&quot;\n                }\n            }\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1524699817877,
        "Question_favorite_count":null,
        "Question_last_edit_time":1592644375060,
        "Question_score":1.0,
        "Question_view_count":1703.0,
        "Answer_body":"<p>looks like the sagemaker notebook wizard has you create a role that has limited s3 access. If I add this and the default <code>AmazonSageMakerFullAccess<\/code> the user is properly restricted. <a href=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9BjRD.png\" alt=\"Amazon make sagemaker role\"><\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/IM7WW.png\" alt=\"choose iam roles\"><\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50032795",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1525129267447,
        "Question_original_content":"prevent access bucket user try add iam user amazonfullaccess polici log user bucket root account download file document state attach amazonfullaccess polici role follow allow access bucket includ string bucket store train data model artifact result model train includ string object train data object tag object true kei valu case sensit inform object tag amazon simpl storag servic develop guid add bucket polici allow access execut role inform bucket polici user polici amazon simpl storag servic develop guid inaccur user access bucket lack limit access polici version statement effect allow action resourc effect allow action ecr getauthorizationtoken ecr getdownloadurlforlay ecr batchgetimag ecr batchchecklayeravail cloudwatch putmetricdata cloudwatch putmetricalarm cloudwatch describealarm cloudwatch deletealarm createnetworkinterfac createnetworkinterfacepermiss deletenetworkinterfac deletenetworkinterfacepermiss describenetworkinterfac describevpc describedhcpopt describesubnet describesecuritygroup applic autosc deletescalingpolici applic autosc deletescheduledact applic autosc deregisterscalabletarget applic autosc describescalabletarget applic autosc describescalingact applic autosc describescalingpolici applic autosc describescheduledact applic autosc putscalingpolici applic autosc putscheduledact applic autosc registerscalabletarget log createloggroup log createlogstream log describelogstream log getlogev log putlogev resourc effect allow action getobject putobject deleteobject resourc arn aw arn aw arn aw effect allow action createbucket getbucketloc listbucket listallmybucket resourc effect allow action getobject resourc condit stringequalsignorecas existingobjecttag true action iam createservicelinl effect allow resourc arn aw iam role aw servic role applic autosc amazonaw com awsserviceroleforapplicationautosc endpoint condit stringlik iam awsservicenam applic autosc amazonaw com effect allow action iam passrol resourc condit stringequ iam passedtoservic amazonaw com",
        "Question_preprocessed_content":"prevent access bucket user try add iam user polici log user bucket root account download file document state attach amazonfullaccess polici role follow allow access bucket includ string bucket store train data model artifact result model train includ string object train data object tag object true kei valu case sensit inform object tag amazon simpl storag servic develop guid add bucket polici allow access execut role inform bucket polici user polici amazon simpl storag servic develop guid inaccur user access bucket lack limit access polici",
        "Answer_original_content":"look like notebook wizard creat role limit access add default amazonfullaccess user properli restrict",
        "Answer_preprocessed_content":"look like notebook wizard creat role limit access add default user properli restrict",
        "Question_gpt_summary_original":"the user is encountering challenges in limiting access to s3 buckets for users with the amazonfullaccess policy attached.",
        "Question_gpt_summary":"user encount challeng limit access bucket user amazonfullaccess polici attach",
        "Answer_gpt_summary_original":"the answer suggests creating a role with limited s3 access and adding it along with the default amazonfullaccess policy to restrict user access to s3 buckets.",
        "Answer_gpt_summary":"answer suggest creat role limit access ad default amazonfullaccess polici restrict user access bucket"
    },
    {
        "Question_id":null,
        "Question_title":"Wandb Sweep running bash scripts",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019d like to use Wandb sweep but running different parameter settings with bash. Is it possible?<br>\nOr I have to have a python file as entry point to parse the parameters and execute bash from python.<\/p>\n<p>Thanks,<\/p>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1660165958652,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-sweep-running-bash-scripts\/2895",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-11T19:18:02.209Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevindong\">@kevindong<\/a>,<\/p>\n<p>This should be possible, I was able to set up a sweep config which can run a bash shell:<\/p>\n<pre><code class=\"lang-auto\">program: main.sh\nmethod: random\nmetric:\n  name: validation_loss\n  goal: minimize\nparameters:\n  learning_rate:\n    min: 0.0001\n    max: 0.1\n  optimizer:\n    values: [\"adam\", \"sgd\"]\ncommand:\n  - ${env}\n  - bash\n  - ${program}\n  - ${args}\n<\/code><\/pre>\n<p>However, there is no equivalent of  <code>wandb.log<\/code> for bash, so there is no way to communicate any data back to W&amp;B.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-12T23:26:08.418Z",
                "Answer_body":"<p>Thanks Ramit. your code above works.<\/p>\n<p>However, if I use bash script as the ${program}, it seems the wandb server can\u2019t know if the run is successful or not. All runs started in such a way will end up as \u201cCrashed\u201d state in the sweep table on the webapp<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-12T23:28:25.171Z",
                "Answer_body":"<p>How does Wandb determine the status code?<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-16T23:20:20.207Z",
                "Answer_body":"<p>Hey Kevin,<\/p>\n<p>W&amp;B will not be able to send off a status code if you are operating directly through bash - the exit code is communicated to our servers through <code>wandb.finish<\/code>, which is written for python.<\/p>\n<p>You might want to set up an entrypoint through python and run your bash script through there if you are looking to interact with W&amp;B more.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-22T20:38:41.673Z",
                "Answer_body":"<p>Hi Kevin,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>\n<p>Best,<br>\nWeights &amp; Biases<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-25T22:11:09.231Z",
                "Answer_body":"<p>Hi Kevin, since we have not heard back from you we are going to close this request. If you would like to re-open the conversation, please let us know!<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-15T23:20:34.801Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"sweep run bash script like us sweep run differ paramet set bash possibl python file entri point pars paramet execut bash python thank",
        "Question_preprocessed_content":"sweep run bash script like us sweep run differ paramet set bash possibl python file entri point pars paramet execut bash python thank",
        "Answer_original_content":"kevindong possibl abl set sweep config run bash shell program main method random metric valid loss goal minim paramet learn rate min max optim valu adam sgd command env bash program arg equival log bash wai commun data thank ramit thank ramit code work us bash script program server know run success run start wai end crash state sweep tabl webapp determin statu code hei kevin abl send statu code oper directli bash exit code commun server finish written python want set entrypoint python run bash script look interact kevin want follow support request heard let know assist issu resolv best kevin heard go close request like open convers let know topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"possibl abl set sweep config run bash shell equival bash wai commun data thank ramit thank ramit code work us bash script server know run success run start wai end crash state sweep tabl webapp determin statu code hei kevin abl send statu code oper directli bash exit code commun server written python want set entrypoint python run bash script look interact kevin want follow support request heard let know assist issu resolv best kevin heard go close request like convers let know topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking to use sweep to run different parameter settings with bash, and is wondering if it is possible to do so without having to use a python file as an entry point.",
        "Question_gpt_summary":"user look us sweep run differ paramet set bash wonder possibl have us python file entri point",
        "Answer_gpt_summary_original":"possible solutions to running different parameter settings with bash using sweep without having to use a python file as an entry point are provided in the answer. the user can set up a sweep config that can run a bash shell, but there is no equivalent of .log for bash, so there is no way to communicate any data back to w&b. the exit code is communicated to w&b servers through .finish, which is written for python. if the user wants to interact with w&b more, they might want to set up an entry point through python and run the bash script through there.",
        "Answer_gpt_summary":"possibl solut run differ paramet set bash sweep have us python file entri point provid answer user set sweep config run bash shell equival log bash wai commun data exit code commun server finish written python user want interact want set entri point python run bash script"
    },
    {
        "Question_id":36285329.0,
        "Question_title":"How to import a third party library \"causalImpact\" using R script in AzureML studio?",
        "Question_body":"<p>I tried to import causalImpact library from github using \"devtools\" in AzureML studio for one of my projects.\ncode used was:<\/p>\n\n<pre><code>library(devtools)\ndevtools::install_github(\"google\/CausalImpact\")\n<\/code><\/pre>\n\n<p>Unfortunately, Azure doesn't support this.So tried importing it following the procedure in this <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning\/\" rel=\"nofollow\">blog<\/a>.It is giving multiple errors on the name of dependent packages of casualImpact(i.e. BOOM, BH etc.). Can anyone help me out in importing this package on Azure?<\/p>\n\n<p>This is the R-script I used following the link given above:<\/p>\n\n<pre><code>library(assertthat)\nlibrary(dplyr)\nlibrary(hflights)\nlibrary(Lahman)\nlibrary(magrittr)\nlibrary(LGPL)\ninstall.packages(\"src\/BH_1.55.0-3.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"BH \", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\nlibrary(BH)\ninstall.packages(\"src\/Boom_0.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"Boom \", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\ninstall.packages(\"src\/BoomSpikeSlab.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"BoomSpikeSlab\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n\ninstall.packages(\"src\/bsts_0.5.1.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"bsts\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\nlibrary(zoo)\nlibrary(xts)\ninstall.packages(\"src\/CausalImpact.zip\", lib = \".\", repos = NULL, verbose = TRUE)\nsuccess &lt;- library(\"CausalImpact\", lib.loc=\".\", logical.return = TRUE, verbose=TRUE)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1459257130213,
        "Question_favorite_count":null,
        "Question_last_edit_time":1459402444996,
        "Question_score":2.0,
        "Question_view_count":326.0,
        "Answer_body":"<p>You will have to upload all dependent packages of casualImpact as a zip file - see sample <a href=\"http:\/\/gallery.azureml.net\/Details\/7507f907deb845d9b9b193b455a8615d\" rel=\"nofollow\">here<\/a> which shows uploading two packages required for xgboost<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36285329",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1459314427723,
        "Question_original_content":"import parti librari causalimpact script studio tri import causalimpact librari github devtool studio project code librari devtool devtool instal github googl causalimpact unfortun azur support tri import follow procedur blog give multipl error depend packag casualimpact boom help import packag azur script follow link given librari assertthat librari dplyr librari hflight librari lahman librari magrittr librari lgpl instal packag src zip lib repo null verbos true success librari lib loc logic return true verbos true librari instal packag src boom zip lib repo null verbos true success librari boom lib loc logic return true verbos true instal packag src boomspikeslab zip lib repo null verbos true success librari boomspikeslab lib loc logic return true verbos true instal packag src bst zip lib repo null verbos true success librari bst lib loc logic return true verbos true librari zoo librari xt instal packag src causalimpact zip lib repo null verbos true success librari causalimpact lib loc logic return true verbos true",
        "Question_preprocessed_content":"import parti librari causalimpact script studio tri import causalimpact librari github devtool studio project code unfortun azur support tri import follow procedur give multipl error depend packag boom help import packag azur follow link given",
        "Answer_original_content":"upload depend packag casualimpact zip file sampl show upload packag requir xgboost",
        "Answer_preprocessed_content":"upload depend packag casualimpact zip file sampl show upload packag requir xgboost",
        "Question_gpt_summary_original":"the user is encountering challenges in importing the third-party library \"causalimpact\" using an r script in azure studio, and is receiving multiple errors related to dependent packages.",
        "Question_gpt_summary":"user encount challeng import parti librari causalimpact script azur studio receiv multipl error relat depend packag",
        "Answer_gpt_summary_original":"to resolve the issue of importing the third-party library \"causalimpact\" in azure studio, the user needs to upload all the dependent packages of causalimpact as a zip file. the answer provides a sample that shows how to upload two packages required for xgboost.",
        "Answer_gpt_summary":"resolv issu import parti librari causalimpact azur studio user need upload depend packag causalimpact zip file answer provid sampl show upload packag requir xgboost"
    },
    {
        "Question_id":null,
        "Question_title":"Creating datasets in Azure Machine Learning service from more than 100 paths",
        "Question_body":"Hi,\n\nI need to create a dataset in Azure Machine Learning service from an Azure Data Lake Gen2 registered as a Datastore. Data in the lake are 1000's of avro files stored by an Event Hub Capture following the pattern [EventHub]\/[Partition]\/[YYYY]\/[MM]\/[DD]\/[HH]\/[mm]\/[ss], so there is one path for each file.\n\nAccording to the datasets documentation it is recommended \"... creating dataset referencing less than 100 paths in datastores for optimal performance.\"\n\nWhat would be the alternative\/recommended approach in my application? Streaming data are continuously captured by the Event Hub.\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1591718920387,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/34010\/creating-datasets-in-azure-machine-learning-servic.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-11T04:48:23.87Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nYou can create dataset with globing pattern.\nds = Dataset.File.from_files((datastore, '[EventHub]\/[Partition]\/**))\n\nThe mount time should be less than 1 min.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"creat dataset servic path need creat dataset servic azur data lake gen regist datastor data lake avro file store event hub captur follow pattern eventhub partit path file accord dataset document recommend creat dataset referenc path datastor optim perform altern recommend approach applic stream data continu captur event hub thank",
        "Question_preprocessed_content":"creat dataset servic path need creat dataset servic azur data lake gen regist datastor data lake avro file store event hub captur follow pattern path file accord dataset document recommend creat dataset referenc path datastor optim approach applic stream data continu captur event hub thank",
        "Answer_original_content":"creat dataset globe pattern dataset file file datastor eventhub partit mount time min",
        "Answer_preprocessed_content":"creat dataset globe pattern mount time min",
        "Question_gpt_summary_original":"the user is facing a challenge of creating a dataset in service from an azure data lake gen2 registered as a datastore, which contains 1000's of avro files stored by an event hub capture following a specific pattern, and is looking for an alternative\/recommended approach as the datasets documentation recommends creating datasets referencing less than 100 paths in datastores for optimal performance.",
        "Question_gpt_summary":"user face challeng creat dataset servic azur data lake gen regist datastor contain avro file store event hub captur follow specif pattern look altern recommend approach dataset document recommend creat dataset referenc path datastor optim perform",
        "Answer_gpt_summary_original":"possible solution: the user can create a dataset with globing pattern using the code \"ds = dataset.file.from_files((datastore, '[eventhub]\/[partition]\/**))\". this approach can help in creating a dataset from the azure data lake gen2 registered as a datastore, which contains 1000's of avro files stored by an event hub capture following a specific pattern. the mount time for this approach should be less than 1 minute, which can help in optimizing the performance.",
        "Answer_gpt_summary":"possibl solut user creat dataset globe pattern code dataset file file datastor eventhub partit approach help creat dataset azur data lake gen regist datastor contain avro file store event hub captur follow specif pattern mount time approach minut help optim perform"
    },
    {
        "Question_id":null,
        "Question_title":"Read\/Write from\/to Blob Storage in AzureML",
        "Question_body":"I am using AzureML and I have a blob storage container, where I want read data from it and write data to it. Can I achieve that with the path that is being passed to the experiment as follows (from here):\n\n from azureml.core import Workspace\n ws: Workspace = Workspace.from_config()\n compute_target: ComputeTarget = ws.compute_targets['<compute-target-name>']\n ds: Datastore = ws.get_default_datastore()\n    \n data_ref = ds.path('<path\/on\/datastore>').as_mount()\n    \n config = ScriptRunConfig(\n     source_directory='.',\n     script='script.py',\n     arguments=[str(data_ref)],               # returns environment variable $AZUREML_DATAREFERENCE_example_data\n     compute_target=compute_target,\n )\n    \n config.run_config.data_references[data_ref.data_reference_name] = data_ref.to_config()\n\n\n\nIf so, what is the purpose of OutputFileDatasetConfig class in the API? Is it just a convenient shortcut to the path in the container?\n\n from azureml.data import OutputFileDatasetConfig\n    \n output = OutputFileDatasetConfig(folder, destination=(datastore_obj, path_to_folder))\n arguments = [output]\n\n\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1613410517227,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/273787\/readwrite-fromto-blob-storage-in-azureml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-16T08:07:14.03Z",
                "Answer_score":0,
                "Answer_body":"@Sep-8038 I think you can use this notebook to try the exact scenario as above.\nOutput is configured to write the result back to the blob store under the configured folder. More options to configure the output can be seen by running 'help(OutputFileDatasetConfig)'\n\nHope this helps!!",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":13.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"read write blob storag blob storag contain want read data write data achiev path pass experi follow core import workspac workspac workspac config comput target computetarget comput target datastor default datastor data ref path mount config scriptrunconfig sourc directori script script argument str data ref return environ variabl datarefer exampl data comput target comput target config run config data refer data ref data refer data ref config purpos outputfiledatasetconfig class api conveni shortcut path contain data import outputfiledatasetconfig output outputfiledatasetconfig folder destin datastor obj path folder argument output thank",
        "Question_preprocessed_content":"blob storag blob storag contain want read data write data achiev path pass experi follow core import workspac workspac computetarget datastor config scriptrunconfig return environ variabl purpos outputfiledatasetconfig class api conveni shortcut path contain data import outputfiledatasetconfig output outputfiledatasetconfig argument thank",
        "Answer_original_content":"sep think us notebook try exact scenario output configur write result blob store configur folder option configur output seen run help outputfiledatasetconfig hope help",
        "Answer_preprocessed_content":"think us notebook try exact scenario output configur write result blob store configur folder option configur output seen run help hope help",
        "Question_gpt_summary_original":"the user is attempting to read and write data from\/to a blob storage container using , and is inquiring about the purpose of the outputfiledatasetconfig class in the api and whether it is a convenient shortcut to the path in the container.",
        "Question_gpt_summary":"user attempt read write data blob storag contain inquir purpos outputfiledatasetconfig class api conveni shortcut path contain",
        "Answer_gpt_summary_original":"the answer suggests using a notebook to try the scenario of reading and writing data to a blob storage container in . the outputfiledatasetconfig class can be used to configure the output and write the result back to the blob store under the configured folder. running 'help(outputfiledatasetconfig)' will provide more options to configure the output.",
        "Answer_gpt_summary":"answer suggest notebook try scenario read write data blob storag contain outputfiledatasetconfig class configur output write result blob store configur folder run help outputfiledatasetconfig provid option configur output"
    },
    {
        "Question_id":65112585.0,
        "Question_title":"Pip installation stuck in infinite loop if unresolvable conflicts in dependencies",
        "Question_body":"<p>Pip installation is stuck in an infinite loop if there are unresolvable conflicts in dependencies. To reproduce, <code>pip==20.3.0<\/code> and:<\/p>\n<pre><code>pip install pyarrow==2.0.0 azureml-defaults==1.18.0\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1606928045387,
        "Question_favorite_count":null,
        "Question_last_edit_time":1608739608112,
        "Question_score":13.0,
        "Question_view_count":2146.0,
        "Answer_body":"<p>Workarounds:<\/p>\n<p>Local environment:\nDowngrade pip to &lt; 20.3<\/p>\n<p>Conda environment created from yaml:\nThis will be seen only if conda-forge is highest priority channel, anaconda channel doesn't have pip 20.3 (as of now). To mitigate the issue please explicitly specify pip&lt;20.3 (!=20.3 or =20.2.4 pin to other version) as a conda dependency in the conda specification file<\/p>\n<p>AzureML experimentation:\nFollow the case above to make sure pinned pip resulted as a conda dependency in the environment object, either from yml file or programmatically<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":12.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65112585",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1606928528563,
        "Question_original_content":"pip instal stuck infinit loop unresolv conflict depend pip instal stuck infinit loop unresolv conflict depend reproduc pip pip instal pyarrow default",
        "Question_preprocessed_content":"pip instal stuck infinit loop unresolv conflict depend pip instal stuck infinit loop unresolv conflict depend reproduc",
        "Answer_original_content":"workaround local environ downgrad pip conda environ creat yaml seen conda forg highest prioriti channel anaconda channel pip mitig issu explicitli specifi pip pin version conda depend conda specif file experiment follow case sure pin pip result conda depend environ object yml file programmat",
        "Answer_preprocessed_content":"workaround local environ downgrad pip conda environ creat yaml seen highest prioriti channel anaconda channel pip mitig issu explicitli specifi conda depend conda specif file experiment follow case sure pin pip result conda depend environ object yml file programmat",
        "Question_gpt_summary_original":"the user encountered an issue where pip installation was stuck in an infinite loop due to unresolvable conflicts in dependencies.",
        "Question_gpt_summary":"user encount issu pip instal stuck infinit loop unresolv conflict depend",
        "Answer_gpt_summary_original":"possible solutions to resolve the issue of pip installation stuck in an infinite loop due to unresolvable conflicts in dependencies are: downgrading pip to <20.3 in the local environment, creating a conda environment from yaml with explicitly specifying pip<20.3 as a conda dependency, and ensuring that the pinned pip resulted as a conda dependency in the environment object.",
        "Answer_gpt_summary":"possibl solut resolv issu pip instal stuck infinit loop unresolv conflict depend downgrad pip local environ creat conda environ yaml explicitli specifi pip conda depend ensur pin pip result conda depend environ object"
    },
    {
        "Question_id":null,
        "Question_title":"Dialogflow cx v3 DetectIntentRequest returning no-match",
        "Question_body":"I am trying to create and manage agents using exclusively the api.  I have created an agent with one intent only:name: \"projects\/???\/locations\/global\/agents\/1828e34b-78bc-48f5-9212-6dd83497d409\/intents\/b9e91883-f358-46ea-9661-a8a39c7d2557\"\ndisplay_name: \"test-age\"\ntraining_phrases {\nparts {\ntext: \" I am \"\n}\nparts {\ntext: \" 23 \"\nparameter_id: \"p0\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" my age is \"\n}\nparts {\ntext: \" 68 \"\nparameter_id: \"p1\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" I am \"\n}\nparts {\ntext: \" 44 \"\nparameter_id: \"p2\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" age \"\n}\nparts {\ntext: \" 81 \"\nparameter_id: \"p3\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" age is \"\n}\nparts {\ntext: \" 35 \"\nparameter_id: \"p4\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" the age is \"\n}\nparts {\ntext: \" 29 \"\nparameter_id: \"p5\"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 37 \"\nparameter_id: \"p6\"\n}\nparts {\ntext: \" years of age \"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 45 \"\nparameter_id: \"p7\"\n}\nparts {\ntext: \" years \"\n}\nrepeat_count: 1\n}\ntraining_phrases {\nparts {\ntext: \" 52 \"\nparameter_id: \"p8\"\n}\nparts {\ntext: \" years old \"\n}\nrepeat_count: 1\n}\nparameters {\nid: \"p0\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p1\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p2\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p3\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p4\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p5\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p6\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p7\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\nparameters {\nid: \"p8\"\nentity_type: \"projects\/-\/locations\/-\/agents\/-\/entityTypes\/sys.number-integer\"\n}\npriority: 500000However, when I try to detect that intent using the DetectIntentRequest as shown in the github samples I keep getting no-match results:====================\nQuery Text: ' I am 55 '\nDetected Intent: text: \" I am 55 \"\nlanguage_code: \"en\"\nresponse_messages {\ntext {\ntext: \"Sorry, could you say that again?\"\n}\n}\ncurrent_page {\nname: \"projects\/???\/locations\/global\/agents\/1828e34b-78bc-48f5-9212-6dd83497d409\/flows\/00000000-0000-0000-0000-000000000000\/pages\/START_PAGE\"\ndisplay_name: \"Start Page\"\n}\nintent_detection_confidence: 0.3\ndiagnostic_info {\nfields {\nkey: \"Alternative Matched Intents\"\nvalue {\nlist_value {\n}\n}\n}I did train the agent in advance. Appreciate any help",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1666792500000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Dialogflow-cx-v3-DetectIntentRequest-returning-no-match\/td-p\/482545\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-27T10:17:00",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi, mihai527,\n\nI understand that you are experiencing the issue with the transcription which is not responding as expected, and this is a transient issue. Instead of identifying the user age, it is Invoking the No-Match event. Please let me know if I have misunderstood.\n\nHere it seems like an issue with the speech adaption. In regards to that, can you please Enable speech adaptation? The auto speech adaptation feature improves the speech recognition accuracy of your agent by automatically using conversation state to pass relevant entities and training phrases as speech context hints for all detect intent requests. This feature is disabled by default.\n\nYou may follow this doc\u00a0to achieve this. Please let me know if this resolves your issue or not. Also, you may follow this doc, which talks about Voice agent design best practices.\n\nPlease let me know if you have any questions regarding the information provided above. I will be happy to assist you.\n\nI will be looking forward to your response."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"dialogflow detectintentrequest return match try creat manag agent exclus api creat agent intent project locat global agent ddd intent aacd displai test ag train phrase part text part text paramet repeat count train phrase part text ag part text paramet repeat count train phrase part text part text paramet repeat count train phrase part text ag part text paramet repeat count train phrase part text ag part text paramet repeat count train phrase part text ag part text paramet repeat count train phrase part text paramet part text year ag repeat count train phrase part text paramet part text year repeat count train phrase part text paramet part text year old repeat count paramet entiti type project locat agent entitytyp sy number integ paramet entiti type project locat agent entitytyp sy number integ paramet entiti type project locat agent entitytyp sy number integ paramet entiti type project locat agent entitytyp sy number integ paramet entiti type project locat agent entitytyp sy number integ paramet entiti type project locat agent entitytyp sy number integ paramet entiti type project locat agent entitytyp sy number integ paramet entiti type project locat agent entitytyp sy number integ paramet entiti type project locat agent entitytyp sy number integ prioriti try detect intent detectintentrequest shown github sampl get match result queri text detect intent text languag code respons messag text text sorri current page project locat global agent ddd flow page start page displai start page intent detect confid diagnost info field kei altern match intent valu list valu train agent advanc appreci help",
        "Question_preprocessed_content":"dialogflow detectintentrequest return try creat manag agent exclus api creat agent intent part part part part part part part part part paramet paramet paramet paramet paramet paramet paramet paramet paramet prioriti try detect intent detectintentrequest shown github sampl get queri text detect intent text train agent advanc appreci help",
        "Answer_original_content":"mihai understand experienc issu transcript respond expect transient issu instead identifi user ag invok match event let know misunderstood like issu speech adapt regard enabl speech adapt auto speech adapt featur improv speech recognit accuraci agent automat convers state pass relev entiti train phrase speech context hint detect intent request featur disabl default follow docto achiev let know resolv issu follow doc talk voic agent design best practic let know question inform provid happi assist look forward respons",
        "Answer_preprocessed_content":"mihai understand experienc issu transcript respond expect transient issu instead identifi user ag invok event let know misunderstood like issu speech adapt regard enabl speech adapt auto speech adapt featur improv speech recognit accuraci agent automat convers state pass relev entiti train phrase speech context hint detect intent request featur disabl default follow docto achiev let know resolv issu follow doc talk voic agent design best practic let know question inform provid happi assist look forward respons",
        "Question_gpt_summary_original":"the user is encountering challenges with dialogflow cx v3's detectintentrequest, which is returning no-match results despite the agent being trained in advance.",
        "Question_gpt_summary":"user encount challeng dialogflow detectintentrequest return match result despit agent train advanc",
        "Answer_gpt_summary_original":"possible solutions to the issue of no-match results in dialogflow cx v3's detectintentrequest include enabling speech adaptation and following the provided documentation to achieve this. the auto speech adaptation feature can improve speech recognition accuracy by using conversation state to pass relevant entities and training phrases as speech context hints for all detect intent requests. additionally, following the voice agent design best practices may also be helpful.",
        "Answer_gpt_summary":"possibl solut issu match result dialogflow detectintentrequest includ enabl speech adapt follow provid document achiev auto speech adapt featur improv speech recognit accuraci convers state pass relev entiti train phrase speech context hint detect intent request addition follow voic agent design best practic help"
    },
    {
        "Question_id":null,
        "Question_title":"certification test for AI-900: Microsoft Azure AI Fundamentals not available",
        "Question_body":"Hallo, i would like make an appointment for Exam AI-900: Microsoft Azure AI Fundamentals.\nHowever this exam is currently not available at Pearson vue or Certiport. When can i expect this again? Is there an alternative ?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662205880130,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi Jurian,\n\nThis is available in PearsonVue check this. ai-900\n\nAny specific region you are trying from?\n\n\n\n\n\n==\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/992629\/exam-ai-900-microsoft-azure-ai-fundamentals-not-av.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-03T13:15:33.98Z",
                "Answer_score":0,
                "Answer_body":"Hi Jurian,\n\nThis is available in PearsonVue check this. ai-900\n\nAny specific region you are trying from?\n\n\n\n\n\n==\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1662210933980,
        "Question_original_content":"certif test microsoft azur fundament avail hallo like appoint exam microsoft azur fundament exam current avail pearson vue certiport expect altern",
        "Question_preprocessed_content":"certif test microsoft azur fundament avail hallo like appoint exam microsoft azur fundament exam current avail pearson vue certiport expect altern",
        "Answer_original_content":"jurian avail pearsonvu check specif region try accept answer inform help help commun",
        "Answer_preprocessed_content":"jurian avail pearsonvu check specif region try accept answer inform help help commun",
        "Question_gpt_summary_original":"the user is looking to take the ai-900: microsoft azure ai fundamentals certification test, but is unable to find it available at pearson vue or certiport.",
        "Question_gpt_summary":"user look microsoft azur fundament certif test unabl avail pearson vue certiport",
        "Answer_gpt_summary_original":"possible solutions: \n- check pearson vue again for the ai-900: microsoft azure ai fundamentals certification test.\n- specify the region where the user is trying to take the test. \n\nsummary: the answer suggests checking pearson vue again for the certification test and asking for the specific region where the user is trying to take the test.",
        "Answer_gpt_summary":"possibl solut check pearson vue microsoft azur fundament certif test specifi region user try test summari answer suggest check pearson vue certif test ask specif region user try test"
    },
    {
        "Question_id":null,
        "Question_title":"translate api may give different translation variations in response to the same request",
        "Question_body":"I am using Google cloud translate api v3 NMT to translate sentences from English to Hebrew. I noted that for the same source sentence I may get slightly different results on subsequent calls. I see these going back and forth in a short time frame, so it is NOT a result of the model being updated. For my application I would like to either get all possible variations or at least get reproducible results. Is there an option to get all variations or to set the random seed? Or must I resort to multiple polling and caching on my side?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651038840000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":49.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/translate-api-may-give-different-translation-variations-in\/td-p\/417588\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"",
                "Answer_has_accepted":false,
                "Answer_score":0,
                "Answer_body":"Hi, what I would recommend to you is, as you said in your second question, to implement multiple polling and store it on your side so that you have a better knowledge of what's going on on your end."
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"translat api differ translat variat respons request googl cloud translat api nmt translat sentenc english hebrew note sourc sentenc slightli differ result subsequ call go forth short time frame result model updat applic like possibl variat reproduc result option variat set random seed resort multipl poll cach",
        "Question_preprocessed_content":"translat api differ translat variat respons request googl cloud translat api nmt translat sentenc english hebrew note sourc sentenc slightli differ result subsequ call go forth short time frame result model updat applic like possibl variat reproduc result option variat set random seed resort multipl poll cach",
        "Answer_original_content":"recommend said second question implement multipl poll store better knowledg go end",
        "Answer_preprocessed_content":"recommend said second question implement multipl poll store better knowledg go end",
        "Question_gpt_summary_original":"the user is encountering challenges with google cloud translate api v3 nmt, where they are receiving different translation variations in response to the same request.",
        "Question_gpt_summary":"user encount challeng googl cloud translat api nmt receiv differ translat variat respons request",
        "Answer_gpt_summary_original":"possible solution: implement multiple polling and store the translations on your side to have a better understanding of the variations in the translations.",
        "Answer_gpt_summary":"possibl solut implement multipl poll store translat better understand variat translat"
    },
    {
        "Question_id":null,
        "Question_title":"Question: how to define custom Module in Designer",
        "Question_body":"How do I define a custom Module in Azure ML Designer? In Studio (classic), I found this documentation:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/custom-r-modules\n\nbut I have yet to find something similar for the modern version of Azure ML. The closest thing I've found is Execute Python Script, but that has several limitations:\n\nUnnecessary overhead from casting to a pandas DataFrame and back to the internal data structure\n\n\nNo ability to add parameters\n\n\nDifficult to reuse\n\n\nCan't customize number of ports, module name, etc.\n\nAlso, is it possible to upstream any custom Modules I write? For example, there are currently no Data Transforms for handling timestamps, which I imagine are common in many datasets. I would love to contribute one upstream so that other users can use it, but I haven't yet found a GitHub repo for it.",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1594757308663,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/46908\/question-how-to-define-custom-module-in-designer.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-15T08:35:00.713Z",
                "Answer_score":2,
                "Answer_body":"Hi @AdamStewart-2203 , Thanks for your feedback.\nThis is Blanca, working on AML Designer.\nI'd love to set up a call with you to understand our scenario and introduce our new features.\nPlease contact keli19@microsoft.com, since I serched your name but not sure to contact whom....",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-03-12T13:32:32.347Z",
                "Answer_score":0,
                "Answer_body":"Hi, I have the same question. However, it seems that the solution has not been communicated via this blog.\nCould you please share the solution? Many thanks, Koen",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-08-16T11:25:49.477Z",
                "Answer_score":0,
                "Answer_body":"I agree with Koen.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":40.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"question defin custom modul design defin custom modul design studio classic document http doc microsoft com azur machin learn studio custom modul similar modern version closest thing execut python script limit unnecessari overhead cast panda datafram intern data structur abil add paramet difficult reus custom number port modul possibl upstream custom modul write exampl current data transform handl timestamp imagin common dataset love contribut upstream user us haven github repo",
        "Question_preprocessed_content":"question defin custom modul design defin custom modul design studio document similar modern version closest thing execut python script limit unnecessari overhead cast panda datafram intern data structur abil add paramet difficult reus custom number port modul possibl upstream custom modul write exampl current data transform handl timestamp imagin common dataset love contribut upstream user us haven github repo",
        "Answer_original_content":"adamstewart thank feedback blanca work aml design love set understand scenario introduc new featur contact keli microsoft com serch sure contact question solut commun blog share solut thank koen agre koen",
        "Answer_preprocessed_content":"thank feedback blanca work aml design love set understand scenario introduc new featur contact serch sure contact question solut commun blog share solut thank koen agre koen",
        "Question_gpt_summary_original":"the user is facing challenges in defining custom modules in designer, including difficulty in reusing modules, customizing ports and module names, and upstreaming custom modules.",
        "Question_gpt_summary":"user face challeng defin custom modul design includ difficulti reus modul custom port modul name upstream custom modul",
        "Answer_gpt_summary_original":"there are no specific solutions mentioned in the answer. the response is an offer to set up a call to understand the user's scenario and introduce new features. two other users also express similar concerns but no solutions are provided.",
        "Answer_gpt_summary":"specif solut mention answer respons offer set understand user scenario introduc new featur user express similar concern solut provid"
    },
    {
        "Question_id":null,
        "Question_title":"AML Hyper Drive - Stuck",
        "Question_body":"I am using a HyperDriveStep in a Pipeline and often get stuck in HyperDriveStep.\nI am using our company subscription and we have sufficient cores allocated for our ML development.\n\nWe have plan to use this on production very soon. Is it production ready (HyperDriveStep in AML)....!!!??\n\n\n\n\nHere is the HyperDrive Execution log: I have canceled the child run after 16 hours of waiting......!!!!!\n\n[2021-11-02 16:01:15Z] Submitting 1 runs, first five are: 5523baba:941f5822-2f1f-4075-a475-5980aa8c6d45\n[2021-11-02 16:03:18Z] Completing processing run id 941f5822-2f1f-4075-a475-5980aa8c6d45.\n[2021-11-02 16:03:18Z] Submitting 1 runs, first five are: 059c259e:7b1e6c57-6469-43cf-8ca7-f05837f0a8fc\n[2021-11-03 08:11:23Z] Execution of experiment canceled, update experiment status and cancel submitted nodes\n\n\n\n\nMy Run ID is : 7e90bbd9-c843-46e0-a611-7810d15a8c92\n\nIs there anyone can help us on this....!!!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1635931220580,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/613972\/aml-hyper-drive-stuck.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-03T12:11:29.953Z",
                "Answer_score":1,
                "Answer_body":"@MohammadZahirulIslam-7110 Thanks for the question. Please share details of your experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue from the back-end? This option is available from the top right hand corner of the portal by clicking the smiley face, Please select the option Microsoft can email you about the feedback along with a screen shot so our service team can lookup and advise through email.\nFor an example of using HyperDriveStep, see the notebook https:\/\/aka.ms\/pl-hyperdrive and Azure Machine Learning Pipeline with HyperDriveStep.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"aml hyper drive stuck hyperdrivestep pipelin stuck hyperdrivestep compani subscript suffici core alloc develop plan us product soon product readi hyperdrivestep aml hyperdr execut log cancel child run hour wait submit run baba aacd complet process run aacd submit run bec ffafc execut experi cancel updat experi statu cancel submit node run ebbd dac help",
        "Question_preprocessed_content":"aml hyper drive stuck hyperdrivestep pipelin stuck hyperdrivestep compani subscript suffici core alloc develop plan us product soon product readi hyperdr execut log cancel child run hour submit run complet process run submit run execut experi cancel updat experi statu cancel submit node run help",
        "Answer_original_content":"mohammadzahirulislam thank question share detail experi issu azur com portal servic engin lookup issu end option avail right hand corner portal click smilei face select option microsoft email feedback screen shot servic team lookup advis email exampl hyperdrivestep notebook http aka hyperdr pipelin hyperdrivestep",
        "Answer_preprocessed_content":"thank question share detail experi issu portal servic engin lookup issu option avail right hand corner portal click smilei face select option microsoft email feedback screen shot servic team lookup advis email exampl hyperdrivestep notebook pipelin hyperdrivestep",
        "Question_gpt_summary_original":"the user is encountering challenges with their aml hyperdrive pipeline, as they are often getting stuck and have had to cancel a child run after 16 hours of waiting.",
        "Question_gpt_summary":"user encount challeng aml hyperdr pipelin get stuck cancel child run hour wait",
        "Answer_gpt_summary_original":"the answer suggests that the user should share details of their experiment and issue from the ml.azure.com portal for a service engineer to lookup the issue from the back-end. the user can do this by clicking the smiley face in the top right-hand corner of the portal and selecting the option for microsoft to email them about the feedback along with a screenshot. additionally, the answer provides a link to a notebook and pipeline with hyperdrivestep as an example.",
        "Answer_gpt_summary":"answer suggest user share detail experi issu azur com portal servic engin lookup issu end user click smilei face right hand corner portal select option microsoft email feedback screenshot addition answer provid link notebook pipelin hyperdrivestep exampl"
    },
    {
        "Question_id":49374476.0,
        "Question_title":"How do I call a SageMaker Endpoint using the AWS CLI (",
        "Question_body":"<p>I'm trying to invoke the iris endpointfrom the <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\/tensorflow_iris_dnn_classifier_using_estimators.ipynb\" rel=\"nofollow noreferrer\">SageMaker example notebooks<\/a> using the aws cli. I've tried using the following command:<\/p>\n\n<pre><code>!aws sagemaker-runtime invoke-endpoint \\\n--endpoint-name sagemaker-tensorflow-py2-cpu-2018-03-19-21-27-52-956 \\\n--body \"[6.4, 3.2, 4.5, 1.5]\" \\\n--content-type \"application\/json\" \\\noutput.json\n<\/code><\/pre>\n\n<p>I get the following response:<\/p>\n\n<pre><code>{\n    \"InvokedProductionVariant\": \"AllTraffic\", \n    \"ContentType\": \"*\/*\"\n}\n<\/code><\/pre>\n\n<p>What am I doing wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1521505092823,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":4027.0,
        "Answer_body":"<p>If you've gotten that response, your request is successful. The output should be in the output file you specified - output.json :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49374476",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1521567781630,
        "Question_original_content":"endpoint aw cli try invok iri endpointfrom exampl notebook aw cli tri follow command runtim invok endpoint endpoint tensorflow cpu bodi content type applic json output json follow respons invokedproductionvari alltraff contenttyp wrong",
        "Question_preprocessed_content":"endpoint aw cli try invok iri endpointfrom exampl notebook aw cli tri follow command follow respons wrong",
        "Answer_original_content":"gotten respons request success output output file specifi output json",
        "Answer_preprocessed_content":"gotten respons request success output output file specifi",
        "Question_gpt_summary_original":"the user is encountering challenges calling an endpoint using the aws cli, resulting in an unexpected response.",
        "Question_gpt_summary":"user encount challeng call endpoint aw cli result unexpect respons",
        "Answer_gpt_summary_original":"the answer does not provide any solutions to the challenge of encountering unexpected responses while calling an endpoint using the aws cli. however, it suggests that if the user has received a response, the request is successful and the output should be in the specified output file.",
        "Answer_gpt_summary":"answer provid solut challeng encount unexpect respons call endpoint aw cli suggest user receiv respons request success output specifi output file"
    },
    {
        "Question_id":null,
        "Question_title":"[BUG] Config param name inclusing a dot",
        "Question_body":"<p>I have a config param with the following name:<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555.png\" data-download-href=\"\/uploads\/short-url\/dqR4ex4NYNlpeVlIDbLPCuUMzPL.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555_2_690x152.png\" alt=\"image\" data-base62-sha1=\"dqR4ex4NYNlpeVlIDbLPCuUMzPL\" width=\"690\" height=\"152\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555_2_690x152.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/5e256d34409e2dff601a90b30f46f777c31ca555_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">893\u00d7198 10.5 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>However, when I call him in my graph, there is a bug and the graph stays white:<\/p>            <div class=\"onebox imgur-album\">\n              <a href=\"https:\/\/imgur.com\/a\/EIML6VO\" target=\"_blank\" rel=\"noopener nofollow ugc\">\n                <span class=\"outer-box\" style=\"width:600px\">\n                  <span class=\"inner-box\">\n                    <span class=\"album-title\">[Album] imgur.com<\/span>\n                  <\/span>\n                <\/span>\n                <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/2f6fef7dba514e4297115fb6caaa0d05a941ddf4.jpeg\" title=\"imgur.com\" height=\"315\" width=\"600\">\n              <\/a>\n            <\/div>\n",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1633963376319,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":310.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/bug-config-param-name-inclusing-a-dot\/916",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-10-14T09:59:15.348Z",
                "Answer_body":"<p>Hi,<br>\nThanks for posting that here. I\u2019ve been able to reproduce this and will pass it on to get it fixed.<br>\nSorry for any inconvenience this has caused.<br>\nThanks again <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-10-15T14:34:30.068Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/boilario\">@boilario<\/a>,<br>\nwe have created a ticket for this bug. The team will look into this soon. I\u2019ll notify you once this is fixed.<br>\nThank you for your patience<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:01:40.993Z",
                "Answer_body":"<p>Hi,<br>\nThanks for posting that here. I\u2019ve been able to reproduce this and will pass it on to get it fixed.<br>\nSorry for any inconvenience this has caused.<br>\nThanks again <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" alt=\":slight_smile:\" title=\":slight_smile:\"><\/p>\n<p>[Discourse post]<\/p>",
                "Answer_score":0.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-20T18:02:06.517Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"bug config param inclus dot config param follow imag graph bug graph stai white album imgur com",
        "Question_preprocessed_content":"config param inclus dot config param follow imag graph bug graph stai white album",
        "Answer_original_content":"thank post iv abl reproduc pass fix sorri inconveni caus thank hei boilario creat ticket bug team look soon ill notifi fix thank patienc thank post iv abl reproduc pass fix sorri inconveni caus thank discours post topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thank post iv abl reproduc pass fix sorri inconveni caus thank hei creat ticket bug team look soon ill notifi fix thank patienc thank post iv abl reproduc pass fix sorri inconveni caus thank discours post topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user encountered a bug when attempting to call a config param with a name including a dot in their graph, resulting in the graph staying white.",
        "Question_gpt_summary":"user encount bug attempt config param includ dot graph result graph stai white",
        "Answer_gpt_summary_original":"there is a bug in the user's graph that causes it to stay white when attempting to call a config param with a name including a dot. the answer acknowledges the issue and assures the user that a ticket has been created for the bug and the team will look into it soon. the user is thanked for their patience.",
        "Answer_gpt_summary":"bug user graph caus stai white attempt config param includ dot answer acknowledg issu assur user ticket creat bug team look soon user thank patienc"
    },
    {
        "Question_id":null,
        "Question_title":"Azure Machine Learning: Update Realtime endpoint",
        "Question_body":"Hi,\n\nI've deployed an endpoint in Azure ML with the command \"az ml model deploy\" and it has created an realtime endpoint (in my case based on AKS cluster type).\n\nNow I want to update this endpoint, because for example I want to change some configuration or code in the score.py. If I try to relaunch the same command above with the option --overwrite, it prints an error regarding the unavailabilty of CPU and memory, even though their configuration is the same as the previous deployment.\n\n\n\n\n{'Azure-cli-ml Version': '1.20.0', 'Error': WebserviceException:\nMessage: Deployment request failed due to insufficient compute resource. For the specified compute target, 1 replica cannot be created per specified CPU\/Memory configuration(3 CPU Cores, 20GB Memory). You can address this problem by adjusting number of replicas, using a different CPU\/memory configuration, or using a different compute target.\nInnerException None\nErrorResponse\n{\n\"error\": {\n\"message\": \"Deployment request failed due to insufficient compute resource. For the specified compute target, 1 replica cannot be created per specified CPU\/Memory configuration(3 CPU Cores, 20GB Memory). You can address this problem by adjusting number of replicas, using a different CPU\/memory configuration, or using a different compute target.\"\n}\n}}\n\n\n\n\nI therefore think that it is not overwriting the endpoint, but creating a parallel environment.\n\nI also tried to create a new version of the endpoint with the commands \"az ml endpoint realtime create-version\" and \"az ml endpoint realtime update-version\", but in this case it always tells me that it doesn't find the endpoint I'm trying to update (despite with the command \"list\" it finds me exactly my endpoint).\n\nError Message:\n\n{\n\"Azure-cli-ml Version\": \"1.20.0\",\n\"Error\": {\n\"Error\": \"Error, no service\/endpoint with name <endpoint-name> found in workspace <workspace-name> in resource group <resource-group-name> of type aksendpoint.\"\n}\n}\n\nSo, how can I overwrite my endpoint? I hope there is a better solution than manually deleting the endpoint each time and recreating it.\n\n\n\n\nThank you very much,\n\nG.",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1611847882743,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/249335\/azure-machine-learning-update-realtime-endpoint.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-29T12:01:04.773Z",
                "Answer_score":0,
                "Answer_body":"@GCocci Thanks, You can use the AKS recipes for real time inference and alternatively take the ParallelRunStep approach for handling offline batch inferences for many models.\n\nThe approaches are captured in the solution accelerator here:\nhttps:\/\/github.com\/microsoft\/solution-accelerator-many-models\n\nHere are the steps to Group all models into a single routing endpoint.\nWe can now group all the services into a single entry point, so that we don't have to handle each endpoint separately. For that, we'll register the endpoints object as a model, and deploy it as a webservice. This webservice will receive the incoming requests and route them to the appropiate model service, acting as the unique entry point for outside requests.\n\nRegister endpoints dict as an AML model\n\n\n\n\nimport joblib\n\n joblib.dump(models_deployed, 'models_deployed.pkl')\n    \n dep_model = Model.register(\n     workspace=ws, \n     model_path ='models_deployed.pkl', \n     model_name='deployed_models_info',\n     tags={'ModelType': '_meta_'},\n     description='Dictionary of the service endpoint where each model is deployed'\n )\n\n\n\nDeploy routing webservice\n\nfrom azureml.core import Environment\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.runconfig import DEFAULT_CPU_IMAGE\nrouting_env = Environment(name=\"many_models_routing_environment\")\nrouting_env_deps = CondaDependencies.create(pip_packages=['azureml-defaults', 'joblib'])\nrouting_env.python.conda_dependencies = routing_env_deps\n\n  routing_infconfig = InferenceConfig(\n         entry_script='routing_webservice.py',\n         source_directory='.\/scripts',\n         environment=routing_env\n     )\n\n\n\nReuse deployment config with lower capacity\n\n deployment_config.cpu_cores = 0.1\n deployment_config.memory_gb = 0.5\n    \n routing_service = Model.deploy(\n     workspace=ws,\n     name='routing-manymodels',\n     models=[dep_model],\n     inference_config=routing_infconfig,\n     deployment_config=deployment_config,\n     deployment_target=deployment_target,\n     overwrite=True\n )\n routing_service.wait_for_deployment(show_output=True)\n    \n assert routing_service.state == 'Healthy'\n    \n print('Routing endpoint deployed with URL: {}'.format(routing_service.scoring_uri))",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-25T14:51:13.933Z",
                "Answer_score":1,
                "Answer_body":"I solved it by rewriting all the code in Python and using the AKSWebService.update_endpoint method to update the endpoint without having to delete it each time (which was happening using the Model.deploy method).\n\nThanks",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":8.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"updat realtim endpoint deploi endpoint command model deploi creat realtim endpoint case base ak cluster type want updat endpoint exampl want chang configur code score try relaunch command option overwrit print error unavailabilti cpu memori configur previou deploy azur cli version error webserviceexcept messag deploy request fail insuffici comput resourc specifi comput target replica creat specifi cpu memori configur cpu core memori address problem adjust number replica differ cpu memori configur differ comput target innerexcept errorrespons error messag deploy request fail insuffici comput resourc specifi comput target replica creat specifi cpu memori configur cpu core memori address problem adjust number replica differ cpu memori configur differ comput target think overwrit endpoint creat parallel environ tri creat new version endpoint command endpoint realtim creat version endpoint realtim updat version case tell endpoint try updat despit command list find exactli endpoint error messag azur cli version error error error servic endpoint workspac resourc group type aksendpoint overwrit endpoint hope better solut manual delet endpoint time recreat thank",
        "Question_preprocessed_content":"updat realtim endpoint deploi endpoint command model deploi creat realtim endpoint want updat endpoint exampl want chang configur code try relaunch command option print error unavailabilti cpu memori configur previou deploy version error webserviceexcept messag deploy request fail insuffici comput resourc specifi comput target replica creat specifi configur address problem adjust number replica differ configur differ comput target innerexcept errorrespons error think overwrit endpoint creat parallel environ tri creat new version endpoint command endpoint realtim endpoint realtim case tell endpoint try updat error messag version error overwrit endpoint hope better solut manual delet endpoint time recreat thank",
        "Answer_original_content":"gcocci thank us ak recip real time infer altern parallelrunstep approach handl offlin batch infer model approach captur solut acceler http github com microsoft solut acceler model step group model singl rout endpoint group servic singl entri point handl endpoint separ regist endpoint object model deploi webservic webservic receiv incom request rout appropi model servic act uniqu entri point outsid request regist endpoint dict aml model import joblib joblib dump model deploi model deploi pkl dep model model regist workspac model path model deploi pkl model deploi model info tag modeltyp meta descript dictionari servic endpoint model deploi deploi rout webservic core import environ core conda depend import condadepend core runconfig import default cpu imag rout env environ model rout environ rout env dep condadepend creat pip packag default joblib rout env python conda depend rout env dep rout infconfig inferenceconfig entri script rout webservic sourc directori script environ rout env reus deploy config lower capac deploy config cpu core deploy config memori rout servic model deploi workspac rout manymodel model dep model infer config rout infconfig deploy config deploy config deploy target deploy target overwrit true rout servic wait deploy output true assert rout servic state healthi print rout endpoint deploi url format rout servic score uri solv rewrit code python akswebservic updat endpoint method updat endpoint have delet time happen model deploi method thank",
        "Answer_preprocessed_content":"thank us ak recip real time infer altern parallelrunstep approach handl offlin batch infer model approach captur solut acceler step group model singl rout endpoint group servic singl entri point handl endpoint separ regist endpoint object model deploi webservic webservic receiv incom request rout appropi model servic act uniqu entri point outsid request regist endpoint dict aml model import joblib workspac tag modeltyp descript dictionari servic endpoint model deploi deploi rout webservic core import environ import condadepend import joblib inferenceconfig reus deploy config lower capac workspac overwrit true assert healthi print solv rewrit code python method updat endpoint have delet time thank",
        "Question_gpt_summary_original":"the user is encountering challenges with updating their realtime endpoint, as they are receiving errors regarding insufficient compute resources and not being able to find the endpoint they are trying to update.",
        "Question_gpt_summary":"user encount challeng updat realtim endpoint receiv error insuffici comput resourc abl endpoint try updat",
        "Answer_gpt_summary_original":"possible solutions to the challenge of updating a realtime endpoint include using aks recipes for real-time inference and taking the parallelrunstep approach for handling offline batch inferences for many models. the solution accelerator provides steps to group all models into a single routing endpoint, which involves registering the endpoints object as a model and deploying it as a webservice. the webservice will receive incoming requests and route them to the appropriate model service, acting as the unique entry point for outside requests. the code can be rewritten in python, and the akswebservice.update_endpoint method can be used to update the endpoint without having to delete it each time.",
        "Answer_gpt_summary":"possibl solut challeng updat realtim endpoint includ ak recip real time infer take parallelrunstep approach handl offlin batch infer model solut acceler provid step group model singl rout endpoint involv regist endpoint object model deploi webservic webservic receiv incom request rout appropri model servic act uniqu entri point outsid request code rewritten python akswebservic updat endpoint method updat endpoint have delet time"
    },
    {
        "Question_id":null,
        "Question_title":"Question on Azure Features and Limitations for Free and Paid Version",
        "Question_body":"Hi,\n\nI am a student from University of Wollonggong Malaysia KDU Penang. I am planning to do a final year project that utilises machine learning to perform image and handwriting recognition using cloud computing. Therefore I would like to understand if there are related products that are provided on Azure. For the free tier and\/or Azure for student, what are the available product(s) that may satisfy my project requirements and what are its limitations. Similarly for the paid version, what additional features are provided and how are the pricings calculated?\n\nThank you",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630749021297,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/539984\/question-on-azure-features-and-limitations-for-fre.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-04T10:37:07.907Z",
                "Answer_score":0,
                "Answer_body":"Hello @TANFANGSHEENUOWMKDU-6628 !\n\n\n\n\nYou can refer to the below mentioned topics to know about the Limitations of Free , Pay as You Go and Student Package of Azure.\n\nStudents Offering :-\nhttps:\/\/azure.microsoft.com\/en-us\/free\/students\/\n\nAzure Pay as You Go :-\nhttps:\/\/azure.microsoft.com\/en-in\/pricing\/purchase-options\/pay-as-you-go\/\n\nAzure Free :-\nhttps:\/\/azure.microsoft.com\/en-in\/free\/\n\nAzure subscription and service limits, quotas, and constraints :-\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/azure-subscription-service-limits\n\n\n\n\n\n\n\nIf the response is helpful, please click \"Accept Answer\" and upvote it.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"question azur featur limit free paid version student univers wollonggong malaysia kdu penang plan final year project utilis machin learn perform imag handwrit recognit cloud comput like understand relat product provid azur free tier azur student avail product satisfi project requir limit similarli paid version addit featur provid price calcul thank",
        "Question_preprocessed_content":"question azur featur limit free paid version student univers wollonggong malaysia kdu penang plan final year project utilis machin learn perform imag handwrit recognit cloud comput like understand relat product provid azur free tier azur student avail product satisfi project requir limit similarli paid version addit featur provid price calcul thank",
        "Answer_original_content":"hello tanfangsheenuowmkdu refer mention topic know limit free pai student packag azur student offer http azur microsoft com free student azur pai http azur microsoft com price purchas option pai azur free http azur microsoft com free azur subscript servic limit quota constraint http doc microsoft com azur azur resourc manag manag azur subscript servic limit respons help click accept answer upvot",
        "Answer_preprocessed_content":"hello refer mention topic know limit free pai student packag azur student offer azur pai azur free azur subscript servic limit quota constraint respons help click accept answer upvot",
        "Question_gpt_summary_original":"the user is seeking information on the features and limitations of azure's free and paid versions for a machine learning project involving image and handwriting recognition.",
        "Question_gpt_summary":"user seek inform featur limit azur free paid version machin learn project involv imag handwrit recognit",
        "Answer_gpt_summary_original":"the answer provides links to resources that explain the limitations of azure's free, pay as you go, and student packages, as well as subscription and service limits, quotas, and constraints.",
        "Answer_gpt_summary":"answer provid link resourc explain limit azur free pai student packag subscript servic limit quota constraint"
    },
    {
        "Question_id":70873792.0,
        "Question_title":"How to handle Sagemaker Batch Transform discarding a file with a failed model request",
        "Question_body":"<p>I have a large number of JSON requests for a model split across multiple files in an S3 bucket. I would like to use Sagemaker's Batch Transform feature to process all of these requests (I have done a couple of test runs using small amounts of data and the transform job succeeds). My main issue is here (<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html#batch-transform-errors<\/a>), specifically:<\/p>\n<blockquote>\n<p>If a batch transform job fails to process an input file because of a problem with the dataset, SageMaker marks the job as failed. If an input file contains a bad record, the transform job doesn't create an output file for that input file because doing so prevents it from maintaining the same order in the transformed data as in the input file. When your dataset has multiple input files, a transform job continues to process input files even if it fails to process one. The processed files still generate useable results.<\/p>\n<\/blockquote>\n<p>This is not preferable mainly because if 1 request fails (whether its a transient error, a malformmated request, or something wrong with the model container) in a file with a large number of requests, all of those requests will get discarded (even if all of them succeeded and the last one failed). I would ideally prefer Sagemaker to just write the output of the failed response to the file and keep going, rather than discarding the entire file.<\/p>\n<p>My question is, are there any suggestions to mitigating this issue? I was thinking about storing 1 request per file in S3, but this seems somewhat ridiculous? Even if I did this, is there a good way of seeing which requests specifically failed after the transform job finishes?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643261540133,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":357.0,
        "Answer_body":"<p>You've got the right idea: the fewer datapoints are in each file, the less likely a given file is to fail. The issue is that while you can pass a prefix with many files to <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTransformJob.html#SageMaker-CreateTransformJob-request-MaxPayloadInMB\" rel=\"nofollow noreferrer\">CreateTransformJob<\/a>, partitioning one datapoint per file at least requires an S3 read per datapoint, plus a model invocation per datapoint, which is probably not great. Be aware also that <a href=\"https:\/\/forums.aws.amazon.com\/thread.jspa?messageID=1000415&amp;tstart=0\" rel=\"nofollow noreferrer\">apparently there are hidden rate limits<\/a>.<\/p>\n<p>Here are a couple options:<\/p>\n<ol>\n<li><p>Partition into small-ish files, and plan on failures being rare. Hopefully, not many of your datapoints would actually fail. If you partition your dataset into e.g. 100 files, then a single failure only requires reprocessing 1% of your data. Note that Sagemaker has built-in retries, too, so most of the time failures should be caused by your data\/logic, not randomness on Sagemaker's side.<\/p>\n<\/li>\n<li><p>Deal with failures directly in your model. The same doc you quoted in your question also says:<\/p>\n<\/li>\n<\/ol>\n<blockquote>\n<p>If you are using your own algorithms, you can use placeholder text, such as ERROR, when the algorithm finds a bad record in an input file. For example, if the last record in a dataset is bad, the algorithm places the placeholder text for that record in the output file.<\/p>\n<\/blockquote>\n<p>Note that the reason Batch Transform does this whole-file failure is to maintain a 1-1 mapping between rows in the input and the output. If you can substitute the output for failed datapoints with an error message from inside your model, without actually causing the model itself to fail processing, Batch Transform will be happy.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1644528409280,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70873792",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1644424499008,
        "Question_original_content":"handl batch transform discard file fail model request larg number json request model split multipl file bucket like us batch transform featur process request coupl test run small amount data transform job succe main issu http doc aw amazon com latest batch transform html batch transform error specif batch transform job fail process input file problem dataset mark job fail input file contain bad record transform job creat output file input file prevent maintain order transform data input file dataset multipl input file transform job continu process input file fail process process file gener useabl result prefer mainli request fail transient error malformm request wrong model contain file larg number request request discard succeed fail ideal prefer write output fail respons file go discard entir file question suggest mitig issu think store request file somewhat ridicul good wai see request specif fail transform job finish",
        "Question_preprocessed_content":"handl batch transform discard file fail model request larg number json request model split multipl file bucket like us batch transform featur process request main issu specif batch transform job fail process input file problem dataset mark job fail input file contain bad record transform job creat output file input file prevent maintain order transform data input file dataset multipl input file transform job continu process input file fail process process file gener useabl result prefer mainli request fail file larg number request request discard ideal prefer write output fail respons file go discard entir file question suggest mitig issu think store request file somewhat ridicul good wai see request specif fail transform job finish",
        "Answer_original_content":"got right idea fewer datapoint file like given file fail issu pass prefix file createtransformjob partit datapoint file requir read datapoint plu model invoc datapoint probabl great awar appar hidden rate limit coupl option partit small ish file plan failur rare hopefulli datapoint actual fail partit dataset file singl failur requir reprocess data note built retri time failur caus data logic random deal failur directli model doc quot question sai algorithm us placehold text error algorithm find bad record input file exampl record dataset bad algorithm place placehold text record output file note reason batch transform file failur maintain map row input output substitut output fail datapoint error messag insid model actual caus model fail process batch transform happi",
        "Answer_preprocessed_content":"got right idea fewer datapoint file like given file fail issu pass prefix file createtransformjob partit datapoint file requir read datapoint plu model invoc datapoint probabl great awar appar hidden rate limit coupl option partit file plan failur rare hopefulli datapoint actual fail partit dataset file singl failur requir reprocess data note retri time failur caus random deal failur directli model doc quot question sai algorithm us placehold text error algorithm find bad record input file exampl record dataset bad algorithm place placehold text record output file note reason batch transform failur maintain map row input output substitut output fail datapoint error messag insid model actual caus model fail process batch transform happi",
        "Question_gpt_summary_original":"the user is facing a challenge of how to handle batch transform discarding a file with a failed model request, and is looking for suggestions to mitigate this issue.",
        "Question_gpt_summary":"user face challeng handl batch transform discard file fail model request look suggest mitig issu",
        "Answer_gpt_summary_original":"the answer suggests two possible solutions to mitigate the issue of batch transform discarding a file with a failed model request. the first solution is to partition the dataset into small-ish files, and plan on failures being rare. the second solution is to deal with failures directly in the model by using placeholder text, such as error, when the algorithm finds a bad record in an input file.",
        "Answer_gpt_summary":"answer suggest possibl solut mitig issu batch transform discard file fail model request solut partit dataset small ish file plan failur rare second solut deal failur directli model placehold text error algorithm find bad record input file"
    },
    {
        "Question_id":null,
        "Question_title":"How to connect to KeyVault to azureml real time endpoint using managed identity?",
        "Question_body":"I am trying to connect keyvault to an insanely simple app with score file as follows:\n\n def init():\n     pass\n def run(raw_data):\n     KVUri=\"<<AN ACTUAL KEYVAULT URI>>\"\n     credential = DefaultAzureCredential()\n     client = SecretClient(vault_url=KVUri, credential=credential)\n     retrieved_secret = client.get_secret(\"test-secret\")\n     return  [retrieved_secret]\n\n\n\nI deploy using python SDK as follows:\n\n # create an online endpoint\n endpoint = ManagedOnlineEndpoint(\n     name=local_endpoint_name,\n     description=\"this is a sample online endpoint\",\n     auth_mode=\"key\",\n )\n ml_client.begin_create_or_update(endpoint)\n model = Model(path=\"..\/model\/dummy.txt\")\n env = Environment(\n     conda_file=\".\/conda.yaml\",\n     image=\"mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:20210727.v1\",\n )\n    \n blue_deployment = ManagedOnlineDeployment(\n     name=\"blue\",\n     endpoint_name=local_endpoint_name,\n     model=model,\n     environment=env,\n     code_configuration=CodeConfiguration(\n         code=\".\", scoring_script=\"app.py\"\n     ),\n     instance_type=\"Standard_F2s_v2\",\n     instance_count=1,\n )\n ml_client.begin_create_or_update(blue_deployment)\n\n\n\nAfter the deployment I add System assigned managed identity to azure key vault (Access Controls IAM > grant access to this resource). I assign access to instance of online endpoint, but when testing I get:\n\nFailed to test real-time endpoint\nDefaultAzureCredential failed to retrieve a token from the included credentials. Attempted credentials: EnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured. Visit https:\/\/aka.ms\/azsdk\/python\/identity\/environmentcredential\/troubleshoot to troubleshoot.this issue. ManagedIdentityCredential: request() got an unexpected keyword argument 'tenant_id' To mitigate this issue, please refer to the troubleshooting guidelines here at https:\/\/aka.ms\/azsdk\/python\/identity\/defaultazurecredential\/troubleshoot.\n\nDo you know what is the source of an error? How should I do it correctly?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1660172274087,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/963249\/how-to-connect-to-keyvault-to-azureml-real-time-en.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-10T23:45:33.143Z",
                "Answer_score":0,
                "Answer_body":"Hi @MateuszMacias-3397\n\nAs mentioned in your error : Attempted credentials: EnvironmentCredential: EnvironmentCredential authentication unavailable. Environment variables are not fully configured.\n\nYou are missing to mention some configuration in your python code\nNo where In your code its mention to retrieve the secret from which client , Mention the client ID or which tenant. This image from the same URL you mentioned.\n\nIm not from Developers side but thought if i could help.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":20.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"connect keyvault real time endpoint manag ident try connect keyvault insan simpl app score file follow def init pass def run raw data kvuri credenti defaultazurecredenti client secretcli vault url kvuri credenti credenti retriev secret client secret test secret return retriev secret deploi python sdk follow creat onlin endpoint endpoint managedonlineendpoint local endpoint descript sampl onlin endpoint auth mode kei client begin creat updat endpoint model model path model dummi txt env environ conda file conda yaml imag mcr microsoft com openmpi ubuntu blue deploy managedonlinedeploy blue endpoint local endpoint model model environ env code configur codeconfigur code score script app instanc type standard instanc count client begin creat updat blue deploy deploy add assign manag ident azur kei vault access control iam grant access resourc assign access instanc onlin endpoint test fail test real time endpoint defaultazurecredenti fail retriev token includ credenti attempt credenti environmentcredenti environmentcredenti authent unavail environ variabl fulli configur visit http aka azsdk python ident environmentcredenti troubleshoot troubleshoot issu managedidentitycredenti request got unexpect keyword argument tenant mitig issu refer troubleshoot guidelin http aka azsdk python ident defaultazurecredenti troubleshoot know sourc error correctli",
        "Question_preprocessed_content":"connect keyvault real time endpoint manag ident try connect keyvault insan simpl app score file follow def init pass def kvuri credenti defaultazurecredenti client credenti credenti return deploi python sdk follow creat onlin endpoint endpoint managedonlineendpoint model env environ managedonlinedeploy deploy add assign manag ident azur kei vault assign access instanc onlin endpoint test fail test endpoint defaultazurecredenti fail retriev token includ credenti attempt credenti environmentcredenti environmentcredenti authent unavail environ variabl fulli configur visit issu managedidentitycredenti request got unexpect keyword argument mitig issu refer troubleshoot guidelin know sourc error correctli",
        "Answer_original_content":"mateuszmacia mention error attempt credenti environmentcredenti environmentcredenti authent unavail environ variabl fulli configur miss mention configur python code code mention retriev secret client mention client tenant imag url mention develop thought help",
        "Answer_preprocessed_content":"mention error attempt credenti environmentcredenti environmentcredenti authent unavail environ variabl fulli configur miss mention configur python code code mention retriev secret client mention client tenant imag url mention develop thought help",
        "Question_gpt_summary_original":"the user is encountering challenges connecting to keyvault using a managed identity and deploying an online endpoint with a score file.",
        "Question_gpt_summary":"user encount challeng connect keyvault manag ident deploi onlin endpoint score file",
        "Answer_gpt_summary_original":"possible solutions from the answer include checking the python code for missing configurations related to retrieving secrets from keyvault, specifying the client id or tenant, and ensuring that environment variables are fully configured.",
        "Answer_gpt_summary":"possibl solut answer includ check python code miss configur relat retriev secret keyvault specifi client tenant ensur environ variabl fulli configur"
    },
    {
        "Question_id":65824766.0,
        "Question_title":"SSH automation in jenkins",
        "Question_body":"<p>So I've been working on the automation of processes and it includes fetching data from an external source through DVC(data version control) for which I am using SSH client to pull and push changes. For automation, I'm using <strong>Jenkins<\/strong> and the problem I'm facing is that for ssh we need to give a password on runtime, and in automation that's not an option. I've tried multiple ways to specify passwords for ssh like sshpass and ssh config but it turns out Jenkins when building creates some file name <strong>script.sh<\/strong> in a directory <em>repoName@tmp<\/em> in var\/lib\/jenkins\/.... and therefore it is giving permission denied error. no matter what I try. If anyone could give any suggestions to this problem it would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611222255303,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":121.0,
        "Answer_body":"<p>You could use key-based auth for SSH instead instead of password auth so that your Jenkins user can access your SSH DVC remote without needing to specify a password.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65824766",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1611228029327,
        "Question_original_content":"ssh autom jenkin work autom process includ fetch data extern sourc data version control ssh client pull push chang autom jenkin problem face ssh need password runtim autom option tri multipl wai specifi password ssh like sshpass ssh config turn jenkin build creat file script directori reponam tmp var lib jenkin give permiss deni error matter try suggest problem appreci",
        "Question_preprocessed_content":"ssh autom jenkin work autom process includ fetch data extern sourc ssh client pull push chang autom jenkin problem face ssh need password runtim autom option tri multipl wai specifi password ssh like sshpass ssh config turn jenkin build creat file directori reponam give permiss deni error matter try suggest problem appreci",
        "Answer_original_content":"us kei base auth ssh instead instead password auth jenkin user access ssh remot need specifi password",
        "Answer_preprocessed_content":"us auth ssh instead instead password auth jenkin user access ssh remot need specifi password",
        "Question_gpt_summary_original":"the user is facing a challenge with automating processes using jenkins, where they need to provide a password for ssh on runtime but are unable to do so due to permission denied errors.",
        "Question_gpt_summary":"user face challeng autom process jenkin need provid password ssh runtim unabl permiss deni error",
        "Answer_gpt_summary_original":"the solution to the challenge of providing a password for ssh on runtime while automating processes using jenkins is to use key-based authentication instead of password authentication. this will allow the jenkins user to access the ssh remote without needing to specify a password, thus avoiding permission denied errors.",
        "Answer_gpt_summary":"solut challeng provid password ssh runtim autom process jenkin us kei base authent instead password authent allow jenkin user access ssh remot need specifi password avoid permiss deni error"
    },
    {
        "Question_id":null,
        "Question_title":"Set y ranges in Parallel Coordinates plot",
        "Question_body":"<p>I want to compare many runs through a parallel coordinates plot and I need to evaluate them in terms of different metrics.<br>\nWhat I get by default when creating it manually is the following (where the various kinds of AUC scores is what I am interested in):<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/8\/81cde959eb67c03e8cd28d5c2f65b305ce1d20e8.png\" data-download-href=\"\/uploads\/short-url\/iwiKWiM6XBlEBWoJ1rFVitBIl5e.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/81cde959eb67c03e8cd28d5c2f65b305ce1d20e8_2_690x205.png\" alt=\"image\" data-base62-sha1=\"iwiKWiM6XBlEBWoJ1rFVitBIl5e\" width=\"690\" height=\"205\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/81cde959eb67c03e8cd28d5c2f65b305ce1d20e8_2_690x205.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/81cde959eb67c03e8cd28d5c2f65b305ce1d20e8_2_1035x307.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/8\/81cde959eb67c03e8cd28d5c2f65b305ce1d20e8_2_1380x410.png 2x\" data-dominant-color=\"FCFAFA\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1772\u00d7527 130 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>However, as you can see the AUCs columns have different yticks and range depending on the corresponding values, which makes it impossible to interpret them intuitively. Is it possible to set column ranges manually?<\/p>\n<p><strong>More context:<\/strong> I think this is related to <a href=\"https:\/\/community.wandb.ai\/t\/scaling-in-the-parelles-coordinates-plot\/3274\">another question<\/a> on this blog. The reason why this would be convenient is when you want to compare different metrics or the same metric for different models.<br>\nIn particular, I want to compare the performance of a baseline solution (<code>Cumulative AUC (MELD)<\/code> ) VS my model (<code>Cumulative AUC<\/code>). So intuitively I would see whether the lines go up passing from one column to the other to determin whether the second column shows better performance. Unfortunately, that is misleading if I the two columns have different ranges.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1674643695926,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":95.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/set-y-ranges-in-parallel-coordinates-plot\/3745",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-01-26T13:19:50.372Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/lclissa\">@lclissa<\/a>, thanks for your feedback! I also answeres the other question, so I am aware of this and currently it is not possible, so I will create a new feature request, thank you very much for providing the context. As I suggested in the other thread, a workaround could be to create two new runs logging the limits for each metric. Please let me know if I can help you in any other way!<\/p>",
                "Answer_score":20.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-28T08:06:58.603Z",
                "Answer_body":"<p><s>I can confirm that logging the endpoints works, thanks! <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/smiley.png?v=12\" title=\":smiley:\" class=\"emoji\" alt=\":smiley:\" loading=\"lazy\" width=\"20\" height=\"20\"> <\/s><\/p>\n<p><strong>Edit:<\/strong><br>\nUnfortunately, the workaround only partially solves the problem. In fact, this approach breaks when I need to perform dynamic filtering.<\/p>\n<p>For example, I logged 0.5 and 1 as endpoints for all AUC metrics and it works for the \u201cglobal\u201d view.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/f\/f2f388e4698ff37a2132169a36cb9b0c407f1f6e.jpeg\" data-download-href=\"\/uploads\/short-url\/yFfjKKjJKcUQfnXgHXEiKuBGCBw.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/f\/f2f388e4698ff37a2132169a36cb9b0c407f1f6e_2_690x197.jpeg\" alt=\"image\" data-base62-sha1=\"yFfjKKjJKcUQfnXgHXEiKuBGCBw\" width=\"690\" height=\"197\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/f\/f2f388e4698ff37a2132169a36cb9b0c407f1f6e_2_690x197.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/f\/f2f388e4698ff37a2132169a36cb9b0c407f1f6e_2_1035x295.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/f\/f2f388e4698ff37a2132169a36cb9b0c407f1f6e_2_1380x394.jpeg 2x\" data-dominant-color=\"E7CEE2\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1725\u00d7494 162 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>However, for some reasons I then want to inspect the parallel coordinates plot of the best\/worst runs. In this case, the filtered view would have unmatched lower\/upper range endpoints respectively.<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/d\/dd4de800b4f5e4bef26a446afa0f7d5945b4d7e1.png\" data-download-href=\"\/uploads\/short-url\/vzKqVGUgTpaxrnLne4KZtVVNnod.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/dd4de800b4f5e4bef26a446afa0f7d5945b4d7e1_2_690x197.png\" alt=\"image\" data-base62-sha1=\"vzKqVGUgTpaxrnLne4KZtVVNnod\" width=\"690\" height=\"197\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/dd4de800b4f5e4bef26a446afa0f7d5945b4d7e1_2_690x197.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/dd4de800b4f5e4bef26a446afa0f7d5945b4d7e1_2_1035x295.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/dd4de800b4f5e4bef26a446afa0f7d5945b4d7e1_2_1380x394.png 2x\" data-dominant-color=\"FBF8FB\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1725\u00d7494 132 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>Of course, it would be impossible to manually log the endpoints for each possible filter, so I think the only way to handle this situation would be by setting a specified range for each column (unless one could log directly one line per run in the same parallel coordinates plot).<\/p>\n<p>Ideally, it would be nice if the range could be set:<\/p>\n<ul>\n<li>programmatically when logging + editable from the dashboard<\/li>\n<li>optionally for groups of metrics<\/li>\n<li>with a smart dynamic default (e.g. <code>[min(group_metrics) - offset, max(group_metrics) + offset]<\/code> )<\/li>\n<\/ul>\n<p>I would be happy to contribute if needed, although I would need some guidance as I am not so expert of a programmer <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\" loading=\"lazy\" width=\"20\" height=\"20\"><br>\nP.S. Can you link me the feature request so I can track it?<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-01-31T13:30:10.704Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/lclissa\">@lclissa<\/a>, thanks for explaining this! I\u2019ll share this feedback with our Product Team. Regarding the feature request, unfortunately this is something internal, so I cannot share the link to it. We\u2019ll notify you if this is implemented in the future.<\/p>",
                "Answer_score":15.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"set rang parallel coordin plot want compar run parallel coordin plot need evalu term differ metric default creat manual follow kind auc score interest imag auc column differ ytick rang depend correspond valu make imposs interpret intuit possibl set column rang manual context think relat question blog reason conveni want compar differ metric metric differ model particular want compar perform baselin solut cumul auc meld model cumul auc intuit line pass column determin second column show better perform unfortun mislead column differ rang",
        "Question_preprocessed_content":"set rang parallel coordin plot want compar run parallel coordin plot need evalu term differ metric default creat manual follow imag auc column differ ytick rang depend correspond valu make imposs interpret intuit possibl set column rang manual context think relat question blog reason conveni want compar differ metric metric differ model particular want compar perform baselin solut model intuit line pass column determin second column show better perform unfortun mislead column differ rang",
        "Answer_original_content":"lclissa thank feedback answer question awar current possibl creat new featur request thank provid context suggest thread workaround creat new run log limit metric let know help wai confirm log endpoint work thank edit unfortun workaround partial solv problem fact approach break need perform dynam filter exampl log endpoint auc metric work global view imag reason want inspect parallel coordin plot best worst run case filter view unmatch lower upper rang endpoint respect imag cours imposs manual log endpoint possibl filter think wai handl situat set specifi rang column log directli line run parallel coordin plot ideal nice rang set programmat log edit dashboard option group metric smart dynam default min group metric offset max group metric offset happi contribut need need guidanc expert programm link featur request track lclissa thank explain ill share feedback product team featur request unfortun intern share link notifi implement futur",
        "Answer_preprocessed_content":"thank feedback answer question awar current possibl creat new featur request thank provid context suggest thread workaround creat new run log limit metric let know help wai confirm log endpoint work thank edit unfortun workaround partial solv problem fact approach break need perform dynam filter exampl log endpoint auc metric work global view imag reason want inspect parallel coordin plot run case filter view unmatch rang endpoint respect imag cours imposs manual log endpoint possibl filter think wai handl situat set specifi rang column ideal nice rang set programmat log edit dashboard option group metric smart dynam default happi contribut need need guidanc expert programm link featur request track thank explain ill share feedback product team featur request unfortun intern share link notifi implement futur",
        "Question_gpt_summary_original":"the user is looking for a way to manually set the y-ranges in a parallel coordinates plot in order to compare different metrics or the same metric for different models.",
        "Question_gpt_summary":"user look wai manual set rang parallel coordin plot order compar differ metric metric differ model",
        "Answer_gpt_summary_original":"possible solutions mentioned in the answer include creating two new runs logging the limits for each metric as a workaround, setting a specified range for each column, and programmatically setting the range when logging and optionally editing it from the dashboard. the user also expressed a willingness to contribute if needed. however, the feature request for manually setting y-ranges in a parallel coordinates plot is currently not possible and is an internal matter.",
        "Answer_gpt_summary":"possibl solut mention answer includ creat new run log limit metric workaround set specifi rang column programmat set rang log option edit dashboard user express willing contribut need featur request manual set rang parallel coordin plot current possibl intern matter"
    },
    {
        "Question_id":null,
        "Question_title":"Azurerml v2 Pipeline: Steps not running in the mentioned conda environment",
        "Question_body":"Hello I am trying to create a pipeline using the documentation of Azureml v2. But the steps in a Job are not running in the environment mentioned in the yaml script of the pipeline. I don't know where I am going wrong. I am hereby attaching the yaml I created to create the pipeline. Please do let me know how to tackle this issue:\n\n$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/pipelineJob.schema.json\ntype: pipeline\nexperiment_name: ccep_training\ndescription: Training Pipeline to train a model that predicts coke and non coke bottles\ninputs:\n training_images:\n    type: uri_folder\n    mode: download # pick ro_mount, rw_mount or download\n    path: azureml:\/\/datastores\/ccepdatastore\/paths\/yolo\/dummy_dataset\/**\noutputs:\n  step_output_train:\n    type: uri_folder\nsettings:\n  default_datastore: <NAME OF DATASTORE>\n  continue_on_step_failure: false\njobs:\n  train:\n    name: training\n    display_name: Model-training\n    environment: azureml:ccep_train_env@latest\n    code: ..\/..\/ccep\/training\n    command: >-\n      python train_aml.py\n      --model_name ${<!-- -->{inputs.model_name_train}}\n      --step_output ${<!-- -->{outputs.step_output}}\n      \n    inputs:\n      model_name_train: \"ccep.pt\"\n      model_name_tflite: \"ccep-fp16.tflite\"\n      dataset_version: \"latest\"\n      epochs: 1\n      data_file_path: ${<!-- -->{parent.inputs.training_images}}\n      caller_run_id_param: none\n      dataset_name: \"dummy_dataset\"\n    outputs:\n      step_output: ${<!-- -->{parent.outputs.step_output_train}}\n    compute: azureml:ccepcomputeclust\n    resources:\n      instance_count: 1",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1656965879723,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/913607\/azurerml-v2-pipeline-steps-not-running-in-the-ment.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-05T12:16:14.39Z",
                "Answer_score":0,
                "Answer_body":"@RachitAhuja-3532 Looking up some of the samples from the azureml previews repo, does the code need to specify the local_path key in the yaml before specifying the path?\n\n code:\n   local_path: .\/train_src",
                "Answer_comment_count":4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"azurerml pipelin step run mention conda environ hello try creat pipelin document step job run environ mention yaml script pipelin know go wrong attach yaml creat creat pipelin let know tackl issu schema http schema azureedg net latest pipelinejob schema json type pipelin experi ccep train descript train pipelin train model predict coke non coke bottl input train imag type uri folder mode download pick mount mount download path datastor ccepdatastor path yolo dummi dataset output step output train type uri folder set default datastor continu step failur fals job train train displai model train environ ccep train env latest code ccep train command python train aml model input model train step output output step output input model train ccep model tflite ccep tflite dataset version latest epoch data file path parent input train imag caller run param dataset dummi dataset output step output parent output step output train comput ccepcomputeclust resourc instanc count",
        "Question_preprocessed_content":"azurerml pipelin step run mention conda environ hello try creat pipelin document step job run environ mention yaml script pipelin know go wrong attach yaml creat creat pipelin let know tackl issu schema type pipelin descript train pipelin train model predict coke non coke bottl input type mode download pick download path output type set fals job train train environ code command python input latest epoch output comput ccepcomputeclust resourc",
        "Answer_original_content":"rachitahuja look sampl preview repo code need specifi local path kei yaml specifi path code local path train src",
        "Answer_preprocessed_content":"look sampl preview repo code need specifi kei yaml specifi path code",
        "Question_gpt_summary_original":"the user is encountering challenges with running steps in a job in the environment mentioned in the yaml script of the pipeline for an  v2 pipeline.",
        "Question_gpt_summary":"user encount challeng run step job environ mention yaml script pipelin pipelin",
        "Answer_gpt_summary_original":"the answer suggests that the user should check if the yaml script in the  v2 pipeline code needs to specify the \"local_path\" key before specifying the path. this may help resolve the challenges encountered while running steps in the job.",
        "Answer_gpt_summary":"answer suggest user check yaml script pipelin code need specifi local path kei specifi path help resolv challeng encount run step job"
    },
    {
        "Question_id":56466592.0,
        "Question_title":"SageMaker: ClientError: .lst file missing in the train_lst channel. (customized image classification)",
        "Question_body":"<p><strong><em>Problem:<\/em><\/strong><\/p>\n\n<p>Jobs repeatedly fail after 5 minutes with the error<\/p>\n\n<blockquote>\n  <p>ClientError: .lst file missing in the train_lst channel.<\/p>\n<\/blockquote>\n\n<p><strong><em>Context:<\/em><\/strong><\/p>\n\n<p>Working within the AWS console, I have a binary classification task of images. I have labeled the classes in their filenames, per a guide.<\/p>\n\n<p>Eventually I started hitting errors that revealed that for this particular algorithm a <code>.lst<\/code> file is required for gathering the labels, since \"Content Type\" is specified as image, which apparently requires a lst file.<\/p>\n\n<p><strong><em>Example Data:<\/em><\/strong><\/p>\n\n<p>I am trying to match the examples I see on <a href=\"https:\/\/stackoverflow.com\/questions\/51670563\/invalid-lst-file-in-sagemaker\">StackOverflow<\/a> and elsewhere online. The current iteration of <code>trn_list.lst<\/code> looks like this:<\/p>\n\n<pre><code>292 \\t 1 \\t dog-292.jpeg\n214 \\t 1 \\t dog-214.jpeg\n290 \\t 0 \\t cat-290.jpeg\n288 \\t 1 \\t dog-288.jpeg\n160 \\t 1 \\t dog-160.jpeg\n18 \\t 0 \\t cat-18.jpeg\n215 \\t 1 \\t dog-215.jpeg\n254 \\t 1 \\t dog-254.jpeg\n53 \\t 1 \\t dog-53.jpeg\n337 \\t 0 \\t cat-337.jpeg\n284 \\t 0 \\t cat-284.jpeg\n177 \\t 1 \\t dog-177.jpeg\n192 \\t 1 \\t dog-192.jpeg\n228 \\t 0 \\t cat-228.jpeg\n305 \\t 0 \\t cat-305.jpeg\n258 \\t 1 \\t dog-258.jpeg\n75 \\t 0 \\t cat-75.jpeg\n148 \\t 0 \\t cat-148.jpeg\n268 \\t 1 \\t dog-268.jpeg\n281 \\t 1 \\t dog-281.jpeg\n24 \\t 1 \\t dog-24.jpeg\n328 \\t 1 \\t dog-328.jpeg\n99 \\t 1 \\t dog-99.jpeg\n<\/code><\/pre>\n\n<p>The bucket has no sub-folders, so I just put the .lst on the <\/p>\n\n<p>In one iteration I allowed my R program that creates the .lst to replace the <code>\\t<\/code> with actual tabs when it writes it out. In other iterations I left the actual delimiters (<code>\\t<\/code>) in there. Didn't seem to affect it (?).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559761618273,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":608.0,
        "Answer_body":"<p>When you are using SageMaker training jobs you are actually deploying a Docker image to a cluster of EC2 instances. The Docker has a python file that is running the training code in a similar way that you train it on your machine. In the training code you are referring to local folders when it expects to find the data such as the images to train on and the meta-data to use for that training. <\/p>\n\n<p>The \"magic\" is how to get the data from S3 to be available locally for the training instances. This is done using the definition of the channels in your training job configuration. Each channel definition creates a local folder on the training instance and copies the data from S3 to that local folder. You need to match the names and the S3 location and file formats.<\/p>\n\n<p>Here is the documentation of the definition of a channel in SageMaker: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_Channel.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_Channel.html<\/a><\/p>\n\n<p>For the specific example of the built-in algorithm for image classification and if you use the Image format for training, specify <code>train<\/code>, <code>validation<\/code>, <code>train_lst<\/code>, and <code>validation_lst<\/code> channels as values for the <code>InputDataConfig<\/code> parameter of the <code>CreateTrainingJob<\/code> request. Specify the individual image data (.jpg or .png files) for the train and validation channels. Specify one .lst file in each of the train_lst and validation_lst channels. Set the content type for all four channels to <code>application\/x-image<\/code>.<\/p>\n\n<p>See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html#IC-inputoutput\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html#IC-inputoutput<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1559985734928,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56466592",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1559907535367,
        "Question_original_content":"clienterror lst file miss train lst channel custom imag classif problem job repeatedli fail minut error clienterror lst file miss train lst channel context work aw consol binari classif task imag label class filenam guid eventu start hit error reveal particular algorithm lst file requir gather label content type specifi imag appar requir lst file exampl data try match exampl stackoverflow onlin current iter trn list lst look like dog jpeg dog jpeg cat jpeg dog jpeg dog jpeg cat jpeg dog jpeg dog jpeg dog jpeg cat jpeg cat jpeg dog jpeg dog jpeg cat jpeg cat jpeg dog jpeg cat jpeg cat jpeg dog jpeg dog jpeg dog jpeg dog jpeg dog jpeg bucket sub folder lst iter allow program creat lst replac actual tab write iter left actual delimit affect",
        "Question_preprocessed_content":"clienterror lst file miss channel problem job repeatedli fail minut error clienterror lst file miss channel context work aw consol binari classif task imag label class filenam guid eventu start hit error reveal particular algorithm file requir gather label content type specifi imag appar requir lst file exampl data try match exampl stackoverflow onlin current iter look like bucket lst iter allow program creat lst replac actual tab write iter left actual delimit affect",
        "Answer_original_content":"train job actual deploi docker imag cluster instanc docker python file run train code similar wai train machin train code refer local folder expect data imag train meta data us train magic data avail local train instanc definit channel train job configur channel definit creat local folder train instanc copi data local folder need match name locat file format document definit channel http doc aw amazon com latest api channel html specif exampl built algorithm imag classif us imag format train specifi train valid train lst valid lst channel valu inputdataconfig paramet createtrainingjob request specifi individu imag data jpg png file train valid channel specifi lst file train lst valid lst channel set content type channel applic imag detail http doc aw amazon com latest imag classif html inputoutput",
        "Answer_preprocessed_content":"train job actual deploi docker imag cluster instanc docker python file run train code similar wai train machin train code refer local folder expect data imag train us train magic data avail local train instanc definit channel train job configur channel definit creat local folder train instanc copi data local folder need match name locat file format document definit channel specif exampl algorithm imag classif us imag format train specifi channel valu paramet request specifi individu imag data train valid channel specifi lst file channel set content type channel detail",
        "Question_gpt_summary_original":"the user is encountering a challenge with a customized image classification task on aws console, where jobs repeatedly fail after 5 minutes with the error \"clienterror: .lst file missing in the train_lst channel.\"",
        "Question_gpt_summary":"user encount challeng custom imag classif task aw consol job repeatedli fail minut error clienterror lst file miss train lst channel",
        "Answer_gpt_summary_original":"the solution to the challenge of repeated job failures in a customized image classification task on aws console is to ensure that the data is available locally for the training instances by defining channels in the training job configuration. the names and s3 location and file formats should match, and the documentation of the definition of a channel should be followed. for the specific example of the built-in algorithm for image classification, the inputdataconfig parameter of the createtrainingjob request should be specified with train, validation, train_lst, and validation_lst channels as values for the image format for training. the individual image data (.jpg or .png files) should be specified for the train and validation channels, and one .lst file should be specified in each of the train_lst and validation_lst channels. the content type for all four channels should be set to application\/x-image.",
        "Answer_gpt_summary":"solut challeng repeat job failur custom imag classif task aw consol ensur data avail local train instanc defin channel train job configur name locat file format match document definit channel follow specif exampl built algorithm imag classif inputdataconfig paramet createtrainingjob request specifi train valid train lst valid lst channel valu imag format train individu imag data jpg png file specifi train valid channel lst file specifi train lst valid lst channel content type channel set applic imag"
    },
    {
        "Question_id":null,
        "Question_title":"Regular expression not detecting latest successful run for required operation dependency",
        "Question_body":"<p>According to the docs I can specify multiple supported operations for a required statement by using regular expressions<\/p>\n<aside class=\"quote no-group\" data-username=\"guildai\" data-post=\"1\" data-topic=\"197\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/guildai\/40\/103_2.png\" class=\"avatar\"><a href=\"https:\/\/my.guild.ai\/t\/guild-file-reference\/197\/1\">Guild File Reference<\/a>\n<\/div>\n<blockquote>\n<p>Value is a regular expression matching a suitable operation name. Multiple operations are supported by specifying the appropriate regular expression.<\/p>\n<\/blockquote>\n<\/aside>\n<p>An example is also given<\/p>\n<aside class=\"quote no-group\" data-username=\"guildai\" data-post=\"1\" data-topic=\"192\">\n<div class=\"title\">\n<div class=\"quote-controls\"><\/div>\n<img alt=\"\" width=\"20\" height=\"20\" src=\"https:\/\/sjc6.discourse-cdn.com\/standard11\/user_avatar\/my.guild.ai\/guildai\/40\/103_2.png\" class=\"avatar\"><a href=\"https:\/\/my.guild.ai\/t\/guild-file-cheatsheet\/192\/1\">Guild File Cheatsheet<\/a>\n<\/div>\n<blockquote>\n<p>Require <code>model.ckpt<\/code> from any operation that starts with <code>train-<\/code> :<\/p>\n<pre><code>train:\n  requires:\n    - operation: ^train-\n      select: model\\.ckpt\n<\/code><\/pre>\n<\/blockquote>\n<\/aside>\n<p>However whenever I try to selecting the latest run using regular expressions, for example using the following guild file<\/p>\n<pre><code>\ntrain-pull:\n    main: script\n    \ntrain-extract:\n    main: script\n\ntest:\n  requires:\n  - operation: ^train-\n<\/code><\/pre>\n<p>The latest successful run is not detected<\/p>\n<pre><code>guild: run failed because a dependency was not met: could not resolve 'operation:^train-' in ^train- resource: no suitable run for ^train-\n<\/code><\/pre>\n<p>If I specify the run id then the run is detected, but this seems to work regardless of if the passed run id is compatible with the regular expression given in the guild file.<\/p>\n<p>Instead of this behavior I would expect guild to detect the latest run done for either train-pull  or train-extract and use it as the operation requirement for test.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1614186759263,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":374.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/regular-expression-not-detecting-latest-successful-run-for-required-operation-dependency\/549",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-02-24T17:23:54.402Z",
                "Answer_body":"<p>There\u2019s a bug in the docs. The regex is a full match, not a starts-with. You need this:<\/p>\n<pre><code class=\"lang-yaml\">test:\n  requires:\n  - operation: ^train-.+\n<\/code><\/pre>\n<p>Also, as a tip, you can include a <code>name<\/code> attribute to cleanup the interface to that requirement.<\/p>\n<pre><code class=\"lang-yaml\">test:\n  requires:\n  - operation: ^train-.+\n    name: train\n<\/code><\/pre>\n<p>This lets you specify a run ID for the dependency using a flag-like syntax:<\/p>\n<pre><code class=\"lang-command\">guild run test train=&lt;run ID&gt;\n<\/code><\/pre>\n<p>Use whatever name makes most sense. I just picked <code>train<\/code> as an example.<\/p>\n<p>Guild intentionally disregards the op requirement when you specify a full run ID. The rationale here is that if you\u2019re specifying a full ID, you are intentionally overriding any specs in the Guild file. If you use a partial run ID, Guild uses the op requirement spec.<\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-24T17:31:43.113Z",
                "Answer_body":"<p>Thanks for the quick reply, that worked perfectly.<\/p>",
                "Answer_score":46.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"regular express detect latest success run requir oper depend accord doc specifi multipl support oper requir statement regular express file refer valu regular express match suitabl oper multipl oper support specifi appropri regular express exampl given file cheatsheet requir model ckpt oper start train train requir oper train select model ckpt try select latest run regular express exampl follow file train pull main script train extract main script test requir oper train latest success run detect run fail depend met resolv oper train train resourc suitabl run train specifi run run detect work regardless pass run compat regular express given file instead behavior expect detect latest run train pull train extract us oper requir test",
        "Question_preprocessed_content":"regular express detect latest success run requir oper depend accord doc specifi multipl support oper requir statement regular express file refer valu regular express match suitabl oper multipl oper support specifi appropri regular express exampl given file cheatsheet requir oper start try select latest run regular express exampl follow file latest success run detect specifi run run detect work regardless pass run compat regular express given file instead behavior expect detect latest run us oper requir test",
        "Answer_original_content":"there bug doc regex match start need test requir oper train tip includ attribut cleanup interfac requir test requir oper train train let specifi run depend flag like syntax run test train us make sens pick train exampl intention disregard requir specifi run rational your specifi intention overrid spec file us partial run us requir spec thank quick repli work perfectli",
        "Answer_preprocessed_content":"there bug doc regex match need tip includ attribut cleanup interfac requir let specifi run depend syntax us make sens pick exampl intention disregard requir specifi run rational your specifi intention overrid spec file us partial run us requir spec thank quick repli work perfectli",
        "Question_gpt_summary_original":"the user encountered challenges with a regular expression not detecting the latest successful run for a required operation dependency.",
        "Question_gpt_summary":"user encount challeng regular express detect latest success run requir oper depend",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n\n- use a different regular expression that matches the required operation dependency. \n- include a name attribute to clean up the interface to that requirement. \n- specify a run id for the dependency using a flag-like syntax. \n- use a full run id to intentionally override any specs in the guild file.",
        "Answer_gpt_summary":"possibl solut answer us differ regular express match requir oper depend includ attribut clean interfac requir specifi run depend flag like syntax us run intention overrid spec file"
    },
    {
        "Question_id":48539564.0,
        "Question_title":"Tensorflow - Checkpoints not saving to Sagemaker Notebook Instance",
        "Question_body":"<p>I am running a Python script with Tensorflow in Amazon Sagemaker notebook instance.  I have no trouble writing to the storage in the notebook normally, but for some reason I am unsuccessful when trying to save Tensorflow model checkpoints.  This code previously worked before it was ported to Sagemaker.<\/p>\n\n<p>Below is a reduced version of my code:<\/p>\n\n<pre><code>bucket = 'sagemaker-complaints-data'    \nprefix = 'DeepTestV2' # place to upload training files within the bucket\ntimestamp = str(int(time()))\nout_dir = os.path.abspath(os.path.join(bucket, prefix, \"runs\", timestamp))\ncheckpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\ncheckpoint_prefix = os.path.join(checkpoint_dir, \"model\")\npath = saver.save(sess, checkpoint_prefix, global_step=current_step)\nprint(\"Saved model checkpoint to {}\\n\".format(path))\n<\/code><\/pre>\n\n<p>No errors are being thrown and the print statement is outputting the correct path.  I have researched whether there are any known issues with using checkpoints in Sagemaker but have come across literally no posts describing this.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1517393582673,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":690.0,
        "Answer_body":"<p>I have found out where this is - for some reason \"checkpoints\" seems to be a reserved word - changing the word to \"checks\" allowed me to write the folder.  Hope this helps someone!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48539564",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1518715595183,
        "Question_original_content":"tensorflow checkpoint save notebook instanc run python script tensorflow notebook instanc troubl write storag notebook normal reason unsuccess try save tensorflow model checkpoint code previous work port reduc version code bucket complaint data prefix deeptestv place upload train file bucket timestamp str int time dir path abspath path join bucket prefix run timestamp checkpoint dir path abspath path join dir checkpoint checkpoint prefix path join checkpoint dir model path saver save sess checkpoint prefix global step current step print save model checkpoint format path error thrown print statement output correct path research known issu checkpoint come liter post describ",
        "Question_preprocessed_content":"tensorflow checkpoint save notebook instanc run python script tensorflow notebook instanc troubl write storag notebook normal reason unsuccess try save tensorflow model checkpoint code previous work port reduc version code error thrown print statement output correct path research known issu checkpoint come liter post describ",
        "Answer_original_content":"reason checkpoint reserv word chang word check allow write folder hope help",
        "Answer_preprocessed_content":"reason checkpoint reserv word chang word check allow write folder hope help",
        "Question_gpt_summary_original":"the user is encountering challenges when attempting to save tensorflow model checkpoints to a notebook instance.",
        "Question_gpt_summary":"user encount challeng attempt save tensorflow model checkpoint notebook instanc",
        "Answer_gpt_summary_original":"the solution to the challenge of saving tensorflow model checkpoints to a notebook instance is to change the folder name from \"checkpoints\" to something else, such as \"checks\".",
        "Answer_gpt_summary":"solut challeng save tensorflow model checkpoint notebook instanc chang folder checkpoint check"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker Data Wrangler UI Features",
        "Question_body":"The SageMaker Data Wrangler UI in SageMaker Studio doesn't seem to support all the features that the API does. When will the UI support:\n\nLoading all s3 objects under a prefix? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_csv.html#awswrangler.s3.read_csv\nLoading JSON objects in addition to CSV and Parquet files? https:\/\/aws-data-wrangler.readthedocs.io\/en\/stable\/stubs\/awswrangler.s3.read_json.html#awswrangler.s3.read_json",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1641943985816,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":61.0,
        "Answer_body":"As mentioned by Tulio Alberto in comments, Amazon SageMaker Data Wrangler (the graphical data preparation feature inside Amazon SageMaker) is separate from AWS Data Wrangler (an open-source data prep utility published by AWS Labs): The two tools are based on different technologies and don't necessarily aim for full feature parity - they just happen to share similar names.\n\nTo my knowledge there's no committed timeline we can share at the moment for when these particular features will make it to SageMaker Data Wrangler, but I think as feature requests they make sense and the reasoning for both is pretty clear: I'm aware that both have been discussed to some extent internally already, and I'd personally like to see them launch too!\n\nThanks for sharing the feedback, and apologies for the naming confusion!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUcsIt78jnSTW8Ta9__kUm-w\/sage-maker-data-wrangler-ui-features",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-12T20:59:06.281Z",
                "Answer_score":2,
                "Answer_body":"As mentioned by Tulio Alberto in comments, Amazon SageMaker Data Wrangler (the graphical data preparation feature inside Amazon SageMaker) is separate from AWS Data Wrangler (an open-source data prep utility published by AWS Labs): The two tools are based on different technologies and don't necessarily aim for full feature parity - they just happen to share similar names.\n\nTo my knowledge there's no committed timeline we can share at the moment for when these particular features will make it to SageMaker Data Wrangler, but I think as feature requests they make sense and the reasoning for both is pretty clear: I'm aware that both have been discussed to some extent internally already, and I'd personally like to see them launch too!\n\nThanks for sharing the feedback, and apologies for the naming confusion!",
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-02-12T01:48:49.303Z",
                "Answer_score":0,
                "Answer_body":"Hi Tulio, thanks for the clarification. But doesn't SageMaker Data Wrangler generate code that complies with\/uses AWS Data Wrangler? Isn't there some (if tenuous) connection between the two?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-13T05:22:27.466Z",
                "Answer_score":0,
                "Answer_body":"Hi,\n\nSageMaker Data Wrangler in Studio just launched the JSON\/ORC support and we support import files under a prefix already. Please see the following links\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/data-wrangler-import.html#data-wrangler-import-s3\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/prepare-and-analyze-json-and-orc-data-with-amazon-sagemaker-data-wrangler\/",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1642021146280,
        "Question_original_content":"data wrangler featur data wrangler studio support featur api support load object prefix http aw data wrangler readthedoc stabl stub awswrangl read csv html awswrangl read csv load json object addit csv parquet file http aw data wrangler readthedoc stabl stub awswrangl read json html awswrangl read json",
        "Question_preprocessed_content":"data wrangler featur data wrangler studio support featur api support load object prefix load json object addit csv parquet file",
        "Answer_original_content":"mention tulio alberto comment data wrangler graphic data prepar featur insid separ aw data wrangler open sourc data prep util publish aw lab tool base differ technolog necessarili aim featur pariti happen share similar name knowledg commit timelin share moment particular featur data wrangler think featur request sens reason pretti clear awar discuss extent intern person like launch thank share feedback apolog name confus",
        "Answer_preprocessed_content":"mention tulio alberto comment data wrangler separ aw data wrangler tool base differ technolog necessarili aim featur pariti happen share similar name knowledg commit timelin share moment particular featur data wrangler think featur request sens reason pretti clear awar discuss extent intern person like launch thank share feedback apolog name confus",
        "Question_gpt_summary_original":"the user is encountering challenges with the data wrangler ui in studio not supporting all the features that the api does, such as loading all s3 objects under a prefix and loading json objects in addition to csv and parquet files.",
        "Question_gpt_summary":"user encount challeng data wrangler studio support featur api load object prefix load json object addit csv parquet file",
        "Answer_gpt_summary_original":"the answer suggests that the data wrangler ui in studio and the aws data wrangler are two separate tools based on different technologies and do not necessarily aim for full feature parity. there is no committed timeline for when the features requested by the user will be added to data wrangler, but they make sense as feature requests and have been discussed internally. the answer also apologizes for any confusion caused by the similar names of the two tools.",
        "Answer_gpt_summary":"answer suggest data wrangler studio aw data wrangler separ tool base differ technolog necessarili aim featur pariti commit timelin featur request user ad data wrangler sens featur request discuss intern answer apolog confus caus similar name tool"
    },
    {
        "Question_id":63162310.0,
        "Question_title":"Models registered in workspace do not show up in Designer (preview)",
        "Question_body":"<p>I registered a model in my AML workspace, and I can see it in the Model List:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/rtL5Q.png\" rel=\"nofollow noreferrer\">Model List view<\/a><\/p>\n<p>But I cannot see it in Designer (preview), which prevents me from using the new model there.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/WpvIb.png\" rel=\"nofollow noreferrer\">Designer view<\/a><\/p>\n<p>Looks like a bug to me. Datasets work fine.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_creation_time":1596056234913,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":81.0,
        "Answer_body":"<p>This is known issue as the models registered in workspace cannot be consumed in Designer without the new custom module capability (in private preview) available.<\/p>\n<p>The models showing up in Designer today are these generated from Designer training -&gt; inference pipeline conversion and can only be used in Designer (not registered in the workspace).\nWe have an effort ongoing to reduce the confusion.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1596073668927,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63162310",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1596073257467,
        "Question_original_content":"model regist workspac design preview regist model aml workspac model list model list view design preview prevent new model design view look like bug dataset work fine",
        "Question_preprocessed_content":"model regist workspac design regist model aml workspac model list model list view design prevent new model design view look like bug dataset work fine",
        "Answer_original_content":"known issu model regist workspac consum design new custom modul capabl privat preview avail model show design todai gener design train infer pipelin convers design regist workspac effort ongo reduc confus",
        "Answer_preprocessed_content":"known issu model regist workspac consum design new custom modul capabl avail model show design todai gener design train infer pipelin convers design effort ongo reduc confus",
        "Question_gpt_summary_original":"the user is encountering a challenge where models registered in their  workspace are not showing up in the designer (preview) view.",
        "Question_gpt_summary":"user encount challeng model regist workspac show design preview view",
        "Answer_gpt_summary_original":"possible solutions: currently, models registered in the  workspace cannot be consumed in designer without the new custom module capability (in private preview) available. the models showing up in designer today are those generated from designer training -> inference pipeline conversion and can only be used in designer (not registered in the workspace). the team is working to reduce the confusion.",
        "Answer_gpt_summary":"possibl solut current model regist workspac consum design new custom modul capabl privat preview avail model show design todai gener design train infer pipelin convers design regist workspac team work reduc confus"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker debugger built in rule CreateXgboostRule not generating report as expected",
        "Question_body":"I'm currently working with a SageMaker hosted XGBoost model; I've added the built in rule \"CreateXgboostRule\" to generate a training report, however, only the ProfilerReport is generated in the S3 rule-output folder - the expected result based on the dev doc is for a CreateXGBoostRule folder as well within this same folder.\n\nThe code I'm using is based directly on the example provided in: https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/debugger-training-xgboost-report.html\n\nimport boto3\nimport sagemaker\nfrom sagemaker.estimator import Estimator\nfrom sagemaker import image_uris\nfrom sagemaker.debugger import Rule, rule_configs\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\n\nregion = boto3.Session().region_name\nxgboost_container=sagemaker.image_uris.retrieve(\"xgboost\", region, \"1.2-1\")\n\nestimator=Estimator(\n    role=sagemaker.get_execution_role()\n    image_uri=xgboost_container,\n    base_job_name=\"debugger-xgboost-report-demo\",\n    instance_count=1,\n    instance_type=\"ml.m5.2xlarge\",\n    \n    # Add the Debugger XGBoost report rule\n    rules=rules\n)\n\nestimator.fit(wait=False)\n\n\nI've tried rewriting the estimator a number of ways, verified \"rules\" is receiving an array of objects, tried different versions of XGBoost within the region, but everything still results in the built in rule only creating the ProfilerReport with no CreateXGBoostRule directory under rule-output.\n\nAny ideas would be greatly appreciated! Thanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662562294513,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":48.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUuZEDvbaeRfqOT_g2Sxlw7w\/sage-maker-debugger-built-in-rule-create-xgboost-rule-not-generating-report-as-expected",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-08T13:53:26.107Z",
                "Answer_score":0,
                "Answer_body":"Hi Dennis, I tried a code above in AWS SageMaker notebook and didn't get any ProfilerReport as well.\n\nThe possible issue is that there is no data to train hence nothing to report. To prove this hypothesis I decided to take a sample XGBoost notebook and to add report functionality in it. This link tells how to access sample notebooks. I used \"xgboost_customer_churn.ipynb\".\n\nHere some changes that I made:\n\nIn order to be able to run it smoothly, instead of \"1.6-1\" XGBoost image I set the \"1.2-1\":\ncontainer = sagemaker.image_uris.retrieve(\"xgboost\", sess.boto_region_name, \"1.2-1\")\n\nI added reporting to the training cell:\nfrom sagemaker.debugger import Rule, rule_configs\n\nrules=[\n    Rule.sagemaker(rule_configs.create_xgboost_report())\n]\nsess = sagemaker.Session()\n\nxgb = sagemaker.estimator.Estimator(\n    container,\n    role,\n    instance_count=1,\n    instance_type=\"ml.m4.xlarge\",\n    output_path=\"s3:\/\/{}\/{}\/output\".format(bucket, prefix),\n    sagemaker_session=sess,\n    base_job_name=\"debugger-xgboost-report-demo\",\n    rules=rules\n)\n\nxgb.set_hyperparameters(\n    max_depth=5,\n    eta=0.2,\n    gamma=4,\n    min_child_weight=6,\n    subsample=0.8,\n    verbosity=0,\n    objective=\"binary:logistic\",\n    num_round=100,\n)\n\nxgb.fit({\"train\": s3_input_train, \"validation\": s3_input_validation})\n\n\nAll the rest I run without changes.\n\nHowever, it was a little confusing to fing a ProfilerReport. In this particular example it had the path: sagemaker-region-11122233\/sagemaker\/DEMO-xgboost-churn\/output\/debugger-xgboost-report-demo-2022-00-000\/rule-output\/CreateXgboostReport\/\n\nIn order to get this path you can use (run this code in the SageMaker notebook):\n\nxgb.output_path - gives the first part of the path (including the bucket name)\n\nxgb.latest_training_job.job_name - gives the second part of the path (training job name)\n\nIf you combine these two, you will get the full path towards the rule-output directory.\n\nI hope this helps.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"debugg built rule createxgboostrul gener report expect current work host xgboost model ad built rule createxgboostrul gener train report profilerreport gener rule output folder expect result base dev doc createxgboostrul folder folder code base directli exampl provid http doc aw amazon com latest debugg train xgboost report html import boto import estim import estim import imag uri debugg import rule rule config rule rule rule config creat xgboost report region boto session region xgboost contain imag uri retriev xgboost region estim estim role execut role imag uri xgboost contain base job debugg xgboost report demo instanc count instanc type xlarg add debugg xgboost report rule rule rule estim fit wait fals tri rewrit estim number wai verifi rule receiv arrai object tri differ version xgboost region result built rule creat profilerreport createxgboostrul directori rule output idea greatli appreci thank",
        "Question_preprocessed_content":"debugg built rule createxgboostrul gener report expect current work host xgboost model ad built rule createxgboostrul gener train report profilerreport gener folder expect result base dev doc createxgboostrul folder folder code base directli exampl provid import boto import estim import estim import debugg import rule rule region region estim estim add debugg xgboost report rule rule rule tri rewrit estim number wai verifi rule receiv arrai object tri differ version xgboost region result built rule creat profilerreport createxgboostrul directori idea greatli appreci thank",
        "Answer_original_content":"denni tri code notebook profilerreport possibl issu data train report prove hypothesi decid sampl xgboost notebook add report function link tell access sampl notebook xgboost custom churn ipynb chang order abl run smoothli instead xgboost imag set contain imag uri retriev xgboost sess boto region ad report train cell debugg import rule rule config rule rule rule config creat xgboost report sess session xgb estim estim contain role instanc count instanc type xlarg output path output format bucket prefix session sess base job debugg xgboost report demo rule rule xgb set hyperparamet max depth eta gamma min child weight subsampl verbos object binari logist num round xgb fit train input train valid input valid rest run chang littl confus fing profilerreport particular exampl path region demo xgboost churn output debugg xgboost report demo rule output createxgboostreport order path us run code notebook xgb output path give path includ bucket xgb latest train job job give second path train job combin path rule output directori hope help",
        "Answer_preprocessed_content":"denni tri code notebook profilerreport possibl issu data train report prove hypothesi decid sampl xgboost notebook add report function link tell access sampl notebook chang order abl run smoothli instead xgboost imag set contain ad report train cell debugg import rule rule sess session xgb contain role prefix rule rule gamma verbos object binari logist valid rest run chang littl confus fing profilerreport particular exampl path order path us give path give second path combin path directori hope help",
        "Question_gpt_summary_original":"the user is encountering challenges with the debugger built in rule createxgboostrule not generating the expected report in the s3 rule-output folder.",
        "Question_gpt_summary":"user encount challeng debugg built rule createxgboostrul gener expect report rule output folder",
        "Answer_gpt_summary_original":"the possible solution to the issue of the debugger not generating the expected report is that there might be no data to train. to test this hypothesis, the user can access a sample xgboost notebook and add reporting functionality to it. the user can also make some changes to the code and run it without any further changes. the path to the profiler report can be found by combining the output path and the training job name.",
        "Answer_gpt_summary":"possibl solut issu debugg gener expect report data train test hypothesi user access sampl xgboost notebook add report function user chang code run chang path profil report combin output path train job"
    },
    {
        "Question_id":72886409.0,
        "Question_title":"MLflow proxied artifact access: Unable to locate credentials",
        "Question_body":"<p>I am using MLflow to track my experiments. I am using an S3 bucket as an artifact store. For acessing it, I want to use <em>proxied artifact access<\/em>, as described in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>, however this does not work for me, since it locally looks for credentials (but the server should handle this).<\/p>\n<h2>Expected Behaviour<\/h2>\n<p>As described in the docs, I would expect that locally, I do not need to specify my AWS credentials, since the server handles this for me. From <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>:<\/p>\n<blockquote>\n<p>This eliminates the need to allow end users to have direct path access to a remote object store (e.g., s3, adls, gcs, hdfs) for artifact handling and eliminates the need for an end-user to provide access credentials to interact with an underlying object store.<\/p>\n<\/blockquote>\n<h2>Actual Behaviour \/ Error<\/h2>\n<p>Whenever I run an experiment on my machine, I am running into the following error:<\/p>\n<p><code>botocore.exceptions.NoCredentialsError: Unable to locate credentials<\/code><\/p>\n<p>So the error is local. However, this should not happen since the server should handle the auth instead of me needing to store my credentials locally. Also, I would expect that I would not even need library <code>boto3<\/code> locally.<\/p>\n<h2>Solutions Tried<\/h2>\n<p>I am aware that I need to create a new experiment, because existing experiments might still use a different artifact location which is proposed in <a href=\"https:\/\/stackoverflow.com\/a\/71417933\/10465165\">this SO answer<\/a> as well as in the note in the <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">docs<\/a>. Creating a new experiment did not solve the error for me. Whenever I run the experiment, I get an explicit log in the console validating this:<\/p>\n<p><code>INFO mlflow.tracking.fluent: Experiment with name 'test' does not exist. Creating a new experiment.<\/code><\/p>\n<p>Related Questions (<a href=\"https:\/\/stackoverflow.com\/questions\/72206086\/cant-log-mlflow-artifacts-to-s3-with-docker-based-tracking-server\">#1<\/a> and <a href=\"https:\/\/stackoverflow.com\/questions\/72236258\/mlflow-unable-to-store-artifacts-to-s3\/72261826#comment128726676_72261826\">#2<\/a>) refer to a different scenario, which is also <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-4-mlflow-with-remote-tracking-server-backend-and-artifact-stores\" rel=\"nofollow noreferrer\">described in the docs<\/a><\/p>\n<h2>Server Config<\/h2>\n<p>The server runs on a kubernetes pod with the following config:<\/p>\n<pre><code>mlflow server \\\n    --host 0.0.0.0 \\\n    --port 5000 \\\n    --backend-store-uri postgresql:\/\/user:pw@endpoint \\\n    --artifacts-destination s3:\/\/my_bucket\/artifacts \\\n    --serve-artifacts \\\n    --default-artifact-root s3:\/\/my_bucket\/artifacts \\\n<\/code><\/pre>\n<p>I would expect my config to be correct, looking at doc <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#scenario-5\" rel=\"nofollow noreferrer\">page 1<\/a> and <a href=\"https:\/\/mlflow.org\/docs\/latest\/tracking.html#using-the-tracking-server-for-proxied-artifact-access\" rel=\"nofollow noreferrer\">page 2<\/a><\/p>\n<p>I am able to see the mlflow UI if I forward the port to my local machine. I also see the experiment runs as failed, because of the error I sent above.<\/p>\n<h2>My Code<\/h2>\n<p>The relevant part of my code which fails is the logging of the model:<\/p>\n<pre><code>mlflow.set_tracking_uri(&quot;http:\/\/localhost:5000&quot;)\nmlflow.set_experiment(&quot;test2)\n\n...\n\n# this works\nmlflow.log_params(hyperparameters)\n                        \nmodel = self._train(model_name, hyperparameters, X_train, y_train)\ny_pred = model.predict(X_test)\nself._evaluate(y_test, y_pred)\n\n# this fails with the error from above\nmlflow.sklearn.log_model(model, &quot;artifacts&quot;)\n\n<\/code><\/pre>\n<h2>Question<\/h2>\n<p>I am probably overlooking something. Is there a need to locally indicate that I want to use proxied artified access? If yes, how do I do this? Is there something I have missed?<\/p>\n<h2>Full Traceback<\/h2>\n<pre><code>  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/models\/model.py&quot;, line 295, in log\n    mlflow.tracking.fluent.log_artifacts(local_path, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/fluent.py&quot;, line 726, in log_artifacts\n    MlflowClient().log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/client.py&quot;, line 1001, in log_artifacts\n    self._tracking_client.log_artifacts(run_id, local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/tracking\/_tracking_service\/client.py&quot;, line 346, in log_artifacts\n    self._get_artifact_repo(run_id).log_artifacts(local_dir, artifact_path)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 141, in log_artifacts\n    self._upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/mlflow\/store\/artifact\/s3_artifact_repo.py&quot;, line 117, in _upload_file\n    s3_client.upload_file(Filename=local_file, Bucket=bucket, Key=key, ExtraArgs=extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/inject.py&quot;, line 143, in upload_file\n    return transfer.upload_file(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/boto3\/s3\/transfer.py&quot;, line 288, in upload_file\n    future.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 103, in result\n    return self._coordinator.result()\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/futures.py&quot;, line 266, in result\n    raise self._exception\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 139, in __call__\n    return self._execute_main(kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/tasks.py&quot;, line 162, in _execute_main\n    return_value = self._main(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/s3transfer\/upload.py&quot;, line 758, in _main\n    client.put_object(Bucket=bucket, Key=key, Body=body, **extra_args)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 508, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 898, in _make_api_call\n    http, parsed_response = self._make_request(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/client.py&quot;, line 921, in _make_request\n    return self._endpoint.make_request(operation_model, request_dict)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 119, in make_request\n    return self._send_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 198, in _send_request\n    request = self.create_request(request_dict, operation_model)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/endpoint.py&quot;, line 134, in create_request\n    self._event_emitter.emit(\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 412, in emit\n    return self._emitter.emit(aliased_event_name, **kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 256, in emit\n    return self._emit(event_name, kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/hooks.py&quot;, line 239, in _emit\n    response = handler(**kwargs)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 103, in handler\n    return self.sign(operation_name, request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/signers.py&quot;, line 187, in sign\n    auth.add_auth(request)\n  File \/dir\/venv\/lib\/python3.9\/site-packages\/botocore\/auth.py&quot;, line 407, in add_auth\n    raise NoCredentialsError()\nbotocore.exceptions.NoCredentialsError: Unable to locate credentials\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1657122030593,
        "Question_favorite_count":null,
        "Question_last_edit_time":1657122771947,
        "Question_score":0.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>The problem is that the server is running on wrong run parameters, the <code>--default-artifact-root<\/code> needs to either be removed or set to <code>mlflow-artifacts:\/<\/code>.<\/p>\n<p>From <code>mlflow server --help<\/code>:<\/p>\n<pre><code>  --default-artifact-root URI  Directory in which to store artifacts for any\n                               new experiments created. For tracking server\n                               backends that rely on SQL, this option is\n                               required in order to store artifacts. Note that\n                               this flag does not impact already-created\n                               experiments with any previous configuration of\n                               an MLflow server instance. By default, data\n                               will be logged to the mlflow-artifacts:\/ uri\n                               proxy if the --serve-artifacts option is\n                               enabled. Otherwise, the default location will\n                               be .\/mlruns.\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72886409",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1657186814368,
        "Question_original_content":"proxi artifact access unabl locat credenti track experi bucket artifact store acess want us proxi artifact access describ doc work local look credenti server handl expect behaviour describ doc expect local need specifi aw credenti server handl doc elimin need allow end user direct path access remot object store adl gc hdf artifact handl elimin need end user provid access credenti interact underli object store actual behaviour error run experi machin run follow error botocor except nocredentialserror unabl locat credenti error local happen server handl auth instead need store credenti local expect need librari boto local solut tri awar need creat new experi exist experi us differ artifact locat propos answer note doc creat new experi solv error run experi explicit log consol valid info track fluent experi test exist creat new experi relat question refer differ scenario describ doc server config server run kubernet pod follow config server host port backend store uri postgresql user endpoint artifact destin bucket artifact serv artifact default artifact root bucket artifact expect config correct look doc page page abl forward port local machin experi run fail error sent code relev code fail log model set track uri http localhost set experi test work log param hyperparamet model self train model hyperparamet train train pred model predict test self evalu test pred fail error sklearn log model model artifact question probabl overlook need local indic want us proxi artifi access ye miss traceback file dir venv lib python site packag model model line log track fluent log artifact local path artifact path file dir venv lib python site packag track fluent line log artifact client log artifact run local dir artifact path file dir venv lib python site packag track client line log artifact self track client log artifact run local dir artifact path file dir venv lib python site packag track track servic client line log artifact self artifact repo run log artifact local dir artifact path file dir venv lib python site packag store artifact artifact repo line log artifact self upload file file dir venv lib python site packag store artifact artifact repo line upload file client upload file filenam local file bucket bucket kei kei extraarg extra arg file dir venv lib python site packag boto inject line upload file return transfer upload file file dir venv lib python site packag boto transfer line upload file futur result file dir venv lib python site packag stransfer futur line result return self coordin result file dir venv lib python site packag stransfer futur line result rais self except file dir venv lib python site packag stransfer task line return self execut main kwarg file dir venv lib python site packag stransfer task line execut main return valu self main kwarg file dir venv lib python site packag stransfer upload line main client object bucket bucket kei kei bodi bodi extra arg file dir venv lib python site packag botocor client line api return self api oper kwarg file dir venv lib python site packag botocor client line api http pars respons self request file dir venv lib python site packag botocor client line request return self endpoint request oper model request dict file dir venv lib python site packag botocor endpoint line request return self send request request dict oper model file dir venv lib python site packag botocor endpoint line send request request self creat request request dict oper model file dir venv lib python site packag botocor endpoint line creat request self event emitt emit file dir venv lib python site packag botocor hook line emit return self emitt emit alias event kwarg file dir venv lib python site packag botocor hook line emit return self emit event kwarg file dir venv lib python site packag botocor hook line emit respons handler kwarg file dir venv lib python site packag botocor signer line handler return self sign oper request file dir venv lib python site packag botocor signer line sign auth add auth request file dir venv lib python site packag botocor auth line add auth rais nocredentialserror botocor except nocredentialserror unabl locat credenti",
        "Question_preprocessed_content":"proxi artifact access unabl locat credenti track experi bucket artifact store acess want us proxi artifact access describ doc work local look credenti expect behaviour describ doc expect local need specifi aw credenti server handl doc elimin need allow end user direct path access remot object store artifact handl elimin need provid access credenti interact underli object store actual behaviour error run experi machin run follow error error local happen server handl auth instead need store credenti local expect need librari local solut tri awar need creat new experi exist experi us differ artifact locat propos answer note doc creat new experi solv error run experi explicit log consol valid relat question refer differ scenario describ doc server config server run kubernet pod follow config expect config correct look doc page page abl forward port local machin experi run fail error sent code relev code fail log model question probabl overlook need local indic want us proxi artifi access ye miss traceback",
        "Answer_original_content":"problem server run wrong run paramet default artifact root need remov set artifact server help default artifact root uri directori store artifact new experi creat track server backend reli sql option requir order store artifact note flag impact creat experi previou configur server instanc default data log artifact uri proxi serv artifact option enabl default locat mlrun",
        "Answer_preprocessed_content":"problem server run wrong run paramet need remov set",
        "Question_gpt_summary_original":"the user is unable to locate credentials when attempting to use proxied artifact access to access an s3 bucket, despite following the documentation and creating a new experiment.",
        "Question_gpt_summary":"user unabl locat credenti attempt us proxi artifact access access bucket despit follow document creat new experi",
        "Answer_gpt_summary_original":"the solution to the problem of being unable to locate credentials when attempting to use proxied artifact access to access an s3 bucket is to ensure that the server is running on the correct run parameters. specifically, the --default-artifact-root needs to either be removed or set to -artifacts:\/.",
        "Answer_gpt_summary":"solut problem unabl locat credenti attempt us proxi artifact access access bucket ensur server run correct run paramet specif default artifact root need remov set artifact"
    },
    {
        "Question_id":72473641.0,
        "Question_title":"How do launch experiments in DVC?",
        "Question_body":"<p>I want to launch some experiments in DVC. But when I set values of experiment parameters, DVC deletes file 'params.yaml', and experiment doesn't set in queue.<\/p>\n<p>Simplified code for example:\nPython file 'test.py':<\/p>\n<pre><code>import numpy as np\nimport json\nimport yaml\n\nparams = yaml.safe_load(open('params.yaml'))[&quot;test&quot;]\n\nprecision = np.random.random()\nrecall = params['value']\naccuracy = np.random.random()\n \n\nrows = {'precision': precision,\n        'recall': recall,\n        'accuracy': accuracy}\n\n\nwith open(params['metrics_path'], 'w') as outfile:\n    json.dump(rows, outfile)\n\nfpr = 10*np.random.random((1,10)).tolist()\ntpr = 10*np.random.random((1,10)).tolist()\n\nwith open('plot.json', 'w') as outfile2:\n    json.dump(\n      {\n        &quot;roc&quot;: [ {&quot;fpr&quot;: f, &quot;tpr&quot;: t} for f, t in zip(fpr, tpr) ]\n      }, \n      outfile2\n      )\n<\/code><\/pre>\n<p>params.yaml:<\/p>\n<pre><code>test:\n  metrics_path: &quot;scores.json&quot;\n  value: 1\n<\/code><\/pre>\n<p>dvc.yaml:<\/p>\n<pre><code>stages:\n  test:\n    cmd: python test.py\n    deps:\n    - test.py\n    params:\n    - test.metrics_path\n    - test.value\n    metrics:\n    - scores.json:\n        cache: false\n    plots:\n    - plot.json:\n        cache: false\n        x: fpr\n        y: tpr\n<\/code><\/pre>\n<p>It is strange behavior. Is it possible to fix it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_creation_time":1654161042840,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":167.0,
        "Answer_body":"<p>I solved my problem. It is necessary, that all files (executable scripts, 'dvc.yaml', 'params.yaml') be tracked by git. In this case <code>dvc exp run<\/code> command works correctly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72473641",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1654244934587,
        "Question_original_content":"launch experi want launch experi set valu experi paramet delet file param yaml experi set queue simplifi code exampl python file test import numpi import json import yaml param yaml safe load open param yaml test precis random random recal param valu accuraci random random row precis precis recal recal accuraci accuraci open param metric path outfil json dump row outfil fpr random random tolist tpr random random tolist open plot json outfil json dump roc fpr tpr zip fpr tpr outfil param yaml test metric path score json valu yaml stage test cmd python test dep test param test metric path test valu metric score json cach fals plot plot json cach fals fpr tpr strang behavior possibl fix",
        "Question_preprocessed_content":"launch experi want launch experi set valu experi paramet delet file experi set queue simplifi code exampl python file yaml strang behavior possibl fix",
        "Answer_original_content":"solv problem necessari file execut script yaml param yaml track git case exp run command work correctli",
        "Answer_preprocessed_content":"solv problem necessari file track git case command work correctli",
        "Question_gpt_summary_original":"the user is encountering challenges with launching experiments in , as the values of experiment parameters are being deleted from the file 'params.yaml', preventing the experiment from being set in the queue.",
        "Question_gpt_summary":"user encount challeng launch experi valu experi paramet delet file param yaml prevent experi set queue",
        "Answer_gpt_summary_original":"the solution to the problem of experiment parameters being deleted from the 'params.yaml' file and preventing the experiment from being set in the queue is to track all files, including executable scripts, '.yaml', and 'params.yaml', by git. this will ensure that the 'exp run' command works correctly.",
        "Answer_gpt_summary":"solut problem experi paramet delet param yaml file prevent experi set queue track file includ execut script yaml param yaml git ensur exp run command work correctli"
    },
    {
        "Question_id":52532078.0,
        "Question_title":"How to find the path to blob?",
        "Question_body":"<p>I want to export a machine learning model I created in Azure Machine Learning studio. One of the required input is \"Path to blob beginning with container\"<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/xvSDd.png\" alt=\"Here is the screenshoot from azure export\"><\/p>\n\n<p>How do I find this path? I have already created a blob storage but I have no idea how to find the path to the blob storage. <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1538034898217,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1538038508827,
        "Question_score":2.0,
        "Question_view_count":15096.0,
        "Answer_body":"<p>you should be able to find this from the Azure portal. Open the storage account, drill down into blobs, then your container. Use properties for the context menu, the URL should be the path ?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/r5hxi.jpg\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52532078",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1538035308943,
        "Question_original_content":"path blob want export machin learn model creat studio requir input path blob begin contain path creat blob storag idea path blob storag",
        "Question_preprocessed_content":"path blob want export machin learn model creat studio requir input path blob begin contain path creat blob storag idea path blob storag",
        "Answer_original_content":"abl azur portal open storag account drill blob contain us properti context menu url path",
        "Answer_preprocessed_content":"abl azur portal open storag account drill blob contain us properti context menu url path",
        "Question_gpt_summary_original":"The user is struggling to find the path to a blob storage in Azure Machine Learning studio in order to export a machine learning model.",
        "Question_gpt_summary":"user struggl path blob storag studio order export machin learn model",
        "Answer_gpt_summary_original":"Possible solutions from the answer are:\n\n- Open the Azure portal and navigate to the storage account.\n- Drill down into the blobs and find the container.\n- Use properties from the context menu to get the URL path.\n\nIn summary, the answer provides steps to locate the path of a blob storage in Azure portal.",
        "Answer_gpt_summary":"possibl solut answer open azur portal navig storag account drill blob contain us properti context menu url path summari answer provid step locat path blob storag azur portal"
    },
    {
        "Question_id":null,
        "Question_title":"Image Classification Algorithm Class Activation Map",
        "Question_body":"Hey!\n\nI am using SageMaker's built in image classification algorithm and currently have a trained model that can make predictions. I am wondering if there is an easy way to use this model to make a class activation map to better debug my classifier and see why it is making the predictions it is?\n\nThanks!",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1552316346000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":29.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUdCD7nXuTJegFm_lGunPzw\/image-classification-algorithm-class-activation-map",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-03-14T17:05:21.000Z",
                "Answer_score":0,
                "Answer_body":"Hi Steveleo,\n\nThanks for trying out the image classification algorithm. The trained model from SageMaker built-in image classification is a standard MXNet model. Here is a notebook https:\/\/github.com\/dmlc\/mxnet-notebooks\/blob\/master\/python\/how_to\/predict.ipynb that you can follow to extract features.\n\nThanks,\nXiong",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-03-14T20:59:33.000Z",
                "Answer_score":0,
                "Answer_body":"For the class activation map, please check this http:\/\/mxnet.incubator.apache.org\/versions\/master\/tutorials\/vision\/cnn_visualization.html .",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"imag classif algorithm class activ map hei built imag classif algorithm current train model predict wonder easi wai us model class activ map better debug classifi make predict thank",
        "Question_preprocessed_content":"imag classif algorithm class activ map hei built imag classif algorithm current train model predict wonder easi wai us model class activ map better debug classifi make predict thank",
        "Answer_original_content":"steveleo thank try imag classif algorithm train model built imag classif standard mxnet model notebook http github com dmlc mxnet notebook blob master python predict ipynb follow extract featur thank xiong class activ map check http mxnet incub apach org version master tutori vision cnn visual html",
        "Answer_preprocessed_content":"steveleo thank try imag classif algorithm train model imag classif standard mxnet model notebook follow extract featur thank xiong class activ map check",
        "Question_gpt_summary_original":"the user is looking for an easy way to use a trained image classification algorithm model to create a class activation map to debug their classifier and understand why it is making the predictions it is.",
        "Question_gpt_summary":"user look easi wai us train imag classif algorithm model creat class activ map debug classifi understand make predict",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. follow the notebook provided in the answer to extract features from the trained model.\n2. check the link provided in the answer for creating a class activation map to debug the classifier and understand its predictions.",
        "Answer_gpt_summary":"possibl solut answer follow notebook provid answer extract featur train model check link provid answer creat class activ map debug classifi understand predict"
    },
    {
        "Question_id":35627916.0,
        "Question_title":"Azure machine learning even sampling",
        "Question_body":"<p>I'm trying to do some basic multi-label classification in Azure ML. I have some basic data in the following format:<\/p>\n\n<pre><code>value_x value_y label\nx1      y1      label1\nx2      y2      label1\nx3      y3      label2\n.....\n<\/code><\/pre>\n\n<p>My problem is that in my data certain labels (out of a total of five) are overrepresented, as about 40% of the data is label1, about 20% is label 2 and the rest around 10%. <\/p>\n\n<p>I would like to get a sampling out of these to train my model, so that each label is represented in equal amounts. <\/p>\n\n<p>Tried the stratification option in the Sampling module on the labels column, but that just gives me a sampling with the same distribution of labels as in the initial dataset.<\/p>\n\n<p>Any idea how I could do this with a module?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1456404822827,
        "Question_favorite_count":null,
        "Question_last_edit_time":1456410979576,
        "Question_score":0.0,
        "Question_view_count":539.0,
        "Answer_body":"<p>I was able to do this using a combination of <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/70530644-c97a-4ab6-85f7-88bf30a8be5f\" rel=\"nofollow\">Split Data<\/a>, <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/a8726e34-1b3e-4515-b59a-3e4a475654b8\" rel=\"nofollow\">Partition and Sample<\/a>, and <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/b2ebdabd-217d-4915-86cc-5b05972f7270\" rel=\"nofollow\">Add Rows<\/a> modules.  There may be an easier way to do it, but I did confirm it works.  :)  I published my work at <a href=\"http:\/\/gallery.azureml.net\/Details\/1245147fd7004e91bc7a3683cda19cc7\" rel=\"nofollow\">http:\/\/gallery.azureml.net\/Details\/1245147fd7004e91bc7a3683cda19cc7<\/a> so you can grab it directly from there, and run to confirm it does what you expect.  <\/p>\n\n<p>Since you said you wanted a sampling of the data, I just reduced each of the labels to 10% to have all labels represented equally.  Since you have a good understanding of the distribution in your dataset, leave label 3, 4, and 5 all at about 10%, and reduce label 1 by 1\/4 and label 2 by 1\/2 to get about 10% of them as well.  <\/p>\n\n<p>To explain what I did in the workspace linked above:<\/p>\n\n<ul>\n<li>I used some \"Split Data\" modules to filter out the label1 and label2 data.  In the Split Data module, change the Splitting mode to \"Regular Expression\" and set the regular expression to <strong>\\\"Label\" ^label1<\/strong> (to get the label1 data, for example).  <\/li>\n<li>Then I used some \"Partition and Sample\" modules to reduce the size of the label1 and label2 data appropriately.  <\/li>\n<li>Finally, I used some \"Add Rows\" modules to join all of the data back together again.  <\/li>\n<\/ul>\n\n<p>Finally, I didn't include this in my work, but you can also look at the <a href=\"https:\/\/msdn.microsoft.com\/library\/azure\/9f3fe1c4-520e-49ac-a152-2e104169912a\" rel=\"nofollow\">SMOTE<\/a> module.  It will increase the number of low-occurring samples using synthetic minority oversampling.  <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35627916",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1456458689160,
        "Question_original_content":"sampl try basic multi label classif basic data follow format valu valu label label label label problem data certain label total overrepres data label label rest like sampl train model label repres equal amount tri stratif option sampl modul label column give sampl distribut label initi dataset idea modul",
        "Question_preprocessed_content":"sampl try basic classif basic data follow format problem data certain label overrepres data label label rest like sampl train model label repres equal amount tri stratif option sampl modul label column give sampl distribut label initi dataset idea modul",
        "Answer_original_content":"abl combin split data partit sampl add row modul easier wai confirm work publish work http galleri net detail fdebcacdacc grab directli run confirm expect said want sampl data reduc label label repres equal good understand distribut dataset leav label reduc label label explain workspac link split data modul filter label label data split data modul chang split mode regular express set regular express label label label data exampl partit sampl modul reduc size label label data appropri final add row modul join data final includ work look smote modul increas number low occur sampl synthet minor oversampl",
        "Answer_preprocessed_content":"abl combin split data partit sampl add row modul easier wai confirm work publish work grab directli run confirm expect said want sampl data reduc label label repres equal good understand distribut dataset leav label reduc label label explain workspac link split data modul filter label label data split data modul chang split mode regular express set regular express label partit sampl modul reduc size label label data appropri final add row modul join data final includ work look smote modul increas number sampl synthet minor oversampl",
        "Question_gpt_summary_original":"the user is encountering a challenge in trying to do multi-label classification in which certain labels are overrepresented, and is looking for a way to sample the data so that each label is represented in equal amounts.",
        "Question_gpt_summary":"user encount challeng try multi label classif certain label overrepres look wai sampl data label repres equal amount",
        "Answer_gpt_summary_original":"the answer suggests using a combination of split data, partition and sample, and add rows modules to sample the data so that each label is represented in equal amounts. the user reduced each of the labels to 10% to have all labels represented equally. the answer also suggests using the smote module to increase the number of low-occurring samples using synthetic minority oversampling.",
        "Answer_gpt_summary":"answer suggest combin split data partit sampl add row modul sampl data label repres equal amount user reduc label label repres equal answer suggest smote modul increas number low occur sampl synthet minor oversampl"
    },
    {
        "Question_id":null,
        "Question_title":"Unable to creata a compute instance",
        "Question_body":"I'm trying to follow the steps given here - https:\/\/docs.microsoft.com\/en-us\/learn\/modules\/explore-analyze-data-with-python\/2-exercise-explore-data\n\nI've tried regions east us2 and east us for creating the instance but it fails after taking more than half an hour. I tried virtual machine sizes - Standard_DS11_v2 & Standard_DS3_v2.\n\nAny help would be appreciated.\n\nEdit - I don't have any other instances running in my subscription, so it should not be a quota issue. The error message says \"An internal server error occurred.\".",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1616247083753,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Good day @AatishSuman-7641\n\nDid you read the comment in the compute page?\n\nPlease confirm that you are using an account which fit the limitations\n\nFor more information please check this post:\n\nhttps:\/\/azure.microsoft.com\/en-us\/blog\/update-2-on-microsoft-cloud-services-continuity\/\n\nNote: I followed the tutorial which you provided the link to and it is working well for me. Therefore, I assume the issue is related to the above comment.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/323742\/unable-to-creata-a-compute-instance.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-20T23:14:19.877Z",
                "Answer_score":0,
                "Answer_body":"Good day @AatishSuman-7641\n\nDid you read the comment in the compute page?\n\nPlease confirm that you are using an account which fit the limitations\n\nFor more information please check this post:\n\nhttps:\/\/azure.microsoft.com\/en-us\/blog\/update-2-on-microsoft-cloud-services-continuity\/\n\nNote: I followed the tutorial which you provided the link to and it is working well for me. Therefore, I assume the issue is related to the above comment.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-03-20T13:46:48.237Z",
                "Answer_score":0,
                "Answer_body":"Hi @AatishSuman-7641\nThank You for posting in Q & A.\n\nPlease verify in azure portal you have Quota available for this virtual machine sizes - Standard_DS11_v2 & Standard_DS3_v2.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/templates\/error-resource-quota#solution\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/networking\/check-usage-against-limits\n\nCan you share exact error message so that we can have more information to provide a solution.\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, this can be beneficial to other community members.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":6.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1616282059876,
        "Question_original_content":"unabl creata comput instanc try follow step given http doc microsoft com learn modul explor analyz data python exercis explor data tri region east east creat instanc fail take half hour tri virtual machin size standard standard help appreci edit instanc run subscript quota issu error messag sai intern server error occur",
        "Question_preprocessed_content":"unabl creata comput instanc try follow step given tri region east east creat instanc fail take half hour tri virtual machin size help appreci edit instanc run subscript quota issu error messag sai intern server error",
        "Answer_original_content":"good dai aatishsuman read comment comput page confirm account fit limit inform check post http azur microsoft com blog updat microsoft cloud servic continu note follow tutori provid link work assum issu relat comment",
        "Answer_preprocessed_content":"good dai read comment comput page confirm account fit limit inform check post note follow tutori provid link work assum issu relat comment",
        "Question_gpt_summary_original":"the user is unable to create a compute instance despite trying different regions and virtual machine sizes, and is receiving an \"internal server error\" message.",
        "Question_gpt_summary":"user unabl creat comput instanc despit try differ region virtual machin size receiv intern server error messag",
        "Answer_gpt_summary_original":"the possible solution to the user's problem of being unable to create a compute instance despite trying different regions and virtual machine sizes and receiving an \"internal server error\" message is to confirm that they are using an account that fits the limitations. the user is advised to check a post for more information.",
        "Answer_gpt_summary":"possibl solut user problem unabl creat comput instanc despit try differ region virtual machin size receiv intern server error messag confirm account fit limit user advis check post inform"
    },
    {
        "Question_id":null,
        "Question_title":"Integrate Logic Apps with Machine Learning",
        "Question_body":"Hello everyone,\n\nIs it possible to integrate Logic Apps with Machine Learning so that i can trigger the Logic App and then the Logic App starts a ML model training automatically?\n\nThank you!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1624612238463,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/451893\/integrate-logic-apps-with-machine-learning.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-28T11:28:51.723Z",
                "Answer_score":1,
                "Answer_body":"@MichalisPapallis-0974 With respect to Azure Machine learning you can create trigger for HTTP action to run published ML pipelines, these are basically endpoints that are published of the pipeline. So, if you have setup your pipeline using the SDK then you can use that endpoint with a trigger to run the pipeline.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"integr logic app machin learn hello possibl integr logic app machin learn trigger logic app logic app start model train automat thank",
        "Question_preprocessed_content":"integr logic app machin learn hello possibl integr logic app machin learn trigger logic app logic app start model train automat thank",
        "Answer_original_content":"michalispap respect creat trigger http action run publish pipelin basic endpoint publish pipelin setup pipelin sdk us endpoint trigger run pipelin",
        "Answer_preprocessed_content":"respect creat trigger http action run publish pipelin basic endpoint publish pipelin setup pipelin sdk us endpoint trigger run pipelin",
        "Question_gpt_summary_original":"the user is looking to integrate logic apps with machine learning to trigger a ml model training automatically.",
        "Question_gpt_summary":"user look integr logic app machin learn trigger model train automat",
        "Answer_gpt_summary_original":"possible solution: the user can create a trigger for an http action to run published ml pipelines, which are endpoints published from the pipeline. if the pipeline has been set up using the sdk, the user can use that endpoint with a trigger to run the pipeline.",
        "Answer_gpt_summary":"possibl solut user creat trigger http action run publish pipelin endpoint publish pipelin pipelin set sdk user us endpoint trigger run pipelin"
    },
    {
        "Question_id":63642175.0,
        "Question_title":"Error when invoking AWS SageMaker endpoint using boto3 : \"Unable to parse data as JSON. Make sure the Content-Type header is set to \"application\/json\"",
        "Question_body":"<p>I am trying to invoke an AWS SageMaker endpoint through the following simple code using boto3<\/p>\n<pre><code>import boto3\n\nsession = boto3.Session(profile_name='mlacc',\n                        region_name='us-west-2')\n\nsagemaker_client = session.client('sagemaker-runtime')\n\nrequest_body = &quot;{\\n    \\&quot;requestSource\\&quot;: \\&quot;unittest\\&quot;,\\n    \\&quot;clusters\\&quot;: [{\\n        \\&quot;clusterMetadata\\&quot;: {\\n &quot;\n&quot;\\&quot;clusterId\\&quot;: \\&quot;id1\\&quot;,\\n            \\&quot;topic\\&quot;: [\\&quot;corona virus\\&quot;, \\&quot;Donald Trump\\&quot;],\\n            &quot;\n&quot;\\&quot;clusterSize\\&quot;: 2\\n        },\\n        \\&quot;documents\\&quot;: [{\\n            \\&quot;uid\\&quot;: \\&quot;1\\&quot;,\\n            &quot;\n&quot;\\&quot;content\\&quot;: \\&quot;content2\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: \\&quot;This is a &quot;\n&quot;title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,\\n            &quot;\n&quot;\\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }, {\\n            \\&quot;uid\\&quot;: \\&quot;2\\&quot;,&quot;\n&quot;\\n            \\&quot;content\\&quot;: \\&quot;content2\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: &quot;\n&quot;\\&quot;This is a title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,&quot;\n&quot;\\n            \\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }, {\\n            \\&quot;uid\\&quot;: &quot;\n&quot;\\&quot;2\\&quot;,\\n            \\&quot;content\\&quot;: \\&quot;content3\\&quot;,\\n            \\&quot;domain\\&quot;: \\&quot;CNN.com\\&quot;,\\n            \\&quot;title\\&quot;: &quot;\n&quot;\\&quot;This is a title\\&quot;,\\n            \\&quot;similarityScore\\&quot;: 1.3,\\n            \\&quot;published_at\\&quot;: 1566264017,&quot;\n&quot;\\n            \\&quot;domain_rank\\&quot;: 1,\\n            \\&quot;trust_domain_score\\&quot;: 1\\n        }]\\n    }]\\n}&quot;\n\n\nresponse = sagemaker_client.invoke_endpoint(\n    EndpointName='myEndpoint22',\n    Body=request_body,\n    ContentType='application\/json',\n)\n\nresponse_json = response['Body'].read().decode('utf-8')\n\nprint(response_json)\n<\/code><\/pre>\n<p>I get the following error when I run this code<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/Users\/rppatwa\/Desktop\/WorkDocs\/CodePlayground\/SimplePythonProject\/src\/PrototypeTesting\/SummarizationLocal.py&quot;, line 205, in &lt;module&gt;\n    main()\n  File &quot;\/Users\/rppatwa\/Desktop\/WorkDocs\/CodePlayground\/SimplePythonProject\/src\/PrototypeTesting\/SummarizationLocal.py&quot;, line 186, in main\n    ContentType='application\/json',\n  File &quot;\/Users\/rppatwa\/anaconda3\/lib\/python3.7\/site-packages\/botocore\/client.py&quot;, line 316, in _api_call\n    return self._make_api_call(operation_name, kwargs)\n  File &quot;\/Users\/rppatwa\/anaconda3\/lib\/python3.7\/site-packages\/botocore\/client.py&quot;, line 635, in _make_api_call\n    raise error_class(parsed_response, operation_name)\nbotocore.errorfactory.ModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received client error (400) from model with message &quot;Unable to parse data as JSON. Make sure the Content-Type header is set to &quot;application\/json&quot;&quot;. See https:\/\/us-west-2.console.aws.amazon.com\/cloudwatch\/home?region=us-west-2#logEventViewer:group=\/aws\/sagemaker\/Endpoints\/KeyurshaASMLModel in account 753843489946 for more information.\n<\/code><\/pre>\n<p>If I inline the Body (not using the request_json) this call succeeds. Please let me know what I am missing.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_creation_time":1598659048840,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1170.0,
        "Answer_body":"<p>You need to remove the trailing comma after  <code>ContentType='application\/json',<\/code> and try below snippet for passing JSON to body field.<\/p>\n<pre><code>import json \njson.dumps(request_body) \ntest=json.dumps(request_body).encode()\n<\/code><\/pre>\n<p>This will also validate the JSON that you are passing.Now pass test to body for invoking the endopoint.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63642175",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1598927909716,
        "Question_original_content":"error invok endpoint boto unabl pars data json sure content type header set applic json try invok endpoint follow simpl code boto import boto session boto session profil mlacc region west client session client runtim request bodi requestsourc unittest cluster clustermetadata clusterid topic corona viru donald trump clusters document uid content content domain cnn com titl titl similarityscor publish domain rank trust domain score uid content content domain cnn com titl titl similarityscor publish domain rank trust domain score uid content content domain cnn com titl titl similarityscor publish domain rank trust domain score respons client invok endpoint endpointnam myendpoint bodi request bodi contenttyp applic json respons json respons bodi read decod utf print respons json follow error run code traceback recent file user rppatwa desktop workdoc codeplayground simplepythonproject src prototypetest summarizationloc line main file user rppatwa desktop workdoc codeplayground simplepythonproject src prototypetest summarizationloc line main contenttyp applic json file user rppatwa anaconda lib python site packag botocor client line api return self api oper kwarg file user rppatwa anaconda lib python site packag botocor client line api rais error class pars respons oper botocor errorfactori modelerror error occur modelerror call invokeendpoint oper receiv client error model messag unabl pars data json sure content type header set applic json http west consol aw amazon com cloudwatch home region west logeventview group aw endpoint keyurshaasmlmodel account inform inlin bodi request json succe let know miss thank",
        "Question_preprocessed_content":"error invok endpoint boto unabl pars data json sure header set try invok endpoint follow simpl code boto follow error run code inlin bodi succe let know miss thank",
        "Answer_original_content":"need remov trail comma contenttyp applic json try snippet pass json bodi field import json json dump request bodi test json dump request bodi encod valid json pass pass test bodi invok endopoint",
        "Answer_preprocessed_content":"need remov trail comma try snippet pass json bodi field valid json pass test bodi invok endopoint",
        "Question_gpt_summary_original":"the user is encountering an error when invoking an endpoint using boto3, receiving a \"unable to parse data as json. make sure the content-type header is set to \"application\/json\" error.",
        "Question_gpt_summary":"user encount error invok endpoint boto receiv unabl pars data json sure content type header set applic json error",
        "Answer_gpt_summary_original":"the solution to the error encountered when invoking an endpoint using boto3 is to remove the trailing comma after contenttype='application\/json'. additionally, the user should try passing the json to the body field using the provided code snippet, which includes importing and encoding the json to validate it before passing it to the endpoint.",
        "Answer_gpt_summary":"solut error encount invok endpoint boto remov trail comma contenttyp applic json addition user try pass json bodi field provid code snippet includ import encod json valid pass endpoint"
    },
    {
        "Question_id":60643094.0,
        "Question_title":"Unable to open pickled Sagemaker XGBoost model",
        "Question_body":"<p>I'm trying to open a pickled XGBoost model I created in AWS Sagemaker to look at feature importances in the model. I'm trying to follow the answers in <a href=\"https:\/\/stackoverflow.com\/questions\/55621967\/feature-importance-for-xgboost-in-sagemaker\">this post<\/a>. However, I get an the error shown below. When I try to call <code>Booster.save_model<\/code>, I get an error saying <code>'Estimator' object has no attribute 'save_model'<\/code>. How can I resolve this? <\/p>\n\n<pre><code># Build initial model\nsess = sagemaker.Session()\ns3_input_train = sagemaker.s3_input(s3_data='s3:\/\/{}\/{}\/train\/'.format(bucket, prefix), content_type='csv')\nxgb_cont = get_image_uri(region, 'xgboost', repo_version='0.90-1')\nxgb = sagemaker.estimator.Estimator(xgb_cont, role, train_instance_count=1, train_instance_type='ml.m4.4xlarge',\n                                    output_path='s3:\/\/{}\/{}'.format(bucket, prefix), sagemaker_session=sess)\nxgb.set_hyperparameters(eval_metric='rmse', objective='reg:squarederror', num_round=100)\nts = strftime(\"%Y-%m-%d-%H-%M-%S\", gmtime())\nxgb_name = 'xgb-initial-' + ts\nxgb.set_hyperparameters(eta=0.1, alpha=0.5, max_depth=10)\nxgb.fit({'train': s3_input_train}, job_name=xgb_name)\n\n# Load model to get feature importances\nmodel_path = 's3:\/\/{}\/{}\/\/output\/model.tar.gz'.format(bucket, prefix, xgb_name)\nfs = s3fs.S3FileSystem()\nwith fs.open(model_path, 'rb') as f:\n    with tarfile.open(fileobj=f, mode='r') as tar_f:\n        with tar_f.extractfile('xgboost-model') as extracted_f:\n            model = pickle.load(extracted_f)\n\nXGBoostError: [19:16:42] \/workspace\/src\/learner.cc:682: Check failed: header == serialisation_header_: \n\n  If you are loading a serialized model (like pickle in Python) generated by older\n  XGBoost, please export the model by calling `Booster.save_model` from that version\n  first, then load it back in current version.  There's a simple script for helping\n  the process. See:\n\n    https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\n\n  for reference to the script, and more details about differences between saving model and\n  serializing.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1583954281893,
        "Question_favorite_count":null,
        "Question_last_edit_time":1583958473892,
        "Question_score":4.0,
        "Question_view_count":4586.0,
        "Answer_body":"<p>Which version of XGBoost are you using in the notebook? The model format has changed in XGBoost 1.0. See <a href=\"https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html\" rel=\"nofollow noreferrer\">https:\/\/xgboost.readthedocs.io\/en\/latest\/tutorials\/saving_model.html<\/a>. Short version: if you're using 1.0 in the notebook, you can't load a pickled model. <\/p>\n\n<p>Here's a working example using XGBoost in script mode (which is much more flexible than the built in algo):<\/p>\n\n<ul>\n<li><a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/09-XGBoost-script-mode.ipynb\" rel=\"nofollow noreferrer\">https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/09-XGBoost-script-mode.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/xgb.py\" rel=\"nofollow noreferrer\">https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/-\/blob\/master\/sagemaker\/xgb.py<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60643094",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1584357657183,
        "Question_original_content":"unabl open pickl xgboost model try open pickl xgboost model creat look featur import model try follow answer post error shown try booster save model error sai estim object attribut save model resolv build initi model sess session input train input data train format bucket prefix content type csv xgb cont imag uri region xgboost repo version xgb estim estim xgb cont role train instanc count train instanc type xlarg output path format bucket prefix session sess xgb set hyperparamet eval metric rmse object reg squarederror num round strftime gmtime xgb xgb initi xgb set hyperparamet eta alpha max depth xgb fit train input train job xgb load model featur import model path output model tar format bucket prefix xgb sf sfilesystem open model path tarfil open fileobj mode tar tar extractfil xgboost model extract model pickl load extract xgboosterror workspac src learner check fail header serialis header load serial model like pickl python gener older xgboost export model call booster save model version load current version simpl script help process http xgboost readthedoc latest tutori save model html refer script detail differ save model serial",
        "Question_preprocessed_content":"unabl open pickl xgboost model try open pickl xgboost model creat look featur import model try follow answer post error shown try error sai resolv",
        "Answer_original_content":"version xgboost notebook model format chang xgboost http xgboost readthedoc latest tutori save model html short version notebook load pickl model work exampl xgboost script mode flexibl built algo http gitlab com juliensimon dlnotebook blob master xgboost script mode ipynb http gitlab com juliensimon dlnotebook blob master xgb",
        "Answer_preprocessed_content":"version xgboost notebook model format chang xgboost short version notebook load pickl model work exampl xgboost script mode",
        "Question_gpt_summary_original":"the user is encountering challenges when trying to open a pickled xgboost model to look at feature importances.",
        "Question_gpt_summary":"user encount challeng try open pickl xgboost model look featur import",
        "Answer_gpt_summary_original":"the answer suggests that the user should check the version of xgboost they are using in the notebook, as the model format has changed in xgboost 1.0. if the user is using 1.0 in the notebook, they won't be able to load a pickled model. the answer provides a working example using xgboost in script mode, which is more flexible than the built-in algorithm.",
        "Answer_gpt_summary":"answer suggest user check version xgboost notebook model format chang xgboost user notebook won abl load pickl model answer provid work exampl xgboost script mode flexibl built algorithm"
    },
    {
        "Question_id":null,
        "Question_title":"Save long tensors using wandb.save OR wandb.log?",
        "Question_body":"<p>Hi all,<br>\nIn my experimentation, I want to save some tensor_10k_size for a dataset which has 90k samples.  so that I can compare how the tensors look for different datasets. It\u2019s part of my thesis.<br>\nWhich is a better option to save tensors in my scripts using wandb.save(\u2018filename\u2019)<br>\nor<br>\nwandb.log(tensor)<\/p>\n<p>Thanks,<br>\nPrachi<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1657564024663,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":222.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/save-long-tensors-using-wandb-save-or-wandb-log\/2737",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-07-11T18:57:46.141Z",
                "Answer_body":"<p>Hi Prachi,<\/p>\n<p>It completely depends on what you are trying to do with the tensor. If you are trying to visualize the tensor (a tensor is usually presented as a histogram - unless it is of shape (W, H, C) in which case it can be coerced to an image - it can be logged using <code>wandb.log<\/code>.<\/p>\n<p>If you want to load up those tensors again to be used in your code -<code>wandb.save<\/code> would be the way to go.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":20.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-07-13T01:42:51.237Z",
                "Answer_body":"<p>Hi Ramit,<br>\nThat was my hazy view, you made it clear.<\/p>\n<p>Thanks,<br>\nPrachi<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-11T01:43:35.647Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"save long tensor save log experiment want save tensor size dataset sampl compar tensor look differ dataset thesi better option save tensor script save filenam log tensor thank prachi",
        "Question_preprocessed_content":"save long tensor save log experiment want save dataset sampl compar tensor look differ dataset thesi better option save tensor script save log thank prachi",
        "Answer_original_content":"prachi complet depend try tensor try visual tensor tensor usual present histogram shape case coerc imag log log want load tensor code save wai thank ramit ramit hazi view clear thank prachi topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"prachi complet depend try tensor try visual tensor case coerc imag log want load tensor code wai thank ramit ramit hazi view clear thank prachi topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking for the best way to save long tensors of 10k size for a dataset of 90k samples for their thesis.",
        "Question_gpt_summary":"user look best wai save long tensor size dataset sampl thesi",
        "Answer_gpt_summary_original":"there are no specific solutions provided in the answer for saving long tensors of 10k size for a dataset of 90k samples. however, the answer suggests that the method of saving the tensor depends on the intended use of the tensor. if the tensor is to be visualized, it can be logged using .log, and if it needs to be loaded again for use in the code, .save would be the way to go.",
        "Answer_gpt_summary":"specif solut provid answer save long tensor size dataset sampl answer suggest method save tensor depend intend us tensor tensor visual log log need load us code save wai"
    },
    {
        "Question_id":null,
        "Question_title":"Differences between dvc repro and dvc status",
        "Question_body":"<p>Is it guaranteed that if <code>dvc status Dvcfile<\/code> shows: nothing to reproduce, the <code>dvc repro Dvcfile<\/code> will not do anything ?<\/p>\n<p>For my use case, it looks like <code>dvc repro<\/code> (even when there is nothing to reproduce) takes more time to run than <code>dvc status<\/code>. I could then switch to calling <code>dvc status<\/code> to find out if there\u2019s nothing to reproduce and have a faster pipeline.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1537891906524,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":736.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/differences-between-dvc-repro-and-dvc-status\/101",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-09-26T18:02:53.181Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tmain\">@tmain<\/a> !<\/p>\n<p>Sorry for a late response.<\/p>\n<blockquote>\n<p>Is it guaranteed that if  <code>dvc status Dvcfile<\/code>  shows: nothing to reproduce, the  <code>dvc repro Dvcfile<\/code>  will not do anything ?<\/p>\n<\/blockquote>\n<p>Yes, it is indeed guaranteed.<\/p>\n<blockquote>\n<p>For my use case, it looks like  <code>dvc repro<\/code>  (even when there is nothing to reproduce) takes more time to run than  <code>dvc status<\/code> . I could then switch to calling  <code>dvc status<\/code>  to find out if there\u2019s nothing to reproduce and have a faster pipeline.<\/p>\n<\/blockquote>\n<p>Repro works in a bit different way than the status does so there might be differences in processing time. Sorry, I\u2019m not sure I follow, could you please elaborate on what you mean by <code>switch<\/code>?<\/p>\n<p>Thanks,<br>\nRuslan<\/p>",
                "Answer_score":11.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-09-26T18:17:13.571Z",
                "Answer_body":"<p>By <code>switch<\/code> I meant the following.<\/p>\n<p>In order to find out whether I need to repro something, I used to call <code>dvc repro Dvcfile --dry<\/code> and parse the output of that command. If I got <code>Nothing to reproduce<\/code>, I would continue without calling <code>dvc repro Dvcfile<\/code>.<\/p>\n<p>Now instead, I can do <code>dvc status Dvcfile<\/code>, find out whether there is something to reproduce, and call <code>dvc repro Dvcfile<\/code> if there is.<\/p>",
                "Answer_score":6.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2018-09-26T19:09:08.118Z",
                "Answer_body":"<p>Ah, got it. Thank you for clarifying! Indeed, <code>dvc status<\/code> was intended to be used  precisely for checking if anything in your pipeline has changed and requires reproduction. And <code>repro --dry<\/code> is actually intended for visualizing the order and list of the commands that will be run in order to reproduce the pipeline <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=6\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":1.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"differ repro statu guarante statu file show reproduc repro file us case look like repro reproduc take time run statu switch call statu there reproduc faster pipelin",
        "Question_preprocessed_content":"differ repro statu guarante show reproduc us case look like take time run switch call there reproduc faster pipelin",
        "Answer_original_content":"tmain sorri late respons guarante statu file show reproduc repro file ye guarante us case look like repro reproduc take time run statu switch call statu there reproduc faster pipelin repro work bit differ wai statu differ process time sorri sure follow elabor mean switch thank ruslan switch meant follow order need repro repro file dry pars output command got reproduc continu call repro file instead statu file reproduc repro file got thank clarifi statu intend precis check pipelin chang requir reproduct repro dry actual intend visual order list command run order reproduc pipelin",
        "Answer_preprocessed_content":"sorri late respons guarante show reproduc ye guarante us case look like take time run switch call there reproduc faster pipelin repro work bit differ wai statu differ process time sorri sure follow elabor mean thank ruslan meant follow order need repro pars output command got continu call instead reproduc got thank clarifi intend precis check pipelin chang requir reproduct actual intend visual order list command run order reproduc pipelin",
        "Question_gpt_summary_original":"the user is facing a challenge of determining whether it is more efficient to use the repro or status file to determine if there is nothing to reproduce, as the repro file takes more time to run than the status file.",
        "Question_gpt_summary":"user face challeng determin effici us repro statu file determin reproduc repro file take time run statu file",
        "Answer_gpt_summary_original":"the answer suggests that the user can switch from using the repro file to using the status file to determine if there is nothing to reproduce, as the repro file takes more time to run than the status file. the user can call the status file to find out if there is anything to reproduce and then call the repro file if there is. the status file is intended to be used for checking if anything in the pipeline has changed and requires reproduction, while the repro --dry is intended for visualizing the order and list of the commands that will be run to reproduce the pipeline.",
        "Answer_gpt_summary":"answer suggest user switch repro file statu file determin reproduc repro file take time run statu file user statu file reproduc repro file statu file intend check pipelin chang requir reproduct repro dry intend visual order list command run reproduc pipelin"
    },
    {
        "Question_id":null,
        "Question_title":"Why is the different components not grouped in the designer any more?",
        "Question_body":"Hi, Guys,\nI'm new to Azure Ml, and when i started using the Designer, the different components where grouped.\nWhen you chose for example \"model scoring and evaluation\", and then all components related to that topic was there.\nNow it's like all components available is just listed in alphabetic order.\nWhat is the point with that change? and can i get the grouped components back?",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1661248060107,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/978187\/why-is-the-different-components-not-grouped-in-the.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-23T11:52:22.877Z",
                "Answer_score":0,
                "Answer_body":"@PedersenBjarke-7167 Yes, you could select the filter next to search and select Built-in to see the grouped components again.\nThe change allows users to view only the required modules or components and also filter then according to the available criteria. If you want to go back to the older view, simply select the built in assets only to group by functionality.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.\n\n\n\n\n\nRef: https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/972775\/change-in-machine-learning-designer.html",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"differ compon group design gui new start design differ compon group chose exampl model score evalu compon relat topic like compon avail list alphabet order point chang group compon",
        "Question_preprocessed_content":"differ compon group design gui new start design differ compon group chose exampl model score evalu compon relat topic like compon avail list alphabet order point chang group compon",
        "Answer_original_content":"pedersenbjark ye select filter search select built group compon chang allow user view requir modul compon filter accord avail criteria want older view simpli select built asset group function answer help click upvot help commun member read thread ref http doc microsoft com answer question chang machin learn design html",
        "Answer_preprocessed_content":"ye select filter search select group compon chang allow user view requir modul compon filter accord avail criteria want older view simpli select built asset group function answer help click upvot help commun member read thread ref",
        "Question_gpt_summary_original":"the user is encountering a challenge with the designer, as the components are no longer grouped and instead listed in alphabetical order.",
        "Question_gpt_summary":"user encount challeng design compon longer group instead list alphabet order",
        "Answer_gpt_summary_original":"possible solutions to the challenge of components no longer being grouped in the designer include selecting the filter next to search and choosing \"built-in\" to see the grouped components again. users can also filter components according to available criteria. to go back to the older view, users can select \"built-in assets only\" to group by functionality.",
        "Answer_gpt_summary":"possibl solut challeng compon longer group design includ select filter search choos built group compon user filter compon accord avail criteria older view user select built asset group function"
    },
    {
        "Question_id":null,
        "Question_title":"Get a link to share project",
        "Question_body":"<p>I have an existing project. I want to be able to make it public and share it to someone so that they can take a look at the graphs<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1644130321779,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":409.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/get-a-link-to-share-project\/1873",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-02-07T14:58:08.519Z",
                "Answer_body":"<p>Hi Aditya!<\/p>\n<p>You would need to turn off the \u201cForce all projects in this team to be private\u201d setting in your user settings (<a href=\"http:\/\/wandb.ai\/settings\" class=\"inline-onebox-loading\">wandb.ai\/settings<\/a>) to make this project Public or Open. Please let me know if you still run into this issue.<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-02-10T15:27:45.701Z",
                "Answer_body":"<p>Hi Aditya, this was a bug that was fixed earlier this week. Please let me know if you still run into this issue<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-07T06:52:52.497Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"link share project exist project want abl public share look graph",
        "Question_preprocessed_content":"link share project exist project want abl public share look graph",
        "Answer_original_content":"aditya need turn forc project team privat set user set set project public open let know run issu warmli lesli aditya bug fix earlier week let know run issu topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"aditya need turn forc project team privat set user set project public open let know run issu warmli lesli aditya bug fix earlier week let know run issu topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking for a way to make their existing project public and share it with someone so they can view the graphs.",
        "Question_gpt_summary":"user look wai exist project public share view graph",
        "Answer_gpt_summary_original":"possible solutions: \n- turn off the \"force all projects in this team to be private\" setting in user settings to make the project public or open. \n- check if the issue still persists as it may have been a bug that was fixed earlier.",
        "Answer_gpt_summary":"possibl solut turn forc project team privat set user set project public open check issu persist bug fix earlier"
    },
    {
        "Question_id":null,
        "Question_title":"Embed Plotly Graphs in HTML",
        "Question_body":"<p>I would like to use some of my wandb plots to <a href=\"https:\/\/plotly.com\/python\/embedding-plotly-graphs-in-HTML\/\" rel=\"noopener nofollow ugc\">Embed Plotly Graphs in HTML<\/a>. Is there anyway to get similar functionality? If it cannot be done directly on wandb I see the other option as downloading the graphs, and uploading them to Plotly Chart Studio where they can then be embedded in HTML.<\/p>\n<p>I do like reports, but this would allow for the inclusion of plots in a wider scope of documents. For example, papers for publication, books, or even a dissertation.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1661353640713,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":130.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/embed-plotly-graphs-in-html\/2986",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-29T15:35:37.353Z",
                "Answer_body":"<p>Hi Michael,<\/p>\n<p>Thank you for this feature request! At this moment we don\u2019t have this functionality. At the moment you can share wandb charts by exporting them by clicking on the top right of the chart and clicking on the three dots. From there, you can navigate to \u2018Export panel\u2019 to get a variety of ways to share it. We do have a ticket out to allow for this functionality though and I\u2019ll let you know if we have any updates on it.<\/p>\n<p>Warmly,<br>\nLeslie<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-23T15:08:20.359Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"emb plotli graph html like us plot emb plotli graph html similar function directli option download graph upload plotli chart studio embed html like report allow inclus plot wider scope document exampl paper public book dissert",
        "Question_preprocessed_content":"emb plotli graph html like us plot emb plotli graph html similar function directli option download graph upload plotli chart studio embed html like report allow inclus plot wider scope document exampl paper public book dissert",
        "Answer_original_content":"michael thank featur request moment dont function moment share chart export click right chart click dot navig export panel varieti wai share ticket allow function ill let know updat warmli lesli topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"michael thank featur request moment dont function moment share chart export click right chart click dot navig export panel varieti wai share ticket allow function ill let know updat warmli lesli topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user would like to embed plotly graphs in html, and is considering downloading the graphs and uploading them to plotly chart studio as an alternative.",
        "Question_gpt_summary":"user like emb plotli graph html consid download graph upload plotli chart studio altern",
        "Answer_gpt_summary_original":"there is currently no functionality to directly embed plotly graphs in html, but the user can export the charts and share them in various ways. the plotly team has a ticket to allow for this functionality in the future.",
        "Answer_gpt_summary":"current function directli emb plotli graph html user export chart share wai plotli team ticket allow function futur"
    },
    {
        "Question_id":70637798.0,
        "Question_title":"wandb: artifact.add_reference() option to add specific (not current) versionId or ETag to stop the need for re-upload to s3?",
        "Question_body":"<p>I feel like this should be possible, but I looked through the wandb SDK code and I can't find an easy\/logical way to do it. It <em>might<\/em> be possible to hack it by modifying the manifest entries at some point later (but maybe before the artifact is logged to wandb as then the manifest and the entries might be locked)? I saw things like this in the SDK code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>version = manifest_entry.extra.get(&quot;versionID&quot;)\netag = manifest_entry.extra.get(&quot;etag&quot;)\n<\/code><\/pre>\n<p>So, I figure we can probably edit those?<\/p>\n<h2>UPDATE<\/h2>\n<p>So, I tried to hack it together with something like this and it works but it feels wrong:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\nAPI_KEY = os.environ.get(&quot;WANDB_API_KEY&quot;)\n\napi = api = wandb.Api(overrides={&quot;entity&quot;: ENTITY, &quot;project&quot;: ENTITY})\nrun = wandb.init(entity=ENTITY, project=PROJECT, job_type=&quot;test upload&quot;)\nfile = &quot;admin2Codes.txt&quot;  # &quot;admin1CodesASCII.txt&quot; # (both already on s3 with a couple versions)\nartifact = wandb.Artifact(&quot;test_data&quot;, type=&quot;dataset&quot;)\n\n# modify one of the local files so it has a new md5hash etc.\nwith open(file, &quot;a&quot;) as f:\n    f.write(&quot;new_line_1\\n&quot;)\n\n# upload local file to s3\nlocal_file_path = file\ns3_url = f&quot;s3:\/\/bucket\/prefix\/{file}&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\nhead_response = s3_client.head_object(Bucket=s3_bucket, Key=key)\nversion_id: str = head_response[&quot;VersionId&quot;]\nprint(version_id)\n\n# upload a link\/ref to this s3 object in wandb:\nartifact.add_reference(s3_dir)\n# at this point we might be able to modify the artifact._manifest.entries and each entry.extra.get(&quot;etag&quot;) etc.?\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n# set these to an older version on s3 that we know we want (rather than latest) - do this via wandb public API:\ndataset_v2 = api.artifact(f&quot;{ENTITY}\/{PROJECT}\/test_data:v2&quot;, type=&quot;dataset&quot;)\n# artifact._manifest.add_entry(dataset_v2.manifest.entries[&quot;admin1CodesASCII.txt&quot;])\nartifact._manifest.entries[&quot;admin1CodesASCII.txt&quot;] = dataset_v2.manifest.entries[\n    &quot;admin1CodesASCII.txt&quot;\n]\n# verify that it did change:\nprint([(name, entry.extra) for name, entry in artifact._manifest.entries.items()])\n\nrun.log_artifact(artifact)  # at this point the manifest is locked I believe?\nartifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\nprint(artifact.name)\nrun_id = run.id\nrun.finish()\ncurr_run = api.run(f&quot;{ENTITY}\/{PROJECT}\/{run_id}&quot;)\nused_artifacts = curr_run.used_artifacts()\nlogged_artifacts = curr_run.logged_artifacts()\n<\/code><\/pre>\n<p>Am I on the right track here? I guess the other workaround is to make a copy on s3 (so that older version is the latest again) but I wanted to avoid this as the 1 file that I want to use an old version of is a large NLP model and the only files I want to change are small config.json files etc. (so seems very wasteful to upload all files again).<\/p>\n<p>I was also wondering if when I copy an old version of an object back into the same key in the bucket if that creates a real copy or just like a pointer to the same underlying object. Neither boto3 nor AWS documentation makes that clear - although it seems like it is a proper copy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1641691804600,
        "Question_favorite_count":null,
        "Question_last_edit_time":1643119769043,
        "Question_score":0.0,
        "Question_view_count":121.0,
        "Answer_body":"<p>I think I found the correct way to do it now:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport wandb\nimport boto3\nfrom wandb.util import md5_file\n\nENTITY = os.environ.get(&quot;WANDB_ENTITY&quot;)\nPROJECT = os.environ.get(&quot;WANDB_PROJECT&quot;)\n\n\ndef wandb_update_only_some_files_in_artifact(\n    existing_artifact_name: str,\n    new_s3_file_urls: list[str],\n    entity: str = ENTITY,\n    project: str = PROJECT,\n) -&gt; Artifact:\n    &quot;&quot;&quot;If you want to just update a config.json file for example,\n    but the rest of the artifact can remain the same, then you can\n    use this functions like so:\n    wandb_update_only_some_files_in_artifact(\n        &quot;old_artifact:v3&quot;,\n        [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n    )\n    and then all the other files like model.bin will be the same as in v3,\n    even if there was a v4 or v5 in between (as the v3 VersionIds are used)\n\n    Args:\n        existing_artifact_name (str): name with version like &quot;old_artifact:v3&quot;\n        new_s3_file_urls (list[str]): files that should be updated\n        entity (str, optional): wandb entity. Defaults to ENTITY.\n        project (str, optional): wandb project. Defaults to PROJECT.\n\n    Returns:\n        Artifact: the new artifact object\n    &quot;&quot;&quot;\n    api = wandb.Api(overrides={&quot;entity&quot;: entity, &quot;project&quot;: project})\n    old_artifact = api.artifact(existing_artifact_name)\n    old_artifact_name = re.sub(r&quot;:v\\d+$&quot;, &quot;&quot;, old_artifact.name)\n    with wandb.init(entity=entity, project=project) as run:\n        new_artifact = wandb.Artifact(old_artifact_name, type=old_artifact.type)\n\n        s3_file_names = [s3_url.split(&quot;\/&quot;)[-1] for s3_url in new_s3_file_urls]\n        # add the new ones:\n        for s3_url, filename in zip(new_s3_file_urls, s3_file_names):\n            new_artifact.add_reference(s3_url, filename)\n        # add the old ones:\n        for filename, entry in old_artifact.manifest.entries.items():\n            if filename in s3_file_names:\n                continue\n            new_artifact.add_reference(entry, filename)\n            # this also works but feels hackier:\n            # new_artifact._manifest.entries[filename] = entry\n\n        run.log_artifact(new_artifact)\n        new_artifact.wait()  # wait for upload to finish (blocking - but should be very quick given it is just an s3 link)\n        print(new_artifact.name)\n        print(run.id)\n    return new_artifact\n\n\n# usage:\nlocal_file_path = &quot;config.json&quot; # modified file\ns3_url = &quot;s3:\/\/bucket\/prefix\/config.json&quot;\ns3_url_arr = s3_url.replace(&quot;s3:\/\/&quot;, &quot;&quot;).split(&quot;\/&quot;)\ns3_bucket = s3_url_arr[0]\nkey = &quot;\/&quot;.join(s3_url_arr[1:])\n\ns3_client = boto3.client(&quot;s3&quot;)\nfile_digest = md5_file(local_file_path)\ns3_client.upload_file(\n    local_file_path,\n    s3_bucket,\n    key,\n    # save the md5_digest in metadata,\n    # can be used later to only upload new files to s3,\n    # as AWS doesn't digest the file consistently in the E-tag\n    ExtraArgs={&quot;Metadata&quot;: {&quot;md5_digest&quot;: file_digest}},\n)\n\nwandb_update_only_some_files_in_artifact(\n    &quot;old_artifact:v3&quot;,\n    [&quot;s3:\/\/bucket\/prefix\/config.json&quot;],\n)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1643119813376,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70637798",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643119052816,
        "Question_original_content":"artifact add refer option add specif current versionid etag stop need upload feel like possibl look sdk code easi logic wai possibl hack modifi manifest entri point later mayb artifact log manifest entri lock saw thing like sdk code version manifest entri extra versionid etag manifest entri extra etag figur probabl edit updat tri hack like work feel wrong import import import boto util import file entiti environ entiti project environ project api kei environ api kei api api api overrid entiti entiti project entiti run init entiti entiti project project job type test upload file admincod txt admincodesascii txt coupl version artifact artifact test data type dataset modifi local file new mdhash open file write new line upload local file local file path file url bucket prefix file url arr url replac split bucket url arr kei join url arr client boto client file digest file local file path client upload file local file path bucket kei save digest metadata later upload new file aw digest file consist tag extraarg metadata digest file digest head respons client head object bucket bucket kei kei version str head respons versionid print version upload link ref object artifact add refer dir point abl modifi artifact manifest entri entri extra etag print entri extra entri artifact manifest entri item set older version know want latest public api dataset api artifact entiti project test data type dataset artifact manifest add entri dataset manifest entri admincodesascii txt artifact manifest entri admincodesascii txt dataset manifest entri admincodesascii txt verifi chang print entri extra entri artifact manifest entri item run log artifact artifact point manifest lock believ artifact wait wait upload finish block quick given link print artifact run run run finish curr run api run entiti project run artifact curr run artifact log artifact curr run log artifact right track guess workaround copi older version latest want avoid file want us old version larg nlp model file want chang small config json file wast upload file wonder copi old version object kei bucket creat real copi like pointer underli object boto aw document make clear like proper copi",
        "Question_preprocessed_content":"option add specif versionid etag stop need feel like possibl look sdk code wai possibl hack modifi manifest entri point later saw thing like sdk code figur probabl edit updat tri hack like work feel wrong right track guess workaround copi want avoid file want us old version larg nlp model file want chang small file wonder copi old version object kei bucket creat real copi like pointer underli object boto aw document make clear like proper copi",
        "Answer_original_content":"think correct wai import import import boto util import file entiti environ entiti project environ project def updat file artifact exist artifact str new file url list str entiti str entiti project str project artifact want updat config json file exampl rest artifact remain us function like updat file artifact old artifact bucket prefix config json file like model bin versionid arg exist artifact str version like old artifact new file url list str file updat entiti str option entiti default entiti project str option project default project return artifact new artifact object api api overrid entiti entiti project project old artifact api artifact exist artifact old artifact sub old artifact init entiti entiti project project run new artifact artifact old artifact type old artifact type file name url split url new file url add new on url filenam zip new file url file name new artifact add refer url filenam add old on filenam entri old artifact manifest entri item filenam file name continu new artifact add refer entri filenam work feel hackier new artifact manifest entri filenam entri run log artifact new artifact new artifact wait wait upload finish block quick given link print new artifact print run return new artifact usag local file path config json modifi file url bucket prefix config json url arr url replac split bucket url arr kei join url arr client boto client file digest file local file path client upload file local file path bucket kei save digest metadata later upload new file aw digest file consist tag extraarg metadata digest file digest updat file artifact old artifact bucket prefix config json",
        "Answer_preprocessed_content":"think correct wai",
        "Question_gpt_summary_original":"the user is encountering challenges with finding an easy\/logical way to add a specific versionid or etag to an artifact in order to avoid re-uploading to s3.",
        "Question_gpt_summary":"user encount challeng find easi logic wai add specif versionid etag artifact order avoid upload",
        "Answer_gpt_summary_original":"the answer provides a python code snippet that includes a function called `_update_only_some_files_in_artifact()`. this function can be used to update specific files in an artifact without re-uploading the entire artifact to s3. the function takes in the name of the existing artifact, a list of new s3 file urls, and optional entity and project parameters. the function then creates a new artifact with the same name as the existing artifact, adds the new files to it, and adds the old files from the existing artifact that were not updated. finally, the new artifact is uploaded to s3. the code also includes an example usage of uploading a file to s3 with an md5 digest.",
        "Answer_gpt_summary":"answer provid python code snippet includ function call updat file artifact function updat specif file artifact upload entir artifact function take exist artifact list new file url option entiti project paramet function creat new artifact exist artifact add new file add old file exist artifact updat final new artifact upload code includ exampl usag upload file digest"
    },
    {
        "Question_id":null,
        "Question_title":"How to automate sagemaker batch transform?",
        "Question_body":"does cloudformation support sagemaker batch transform? if yes, can the jobs be triggered\/run automatically once the stack is created?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649085888036,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":546.0,
        "Answer_body":"While CloudFormation doesn't currently offer a resource for a SageMaker Batch Transform (resource list here in the docs), there are plenty of other integration points to automate running these jobs.\n\nCloudFormation\n\nI'd actually argue that CloudFormation is probably not a great fit for this anyway because CloudFormation defines resources which can be created, updated, and deleted. I could maybe see a correspondence between \"Create\" = \"Run a job\", maybe \"Delete\" = \"Delete job outputs\", and possibly \"Update\" = \"Re-run the job\"? But these are opinionated choices that might not make sense in every case.\n\nIf you really wanted, you could create a Custom CloudFormation resource backed by an AWS Lambda function using the CreateTransformJob API (via whatever language you prefer e.g. boto3 in Python).\n\nNote that:\n\nIf you wanted to use the SageMaker Python SDK (import sagemaker, Transformer, etc) instead of the low-level boto3 interface in Python - you'd need to install this extra library in your Lambda function. Tools like AWS SAM and CDK can help with this.\nThe maximum Lambda timeout is 15 minutes, you may not want to keep your Lambda function running (billable) just waiting for the transform to complete anyway, and even the overall Custom Resource will have a longer max timeout within which it must stabilize after a create\/update\/delete request... So additional orchestration may be required beyond a single synchronous Lambda function call.\nOther (better?) options\n\nAs mentioned above, you can create, describe and stop SageMaker Batch Transform jobs from any environment where you're able to call AWS APIs \/ use AWS SDKs... And you can even use the high-level open-source sagemaker SDK from anywhere you install it. Interesting options might include:\n\nAmazon SageMaker Pipelines: SageMaker Pipelines have native \"steps\" for a range of SageMaker processes, including transform jobs but also training, pre-processing and more. You can define a multi-step pipeline from the SageMaker Python SDK (in your notebook or elsewhere) and then start it running on-demand (with parameters) by calling the StartPipelineExecution API.\nAWS Step Functions: Step Functions provides general-purpose serverless orchestration so while the orchestration for SageMaker jobs in particular might be a little more complex (one step to start the job, then a polling check to check wait for completion) - the visual workflow editor and range of integrations to other services may be useful.\nAmazon S3 Lambda integrations can trigger an event automatically (to start your transform job) when new data is uploaded to Amazon S3.\nScheduled EventBridge Rules can run actions on a regular schedule (such as calling Lambda functions, kicking off these pipelines, etc) - in case you need a schedule-based execution rather than in response to some event.\n\nThe choice will depend on what the initial trigger for your workflow would be (schedule? Data upload? Some other AWS event? An API call from outside AWS?) and what other steps need to be orchestrated as well as your transform job in the overall flow.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyENAstk3Q_--wYwScAIq-A\/how-to-automate-sagemaker-batch-transform",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-04-04T16:24:21.868Z",
                "Answer_score":1,
                "Answer_body":"While CloudFormation doesn't currently offer a resource for a SageMaker Batch Transform (resource list here in the docs), there are plenty of other integration points to automate running these jobs.\n\nCloudFormation\n\nI'd actually argue that CloudFormation is probably not a great fit for this anyway because CloudFormation defines resources which can be created, updated, and deleted. I could maybe see a correspondence between \"Create\" = \"Run a job\", maybe \"Delete\" = \"Delete job outputs\", and possibly \"Update\" = \"Re-run the job\"? But these are opinionated choices that might not make sense in every case.\n\nIf you really wanted, you could create a Custom CloudFormation resource backed by an AWS Lambda function using the CreateTransformJob API (via whatever language you prefer e.g. boto3 in Python).\n\nNote that:\n\nIf you wanted to use the SageMaker Python SDK (import sagemaker, Transformer, etc) instead of the low-level boto3 interface in Python - you'd need to install this extra library in your Lambda function. Tools like AWS SAM and CDK can help with this.\nThe maximum Lambda timeout is 15 minutes, you may not want to keep your Lambda function running (billable) just waiting for the transform to complete anyway, and even the overall Custom Resource will have a longer max timeout within which it must stabilize after a create\/update\/delete request... So additional orchestration may be required beyond a single synchronous Lambda function call.\nOther (better?) options\n\nAs mentioned above, you can create, describe and stop SageMaker Batch Transform jobs from any environment where you're able to call AWS APIs \/ use AWS SDKs... And you can even use the high-level open-source sagemaker SDK from anywhere you install it. Interesting options might include:\n\nAmazon SageMaker Pipelines: SageMaker Pipelines have native \"steps\" for a range of SageMaker processes, including transform jobs but also training, pre-processing and more. You can define a multi-step pipeline from the SageMaker Python SDK (in your notebook or elsewhere) and then start it running on-demand (with parameters) by calling the StartPipelineExecution API.\nAWS Step Functions: Step Functions provides general-purpose serverless orchestration so while the orchestration for SageMaker jobs in particular might be a little more complex (one step to start the job, then a polling check to check wait for completion) - the visual workflow editor and range of integrations to other services may be useful.\nAmazon S3 Lambda integrations can trigger an event automatically (to start your transform job) when new data is uploaded to Amazon S3.\nScheduled EventBridge Rules can run actions on a regular schedule (such as calling Lambda functions, kicking off these pipelines, etc) - in case you need a schedule-based execution rather than in response to some event.\n\nThe choice will depend on what the initial trigger for your workflow would be (schedule? Data upload? Some other AWS event? An API call from outside AWS?) and what other steps need to be orchestrated as well as your transform job in the overall flow.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1649089461868,
        "Question_original_content":"autom batch transform cloudform support batch transform ye job trigger run automat stack creat",
        "Question_preprocessed_content":"autom batch transform cloudform support batch transform ye job automat stack creat",
        "Answer_original_content":"cloudform current offer resourc batch transform resourc list doc plenti integr point autom run job cloudform actual argu cloudform probabl great fit cloudform defin resourc creat updat delet mayb correspond creat run job mayb delet delet job output possibl updat run job opinion choic sens case want creat custom cloudform resourc back aw lambda function createtransformjob api languag prefer boto python note want us python sdk import transform instead low level boto interfac python need instal extra librari lambda function tool like aw sam cdk help maximum lambda timeout minut want lambda function run billabl wait transform complet overal custom resourc longer max timeout stabil creat updat delet request addit orchestr requir singl synchron lambda function better option mention creat stop batch transform job environ abl aw api us aw sdk us high level open sourc sdk instal interest option includ pipelin pipelin nativ step rang process includ transform job train pre process defin multi step pipelin python sdk notebook start run demand paramet call startpipelineexecut api aw step function step function provid gener purpos serverless orchestr orchestr job particular littl complex step start job poll check check wait complet visual workflow editor rang integr servic us amazon lambda integr trigger event automat start transform job new data upload amazon schedul eventbridg rule run action regular schedul call lambda function kick pipelin case need schedul base execut respons event choic depend initi trigger workflow schedul data upload aw event api outsid aw step need orchestr transform job overal flow",
        "Answer_preprocessed_content":"cloudform current offer resourc batch transform plenti integr point autom run job cloudform actual argu cloudform probabl great fit cloudform defin resourc creat updat delet mayb correspond creat run job mayb delet delet job output possibl updat job opinion choic sens case want creat custom cloudform resourc back aw lambda function createtransformjob api note want us python sdk instead boto interfac python need instal extra librari lambda function tool like aw sam cdk help maximum lambda timeout minut want lambda function run wait transform complet overal custom resourc longer max timeout stabil addit orchestr requir singl synchron lambda function option mention creat stop batch transform job environ abl aw api us aw us sdk instal interest option includ pipelin pipelin nativ step rang process includ transform job train defin pipelin python sdk start run call startpipelineexecut api aw step function step function provid serverless orchestr orchestr job particular littl complex visual workflow editor rang integr servic us amazon lambda integr trigger event automat new data upload amazon schedul eventbridg rule run action regular schedul case need execut respons event choic depend initi trigger workflow step need orchestr transform job overal flow",
        "Question_gpt_summary_original":"the user is looking to automate batch transform jobs using cloudformation and wants to know if the jobs can be triggered\/run automatically once the stack is created.",
        "Question_gpt_summary":"user look autom batch transform job cloudform want know job trigger run automat stack creat",
        "Answer_gpt_summary_original":"possible solutions for automating batch transform jobs using cloudformation include creating a custom cloudformation resource backed by an aws lambda function using the createtransformjob api, using aws sam or cdk to install the necessary libraries in the lambda function, and using other aws services such as pipelines, aws step functions, amazon s3 lambda integrations, and scheduled eventbridge rules to trigger and orchestrate the transform jobs. the choice of solution depends on the initial trigger for the workflow and the other steps that need to be orchestrated along with the transform job in the overall flow.",
        "Answer_gpt_summary":"possibl solut autom batch transform job cloudform includ creat custom cloudform resourc back aw lambda function createtransformjob api aw sam cdk instal necessari librari lambda function aw servic pipelin aw step function amazon lambda integr schedul eventbridg rule trigger orchestr transform job choic solut depend initi trigger workflow step need orchestr transform job overal flow"
    },
    {
        "Question_id":52876202.0,
        "Question_title":"How to bulk test the Sagemaker Object detection model with a .mat dataset or S3 folder of images?",
        "Question_body":"<p>I have trained the following Sagemaker model: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/introduction_to_amazon_algorithms\/object_detection_pascalvoc_coco<\/a><\/p>\n\n<p>I've tried both the JSON and RecordIO version. In both, the algorithm is tested on ONE sample image. However, I have a dataset of 2000 pictures, which I would like to test. I have saved the 2000 jpg pictures in a folder within an S3 bucket and I also have two .mat files (pics + ground truth). How can I apply this model to all 2000 pictures at once and then save the results, rather than doing it one picture at a time?<\/p>\n\n<p>I am using the code below to load a single picture from my S3 bucket:<\/p>\n\n<pre><code>object = bucket.Object('pictures\/pic1.jpg')\nobject.download_file('pic1.jpg')\nimg=mpimg.imread('pic1.jpg')\nimg_name = 'pic1.jpg'\nimgplot = plt.imshow(img)\nplt.show(imgplot)\n\nwith open(img_name, 'rb') as image:\n    f = image.read()\n    b = bytearray(f)\n    ne = open('n.txt','wb')\n    ne.write(b)\n\nimport json\nobject_detector.content_type = 'image\/jpeg'\nresults = object_detector.predict(b)\ndetections = json.loads(results)\nprint (detections['prediction'])\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1539872573950,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":112.0,
        "Answer_body":"<p>I'm not sure if I understood your question correctly. However, if you want to feed multiple images to the model at once, you can create a multi-dimensional array of images (byte arrays) to feed the model.<\/p>\n\n<p>The code would look something like this.<\/p>\n\n<pre><code>import numpy as np\n...\n\n#  predict_images_list is a Python list of byte arrays\npredict_images = np.stack(predict_images_list)\n\nwith graph.as_default():\n    #  results is an list of typical results you'd get.\n    results = object_detector.predict(predict_images)\n<\/code><\/pre>\n\n<p>But, I'm not sure if it's a good idea to feed 2000 images at once. Better to batch them in 20-30 images at a time and predict. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52876202",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1539965456440,
        "Question_original_content":"bulk test object detect model mat dataset folder imag train follow model http github com awslab amazon exampl tree master introduct amazon algorithm object detect pascalvoc coco tri json recordio version algorithm test sampl imag dataset pictur like test save jpg pictur folder bucket mat file pic ground truth appli model pictur save result pictur time code load singl pictur bucket object bucket object pictur pic jpg object download file pic jpg img mpimg imread pic jpg img pic jpg imgplot plt imshow img plt imgplot open img imag imag read bytearrai open txt write import json object detector content type imag jpeg result object detector predict detect json load result print detect predict",
        "Question_preprocessed_content":"bulk test object detect model mat dataset folder imag train follow model tri json recordio version algorithm test sampl imag dataset pictur like test save jpg pictur folder bucket mat file appli model pictur save result pictur time code load singl pictur bucket",
        "Answer_original_content":"sure understood question correctli want feed multipl imag model creat multi dimension arrai imag byte arrai feed model code look like import numpi predict imag list python list byte arrai predict imag stack predict imag list graph default result list typic result result object detector predict predict imag sure good idea feed imag better batch imag time predict",
        "Answer_preprocessed_content":"sure understood question correctli want feed multipl imag model creat arrai imag feed model code look like sure good idea feed imag better batch imag time predict",
        "Question_gpt_summary_original":"the user is facing a challenge of how to bulk test an object detection model with a .mat dataset or s3 folder of images.",
        "Question_gpt_summary":"user face challeng bulk test object detect model mat dataset folder imag",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- create a multi-dimensional array of images (byte arrays) to feed the model.\n- batch the images in 20-30 images at a time and predict.",
        "Answer_gpt_summary":"possibl solut answer creat multi dimension arrai imag byte arrai feed model batch imag imag time predict"
    },
    {
        "Question_id":null,
        "Question_title":"ray+dask native support be added to Azure Machine Learning",
        "Question_body":"While distributed dask can be setup manually on AML compute, the process requires lot of configs to be maintained. Is there any native support.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1666227934133,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@Divya-0887 Thanks for the question. you can do is to setup the compute cluster & compute instance in the same vnet and pip install ray-on-aml. This allows both interactive and job use of Ray and Dask right within Azure ML.\n\nHere is the document Library to turn Azure ML Compute into Ray and Dask cluster.\nhttps:\/\/techcommunity.microsoft.com\/t5\/ai-machine-learning-blog\/library-to-turn-azure-ml-compute-into-ray-and-dask-cluster\/ba-p\/3048784",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1055350\/raydask-native-support-be-added-to-azure-machine-l.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-20T11:44:42.8Z",
                "Answer_score":0,
                "Answer_body":"@Divya-0887 Thanks for the question. you can do is to setup the compute cluster & compute instance in the same vnet and pip install ray-on-aml. This allows both interactive and job use of Ray and Dask right within Azure ML.\n\nHere is the document Library to turn Azure ML Compute into Ray and Dask cluster.\nhttps:\/\/techcommunity.microsoft.com\/t5\/ai-machine-learning-blog\/library-to-turn-azure-ml-compute-into-ray-and-dask-cluster\/ba-p\/3048784",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1666266282800,
        "Question_original_content":"rai dask nativ support ad distribut dask setup manual aml comput process requir lot config maintain nativ support",
        "Question_preprocessed_content":"rai dask nativ support ad distribut dask setup manual aml comput process requir lot config maintain nativ support",
        "Answer_original_content":"divya thank question setup comput cluster comput instanc vnet pip instal rai aml allow interact job us rai dask right document librari turn comput rai dask cluster http techcommun microsoft com machin learn blog librari turn azur comput rai dask cluster",
        "Answer_preprocessed_content":"thank question setup comput cluster comput instanc vnet pip instal allow interact job us rai dask right document librari turn comput rai dask cluster",
        "Question_gpt_summary_original":"the user is seeking native support for ray+dask on  compute to simplify the process of configuring distributed dask.",
        "Question_gpt_summary":"user seek nativ support rai dask comput simplifi process configur distribut dask",
        "Answer_gpt_summary_original":"possible solutions to simplify the process of configuring distributed dask on  compute include setting up the compute cluster and compute instance in the same virtual network and installing ray-on-aml. this allows for both interactive and job use of ray and dask within the same environment. the document library provided in the answer can be used to turn  compute into a ray and dask cluster.",
        "Answer_gpt_summary":"possibl solut simplifi process configur distribut dask comput includ set comput cluster comput instanc virtual network instal rai aml allow interact job us rai dask environ document librari provid answer turn comput rai dask cluster"
    },
    {
        "Question_id":null,
        "Question_title":"Ground Truth Text Format",
        "Question_body":"Hello\nWe are trying to setup labeling for text with ground truth. Is it possible to format the source text to be labeled in some way e.g. html, markdown? A new line would already help a lot. I could not find any documentation on this here https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-data-input.html\n\nThanks\nNicolas",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1556263122000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":125.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUzhqhRbWOReCIYtX925_VPw\/ground-truth-text-format",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-04-29T06:56:34.000Z",
                "Answer_score":0,
                "Answer_body":"When you post tasks to Ground Truth they are displayed to workers using a web interface so HTML is the best bet for formatting the text you want annotated. For example, you could simply use the following to create a multi-line input:\n\n{\"source\": \"Lorem ipsum <br\/>dolor sit amet\"}\n\n\nUnfortunately, by default, inputs are HTML escaped to prevent confusion between your variable text and HTML. As a result if you use the Text Classification widget with the text above will just be displayed to workers as Lorem ipsum <br\/>dolor sit amet. To pass those values without escaping them you'll need to create a custom template that includes a filter on the variable to prevent it from being escaped.\n\nTo setup a custom task, start by creating Lambdas to handle for the pre and post processing required. Information on setting these up can be found at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step3.html\n\nI've included simple pre and post Lambda examples below that I find are a good place to start for text annotation. Note that the post Lambda doesn't do any answer consolidation, just simply passes back all of the answers provided by Workers.\n\nCreate a custom template in Ground Truth and use the Sentiment Analysis template to create a starter task for text annotation. To prevent it from escaping HTML values, update the Liquid variables to include the skip_autoescape filter.\n\n{{ task.input.text | skip_autoescape }}\n\n\nYou can find more info on using Liquid template values here:\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/sms-custom-templates-step2.html#sms-custom-templates-step2-automate\n\nPre\n\ndef lambda_handler(event, context):\r\n    print(event)\r\n    source = event['dataObject'].get('source')\r\n\r\n    if source:\r\n        print(\"text is {}\".format(source))\r\n    else:\r\n        print(\"Missing text in dataObject\")\r\n        return {}\r\n    \r\n    response = {\r\n        \"taskInput\": {\r\n            \"text\": source\r\n        }\r\n    }\r\n    print(response)\r\n    return response\n\n\nPost\n\nimport json\r\nimport boto3\r\nfrom urllib.parse import urlparse\r\n\r\n\r\ndef lambda_handler(event, context):\r\n    print(json.dumps(event))\r\n\r\n    payload = get_payload(event)\r\n    print(json.dumps(payload))\r\n\r\n    consolidated_response = []\r\n    for dataset in payload:\r\n        annotations = dataset['annotations']\r\n        responses = []\r\n        for annotation in annotations:\r\n            response = json.loads(annotation['annotationData']['content'])\r\n            if 'annotatedResult' in response:\r\n                response = response['annotatedResult']\r\n\r\n            responses.append({\r\n                'workerId': annotation['workerId'],\r\n                'annotation': response\r\n            })\r\n\r\n        consolidated_response.append({\r\n            'datasetObjectId': dataset['datasetObjectId'],\r\n            'consolidatedAnnotation' : {\r\n                'content': {\r\n                    event['labelAttributeName']: {\r\n                        'responses': responses\r\n                    }\r\n                }\r\n            }\r\n        })\r\n\r\n    print(json.dumps(consolidated_response))\r\n    return consolidated_response\r\n\r\n\r\ndef get_payload(event):\r\n    if 'payload' in event:\r\n        parsed_url = urlparse(event['payload']['s3Uri'])\r\n        s3 = boto3.client('s3')\r\n        text_file = s3.get_object(Bucket=parsed_url.netloc, Key=parsed_url.path[1:])\r\n        return json.loads(text_file['Body'].read())\r\n    else:\r\n        return event.get('test_payload',[])",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-18T16:24:52.000Z",
                "Answer_score":0,
                "Answer_body":"Ah, great. That was exactly what I was missing. I simply changed the template to {{ task.input.taskObject | skip_autoescape }} and left out the pre-processing lambda.\n\nEdited by: nicolasdoodle on Apr 28, 2019 11:56 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-04-29T04:49:25.000Z",
                "Answer_score":0,
                "Answer_body":"I'm trying to do the same thing. But couldn't make it work. Did you still use the post-processing lambda? Or did you just change the template.liquid file\n\nEdited by: apoorvsrivastava on Jun 18, 2021 9:28 AM",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"ground truth text format hello try setup label text ground truth possibl format sourc text label wai html markdown new line help lot document http doc aw amazon com latest sm data input html thank nicola",
        "Question_preprocessed_content":"ground truth text format hello try setup label text ground truth possibl format sourc text label wai html markdown new line help lot document thank nicola",
        "Answer_original_content":"post task ground truth displai worker web interfac html best bet format text want annot exampl simpli us follow creat multi line input sourc lorem ipsum dolor sit amet unfortun default input html escap prevent confus variabl text html result us text classif widget text displai worker lorem ipsum dolor sit amet pass valu escap need creat custom templat includ filter variabl prevent escap setup custom task start creat lambda handl pre post process requir inform set http doc aw amazon com latest sm custom templat step html includ simpl pre post lambda exampl good place start text annot note post lambda answer consolid simpli pass answer provid worker creat custom templat ground truth us sentiment analysi templat creat starter task text annot prevent escap html valu updat liquid variabl includ skip autoescap filter task input text skip autoescap info liquid templat valu http doc aw amazon com latest sm custom templat step html sm custom templat step autom pre def lambda handler event context print event sourc event dataobject sourc sourc print text format sourc print miss text dataobject return respons taskinput text sourc print respons return respons post import json import boto urllib pars import urlpars def lambda handler event context print json dump event payload payload event print json dump payload consolid respons dataset payload annot dataset annot respons annot annot respons json load annot annotationdata content annotatedresult respons respons respons annotatedresult respons append workerid annot workerid annot respons consolid respons append datasetobjectid dataset datasetobjectid consolidatedannot content event labelattributenam respons respons print json dump consolid respons return consolid respons def payload event payload event pars url urlpars event payload suri boto client text file object bucket pars url netloc kei pars url path return json load text file bodi read return event test payload great exactli miss simpli chang templat task input taskobject skip autoescap left pre process lambda edit nicolasdoodl apr try thing couldn work us post process lambda chang templat liquid file edit apoorvsrivastava jun",
        "Answer_preprocessed_content":"post task ground truth displai worker web interfac html best bet format text want annot exampl simpli us follow creat input sourc lorem ipsum dolor sit amet unfortun default input html escap prevent confus variabl text html result us text classif widget text displai worker lorem ipsum dolor sit amet pass valu escap need creat custom templat includ filter variabl prevent escap setup custom task start creat lambda handl pre post process requir inform set includ simpl pre post lambda exampl good place start text annot note post lambda answer consolid simpli pass answer provid worker creat custom templat ground truth us sentiment analysi templat creat starter task text annot prevent escap html valu updat liquid variabl includ filter info liquid templat valu pre def context print sourc sourc print print return respons print return respons post import json import boto import urlpars def context payload dataset payload annot dataset respons annot annot respons annotatedresult respons respons respons workerid annot annot respons datasetobjectid dataset consolidatedannot return def payload event urlpars return return great exactli miss simpli chang templat left lambda edit nicolasdoodl apr try thing couldn work us lambda chang file edit apoorvsrivastava jun",
        "Question_gpt_summary_original":"the user nicolas is trying to set up labeling for text with ground truth and is looking for a way to format the source text, such as html or markdown, to make the labeling process easier.",
        "Question_gpt_summary":"user nicola try set label text ground truth look wai format sourc text html markdown label process easier",
        "Answer_gpt_summary_original":"the answer suggests using html formatting for text annotation tasks in ground truth. it also provides instructions on how to prevent html values from being escaped and how to create a custom template for text annotation. the solution involves creating pre and post-processing lambdas and updating the liquid variables to include the skip_autoescape filter. the post-processing lambda is optional and is used for answer consolidation.",
        "Answer_gpt_summary":"answer suggest html format text annot task ground truth provid instruct prevent html valu escap creat custom templat text annot solut involv creat pre post process lambda updat liquid variabl includ skip autoescap filter post process lambda option answer consolid"
    },
    {
        "Question_id":null,
        "Question_title":"unable to call Workspace class and save to variable ws, MSLearning Path module 08",
        "Question_body":"I keep getting error, running commands in Jupyter notebook.\n\nimport azureml.core\nfrom azureml.core import Workspace\nws = Workspace.from_config()\nprint('Ready to use Azure ML {} to work with {}'.format(azureml.core.VERSION, ws.name))\n\nreturn RED text line number = 4 # Load the workspace from the saved config file\n----> 5 ws = Workspace.from_config()\nanother line in RED = --> 292 resource_group=resource_group)\n\nI seem to be unable to call the Workspace Class\n\nI have tried these\n\nprint(Workspace.get('worksp_mike1')) fails with TypeError, _get_ambient_new() takes 1 positional argument but 2 were given\n\n\nWorkspace.get_details() ... return = TypeError: get_details() missing 1 required positional argument: 'self'\n\n\nI have run this successfully = !pip install --upgrade azureml-sdk azureml-widgets\n\n\nthis also runs OK, print(\"SDK version:\", azureml.core.VERSION) = SDK version: 1.21.0\n\n\nI get the same ws error in previous modules....\n\nthis is where I am, module 08 in Jupyter, \"Create a Pipeline\"\nhttps:\/\/microsoftlearning.github.io\/mslearn-dp100\/",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1611691140170,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/246099\/unable-to-call-workspace-class-and-save-to-variabl.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-27T07:50:26.537Z",
                "Answer_score":0,
                "Answer_body":"@MikeRichardson-3493 In this case it looks like the step wants to load a workspace from a config file. The config file will contain the subscription id, resource group and workspace name and when you run Workspace.from_config() the details are loaded from this file and used in the next cells. A configuration sample notebook is available here to setup and you can try to create this and load the workspace details and use them.\n\n from azureml.core import Workspace\n    \n subscription_id = \"<my-subscription-id>\"\n resource_group = \"<my-resource-group>\"\n workspace_name = \"<my-workspace-name>\"\n    \n ws = Workspace(subscription_id = subscription_id, resource_group = resource_group, workspace_name = workspace_name)\n ws.write_config()",
                "Answer_comment_count":3,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-02-14T07:58:27.507Z",
                "Answer_score":0,
                "Answer_body":"I'm not sure what the actual issue is, as I was also initially unable to run the code block which instantiates the Workspace object, and was getting the same error as the OP.\n\nThis was when I cloned the repository into an Azure ML workspace I had previously created.\n\nTo fix it, I created a new workspace (https:\/\/microsoftlearning.github.io\/mslearn-dp100\/instructions\/01-create-a-workspace.html) and cloned the git repository for the learning path as specified, and then it somehow worked fine.\n\nI did not need to add the config file - from my understanding, you should only need to do that if you're using the SDK on your own machine, not if you're accessing Jupyter for your workspace via ML studio.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"unabl workspac class save variabl mslearn path modul get error run command jupyt notebook import core core import workspac workspac config print readi us work format core version return red text line number load workspac save config file workspac config line red resourc group resourc group unabl workspac class tri print workspac worksp mike fail typeerror ambient new take posit argument given workspac detail return typeerror detail miss requir posit argument self run successfulli pip instal upgrad sdk widget run print sdk version core version sdk version error previou modul modul jupyt creat pipelin http microsoftlearn github mslearn",
        "Question_preprocessed_content":"unabl workspac class save variabl mslearn path modul get error run command jupyt notebook import core core import workspac print return red text line number load workspac save config file line red unabl workspac class tri fail typeerror take posit argument given return typeerror miss requir posit argument self run successfulli pip instal sdk widget run print sdk version error previou modul jupyt creat pipelin",
        "Answer_original_content":"mikerichardson case look like step want load workspac config file config file contain subscript resourc group workspac run workspac config detail load file cell configur sampl notebook avail setup try creat load workspac detail us core import workspac subscript resourc group workspac workspac subscript subscript resourc group resourc group workspac workspac write config sure actual issu initi unabl run code block instanti workspac object get error clone repositori workspac previous creat fix creat new workspac http microsoftlearn github mslearn instruct creat workspac html clone git repositori learn path specifi work fine need add config file understand need sdk machin access jupyt workspac studio",
        "Answer_preprocessed_content":"case look like step want load workspac config file config file contain subscript resourc group workspac run detail load file cell configur sampl notebook avail setup try creat load workspac detail us core import workspac sure actual issu initi unabl run code block instanti workspac object get error clone repositori workspac previous creat fix creat new workspac clone git repositori learn path specifi work fine need add config file understand need sdk machin access jupyt workspac studio",
        "Question_gpt_summary_original":"the user is unable to call the workspace class and save to a variable, resulting in errors when running commands in jupyter notebook.",
        "Question_gpt_summary":"user unabl workspac class save variabl result error run command jupyt notebook",
        "Answer_gpt_summary_original":"the answer suggests that the user should load the workspace details from a configuration file and use them in the next cells. a sample notebook is available to set up the configuration. the answer also mentions that creating a new workspace and cloning the git repository for the learning path can fix the issue. the user may not need to add the config file if accessing jupyter for the workspace via ml studio.",
        "Answer_gpt_summary":"answer suggest user load workspac detail configur file us cell sampl notebook avail set configur answer mention creat new workspac clone git repositori learn path fix issu user need add config file access jupyt workspac studio"
    },
    {
        "Question_id":54924835.0,
        "Question_title":"Sagemaker LDA topic model - how to access the params of the trained model? Also is there a simple way to capture coherence",
        "Question_body":"<p>I'm new to Sagemaker and am running some tests to measure the performance of NTM and LDA on AWS compared with LDA mallet and native Gensim LDA model.<\/p>\n\n<p>I'm wanting to inspect the trained models on Sagemaker and look at stuff like what words have the highest contribution for each topic. And also to get a measure of model coherence.<\/p>\n\n<p>I have been able to successfully get what words have the highest contribution for each topic for NTM on Sagemaker by downloading the output file untarring it and unzipping to expose 3 files params, symbol.json and meta.json. <\/p>\n\n<p>However, when I try to do the same process for LDA, the untarred output file cannot be unzipped.<\/p>\n\n<p>Maybe I'm missing something or should do something different for LDA compared with NTM but I have not been able to find any documentation on this. Also, anyone found a simple way to calculate model coherence?<\/p>\n\n<p>Any assistance would be greatly appreciated!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1551353848073,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":521.0,
        "Answer_body":"<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/scientific_details_of_algorithms\/lda_topic_modeling\/LDA-Science.ipynb\" rel=\"nofollow noreferrer\">This SageMaker notebook<\/a>, which dives into the scientific details of LDA, also demonstrates how to inspect the model artifacts. Specifically, how to obtain the estimates for the Dirichlet prior <code>alpha<\/code> and the topic-word distribution matrix <code>beta<\/code>. You can find the instructions in the section titled <em>\"Inspecting the Trained Model\"<\/em>. For convenience, I will reproduce the relevant code here:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import tarfile\nimport mxnet as mx\n\n# extract the tarball\ntarflie_fname = FILENAME_PREFIX + 'model.tar.gz' # wherever the tarball is located\nwith tarfile.open(tarfile_fname) as tar:\n    tar.extractall()\n\n# obtain the model file (should be the only file starting with \"model_\")\nmodel_list = [\n    fname\n    for fname in os.listdir(FILENAME_PREFIX)\n    if fname.startswith('model_')\n]\nmodel_fname = model_list[0]\n\n# load the contents of the model file into MXNet arrays\nalpha, beta = mx.ndarray.load(model_fname)\n<\/code><\/pre>\n\n<p>That should get you the model data. Note that the topics, which are stored as rows of <code>beta<\/code>, are not presented in any particular order.<\/p>",
        "Answer_comment_count":11.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54924835",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1551475425323,
        "Question_original_content":"lda topic model access param train model simpl wai captur coher new run test measur perform ntm lda aw compar lda mallet nativ gensim lda model want inspect train model look stuff like word highest contribut topic measur model coher abl successfulli word highest contribut topic ntm download output file untar unzip expos file param symbol json meta json try process lda untar output file unzip mayb miss differ lda compar ntm abl document simpl wai calcul model coher assist greatli appreci",
        "Question_preprocessed_content":"lda topic model access param train model simpl wai captur coher new run test measur perform ntm lda aw compar lda mallet nativ gensim lda model want inspect train model look stuff like word highest contribut topic measur model coher abl successfulli word highest contribut topic ntm download output file untar unzip expos file param try process lda untar output file unzip mayb miss differ lda compar ntm abl document simpl wai calcul model coher assist greatli appreci",
        "Answer_original_content":"notebook dive scientif detail lda demonstr inspect model artifact specif obtain estim dirichlet prior alpha topic word distribut matrix beta instruct section titl inspect train model conveni reproduc relev code import tarfil import mxnet extract tarbal tarfli fname filenam prefix model tar tarbal locat tarfil open tarfil fname tar tar extractal obtain model file file start model model list fname fname listdir filenam prefix fname startswith model model fname model list load content model file mxnet arrai alpha beta ndarrai load model fname model data note topic store row beta present particular order",
        "Answer_preprocessed_content":"notebook dive scientif detail lda demonstr inspect model artifact specif obtain estim dirichlet prior distribut matrix instruct section titl inspect train model conveni reproduc relev code model data note topic store row present particular order",
        "Question_gpt_summary_original":"the user is encountering challenges accessing the parameters of a trained lda topic model, as well as finding a simple way to capture model coherence.",
        "Question_gpt_summary":"user encount challeng access paramet train lda topic model find simpl wai captur model coher",
        "Answer_gpt_summary_original":"the answer suggests a notebook that provides instructions on how to inspect the model artifacts of a trained lda topic model, including obtaining the estimates for the dirichlet prior alpha and the topic-word distribution matrix beta. the relevant code for extracting the model data is also provided.",
        "Answer_gpt_summary":"answer suggest notebook provid instruct inspect model artifact train lda topic model includ obtain estim dirichlet prior alpha topic word distribut matrix beta relev code extract model data provid"
    },
    {
        "Question_id":null,
        "Question_title":"Continuing an artifact",
        "Question_body":"<p>We are running long data preparation run (30+ hours) to pre-build source files for training.  However, part of the dataset was not ready and was excluding from the current run (which is 20+ hours into the run).  I would like to process the remaining data and ADD it to this current artifact.<br>\nI note that whenever I run this code is creates a new version of the artifact.<br>\nHow can I append new data to an existing artifact?<\/p>\n<p>Second question: Can I add new data in-parallel with the original job.  That is, can two different processes add data to the same artifact at the same time?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1664425707593,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":710.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a><\/p>\n<p>After speaking with the the team, you have options via our wandb artifact upsert calls to append to a non-finalized artifact as output of a run, see <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/run#upsert_artifact\">here<\/a>.However, we highly recommend you utilize S3 URI reference instead as it would be the more straightforward approach. Add all data to the S3 bucket <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/track-external-files#amazon-s3-gcs-references\">then setup reference URI<\/a> to generate the new artifact with all your processed data. If you run into any issues, please let me know.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/continuing-an-artifact\/3198",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-29T23:56:05.719Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a> , thank-you for writing in, we will be happy to help here. As per my response to your email inquiry, I am discussing this with the team as to how best to approach the above. Once I hear back,  I will provide you an update. Thanks<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-30T19:32:06.301Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a><\/p>\n<p>After speaking with the the team, you have options via our wandb artifact upsert calls to append to a non-finalized artifact as output of a run, see <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/run#upsert_artifact\">here<\/a>.However, we highly recommend you utilize S3 URI reference instead as it would be the more straightforward approach. Add all data to the S3 bucket <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/track-external-files#amazon-s3-gcs-references\">then setup reference URI<\/a> to generate the new artifact with all your processed data. If you run into any issues, please let me know.<\/p>",
                "Answer_score":1.2,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2022-10-04T20:13:50.353Z",
                "Answer_body":"<p>Thank you.  I think that I am starting understand that Artifact are intended as locked or frozen containers of objects.  And this since projects and experiments will reference them historically, they cant be changed once finalized, since that would mess with the historical references.<br>\nWe are already using s3 references for all our files in the datasets.<br>\nThank you, Kevin<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-05T23:08:16.163Z",
                "Answer_body":"<p>Thank you <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a> for confirming  you are using S3 references for all your files.<\/p>\n<p>In regards to Artifacts, your understanding is correct in that Artifacts are containers of objects to track history of changes and can be thought of as a versioned directory. This makes it easy to get a complete and auditable history of changes to your files. Anytime you change the contents of an Artifact, W&amp;B will create a new version of your artifact instead overwriting the previous contents (maintaining the historical reference you mentioned). Please do reach out again anytime you have additional questions.<\/p>",
                "Answer_score":15.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-12-04T23:08:54.825Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1664566326300,
        "Question_original_content":"continu artifact run long data prepar run hour pre build sourc file train dataset readi exclud current run hour run like process remain data add current artifact note run code creat new version artifact append new data exist artifact second question add new data parallel origin job differ process add data artifact time",
        "Question_preprocessed_content":"continu artifact run long data prepar run sourc file train dataset readi exclud current run like process remain data add current artifact note run code creat new version artifact append new data exist artifact second question add new data origin job differ process add data artifact time",
        "Answer_original_content":"kevinashaw speak team option artifact upsert call append non final artifact output run highli recommend util uri refer instead straightforward approach add data bucket setup refer uri gener new artifact process data run issu let know",
        "Answer_preprocessed_content":"speak team option artifact upsert call append artifact output run highli recommend util uri refer instead straightforward approach add data bucket setup refer uri gener new artifact process data run issu let know",
        "Question_gpt_summary_original":"the user is facing the challenge of continuing an artifact by appending new data to an existing artifact, as well as the possibility of adding new data in-parallel with the original job.",
        "Question_gpt_summary":"user face challeng continu artifact append new data exist artifact possibl ad new data parallel origin job",
        "Answer_gpt_summary_original":"possible solutions to the challenge of continuing an artifact by appending new data to an existing artifact, as well as the possibility of adding new data in-parallel with the original job, include using artifact upsert calls to append to a non-finalized artifact as output of a run or utilizing an s3 uri reference to add all data to the s3 bucket and then set up a reference uri to generate the new artifact with all processed data. the latter is recommended as the more straightforward approach.",
        "Answer_gpt_summary":"possibl solut challeng continu artifact append new data exist artifact possibl ad new data parallel origin job includ artifact upsert call append non final artifact output run util uri refer add data bucket set refer uri gener new artifact process data recommend straightforward approach"
    },
    {
        "Question_id":null,
        "Question_title":"How is Azure ML instance cost calculated?",
        "Question_body":"Hello,\n\nI've created an endpoint for scoring using a modified version of this tutorial: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-existing-model. I used this command for specifying resources: AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=12) and my resource group is \"West EU\". I'd like to know how the cost is calculated. I assume that the requested resources are converted to an instance (is that right?). I have found this useful website calculator but I cannot find which instance I am using.\n\nHow can I retrieve the information? Can I also do it programatically?\n\nMany thanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606732706930,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/180023\/how-is-azure-ml-instance-cost-calculated.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-30T13:35:11.803Z",
                "Answer_score":0,
                "Answer_body":"@martin-3510 Thanks for the question. On our website all the prices of Azure ML service are displayed and FAQ at the end.\nhttps:\/\/azure.microsoft.com\/en-us\/pricing\/details\/machine-learning\/\nYes, when you deploy your model to ACI with a default environment you can customize the deploy configuration (i.e. the number of cores and amount of memory made available for the deployment) using the AciWebservice.deploy_configuration().",
                "Answer_comment_count":4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"instanc cost calcul hello creat endpoint score modifi version tutori http doc microsoft com azur machin learn deploi exist model command specifi resourc aciwebservic deploi configur cpu core memori resourc group west like know cost calcul assum request resourc convert instanc right us websit calcul instanc retriev inform programat thank",
        "Question_preprocessed_content":"instanc cost calcul hello creat endpoint score modifi version tutori command specifi resourc resourc group west like know cost calcul assum request resourc convert instanc us websit calcul instanc retriev inform programat thank",
        "Answer_original_content":"martin thank question websit price servic displai faq end http azur microsoft com price detail machin learn ye deploi model aci default environ custom deploi configur number core memori avail deploy aciwebservic deploi configur",
        "Answer_preprocessed_content":"thank question websit price servic displai faq end ye deploi model aci default environ custom deploi configur",
        "Question_gpt_summary_original":"the user is looking to understand how the cost of their instance is calculated and how they can retrieve the information programmatically.",
        "Question_gpt_summary":"user look understand cost instanc calcul retriev inform programmat",
        "Answer_gpt_summary_original":"possible solutions from the answer are: \n- check the pricing details and faq on the azure website to understand how the cost of the instance is calculated.\n- customize the deploy configuration of the model when deploying it to aci, including the number of cores and amount of memory made available for the deployment.",
        "Answer_gpt_summary":"possibl solut answer check price detail faq azur websit understand cost instanc calcul custom deploi configur model deploi aci includ number core memori avail deploy"
    },
    {
        "Question_id":null,
        "Question_title":"Synapse Analytics Auto ML Predict No module named 'azureml.automl'",
        "Question_body":"ref: https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/788637\/azure-synapse-ml-predict-errno-20-not-a-directory.html\n\n\n\n\nI get the following error with Apache Spark version 3.1 : ModuleNotFoundError: No module named 'azureml.automl'\n\nwith version 2.4",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1648542317563,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/791601\/synapse-analytics-auto-ml-no-module-named-39azurem.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-29T09:19:20.4Z",
                "Answer_score":0,
                "Answer_body":"Hello @ThiloBarth-2620,\n\nThanks for the question and using MS Q&A platform.\n\nIf you are using import azureml.automl in Apache spark 3.1 runtime, you will experience the error message stating No module named 'azureml.automl'.\n\nAs mentioned in the official document could you please try using from notebookutils.mssparkutils import azureML and it will work as excepted.\n\nHere is the sample notebook for Score machine learning models with PREDICT in serverless Apache Spark pools\n\n #!\/usr\/bin\/env python\n # coding: utf-8\n    \n # ## Azure_Synapse_ML_predict\n    \n    \n # In[Cell-1]:\n    \n    \n from notebookutils.mssparkutils import azureML\n    \n    \n # In[Cell-2]:  \n    \n ws = azureML.getWorkspace(\"AzureMLService\")\n        \n # In[Cell-3]:    \n    \n from azureml.core import Workspace, Model\n    \n model = Model(ws, id=\"linear_regression:1\")\n    \n model.download('.\/')\n        \n # In[Cell-4]:    \n    \n from pyspark.sql.functions import col, pandas_udf,udf,lit\n    \n from notebookutils.mssparkutils import azureML\n    \n from azureml.core import Workspace, Model\n    \n from azureml.core.authentication import ServicePrincipalAuthentication\n    \n import azure.synapse.ml.predict as pcontext\n    \n import azure.synapse.ml.predict.utils._logger as synapse_predict_logger\n    \n spark.conf.set(\"spark.synapse.ml.predict.enabled\",\"true\")\n    \n # In[Cell-5]:    \n    \n AML_MODEL_URI_SKLEARN= \"aml:\/\/linear_regression:1\"\n    \n # In[Cell-6]:    \n    \n model = pcontext.bind_model(\n    \n return_types=\"Array<float>\",\n    \n runtime=\"mlflow\",\n    \n model_alias=\"linear_regression:1\",\n    \n model_uri=AML_MODEL_URI_SKLEARN,\n    \n aml_workspace=ws\n    \n ).register()    \n    \n # In[Cell-7]:    \n    \n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n df = spark.read     .format(\"csv\")     .option(\"header\", \"true\")     .csv(DATA_FILE,\n         inferSchema=True)\n df.createOrReplaceTempView('data')\n df.show(10)    \n    \n # In[Cell-8]:    \n    \n #Call PREDICT using Spark SQL API\n    \n predictions = spark.sql(\n               \"\"\"\n             SELECT PREDICT('linear_regression:1',\n             hematocrit,neutrophils,sodium,glucose,bloodureanitro,creatinine,bmi,pulse,respiration)\n             AS predict FROM data\n              \"\"\"\n         ).show()\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":5,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-01T17:02:19.717Z",
                "Answer_score":1,
                "Answer_body":"I solved it. In my case it works best like this:",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"synaps analyt auto predict modul name automl ref http doc microsoft com answer question azur synaps predict errno directori html follow error apach spark version modulenotfounderror modul name automl version",
        "Question_preprocessed_content":"synaps analyt auto predict modul name ref follow error apach spark version modulenotfounderror modul name version",
        "Answer_original_content":"hello thilobarth thank question platform import automl apach spark runtim experi error messag state modul name automl mention offici document try notebookutil mssparkutil import work except sampl notebook score machin learn model predict serverless apach spark pool usr bin env python code utf azur synaps predict cell notebookutil mssparkutil import cell getworkspac servic cell core import workspac model model model linear regress model download cell pyspark sql function import col panda udf udf lit notebookutil mssparkutil import core import workspac model core authent import serviceprincipalauthent import azur synaps predict pcontext import azur synaps predict util logger synaps predict logger spark conf set spark synaps predict enabl true cell aml model uri sklearn aml linear regress cell model pcontext bind model return type arrai runtim model alia linear regress model uri aml model uri sklearn aml workspac regist cell data file abfss data cheprasynaps df core window net aml lengthofstai cook small csv spark read format csv option header true csv data file inferschema true createorreplacetempview data cell predict spark sql api predict spark sql select predict linear regress hematocrit neutrophil sodium glucos bloodureanitro creatinin bmi puls respir predict data hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder solv case work best like",
        "Answer_preprocessed_content":"hello thank question platform import automl apach spark runtim experi error messag state modul name mention offici document try import work except sampl notebook score machin learn model predict serverless apach spark pool python code import getworkspac core import workspac model model model import col import core import workspac model import serviceprincipalauthent import pcontext import model runtim format option inferschema true predict spark sql api predict select hematocrit neutrophil sodium glucos bloodureanitro creatinin bmi puls respir predict data hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder solv case work best like",
        "Question_gpt_summary_original":"the user encountered a challenge with synapse analytics auto ml predict, where they received an error message of \"no module named '.automl'\".",
        "Question_gpt_summary":"user encount challeng synaps analyt auto predict receiv error messag modul name automl",
        "Answer_gpt_summary_original":"the solution to the error message \"no module named '.automl'\" in synapse analytics auto ml predict is to use \"from notebookutils.mssparkutils import\" instead of \"import .automl\" in apache spark 3.1 runtime. the answer provides a sample notebook for scoring machine learning models with predict in serverless apache spark pools.",
        "Answer_gpt_summary":"solut error messag modul name automl synaps analyt auto predict us notebookutil mssparkutil import instead import automl apach spark runtim answer provid sampl notebook score machin learn model predict serverless apach spark pool"
    },
    {
        "Question_id":null,
        "Question_title":"Loosen azureml-dataprep requirements to cloudpickle<=2.0.0",
        "Question_body":"Hi,\n\nI couldn\u2019t find a specific github repo for azureml-dataprep so I decided to also write you here. Can you forward it to the devs?\n\n\n\n\nazureml-dataprep (which is a depedency for azureml-dataset-runtime) has requirement cloudpickle<2.0.0 and >=1.1.0. However there is to my knowledage no breaking features going from cloudpickle==1.6.0 to cloudpickle==2.0.0. cloudpickle==2.0.0 introduces some very effective tools for serializing helper scripts which is very helful when working with azureml. So azureml-dataprep should allow cloudpickle<=2.0.0\n\nIntro to new cloudpickle:\nhttps:\/\/github.com\/cloudpipe\/cloudpickle#overriding-pickles-serialization-mechanism-for-importable-constructs\nPR:\nhttps:\/\/github.com\/cloudpipe\/cloudpickle\/pull\/417\nGithub issue:\nhttps:\/\/github.com\/Azure\/MachineLearningNotebooks\/issues\/1637",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1637242355487,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@ThomasH-1455\n\nThank you so much for the contribute, I have sent an email to the author for the PR review and merge.\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/632441\/loosen-azureml-dataprep-requirements-to-cloudpickl.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-19T02:48:45.06Z",
                "Answer_score":0,
                "Answer_body":"@ThomasH-1455\n\nThank you so much for the contribute, I have sent an email to the author for the PR review and merge.\n\nHope this will help. Please let us know if any further queries.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
                "Answer_comment_count":1,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1637290125060,
        "Question_original_content":"loosen dataprep requir cloudpickl specif github repo dataprep decid write forward dev dataprep deped dataset runtim requir cloudpickl knowledag break featur go cloudpickl cloudpickl cloudpickl introduc effect tool serial helper script hel work dataprep allow cloudpickl intro new cloudpickl http github com cloudpip cloudpickl overrid pickl serial mechan import construct http github com cloudpip cloudpickl pull github issu http github com azur machinelearningnotebook issu",
        "Question_preprocessed_content":"loosen dataprep requir specif github repo dataprep decid write forward dev dataprep requir knowledag break featur go introduc effect tool serial helper script hel work dataprep allow intro new cloudpickl github issu",
        "Answer_original_content":"thomash thank contribut sent email author review merg hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"thank contribut sent email author review merg hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Question_gpt_summary_original":"the user is encountering a challenge with the dependency -dataprep, which requires cloudpickle<2.0.0 and >=1.1.0, but there are no breaking features going from cloudpickle==1.6.0 to cloudpickle==2.0.0.",
        "Question_gpt_summary":"user encount challeng depend dataprep requir cloudpickl break featur go cloudpickl cloudpickl",
        "Answer_gpt_summary_original":"there are no explicit solutions provided in the answer. the answer is thanking someone for their contribution and suggesting ways to stay updated on the discussion.",
        "Answer_gpt_summary":"explicit solut provid answer answer thank contribut suggest wai stai updat discuss"
    },
    {
        "Question_id":71378280.0,
        "Question_title":"Error with DVC on Google Colab - dvc.scm.CloneError: Failed to clone repo",
        "Question_body":"<p>I'm having a problem trying to run &quot;dvc pull&quot; on Google Colab. I have two repositories (let's call them A and B) where repository A is for my machine learning codes and repository B is for my dataset.<\/p>\n<p>I've successfully pushed my dataset to repository B with DVC (using gdrive as my remote storage) and I also managed to successfully run &quot;dvc import&quot; (as well as &quot;dvc pull\/update&quot;) on my local project of repository A.<\/p>\n<p>The problem comes when I use colab to run my project. So what I did was the following:<\/p>\n<ol>\n<li>Created a new notebook on colab<\/li>\n<li>Successfully git-cloned my machine learning project (repository A)<\/li>\n<li>Ran &quot;!pip install dvc&quot;<\/li>\n<li>Ran &quot;!dvc pull -v&quot; (This is what causes the error)<\/li>\n<\/ol>\n<p>On step 4, I got the error (this is the full stack trace. Note that I changed the repo URL in the stack trace for confidentiality reasons)<\/p>\n<pre><code>2022-03-08 08:53:31,863 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/config.local' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/tmp' to gitignore file.\n2022-03-08 08:53:31,866 DEBUG: Adding '\/content\/&lt;my_project_A&gt;\/.dvc\/cache' to gitignore file.\n2022-03-08 08:53:31,916 DEBUG: Creating external repo https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git@3a3f4559efabff8ec74486da39b86688d1b98d75\n2022-03-08 08:53:31,916 DEBUG: erepo: git clone 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to a temporary dir\nEverything is up to date.\n2022-03-08 08:53:32,154 ERROR: failed to pull data from the cloud - Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 185, in clone\n    tmp_repo = clone_from()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1148, in clone_from\n    return cls._clone(git, url, to_path, GitCmdObjectDB, progress, multi_options, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/repo\/base.py&quot;, line 1079, in _clone\n    finalize_process, decode_streams=False)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 176, in handle_process_output\n    return finalizer(process)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/util.py&quot;, line 386, in finalize_process\n    proc.wait(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/git\/cmd.py&quot;, line 502, in wait\n    raise GitCommandError(remove_password_if_present(self.args), status, errstr)\ngit.exc.GitCommandError: Cmd('git') failed due to: exit code(128)\n  cmdline: git clone -v --no-single-branch --progress https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git \/tmp\/tmp2x7y7xgedvc-clone\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 104, in clone\n    return Git.clone(url, to_path, progress=pbar.update_git, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/__init__.py&quot;, line 121, in clone\n    backend.clone(url, to_path, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/scmrepo\/git\/backend\/gitpython.py&quot;, line 190, in clone\n    raise CloneError(url, to_path) from exc\nscmrepo.exceptions.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/command\/data_sync.py&quot;, line 41, in run\n    glob=self.args.glob,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/pull.py&quot;, line 38, in pull\n    run_cache=run_cache,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 49, in wrapper\n    return f(repo, *args, **kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/fetch.py&quot;, line 50, in fetch\n    revs=revs,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/__init__.py&quot;, line 437, in used_objs\n    with_deps=with_deps,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/repo\/index.py&quot;, line 190, in used_objs\n    filter_info=filter_info,\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/stage\/__init__.py&quot;, line 660, in get_used_objs\n    for odb, objs in out.get_used_objs(*args, **kwargs).items():\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 918, in get_used_objs\n    return self.get_used_external(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/output.py&quot;, line 973, in get_used_external\n    return dep.get_used_objs(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 94, in get_used_objs\n    used, _ = self._get_used_and_obj(**kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/dependency\/repo.py&quot;, line 108, in _get_used_and_obj\n    locked=locked, cache_dir=local_odb.cache_dir\n  File &quot;\/usr\/lib\/python3.7\/contextlib.py&quot;, line 112, in __enter__\n    return next(self.gen)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 35, in external_repo\n    path = _cached_clone(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 155, in _cached_clone\n    clone_path, shallow = _clone_default_branch(url, rev, for_write=for_write)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 45, in wrapper\n    return deco(call, *dargs, **dkwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/flow.py&quot;, line 274, in wrap_with\n    return call()\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/funcy\/decorators.py&quot;, line 66, in __call__\n    return self._func(*self._args, **self._kwargs)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/external_repo.py&quot;, line 220, in _clone_default_branch\n    git = clone(url, clone_path)\n  File &quot;\/usr\/local\/lib\/python3.7\/dist-packages\/dvc\/scm.py&quot;, line 106, in clone\n    raise CloneError(str(exc))\ndvc.scm.CloneError: Failed to clone repo 'https:\/\/gitlab.com\/&lt;my-dataset-repo-B&gt;.git' to '\/tmp\/tmp2x7y7xgedvc-clone'\n------------------------------------------------------------\n2022-03-08 08:53:32,161 DEBUG: Analytics is enabled.\n2022-03-08 08:53:32,192 DEBUG: Trying to spawn '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n2022-03-08 08:53:32,193 DEBUG: Spawned '['daemon', '-q', 'analytics', '\/tmp\/tmp4x5js0dk']'\n<\/code><\/pre>\n<p>And btw this is how I cloned my git repository (repo A)<\/p>\n<pre><code>!git config - global user.name &quot;Zharfan&quot;\n!git config - global user.email &quot;zharfan@myemail.com&quot;\n!git clone https:\/\/&lt;MyTokenName&gt;:&lt;MyToken&gt;@link-to-my-repo-A.git\n<\/code><\/pre>\n<p>Does anyone know why? Any help would be greatly appreciated. Thank you in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":12,
        "Question_creation_time":1646641948613,
        "Question_favorite_count":null,
        "Question_last_edit_time":1652856778060,
        "Question_score":1.0,
        "Question_view_count":707.0,
        "Answer_body":"<p>To summarize the discussion in the comments thread.<\/p>\n<p>Most likely it's happening since DVC can't get access to a private repo on GitLab. (The error message is obscure and should be fixed.)<\/p>\n<p>The same way you would not be able to run:<\/p>\n<pre><code>!git clone https:\/\/gitlab.com\/org\/&lt;private-repo&gt;\n<\/code><\/pre>\n<p>It also returns a pretty obscure error:<\/p>\n<pre><code>Cloning into '&lt;private-repo&gt;'...\nfatal: could not read Username for 'https:\/\/gitlab.com': No such device or address\n<\/code><\/pre>\n<p>(I think it's something related to how tty is setup in Colab?)<\/p>\n<p>The best approach to solve this is to use SSH like described <a href=\"https:\/\/medium.com\/@sadiaafrinpurba\/how-to-clone-private-github-repo-in-google-colab-using-ssh-77384cfef18f\" rel=\"nofollow noreferrer\">here<\/a> for example.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71378280",
        "Tool":"DVC",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1647022114532,
        "Question_original_content":"error googl colab scm cloneerror fail clone repo have problem try run pull googl colab repositori let repositori machin learn code repositori dataset successfulli push dataset repositori gdrive remot storag manag successfulli run import pull updat local project repositori problem come us colab run project follow creat new notebook colab successfulli git clone machin learn project repositori ran pip instal ran pull caus error step got error stack trace note chang repo url stack trace confidenti reason debug ad content config local gitignor file debug ad content tmp gitignor file debug ad content cach gitignor file debug creat extern repo http gitlab com git afefabffecdabdbd debug erepo git clone http gitlab com git temporari dir date error fail pull data cloud fail clone repo http gitlab com git tmp tmpxyxg clone traceback recent file usr local lib python dist packag scmrepo git backend gitpython line clone tmp repo clone file usr local lib python dist packag git repo base line clone return cl clone git url path gitcmdobjectdb progress multi option kwarg file usr local lib python dist packag git repo base line clone final process decod stream fals file usr local lib python dist packag git cmd line handl process output return final process file usr local lib python dist packag git util line final process proc wait kwarg file usr local lib python dist packag git cmd line wait rais gitcommanderror remov password present self arg statu errstr git exc gitcommanderror cmd git fail exit code cmdline git clone singl branch progress http gitlab com git tmp tmpxyxg clone except direct caus follow except traceback recent file usr local lib python dist packag scm line clone return git clone url path progress pbar updat git kwarg file usr local lib python dist packag scmrepo git init line clone backend clone url path kwarg file usr local lib python dist packag scmrepo git backend gitpython line clone rais cloneerror url path exc scmrepo except cloneerror fail clone repo http gitlab com git tmp tmpxyxg clone handl except except occur traceback recent file usr local lib python dist packag command data sync line run glob self arg glob file usr local lib python dist packag repo init line wrapper return repo arg kwarg file usr local lib python dist packag repo pull line pull run cach run cach file usr local lib python dist packag repo init line wrapper return repo arg kwarg file usr local lib python dist packag repo fetch line fetch rev rev file usr local lib python dist packag repo init line obj dep dep file usr local lib python dist packag repo index line obj filter info filter info file usr local lib python dist packag stage init line obj odb obj obj arg kwarg item file usr local lib python dist packag output line obj return self extern kwarg file usr local lib python dist packag output line extern return dep obj kwarg file usr local lib python dist packag depend repo line obj self obj kwarg file usr local lib python dist packag depend repo line obj lock lock cach dir local odb cach dir file usr lib python contextlib line enter return self gen file usr local lib python dist packag extern repo line extern repo path cach clone url rev write write file usr local lib python dist packag extern repo line cach clone clone path shallow clone default branch url rev write write file usr local lib python dist packag funci decor line wrapper return deco darg dkwarg file usr local lib python dist packag funci flow line wrap return file usr local lib python dist packag funci decor line return self func self arg self kwarg file usr local lib python dist packag extern repo line clone default branch git clone url clone path file usr local lib python dist packag scm line clone rais cloneerror str exc scm cloneerror fail clone repo http gitlab com git tmp tmpxyxg clone debug analyt enabl debug try spawn daemon analyt tmp tmpxjsdk debug spawn daemon analyt tmp tmpxjsdk btw clone git repositori repo git config global user zharfan git config global user email zharfan myemail com git clone http link repo git know help greatli appreci thank advanc",
        "Question_preprocessed_content":"error googl colab fail clone repo have problem try run pull googl colab repositori repositori machin learn code repositori dataset successfulli push dataset repositori manag successfulli run import local project repositori problem come us colab run project follow creat new notebook colab successfulli machin learn project ran pip instal ran pull step got error btw clone git repositori know help greatli appreci thank advanc",
        "Answer_original_content":"summar discuss comment thread like happen access privat repo gitlab error messag obscur fix wai abl run git clone http gitlab com org return pretti obscur error clone fatal read usernam http gitlab com devic address think relat tty setup colab best approach solv us ssh like describ exampl",
        "Answer_preprocessed_content":"summar discuss comment thread like happen access privat repo gitlab wai abl run return pretti obscur error think relat tty setup colab best approach solv us ssh like describ exampl",
        "Question_gpt_summary_original":"the user encountered a challenge when attempting to run \"pull\" on google colab, where they had successfully pushed their dataset to a repository (b) and imported\/pulled\/updated their machine learning project (a) on their local project, but encountered an error when running \"!pull -v\" on colab.",
        "Question_gpt_summary":"user encount challeng attempt run pull googl colab successfulli push dataset repositori import pull updat machin learn project local project encount error run pull colab",
        "Answer_gpt_summary_original":"the possible solution to the challenge of encountering an error when running \"pull\" on google colab is that the user may not have access to a private repository on gitlab. the error message is obscure and should be fixed. the best approach to solve this is to use ssh.",
        "Answer_gpt_summary":"possibl solut challeng encount error run pull googl colab user access privat repositori gitlab error messag obscur fix best approach solv us ssh"
    },
    {
        "Question_id":71063820.0,
        "Question_title":"AzureML: Dataset Profile fails when parquet file is empty",
        "Question_body":"<p>I have created a Tabular Dataset using Azure ML python API. Data under question is a bunch of parquet files (~10K parquet files each of size of 330 KB) residing in Azure Data Lake Gen 2 spread across multiple partitions. When I trigger &quot;Generate Profile&quot; operation for the dataset, it throws following error while handling empty parquet file and then the profile generation stops.<\/p>\n<pre><code>User program failed with ExecutionError: \nError Code: ScriptExecution.StreamAccess.Validation\nValidation Error Code: NotSupported\nValidation Target: ParquetFile\nFailed Step: 77866d0a-8243-4d3d-8bc6-599d466488dd\nError Message: ScriptExecutionException was caused by StreamAccessException.\n  Failed to read Parquet file at: &lt;my_blob_path&gt;\/20211217.parquet\n    Current parquet file is not supported.\n      Exception of type 'Thrift.Protocol.TProtocolException' was thrown.\n| session_id=6be4db0b-bdc1-4dd6-b8a6-6e9466f7bc54\n\n<\/code><\/pre>\n<p>By empty parquet file, I mean that the if I read the individual parquet file using pandas (<code>pd.read_parquet<\/code>), it results in an empty DF (df.empty == True).<\/p>\n<p>Any suggestion to avoid this error will be appreciated.<\/p>\n<p><strong>Update<\/strong>\nThe issue has been fixed in the following version:<\/p>\n<ul>\n<li>azureml-dataprep : 3.0.1<\/li>\n<li>azureml-core :  1.40.0<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1644490387177,
        "Question_favorite_count":null,
        "Question_last_edit_time":1648643496672,
        "Question_score":0.0,
        "Question_view_count":255.0,
        "Answer_body":"<p>Thanks for reporting it.\nThis is a bug in handling of the parquet files with columns but empty row set. This has been fixed already and will be included in next release.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71063820",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1646432534340,
        "Question_original_content":"dataset profil fail parquet file creat tabular dataset python api data question bunch parquet file parquet file size resid azur data lake gen spread multipl partit trigger gener profil oper dataset throw follow error handl parquet file profil gener stop user program fail executionerror error code scriptexecut streamaccess valid valid error code notsupport valid target parquetfil fail step ddd error messag scriptexecutionexcept caus streamaccessexcept fail read parquet file parquet current parquet file support except type thrift protocol tprotocolexcept thrown session bedbb bdc efbc parquet file mean read individu parquet file panda read parquet result true suggest avoid error appreci updat issu fix follow version dataprep core",
        "Question_preprocessed_content":"dataset profil fail parquet file creat tabular dataset python api data question bunch parquet file resid azur data lake gen spread multipl partit trigger gener profil oper dataset throw follow error handl parquet file profil gener stop parquet file mean read individu parquet file panda result suggest avoid error appreci updat issu fix follow version dataprep core",
        "Answer_original_content":"thank report bug handl parquet file column row set fix includ releas",
        "Answer_preprocessed_content":"thank report bug handl parquet file column row set fix includ releas",
        "Question_gpt_summary_original":"the user encountered an error when attempting to generate a profile for a dataset consisting of ~10k parquet files, which was caused by an empty parquet file.",
        "Question_gpt_summary":"user encount error attempt gener profil dataset consist parquet file caus parquet file",
        "Answer_gpt_summary_original":"the error encountered by the user when generating a profile for a dataset consisting of ~10k parquet files was caused by an empty parquet file. the answer suggests that this is a known bug and has already been fixed. the solution is to wait for the next release that includes the fix.",
        "Answer_gpt_summary":"error encount user gener profil dataset consist parquet file caus parquet file answer suggest known bug fix solut wait releas includ fix"
    },
    {
        "Question_id":null,
        "Question_title":"Connection Error 403",
        "Question_body":"Hi, I keep on getting this error while trying to access the Twitter API using the TwitterClient. The Client builds successfully but then it shows this error when I run: dotnet run",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_creation_time":1663611187617,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1014310\/connction-error-403.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-19T20:27:36.457Z",
                "Answer_score":1,
                "Answer_body":"This is not an Azure error, right?\n\nThis is the developer forum https:\/\/developer.microsoft.com\/en-us\/community\/\nAnd this is the .NET forum https:\/\/techcommunity.microsoft.com\/t5\/net\/ct-p\/dotnet",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":36.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"connect error get error try access twitter api twittercli client build successfulli show error run dotnet run",
        "Question_preprocessed_content":"connect error get error try access twitter api twittercli client build successfulli show error run dotnet run",
        "Answer_original_content":"azur error right develop forum http develop microsoft com commun net forum http techcommun microsoft com net dotnet",
        "Answer_preprocessed_content":"azur error right develop forum net forum",
        "Question_gpt_summary_original":"the user is encountering a connection error 403 while trying to access the twitter api using the twitterclient.",
        "Question_gpt_summary":"user encount connect error try access twitter api twittercli",
        "Answer_gpt_summary_original":"the answer does not provide any solutions to the connection error 403 while accessing the twitter api using the twitterclient. instead, it redirects the user to the developer forum and .net forum of microsoft.",
        "Answer_gpt_summary":"answer provid solut connect error access twitter api twittercli instead redirect user develop forum net forum microsoft"
    },
    {
        "Question_id":null,
        "Question_title":"Integrate Azure ML Studio with Azure DevOps repo",
        "Question_body":"Hello,\n\nI am working with Azure Machine Learning Studio via GUI (upload dataset, run experiments, register ML models, register endpoints, develop code in notebooks, ...) and would like to have that code available in Azure DevOps repo in order to restore it if necessary.\n\nDoes anyone know how that integration between Azure ML Studio and Azure DevOps repo could be done?\n\nThanks,",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_creation_time":1666021308520,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1051117\/integrate-azure-ml-studio-with-azure-devops-repo.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-17T20:52:43.673Z",
                "Answer_score":0,
                "Answer_body":"Hi, @PedroGonzalez-2046\n\nI checked the documentation of Azure Machine Learning Studio, but it seems that there is no integration function with the version control system.\n\nYou can download the created model code by referring to the following questions.\n\nhttps:\/\/learn.microsoft.com\/en-us\/answers\/questions\/355882\/azure-ml-studio-and-gitazuredevops.html\n\nBy applying this to build a regular batch mechanism, you can periodically acquire the latest model on ML Studio with code and reflect it in Git management.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-18T10:02:33.74Z",
                "Answer_score":0,
                "Answer_body":"@PedroGonzalez-2046 You can use MLOps to setup Azure ML pipelines and experiments with Azure DevOps. A good working sample is available in the MLOpsPython repo. The setup should be straightforward where any changes in your pipeline code will trigger a job in your Azure ML workspace.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":5,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"integr studio azur devop repo hello work studio gui upload dataset run experi regist model regist endpoint develop code notebook like code avail azur devop repo order restor necessari know integr studio azur devop repo thank",
        "Question_preprocessed_content":"integr studio azur devop repo hello work studio gui like code avail azur devop repo order restor necessari know integr studio azur devop repo thank",
        "Answer_original_content":"pedrogonzalez check document studio integr function version control download creat model code refer follow question http learn microsoft com answer question azur studio gitazuredevop html appli build regular batch mechan period acquir latest model studio code reflect git manag pedrogonzalez us mlop setup pipelin experi azur devop good work sampl avail mlopspython repo setup straightforward chang pipelin code trigger job workspac answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"check document studio integr function version control download creat model code refer follow question appli build regular batch mechan period acquir latest model studio code reflect git manag us mlop setup pipelin experi azur devop good work sampl avail mlopspython repo setup straightforward chang pipelin code trigger job workspac answer help click upvot help commun member read thread",
        "Question_gpt_summary_original":"the user is looking for a way to integrate their studio with an azure devops repository in order to back up their code and datasets.",
        "Question_gpt_summary":"user look wai integr studio azur devop repositori order code dataset",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. download the created model code and build a regular batch mechanism to periodically acquire the latest model on ml studio with code and reflect it in git management.\n2. use mlops to set up pipelines and experiments with azure devops. a good working sample is available in the mlopspython repo. the setup should be straightforward where any changes in your pipeline code will trigger a job in your workspace.",
        "Answer_gpt_summary":"possibl solut answer download creat model code build regular batch mechan period acquir latest model studio code reflect git manag us mlop set pipelin experi azur devop good work sampl avail mlopspython repo setup straightforward chang pipelin code trigger job workspac"
    },
    {
        "Question_id":65753455.0,
        "Question_title":"How to get the version of gremlin python client on AWS SageMaker",
        "Question_body":"<p>What is the command to check the version of Gremlin Python client running on a AWS Sagemaker jupyter notebook? I would like to run the command on the jupyter notebook cell.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1610823303577,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":198.0,
        "Answer_body":"<p>From a notebook cell you should be able to just ask Pip which version is being used<\/p>\n<pre><code>!pip list | grep gremlinpython\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65753455",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1610839775480,
        "Question_original_content":"version gremlin python client command check version gremlin python client run jupyt notebook like run command jupyt notebook cell",
        "Question_preprocessed_content":"version gremlin python client command check version gremlin python client run jupyt notebook like run command jupyt notebook cell",
        "Answer_original_content":"notebook cell abl ask pip version pip list grep gremlinpython",
        "Answer_preprocessed_content":"notebook cell abl ask pip version",
        "Question_gpt_summary_original":"the user would like to know the command to check the version of the gremlin python client running on a jupyter notebook.",
        "Question_gpt_summary":"user like know command check version gremlin python client run jupyt notebook",
        "Answer_gpt_summary_original":"the solution to check the version of the gremlin python client running on a jupyter notebook is to run the command \"!pip list | grep gremlinpython\" in a notebook cell.",
        "Answer_gpt_summary":"solut check version gremlin python client run jupyt notebook run command pip list grep gremlinpython notebook cell"
    },
    {
        "Question_id":null,
        "Question_title":"how many models can be deployed in single node in azure kubernetes service?",
        "Question_body":"Working on deployment of 170 ml models using ML studio and azure Kubernetes service which is referred on the below doc link \"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/master\/articles\/machine-learning\/how-to-deploy-azure-kubernetes-service.md\".\n\nWe are training the model using python script with the custom environment and we are registering the ml model on the Azure ML services. Once we register the mode we are deploying it on the AKS by using the container images.\n\nWhile deploying the ML model we are able to deploy up 10 to 11 models per pods for each Node in AKS. When we try to deploy the model on the same node we are getting deployment timeout error and we are getting the below error message.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630746472547,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi @suvedharan-5910\n\nThe number of models to be deployed is limited to 1,000 models per deployment (per container).\n\nAutoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. Since all inference requests go through it, it has the necessary data to automatically scale the deployed model(s).\nmore details\n\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, so that it can help others in the community looking for help on similar topics.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/540001\/how-many-models-can-be-deployed-in-single-node-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-09-04T12:32:05.4Z",
                "Answer_score":0,
                "Answer_body":"Hi @suvedharan-5910\n\nThe number of models to be deployed is limited to 1,000 models per deployment (per container).\n\nAutoscaling for Azure ML model deployments is azureml-fe, which is a smart request router. Since all inference requests go through it, it has the necessary data to automatically scale the deployed model(s).\nmore details\n\n\n\n\n\nIf the Answer is helpful, please click Accept Answer and up-vote, so that it can help others in the community looking for help on similar topics.",
                "Answer_comment_count":2,
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":14.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1630758725400,
        "Question_original_content":"model deploi singl node azur kubernet servic work deploy model studio azur kubernet servic refer doc link http github com microsoftdoc azur doc blob master articl machin learn deploi azur kubernet servic train model python script custom environ regist model servic regist mode deploi ak contain imag deploi model abl deploi model pod node ak try deploi model node get deploy timeout error get error messag",
        "Question_preprocessed_content":"model deploi singl node azur kubernet servic work deploy model studio azur kubernet servic refer doc link train model python script custom environ regist model servic regist mode deploi ak contain imag deploi model abl deploi model pod node ak try deploi model node get deploy timeout error get error messag",
        "Answer_original_content":"suvedharan number model deploi limit model deploy contain autosc model deploy smart request router infer request necessari data automat scale deploi model detail answer help click accept answer vote help commun look help similar topic",
        "Answer_preprocessed_content":"number model deploi limit model deploy autosc model deploy smart request router infer request necessari data automat scale deploi model detail answer help click accept answer help commun look help similar topic",
        "Question_gpt_summary_original":"the user is encountering challenges with deploying multiple ml models on a single node in azure kubernetes service, as they are receiving deployment timeout errors when attempting to deploy more than 10-11 models per pod.",
        "Question_gpt_summary":"user encount challeng deploi multipl model singl node azur kubernet servic receiv deploy timeout error attempt deploi model pod",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- the number of models to be deployed is limited to 1,000 models per deployment (per container).\n- autoscaling for model deployments is available using a smart request router called -fe.\n- all inference requests go through -fe, which has the necessary data to automatically scale the deployed model(s).\n\nin summary, the user can deploy up to 1,000 models per container and use autoscaling with -fe to automatically scale the deployed models based on inference requests.",
        "Answer_gpt_summary":"possibl solut answer number model deploi limit model deploy contain autosc model deploy avail smart request router call infer request necessari data automat scale deploi model summari user deploi model contain us autosc automat scale deploi model base infer request"
    },
    {
        "Question_id":null,
        "Question_title":"Using R model in SageMaker ML pipelines",
        "Question_body":"Hi there,\n\nIs it possible to use R model training and serving in SageMaker ML Pipelines? Looked in examples here. And it doesn't look that R is fully supported currently by ML Pipelines. Any examples and success stories are very welcome.\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1643230196748,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":97.0,
        "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3 using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker https:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU17aS4s7uSRqmiLuveuchBw\/using-r-model-in-sage-maker-ml-pipelines",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-28T21:07:43.709Z",
                "Answer_score":1,
                "Answer_body":"In general it is possible to use the SageMaker python SDK and boto3 using the reticulate package in R, However do not have direct examples of SageMaker Pipelines using R.\n\nIt is possible to orchestrate the production pipeline using the R Containers for training and serving and setting up the DAG can be done with reticulate and SageMaker Python SDK and can be achieved using the AWS Step Functions. Please refer to the following example for reference.\n\nhttps:\/\/github.com\/aws-samples\/reinvent2020-aim404-productionize-r-using-amazon-sagemaker https:\/\/www.youtube.com\/watch?v=Zpp0nfvqDCA",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1643404063708,
        "Question_original_content":"model pipelin possibl us model train serv pipelin look exampl look fulli support current pipelin exampl success stori welcom thank",
        "Question_preprocessed_content":"model pipelin possibl us model train serv pipelin look exampl look fulli support current pipelin exampl success stori welcom thank",
        "Answer_original_content":"gener possibl us python sdk boto reticul packag direct exampl pipelin possibl orchestr product pipelin contain train serv set dag reticul python sdk achiev aw step function refer follow exampl refer http github com aw sampl reinvent aim production amazon http youtub com watch zppnfvqdca",
        "Answer_preprocessed_content":"gener possibl us python sdk boto reticul packag direct exampl pipelin possibl orchestr product pipelin contain train serv set dag reticul python sdk achiev aw step function refer follow exampl refer",
        "Question_gpt_summary_original":"the user is looking to use r models in ml pipelines and is seeking examples and success stories.",
        "Question_gpt_summary":"user look us model pipelin seek exampl success stori",
        "Answer_gpt_summary_original":"possible solutions for using r models in ml pipelines include using the python sdk and boto3 with the reticulate package in r, orchestrating the production pipeline using r containers for training and serving, and setting up the dag using reticulate and the python sdk. aws step functions can also be used to achieve this. the provided github example and youtube video can be used as references.",
        "Answer_gpt_summary":"possibl solut model pipelin includ python sdk boto reticul packag orchestr product pipelin contain train serv set dag reticul python sdk aw step function achiev provid github exampl youtub video refer"
    },
    {
        "Question_id":null,
        "Question_title":"update real interference pipeline",
        "Question_body":"I deployed my Training pipeline and my Real-time inference pipeline.\nWith the REST-Api of my training pipeline I'm able to retrain my ML model. Is it possible to use that retrained model automated in my real inference pipeline?\nWhen i trigger the pipeline in ML studio I have to update my real inference pipeline manually. Since I want to trigger my retraining external that is not possible.\nThanks in advance.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1615298736310,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, here's a reference on which technology to use based on a given scenario. For your scenario, you should be able to create an Azure Machine Learning pipeline using the SDK to trigger a pipeline based on a time\/change based schedule and then update the web service accordingly. Depending on the complexity of your triggers or data prep needs, you can leverage other technologies such as Logic Apps or Azure Data Factory to trigger your Azure Machine Learning pipeline. Currently, you can only use the Azure Machine Learning SDK to automatically update the web service. Hope this helps.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/305899\/update-real-interference-pipeline.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-12T19:59:09.187Z",
                "Answer_score":0,
                "Answer_body":"Hi, here's a reference on which technology to use based on a given scenario. For your scenario, you should be able to create an Azure Machine Learning pipeline using the SDK to trigger a pipeline based on a time\/change based schedule and then update the web service accordingly. Depending on the complexity of your triggers or data prep needs, you can leverage other technologies such as Logic Apps or Azure Data Factory to trigger your Azure Machine Learning pipeline. Currently, you can only use the Azure Machine Learning SDK to automatically update the web service. Hope this helps.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-03-09T21:20:25.76Z",
                "Answer_score":0,
                "Answer_body":"Hi, when you deploy a web service from designer, you can select to \"deploy as a new real-time endpoint\" or \"replace an existing real-time endpoint\". The option (replace an existing real-time endpoint) enables you to update the previous endpoint. Currently, there's no way to trigger an update to real-time endpoint, it's still a manual process (I will verify and share updates). To programmatically update your web service using Azure ML SDK, please refer to this document.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1615579149187,
        "Question_original_content":"updat real interfer pipelin deploi train pipelin real time infer pipelin rest api train pipelin abl retrain model possibl us retrain model autom real infer pipelin trigger pipelin studio updat real infer pipelin manual want trigger retrain extern possibl thank advanc",
        "Question_preprocessed_content":"updat real interfer pipelin deploi train pipelin infer pipelin train pipelin abl retrain model possibl us retrain model autom real infer pipelin trigger pipelin studio updat real infer pipelin manual want trigger retrain extern possibl thank advanc",
        "Answer_original_content":"refer technolog us base given scenario scenario abl creat pipelin sdk trigger pipelin base time chang base schedul updat web servic accordingli depend complex trigger data prep need leverag technolog logic app azur data factori trigger pipelin current us sdk automat updat web servic hope help",
        "Answer_preprocessed_content":"refer technolog us base given scenario scenario abl creat pipelin sdk trigger pipelin base base schedul updat web servic accordingli depend complex trigger data prep need leverag technolog logic app azur data factori trigger pipelin current us sdk automat updat web servic hope help",
        "Question_gpt_summary_original":"the user is looking for a way to automatically update their real-time inference pipeline when they trigger a retraining of their ml model externally.",
        "Question_gpt_summary":"user look wai automat updat real time infer pipelin trigger retrain model extern",
        "Answer_gpt_summary_original":"possible solutions from the answer include creating a pipeline using the sdk to trigger a pipeline based on a time\/change based schedule and then updating the web service accordingly. depending on the complexity of the triggers or data prep needs, other technologies such as logic apps or azure data factory can be leveraged to trigger the pipeline. currently, the only way to automatically update the web service is through the sdk.",
        "Answer_gpt_summary":"possibl solut answer includ creat pipelin sdk trigger pipelin base time chang base schedul updat web servic accordingli depend complex trigger data prep need technolog logic app azur data factori leverag trigger pipelin current wai automat updat web servic sdk"
    },
    {
        "Question_id":null,
        "Question_title":"Azure ML studio designer modules and the Python SDK",
        "Question_body":"I have been searching the documentation and cannot find this answer. Is every module in the Azure ML designer available in the Python SDK? I would like to create pipelines via the SDK using transformation and feature engineering methods available in the Designer, but can&amp;amp;#39;t find any reference to those modules in the SDK.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1591108790960,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/31305\/azure-ml-studio-designer-modules-and-the-python-sd.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-06-02T22:57:44.887Z",
                "Answer_score":0,
                "Answer_body":"You can use the dataprep package for common data preparation tasks, check out the following documentation on Azure ML SDK Data Preparation. You can also use the Dataset package for working with datasets, check out tabulardataset and filedataset sample notebooks.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":3.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"studio design modul python sdk search document answer modul design avail python sdk like creat pipelin sdk transform featur engin method avail design amp refer modul sdk",
        "Question_preprocessed_content":"studio design modul python sdk search document answer modul design avail python sdk like creat pipelin sdk transform featur engin method avail design amp refer modul sdk",
        "Answer_original_content":"us dataprep packag common data prepar task check follow document sdk data prepar us dataset packag work dataset check tabulardataset filedataset sampl notebook",
        "Answer_preprocessed_content":"us dataprep packag common data prepar task check follow document sdk data prepar us dataset packag work dataset check tabulardataset filedataset sampl notebook",
        "Question_gpt_summary_original":"the user is trying to determine if all modules available in the designer are also available in the python sdk for creating pipelines with transformation and feature engineering methods.",
        "Question_gpt_summary":"user try determin modul avail design avail python sdk creat pipelin transform featur engin method",
        "Answer_gpt_summary_original":"possible solutions to determine if all modules available in the designer are also available in the python sdk for creating pipelines with transformation and feature engineering methods are: \n1. use the dataprep package for common data preparation tasks and refer to the sdk data preparation documentation. \n2. use the dataset package for working with datasets and refer to the tabulardataset and filedataset sample notebooks.",
        "Answer_gpt_summary":"possibl solut determin modul avail design avail python sdk creat pipelin transform featur engin method us dataprep packag common data prepar task refer sdk data prepar document us dataset packag work dataset refer tabulardataset filedataset sampl notebook"
    },
    {
        "Question_id":null,
        "Question_title":"Is there a way to automate failure handling and retries when using Amazon SageMaker batch transform?",
        "Question_body":"How does Amazon SageMaker batch transform handle failures? Is there a way to automate failure handling and retries built into the service?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1593595381000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":120.0,
        "Answer_body":"You can use the ModelClientConfig API to configure the timeout and maximum number of retries for processing a transform job invocation. The maximum number of automated retries is three.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE10OtSwDRCiB-0pP6wflYQ\/is-there-a-way-to-automate-failure-handling-and-retries-when-using-amazon-sage-maker-batch-transform",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-07-01T15:34:51.000Z",
                "Answer_score":0,
                "Answer_body":"You can use the ModelClientConfig API to configure the timeout and maximum number of retries for processing a transform job invocation. The maximum number of automated retries is three.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1593617691000,
        "Question_original_content":"wai autom failur handl retri batch transform batch transform handl failur wai autom failur handl retri built servic",
        "Question_preprocessed_content":"wai autom failur handl retri batch transform batch transform handl failur wai autom failur handl retri built servic",
        "Answer_original_content":"us modelclientconfig api configur timeout maximum number retri process transform job invoc maximum number autom retri",
        "Answer_preprocessed_content":"us modelclientconfig api configur timeout maximum number retri process transform job invoc maximum number autom retri",
        "Question_gpt_summary_original":"the user is looking for a way to automate failure handling and retries when using batch transform.",
        "Question_gpt_summary":"user look wai autom failur handl retri batch transform",
        "Answer_gpt_summary_original":"to automate failure handling and retries when using batch transform, the user can use the modelclientconfig api to configure the timeout and maximum number of retries for processing a transform job invocation. the maximum number of automated retries is three.",
        "Answer_gpt_summary":"autom failur handl retri batch transform user us modelclientconfig api configur timeout maximum number retri process transform job invoc maximum number autom retri"
    },
    {
        "Question_id":null,
        "Question_title":"Chrome Inline JavaScript Issue when Rendering Molecules",
        "Question_body":"<p>Hello, everyone.<br>\nI recently have been trying to log wandb.Molecule objects via PyTorch Lightning. However, when I view the logged Molecules in my web browser (Google Chrome, Version 95.0.4638.69, 64-Bit), I see the following error, preventing my Molecules from being rendered.<\/p>\n<p><em>[Report Only] Refused to apply inline style because it violates the following Content Security Policy directive: \u201cdefault-src \u2018none\u2019\u201d. Either the \u2018unsafe-inline\u2019 keyword, a hash (\u2018sha256-NFPvvJTeausaOnuU9syzBhm5OjQ9MGcbA9SexsBrsF4=\u2019), or a nonce (\u2018nonce-\u2026\u2019) is required to enable inline execution. Note also that \u2018style-src\u2019 was not explicitly set, so \u2018default-src\u2019 is used as a fallback.<\/em><\/p>\n<p>It looks to me like Chrome is preventing WandB\u2019s web site from rendering inline JavaScript. For reference, I have disabled any browser extension that may be affecting this, and that didn\u2019t seem to help.<br>\nAny ideas as to how to get around this issue? Without of course using a new browser <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/wink.png?v=10\" title=\":wink:\" class=\"emoji\" alt=\":wink:\"><\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1635974407326,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":280.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/chrome-inline-javascript-issue-when-rendering-molecules\/1217",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-04T19:01:46.225Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/amorehead\">@amorehead<\/a>!<\/p>\n<p>Thank you for your question! This seems to be an issue on our end. I have notified the engineers of this issue and we will have it resolved ASAP.<\/p>\n<p>Thank you,<br>\nRamit<br>\nWeights &amp; Biases Support<\/p>",
                "Answer_score":0.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-04T21:03:05.057Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/amorehead\">@amorehead<\/a>! This problem has been fixed, you should be able to render molecules in your workspace now.<\/p>",
                "Answer_score":5.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-04T21:29:31.000Z",
                "Answer_body":"<p>Ramit,<\/p>\n<p>Thank you very much for letting me know, and please thank the team on my behalf for their very quick response!<\/p>\n<p>Best,<br>\nAlex<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-03T21:30:09.055Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.2,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"chrome inlin javascript issu render molecul hello recent try log molecul object pytorch lightn view log molecul web browser googl chrome version bit follow error prevent molecul render report refus appli inlin style violat follow content secur polici direct default src unsaf inlin keyword hash sha nfpvvjteausaonuusyzbhmojqmgcbasexsbrsf nonc nonc requir enabl inlin execut note style src explicitli set default src fallback look like chrome prevent web site render inlin javascript refer disabl browser extens affect didnt help idea issu cours new browser",
        "Question_preprocessed_content":"chrome inlin javascript issu render molecul hello recent try log molecul object pytorch lightn view log molecul web browser follow error prevent molecul render report refus appli inlin style violat follow content secur polici direct keyword hash nonc requir enabl inlin execut note explicitli set fallback look like chrome prevent web site render inlin javascript refer disabl browser extens affect didnt help idea issu cours new browser",
        "Answer_original_content":"amorehead thank question issu end notifi engin issu resolv asap thank ramit support amorehead problem fix abl render molecul workspac ramit thank let know thank team behalf quick respons best alex topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thank question issu end notifi engin issu resolv asap thank ramit support problem fix abl render molecul workspac ramit thank let know thank team behalf quick respons best alex topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering an issue with chrome preventing inline javascript from being rendered when attempting to log .molecule objects via pytorch lightning.",
        "Question_gpt_summary":"user encount issu chrome prevent inlin javascript render attempt log molecul object pytorch lightn",
        "Answer_gpt_summary_original":"the issue of chrome preventing inline javascript from being rendered when attempting to log .molecule objects via pytorch lightning has been resolved by the engineers. the user should now be able to render molecules in their workspace.",
        "Answer_gpt_summary":"issu chrome prevent inlin javascript render attempt log molecul object pytorch lightn resolv engin user abl render molecul workspac"
    },
    {
        "Question_id":null,
        "Question_title":"Data Wrangler Full Outer Join Not Working As Expected Nor Concatenate",
        "Question_body":"I've got two CSV files that are loaded into Data Wrangler that are intended to augment each other. The tables have some columns that are the same (in name) and some that are not, many of the rows are missing entries for many of the columns. The two tables represent separate datasets. Consider the example below: Table 1:\n\nFilename\tLabelA\tLabelB\n.\/A\/001.dat\t1\t1\n.\/A\/002.dat\t0\t1\n\nTable 2:\n\nFilename\tLabelB\tLabelC\n.\/B\/001.dat\t\t0\n.\/B\/002.dat\t0\t1\n\nI am looking to merge \/ concatenate the two table. The problem is that neither Data Wrangler join nor concatenate seems to work (at least as expected).\n\nDesired result:\n\nFilename\tLabelA\tLabelB\tLabelC\n.\/A\/001.dat\t1\t1\t\n.\/A\/002.dat\t0\t1\t\n.\/B\/001.dat\t\t\t0\n.\/B\/002.dat\t\t0\t1\n\nWhen using a \"Full Outer\" join and ask to combine \"Filename\" and \"LabelB\" columns, it will take all the values from Table 1 OR Table 2 even if Table 1 does not have that entry (for example, some rows will have Filename = <nothing> rather than Filename = .\/B\/001.dat).\n\nWhen using concatenate, Data Wrangler errors on the fact that it cannot match EVERY column between the tables.\n\nNow in my example there are many columns and many rows which precludes a manual process of joining without merging columns and then going through a renaming and merging process one-by-one. How do get these tables to simply merge? I feel I must be missing something obvious. I am about to give up on Data Wrangler and do it all in a python script using pandas, but I thought I should give Data Wrangler a try while learning the MLops process.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1651799011602,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4DJAYgTvQQKdl-jplyRtmw\/data-wrangler-full-outer-join-not-working-as-expected-nor-concatenate",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-09T11:29:24.682Z",
                "Answer_score":0,
                "Answer_body":"Hi, @b33fcafe\n\nShouldn't you use Concatenate instead of Join because you don't want to join data horizontally but vertically?\n\nAfter importing Table1 and Table2, add the missing columns and then Concatenate to get the expected result.\n\nIf duplicate data occurs, additional processing is possible in subsequent steps.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"data wrangler outer join work expect concaten got csv file load data wrangler intend augment tabl column row miss entri column tabl repres separ dataset consid exampl tabl filenam labela labelb dat dat tabl filenam labelb labelc dat dat look merg concaten tabl problem data wrangler join concaten work expect desir result filenam labela labelb labelc dat dat dat dat outer join ask combin filenam labelb column valu tabl tabl tabl entri exampl row filenam filenam dat concaten data wrangler error fact match column tabl exampl column row preclud manual process join merg column go renam merg process tabl simpli merg feel miss obviou data wrangler python script panda thought data wrangler try learn mlop process",
        "Question_preprocessed_content":"data wrangler outer join work expect concaten got csv file load data wrangler intend augment tabl column row miss entri column tabl repres separ dataset consid exampl tabl filenam labela labelb tabl filenam labelb labelc look merg concaten tabl problem data wrangler join concaten work desir result filenam labela labelb labelc outer join ask combin filenam labelb column valu tabl tabl tabl entri concaten data wrangler error fact match column tabl exampl column row preclud manual process join merg column go renam merg process tabl simpli merg feel miss obviou data wrangler python script panda thought data wrangler try learn mlop process",
        "Answer_original_content":"bfcafe shouldn us concaten instead join want join data horizont vertic import tabl tabl add miss column concaten expect result duplic data occur addit process possibl subsequ step",
        "Answer_preprocessed_content":"shouldn us concaten instead join want join data horizont vertic import tabl tabl add miss column concaten expect result duplic data occur addit process possibl subsequ step",
        "Question_gpt_summary_original":"the user is encountering challenges with data wrangler's full outer join and concatenate functions when attempting to merge two csv files with some shared columns and many missing entries.",
        "Question_gpt_summary":"user encount challeng data wrangler outer join concaten function attempt merg csv file share column miss entri",
        "Answer_gpt_summary_original":"possible solutions to the user's challenge with data wrangler's full outer join and concatenate functions when merging two csv files with shared columns and many missing entries are: \n1. use concatenate instead of join because the user wants to merge data vertically.\n2. import table1 and table2, add the missing columns, and then concatenate to get the expected result.\n3. if duplicate data occurs, additional processing is possible in subsequent steps.",
        "Answer_gpt_summary":"possibl solut user challeng data wrangler outer join concaten function merg csv file share column miss entri us concaten instead join user want merg data vertic import tabl tabl add miss column concaten expect result duplic data occur addit process possibl subsequ step"
    },
    {
        "Question_id":null,
        "Question_title":"Why PyTorch is using only one GPU ?",
        "Question_body":"Azure does not use the two GPUs of my node with PyTorch (and Hugging Face). The monitoring tool of Azure shows the GPU usage is stuck at 50%.\nIts a Standard_NC12, so it has two K80s.\n\n\n\n\nI tried this way :\nhttps:\/\/azure.github.io\/azureml-cheatsheets\/docs\/cheatsheets\/python\/v1\/distributed-training\/#distributeddataparallel-per-process-launch\nand it looked like this in my notebook :\n\n\n\n\n\nI copied the docker file from the curated environments and added the libraries I needed successfully :\n\n FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.1-cudnn8-ubuntu18.04:20220329.v1\n    \n ENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/pytorch-1.10\n    \n # Create conda environment\n RUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n     python=3.8 \\\n     pip=20.2.4 \\\n     pytorch=1.10.0 \\\n     torchvision=0.11.1 \\\n     torchaudio=0.10.0 \\\n     cudatoolkit=11.1.1 \\\n     nvidia-apex=0.1.0 \\\n     gxx_linux-64 \\\n     -c anaconda -c pytorch -c conda-forge\n    \n # Prepend path to AzureML conda environment\n ENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n    \n # Install pip dependencies\n RUN pip install 'matplotlib>=3.3,<3.4' \\\n                 'psutil>=5.8,<5.9' \\\n                 'tqdm>=4.59,<4.63' \\\n                 'pandas>=1.3,<1.4' \\\n                 'scipy>=1.5,<1.8' \\\n                 'numpy>=1.10,<1.22' \\\n                 'ipykernel~=6.0' \\\n                 'azureml-core==1.40.0' \\\n                 'azureml-defaults==1.40.0' \\\n                 'azureml-mlflow==1.40.0' \\\n                 'azureml-telemetry==1.40.0' \\\n                 'tensorboard==2.6.0' \\\n                 'tensorflow-gpu==2.6.0' \\\n                 'onnxruntime-gpu>=1.7,<1.10' \\\n                 'horovod==0.23' \\\n                 'future==0.18.2' \\\n                 'wandb' \\\n                 'transformers' \\\n                 'einops' \\\n                 'torch-tb-profiler==0.3.1'\n    \n    \n # This is needed for mpi to locate libpython\n ENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n    \n RUN export CUDA_VISIBLE_DEVICES=0,1\n\n\n\nI tried everything, I even added the CUDA_VISIBLE_DEVICES=0,1 inside the docker file.\n\nMy cluster is correctly configured because my colleague can use another tool (Detr with Lightning) and use 100% of the computing power.\nI copied his docker file and the result was the same, so our guess is that his tool is automatically managing all GPUs for him.\n\nDoes anyone know why the cluster is using only one GPU ?",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_creation_time":1653503234337,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/864175\/why-pytorch-is-using-only-one-gpu.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-05-27T15:37:03Z",
                "Answer_score":0,
                "Answer_body":"That's interesting because it was written :\nVirtual machine size\nStandard_NC12 (12 cores, 112 GB RAM, 680 GB disk)\nProcessing unit\nGPU - 2 x NVIDIA Tesla K80\nThen I guess I did not understand it properly and I am stuck using 50% of 1 K80.\n\nprint(torch.cuda.device_count()) gives :\n2\n\nnode_count = 2 leads to :\nRequested 2 nodes but AzureMLCompute cluster only has 1 maximum nodes.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-01T08:42:04.183Z",
                "Answer_score":0,
                "Answer_body":"(I also realized in the job's properties raw json that gpuCount is 0 in the compute and computeRequest sections)",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-06-02T12:19:04.443Z",
                "Answer_score":1,
                "Answer_body":"model = nn.DataParallel(model)\ndid the job.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"pytorch gpu azur us gpu node pytorch hug face monitor tool azur show gpu usag stuck standard tri wai http azur github cheatsheet doc cheatsheet python distribut train distributeddataparallel process launch look like notebook copi docker file curat environ ad librari need successfulli mcr microsoft com openmpi cuda cudnn ubuntu env conda environ path env pytorch creat conda environ run conda creat conda environ path python pip pytorch torchvis torchaudio cudatoolkit nvidia apex gxx linux anaconda pytorch conda forg prepend path conda environ env path conda environ path bin path instal pip depend run pip instal matplotlib horovod futur transform einop torch profil need mpi locat libpython env librari path conda environ path lib librari path run export cuda visibl devic tri ad cuda visibl devic insid docker file cluster correctli configur colleagu us tool detr lightn us comput power copi docker file result guess tool automat manag gpu know cluster gpu",
        "Question_preprocessed_content":"pytorch gpu azur us gpu node pytorch monitor tool azur show gpu usag stuck tri wai look like notebook copi docker file curat environ ad librari need successfulli env creat conda environ run conda creat anaconda pytorch prepend path conda environ env path instal pip depend run pip instal transform einop need mpi locat libpython env run export tri ad insid docker file cluster correctli configur colleagu us tool us comput power copi docker file result guess tool automat manag gpu know cluster gpu",
        "Answer_original_content":"interest written virtual machin size standard core ram disk process unit gpu nvidia tesla guess understand properli stuck print torch cuda devic count give node count lead request node comput cluster maximum node realiz job properti raw json gpucount comput computerequest section model dataparallel model job",
        "Answer_preprocessed_content":"interest written virtual machin size process unit gpu nvidia tesla guess understand properli stuck give lead request node comput cluster maximum node realiz job properti raw json gpucount comput computerequest section model job",
        "Question_gpt_summary_original":"the user is encountering challenges with pytorch not utilizing both gpus of their node, despite their attempts to configure the environment correctly.",
        "Question_gpt_summary":"user encount challeng pytorch util gpu node despit attempt configur environ correctli",
        "Answer_gpt_summary_original":"the solution to the challenge of pytorch not utilizing both gpus of the node is to use the nn.dataparallel(model) function. the user had attempted to configure the environment correctly but was only able to use 50% of 1 k80 gpu. the virtual machine size was standard_nc12 with 2 x nvidia tesla k80 gpus, but the user did not understand it properly. the print(torch.cuda.device_count()) function gave 2, but the node_count was only 1. the requested 2 nodes were not available in the compute cluster. the job's properties raw json showed that gpucount was 0 in the compute and computerequest sections.",
        "Answer_gpt_summary":"solut challeng pytorch util gpu node us dataparallel model function user attempt configur environ correctli abl us gpu virtual machin size standard nvidia tesla gpu user understand properli print torch cuda devic count function gave node count request node avail comput cluster job properti raw json show gpucount comput computerequest section"
    },
    {
        "Question_id":73127303.0,
        "Question_title":"Get Experiment that Created Model in MLflow",
        "Question_body":"<p>I want to get the name of the experiment that contains the run that created a registered MLflow model. How can I do this using MLflow, if I just have the name of the model and the version?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1658855429667,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":44.0,
        "Answer_body":"<p>As @Andre has said, I had to write my own function to achieve this,<\/p>\n<pre><code>def get_model_experiment(model_name, model_version):\n    # get run_id of the model version\n    run_id = mlflow_client.get_model_version(model_name, model_version).run_id\n\n    # get the experiment_id from the run_id\n    experiment_id = mlflow_client.get_run(run_id).info.experiment_id\n\n    # get the experiment name from the experiment_id\n    return mlflow_client.get_experiment(experiment_id).name\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73127303",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1659073607990,
        "Question_original_content":"experi creat model want experi contain run creat regist model model version",
        "Question_preprocessed_content":"experi creat model want experi contain run creat regist model model version",
        "Answer_original_content":"andr said write function achiev def model experi model model version run model version run client model version model model version run experi run experi client run run info experi experi experi return client experi experi",
        "Answer_preprocessed_content":"said write function achiev",
        "Question_gpt_summary_original":"the user wants to find the name of the experiment that created a registered model using , given the name of the model and its version.",
        "Question_gpt_summary":"user want experi creat regist model given model version",
        "Answer_gpt_summary_original":"the answer suggests writing a custom function called \"get_model_experiment\" to find the name of the experiment that created a registered model using its name and version. the function retrieves the run id of the model version, then gets the experiment id from the run id, and finally returns the name of the experiment using the experiment id.",
        "Answer_gpt_summary":"answer suggest write custom function call model experi experi creat regist model version function retriev run model version get experi run final return experi experi"
    },
    {
        "Question_id":null,
        "Question_title":"How to retrain Automated ML generated Model with new Data",
        "Question_body":"Hi,\ni generated a machine learning model with Automated ML in Azure Machine Learning. Time to time i get new data, and i wanna feed the generated model with it.\nIs there a way to automatically retrain that model and updating the belonging endpoint.\n\nI have seen some intructions to this using Data Factory and Pipelines. Since I'm using Automated ML I'm not able to do it this way.\n\nThanks in advance.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1606117190360,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/171971\/how-to-retrain-automated-ml-generated-model-with-n.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-11-23T08:20:40.52Z",
                "Answer_score":0,
                "Answer_body":"@27051995 Thanks for the question. Model Deployment : AutoML models can be either downloaded locally or deployed to AML (AKS \/ ACI) for inference., Please use the Model Lifecycle management \/ MLOps: AzureML MLOps capabilities for continuous integration, retraining and automation pipelines.\n\nThe link below explains how to use Azure DevOps Project for build and release\/deployment pipelines along with Azure ML services for model retraining pipeline, model management and operationalization.\nhttps:\/\/github.com\/microsoft\/MLOpsPython\n\nPlease follow the below document for retrain your model on new data.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-model-management-and-deployment#retrain-your-model-on-new-data",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":4.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"retrain autom gener model new data gener machin learn model autom time time new data wanna feed gener model wai automat retrain model updat belong endpoint seen intruct data factori pipelin autom abl wai thank advanc",
        "Question_preprocessed_content":"retrain autom gener model new data gener machin learn model autom time time new data wanna feed gener model wai automat retrain model updat belong endpoint seen intruct data factori pipelin autom abl wai thank advanc",
        "Answer_original_content":"thank question model deploy automl model download local deploi aml ak aci infer us model lifecycl manag mlop mlop capabl continu integr retrain autom pipelin link explain us azur devop project build releas deploy pipelin servic model retrain pipelin model manag operation http github com microsoft mlopspython follow document retrain model new data http doc microsoft com azur machin learn concept model manag deploy retrain model new data",
        "Answer_preprocessed_content":"thank question model deploy automl model download local deploi aml us model lifecycl manag mlop mlop capabl continu integr retrain autom pipelin link explain us azur devop project build pipelin servic model retrain pipelin model manag operation follow document retrain model new data",
        "Question_gpt_summary_original":"the user is looking for a way to automatically retrain a machine learning model generated with automated ml and update the belonging endpoint with new data.",
        "Question_gpt_summary":"user look wai automat retrain machin learn model gener autom updat belong endpoint new data",
        "Answer_gpt_summary_original":"possible solutions from the answer include: \n- downloading the automl model locally or deploying it to aml (aks\/aci) for inference. \n- using model lifecycle management\/mlops capabilities for continuous integration, retraining, and automation pipelines. \n- following the azure devops project for build and release\/deployment pipelines along with services for model retraining pipeline, model management, and operationalization. \n- retraining the model on new data by following the document provided in the answer. \n\nin summary, the answer provides various solutions for automatically retraining a machine learning model generated with automated ml and updating the belonging endpoint with new data.",
        "Answer_gpt_summary":"possibl solut answer includ download automl model local deploi aml ak aci infer model lifecycl manag mlop capabl continu integr retrain autom pipelin follow azur devop project build releas deploy pipelin servic model retrain pipelin model manag operation retrain model new data follow document provid answer summari answer provid solut automat retrain machin learn model gener autom updat belong endpoint new data"
    },
    {
        "Question_id":null,
        "Question_title":"Compare command not working with hiplot on Windows 10",
        "Question_body":"<p>I am able to use <code>hiplot<\/code> in Jupyter notebook and also start using it via command line using <code>python -m hiplot<\/code>. But when I try to use it with <code>guild compare<\/code> I get invalid windows application error.<\/p>\n<pre><code>C:\\Users\\sarat.chinni\\Codes_sequencing\\hiplot&gt;guild compare --tool hiplot\nPreparing data for compare\nTraceback (most recent call last):\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 194, in _run_module_as_main\n    return _run_code(code, main_globals, None,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\runpy.py\", line 87, in _run_code\n    exec(code, run_globals)\n  File \"C:\\Users\\sarat.chinni\\Codes_sequencing\\biobench\\sandbox\\Sarat\\supervised_sequencing\\.venv\\Scripts\\guild.exe\\__main__.py\", line 7, in &lt;module&gt;\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main_bootstrap.py\", line 40, in main\n    _main()\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main_bootstrap.py\", line 66, in _main\n    guild.main.main()\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\main.py\", line 33, in main\n    main_cmd.main(standalone_mode=False)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 1259, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\click\\decorators.py\", line 21, in new_func\n    return f(get_current_context(), *args, **kwargs)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\click_util.py\", line 213, in fn\n    return fn0(*(args + (Args(**kw),)))\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare.py\", line 224, in compare\n    compare_impl.main(args, ctx)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 73, in main\n    _compare_with_tool(args)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 566, in _compare_with_tool\n    _compare_with_hiplot(args)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\commands\\compare_impl.py\", line 579, in _compare_with_hiplot\n    hiplot.compare_runs(get_data_cb)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 36, in compare_runs\n    _handle_default(hiplot, data)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 52, in _handle_default\n    _generate_hiplot_html(hiplot, csv_path, html_path)\n  File \"c:\\users\\sarat.chinni\\codes_sequencing\\biobench\\sandbox\\sarat\\supervised_sequencing\\.venv\\lib\\site-packages\\guild\\plugins\\hiplot.py\", line 100, in _generate_hiplot_html\n    html = subprocess.check_output([hiplot_include, \"--format\", \"html\", csv_path])\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 411, in check_output\n    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 489, in run\n    with Popen(*popenargs, **kwargs) as process:\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 854, in __init__\n    self._execute_child(args, executable, preexec_fn, close_fds,\n  File \"C:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.8_3.8.2032.0_x64__qbz5n2kfra8p0\\lib\\subprocess.py\", line 1307, in _execute_child\n    hp, ht, pid, tid = _winapi.CreateProcess(executable, args,\nOSError: [WinError 193] %1 is not a valid Win32 application\n<\/code><\/pre>\n<p>Does this has to do anything with <code>hiplot<\/code> is python script and not windows executable?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_creation_time":1610356086236,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":270.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/compare-command-not-working-with-hiplot-on-windows-10\/514",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-11T15:13:45.508Z",
                "Answer_body":"<p>I\u2019m able to recreate. Yes it\u2019s a fault with Guild in assuming that HiPlot provides a Windows script for the library (and lack of tests on Windows). This will be fixed in the next release.<\/p>\n<p>There\u2019s unfortunately no easy work around for this, beyond running in a Linux VM.<\/p>\n<p>I\u2019ll update here with progress.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T07:19:13.577Z",
                "Answer_body":"<p>Thanks for addressing this issue, I am able to use <code>hiplot<\/code> on windows with <code>guildai-0.7.3.dev1<\/code>.<\/p>\n<p>I am not able to install <code>guildai-0.7.2<\/code>(latest stable release) with python-3.8 in windows-10, looks like wheel files are missing in <a href=\"https:\/\/pypi.org\/project\/guildai\/#files\" rel=\"noopener nofollow ugc\">PyPi website<\/a>.<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T14:53:54.449Z",
                "Answer_body":"<p>Yes indeed \u2014 good catch! I\u2019m working to get the missing Windows platforms built now.<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-01-20T16:25:08.779Z",
                "Answer_body":"<p>All fixed - thanks for the note!<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"compar command work hiplot window abl us hiplot jupyt notebook start command line python hiplot try us compar invalid window applic error user sarat chinni code sequenc hiplot compar tool hiplot prepar data compar traceback recent file program file windowsapp pythonsoftwarefound python qbznkfrap lib runpi line run modul main return run code code main global file program file windowsapp pythonsoftwarefound python qbznkfrap lib runpi line run code exec code run global file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv script ex main line file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag main bootstrap line main main file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag main bootstrap line main main main file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag main line main main cmd main standalon mode fals file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag click core line return self main arg kwarg file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag click core line main self invok ctx file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag click core line invok return process result sub ctx command invok sub ctx file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag click core line invok return ctx invok self callback ctx param file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag click core line invok return callback arg kwarg file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag click decor line new func return current context arg kwarg file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag click util line return arg arg file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag command compar line compar compar impl main arg ctx file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag command compar impl line main compar tool arg file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag command compar impl line compar tool compar hiplot arg file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag command compar impl line compar hiplot hiplot compar run data file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag plugin hiplot line compar run handl default hiplot data file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag plugin hiplot line handl default gener hiplot html hiplot csv path html path file user sarat chinni code sequenc biobench sandbox sarat supervis sequenc venv lib site packag plugin hiplot line gener hiplot html html subprocess check output hiplot includ format html csv path file program file windowsapp pythonsoftwarefound python qbznkfrap lib subprocess line check output return run popenarg stdout pipe timeout timeout check true file program file windowsapp pythonsoftwarefound python qbznkfrap lib subprocess line run popen popenarg kwarg process file program file windowsapp pythonsoftwarefound python qbznkfrap lib subprocess line init self execut child arg execut preexec close fd file program file windowsapp pythonsoftwarefound python qbznkfrap lib subprocess line execut child pid tid winapi createprocess execut arg oserror winerror valid win applic hiplot python script window execut",
        "Question_preprocessed_content":"compar command work hiplot window abl us jupyt notebook start command line try us invalid window applic error python script window execut",
        "Answer_original_content":"abl recreat ye fault assum hiplot provid window script librari lack test window fix releas there unfortun easi work run linux ill updat progress thank address issu abl us hiplot window dev abl instal latest stabl releas python window look like wheel file miss pypi websit ye good catch work miss window platform built fix thank note",
        "Answer_preprocessed_content":"abl recreat ye fault assum hiplot provid window script librari fix releas there unfortun easi work run linux ill updat progress thank address issu abl us window abl instal look like wheel file miss pypi websit ye good catch work miss window platform built fix thank note",
        "Question_gpt_summary_original":"the user is encountering an issue with using the 'guild compare' command with hiplot on windows 10, resulting in an \"invalid windows application error\".",
        "Question_gpt_summary":"user encount issu compar command hiplot window result invalid window applic error",
        "Answer_gpt_summary_original":"possible solutions extracted from the answer are:\n- the issue with using the 'guild compare' command with hiplot on windows 10 will be fixed in the next release.\n- running in a linux vm is the only workaround for this issue.\n- hiplot can be used on windows with guildai-0.7.3.dev1.\n- the missing wheel files for guildai-0.7.2 with python-3.8 in windows-10 are being built and will be fixed soon.",
        "Answer_gpt_summary":"possibl solut extract answer issu compar command hiplot window fix releas run linux workaround issu hiplot window dev miss wheel file python window built fix soon"
    },
    {
        "Question_id":71777914.0,
        "Question_title":"Change AWS SageMaker LogGroup Prefix?",
        "Question_body":"<p>We have applications for multiple tenants on our AWS account and would like to distinguish between them in different IAM roles. In most places this is already possible by limiting resource access based on naming patterns.<\/p>\n<p>For CloudWatch log groups of SageMaker training jobs however I have not seen a working solution yet. The tenants can choose the job name arbitrarily, and hence the only part of the LogGroup name that is available for pattern matching would be the prefix before the job name. This prefix however seems to be fixed to <code>\/aws\/sagemaker\/TrainingJobs<\/code>.<\/p>\n<p>Is there a way to change or extend this prefix in order to make such limiting possible? Say, for example <code>\/aws\/sagemaker\/TrainingJobs\/&lt;product&gt;-&lt;stage&gt;-&lt;component&gt;\/&lt;training-job-name&gt;-...<\/code> so that a resource limitation like <code>\/aws\/sagemaker\/TrainingJobs\/&lt;product&gt;-*<\/code> becomes possible?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1649316153020,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":48.0,
        "Answer_body":"<p>I think it is not possible to change the log streams names for any of the SageMaker services.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71777914",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1651281451296,
        "Question_original_content":"chang loggroup prefix applic multipl tenant aw account like distinguish differ iam role place possibl limit resourc access base name pattern cloudwatch log group train job seen work solut tenant choos job arbitrarili loggroup avail pattern match prefix job prefix fix aw trainingjob wai chang extend prefix order limit possibl exampl aw trainingjob resourc limit like aw trainingjob possibl",
        "Question_preprocessed_content":"chang loggroup prefix applic multipl tenant aw account like distinguish differ iam role place possibl limit resourc access base name pattern cloudwatch log group train job seen work solut tenant choos job arbitrarili loggroup avail pattern match prefix job prefix fix wai chang extend prefix order limit possibl exampl resourc limit like possibl",
        "Answer_original_content":"think possibl chang log stream name servic",
        "Answer_preprocessed_content":"think possibl chang log stream name servic",
        "Question_gpt_summary_original":"the user is encountering a challenge in distinguishing between multiple tenants on their aws account in cloudwatch log groups of training jobs, as the prefix before the job name is fixed to \"\/aws\/\/trainingjobs\" and cannot be changed or extended.",
        "Question_gpt_summary":"user encount challeng distinguish multipl tenant aw account cloudwatch log group train job prefix job fix aw trainingjob chang extend",
        "Answer_gpt_summary_original":"summary: unfortunately, it is not possible to change the log stream names for any aws services, including cloudwatch log groups for training jobs. therefore, there may not be any solutions to the user's challenge of distinguishing between multiple tenants on their aws account.",
        "Answer_gpt_summary":"summari unfortun possibl chang log stream name aw servic includ cloudwatch log group train job solut user challeng distinguish multipl tenant aw account"
    },
    {
        "Question_id":null,
        "Question_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Question_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1607681930000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":65.0,
        "Answer_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sage-maker-manifest-files-enable-dataset-versioning",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-12-11T10:56:42.000Z",
                "Answer_score":0,
                "Answer_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1607684202000,
        "Question_original_content":"manifest file enabl dataset version algorithm train manifest json file store map imag amazon arn metadata label great option manifest file smaller dataset manifest file small easili version tool save model artifact appear best construct enabl exact dataset version exclud creation uniqu train set hard copi train job scale larg dataset understand accur",
        "Question_preprocessed_content":"manifest file enabl dataset version algorithm train manifest json file store map imag amazon arn metadata label great option manifest file smaller dataset manifest file small easili version tool save model artifact appear best construct enabl exact dataset version exclud creation uniqu train set hard copi train job scale larg dataset understand accur",
        "Answer_original_content":"creat condit immut asset manifest point manifest enabl exact dataset version data store amazon version data asset us manifest file creat version dataset specif usag guarante immut asset manifest point manifest invalid",
        "Answer_preprocessed_content":"creat condit immut asset manifest point manifest enabl exact dataset version data store amazon version data asset us manifest file creat version dataset specif usag guarante immut asset manifest point manifest invalid",
        "Question_gpt_summary_original":"the user is questioning whether manifest files can enable dataset versioning, and is considering this as the best construct for exact dataset versioning.",
        "Question_gpt_summary":"user question manifest file enabl dataset version consid best construct exact dataset version",
        "Answer_gpt_summary_original":"possible solutions: \n\n- creating conditions for immutability of the assets that the manifest points to can enable exact dataset versioning.\n- using a data store in amazon s3 with all versions of the data assets and manifest files for creating and versioning datasets for specific usage can also enable exact dataset versioning.\n- however, if immutability for the assets that the manifest points to is not guaranteed, then the manifest becomes invalid. \n\nsummary: manifest files can enable exact dataset versioning if immutability of the assets that the manifest points to is guaranteed. a data store in amazon s3 with all versions of the data assets and manifest files can also be used for creating and versioning datasets for specific usage.",
        "Answer_gpt_summary":"possibl solut creat condit immut asset manifest point enabl exact dataset version data store amazon version data asset manifest file creat version dataset specif usag enabl exact dataset version immut asset manifest point guarante manifest invalid summari manifest file enabl exact dataset version immut asset manifest point guarante data store amazon version data asset manifest file creat version dataset specif usag"
    },
    {
        "Question_id":null,
        "Question_title":"Train using SageMaker but track output using DVC",
        "Question_body":"<p>I wonder what is the best way to use Sagemaker with DVC in particular for running a train step which is part of a DVC pipeline. The problem I am running into is that SageMaker will create a training job and output somewhere different than the machine that invokes the training job.<\/p>\n<p>Prior to using SageMaker our flow was to ssh into the relevant EC2 instance and run dvc repro. Now we have a command that invokes the same train job from our local machine which essentially kicks out train in an EC2 instance of our choice and stores the output in a s3 bucket.<\/p>\n<p>Is there a way to run the training job, sync the output locally and inform DVC that this is the output of running a particular step in the pipeline? If that is complicated, is there a better alternative to working with SageMaker and DVC?<\/p>",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1623143837370,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":1025.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/train-using-sagemaker-but-track-output-using-dvc\/782",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-06-08T10:31:55.687Z",
                "Answer_body":"<p>Hello <a class=\"mention\" href=\"\/u\/nsorros\">@nsorros<\/a>!<br>\nI don\u2019t have any experience with Sagemaker, so please feel free to correct me if my assumptions are wrong.<br>\nSo, the command that invokes your run is some kind of <code>aws<\/code> cli command?<br>\nAfter you trigger the build I presume you are not awaiting for the result, but the build at Sagemaker starts and will finish \u201cat some point in the future\u201d? Your local machine does not hang the terminal session that you use for triggering until the run is finished?<\/p>",
                "Answer_score":23.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-08T12:00:35.862Z",
                "Answer_body":"<p>Yes in all. The terminal does hang streaming the logs but there is no harm in closing the terminal and reading the logs using the aws cli. Also I am not invoking the job through the aws cli but from the python sdk but the end result is the same. Looking forward to hearing your thoughts<\/p>",
                "Answer_score":2.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-08T12:23:01.909Z",
                "Answer_body":"<p>The fact that our training can be detached can be problematic. DVC is build with assumption that after <code>dvc run<\/code> or <code>dvc repro<\/code> there will be and output as defined in <code>dvc.yaml<\/code>. The fact that SageMaker does not do that, can potentially limit the dvc capabilities.<\/p>\n<p>One more question: how does experiment execution look like from SageMaker point of view? Does it clone your project repository and do <code>dvc repro<\/code> there or, does it just call the training code? How does it look like?<\/p>",
                "Answer_score":3.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-08T13:36:34.625Z",
                "Answer_body":"<p>It does not clone the project, it packages the project in a docker container that AWS has created or that  we have created, we choose the latter. Our docker container contains all dependencies and then we package the code separately in a tar and send it separaty, essentially there is a bit of code that sagemaker has which copies the code inside before it executes. We do that so we can do quick experimentation i.e. change the code and quickly launch and experiment instead of creating another big container and pushing.<\/p>\n<p>We actually do not run <code>dvc repro<\/code> inside the container, we run <code>train.py<\/code> with some args, similar to the command that we have instructed dvc repro to run at that stage. We could trigger <code>dvc repro TRAIN_STAGE<\/code> instead but would that work? What would it require present for dvc to work properly? cause at the moment it does not have any dvc related files.<\/p>\n<p>Generally I am keen to hear ideas how we can work around it or what dvc requires \/ assumes to offer suggestions. So from what I understand, there is no way of telling DVC that an output was created from running a dvc repro command somewhere else.<\/p>",
                "Answer_score":62.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-06-09T10:48:31.476Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/nsorros\">@nsorros<\/a><br>\nSorry for so much questions, but I am trying to understand how sagemaker works:<\/p>\n<p>how does model file gets created? Is it created in docker image and then sagemaker takes care of moving it to s3?<\/p>\n<p>How do you run train? Do you define some script for sagemaker with the steps that need to be taken to create <code>model<\/code>?<\/p>",
                "Answer_score":12.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-27T10:31:19.748Z",
                "Answer_body":"<p>I am not a expert of Sagemaker but I try to use it with DVC.<br>\nHas anyone found a solution to use it with the dvc command?<\/p>\n<p>There is 2 input mode File ( local copy of the s3 bucket in the docker container)  or Pipe  (stream).<\/p>\n<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\">\n  <header class=\"source\">\n      <img src=\"https:\/\/a0.awsstatic.com\/main\/images\/site\/fav\/favicon.ico\" class=\"site-icon\" width=\"16\" height=\"16\">\n\n      <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" target=\"_blank\" rel=\"noopener nofollow ugc\" title=\"02:06PM - 23 May 2018\">Amazon Web Services \u2013 23 May 18<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/345;\"><img src=\"https:\/\/d2908q01vomqb2.cloudfront.net\/f1f836cb4ea6efb2a0b1b99f41ad8b103eff4b59\/2018\/05\/23\/pipe-mode-social.gif\" class=\"thumbnail\" width=\"690\" height=\"345\"><\/div>\n\n<h3><a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/using-pipe-input-mode-for-amazon-sagemaker-algorithms\/\" target=\"_blank\" rel=\"noopener nofollow ugc\">Using Pipe input mode for Amazon SageMaker algorithms | Amazon Web Services<\/a><\/h3>\n\n  <p>Today, we are introducing Pipe input mode support for the Amazon SageMaker built-in algorithms. With Pipe input mode, your dataset is streamed directly to your training instances instead of being downloaded first. This means that your training jobs...<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":11.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-29T10:15:53.420Z",
                "Answer_body":"<p>Hi all.<br>\nWhat are the benefits of using sagemaker?<\/p>\n<ul>\n<li>spot instances<\/li>\n<li>sync intermediate data stored in \/opt\/ml\/checkpoints, \/opt\/ml\/models\u2026 to S3 to be able to resume<\/li>\n<\/ul>\n<p>Sync intermediate data is configurable through sagemaker stimators (plus hardware settings, etc\u2026), so you could change the tracked folders to be your project structure including .dvc and dvc.yml outputs (tracked files by dvc).<br>\nSetting them your data will survive across the spot instances if you have implemented a resume mechanism.<\/p>\n<p>Optionally your could use out Task resource of our terraform-provider-iterative<br>\nWe designed a new resource in our terraform provider that essentially mimics sagemaker behaviour or <a class=\"mention\" href=\"\/u\/nsorros\">@nsorros<\/a> own implementation also without any vendor lock.<br>\nIt\u2019s a task (bash script) that will try to run from the beginning to the end surviving spot instances syncing all the intermediate data within a bucket.<br>\nWorks the same in azure, gcp or AWS.<\/p>\n<p>We are looking for beta testers and I will be personally involved with them in setting it successfully. Please ping me if interested!<\/p>",
                "Answer_score":36.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-04-29T03:27:13.471Z",
                "Answer_body":"<aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/github.com\/iterative\/terraform-provider-iterative\">\n  <header class=\"source\">\n      <img src=\"https:\/\/github.githubassets.com\/favicons\/favicon.svg\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/github.com\/iterative\/terraform-provider-iterative\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <div class=\"aspect-image\" style=\"--aspect-ratio:690\/345;\"><img src=\"https:\/\/opengraph.githubassets.com\/917a654674f8c3d2d3e0897afec001d8b96d54e9c602d8a7c90a4f6e3b280173\/iterative\/terraform-provider-iterative\" class=\"thumbnail\" width=\"690\" height=\"345\"><\/div>\n\n<h3><a href=\"https:\/\/github.com\/iterative\/terraform-provider-iterative\" target=\"_blank\" rel=\"noopener nofollow ugc\">GitHub - iterative\/terraform-provider-iterative: \u2601\ufe0f Terraform plugin for...<\/a><\/h3>\n\n  <p>\u2601\ufe0f Terraform plugin for machine learning workloads: spot instance recovery &amp; auto-termination | AWS, GCP, Azure, Kubernetes - GitHub - iterative\/terraform-provider-iterative: \u2601\ufe0f Terraform plugi...<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"train track output wonder best wai us particular run train step pipelin problem run creat train job output differ machin invok train job prior flow ssh relev instanc run repro command invok train job local machin essenti kick train instanc choic store output bucket wai run train job sync output local inform output run particular step pipelin complic better altern work",
        "Question_preprocessed_content":"train track output wonder best wai us particular run train step pipelin problem run creat train job output differ machin invok train job prior flow ssh relev instanc run repro command invok train job local machin essenti kick train instanc choic store output bucket wai run train job sync output local inform output run particular step pipelin complic better altern work",
        "Answer_original_content":"hello nsorro dont experi feel free correct assumpt wrong command invok run kind aw cli command trigger build presum await result build start finish point futur local machin hang termin session us trigger run finish ye termin hang stream log harm close termin read log aw cli invok job aw cli python sdk end result look forward hear thought fact train detach problemat build assumpt run repro output defin yaml fact potenti limit capabl question experi execut look like point view clone project repositori repro train code look like clone project packag project docker contain aw creat creat choos docker contain contain depend packag code separ tar send separati essenti bit code copi code insid execut quick experiment chang code quickli launch experi instead creat big contain push actual run repro insid contain run train arg similar command instruct repro run stage trigger repro train stage instead work requir present work properli caus moment relat file gener keen hear idea work requir assum offer suggest understand wai tell output creat run repro command nsorro sorri question try understand work model file get creat creat docker imag take care move run train defin script step need taken creat model expert try us solut us command input mode file local copi bucket docker contain pipe stream amazon web servic pipe input mode algorithm amazon web servic todai introduc pipe input mode support built algorithm pipe input mode dataset stream directli train instanc instead download mean train job benefit spot instanc sync intermedi data store opt checkpoint opt model abl resum sync intermedi data configur stimat plu hardwar set chang track folder project structur includ yml output track file set data surviv spot instanc implement resum mechan option us task resourc terraform provid iter design new resourc terraform provid essenti mimic behaviour nsorro implement vendor lock task bash script try run begin end surviv spot instanc sync intermedi data bucket work azur gcp aw look beta tester person involv set successfulli ping interest github github iter terraform provid iter terraform plugin terraform plugin machin learn workload spot instanc recoveri auto termin aw gcp azur kubernet github iter terraform provid iter terraform plugi",
        "Answer_preprocessed_content":"hello dont experi feel free correct assumpt wrong command invok run kind cli command trigger build presum await result build start finish point futur local machin hang termin session us trigger run finish ye termin hang stream log harm close termin read log aw cli invok job aw cli python sdk end result look forward hear thought fact train detach problemat build assumpt output defin fact potenti limit capabl question experi execut look like point view clone project repositori train code look like clone project packag project docker contain aw creat creat choos docker contain contain depend packag code separ tar send separati essenti bit code copi code insid execut quick experiment chang code quickli launch experi instead creat big contain push actual run insid contain run arg similar command instruct repro run stage trigger instead work requir present work properli caus moment relat file gener keen hear idea work requir assum offer suggest understand wai tell output creat run repro command sorri question try understand work model file get creat creat docker imag take care move run train defin script step need taken creat expert try us solut us command input mode file pipe amazon web servic pipe input mode algorithm amazon web servic todai introduc pipe input mode support algorithm pipe input mode dataset stream directli train instanc instead download mean train benefit spot instanc sync intermedi data store abl resum sync intermedi data configur stimat chang track folder project structur includ yml output set data surviv spot instanc implement resum mechan option us task resourc design new resourc terraform provid essenti mimic behaviour implement vendor lock task try run begin end surviv spot instanc sync intermedi data bucket work azur gcp aw look beta tester person involv set successfulli ping interest github github terraform plugin terraform plugin machin learn workload spot instanc recoveri aw gcp azur kubernet github terraform",
        "Question_gpt_summary_original":"the user is encountering challenges with using  to run a train step as part of a pipeline, as the output is stored in a different location than the machine that invokes the training job.",
        "Question_gpt_summary":"user encount challeng run train step pipelin output store differ locat machin invok train job",
        "Answer_gpt_summary_original":"there are no clear solutions provided in the answer. the conversation is mainly focused on understanding how  works and how to work around the limitations of the platform. some suggestions include using the aws cli to read logs, packaging the project in a docker container for quick experimentation, and using the pipe input mode for streaming datasets directly to training instances. additionally, the answer mentions a new resource in the terraform provider that mimics  behavior without vendor lock and is looking for beta testers.",
        "Answer_gpt_summary":"clear solut provid answer convers mainli focus understand work work limit platform suggest includ aw cli read log packag project docker contain quick experiment pipe input mode stream dataset directli train instanc addition answer mention new resourc terraform provid mimic behavior vendor lock look beta tester"
    },
    {
        "Question_id":null,
        "Question_title":"AzureML Scoring Script fails with ImportError: no known parent package",
        "Question_body":"On trying to deploy a Model as a Container, endpoint gets created, however, scoring script fails with an error:\n\nImportError: attempted relative import with no known parent package\n\n\n\n\n\nThis is because i'm referencing another module (packaged in the docker image using source_directory) with a relative path from scoring file.\n\nCan you help me in resolving this error?\n\nFiles\\modules structure (a simplified version):\n\nproject\n->src\n-> scoring.py\n-> module1.py\n-> common\n-> module2.py, etc\n-> init.py\n-> init.py\n-> configs\n-> conda_env.yml\n\nIn scoring.py,\nfrom .module1.py import SomeClass\n..\n..\n\nIn module1.py,\nfrom .common.module2.py importSC2\n...\n..\n\n\n\n\n\nAnd below is how an Inference config is initialized:\ninference_config = InferenceConfig(source_directory=\".\/\",\nruntime= \"python\",\nentry_script=\"src\/scoring.py\",\nconda_file=\"configs\/conda_env.yml\"\n)\n\n\n\n\n\nI could not pass entry_script as \"src.scoring\" as this fails the Validation and relative path to scoring file is expected",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1603969996160,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/144492\/azureml-scoring-script-fails-with-importerror-no-k.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-10-29T16:20:54.573Z",
                "Answer_score":0,
                "Answer_body":"@SunilSinghal-3380 Thanks for the question. When deploying your inference script, beyond the entry script (score.py), inferenceConfig also let you specify source directory that include the entry script as well as all other python code (packages as a subfolder in the source directory that has its own init.py, or plain python script files modules). The score.py script can directly import from them because the whole folder including score.py and all other folders will be available at the inference running environment. There is no need to save them as a \"model\".\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.inferenceconfig?view=azure-ml-py\n\nFull sample available at https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/deployment\/deploy-to-cloud\/model-register-and-deploy.ipynb",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-11-02T10:44:15.697Z",
                "Answer_score":0,
                "Answer_body":"@SunilSinghal-3380 Thanks for the details. When you specify a source directory and a path(relative to the source_directory) to entry script, and if your deployment is failing, most likely the issue is with how you entry_script references other files in the source_directory. register-model-deploy-local-advanced.ipynb has an example of how to specify source directory and perform a local deployment for faster troubleshooting.\n\nHow to deploy using environments can be found here model-register-and-deploy.ipynb . InferenceConfig class accepts source_directory and entry_script parameters, where source_directory is a path to the folder that contains all files(score.py and any other additional files) to create the image.\nThis multi-model-register-and-deploy.ipynb has code snippets on how to create InferenceConfig with source_directory and entry_script.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-12T17:44:52.173Z",
                "Answer_score":0,
                "Answer_body":"Hi, I am having the same issue. Could you please direct me to the right resource from where I can get some insight to solve my issue. I have a trained model file which also includes some supported py module files. The tree is shown below:\n\nmodel --> conf --> hmcn.json\n--> data --> (some other required json files)\n--> dataset --> init.py\n--> classification_dataset.py\n--> collator.py\n--> data_preprocessor.py\n--> dataset.py\n--> model --> (some other .py module files)\n--> config.py\n--> HMCN\n--> util.py\n\nIn the above tree HMCN is the model file and as shown there are some python module files which are imported in the score.py (not included in this tree) script for inferencing. I was using the AZURE Web UI and registered the model by selecting the \"model\" directory so that in the artifacts tab the root item is shown as \"model\" and inside the model there are all the files shown in the tree are uploaded.\n\nNow I am trying to create an endpoint and deploy the model by using a \"score.py\" script for real-time inferencing. I did it with the WebUI by going to Endpoints --> Create deployment and in the Environment I selected score.py and choose PyTorch 1.9 curated environment but the during deployment the process fails and gives error that the module not found in the \"score.py\". The module that it is referring is in the \"config.py\" as shown in the model tree. I believe I need to set the path of the model tree so that the \"score.py\" can find the module. But I do now know how.\n\nYour help would be greatly appreciated. Thanks",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":5.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"score script fail importerror known parent packag try deploi model contain endpoint get creat score script fail error importerror attempt rel import known parent packag referenc modul packag docker imag sourc directori rel path score file help resolv error file modul structur simplifi version project src score modul common modul init init config conda env yml score modul import someclass modul common modul importsc infer config initi infer config inferenceconfig sourc directori runtim python entri script src score conda file config conda env yml pass entri script src score fail valid rel path score file expect",
        "Question_preprocessed_content":"score script fail importerror known parent packag try deploi model contain endpoint get creat score script fail error importerror attempt rel import known parent packag referenc modul rel path score file help resolv error structur project src common config import someclass importsc infer config initi runtim python pass fail valid rel path score file expect",
        "Answer_original_content":"sunilsingh thank question deploi infer script entri script score inferenceconfig let specifi sourc directori includ entri script python code packag subfold sourc directori init plain python script file modul score script directli import folder includ score folder avail infer run environ need save model http doc microsoft com python api core core model inferenceconfig view azur sampl avail http github com azur machinelearningnotebook blob master us deploy deploi cloud model regist deploi ipynb sunilsingh thank detail specifi sourc directori path rel sourc directori entri script deploy fail like issu entri script refer file sourc directori regist model deploi local advanc ipynb exampl specifi sourc directori perform local deploy faster troubleshoot deploi environ model regist deploi ipynb inferenceconfig class accept sourc directori entri script paramet sourc directori path folder contain file score addit file creat imag multi model regist deploi ipynb code snippet creat inferenceconfig sourc directori entri script have issu direct right resourc insight solv issu train model file includ support modul file tree shown model conf hmcn json data requir json file dataset init classif dataset collat data preprocessor dataset model modul file config hmcn util tree hmcn model file shown python modul file import score includ tree script inferenc azur web regist model select model directori artifact tab root item shown model insid model file shown tree upload try creat endpoint deploi model score script real time inferenc webui go endpoint creat deploy environ select score choos pytorch curat environ deploy process fail give error modul score modul refer config shown model tree believ need set path model tree score modul know help greatli appreci thank",
        "Answer_preprocessed_content":"thank question deploi infer script entri script inferenceconfig let specifi sourc directori includ entri script python code script directli import folder includ folder avail infer run environ need save model sampl avail thank detail specifi sourc directori path entri script deploy fail like issu refer file exampl specifi sourc directori perform local deploy faster troubleshoot deploi environ inferenceconfig class accept paramet path folder contain addit file creat imag code snippet creat inferenceconfig have issu direct right resourc insight solv issu train model file includ support modul file tree shown model conf data dataset model hmcn tree hmcn model file shown python modul file import script inferenc azur web regist model select model directori artifact tab root item shown model insid model file shown tree upload try creat endpoint deploi model script inferenc webui go endpoint creat deploy environ select choos pytorch curat environ deploy process fail give error modul modul refer shown model tree believ need set path model tree modul know help greatli appreci thank",
        "Question_gpt_summary_original":"the user is encountering an issue with their scoring script failing with an importerror when attempting to deploy a model as a container, due to referencing another module with a relative path from the scoring file.",
        "Question_gpt_summary":"user encount issu score script fail importerror attempt deploi model contain referenc modul rel path score file",
        "Answer_gpt_summary_original":"the answer suggests that when deploying an inference script, the user can specify a source directory that includes the entry script as well as all other python code. the score.py script can directly import from them because the whole folder including score.py and all other folders will be available at the inference running environment. the inferenceconfig class accepts source_directory and entry_script parameters, where source_directory is a path to the folder that contains all files (score.py and any other additional files) to create the image. if the deployment is failing, the issue is most likely with how the entry_script references other files in the source_directory. the answer also provides links to sample code and resources for troubleshooting.",
        "Answer_gpt_summary":"answer suggest deploi infer script user specifi sourc directori includ entri script python code score script directli import folder includ score folder avail infer run environ inferenceconfig class accept sourc directori entri script paramet sourc directori path folder contain file score addit file creat imag deploy fail issu like entri script refer file sourc directori answer provid link sampl code resourc troubleshoot"
    },
    {
        "Question_id":null,
        "Question_title":"Track all experiments with a separate directory",
        "Question_body":"<p>I am reading about experiment management <a href=\"https:\/\/dvc.org\/doc\/user-guide\/experiment-management\" rel=\"noopener nofollow ugc\">https:\/\/dvc.org\/doc\/user-guide\/experiment-management<\/a> and I am interested in exploring the directories organisation pattern where you can keep all your experiments that you have run recorded at the same time and not through git history.<\/p>\n<p>What is not clear to me is how to implement that pattern though? Anyone has a better idea of how this works?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1618393378968,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":230.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/track-all-experiments-with-a-separate-directory\/722",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-14T12:44:30.347Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/nsorros\">@nsorros<\/a>,<\/p>\n<p>Developing workflows for experiment management is still a work in progress for DVC. The most natural way to organize experiments now is to use <code>dvc exp run<\/code> and compare them with <code>dvc exp show<\/code>.<\/p>\n<p>If it\u2019s important for you to have each experiment in its own directory, you could instead look into using <code>foreach<\/code> stages to define a separate stage for each experiment: <a href=\"https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#foreach-stages\">https:\/\/dvc.org\/doc\/user-guide\/project-structure\/pipelines-files#foreach-stages<\/a>. With this approach, keep in mind that each experiment\u2019s metrics will be written to a different file, making it harder to compare them with commands like <code>dvc exp show<\/code>.<\/p>\n<p>As I said, these workflows are still a work in progress, so feel free to provide more context for why you want to use separate directories or open a feature request in <a href=\"https:\/\/github.com\/iterative\/dvc\/discussions\/categories\/new-features-ideas\" class=\"inline-onebox\">Discussions \u00b7 iterative\/dvc \u00b7 GitHub<\/a>.<\/p>",
                "Answer_score":21.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"track experi separ directori read experi manag http org doc user guid experi manag interest explor directori organis pattern experi run record time git histori clear implement pattern better idea work",
        "Question_preprocessed_content":"track experi separ directori read experi manag interest explor directori organis pattern experi run record time git histori clear implement pattern better idea work",
        "Answer_original_content":"nsorro develop workflow experi manag work progress natur wai organ experi us exp run compar exp import experi directori instead look foreach stage defin separ stage experi http org doc user guid project structur pipelin file foreach stage approach mind experi metric written differ file make harder compar command like exp said workflow work progress feel free provid context want us separ directori open featur request discuss iter github",
        "Answer_preprocessed_content":"develop workflow experi manag work progress natur wai organ experi us compar import experi directori instead look stage defin separ stage experi approach mind experi metric written differ file make harder compar command like said workflow work progress feel free provid context want us separ directori open featur request discuss iter github",
        "Question_gpt_summary_original":"the user is interested in exploring a directory organisation pattern to track all experiments, but is unsure of how to implement it.",
        "Question_gpt_summary":"user interest explor directori organis pattern track experi unsur implement",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. use exp run and exp show to organize experiments.\n2. use foreach stages to define a separate stage for each experiment and keep in mind that each experiment's metrics will be written to a different file, making it harder to compare them with commands like exp show.\n3. provide more context for why you want to use separate directories or open a feature request in discussions iterative\/ github.",
        "Answer_gpt_summary":"possibl solut answer us exp run exp organ experi us foreach stage defin separ stage experi mind experi metric written differ file make harder compar command like exp provid context want us separ directori open featur request discuss iter github"
    },
    {
        "Question_id":52003180.0,
        "Question_title":"How to understand output from a Multiclass Neural Network",
        "Question_body":"<p>Built a flow in Azure ML using a Neural network Multiclass module (for settings see picture). \n <a href=\"https:\/\/i.stack.imgur.com\/6xTYY.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/6xTYY.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Some more info about the Multiclass:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/lLUVC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lLUVC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The data flow is simple, split of 80\/20. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/k72ZX.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/k72ZX.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Preparation of the data is made before it goes into Azure. Data looks like this: <a href=\"https:\/\/i.stack.imgur.com\/SoSUa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SoSUa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My problem comes when I want to make sense of the output and if possible transform\/calculate the output to probabilities. Output looks like this: <a href=\"https:\/\/i.stack.imgur.com\/kBZOM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kBZOM.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>My question: If scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? And how sure can I be that actual outcome will be a 1?<\/p>\n\n<p>Can I safely assume that a scored probabilities of 0.80 = 80% chance of outcome? Or what type of outcomes should I watch out for?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_creation_time":1535108970810,
        "Question_favorite_count":null,
        "Question_last_edit_time":1535126329423,
        "Question_score":0.0,
        "Question_view_count":263.0,
        "Answer_body":"<p>To start with, your are in a <em>binary<\/em> classification setting, not in a multi-class one (we normally use this term when number of classes > 2).<\/p>\n\n<blockquote>\n  <p>If scored probabilities output for my model is 0.6 and scored labels = 1, how sure is the model of the scored labels 1? <\/p>\n<\/blockquote>\n\n<p>In <em>practice<\/em>, the scored probabilities are routinely interpreted as the <em>confidence<\/em> of the model; so, in this example, we would say that your model has 60% confidence that the particular sample belongs to class 1 (and, complementary, 40% confidence that it belongs to class 0).<\/p>\n\n<blockquote>\n  <p>And how sure can I be that actual outcome will be a 1?<\/p>\n<\/blockquote>\n\n<p>If you don't have any alternate means of computing such outcomes yourself (e.g. a different model), I cannot see how this question is different from your previous one.<\/p>\n\n<blockquote>\n  <p>Can I safely assume that a scored probabilities of 0.80 = 80% chance of outcome? <\/p>\n<\/blockquote>\n\n<p>This is the kind of statement that would drive a professional statistician mad; nevertheless, the clarifications above regarding the confidence should be enough for your purposes (they are enough indeed for ML practitioners).<\/p>\n\n<p>My answer in <a href=\"https:\/\/stackoverflow.com\/questions\/51367755\/predict-classes-or-class-probabilities\/51423325#51423325\">Predict classes or class probabilities?<\/a> should also be helpful.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1535111877860,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52003180",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1535111519160,
        "Question_original_content":"understand output multiclass neural network built flow neural network multiclass modul set pictur info multiclass data flow simpl split prepar data goe azur data look like problem come want sens output possibl transform calcul output probabl output look like question score probabl output model score label sure model score label sure actual outcom safe assum score probabl chanc outcom type outcom watch",
        "Question_preprocessed_content":"understand output multiclass neural network built flow neural network multiclass modul info multiclass data flow simpl split prepar data goe azur data look like problem come want sens output possibl output probabl output look like question score probabl output model score label sure model score label sure actual outcom safe assum score probabl chanc outcom type outcom watch",
        "Answer_original_content":"start binari classif set multi class normal us term number class score probabl output model score label sure model score label practic score probabl routin interpret confid model exampl model confid particular sampl belong class complementari confid belong class sure actual outcom altern mean comput outcom differ model question differ previou safe assum score probabl chanc outcom kind statement drive profession statistician mad clarif confid purpos practition answer predict class class probabl help",
        "Answer_preprocessed_content":"start binari classif set score probabl output model score label sure model score label practic score probabl routin interpret confid model exampl model confid particular sampl belong class sure actual outcom altern mean comput outcom question differ previou safe assum score probabl chanc outcom kind statement drive profession statistician mad clarif confid purpos answer predict class class probabl help",
        "Question_gpt_summary_original":"the user is encountering challenges in understanding the output from a multiclass neural network and transforming\/calculating the output to probabilities.",
        "Question_gpt_summary":"user encount challeng understand output multiclass neural network transform calcul output probabl",
        "Answer_gpt_summary_original":"the answer suggests that the user is in a binary classification setting and that the scored probabilities output from the model can be interpreted as the confidence of the model. the answer also clarifies that the scored probabilities cannot be used to predict the actual outcome with certainty and that it is not appropriate to assume that a scored probability of 0.80 equals an 80% chance of outcome. the answer recommends referring to a previous answer for more information on predicting classes or class probabilities.",
        "Answer_gpt_summary":"answer suggest user binari classif set score probabl output model interpret confid model answer clarifi score probabl predict actual outcom certainti appropri assum score probabl equal chanc outcom answer recommend refer previou answer inform predict class class probabl"
    },
    {
        "Question_id":null,
        "Question_title":"Group Categorical Values in Azure ML Designer",
        "Question_body":"Hello,\nis there any possibility to do the same in Azure ML Studio designer as it is able in Azure ML Studio? I need to group categorical values, but in Azure ML Studio designer there option for Group Data into Bins. When I am trying to do it by chosing custom edges, it does not seem to work with data column which is categorical.\n\nEDIT\nI have rating 1-5, and I would like to make it in from 1-2-3-4-5 to 1-5. When creating bins I only can get categorized as 1 and 2.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_creation_time":1615818855177,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/315002\/group-categorical-values-in-azure-ml-designer.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-04-05T07:34:31.703Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nWe are sorry that have not heard from you. Azure ML Designer officially supports Group data into Bins with custom edge. I have highlighted the essential part below.\n\nAdd the Group Data Into Bins module to your pipeline in the designer. You can find this module in the category Data Transformation.\n\n\n\n\nConnect the dataset that has numerical data to bin. Quantization can be applied only to columns that contain numeric data.\n\nIf the dataset contains non-numeric columns, use the Select Columns in Dataset module to select a subset of columns to work with.\n\nSpecify the binning mode. The binning mode determines other parameters, so be sure to select the Binning mode option first. The following types of binning are supported:\n\nQuantiles: The quantile method assigns values to bins based on percentile ranks. This method is also known as equal height binning.\n\nEqual Width: With this option, you must specify the total number of bins. The values from the data column are placed in the bins such that each bin has the same interval between starting and ending values. As a result, some bins might have more values if data is clumped around a certain point.\n\nCustom Edges: You can specify the values that begin each bin. The edge value is always the lower boundary of the bin.\n\nFor example, assume you want to group values into two bins. One will have values greater than 0, and one will have values less than or equal to 0. In this case, for bin edges, you enter 0 in Comma-separated list of bin edges. The output of the module will be 1 and 2, indicating the bin index for each row value. Note that the comma-separated value list must be in an ascending order, such as 1, 3, 5, 7.\n\nNote\n\nEntropy MDL mode is defined in Studio (classic) and there's no corresponding open source package which can be leveraged to support in Designer yet.\n\nIf you're using the Quantiles and Equal Width binning modes, use the Number of bins option to specify how many bins, or quantiles, you want to create.\n\n\n\n\nFor Columns to bin, use the column selector to choose the columns that have the values you want to bin. Columns must be a numeric data type.\n\nThe same binning rule is applied to all applicable columns that you choose. If you need to bin some columns by using a different method, use a separate instance of the Group Data into Bins module for each set of columns.\n\nWarning\n\nIf you choose a column that's not an allowed type, a runtime error is generated. The module returns an error as soon as it finds any column of a disallowed type. If you get an error, review all selected columns. The error does not list all invalid columns.\n\nFor Output mode, indicate how you want to output the quantized values:\n\nAppend: Creates a new column with the binned values, and appends that to the input table.\n\nInplace: Replaces the original values with the new values in the dataset.\n\nResultOnly: Returns just the result columns.\n\nIf you select the Quantiles binning mode, use the Quantile normalization option to determine how values are normalized before sorting into quantiles. Note that normalizing values transforms the values but doesn't affect the final number of bins.\n\n\n\n\n\nThe following normalization types are supported:\n\nPercent: Values are normalized within the range [0,100].\n\nPQuantile: Values are normalized within the range [0,1].\n\nQuantileIndex: Values are normalized within the range [1,number of bins].\n\nIf you choose the Custom Edges option, enter a comma-separated list of numbers to use as bin edges in the Comma-separated list of bin edges text box.\n\n\n\n\nThe values mark the point that divides bins. For example, if you enter one bin edge value, two bins will be generated. If you enter two bin edge values, three bins will be generated.\n\nThe values must be sorted in the order that the bins are created, from lowest to highest.\n\nSelect the Tag columns as categorical option to indicate that the quantized columns should be handled as categorical variables.\n\n\n\n\nSubmit the pipeline.\n\nRegards,\nYutong",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":7.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"group categor valu design hello possibl studio design abl studio need group categor valu studio design option group data bin try chose custom edg work data column categor edit rate like creat bin categor",
        "Question_preprocessed_content":"group categor valu design hello possibl studio design abl studio need group categor valu studio design option group data bin try chose custom edg work data column categor edit rate like creat bin categor",
        "Answer_original_content":"hello sorri heard design offici support group data bin custom edg highlight essenti add group data bin modul pipelin design modul categori data transform connect dataset numer data bin quantiz appli column contain numer data dataset contain non numer column us select column dataset modul select subset column work specifi bin mode bin mode determin paramet sure select bin mode option follow type bin support quantil quantil method assign valu bin base percentil rank method known equal height bin equal width option specifi total number bin valu data column place bin bin interv start end valu result bin valu data clump certain point custom edg specifi valu begin bin edg valu lower boundari bin exampl assum want group valu bin valu greater valu equal case bin edg enter comma separ list bin edg output modul indic bin index row valu note comma separ valu list ascend order note entropi mdl mode defin studio classic correspond open sourc packag leverag support design quantil equal width bin mode us number bin option specifi bin quantil want creat column bin us column selector choos column valu want bin column numer data type bin rule appli applic column choos need bin column differ method us separ instanc group data bin modul set column warn choos column allow type runtim error gener modul return error soon find column disallow type error review select column error list invalid column output mode indic want output quantiz valu append creat new column bin valu append input tabl inplac replac origin valu new valu dataset resultonli return result column select quantil bin mode us quantil normal option determin valu normal sort quantil note normal valu transform valu affect final number bin follow normal type support percent valu normal rang pquantil valu normal rang quantileindex valu normal rang number bin choos custom edg option enter comma separ list number us bin edg comma separ list bin edg text box valu mark point divid bin exampl enter bin edg valu bin gener enter bin edg valu bin gener valu sort order bin creat lowest highest select tag column categor option indic quantiz column handl categor variabl submit pipelin regard yutong",
        "Answer_preprocessed_content":"hello sorri heard design offici support group data bin custom edg highlight essenti add group data bin modul pipelin design modul categori data transform connect dataset numer data bin quantiz appli column contain numer data dataset contain column us select column dataset modul select subset column work specifi bin mode bin mode determin paramet sure select bin mode option follow type bin support quantil quantil method assign valu bin base percentil rank method known equal height bin equal width option specifi total number bin valu data column place bin bin interv start end valu result bin valu data clump certain point custom edg specifi valu begin bin edg valu lower boundari bin exampl assum want group valu bin valu greater valu equal case bin edg enter list bin edg output modul indic bin index row valu note valu list ascend order note entropi mdl mode defin studio correspond open sourc packag leverag support design quantil equal width bin mode us number bin option specifi bin quantil want creat column bin us column selector choos column valu want bin column numer data type bin rule appli applic column choos need bin column differ method us separ instanc group data bin modul set column warn choos column allow type runtim error gener modul return error soon find column disallow type error review select column error list invalid column output mode indic want output quantiz valu append creat new column bin valu append input tabl inplac replac origin valu new valu dataset resultonli return result column select quantil bin mode us quantil normal option determin valu normal sort quantil note normal valu transform valu affect final number bin follow normal type support percent valu normal rang pquantil valu normal rang quantileindex valu normal rang choos custom edg option enter list number us bin edg list bin edg text box valu mark point divid bin exampl enter bin edg valu bin gener enter bin edg valu bin gener valu sort order bin creat lowest highest select tag column categor option indic quantiz column handl categor variabl submit pipelin regard yutong",
        "Question_gpt_summary_original":"the user is encountering difficulty in attempting to group categorical values in studio designer, as the option to group data into bins does not seem to work with data columns that are categorical.",
        "Question_gpt_summary":"user encount difficulti attempt group categor valu studio design option group data bin work data column categor",
        "Answer_gpt_summary_original":"the answer provides possible solutions to the user's difficulty in grouping categorical values in studio designer. the solution involves adding the \"group data into bins\" module to the pipeline, selecting the appropriate binning mode (quantiles, equal width, or custom edges), specifying the number of bins, selecting the columns to bin, and choosing the output mode. the answer also warns about selecting columns that are not of the allowed type and provides options for normalizing values before sorting into quantiles.",
        "Answer_gpt_summary":"answer provid possibl solut user difficulti group categor valu studio design solut involv ad group data bin modul pipelin select appropri bin mode quantil equal width custom edg specifi number bin select column bin choos output mode answer warn select column allow type provid option normal valu sort quantil"
    },
    {
        "Question_id":null,
        "Question_title":"how to access\/set up a model registry ?",
        "Question_body":"based on aws docs\/examples (https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-registry-version.html), one can create\/register model that is generated by your training pipeline. first we need to create a model package group ( sample code below). is model registry already set up by default in our account or do we have to create it as well? can you have more than one model registry in an account. I don't see , in the code below, where we are passing the model registry info. I am assuming , we simply create multiple training jobs and save the models in its own model group. is there a sample code\/repo, where there are multiple training steps that calls same model registry with a different group name ( i can use it as a sample). also, once the model is registered, how do i set up such that, a model can be accepted or declined, before promoting. can we set this up via code , please point me to any examples.\n\nimport time\ngroup_name = \"mygroup\"\ninput_dict = { \"ModelPackageGroupName\" : group_name}\n\nresponse =  client.create_model_package_group(**input_dict)\nprint('result :  {}'.format(response['ModelPackageGroupArn']))",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1668477799381,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":25.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUW-r2Vv8-RC-qtY34c3WobQ\/how-to-access-set-up-a-model-registry",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-11-15T12:54:41.012Z",
                "Answer_score":0,
                "Answer_body":"Hello,\n\nModel package group and model registry are 2 different things. Model registry is already in your account but you have to create a model package group where you will have model packages (different versions of the model). see this notebook about using model registry. Your code above is basically creating a model group\/model package group in model registry, and in that model group you will have the different versions of your model.\n\nI don't have an example in mind, but you can easily have a code where you create different models and add them in the model groups accordingly. Following the notebook shared above , you would just add the same code but adapt it to another model group assuming that you have to two trainings happening each with a different purpose.\n\nTo summarize: model registry = N model groups and each model group = N model versions (model packages). Suppose you are doing a sentiment analysis task where you have different models per customer. You could create a Model Package Group per customer and in each group register Model Package Versions for each model created.\n\nYou can automate the testing of your model before registering into the model group by for example checking the loss metric. You can use SageMaker projects for an example that includes a pipeline. Check this example. You could basically have a conditional step in your pipeline.\n\nHope this helps and if I answered your question, please accept it.\n\nThank you",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"access set model registri base aw doc exampl http doc aw amazon com latest model registri version html creat regist model gener train pipelin need creat model packag group sampl code model registri set default account creat model registri account code pass model registri info assum simpli creat multipl train job save model model group sampl code repo multipl train step call model registri differ group us sampl model regist set model accept declin promot set code point exampl import time group mygroup input dict modelpackagegroupnam group respons client creat model packag group input dict print result format respons modelpackagegrouparn",
        "Question_preprocessed_content":"model registri base aw model gener train pipelin need creat model packag group model registri set default account creat model registri account code pass model registri info assum simpli creat multipl train job save model model group sampl multipl train step call model registri differ group model regist set model accept declin promot set code point exampl import time mygroup respons print",
        "Answer_original_content":"hello model packag group model registri differ thing model registri account creat model packag group model packag differ version model notebook model registri code basic creat model group model packag group model registri model group differ version model exampl mind easili code creat differ model add model group accordingli follow notebook share add code adapt model group assum train happen differ purpos summar model registri model group model group model version model packag suppos sentiment analysi task differ model custom creat model packag group custom group regist model packag version model creat autom test model regist model group exampl check loss metric us project exampl includ pipelin check exampl basic condit step pipelin hope help answer question accept thank",
        "Answer_preprocessed_content":"hello model packag group model registri differ thing model registri account creat model packag group model packag notebook model registri code basic creat model packag group model registri model group differ version model exampl mind easili code creat differ model add model group accordingli follow notebook share add code adapt model group assum train happen differ purpos summar model registri model group model group model version suppos sentiment analysi task differ model custom creat model packag group custom group regist model packag version model creat autom test model regist model group exampl check loss metric us project exampl includ pipelin check exampl basic condit step pipelin hope help answer question accept thank",
        "Question_gpt_summary_original":"the user is encountering challenges in setting up a model registry on aws, including creating a model package group, determining if a model registry is already set up, setting up a system to accept or decline models before promoting them, and finding sample code for multiple training steps that call the same model registry.",
        "Question_gpt_summary":"user encount challeng set model registri aw includ creat model packag group determin model registri set set accept declin model promot find sampl code multipl train step model registri",
        "Answer_gpt_summary_original":"possible solutions from the answer include: creating a model package group, understanding the difference between model package group and model registry, finding sample code for multiple training steps that call the same model registry, automating the testing of models before registering them into the model group, and using projects for an example that includes a pipeline. the answer also provides an example of creating a model package group per customer and registering model package versions for each model created.",
        "Answer_gpt_summary":"possibl solut answer includ creat model packag group understand differ model packag group model registri find sampl code multipl train step model registri autom test model regist model group project exampl includ pipelin answer provid exampl creat model packag group custom regist model packag version model creat"
    },
    {
        "Question_id":null,
        "Question_title":"Issue with data lake mounting in custom RStudio application Azure ML",
        "Question_body":"previously while creating a compute instance we were able to see RStudio application by default and we were able to mount\/access the data lake from RStudio.\n![compute creation][1]\n\n\n\n\n![Data lake mont][2]\n2. In current situation we are not able to access RStudio application by default.\n![234345-4.png][3]\n3.with the help of below link we are able to create custom RStudio application\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-manage-compute-instance?tabs=azure-studio\n![custom RStudio app][4]\n\n4.In custom RStudio we are not able to mount\/access the data lake.\n\n![missing data lake][5]\n\nIs there way to mount\/access the data lake in custom RStudio app\n[1]: \/answers\/storage\/attachments\/234344-screenshot-2022-08-23-170502.png\n[2]: \/answers\/storage\/attachments\/234353-5.png\n[3]: \/answers\/storage\/attachments\/234345-4.png\n[4]: \/answers\/storage\/attachments\/234314-2.png\n[5]: \/answers\/storage\/attachments\/234361-3.png",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1661155798650,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/976098\/install-rstudio-application-in-azure-ml-vm.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-23T02:41:16.113Z",
                "Answer_score":0,
                "Answer_body":"@alifshaikh-6049 Thanks for the question. Here is the snapshot for using the Rstudio from ml.azure.com\n\n\nHere is the sample to train Rmodel.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":10.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"issu data lake mount custom rstudio applic previous creat comput instanc abl rstudio applic default abl mount access data lake rstudio comput creation data lake mont current situat abl access rstudio applic default png help link abl creat custom rstudio applic http doc microsoft com azur machin learn creat manag comput instanc tab azur studio custom rstudio app custom rstudio abl mount access data lake miss data lake wai mount access data lake custom rstudio app answer storag attach screenshot png answer storag attach png answer storag attach png answer storag attach png answer storag attach png",
        "Question_preprocessed_content":"issu data lake mount custom rstudio applic previous creat comput instanc abl rstudio applic default abl data lake rstudio current situat abl access rstudio applic default help link abl creat custom rstudio applic custom rstudio abl data lake wai data lake custom rstudio app",
        "Answer_original_content":"alifshaikh thank question snapshot rstudio azur com sampl train rmodel",
        "Answer_preprocessed_content":"thank question snapshot rstudio sampl train rmodel",
        "Question_gpt_summary_original":"the user is encountering challenges with mounting\/accessing the data lake in a custom rstudio application.",
        "Question_gpt_summary":"user encount challeng mount access data lake custom rstudio applic",
        "Answer_gpt_summary_original":"the answer does not provide any solutions to the user's challenge with mounting\/accessing the data lake in a custom rstudio application. instead, it provides a snapshot for using rstudio from ml.azure.com and a sample to train an r model.",
        "Answer_gpt_summary":"answer provid solut user challeng mount access data lake custom rstudio applic instead provid snapshot rstudio azur com sampl train model"
    },
    {
        "Question_id":null,
        "Question_title":"How to move DVC files",
        "Question_body":"<p>Search for this topic, I found <a href=\"https:\/\/discuss.dvc.org\/t\/dvc-heartbeat-discord-gems\/159\/2\">this summary post<\/a> but I cannot open the original question (discord tells me it cannot find it) so I\u2019m creating a new topic. Apologies in advance.<\/p>\n<p>I tried following the instructions given i.e. I ran<\/p>\n<pre><code class=\"lang-auto\">mv my_file.dvc dvc_links\/my_file.dvc\n<\/code><\/pre>\n<p>and edited the working directory in the moved file.<br>\nHowever running <code>dvc status<\/code> gives<\/p>\n<pre><code class=\"lang-auto\">dvc_links\/my_file.dvc:\n        changed checksum\n<\/code><\/pre>\n<p>How do I get dvc to update the checksum in the relocated file? I even tried removing the md5: xx line at the top of the new file, calculating the md5 sum and reinserting it but this didn\u2019t work.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1566355790691,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":1238.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/how-to-move-dvc-files\/205",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2019-08-21T07:58:31.486Z",
                "Answer_body":"<p>Try these steps:<\/p>\n<pre><code class=\"lang-bash\">rm my_file.dvc\ndvc add -f dvc_links\/my_file.dvc my_file\n<\/code><\/pre>\n<p>I think this is not documented and it is not obvious at all. I found it the hard way (by trial and error).<\/p>",
                "Answer_score":142.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T08:09:10.699Z",
                "Answer_body":"<p>Thank you. This worked<\/p>",
                "Answer_score":27.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2019-08-21T18:06:26.870Z",
                "Answer_body":"<p><a class=\"mention\" href=\"\/u\/dashohoxha\">@dashohoxha<\/a> Thank you for your answer! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p><a class=\"mention\" href=\"\/u\/jcf76\">@jcf76<\/a> This is a bug in <code>dvc move<\/code> a patch for which was merged today <a href=\"https:\/\/github.com\/iterative\/dvc\/pull\/2421\" rel=\"nofollow noopener\">https:\/\/github.com\/iterative\/dvc\/pull\/2421<\/a> and will soon be released in a new dvc version. Thank you for the feedback! <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/slight_smile.png?v=9\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":7.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"file search topic summari post open origin question discord tell creat new topic apolog advanc tri follow instruct given ran file link file edit work directori move file run statu give link file chang checksum updat checksum reloc file tri remov line new file calcul sum reinsert didnt work",
        "Question_preprocessed_content":"file search topic summari post open origin question creat new topic apolog advanc tri follow instruct given ran edit work directori move file run give updat checksum reloc file tri remov line new file calcul sum reinsert didnt work",
        "Answer_original_content":"try step file add link file file think document obviou hard wai trial error thank work dashohoxha thank answer jcf bug patch merg todai http github com iter pull soon releas new version thank feedback",
        "Answer_preprocessed_content":"try step think document obviou hard wai thank work thank answer bug patch merg todai soon releas new version thank feedback",
        "Question_gpt_summary_original":"the user is encountering challenges with moving files and updating the checksum in the relocated file.",
        "Question_gpt_summary":"user encount challeng move file updat checksum reloc file",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n1. remove the file and add it again with the updated checksum.\n2. use the command \"add -f _links\/my_file. my_file\" to add the file with the updated checksum.\n3. check for any bug in the move patch and wait for the new version to be released.\n\noverall, the answer provides some steps to try and a possible bug fix for the issue.",
        "Answer_gpt_summary":"possibl solut answer remov file add updat checksum us command add link file file add file updat checksum check bug patch wait new version releas overal answer provid step try possibl bug fix issu"
    },
    {
        "Question_id":68237132.0,
        "Question_title":"No module named 'Cython' setting up Azure ML docker instance",
        "Question_body":"<p>I'm trying to install the following library in my Azure ML instance:<\/p>\n<p><a href=\"https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI\" rel=\"nofollow noreferrer\">https:\/\/github.com\/philferriere\/cocoapi#egg=pycocotools&amp;subdirectory=PythonAPI<\/a><\/p>\n<p>My Dockerfile looks like this:<\/p>\n<pre><code>FROM mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04:20210615.v1\n\nENV AZUREML_CONDA_ENVIRONMENT_PATH \/azureml-envs\/pytorch-1.7\n\n# Create conda environment\nRUN conda create -p $AZUREML_CONDA_ENVIRONMENT_PATH \\\n    python=3.7 \\\n    pip=20.2.4 \\\n    pytorch=1.7.1 \\\n    torchvision=0.8.2 \\\n    torchaudio=0.7.2 \\\n    cudatoolkit=11.0 \\\n    nvidia-apex=0.1.0 \\\n    -c anaconda -c pytorch -c conda-forge\n\n# Prepend path to AzureML conda environment\nENV PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/bin:$PATH\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                'psutil&gt;=5.8,&lt;5.9' \\\n                'tqdm&gt;=4.59,&lt;4.60' \\\n                'pandas&gt;=1.1,&lt;1.2' \\\n                'scipy&gt;=1.5,&lt;1.6' \\\n                'numpy&gt;=1.10,&lt;1.20' \\\n                'azureml-core==1.31.0' \\\n                'azureml-defaults==1.31.0' \\\n                'azureml-mlflow==1.31.0' \\\n                'azureml-telemetry==1.31.0' \\\n                'tensorboard==2.4.0' \\\n                'tensorflow-gpu==2.4.1' \\\n                'onnxruntime-gpu&gt;=1.7,&lt;1.8' \\\n                'horovod[pytorch]==0.21.3' \\\n                'future==0.17.1' \\\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n\n# This is needed for mpi to locate libpython\nENV LD_LIBRARY_PATH $AZUREML_CONDA_ENVIRONMENT_PATH\/lib:$LD_LIBRARY_PATH\n<\/code><\/pre>\n<p>An error is thrown when the library is being installed:<\/p>\n<pre><code>  Cloning https:\/\/github.com\/philferriere\/cocoapi.git to \/tmp\/pip-install-_i3sjryy\/pycocotools\n[91m    ERROR: Command errored out with exit status 1:\n     command: \/azureml-envs\/pytorch-1.7\/bin\/python -c 'import sys, setuptools, tokenize; sys.argv[0] = '&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;'; __file__='&quot;'&quot;'\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py'&quot;'&quot;';f=getattr(tokenize, '&quot;'&quot;'open'&quot;'&quot;', open)(__file__);code=f.read().replace('&quot;'&quot;'\\r\\n'&quot;'&quot;', '&quot;'&quot;'\\n'&quot;'&quot;');f.close();exec(compile(code, __file__, '&quot;'&quot;'exec'&quot;'&quot;'))' egg_info --egg-base \/tmp\/pip-pip-egg-info-o68by1_q\n         cwd: \/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\n    Complete output (5 lines):\n    Traceback (most recent call last):\n      File &quot;&lt;string&gt;&quot;, line 1, in &lt;module&gt;\n      File &quot;\/tmp\/pip-install-_i3sjryy\/pycocotools\/PythonAPI\/setup.py&quot;, line 2, in &lt;module&gt;\n        from Cython.Build import cythonize\n    ModuleNotFoundError: No module named 'Cython'\n<\/code><\/pre>\n<p>I've tried adding Cython as a dependecy in both the pip section and as part of the conda environment but the error is still thrown.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1625322434170,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":209.0,
        "Answer_body":"<p>Solution was to add the following to the Dockerfile:<\/p>\n<pre><code># Install Cython\nRUN pip3 install Cython\n\n# Install pip dependencies\nRUN HOROVOD_WITH_PYTORCH=1 \\\n    pip install 'matplotlib&gt;=3.3,&lt;3.4' \\\n                ...\n                'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&amp;subdirectory=PythonAPI'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68237132",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1625324240192,
        "Question_original_content":"modul name cython set docker instanc try instal follow librari instanc http github com philferrier cocoapi egg pycocotool subdirectori pythonapi dockerfil look like mcr microsoft com openmpi cuda cudnn ubuntu env conda environ path env pytorch creat conda environ run conda creat conda environ path python pip pytorch torchvis torchaudio cudatoolkit nvidia apex anaconda pytorch conda forg prepend path conda environ env path conda environ path bin path instal pip depend run horovod pytorch pip instal matplotlib line file tmp pip instal isjryi pycocotool pythonapi setup line cython build import cython modulenotfounderror modul name cython tri ad cython dependeci pip section conda environ error thrown",
        "Question_preprocessed_content":"modul name cython set docker instanc try instal follow librari instanc dockerfil look like error thrown librari instal tri ad cython dependeci pip section conda environ error thrown",
        "Answer_original_content":"solut add follow dockerfil instal cython run pip instal cython instal pip depend run horovod pytorch pip instal matplotlib git http github com philferrier cocoapi git egg pycocotool subdirectori pythonapi",
        "Answer_preprocessed_content":"solut add follow dockerfil",
        "Question_gpt_summary_original":"the user is encountering a challenge setting up a docker instance, as they are receiving a \"modulenotfounderror: no module named 'cython'\" error when trying to install a library.",
        "Question_gpt_summary":"user encount challeng set docker instanc receiv modulenotfounderror modul name cython error try instal librari",
        "Answer_gpt_summary_original":"the solution to the \"modulenotfounderror: no module named 'cython'\" error when setting up a docker instance is to add the following lines to the dockerfile: \n\n- install cython: run pip3 install cython \n- install pip dependencies: run horovod_with_pytorch=1 \\ pip install 'matplotlib>=3.3,<3.4' \\ ... 'git+https:\/\/github.com\/philferriere\/cocoapi.git#egg=pycocotools&subdirectory=pythonapi'",
        "Answer_gpt_summary":"solut modulenotfounderror modul name cython error set docker instanc add follow line dockerfil instal cython run pip instal cython instal pip depend run horovod pytorch pip instal matplotlib git http github com philferrier cocoapi git egg pycocotool subdirectori pythonapi"
    },
    {
        "Question_id":44868932.0,
        "Question_title":"How to import the pickle file if sacred failed to connect to MongoDB",
        "Question_body":"<p>The experiment software <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/quickstart.html\" rel=\"nofollow noreferrer\">sacred<\/a> was run without MongoDB in the background with a configured <a href=\"http:\/\/sacred.readthedocs.io\/en\/latest\/observers.html#mongo-observer\" rel=\"nofollow noreferrer\">mongo-observer<\/a>. When it tried to write the settings to MongoDB, this failed, creating the file <code>\/tmp\/sacred_mongo_fail__eErwU.pickle<\/code>, with the message<\/p>\n\n<pre><code>Warning: saving to MongoDB failed! Stored experiment entry in \/tmp\/sacred_mongo_fail__eErwU.pickle\nTraceback (most recent calls WITHOUT Sacred internals):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 127, in started_event\n    self.run_entry[experiment][sources] = self.save_sources(ex_info)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/sacred\/observers\/mongo.py\", line 239, in save_sources\n    file = self.fs.find_one({filename: abs_path, md5: md5})\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/__init__.py\", line 261, in find_one\n    for f in self.find(filter, *args, **kwargs):\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/gridfs\/grid_file.py\", line 658, in next\n    next_file = super(GridOutCursor, self).next()\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1114, in next\n    if len(self.__data) or self._refresh():\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 1036, in _refresh\n    self.__collation))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/cursor.py\", line 873, in __send_message\n    **kwargs)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/mongo_client.py\", line 888, in _send_message_with_response\n    server = topology.select_server(selector)\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 214, in select_server\n    address))\n  File \"\/usr\/local\/lib\/python2.7\/dist-packages\/pymongo\/topology.py\", line 189, in select_servers\n    self._error_message(selector))\nServerSelectionTimeoutError: localhost:27017: [Errno 111] Connection refused\n<\/code><\/pre>\n\n<p>How can this pickle file be imported into MongoDB manually?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1498984762217,
        "Question_favorite_count":null,
        "Question_last_edit_time":1498985131100,
        "Question_score":1.0,
        "Question_view_count":537.0,
        "Answer_body":"<ol>\n<li>Load the pickle file, <\/li>\n<li>set the <code>_id<\/code>,<\/li>\n<li>insert <\/li>\n<\/ol>\n\n<p><\/p>\n\n<pre><code>db = pymongo.MongoClient().sacred\nentry = pickle.load(open('\/tmp\/sacred_mongo_fail__eErwU.pickle'))\nentry['_id'] = list(db.runs.find({}, {\"_id\": 1}))[-1]['_id']\ndb.runs.insert_one(entry)\n<\/code><\/pre>\n\n<p>This is quick and dirty, depends on the <code>find<\/code> to list objects in order, and could use <a href=\"https:\/\/stackoverflow.com\/questions\/2138873\/cleanest-way-to-get-last-item-from-python-iterator\">Cleanest way to get last item from Python iterator<\/a> instead of <code>list(...)[-1]<\/code>, but it should work.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1510652289132,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44868932",
        "Tool":"Sacred",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1510651584203,
        "Question_original_content":"import pickl file fail connect mongodb experi softwar run mongodb background configur mongo observ tri write set mongodb fail creat file tmp mongo fail eerwu pickl messag warn save mongodb fail store experi entri tmp mongo fail eerwu pickl traceback recent call intern file usr local lib python dist packag observ mongo line start event self run entri experi sourc self save sourc info file usr local lib python dist packag observ mongo line save sourc file self filenam ab path file usr local lib python dist packag gridf init line self filter arg kwarg file usr local lib python dist packag gridf grid file line file super gridoutcursor self file usr local lib python dist packag pymongo cursor line len self data self refresh file usr local lib python dist packag pymongo cursor line refresh self collat file usr local lib python dist packag pymongo cursor line send messag kwarg file usr local lib python dist packag pymongo mongo client line send messag respons server topolog select server selector file usr local lib python dist packag pymongo topolog line select server address file usr local lib python dist packag pymongo topolog line select server self error messag selector serverselectiontimeouterror localhost errno connect refus pickl file import mongodb manual",
        "Question_preprocessed_content":"import pickl file fail connect mongodb experi softwar run mongodb background configur tri write set mongodb fail creat file messag pickl file import mongodb manual",
        "Answer_original_content":"load pickl file set insert pymongo mongocli entri pickl load open tmp mongo fail eerwu pickl entri list run run insert entri quick dirti depend list object order us cleanest wai item python iter instead list work",
        "Answer_preprocessed_content":"load pickl file set insert quick dirti depend list object order us cleanest wai item python iter instead work",
        "Question_gpt_summary_original":"the user encountered a challenge of importing a pickle file into mongodb manually after failing to connect to mongodb while running an experiment software.",
        "Question_gpt_summary":"user encount challeng import pickl file mongodb manual fail connect mongodb run experi softwar",
        "Answer_gpt_summary_original":"the solution to importing a pickle file into mongodb manually after failing to connect to mongodb while running an experiment software is to load the pickle file, set the _id, and insert it into the mongodb using pymongo.mongoclient(). the solution involves using a quick and dirty method that depends on the find to list objects in order and could use a cleaner way to get the last item from a python iterator instead of list(...)[-1].",
        "Answer_gpt_summary":"solut import pickl file mongodb manual fail connect mongodb run experi softwar load pickl file set insert mongodb pymongo mongocli solut involv quick dirti method depend list object order us cleaner wai item python iter instead list"
    },
    {
        "Question_id":null,
        "Question_title":"Permission denied error when overwritting files to DVC tracked directory",
        "Question_body":"<p>Hi there,<br>\nI\u2019m still figuring DVC out so sorry if this is an obvious question. I feel like I\u2019m missing something.<\/p>\n<p>I have a project where some directories are tracked with DVC.<br>\nWhen someone reruns a script that overwrites existing files inside a DVC tracked directory we get \u201cPermission denied error\u201d. If we try to manually modify the same file with the vim editor we get that the file is flagged as readonly but we can force writting the file and it gets modified. But when we check file permissions on the file they all seem to be set properly. If we delete all files and rerun the script regenerating the files everything works fine again.<\/p>\n<p>Is this how DVC is supposed to work? Shouldn\u2019t we be able to overwrite files and then add the changes to the DVC tracked directory and then push them?<\/p>\n<p>Many thanks in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1676391679082,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":21.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/permission-denied-error-when-overwritting-files-to-dvc-tracked-directory\/1518",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2023-02-14T16:45:36.508Z",
                "Answer_body":"<p>What platform are you running on (windows\/linux\/mac) and what dvc command in the script is giving you the permission error? It would also help if you could run <code>dvc doctor<\/code> in the repo where you get the errors and post the output here.<\/p>\n<p>One possibility I can think of is that if you are using the <code>hardlink<\/code> or <code>symlink<\/code> cache link type setting, you need to be using <code>dvc unprotect<\/code> before you try and edit or write to the dvc-tracked files: <a href=\"https:\/\/dvc.org\/doc\/command-reference\/unprotect\" class=\"inline-onebox\">unprotect<\/a><\/p>",
                "Answer_score":21.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2023-02-16T15:35:02.161Z",
                "Answer_body":"<p>Many thanks <a class=\"mention\" href=\"\/u\/pmrowla\">@pmrowla<\/a>. I am indeed using symlinks. I\u2019ll check that. Thanks!<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"permiss deni error overwrit file track directori figur sorri obviou question feel like miss project directori track rerun script overwrit exist file insid track directori permiss deni error try manual modifi file vim editor file flag readonli forc writ file get modifi check file permiss file set properli delet file rerun script regener file work fine suppos work shouldnt abl overwrit file add chang track directori push thank advanc",
        "Question_preprocessed_content":"permiss deni error overwrit file track directori figur sorri obviou question feel like miss project directori track rerun script overwrit exist file insid track directori permiss deni error try manual modifi file vim editor file flag readonli forc writ file get modifi check file permiss file set properli delet file rerun script regener file work fine suppos work shouldnt abl overwrit file add chang track directori push thank advanc",
        "Answer_original_content":"platform run window linux mac command script give permiss error help run doctor repo error post output possibl think hardlink symlink cach link type set need unprotect try edit write track file unprotect thank pmrowla symlink ill check thank",
        "Answer_preprocessed_content":"platform run command script give permiss error help run repo error post output possibl think cach link type set need try edit write track file unprotect thank symlink ill check thank",
        "Question_gpt_summary_original":"the user is encountering a permission denied error when attempting to overwrite files in a tracked directory, despite the file permissions appearing to be set properly.",
        "Question_gpt_summary":"user encount permiss deni error attempt overwrit file track directori despit file permiss appear set properli",
        "Answer_gpt_summary_original":"possible solutions to the permission denied error when attempting to overwrite files in a tracked directory are to check the platform being used (windows\/linux\/mac), identify the command in the script that is giving the error, and run doctor in the repository where the error occurs and post the output. another solution is to use the \"unprotect\" command before editing or writing to the tracked files if using the hardlink or symlink cache link type setting.",
        "Answer_gpt_summary":"possibl solut permiss deni error attempt overwrit file track directori check platform window linux mac identifi command script give error run doctor repositori error occur post output solut us unprotect command edit write track file hardlink symlink cach link type set"
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to override a Table",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m using a Table as it\u2019s the simplest to log text (Input output expected).<\/p>\n<p>However, I would like to keep only the last logged item in the Table, else I have thousand of rows or hundred of tables if I create a new table instead.<\/p>\n<p>Is there any fix for that ?<\/p>\n<p>Thanks in advance,<br>\nHave a great day <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=11\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_creation_time":1642451101000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":275.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-override-a-table\/1776",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-01-18T20:38:13.605Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ierezell\">@ierezell<\/a>,<\/p>\n<p>I think the simplest way about this would be to create a new table, for the last logged item, log it, and then delete it using python\u2019s <code>del<\/code> command.<\/p>\n<p>This way, you get to store the last row as desired, but deleting the table also means that you don\u2019t use up all your memory in the process.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":6.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-18T20:56:56.616Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a>,<\/p>\n<p>Thanks a lot for the response,<\/p>\n<p>The problem is that I will then have many tables as artifacts in weight and biases\u2026<br>\nI guess it\u2019s the best solution as it will only show the latest table in the report\u2026<\/p>\n<p>I will try that <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=12\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>\n<p>Have a great day<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-19T20:56:58.381Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":0.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"possibl overrid tabl tabl simplest log text input output expect like log item tabl thousand row tabl creat new tabl instead fix thank advanc great dai",
        "Question_preprocessed_content":"possibl overrid tabl tabl simplest log text like log item tabl thousand row tabl creat new tabl instead fix thank advanc great dai",
        "Answer_original_content":"ierezel think simplest wai creat new tabl log item log delet python del command wai store row desir delet tabl mean dont us memori process thank ramit ramit goolri thank lot respons problem tabl artifact weight bias guess best solut latest tabl report try great dai topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"think simplest wai creat new tabl log item log delet python command wai store row desir delet tabl mean dont us memori process thank ramit thank lot respons problem tabl artifact weight bias guess best solut latest tabl report try great dai topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is looking for a way to override a table in order to keep only the last logged item in the table.",
        "Question_gpt_summary":"user look wai overrid tabl order log item tabl",
        "Answer_gpt_summary_original":"the possible solution suggested in the answer is to create a new table for the last logged item, log it, and then delete it using python's del command. this way, the user can store the last row as desired and avoid using up all their memory. however, the user notes that this solution may result in many tables as artifacts in weight and biases.",
        "Answer_gpt_summary":"possibl solut suggest answer creat new tabl log item log delet python del command wai user store row desir avoid memori user note solut result tabl artifact weight bias"
    },
    {
        "Question_id":null,
        "Question_title":"Is it possible to change the scoring URI in ML Service deployment",
        "Question_body":"Hi,\n\nI'm trying to deploy a model as an endpoint in azure ML Studio. model.deploy call returns service which has a scoring_uri. I'm wondering if it is possible to change the scoring_uri from \/score to something else more appropriate. Or potentially register multiple of the uri's under the same FQDN like \/score, \/test, \/retrain under same principal guid service name.\n\ninference_config = InferenceConfig(entry_script=ENTRY_SCRIPT, environment=keras_env)\naci_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n\nservice = Model.deploy(workspace=ws,\nname=SERVICE_NAME,\nmodels=[model],\ninference_config=inference_config,\ndeployment_config=aci_config,\noverwrite=True)\nservice.wait_for_deployment(show_output=True)\n\nuri = service.scoring_uri\n\nhttp:\/\/XXXXXXXX-XXXX-4000-XXXX-aa1a5e7XXXXX.westus2.azurecontainer.io\/score\n\nCan we change the last part from \/score to something else? like \/test or register multiple endpoints.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1630356655677,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/533289\/is-it-possible-to-change-the-scoring-uri-in-ml-ser.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-08-31T04:25:11.45Z",
                "Answer_score":0,
                "Answer_body":"Hi, you cannot modify the web service uri.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":9.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"possibl chang score uri servic deploy try deploi model endpoint studio model deploi return servic score uri wonder possibl chang score uri score appropri potenti regist multipl uri fqdn like score test retrain princip guid servic infer config inferenceconfig entri script entri script environ kera env aci config aciwebservic deploi configur cpu core memori servic model deploi workspac servic model model infer config infer config deploy config aci config overwrit true servic wait deploy output true uri servic score uri http aaaexx westu azurecontain score chang score like test regist multipl endpoint",
        "Question_preprocessed_content":"possibl chang score uri servic deploy try deploi model endpoint studio return servic wonder possibl chang appropri potenti regist multipl uri fqdn like princip guid servic servic model model overwrit true uri chang like regist multipl endpoint",
        "Answer_original_content":"modifi web servic uri",
        "Answer_preprocessed_content":"modifi web servic uri",
        "Question_gpt_summary_original":"the user is trying to deploy a model as an endpoint in studio and is wondering if it is possible to change the scoring_uri from \/score to something else more appropriate or potentially register multiple of the uri's under the same fqdn.",
        "Question_gpt_summary":"user try deploi model endpoint studio wonder possibl chang score uri score appropri potenti regist multipl uri fqdn",
        "Answer_gpt_summary_original":"there are no solutions to changing the scoring_uri from \/score or registering multiple uris under the same fqdn.",
        "Answer_gpt_summary":"solut chang score uri score regist multipl uri fqdn"
    },
    {
        "Question_id":null,
        "Question_title":"Can't create a VM in compute - Creation failed",
        "Question_body":"Hi\nI am unable to create a VM in Compute. Status is at Creating for an hour and then it fails.\nI tried several times without luck.\n\n\n\n\n\n\n\n\n\nDoes anyone know how to solve this?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1615983139490,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"I was able to delete all the failed VMs and create one today.\nThe solution in this case was to wait it out.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/318685\/can39t-create-a-vm-in-compute-creation-failed.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-03-18T07:31:13.92Z",
                "Answer_score":1,
                "Answer_body":"I was able to delete all the failed VMs and create one today.\nThe solution in this case was to wait it out.",
                "Answer_comment_count":0,
                "Answer_has_accepted":true
            },
            {
                "Answer_creation_time":"2021-03-17T13:50:45.417Z",
                "Answer_score":0,
                "Answer_body":"@ArkanVasie-2880 Thanks for the question. There are known issues, We would recommend to raise a Azure support desk ticket from Help+Support blade from Azure portal for your service resource. This will help you to share the details securely and work with an engineer who can provide more insights about the issue.",
                "Answer_comment_count":1,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":12.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":1616052673920,
        "Question_original_content":"creat comput creation fail unabl creat comput statu creat hour fail tri time luck know solv",
        "Question_preprocessed_content":"creat comput creation fail unabl creat comput statu creat hour fail tri time luck know solv",
        "Answer_original_content":"abl delet fail vm creat todai solut case wait",
        "Answer_preprocessed_content":"abl delet fail vm creat todai solut case wait",
        "Question_gpt_summary_original":"the user is unable to create a vm in compute, with the status stuck at creating for an hour before failing.",
        "Question_gpt_summary":"user unabl creat comput statu stuck creat hour fail",
        "Answer_gpt_summary_original":"the solution to the problem of being unable to create a vm in compute with the status stuck at creating for an hour before failing is to wait it out. additionally, deleting all the failed vms and trying again may also be a solution.",
        "Answer_gpt_summary":"solut problem unabl creat comput statu stuck creat hour fail wait addition delet fail vm try solut"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker Studio will not load",
        "Question_body":"Morning of 12\/17 I loaded SageMaker Studio, and created an Autopilot experiment. It ran for 2 hours and was successful.\n\nAfterwards, I exited SageMaker Studio. Ever since that point, I have been unable to re-enter Studio. It is now 24 hours later.\n\nI either receive a response from Chrome:\n\nThis page isn\u2019t working\nd-*************.studio.us-east-2.sagemaker.aws didn\u2019t send any data.\nERR_EMPTY_RESPONSE\n\nOr, I get an error message:\nThe JupyterServer app default encoutered a problem and was stopped.\nDetails: InternalFailure\n\nI get the option to \"Restart Now,\" but it never works.",
        "Question_answer_count":8,
        "Question_comment_count":0,
        "Question_creation_time":1576685163000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":904.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUxoSA7eTzQbK-T4OWjJvSmQ\/sage-maker-studio-will-not-load",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2020-01-06T22:02:27.000Z",
                "Answer_score":0,
                "Answer_body":"Hello. I ran into a similar problem. I think SageMaker Studio, since it's in preview, is not yet a super-stable platform. To resolve this issue I just kept trying to launch the sagemaker studio from the aws console ('open amazon sagemaker studio' button from within the amazon SageMaker > Amazon SageMaker Stuiod > d-***** men). I received the same error about ten times but eventually I was able to get through to see the notebook I had previously created.\n\nHope this helps!",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-07-21T17:45:56.000Z",
                "Answer_score":0,
                "Answer_body":"I see the same issue. SM Studio was working on the evening of 1\/6\/2020. I shut down the Juypter Lab and haven't been able to access it since. I've tried accessing via the SSO dashboard, as well as an admin user via the AWS console.\n\nCHROME returns:\n\nThis page isn\u2019t working d-*********.studio.us-east-2.sagemaker.aws didn\u2019t send any data.\nERR_EMPTY_RESPONSE\n\nFIREFOX returns:\n\nSecure Connection Failed\nAn error occurred during a connection to d-*********.studio.us-east-2.sagemaker.aws.\nThe page you are trying to view cannot be shown because the authenticity of the received data could not be verified.\nPlease contact the website owners to inform them of this problem.\n\nSAFARI returns:\n\nSafari Can't Open the Page\nSafari can't open the page \"https:\/\/d-*******.studio.us-east-2.sagemaker.aws\/jupyter\/default\" because the server unexpectedly dropped the connection. This sometimes occurs when the server is busy. Wait for a few minutes, and then try again.\n\nI tried for a few hours last night and again this morning. The problem persists. Any suggestions or support with this would be greatly appreciated. Thanks.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-08T16:49:47.000Z",
                "Answer_score":0,
                "Answer_body":"Still having the same issue, trying about once a day since the original errors.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-13T11:10:40.000Z",
                "Answer_score":0,
                "Answer_body":"I have the same issue. I have shut it down last night and could not find a way to open it or restart the whole amazon sagemaker studio service.",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-04-20T02:21:05.000Z",
                "Answer_score":0,
                "Answer_body":"I was able to fix this by completely deleting my Studio instance with the AWS CKI. Of course you'll use any ephemaral data. You can then start over from scratch. Here is the sequence of commands.\n\naws sagemaker delete-app --domain-id yourDomainID --user-profile-name yourProfileName --app-type KernelGateway --app-name base-python\naws sagemaker delete-app --domain-id yourDomainID --user-profile-name yourProfileName --app-type JupyterServer --app-name default\naws sagemaker delete-user-profile --domain-id yourDomainID --user-profile-name yourProfileName\naws sagemaker delete-domain --domain-id yourDomainID",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-17T17:05:49.000Z",
                "Answer_score":0,
                "Answer_body":"I am facing a similar problem for one of my SageMaker users. The default app, which is the Jupyter server, keeps failing while trying to launch SageMaker studio. I am receiving the following message: \"The JupyterServer app default encountered a problem and was stopped.\". I click on the Restart button, but the message appears again. Does anyone know how can I solve this issue? Or do you recommend paying for AWS support?",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-10-04T23:40:41.000Z",
                "Answer_score":0,
                "Answer_body":"Had the same issue and managed to fix it (and preserve my data).\n\nIn AWS console navigate to SageMaker Studio\nIn the users section at the top, open the user (user details page) that is experiencing this issue.\nAt the bottom of the page, delete all apps for this user. There will be the default app (which is jupyter server) and sometimes others like KernelGateway. DO NOT DELETE THE ACTUAL USER PROFILE.\nWhen all apps for this user is deleted, go back to SageMaker Studio and click on the \"Open Studio\" link for that user.\nIt will take some time, but SageMaker Studio will reinitialize that Jupyter Notebook.\n\nThe notebook will open and you should have all your data as it was before.\n\nEdited by: Noobie on Oct 4, 2020 4:41 PM\n\nEdited by: Noobie on Oct 4, 2020 4:43 PM",
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2020-01-07T14:18:00.000Z",
                "Answer_score":0,
                "Answer_body":"This method works, thanks for sharing.",
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"studio load morn load studio creat autopilot experi ran hour success exit studio point unabl enter studio hour later receiv respons chrome page isnt work studio east aw didnt send data err respons error messag jupyterserv app default encout problem stop detail internalfailur option restart work",
        "Question_preprocessed_content":"studio load morn load studio creat autopilot experi ran hour success exit studio point unabl studio hour later receiv respons chrome page isnt work didnt send data error messag jupyterserv app default encout problem stop detail internalfailur option restart work",
        "Answer_original_content":"hello ran similar problem think studio preview super stabl platform resolv issu kept try launch studio aw consol open studio button stuiod men receiv error time eventu abl notebook previous creat hope help issu studio work even shut juypter lab haven abl access tri access sso dashboard admin user aw consol chrome return page isnt work studio east aw didnt send data err respons firefox return secur connect fail error occur connect studio east aw page try view shown authent receiv data verifi contact websit owner inform problem safari return safari open page safari open page http studio east aw jupyt default server unexpectedli drop connect occur server busi wait minut try tri hour night morn problem persist suggest support greatli appreci thank have issu try dai origin error issu shut night wai open restart studio servic abl fix complet delet studio instanc aw cki cours us ephemar data start scratch sequenc command delet app domain yourdomainid user profil yourprofilenam app type kernelgatewai app base python delet app domain yourdomainid user profil yourprofilenam app type jupyterserv app default delet user profil domain yourdomainid user profil yourprofilenam delet domain domain yourdomainid face similar problem user default app jupyt server keep fail try launch studio receiv follow messag jupyterserv app default encount problem stop click restart button messag appear know solv issu recommend pai aw support issu manag fix preserv data aw consol navig studio user section open user user detail page experienc issu page delet app user default app jupyt server like kernelgatewai delet actual user profil app user delet studio click open studio link user time studio reiniti jupyt notebook notebook open data edit noobi oct edit noobi oct method work thank share",
        "Answer_preprocessed_content":"hello ran similar problem think studio preview platform resolv issu kept try launch studio aw consol receiv error time eventu abl notebook previous creat hope help issu studio work even shut juypter lab haven abl access tri access sso dashboard admin user aw consol chrome return page isnt work didnt send data firefox return secur connect fail error occur connect page try view shown authent receiv data verifi contact websit owner inform problem safari return safari open page safari open page server unexpectedli drop connect occur server busi wait minut try tri hour night morn problem persist suggest support greatli appreci thank have issu try dai origin error issu shut night wai open restart studio servic abl fix complet delet studio instanc aw cki cours us ephemar data start scratch sequenc command yourdomainid yourprofilenam kernelgatewai yourdomainid yourprofilenam jupyterserv default yourdomainid yourprofilenam yourdomainid face similar problem user default app jupyt server keep fail try launch studio receiv follow messag jupyterserv app default encount problem click restart button messag appear know solv issu recommend pai aw support issu manag fix aw consol navig studio user section open user experienc issu page delet app user default app like kernelgatewai delet actual user profil app user delet studio click open studio link user time studio reiniti jupyt notebook notebook open data edit noobi oct edit noobi oct method work thank share",
        "Question_gpt_summary_original":"the user encountered challenges with loading studio, receiving an empty response from chrome or an internal failure error message when attempting to restart the jupyterserver app.",
        "Question_gpt_summary":"user encount challeng load studio receiv respons chrome intern failur error messag attempt restart jupyterserv app",
        "Answer_gpt_summary_original":"possible solutions to the challenges encountered with loading studio include repeatedly trying to launch the studio from the aws console, deleting the studio instance and starting over from scratch, and deleting all apps for the user in the user details page and then clicking on the \"open studio\" link for that user. these solutions have been tried and tested by different users and have been reported to work.",
        "Answer_gpt_summary":"possibl solut challeng encount load studio includ repeatedli try launch studio aw consol delet studio instanc start scratch delet app user user detail page click open studio link user solut tri test differ user report work"
    },
    {
        "Question_id":null,
        "Question_title":"Manually deleted .dvc files -- these files still appear to be tracked by DVC",
        "Question_body":"<p>Hello,<\/p>\n<p>I have manually deleted .dvc files that were previously tracked (and the tracked files themselves), expecting this to remove them from tracking (I realise now I should have used dvc gc).<\/p>\n<p>My question is how do I now remove these from tracking? I cannot re-add then remove them properly as the original files are not there.<\/p>\n<p>I am syncing data in two machines, on the machine I deleted the files on it says everything is up to date but on my other machine I am getting cache errors for the now deleted items. I am confused as I have pushed the deletes of the .dvc files so I assumed this would prevent them being tracked on both machines. Are they tracked somewhere else? I do not have a dvc.yaml file.<\/p>\n<p>Thanks,<\/p>\n<p>Justin<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611306516441,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":210.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/manually-deleted-dvc-files-these-files-still-appear-to-be-tracked-by-dvc\/634",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-01-22T12:06:01.345Z",
                "Answer_body":"<p>To answer this here as well, deleting the .dvc files and then committing the git changes will work for files which were tracked with <code>dvc add<\/code>. (But prior versions of the files will still be \u201ctracked\u201d in your older git history.)<\/p>\n<p>One other thing to note is that manually removing the .dvc files will still leave entries in your .gitignore that should also be removed. <code>dvc remove &lt;path_to_.dvc_file&gt;<\/code> can be used to both remove the file and cleanup the .gitignore entry.<\/p>\n<p>Regarding the cache errors, this was clarified in a separate email chain to be an unrelated issue - the errors were due to files\/directories which had not yet been completely <code>dvc push<\/code>ed from a coworker\u2019s machine to a remote.<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"manual delet file file appear track hello manual delet file previous track track file expect remov track realis question remov track add remov properli origin file sync data machin machin delet file sai date machin get cach error delet item confus push delet file assum prevent track machin track yaml file thank justin",
        "Question_preprocessed_content":"manual delet file file appear track hello manual delet file previous track expect remov track question remov track remov properli origin file sync data machin machin delet file sai date machin get cach error delet item confus push delet file assum prevent track machin track yaml file thank justin",
        "Answer_original_content":"answer delet file commit git chang work file track add prior version file track older git histori thing note manual remov file leav entri gitignor remov remov remov file cleanup gitignor entri cach error clarifi separ email chain unrel issu error file directori complet push cowork machin remot",
        "Answer_preprocessed_content":"answer delet file commit git chang work file track thing note manual remov file leav entri gitignor remov remov file cleanup gitignor entri cach error clarifi separ email chain unrel issu error complet cowork machin remot",
        "Question_gpt_summary_original":"the user is facing challenges with manually deleting . files that are still being tracked, resulting in cache errors on one machine and confusion about where the files are being tracked.",
        "Question_gpt_summary":"user face challeng manual delet file track result cach error machin confus file track",
        "Answer_gpt_summary_original":"possible solutions from the answer are:\n\n- deleting the . files and then committing the git changes will work for files which were tracked with add.\n- manually removing the . files will still leave entries in the .gitignore that should also be removed. `remove <path_to_._file>` can be used to both remove the file and cleanup the .gitignore entry. \n\nhowever, the cache errors mentioned in the question were clarified to be an unrelated issue caused by files\/directories that had not yet been completely pushed from a coworker's machine to a remote.",
        "Answer_gpt_summary":"possibl solut answer delet file commit git chang work file track add manual remov file leav entri gitignor remov remov remov file cleanup gitignor entri cach error mention question clarifi unrel issu caus file directori complet push cowork machin remot"
    },
    {
        "Question_id":null,
        "Question_title":"Is there a way to get all the machine learning algorigthms tried by azure automl for a machine learning job in python sdk?",
        "Question_body":"I am trying azure automl through python sdk. What I need is to get all the algorithm names along with the best algorithm azure tried for that particular job using python sdk.\nI have been exploring documentation but didn't get much.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1662345469883,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/993641\/is-there-a-way-to-get-all-the-machine-learning-alg.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-09-05T12:46:12.697Z",
                "Answer_score":0,
                "Answer_body":"@Nuron-5007 I believe this can be achieved by getting the best and fitted model from your automl run's get_output and then printing the details of the algorithms used by this model.\n\n automl_config = AutoMLConfig(\u2026)\n automl_run = experiment.submit(automl_config \u2026)\n best_run, fitted_model = automl_run.get_output()\n\n\n\n\nUse fitted_model.steps to print the selected algorithm with its hyperparameter values\nThis is documented here from the Azure ML documentation.\n\nI hope this helps!!\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
                "Answer_comment_count":0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":11.0,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"wai machin learn algorigthm tri azur automl machin learn job python sdk try azur automl python sdk need algorithm name best algorithm azur tri particular job python sdk explor document",
        "Question_preprocessed_content":"wai machin learn algorigthm tri azur automl machin learn job python sdk try azur automl python sdk need algorithm name best algorithm azur tri particular job python sdk explor document",
        "Answer_original_content":"nuron believ achiev get best fit model automl run output print detail algorithm model automl config automlconfig automl run experi submit automl config best run fit model automl run output us fit model step print select algorithm hyperparamet valu document document hope help answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"believ achiev get best fit model automl run print detail algorithm model automlconfig us print select algorithm hyperparamet valu document document hope help answer help click upvot help commun member read thread",
        "Question_gpt_summary_original":"the user is trying to use azure automl through python sdk to get all the algorithm names along with the best algorithm azure tried for a particular job.",
        "Question_gpt_summary":"user try us azur automl python sdk algorithm name best algorithm azur tri particular job",
        "Answer_gpt_summary_original":"the solution to getting all the algorithm names along with the best algorithm azure tried for a particular job using azure automl through python sdk is to get the best and fitted model from the automl run's get_output and then print the details of the algorithms used by this model. the selected algorithm with its hyperparameter values can be printed using fitted_model.steps.",
        "Answer_gpt_summary":"solut get algorithm name best algorithm azur tri particular job azur automl python sdk best fit model automl run output print detail algorithm model select algorithm hyperparamet valu print fit model step"
    },
    {
        "Question_id":null,
        "Question_title":"Data versioning of databases",
        "Question_body":"<p>Hi,<\/p>\n<p>Are you thinking of supporting data versioning of databases.<br>\nTracking changes of transformations from raw data to cleaned data?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1522998440046,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":543.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/data-versioning-of-databases\/20",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-04-06T14:06:45.113Z",
                "Answer_body":"<p>Hi!<\/p>\n<p>Could you please elaborate on the scenario you have in mind?<br>\nWhat kind of tracking are you thinking of?<\/p>\n<p>Dvc can track the way you cleanup your data by storing input, output and a command that you use. You can then replace the input with the new version, run <code>dvc repro<\/code> and have it and the new version of cleaned up data tracked by dvc.<\/p>",
                "Answer_score":8.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"data version databas think support data version databas track chang transform raw data clean data thank",
        "Question_preprocessed_content":"data version databas think support data version databas track chang transform raw data clean data thank",
        "Answer_original_content":"elabor scenario mind kind track think track wai cleanup data store input output command us replac input new version run repro new version clean data track",
        "Answer_preprocessed_content":"elabor scenario mind kind track think track wai cleanup data store input output command us replac input new version run new version clean data track",
        "Question_gpt_summary_original":"the user is inquiring about the possibility of supporting data versioning of databases to track changes of transformations from raw data to cleaned data.",
        "Question_gpt_summary":"user inquir possibl support data version databas track chang transform raw data clean data",
        "Answer_gpt_summary_original":"possible solutions to support data versioning of databases to track changes of transformations from raw data to cleaned data are: storing input, output, and the command used to clean up the data, replacing the input with the new version, running repro, and tracking both the new version of cleaned up data and the repro.",
        "Answer_gpt_summary":"possibl solut support data version databas track chang transform raw data clean data store input output command clean data replac input new version run repro track new version clean data repro"
    },
    {
        "Question_id":null,
        "Question_title":"Track remote data on Azure",
        "Question_body":"<p>Hello DVC community!<\/p>\n<p>I\u2019m currently trying out DVC on Ubuntu 20.04 LTS, with Python 3.8.10, on WSL2 with Windows 10.<br>\nI am running into some seemingly inconsistent issues trying to track ZIP files in a remote Azure BLOB Storage container.<\/p>\n<p>At least in the initial set-up, I am trying to track those files without downloading them to the local machine. Apologies if this is really a basic question but I a bit stumped at the moment.<\/p>\n<p>I used \u201cdvc remote add\u201d and \u201cdvc remote modify\u201d, the generated config \/ config.local files in the .dvc folder look good. I added a valid connection string for the Azure storage account with those commands, then I also added that connection string to my environment with:<\/p>\n<blockquote>\n<p>export AZURE_STORAGE_CONNECTION_STRING=\u2018myconnectionstring\u2019<\/p>\n<\/blockquote>\n<p>I am trying the following command:<\/p>\n<blockquote>\n<p>dvc import-url azure:\/\/[mycontainer]\/[myfile] --to-remote<\/p>\n<\/blockquote>\n<p>The remote is another BLOB container in the same Storage Account where the files that I want to track are located.<\/p>\n<p>Yesterday I managed to use the \u201c<em>import-url \u2026 --to-remote<\/em>\u201d command even though the .dvc files that were created locally landed in the wrong folder (my mistake with paths etc). Today I am simply getting the error:<\/p>\n<blockquote>\n<p>ERROR: unexpected error - : \u2018azure\u2019<\/p>\n<\/blockquote>\n<p>If I try to run:<\/p>\n<blockquote>\n<p>dvc import-url azure:\/\/[mycontainer]\/[myfile] .\/path\/to\/local\/folder\/ --to-remote<\/p>\n<\/blockquote>\n<p>The error I get is:<\/p>\n<blockquote>\n<p>ERROR: failed to import azure:\/\/[mycontainer]\/[myzipfilename]. You could also try downloading it manually, and adding it with <code>dvc add<\/code>. - bad DVC file name \u2018path\/to\/local\/folder\/[myzipfilename].dvc\u2019 is git-ignored.<\/p>\n<\/blockquote>\n<p>Same error as the above appears if I move into the folder where I want the .dvc files to land and run the dvc import-url command from there, regardless of whether I do this with specifying the output file name or not.<\/p>\n<p>EDIT: I ran it with -v and this is what I get:<\/p>\n<blockquote>\n<p>2022-03-09 11:39:09,229 DEBUG: Lockfile for \u2018dvc.yaml\u2019 not found<br>\n2022-03-09 11:39:09,231 ERROR: unexpected error - : \u2018azure\u2019<\/p>\n<\/blockquote>\n<p>Somehow there is no lock file, seems I am missing something super basic <img src=\"https:\/\/emoji.discourse-cdn.com\/apple\/frowning.png?v=12\" title=\":frowning:\" class=\"emoji\" alt=\":frowning:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<p>SECOND EDIT:<br>\nIf I run the import-url command from the root project folder without specifying a path that is different from this project root, it works, with the \u201cproblem\u201d that the local .dvc file is in the root and not in a subdirectory where I actually want it to be. I still get the \u201cLockfile for \u2018dvc.yaml\u2019 not found\u201d debug msg but then \u201cComputing md5 for a large file \u2026\u201d indicating that the command \u201cworks\u201d.<\/p>\n<p>I am getting more confused by the minute lol.<br>\nAny ideas how I can fix this?<\/p>\n<p>Thanks a bunch in advance!<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1646821961294,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":395.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/discuss.dvc.org\/t\/track-remote-data-on-azure\/1110",
        "Tool":"DVC",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-03-09T13:53:07.724Z",
                "Answer_body":"<p>Looks like several different questions.<\/p>\n<p>First, could you please provide some details about the first <code>ERROR: unexpected error - : \u2018azure\u2019<\/code><br>\nYou can get it from <code>dvc import-url azure:\/\/[mycontainer]\/[myfile] --to-remote -vv<\/code><\/p>\n<p>Second, for <code>dvc import-url azure:\/\/[mycontainer]\/[myfile] .\/path\/to\/local\/folder\/ --to-remote <\/code> it looks like your <code>\u2018path\/to\/local\/folder\/[myzipfilename].dvc\u2019 is git-ignored.<\/code> To solve this , you need either modify your <code>.gitignore<\/code> to delete the pattern or switch to a different path.<\/p>\n<p><code>2022-03-09 11:39:09,229 DEBUG: Lockfile for \u2018dvc.yaml\u2019 not found<\/code> is only a DEBUG message, <code>\u2018dvc.yaml\u2019<\/code> is not required in your case.<\/p>\n<blockquote>\n<p>If I run the import-url command from the root project folder without specifying a path that is different from this project root, it works, with the \u201cproblem\u201d that the local .dvc file is in the root and not in a subdirectory where I actually want it to be. I still get the \u201cLockfile for \u2018dvc.yaml\u2019 not found\u201d debug msg but then \u201cComputing md5 for a large file \u2026\u201d indicating that the command \u201cworks\u201d.<\/p>\n<\/blockquote>\n<p>looks like it can work on your root path, so I guess what you need to do is to make some modification on<br>\nyour <code>.gitignore<\/code> file. For example, add an exclude rule <code>!*.dvc<\/code> to make any kind of <code>.dvc<\/code> file being tracked by <code>Git<\/code>(But may not so easy, more details in <a href=\"https:\/\/git-scm.com\/docs\/gitignore\" class=\"inline-onebox\" rel=\"noopener nofollow ugc\">Git - gitignore Documentation<\/a>)<\/p>",
                "Answer_score":12.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-03-11T09:12:55.084Z",
                "Answer_body":"<p>Hi Yanxiang,<\/p>\n<p>you were spot on, your comment seems to have fixed everything. I removed a .gitignore file and voil\u00e0. My simple DVC pipeline runs.<\/p>\n<p>I actually did not start this repo from scratch but from a colleague which was tracking files locally instead of from azure. There was a .gitignore next to directory where those files were. They are not there anymore now, but on Azure, so I removed that .gitignore and everything is flowing. Thank you!<\/p>",
                "Answer_score":26.4,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"track remot data azur hello commun current try ubuntu lt python wsl window run seemingli inconsist issu try track zip file remot azur blob storag contain initi set try track file download local machin apolog basic question bit stump moment remot add remot modifi gener config config local file folder look good ad valid connect string azur storag account command ad connect string environ export azur storag connect string myconnectionstr try follow command import url azur mycontain myfil remot remot blob contain storag account file want track locat yesterdai manag us import url remot command file creat local land wrong folder mistak path todai simpli get error error unexpect error azur try run import url azur mycontain myfil path local folder remot error error fail import azur mycontain myzipfilenam try download manual ad add bad file path local folder myzipfilenam git ignor error appear folder want file land run import url command regardless specifi output file edit ran debug lockfil yaml error unexpect error azur lock file miss super basic second edit run import url command root project folder specifi path differ project root work problem local file root subdirectori actual want lockfil yaml debug msg comput larg file indic command work get confus minut lol idea fix thank bunch advanc",
        "Question_preprocessed_content":"track remot data azur hello commun current try ubuntu lt python wsl window run seemingli inconsist issu try track zip file remot azur blob storag contain initi try track file download local machin apolog basic question bit stump moment remot add remot modifi gener config file folder look good ad valid connect string azur storag account command ad connect string environ export try follow command remot blob contain storag account file want track locat yesterdai manag us command file creat local land wrong folder todai simpli get error error unexpect error azur try run error error fail import try download manual ad bad file error appear folder want file land run command regardless specifi output file edit ran debug lockfil yaml error unexpect error azur lock file miss super basic second edit run command root project folder specifi path differ project root work problem local file root subdirectori actual want lockfil yaml debug msg comput larg file indic command work get confus minut lol idea fix thank bunch advanc",
        "Answer_original_content":"look like differ question provid detail error unexpect error azur import url azur mycontain myfil remot second import url azur mycontain myfil path local folder remot look like path local folder myzipfilenam git ignor solv need modifi gitignor delet pattern switch differ path debug lockfil yaml debug messag yaml requir case run import url command root project folder specifi path differ project root work problem local file root subdirectori actual want lockfil yaml debug msg comput larg file indic command work look like work root path guess need modif gitignor file exampl add exclud rule kind file track git easi detail git gitignor document yanxiang spot comment fix remov gitignor file voil simpl pipelin run actual start repo scratch colleagu track file local instead azur gitignor directori file anymor azur remov gitignor flow thank",
        "Answer_preprocessed_content":"look like differ question provid detail second look like solv need modifi delet pattern switch differ path debug messag requir case run command root project folder specifi path differ project root work problem local file root subdirectori actual want lockfil yaml debug msg comput larg file indic command work look like work root path guess need modif file exampl add exclud rule kind file track yanxiang spot comment fix remov gitignor file voil simpl pipelin run actual start repo scratch colleagu track file local instead azur gitignor directori file anymor azur remov gitignor flow thank",
        "Question_gpt_summary_original":"the user is encountering issues trying to track zip files in a remote azure blob storage container without downloading them to the local machine.",
        "Question_gpt_summary":"user encount issu try track zip file remot azur blob storag contain download local machin",
        "Answer_gpt_summary_original":"the answer provides a few possible solutions to the issue of tracking zip files in a remote azure blob storage container without downloading them to the local machine. one solution involves modifying the .gitignore file to delete the pattern or switch to a different path. another solution involves running the import-url command from the root project folder without specifying a path that is different from this project root. additionally, the answer suggests removing the .gitignore file if it exists and tracking the files from azure instead.",
        "Answer_gpt_summary":"answer provid possibl solut issu track zip file remot azur blob storag contain download local machin solut involv modifi gitignor file delet pattern switch differ path solut involv run import url command root project folder specifi path differ project root addition answer suggest remov gitignor file exist track file azur instead"
    },
    {
        "Question_id":null,
        "Question_title":"Dependencies problem",
        "Question_body":"<p>Hello! Pretty new to guild.ai and trying to use it on my recent project. I tried to run my code with guild.ai wrapper, but it does not run due to a dependencies issue (\u201cbpe_simple_vocab_16e6.txt.gz\u201d is missing from the running folder, despite every other file in the same folder with it being included). I tried to solve it by including requires in my guild.yml:<\/p>\n<p>requires:<\/p>\n<ul>\n<li>file: pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz<\/li>\n<\/ul>\n<p>but despite the relative path is correct, the error pops up saying<\/p>\n<p>ERROR: resolving required source \u2018file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz\u2019 in file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz resource<br>\nTraceback (most recent call last):<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/op_dep.py\u201d, line 236, in resolve_source<br>\nsource_paths = _resolve_source_for_location(<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/op_dep.py\u201d, line 267, in _resolve_source_for_location<br>\nreturn resolver.resolve(resolve_context)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 114, in resolve<br>\nresolved = self._resolve_source_files(source_path, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 138, in _resolve_source_files<br>\nreturn resolve_source_files(source_path, self.source, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 553, in resolve_source_files<br>\nreturn _resolve_source_file_or_archive_files(source_path, source, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 618, in _resolve_source_file_or_archive_files<br>\nreturn _resolve_archive_files(source_path, archive_type, source, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 674, in _resolve_archive_files<br>\nunpacked = _ensure_unpacked(source_path, archive_type, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 686, in _ensure_unpacked<br>\nunpacked = _unpack(source_path, archive_type, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 711, in _unpack<br>\nreturn _gunzip(source_path, unpack_dir)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 749, in _gunzip<br>\nreturn _gen_unpack(<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 806, in _gen_unpack<br>\nextract(unpack_dir, to_extract)<br>\nFile \u201c\/home\/hyang\/.local\/lib\/python3.9\/site-packages\/guild\/resolver.py\u201d, line 781, in extract<br>\nwith open(dest, \u201cwb\u201d) as f_out:<br>\nFileNotFoundError: [Errno 2] No such file or directory: \u2018\/home\/hyang\/deadclip\/CyCLIP\/env\/.guild\/cache\/resources\/6d8d1935c2ed1916a4210b8f6391e63d846c5f02d88deced8f8fa21b\/bpe_simple_vocab_16e6.txt\u2019<br>\nguild: run failed because a dependency was not met: unexpected error resolving \u2018file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz\u2019 in file:pkgs\/openai\/bpe_simple_vocab_16e6.txt.gz resource: FileNotFoundError(2, \u2018No such file or directory\u2019)<\/p>\n<p>I also tried to toy with the relative path with \u201c\u2026\u201d or use the absolute path, but it still didn\u2019t work<\/p>\n<p>Any help would be appreciated :))<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1666248030613,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":87.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/my.guild.ai\/t\/dependencies-problem\/943",
        "Tool":"Guild AI",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-10-21T17:28:45.618Z",
                "Answer_body":"<p>Could you plot the directory tree? Specifically, where\u2019s the sourcecode, the guild file and the resources file are?<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-10-28T20:07:48.476Z",
                "Answer_body":"<p>|-CyCLIP<br>\n-----|-guild.yml<br>\n-----|-pkgs<br>\n----------|-openai<br>\n-----------------|-bpe_simple_vocab_16e6.txt.gz<br>\n-----|-src<br>\n----------|-main.py<\/p>",
                "Answer_score":0.8,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"depend problem hello pretti new try us recent project tri run code wrapper run depend issu bpe simpl vocab txt miss run folder despit file folder includ tri solv includ requir yml requir file pkg openai bpe simpl vocab txt despit rel path correct error pop sai error resolv requir sourc file pkg openai bpe simpl vocab txt file pkg openai bpe simpl vocab txt resourc traceback recent file home hyang local lib python site packag dep line resolv sourc sourc path resolv sourc locat file home hyang local lib python site packag dep line resolv sourc locat return resolv resolv resolv context file home hyang local lib python site packag resolv line resolv resolv self resolv sourc file sourc path unpack dir file home hyang local lib python site packag resolv line resolv sourc file return resolv sourc file sourc path self sourc unpack dir file home hyang local lib python site packag resolv line resolv sourc file return resolv sourc file archiv file sourc path sourc unpack dir file home hyang local lib python site packag resolv line resolv sourc file archiv file return resolv archiv file sourc path archiv type sourc unpack dir file home hyang local lib python site packag resolv line resolv archiv file unpack ensur unpack sourc path archiv type unpack dir file home hyang local lib python site packag resolv line ensur unpack unpack unpack sourc path archiv type unpack dir file home hyang local lib python site packag resolv line unpack return gunzip sourc path unpack dir file home hyang local lib python site packag resolv line gunzip return gen unpack file home hyang local lib python site packag resolv line gen unpack extract unpack dir extract file home hyang local lib python site packag resolv line extract open dest filenotfounderror errno file directori home hyang deadclip cyclip env cach resourc ddcedabfedcfddecedffab bpe simpl vocab txt run fail depend met unexpect error resolv file pkg openai bpe simpl vocab txt file pkg openai bpe simpl vocab txt resourc filenotfounderror file directori tri toi rel path us absolut path didnt work help appreci",
        "Question_preprocessed_content":"depend problem hello pretti new try us recent project tri run code wrapper run depend issu tri solv includ requir yml requir file despit rel path correct error pop sai error resolv requir sourc resourc traceback file line file line return file line resolv resolv file line return file line return sourc file line return sourc file line unpack file line unpack file line return file line return file line file line extract open filenotfounderror file directori run fail depend met unexpect error resolv resourc filenotfounderror tri toi rel path us absolut path didnt work help appreci",
        "Answer_original_content":"plot directori tree specif where sourcecod file resourc file cyclip yml pkg openai bpe simpl vocab txt src main",
        "Answer_preprocessed_content":"plot directori tree specif where sourcecod file resourc file cyclip yml pkg openai src",
        "Question_gpt_summary_original":"the user is encountering a challenge with dependencies when trying to run their code with guild.ai, as the required file \"bpe_simple_vocab_16e6.txt.gz\" is missing from the running folder.",
        "Question_gpt_summary":"user encount challeng depend try run code requir file bpe simpl vocab txt miss run folder",
        "Answer_gpt_summary_original":"the answer suggests that the user should check the directory tree to locate the source code, guild file, and resources file. specifically, they should look for the \"bpe_simple_vocab_16e6.txt.gz\" file in the \"openai\" package under the \"pkgs\" directory.",
        "Answer_gpt_summary":"answer suggest user check directori tree locat sourc code file resourc file specif look bpe simpl vocab txt file openai packag pkg directori"
    },
    {
        "Question_id":68790568.0,
        "Question_title":"\"errorMessage\": \"Parameter validation failed in Lambda calling SageMaker endpoint",
        "Question_body":"<p>I am trying to invoke a SageMaker enpoint from AWS Lambda using a lambda function.<\/p>\n<p>This is a sample API call to the endpoint from SageMaker Studio, working as expected:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/3iTPN.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>here's my Lambda function (<a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"nofollow noreferrer\">inspired from documentation<\/a>):<\/p>\n<pre><code>import os\nimport io\nimport boto3\nimport json\n\n\nENDPOINT_NAME = 'iris-autoscale-6'\nruntime= boto3.client('runtime.sagemaker')\n\ndef lambda_handler(event, context):\n    # print(&quot;Received event: &quot; + json.dumps(event, indent=2))\n    payload = json.loads(json.dumps(event))\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n    print(response)\n    result = json.loads(response['Body'].read().decode())\n    print(result)\n    \n    return result\n<\/code><\/pre>\n<p>My error message:<\/p>\n<pre><code>Test Event Name\nProperTest\n\nResponse\n{\n  &quot;errorMessage&quot;: &quot;Parameter validation failed:\\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object&quot;,\n  &quot;errorType&quot;: &quot;ParamValidationError&quot;,\n  &quot;stackTrace&quot;: [\n    &quot;  File \\&quot;\/var\/task\/lambda_function.py\\&quot;, line 17, in lambda_handler\\n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 386, in _api_call\\n    return self._make_api_call(operation_name, kwargs)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 678, in _make_api_call\\n    api_params, operation_model, context=request_context)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/client.py\\&quot;, line 726, in _convert_to_request_dict\\n    api_params, operation_model)\\n&quot;,\n    &quot;  File \\&quot;\/var\/runtime\/botocore\/validate.py\\&quot;, line 319, in serialize_to_request\\n    raise ParamValidationError(report=report.generate_report())\\n&quot;\n  ]\n}\n\nFunction Logs\nSTART RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512 Version: $LATEST\n{'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}\n[ERROR] ParamValidationError: Parameter validation failed:\nInvalid type for parameter Body, value: {'sepal_length': [5.1, 4.9, 4.7, 4.6, 5], 'sepal_width': [3.5, 3, 3.2, 3.1, 3.6], 'petal_length': [1.4, 1.4, 1.3, 1.5, 1.4], 'petal_width': [0.2, 0.2, 0.2, 0.2, 0.2]}, type: &lt;class 'dict'&gt;, valid types: &lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\nTraceback (most recent call last):\n\u00a0\u00a0File &quot;\/var\/task\/lambda_function.py&quot;, line 17, in lambda_handler\n\u00a0\u00a0\u00a0\u00a0response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 386, in _api_call\n\u00a0\u00a0\u00a0\u00a0return self._make_api_call(operation_name, kwargs)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 678, in _make_api_call\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model, context=request_context)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/client.py&quot;, line 726, in _convert_to_request_dict\n\u00a0\u00a0\u00a0\u00a0api_params, operation_model)\n\u00a0\u00a0File &quot;\/var\/runtime\/botocore\/validate.py&quot;, line 319, in serialize_to_request\n\u00a0\u00a0\u00a0\u00a0raise ParamValidationError(report=report.generate_report())\nEND RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512\nREPORT RequestId: 70278b9f-f75e-4ac9-a827-7ad35d162512  Duration: 26.70 ms  Billed Duration: 27 ms  Memory Size: 128 MB Max Memory Used: 76 MB  Init Duration: 343.10 ms\n<\/code><\/pre>\n<p>Here's the policy attached to the lambda function:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Sid&quot;: &quot;VisualEditor0&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Action&quot;: &quot;sagemaker:InvokeEndpoint&quot;,\n            &quot;Resource&quot;: &quot;arn:aws:sagemaker:ap-south-1:&lt;my-account-id&gt;:endpoint\/iris-autoscale-6&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1629022235437,
        "Question_favorite_count":null,
        "Question_last_edit_time":1629023309567,
        "Question_score":1.0,
        "Question_view_count":499.0,
        "Answer_body":"<p>The issue is that your <code>payload<\/code> has invalid format. It should be one of:<\/p>\n<pre><code>&lt;class 'bytes'&gt;, &lt;class 'bytearray'&gt;, file-like object\n<\/code><\/pre>\n<p>The following should address the error (note: you may have many other issues in your code):<\/p>\n<pre><code>    payload = json.dumps(event)\n    print(payload)\n    \n    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME, ContentType='application\/json', Body=payload.encode())\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68790568",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1629024434667,
        "Question_original_content":"errormessag paramet valid fail lambda call endpoint try invok enpoint aw lambda lambda function sampl api endpoint studio work expect lambda function inspir document import import import boto import json endpoint iri autoscal runtim boto client runtim def lambda handler event context print receiv event json dump event indent payload json load json dump event print payload respons runtim invok endpoint endpointnam endpoint contenttyp applic json bodi payload print respons result json load respons bodi read decod print result return result error messag test event propertest respons errormessag paramet valid fail ninvalid type paramet bodi valu sepal length sepal width petal length petal width type valid type file like object errortyp paramvalidationerror stacktrac file var task lambda function line lambda handler respons runtim invok endpoint endpointnam endpoint contenttyp applic json bodi payload file var runtim botocor client line api return self api oper kwarg file var runtim botocor client line api api param oper model context request context file var runtim botocor client line convert request dict api param oper model file var runtim botocor valid line serial request rais paramvalidationerror report report gener report function log start requestid add version latest sepal length sepal width petal length petal width error paramvalidationerror paramet valid fail invalid type paramet bodi valu sepal length sepal width petal length petal width type valid type file like object traceback recent file var task lambda function line lambda handler respons runtim invok endpoint endpointnam endpoint contenttyp applic json bodi payload file var runtim botocor client line api return self api oper kwarg file var runtim botocor client line api api param oper model context request context file var runtim botocor client line convert request dict api param oper model file var runtim botocor valid line serial request rais paramvalidationerror report report gener report end requestid add report requestid add durat bill durat memori size max memori init durat polici attach lambda function version statement sid visualeditor effect allow action invokeendpoint resourc arn aw south endpoint iri autoscal",
        "Question_preprocessed_content":"errormessag paramet valid fail lambda call endpoint try invok enpoint aw lambda lambda function sampl api endpoint studio work expect lambda function error messag polici attach lambda function",
        "Answer_original_content":"issu payload invalid format file like object follow address error note issu code payload json dump event print payload respons runtim invok endpoint endpointnam endpoint contenttyp applic json bodi payload encod",
        "Answer_preprocessed_content":"issu invalid format follow address error",
        "Question_gpt_summary_original":"the user is encountering a \"parameter validation failed\" error when attempting to invoke an endpoint from aws lambda using a lambda function.",
        "Question_gpt_summary":"user encount paramet valid fail error attempt invok endpoint aw lambda lambda function",
        "Answer_gpt_summary_original":"the solution to the \"parameter validation failed\" error when invoking an endpoint from aws lambda using a lambda function is to ensure that the payload has a valid format, which should be one of: <class 'bytes'>, <class 'bytearray'>, or a file-like object. the suggested solution is to use the json.dumps() method to encode the event payload and then print it before invoking the endpoint with the encoded payload using the runtime.invoke_endpoint() method.",
        "Answer_gpt_summary":"solut paramet valid fail error invok endpoint aw lambda lambda function ensur payload valid format file like object suggest solut us json dump method encod event payload print invok endpoint encod payload runtim invok endpoint method"
    },
    {
        "Question_id":65887231.0,
        "Question_title":"Use mlflow to serve a custom python model for scoring",
        "Question_body":"<p>I am using Python code generated from an ml software with mlflow to read a dataframe, perform some table operations and output a dataframe. I am able to run the code successfully and save the new dataframe as an artifact. However I am unable to log the model using log_model because it is not a lr or classifier model where we train and fit. I want to log a model for this so that it can be served with new data and deployed with a rest API<\/p>\n<pre><code>df = pd.read_csv(r&quot;\/home\/xxxx.csv&quot;)\n\n\nwith mlflow.start_run():\n    def getPrediction(row):\n        \n        perform_some_python_operaions \n\n        return [Status_prediction, Status_0_probability, Status_1_probability]\n    columnValues = []\n    for column in columns:\n        columnValues.append([])\n\n    for index, row in df.iterrows():\n        results = getPrediction(row)\n        for n in range(len(results)):\n            columnValues[n].append(results[n])\n\n    for n in range(len(columns)):\n        df[columns[n]] = columnValues[n]\n\n    df.to_csv('dataset_statistics.csv')\n    mlflow.log_artifact('dataset_statistics.csv')\n   \n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1611586824463,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":3026.0,
        "Answer_body":"<p>MLflow supports <a href=\"https:\/\/mlflow.org\/docs\/latest\/models.html#custom-python-models\" rel=\"nofollow noreferrer\">custom models<\/a> of mlflow.pyfunc flavor.  You can create a custom  class  inherited from the <code>mlflow.pyfunc.PythonModel<\/code>, that needs to provide function <code>predict<\/code> for performing predictions, and optional <code>load_context<\/code> to load the necessary artifacts, like this (adopted from the docs):<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>class MyModel(mlflow.pyfunc.PythonModel):\n\n    def load_context(self, context):\n        # load your artifacts\n\n    def predict(self, context, model_input):\n        return my_predict(model_input.values)\n<\/code><\/pre>\n<p>You can log to MLflow whatever artifacts you need for your models, define Conda environment if necessary, etc.<br \/>\nThen you can use <code>save_model<\/code> with your class to save your implementation, that could be loaded with <code>load_model<\/code> and do the <code>predict<\/code> using your model:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>mlflow.pyfunc.save_model(\n        path=mlflow_pyfunc_model_path, \n        python_model=MyModel(), \n        artifacts=artifacts)\n\n# Load the model in `python_function` format\nloaded_model = mlflow.pyfunc.load_model(mlflow_pyfunc_model_path)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1634187940523,
        "Answer_score":9.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65887231",
        "Tool":"MLflow",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1611592914947,
        "Question_original_content":"us serv custom python model score python code gener softwar read datafram perform tabl oper output datafram abl run code successfulli save new datafram artifact unabl log model log model classifi model train fit want log model serv new data deploi rest api read csv home csv start run def getpredict row perform python operaion return statu predict statu probabl statu probabl columnvalu column column columnvalu append index row iterrow result getpredict row rang len result columnvalu append result rang len column column columnvalu csv dataset statist csv log artifact dataset statist csv",
        "Question_preprocessed_content":"us serv custom python model score python code gener softwar read datafram perform tabl oper output datafram abl run code successfulli save new datafram artifact unabl log model classifi model train fit want log model serv new data deploi rest api",
        "Answer_original_content":"support custom model pyfunc flavor creat custom class inherit pyfunc pythonmodel need provid function predict perform predict option load context load necessari artifact like adopt doc class mymodel pyfunc pythonmodel def load context self context load artifact def predict self context model input return predict model input valu log artifact need model defin conda environ necessari us save model class save implement load load model predict model pyfunc save model path pyfunc model path python model mymodel artifact artifact load model python function format load model pyfunc load model pyfunc model path",
        "Answer_preprocessed_content":"support custom model pyfunc flavor creat custom class inherit need provid function perform predict option load necessari artifact like log artifact need model defin conda environ necessari us class save implement load model",
        "Question_gpt_summary_original":"the user is attempting to use  to serve a custom python model for scoring, but is unable to log the model due to it not being a lr or classifier model.",
        "Question_gpt_summary":"user attempt us serv custom python model score unabl log model classifi model",
        "Answer_gpt_summary_original":"possible solutions to the problem of being unable to log a custom python model in  are: \n- use a custom class inherited from the .pyfunc.pythonmodel that provides a predict function for performing predictions and an optional load_context function to load necessary artifacts. \n- log whatever artifacts are needed for the model and define a conda environment if necessary. \n- use save_model with the custom class to save the implementation, which can be loaded with load_model and used for prediction.",
        "Answer_gpt_summary":"possibl solut problem unabl log custom python model us custom class inherit pyfunc pythonmodel provid predict function perform predict option load context function load necessari artifact log artifact need model defin conda environ necessari us save model custom class save implement load load model predict"
    },
    {
        "Question_id":null,
        "Question_title":"HP sweep - correct way to stop a specific agent (and not the entire sweep)",
        "Question_body":"<p>Hi,<\/p>\n<p>I am conducting a parameter sweep, and I use my dev machine during the night for extra compute. My problem is that I don\u2019t know how to correctly stop the local runs. Any suggestions or best practices would be appreciated.<\/p>\n<p>A related question - if I stop an agent run forcefully (for instance, close the process running the agent) how would the sweep controller handle the run data? would it remove it from the dashboard? would it be indicated in any way?<\/p>\n<p>Thanks,<br>\nTom<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_creation_time":1635686596678,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":534.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/hp-sweep-correct-way-to-stop-a-specific-agent-and-not-the-entire-sweep\/1173",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2021-11-01T19:18:59.802Z",
                "Answer_body":"<p>Hi Tom,<\/p>\n<p>You can use the W&amp;B Dashboard or end the process in order to kill a sweep. You can use the \u201csweep controls\u201d option in the sweeps menu to control your sweep. I have attached an image of the sweep dashboard for context:<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9976ac854c5c805b000120b63110dcc2fe640d06.jpeg\" data-download-href=\"\/uploads\/short-url\/lTBjga2bfz2xwlt26urnnf6KxBY.jpeg?dl=1\" title=\"Screen Shot 2021-11-01 at 12.14.17 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9976ac854c5c805b000120b63110dcc2fe640d06_2_646x500.jpeg\" alt=\"Screen Shot 2021-11-01 at 12.14.17 PM\" data-base62-sha1=\"lTBjga2bfz2xwlt26urnnf6KxBY\" width=\"646\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9976ac854c5c805b000120b63110dcc2fe640d06_2_646x500.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9976ac854c5c805b000120b63110dcc2fe640d06_2_969x750.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9976ac854c5c805b000120b63110dcc2fe640d06_2_1292x1000.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/9976ac854c5c805b000120b63110dcc2fe640d06_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2021-11-01 at 12.14.17 PM<\/span><span class=\"informations\">1418\u00d71096 57.6 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>\n<p>The pause option lets you pause the sweep, which means that the agent will finish the current run and wait till you unpause the run again. You can also stop the run, which will wait for the current run to end, after which it will end the process.<\/p>\n<p>In case you choose to Cancel the run, any run taking place at the time will stop without waiting for the training cycle to end. Any metrics that have already been logged to W&amp;B will still be visible in your dashboard.<\/p>\n<p>Similarly, if you were to forcefully stop a process, the agent will stop running and any metrics already logged to the W&amp;B Dashboard will still be present.<\/p>\n<p>All the best,<br>\nRamit<\/p>",
                "Answer_score":12.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-02T14:35:57.807Z",
                "Answer_body":"<p>Thank you for your response!<\/p>\n<p>In that case, if I may suggest a feature request: since the sweep is aware of the agents currently running, it would be great to kill or pause\\resume a specific agent (instead of all the agents).<\/p>\n<p>Cheers,<br>\nTom<\/p>",
                "Answer_score":37.6,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-02T17:29:18.413Z",
                "Answer_body":"<p>You are welcome! Thank you for your feature request, I will pass this information to the engineering team.<\/p>\n<p>Is there anything else I can help you with?<\/p>",
                "Answer_score":2.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2021-11-03T11:55:52.183Z",
                "Answer_body":"<p>Thank you for your help <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slight_smile.png?v=10\" title=\":slight_smile:\" class=\"emoji\" alt=\":slight_smile:\"><\/p>",
                "Answer_score":7.4,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-01-02T11:56:18.704Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.6,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"sweep correct wai stop specif agent entir sweep conduct paramet sweep us dev machin night extra comput problem dont know correctli stop local run suggest best practic appreci relat question stop agent run forcefulli instanc close process run agent sweep control handl run data remov dashboard indic wai thank tom",
        "Question_preprocessed_content":"sweep correct wai stop specif agent conduct paramet sweep us dev machin night extra comput problem dont know correctli stop local run suggest best practic appreci relat question stop agent run forcefulli sweep control handl run data remov dashboard indic wai thank tom",
        "Answer_original_content":"tom us dashboard end process order kill sweep us sweep control option sweep menu control sweep attach imag sweep dashboard context screen shot paus option let paus sweep mean agent finish current run wait till unpaus run stop run wait current run end end process case choos cancel run run take place time stop wait train cycl end metric log visibl dashboard similarli forcefulli stop process agent stop run metric log dashboard present best ramit thank respons case suggest featur request sweep awar agent current run great kill paus resum specif agent instead agent cheer tom welcom thank featur request pass inform engin team help thank help topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"tom us dashboard end process order kill sweep us sweep control option sweep menu control sweep attach imag sweep dashboard context screen shot paus option let paus sweep mean agent finish current run wait till unpaus run stop run wait current run end end process case choos cancel run run take place time stop wait train cycl end metric log visibl dashboard similarli forcefulli stop process agent stop run metric log dashboard present best ramit thank respons case suggest featur request sweep awar agent current run great kill specif agent cheer tom welcom thank featur request pass inform engin team help thank help topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user tom is encountering challenges with stopping a specific agent in a parameter sweep and is unsure of how the sweep controller will handle the run data if the agent is stopped forcefully.",
        "Question_gpt_summary":"user tom encount challeng stop specif agent paramet sweep unsur sweep control handl run data agent stop forcefulli",
        "Answer_gpt_summary_original":"possible solutions to the challenge of stopping a specific agent in a parameter sweep and handling the run data if the agent is stopped forcefully are:\n\n- use the w&b dashboard or end the process to kill a sweep.\n- use the sweep controls option in the sweeps menu to control the sweep.\n- pause the sweep to let the agent finish the current run and wait till you unpause the run again.\n- stop the run to wait for the current run to end, after which it will end the process.\n- cancel the run to stop any run taking place at the time without waiting for the training cycle to end.\n- any metrics that have already been logged to w&b will still be visible in your dashboard.\n- if you forcefully stop a process, the agent will stop running and any metrics already logged to the w&b dashboard will still be present.\n- a feature request can be made to kill or pause\/resume a specific agent instead of all the agents.",
        "Answer_gpt_summary":"possibl solut challeng stop specif agent paramet sweep handl run data agent stop forcefulli us dashboard end process kill sweep us sweep control option sweep menu control sweep paus sweep let agent finish current run wait till unpaus run stop run wait current run end end process cancel run stop run take place time wait train cycl end metric log visibl dashboard forcefulli stop process agent stop run metric log dashboard present featur request kill paus resum specif agent instead agent"
    },
    {
        "Question_id":56364828.0,
        "Question_title":"Azure ML - Train a model on segments of the data-set",
        "Question_body":"<p>I could really use some help!<\/p>\n\n<p>The company I work for is made up of 52 very different businesses so I can't predict at the company level but instead need to predict business by business then roll up the result to give company wide prediction.<\/p>\n\n<p>I have written an ML model in studio.azureml.net\nIt works great with a 0.947 Coefficient of Determination, but this is for 1 of the businesses.\nI now need to train the model for the other 51.<\/p>\n\n<p>Is there a way to do this in a single ML model rather than having to create 52 very similar models?<\/p>\n\n<p>Any help would be much appreciated !!!<\/p>\n\n<p>Kind Regards\nMartin<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1559146852583,
        "Question_favorite_count":null,
        "Question_last_edit_time":1560930522380,
        "Question_score":1.0,
        "Question_view_count":123.0,
        "Answer_body":"<p>You can use Ensembles, combining several models to improve predictions. The most direct is stacking when the outputs of all the models are trained on the entire dataset. \nThe method that, I think, corresponds the best to your problem is bagging (bootstrap aggregation). You need to divide the training set into different subsets (each corresponding to a certain business), then train a different model on each subset and combine the result of each classifier. \nAnother way is boosting but it is difficult to implement in Azure ML. \nYou can see an example in <a href=\"https:\/\/gallery.azure.ai\/Experiment\/b6b09fc0c26047e6b4c733ab78a86498\" rel=\"nofollow noreferrer\">Azure ML Gallery<\/a>. <\/p>\n\n<p>Quote from book:<\/p>\n\n<blockquote>\n  <p>Stacking and bagging can be easily implemented in Azure Machine\n  Learning, but other ensemble methods are more difficult. Also, it\n  turns out to be very tedious to implement in Azure Machine Learning an\n  ensemble of, say, more than five models. The experiment is filled with\n  modules and is quite difficult to maintain. Sometimes it is worthwhile\n  to use any ensemble method available in R or Python. Adding more\n  models to an ensemble written in a script can be as trivial as\n  changing a number in the code, instead of copying and pasting modules\n  into the experiment.<\/p>\n<\/blockquote>\n\n<p>You may also have a look at <a href=\"http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\" rel=\"nofollow noreferrer\">sklearn (Python)<\/a> and caret (R) documentation for further details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56364828",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1559897214220,
        "Question_original_content":"train model segment data set us help compani work differ busi predict compani level instead need predict busi busi roll result compani wide predict written model studio net work great coeffici determin busi need train model wai singl model have creat similar model help appreci kind regard martin",
        "Question_preprocessed_content":"train model segment us help compani work differ busi predict compani level instead need predict busi busi roll result compani wide predict written model work great coeffici determin busi need train model wai singl model have creat similar model help appreci kind regard martin",
        "Answer_original_content":"us ensembl combin model improv predict direct stack output model train entir dataset method think correspond best problem bag bootstrap aggreg need divid train set differ subset correspond certain busi train differ model subset combin result classifi wai boost difficult implement exampl galleri quot book stack bag easili implement azur machin learn ensembl method difficult turn tediou implement ensembl model experi fill modul difficult maintain worthwhil us ensembl method avail python ad model ensembl written script trivial chang number code instead copi past modul experi look sklearn python caret document detail",
        "Answer_preprocessed_content":"us ensembl combin model improv predict direct stack output model train entir dataset method think correspond best problem bag need divid train set differ subset train differ model subset combin result classifi wai boost difficult implement exampl galleri quot book stack bag easili implement azur machin learn ensembl method difficult turn tediou implement ensembl model experi fill modul difficult maintain worthwhil us ensembl method avail python ad model ensembl written script trivial chang number code instead copi past modul experi look sklearn caret document detail",
        "Question_gpt_summary_original":"the user martin is facing a challenge of training a model on segments of the data-set for the 52 different businesses of the company he works for, and is looking for a way to do this in a single ml model rather than having to create 52 very similar models.",
        "Question_gpt_summary":"user martin face challeng train model segment data set differ busi compani work look wai singl model have creat similar model",
        "Answer_gpt_summary_original":"possible solutions to the challenge of training a single ml model on segments of data for 52 different businesses are ensembles, such as stacking, bagging, and boosting. bagging is recommended for this specific problem, where the training set is divided into subsets for each business, and a different model is trained on each subset, and the results are combined. ensembles can be implemented in , but other methods may be more difficult and tedious to maintain. sklearn (python) and caret (r) documentation can provide further details.",
        "Answer_gpt_summary":"possibl solut challeng train singl model segment data differ busi ensembl stack bag boost bag recommend specif problem train set divid subset busi differ model train subset result combin ensembl implement method difficult tediou maintain sklearn python caret document provid detail"
    },
    {
        "Question_id":70335049.0,
        "Question_title":"Sagemaker Serverless Inference & custom container: Model archiver subprocess fails",
        "Question_body":"<p>I would like to host a model on Sagemaker using the new <a href=\"https:\/\/aws.amazon.com\/about-aws\/whats-new\/2021\/12\/amazon-sagemaker-serverless-inference\/?nc1=h_ls\" rel=\"nofollow noreferrer\">Serverless Inference<\/a>.<\/p>\n<p>I wrote my own container for inference and handler following several guides. These are the requirements:<\/p>\n<pre><code>mxnet\nmulti-model-server\nsagemaker-inference\nretrying\nnltk\ntransformers==4.12.4\ntorch==1.10.0\n<\/code><\/pre>\n<p>On non-serverless endpoints, this container works perfectly well. However, with the serverless version I get the following error message when loading the model:<\/p>\n<pre><code>ERROR - \/.sagemaker\/mms\/models\/model already exists.\n<\/code><\/pre>\n<p>The error is thrown by the following subprocess<\/p>\n<pre><code>['model-archiver', '--model-name', 'model', '--handler', '\/home\/model-server\/handler_service.py:handle', '--model-path', '\/opt\/ml\/model', '--export-path', '\/.sagemaker\/mms\/models', '--archive-format', 'no-archive']\n<\/code><\/pre>\n<p>So something that has to do with the <code>model-archiver<\/code> (which I guess is a process from the MMS package?).<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_creation_time":1639400254370,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":373.0,
        "Answer_body":"<p>So the issue really was related to hosting the model using the sagemaker inference toolkit and MMS which always uses the multi-model scenario which is not supported by serverless inference.<\/p>\n<p>I ended up writing my own Flask API which actually is nearly as easy and more customizable. Ping me for details if you're interested.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70335049",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1639671069487,
        "Question_original_content":"serverless infer custom contain model archiv subprocess fail like host model new serverless infer wrote contain infer handler follow guid requir mxnet multi model server infer retri nltk transform torch non serverless endpoint contain work perfectli serverless version follow error messag load model error mm model model exist error thrown follow subprocess model archiv model model handler home model server handler servic handl model path opt model export path mm model archiv format archiv model archiv guess process mm packag",
        "Question_preprocessed_content":"serverless infer custom contain model archiv subprocess fail like host model new serverless infer wrote contain infer handler follow guid requir endpoint contain work perfectli serverless version follow error messag load model error thrown follow subprocess",
        "Answer_original_content":"issu relat host model infer toolkit mm us multi model scenario support serverless infer end write flask api actual nearli easi customiz ping detail interest",
        "Answer_preprocessed_content":"issu relat host model infer toolkit mm us scenario support serverless infer end write flask api actual nearli easi customiz ping detail interest",
        "Question_gpt_summary_original":"the user encountered challenges with hosting a model on serverless inference, where their custom container worked perfectly well on non-serverless endpoints, but an error was thrown by the model-archiver subprocess when loading the model.",
        "Question_gpt_summary":"user encount challeng host model serverless infer custom contain work perfectli non serverless endpoint error thrown model archiv subprocess load model",
        "Answer_gpt_summary_original":"possible solutions: \n- write a custom flask api to host the model instead of using the inference toolkit and mms.\n- contact the person who provided the answer for more details on how to implement the custom flask api. \n\nsummary: the user can try writing a custom flask api to host the model instead of using the inference toolkit and mms, which may be more customizable. they can contact the person who provided the answer for more details on how to implement the custom flask api.",
        "Answer_gpt_summary":"possibl solut write custom flask api host model instead infer toolkit mm contact person provid answer detail implement custom flask api summari user try write custom flask api host model instead infer toolkit mm customiz contact person provid answer detail implement custom flask api"
    },
    {
        "Question_id":71351821.0,
        "Question_title":"Reading File from Vertex AI and Google Cloud Storage",
        "Question_body":"<p>I am trying to set up a pipeline in GCP\/Vertex AI and am having a lot of trouble. The pipeline is being written using Kubeflow Pipelines and has many different components, one thing in particular is giving me trouble however. Eventually I want to launch this from a Cloud Function with the help of the Cloud Scheduler.<\/p>\n<p>The part that is giving me issues is fairly simple and I believe I just need some form of introduction to how I should be thinking about this setup. I simply want to read and write from files (might be .csv, .txt or similar). I imagine that the analog to the filesystem on my local machine in GCP is the Cloud Storage so this is where I have been trying to read from for the time being (please correct me if I'm wrong). The component I've built is a blatant rip-off of <a href=\"https:\/\/stackoverflow.com\/questions\/48279061\/gcs-read-a-text-file-from-google-cloud-storage-directly-into-python\">this<\/a> post and looks like this.<\/p>\n<pre><code>@component(\n    packages_to_install=[&quot;google-cloud&quot;],\n    base_image=&quot;python:3.9&quot;\n)\n\n\ndef main(\n):\n    import csv\n    from io import StringIO\n\n    from google.cloud import storage\n\n    BUCKET_NAME = &quot;gs:\/\/my_bucket&quot;\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(BUCKET_NAME)\n\n    blob = bucket.blob('test\/test.txt')\n    blob = blob.download_as_string()\n    blob = blob.decode('utf-8')\n\n    blob = StringIO(blob)  #tranform bytes to string here\n\n    names = csv.reader(blob)  #then use csv library to read the content\n    for name in names:\n        print(f&quot;First Name: {name[0]}&quot;)\n<\/code><\/pre>\n<p>The error I'm getting looks like the following:<\/p>\n<pre><code>google.api_core.exceptions.NotFound: 404 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/gs:\/\/pipeline_dev?projection=noAcl&amp;prettyPrint=false: Not Found\n<\/code><\/pre>\n<p>What's going wrong in my brain? I get the feeling that it shouldn't be this difficult to read and write files. I must be missing something fundamental? Any help is highly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_creation_time":1646398826330,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1298.0,
        "Answer_body":"<p>Try specifying bucket name w\/o a gs:\/\/. This should fix the issue. One more stackoverflow post that says the same thing: <a href=\"https:\/\/stackoverflow.com\/questions\/53436615\/cloud-storage-python-client-fails-to-retrieve-bucket\">Cloud Storage python client fails to retrieve bucket<\/a><\/p>\n<p>any storage bucket you try to access in GCP has a unique address to access it. That address starts with a gs:\/\/ always which specifies that it is a cloud storage url. Now, GCS apis are designed such that they need the bucket name only to work with it. Hence, you just pass the bucket name. If you were accessing the bucket via browser you will need the complete address to access and hence the gs:\/\/ prefix as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71351821",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Answer_list":null,
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1646582537360,
        "Question_original_content":"read file googl cloud storag try set pipelin gcp have lot troubl pipelin written kubeflow pipelin differ compon thing particular give troubl eventu want launch cloud function help cloud schedul give issu fairli simpl believ need form introduct think setup simpli want read write file csv txt similar imagin analog filesystem local machin gcp cloud storag try read time correct wrong compon built blatant rip post look like compon packag instal googl cloud base imag python def main import csv import stringio googl cloud import storag bucket bucket storag client storag client bucket storag client bucket bucket blob bucket blob test test txt blob blob download string blob blob decod utf blob stringio blob tranform byte string name csv reader blob us csv librari read content name print error get look like follow googl api core except notfound http storag googleapi com storag pipelin dev project noacl prettyprint fals go wrong brain feel shouldn difficult read write file miss fundament help highli appreci",
        "Question_preprocessed_content":"read file googl cloud storag try set pipelin gcp have lot troubl pipelin written kubeflow pipelin differ compon thing particular give troubl eventu want launch cloud function help cloud schedul give issu fairli simpl believ need form introduct think setup simpli want read write file imagin analog filesystem local machin gcp cloud storag try read time compon built blatant post look like error get look like follow go wrong brain feel shouldn difficult read write file miss fundament help highli appreci",
        "Answer_original_content":"try specifi bucket fix issu stackoverflow post sai thing cloud storag python client fail retriev bucket storag bucket try access gcp uniqu address access address start specifi cloud storag url gc api design need bucket work pass bucket access bucket browser need complet address access prefix",
        "Answer_preprocessed_content":"try specifi bucket fix issu stackoverflow post sai thing cloud storag python client fail retriev bucket storag bucket try access gcp uniqu address access address start specifi cloud storag url gc api design need bucket work pass bucket access bucket browser need complet address access prefix",
        "Question_gpt_summary_original":"the user is encountering challenges with setting up a pipeline in gcp, specifically with reading and writing files from google cloud storage.",
        "Question_gpt_summary":"user encount challeng set pipelin gcp specif read write file googl cloud storag",
        "Answer_gpt_summary_original":"the solution to the challenge of setting up a pipeline in gcp for reading and writing files from google cloud storage is to specify the bucket name without using the gs:\/\/ prefix. this is because gcs apis only require the bucket name to work with it, while the complete address is needed only when accessing the bucket via a browser.",
        "Answer_gpt_summary":"solut challeng set pipelin gcp read write file googl cloud storag specifi bucket prefix gc api requir bucket work complet address need access bucket browser"
    },
    {
        "Question_id":null,
        "Question_title":"Traceback error",
        "Question_body":"<p>Hey guys,<\/p>\n<p>I am totally new to W&amp;B. I am getting a Traceback error when I want to run \u201cwandb.init(project=\u201d\u2026\u201c)\u201d. Last week it still did work. Any tips what to do?? Thank you so much.<\/p>\n<pre><code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py\", line 999, in init\n    run = wi.init()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py\", line 651, in init\n    backend.cleanup()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/backend\/backend.py\", line 246, in cleanup\n    self.interface.join()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 475, in join\n    super().join()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface.py\", line 666, in join\n    _ = self._communicate_shutdown()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 472, in _communicate_shutdown\n    _ = self._communicate(record)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 226, in _communicate\n    return self._communicate_async(rec, local=local).get(timeout=timeout)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py\", line 231, in _communicate_async\n    raise Exception(\"The wandb backend process has shutdown\")\nException: The wandb backend process has shutdown\nwandb: ERROR Abnormal program exit\n---------------------------------------------------------------------------\nException                                 Traceback (most recent call last)\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n    998         try:\n--&gt; 999             run = wi.init()\n   1000             except_exit = wi.settings._except_exit\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py in init(self)\n    650                     # we don't need to do console cleanup at this point\n--&gt; 651                     backend.cleanup()\n    652                     self.teardown()\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/backend\/backend.py in cleanup(self)\n    245         if self.interface:\n--&gt; 246             self.interface.join()\n    247         if self.wandb_process:\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in join(self)\n    474     def join(self) -&gt; None:\n--&gt; 475         super().join()\n    476 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface.py in join(self)\n    665             return\n--&gt; 666         _ = self._communicate_shutdown()\n    667 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in _communicate_shutdown(self)\n    471         record = self._make_record(request=request)\n--&gt; 472         _ = self._communicate(record)\n    473 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in _communicate(self, rec, timeout, local)\n    225     ) -&gt; Optional[pb.Result]:\n--&gt; 226         return self._communicate_async(rec, local=local).get(timeout=timeout)\n    227 \n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/interface\/interface_shared.py in _communicate_async(self, rec, local)\n    230         if self._process_check and self._process and not self._process.is_alive():\n--&gt; 231             raise Exception(\"The wandb backend process has shutdown\")\n    232         future = self._router.send_and_receive(rec, local=local)\n\nException: The wandb backend process has shutdown\n\nThe above exception was the direct cause of the following exception:\n\nException                                 Traceback (most recent call last)\n&lt;ipython-input-49-e3734aa09c65&gt; in &lt;module&gt;\n      1 #Login to wandb\n      2 # #! wandb login config_dict[\"wandb_key\"]\n----&gt; 3 wandb.init()\n      4 #run_name = wandb.run.name\n\n~\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py in init(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\n   1035             if except_exit:\n   1036                 os._exit(-1)\n-&gt; 1037             raise Exception(\"problem\") from error_seen\n   1038     return run\n\nException: problem\n<\/code><\/pre>",
        "Question_answer_count":7,
        "Question_comment_count":0,
        "Question_creation_time":1661465657943,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1364.0,
        "Answer_body":null,
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/community.wandb.ai\/t\/traceback-error\/3008",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2022-08-29T23:11:28.940Z",
                "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/pthielge\">@pthielge<\/a>!<\/p>\n<p>Thanks for writing in, and I am sorry you are facing issues over here. There could be a few issues that could cause this error to show up in your program. Could you share some more information with me so that we could narrow down the scope of this error?<\/p>\n<ul>\n<li>What version of <code>wandb<\/code> are you using?<\/li>\n<li>What operating system are you running this script on?<\/li>\n<li>There should be a folder called <code>wandb<\/code> in your working directory with multiple sub-folders of the format <code>run-&lt;DATETIME&gt;-&lt;ID&gt;<\/code>, could you share the <code>debug.log<\/code> and <code>debug-internal.log<\/code> files from the folder corresponding to this run ID?<\/li>\n<\/ul>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":38.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-08-30T08:53:15.297Z",
                "Answer_body":"<p>Hi Ramit,<\/p>\n<p>thanks for your reply.  I\u2019m using version \u20180.13.2\u2019 in JupyterLab in Windows 11. Somehow, there is only a debug.log and no debug-internal.log file, so I will append it here.<\/p>\n<p>Thanks,<\/p>\n<p>Philip<\/p>\n<pre><code class=\"lang-auto\">2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_setup.py:_flush():76] Configure stats pid to 420\n2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_setup.py:_flush():76] Loading settings from \/home\/p\/pthielge\/.config\/wandb\/settings\n2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_setup.py:_flush():76] Loading settings from \/home\/p\/pthielge\/3D\/S2_Top\/wandb\/settings\n2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_setup.py:_flush():76] Loading settings from environment variables: {'_require_service': 'True'}\n2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_setup.py:_flush():76] Inferring run settings from compute environment: {'program': '&lt;python with no main file&gt;'}\n2022-08-30 10:44:49,612 INFO    MainThread:420 [wandb_init.py:_log_setup():461] Logging user logs to \/home\/p\/pthielge\/3D\/S2_Top\/wandb\/run-20220830_104449-5v6f9ano\/logs\/debug.log\n2022-08-30 10:44:49,613 INFO    MainThread:420 [wandb_init.py:_log_setup():462] Logging internal logs to \/home\/p\/pthielge\/3D\/S2_Top\/wandb\/run-20220830_104449-5v6f9ano\/logs\/debug-internal.log\n2022-08-30 10:44:49,613 INFO    MainThread:420 [wandb_init.py:init():495] calling init triggers\n2022-08-30 10:44:49,613 INFO    MainThread:420 [wandb_init.py:init():499] wandb.init called with sweep_config: {}\nconfig: {}\n2022-08-30 10:44:49,613 INFO    MainThread:420 [wandb_init.py:init():548] starting backend\n2022-08-30 10:44:49,613 INFO    MainThread:420 [wandb_init.py:init():552] setting up manager\n2022-08-30 10:44:49,631 ERROR   MainThread:420 [wandb_init.py:init():1070] error\nTraceback (most recent call last):\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py\", line 1043, in init\n    run = wi.init()\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_init.py\", line 553, in init\n    manager._inform_init(settings=self.settings, run_id=self.settings.run_id)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/wandb_manager.py\", line 161, in _inform_init\n    svc_iface._svc_inform_init(settings=settings, run_id=run_id)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/service\/service_sock.py\", line 39, in _svc_inform_init\n    self._sock_client.send(inform_init=inform_init)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 140, in send\n    self.send_server_request(server_req)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 84, in send_server_request\n    self._send_message(msg)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 81, in _send_message\n    self._sendall_with_error_handle(header + data)\n  File \"\/home\/p\/pthielge\/.local\/lib\/python3.6\/site-packages\/wandb\/sdk\/lib\/sock_client.py\", line 61, in _sendall_with_error_handle\n    sent = self._sock.send(data[total_sent:])\nBrokenPipeError: [Errno 32] Broken pipe\n<\/code><\/pre>",
                "Answer_score":72.8,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-14T16:28:02.005Z",
                "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/pthielge\">@pthielge<\/a>,<\/p>\n<p>Thanks for the logs! Is this an error that you see consistently across the runs or is this just a one-off event? Looks like the communication we set up between your program and our server was dropped, so I don\u2019t expect this to happen consistently.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":62.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-20T17:07:33.911Z",
                "Answer_body":"<p>\u200bHi <a class=\"mention\" href=\"\/u\/pthielge\">@pthielge<\/a>,<\/p>\n<p>We wanted to follow up with you regarding your support request as we have not heard back from you. Please let us know if we can be of further assistance or if your issue has been resolved.<\/p>",
                "Answer_score":12.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-20T17:23:57.681Z",
                "Answer_body":"<p>Hi,<\/p>\n<p>thanks for your reply <a class=\"mention\" href=\"\/u\/ramit_goolry\">@ramit_goolry<\/a>. It seems you\u2019re right, some temporary connection problem. I had this issue for 2 days, and then suddenly it was no longer there, without changing the code.<\/p>\n<p>Thank you so much. Best regards!<\/p>",
                "Answer_score":27.2,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-09-20T17:28:03.479Z",
                "Answer_body":"<p>Got it! In that case, I\u2019ll go ahead and close out this support request. In case this issue shows up again, please let us know!<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
                "Answer_score":37.0,
                "Answer_has_accepted":false
            },
            {
                "Answer_creation_time":"2022-11-19T17:28:32.595Z",
                "Answer_body":"<p>This topic was automatically closed 60 days after the last reply. New replies are no longer allowed.<\/p>",
                "Answer_score":1.0,
                "Answer_has_accepted":false
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":null,
        "Question_original_content":"traceback error hei gui total new get traceback error want run init project week work tip thank traceback recent file home pthielg local lib python site packag sdk init line init run init file home pthielg local lib python site packag sdk init line init backend cleanup file home pthielg local lib python site packag sdk backend backend line cleanup self interfac join file home pthielg local lib python site packag sdk interfac interfac share line join super join file home pthielg local lib python site packag sdk interfac interfac line join self commun shutdown file home pthielg local lib python site packag sdk interfac interfac share line commun shutdown self commun record file home pthielg local lib python site packag sdk interfac interfac share line commun return self commun async rec local local timeout timeout file home pthielg local lib python site packag sdk interfac interfac share line commun async rais except backend process shutdown except backend process shutdown error abnorm program exit except traceback recent local lib python site packag sdk init init job type dir config project entiti reinit tag group note magic config exclud kei config includ kei anonym mode allow val chang resum forc tensorboard sync tensorboard monitor gym save code set try run init exit set exit local lib python site packag sdk init init self need consol cleanup point backend cleanup self teardown local lib python site packag sdk backend backend cleanup self self interfac self interfac join self process local lib python site packag sdk interfac interfac share join self def join self super join local lib python site packag sdk interfac interfac join self return self commun shutdown local lib python site packag sdk interfac interfac share commun shutdown self record self record request request self commun record local lib python site packag sdk interfac interfac share commun self rec timeout local option result return self commun async rec local local timeout timeout local lib python site packag sdk interfac interfac share commun async self rec local self process check self process self process aliv rais except backend process shutdown futur self router send receiv rec local local except backend process shutdown except direct caus follow except except traceback recent login login config dict kei init run run local lib python site packag sdk init init job type dir config project entiti reinit tag group note magic config exclud kei config includ kei anonym mode allow val chang resum forc tensorboard sync tensorboard monitor gym save code set exit exit rais except problem error seen return run except problem",
        "Question_preprocessed_content":"traceback error hei gui total new get traceback error want run init week work tip thank",
        "Answer_original_content":"pthielg thank write sorri face issu issu caus error program share inform narrow scope error version oper run script folder call work directori multipl sub folder format run share debug log debug intern log file folder correspond run thank ramit ramit thank repli version jupyterlab window debug log debug intern log file append thank philip info mainthread setup flush configur stat pid info mainthread setup flush load set home pthielg config set info mainthread setup flush load set home pthielg set info mainthread setup flush load set environ variabl requir servic true info mainthread setup flush infer run set comput environ program info mainthread init log setup log user log home pthielg run vfano log debug log info mainthread init log setup log intern log home pthielg run vfano log debug intern log info mainthread init init call init trigger info mainthread init init init call sweep config config info mainthread init init start backend info mainthread init init set manag error mainthread init init error traceback recent file home pthielg local lib python site packag sdk init line init run init file home pthielg local lib python site packag sdk init line init manag inform init set self set run self set run file home pthielg local lib python site packag sdk manag line inform init svc ifac svc inform init set set run run file home pthielg local lib python site packag sdk servic servic sock line svc inform init self sock client send inform init inform init file home pthielg local lib python site packag sdk lib sock client line send self send server request server req file home pthielg local lib python site packag sdk lib sock client line send server request self send messag msg file home pthielg local lib python site packag sdk lib sock client line send messag self sendal error handl header data file home pthielg local lib python site packag sdk lib sock client line sendal error handl sent self sock send data total sent brokenpipeerror errno broken pipe hei pthielg thank log error consist run event look like commun set program server drop dont expect happen consist thank ramit pthielg want follow support request heard let know assist issu resolv thank repli ramit goolri your right temporari connect problem issu dai suddenli longer chang code thank best regard got case ill ahead close support request case issu show let know thank ramit topic automat close dai repli new repli longer allow",
        "Answer_preprocessed_content":"thank write sorri face issu issu caus error program share inform narrow scope error version oper run script folder call work directori multipl format share file folder correspond run thank ramit ramit thank repli version jupyterlab window file append thank philip hei thank log error consist run event look like commun set program server drop dont expect happen consist thank ramit want follow support request heard let know assist issu resolv thank repli your right temporari connect problem issu dai suddenli longer chang code thank best regard got case ill ahead close support request case issu show let know thank ramit topic automat close dai repli new repli longer allow",
        "Question_gpt_summary_original":"the user is encountering a traceback error when attempting to run \".init(project=\u201d\u2026\u201c)\u201d, which was working last week.",
        "Question_gpt_summary":"user encount traceback error attempt run init project work week",
        "Answer_gpt_summary_original":"there are no specific solutions mentioned in the answer. the support team is asking for more information to narrow down the scope of the error. they suggest sharing the debug.log and debug-internal.log files from the folder corresponding to the run id. the error seems to be a broken communication between the program and the server, which is a temporary issue and not expected to happen consistently. the support team suggests reaching out again if the issue persists.",
        "Answer_gpt_summary":"specif solut mention answer support team ask inform narrow scope error suggest share debug log debug intern log file folder correspond run error broken commun program server temporari issu expect happen consist support team suggest reach issu persist"
    },
    {
        "Question_id":null,
        "Question_title":"SageMaker MXNet local mode not working",
        "Question_body":"Hi, I am trying to fit an MXNet model locally. I am adapting this https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/ and doing the following:\n\nbucket = 'XXXXXXXXXXX'\nprefix = 'sagemaker\/cifar-bench\/data'\n\ninputs = sagemaker_session.upload_data(\n    path='data',\n    bucket=bucket, \n    key_prefix=prefix)\n\nprint('data sent to ' + inputs)\n\n\nInception = MXNet('gluon_cifar_net.py', \n          role=role, \n          train_instance_count=1, \n          train_instance_type='local_gpu',\n          framework_version='1.2.1',\n          base_job_name='cifar10-inception-',\n          hyperparameters={'batch_size': 256, \n                           'optimizer': 'sgd',\n                           'epochs': 100, \n                           'learning_rate': 0.1, \n                           'momentum': 0.9})\n\n\nInception.fit(inputs)\n\n\nwhich returns an OSError: [Errno 2] No such file or directory\n\nIn the error log I can see that there seems to be error at self.latest_training_job = _TrainingJob.start_new(self, inputs) and self.sagemaker_client.create_training_job(**train_request)\n\nHow can I make the local mode work?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_creation_time":1537534843000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":186.0,
        "Answer_body":"It is very likely that you don't have docker-compose (or docker) installed in the box, that is why you are getting a No such file or directory.\n\nIf you want to use the GPU setup I would recommend running on a sagemaker notebook instance. Navigate to one of the example notebooks such as: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb\n\nAnd run the setup.sh cell. This will install and configure all the docker dependencies correctly and then you should be able to use MXNet locally on GPU without any issue.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":null,
        "Question_link":"https:\/\/repost.aws\/questions\/QUQu1fDak6RL2wmivZ5UJwUw\/sage-maker-mx-net-local-mode-not-working",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Answer_list":[
            {
                "Answer_creation_time":"2018-09-21T17:24:55.000Z",
                "Answer_score":0,
                "Answer_body":"It is very likely that you don't have docker-compose (or docker) installed in the box, that is why you are getting a No such file or directory.\n\nIf you want to use the GPU setup I would recommend running on a sagemaker notebook instance. Navigate to one of the example notebooks such as: https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb\n\nAnd run the setup.sh cell. This will install and configure all the docker dependencies correctly and then you should be able to use MXNet locally on GPU without any issue.",
                "Answer_has_accepted":true
            }
        ],
        "Question_follower_count":null,
        "Question_converted_from_issue":null,
        "Question_closed_time":1537550695000,
        "Question_original_content":"mxnet local mode work try fit mxnet model local adapt http aw amazon com blog machin learn us amazon local mode train notebook instanc follow bucket prefix cifar bench data input session upload data path data bucket bucket kei prefix prefix print data sent input incept mxnet gluon cifar net role role train instanc count train instanc type local gpu framework version base job cifar incept hyperparamet batch size optim sgd epoch learn rate momentum incept fit input return oserror errno file directori error log error self latest train job trainingjob start new self input self client creat train job train request local mode work",
        "Question_preprocessed_content":"mxnet local mode work try fit mxnet model local adapt follow bucket prefix input path data bucket bucket print incept role role optim sgd epoch momentum return oserror file directori error log error input local mode work",
        "Answer_original_content":"like docker compos docker instal box get file directori want us gpu setup recommend run notebook instanc navig exampl notebook http github com awslab amazon exampl blob master python sdk mxnet gluon cifar mxnet cifar local mode ipynb run setup cell instal configur docker depend correctli abl us mxnet local gpu issu",
        "Answer_preprocessed_content":"like instal box get file directori want us gpu setup recommend run notebook instanc navig exampl notebook run cell instal configur docker depend correctli abl us mxnet local gpu issu",
        "Question_gpt_summary_original":"the user is encountering challenges with running an mxnet model locally, resulting in an oserror due to an issue with the self.latest_training_job and self._client.create_training_job.",
        "Question_gpt_summary":"user encount challeng run mxnet model local result oserror issu self latest train job self client creat train job",
        "Answer_gpt_summary_original":"possible solutions to the challenge of encountering an oserror due to an issue with self.latest_training_job and self._client.create_training_job while running an mxnet model locally are: \n1. install docker-compose (or docker) in the box.\n2. use a notebook instance to run the mxnet model on gpu.\n3. navigate to an example notebook such as https:\/\/github.com\/awslabs\/amazon--examples\/blob\/master\/-python-sdk\/mxnet_gluon_cifar10\/mxnet_cifar10_local_mode.ipynb and run the setup.sh cell to install and configure all the docker dependencies correctly.",
        "Answer_gpt_summary":"possibl solut challeng encount oserror issu self latest train job self client creat train job run mxnet model local instal docker compos docker box us notebook instanc run mxnet model gpu navig exampl notebook http github com awslab amazon exampl blob master python sdk mxnet gluon cifar mxnet cifar local mode ipynb run setup cell instal configur docker depend correctli"
    }
]