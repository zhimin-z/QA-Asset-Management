[
    {
        "Question_title":"R model deployment with custom Docker image: \"ModuleNotFoundError: No module named 'azureml.api'\"",
        "Question_body":"I am trying to deploy an R inference script to Azure ML Service Endpoint as an Azure Container Instance. I have made the following steps:\n\ncreated a custom Docker image from scratch and pushed it to the Azure Container Registry (associated with AML Workspace)\n\n\nregistered a custom environment in AML Workspace, based on the image in ACR\n\n\ndeployed R entry script (just a simple hello world script with init() and run() functions defined)\n\n\nthe inference configuration uses the custom AML environment\n\n\ndeployment is made with Azure ML R SDK\n\nThe container instance is created, but the endpoint startup runs into error. Here is the output from the container instance:\n\n 2020-10-16T12:56:21,639812796+00:00 - gunicorn\/run \n 2020-10-16T12:56:21,639290594+00:00 - iot-server\/run \n 2020-10-16T12:56:21,640405198+00:00 - rsyslog\/run \n 2020-10-16T12:56:21,735291424+00:00 - nginx\/run \n EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n 2020-10-16T12:56:23,736657191+00:00 - iot-server\/finish 1 0\n 2020-10-16T12:56:23,834747728+00:00 - Exit code 1 is normal. Not restarting iot-server.\n Starting gunicorn 20.0.4\n Listening at: http:\/\/127.0.0.1:31311 (11)\n Using worker: sync\n worker timeout is set to 300\n Booting worker with pid: 38\n \/bin\/bash: \/root\/miniconda3\/lib\/libtinfo.so.6: no version information available (required by \/bin\/bash)\n SPARK_HOME not set. Skipping PySpark Initialization.\n Exception in worker process\n Traceback (most recent call last):\n   File \"\/var\/azureml-server\/app.py\", line 43, in <module>\n     from azureml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azureml.api'\n    \n During handling of the above exception, another exception occurred:\n    \n Traceback (most recent call last):\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/arbiter.py\", line 583, in spawn_worker\n     worker.init_process()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py\", line 119, in init_process\n     self.load_wsgi()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/workers\/base.py\", line 144, in load_wsgi\n     self.wsgi = self.app.wsgi()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/base.py\", line 67, in wsgi\n     self.callable = self.load()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py\", line 49, in load\n     return self.load_wsgiapp()\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/app\/wsgiapp.py\", line 39, in load_wsgiapp\n     return util.import_app(self.app_uri)\n   File \"\/usr\/lib\/python3\/dist-packages\/gunicorn\/util.py\", line 383, in import_app\n     mod = importlib.import_module(module)\n   File \"\/usr\/lib\/python3.8\/importlib\/__init__.py\", line 127, in import_module\n     return _bootstrap._gcd_import(name[level:], package, level)\n   File \"<frozen importlib._bootstrap>\", line 1014, in _gcd_import\n   File \"<frozen importlib._bootstrap>\", line 991, in _find_and_load\n   File \"<frozen importlib._bootstrap>\", line 975, in _find_and_load_unlocked\n   File \"<frozen importlib._bootstrap>\", line 671, in _load_unlocked\n   File \"<frozen importlib._bootstrap_external>\", line 783, in exec_module\n   File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n   File \"\/var\/azureml-server\/wsgi.py\", line 1, in <module>\n     import create_app\n   File \"\/var\/azureml-server\/create_app.py\", line 3, in <module>\n     from app import main\n   File \"\/var\/azureml-server\/app.py\", line 45, in <module>\n     from azure.ml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azure.ml'\n Worker exiting (pid: 38)\n Shutting down: Master\n Reason: Worker failed to boot.\n 2020-10-16T12:56:39,434787859+00:00 - gunicorn\/finish 3 0\n 2020-10-16T12:56:39,435715063+00:00 - Exit code 3 is not normal. Killing image.\n\n\n\nHow do I install the azureml.api dependency, which can not be found? It doesn't seem to be part of the Azure ML SDK. I have installed the following dependencies in my Dockerfile:\n\n RUN apt-get -y install python3-flask python3-rpy2 python3-azure python3-applicationinsights\n RUN pip install azureml-core\n\n\n\nI also have Miniconda installed. Pip refers to Miniconda's pip.\n\nOr, is this dependency available to install at all? Should I use some pre-defined AML environment as the base Docker image? (Note: I am currently using bare FROM: ubuntu). Suggestions how to find and use the base images are also welcome, since this is not documented very well.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1602855221470,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@LauriLehman-8626 Thanks for the question. Here is the document to Create Custom Docker Base Images for Azure Machine Learning Environments for R people.\nWe have used the AzureML RScriptStep pipeline feature which allows you to point to CRAN or Github or custom URLS, but this requires authoring the pipeline in python or YAML.. In R You can also these arguments in the Azuremlsdk R estimator function: https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/estimator.html\nAnother option that are not available through conda install as part of the R script with install.packages(\u201cpath\/*.tar.gz\u201d, repos=NULL))\n\nOne of the challenges is that the build at runtime can take a while to prepare the environment. R likes to compile packages on Linux environments and a large package could have lots of dependencies which would take a while. This is an R on Linux\/PaaS thing, rather than specific to AzureML\n\nTo make start up fast we created a custom docker image where you can tightly control the image ahead of runtime. If you want to go in this direction you can find an example Dockerfile to get you started here..\nhttps:\/\/github.com\/Azure\/azureml-sdk-for-r\/blob\/master\/.azure-pipelines\/docker\/Dockerfile",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/129038\/r-model-deployment-with-custom-docker-image-34modu.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1603104240927,
        "Question_original_content":"model deploy custom docker imag modulenotfounderror modul name api try deploi infer script servic endpoint azur contain instanc follow step creat custom docker imag scratch push azur contain registri associ aml workspac regist custom environ aml workspac base imag acr deploi entri script simpl hello world script init run function defin infer configur us custom aml environ deploy sdk contain instanc creat endpoint startup run error output contain instanc gunicorn run iot server run rsyslog run nginx run edgehubconnectionstr iotedg iothubhostnam set exit iot server finish exit code normal restart iot server start gunicorn listen http worker sync worker timeout set boot worker pid bin bash root miniconda lib libtinfo version inform avail requir bin bash spark home set skip pyspark initi except worker process traceback recent file var server app line api except clientsideexcept import clientsideexcept modulenotfounderror modul name api handl except except occur traceback recent file usr lib python dist packag gunicorn arbit line spawn worker worker init process file usr lib python dist packag gunicorn worker base line init process self load wsgi file usr lib python dist packag gunicorn worker base line load wsgi self wsgi self app wsgi file usr lib python dist packag gunicorn app base line wsgi self callabl self load file usr lib python dist packag gunicorn app wsgiapp line load return self load wsgiapp file usr lib python dist packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file usr lib python dist packag gunicorn util line import app mod importlib import modul modul file usr lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file var server wsgi line import creat app file var server creat app line app import main file var server app line azur api except clientsideexcept import clientsideexcept modulenotfounderror modul name azur worker exit pid shut master reason worker fail boot gunicorn finish exit code normal kill imag instal api depend sdk instal follow depend dockerfil run apt instal python flask python rpy python azur python applicationinsight run pip instal core miniconda instal pip refer miniconda pip depend avail instal us pre defin aml environ base docker imag note current bare ubuntu suggest us base imag welcom document",
        "Question_preprocessed_content":"model deploy custom docker imag modulenotfounderror modul name try deploi infer script servic endpoint azur contain instanc follow step creat custom docker imag scratch push azur contain registri regist custom environ aml workspac base imag acr deploi entri script run function defin infer configur us custom aml environ deploy sdk contain instanc creat endpoint startup run error output contain instanc edgehubconnectionstr set exit code normal restart start gunicorn listen worker sync worker timeout set boot worker pid version inform avail set skip pyspark initi except worker process traceback file line import clientsideexcept modulenotfounderror modul name handl except except occur traceback file line file line file line file line wsgi file line load return file line return file line mod file line return packag level file line file line file line file line file line file line file line import file line app import main file line import clientsideexcept modulenotfounderror modul name worker exit shut master reason worker fail boot exit code normal kill imag instal api depend sdk instal follow depend dockerfil run instal run pip instal core miniconda instal pip refer miniconda pip depend avail instal us aml environ base docker imag suggest us base imag welcom document",
        "Question_gpt_summary_original":"The user is encountering an error while trying to deploy an R inference script to Azure ML Service Endpoint as an Azure Container Instance. The error message indicates that the azureml.api dependency cannot be found. The user has installed several dependencies in their Dockerfile and is using a custom Docker image, but is unsure how to install the missing dependency or if it is available to install at all. The user is also considering using a pre-defined AML environment as the base Docker image but is unsure how to find and use them.",
        "Question_gpt_summary":"user encount error try deploi infer script servic endpoint azur contain instanc error messag indic api depend user instal depend dockerfil custom docker imag unsur instal miss depend avail instal user consid pre defin aml environ base docker imag unsur us",
        "Answer_original_content":"laurilehman thank question document creat custom docker base imag environ peopl rscriptstep pipelin featur allow point cran github custom url requir author pipelin python yaml argument sdk estim function http azur github sdk refer estim html option avail conda instal script instal packag path tar repo null challeng build runtim prepar environ like compil packag linux environ larg packag lot depend linux paa thing specif start fast creat custom docker imag tightli control imag ahead runtim want direct exampl dockerfil start http github com azur sdk blob master azur pipelin docker dockerfil",
        "Answer_preprocessed_content":"thank question document creat custom docker base imag environ peopl rscriptstep pipelin featur allow point cran github custom url requir author pipelin python argument sdk estim function option avail conda instal script repo null challeng build runtim prepar environ like compil packag linux environ larg packag lot depend thing specif start fast creat custom docker imag tightli control imag ahead runtim want direct exampl dockerfil start",
        "Answer_gpt_summary_original":"Solutions provided in the discussion include using the AzureML RScriptStep pipeline feature, using the Azuremlsdk R estimator function, and installing packages using install.packages(). The user can also create a custom Docker image to control the environment ahead of runtime. The discussion also notes that the build at runtime can take a while due to R compiling packages on Linux environments with many dependencies. No personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"solut provid discuss includ rscriptstep pipelin featur sdk estim function instal packag instal packag user creat custom docker imag control environ ahead runtim discuss note build runtim compil packag linux environ depend person opinion bias includ summari"
    },
    {
        "Question_title":"VertexAI Pipelines: The provided location ID doesn't match the endpoint",
        "Question_body":"<p>I have successfully run the following pipeline that creates a dataset, trains a model, and deploys to an endpoint using VertexAIs pipeline tool when everything is based in us-central1. Now, when I change the region to europe-west2, I get the following error:<\/p>\n<pre><code>debug_error_string = &quot;{&quot;created&quot;:&quot;@1647430410.324290053&quot;,&quot;description&quot;:&quot;Error received from peer\nipv4:172.217.169.74:443&quot;,&quot;file&quot;:&quot;src\/core\/lib\/surface\/call.cc&quot;,&quot;file_line&quot;:1066,\n&quot;grpc_message&quot;:&quot;List of found errors:\\t1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\\t&quot;,&quot;grpc_status&quot;:3}&quot;\n<\/code><\/pre>\n<p>This error occurs after the dataset is created in europe-west2, and before the model starts to train. Here is my code:<\/p>\n<pre><code>#import libraries\nfrom typing import NamedTuple\nimport kfp\nfrom kfp import dsl\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import (Artifact, Input, InputPath, Model, Output, \n                        OutputPath, ClassificationMetrics, Metrics, component)\nfrom kfp.v2.components.types.artifact_types import Dataset\nfrom kfp.v2.google.client import AIPlatformClient\nfrom google.cloud import aiplatform\nfrom google_cloud_pipeline_components import aiplatform as gcc_aip\nfrom google.api_core.exceptions import NotFound\n\n@kfp.dsl.pipeline(name=f&quot;lookalike-model-training-v2&quot;,\n                  pipeline_root=PIPELINE_ROOT)\ndef pipeline(\n    bq_source: str = f&quot;bq:\/\/{PROJECT_ID}.{DATASET_ID}.{TABLE_NAME}&quot;,\n    display_name: str = DISPLAY_NAME,\n    project: str = PROJECT_ID,\n    gcp_region: str = &quot;europe-west2&quot;,\n    api_endpoint: str = &quot;europe-west2-aiplatform.googleapis.com&quot;,\n    thresholds_dict_str: str = '{&quot;auPrc&quot;: 0.5}',\n):\n            \n    dataset_create_op = gcc_aip.TabularDatasetCreateOp(\n        project=project,\n        display_name=display_name, \n        bq_source=bq_source,\n        location = gcp_region\n    )\n\n    training_op = gcc_aip.AutoMLTabularTrainingJobRunOp(\n        project=project,\n        display_name=display_name,\n        optimization_prediction_type=&quot;classification&quot;,\n        budget_milli_node_hours=1000,\n        location=gcp_region,\n        predefined_split_column_name=&quot;set&quot;,\n        column_transformations=[\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;agentId&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;postcode&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;isMobile&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;gender&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;timeOfDay&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;set&quot;}},\n            {&quot;categorical&quot;: {&quot;column_name&quot;: &quot;sale&quot;}},\n        ],\n        dataset=dataset_create_op.outputs[&quot;dataset&quot;],\n        target_column=&quot;sale&quot;,\n    )\n\ncompiler.Compiler().compile(\n    pipeline_func=pipeline, package_path=&quot;tab_classif_pipeline.json&quot;\n)\n\nml_pipeline_job = aiplatform.PipelineJob(\n    display_name=f&quot;{MODEL_PREFIX}_training&quot;,\n    template_path=&quot;tab_classif_pipeline.json&quot;,\n    pipeline_root=PIPELINE_ROOT,\n    parameter_values={&quot;project&quot;: PROJECT_ID, &quot;display_name&quot;: DISPLAY_NAME},\n    enable_caching=True,\n    location=&quot;europe-west2&quot;\n)\nml_pipeline_job.submit()\n<\/code><\/pre>\n<p>As previously mentioned, the dataset gets created so I suspect that the issue must lie in <code>training_op = gcc_aip.AutoMLTabularTrainingJobRunOp<\/code><\/p>\n<p>I tried providing another endpoint: <code>eu-aiplatform.googleapis.com<\/code> which yielded the following error:<\/p>\n<pre><code>google.api_core.exceptions.InvalidArgument: 400 List of found errors: 1.Field: name; Message: \nThe provided location ID doesn't match the endpoint. The valid location ID is `us-central1`.\n\nFail to send metric: [rpc error: code = PermissionDenied desc = Permission monitoring.metricDescriptors.create \ndenied (or the resource may not exist).; rpc error: code = PermissionDenied desc = Permission monitoring.timeSeries.create\n denied (or the resource may not exist).]\n<\/code><\/pre>\n<p>I understand that I am not passing api-endpoint to any of the methods above, but I thought I'd highlight that the error changed slightly.<\/p>\n<p>Does anyone know what the issue may be? Or how I can run <code>gcc_aip.AutoMLTabularTrainingJobRunOp<\/code> in europe-west2 (or EU in general)?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1647433181040,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":220.0,
        "Answer_body":"<p>Try Updating the pipeline component using the command:<\/p>\n<p><code>pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71496966",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1649155832383,
        "Question_original_content":"vertexai pipelin provid locat match endpoint successfulli run follow pipelin creat dataset train model deploi endpoint vertexai pipelin tool base central chang region europ west follow error debug error string creat descript error receiv peer ipv file src core lib surfac file line grpc messag list error field messag provid locat match endpoint valid locat central grpc statu error occur dataset creat europ west model start train code import librari type import namedtupl import kfp kfp import dsl kfp import compil kfp dsl import artifact input inputpath model output outputpath classificationmetr metric compon kfp compon type artifact type import dataset kfp googl client import aiplatformcli googl cloud import aiplatform googl cloud pipelin compon import aiplatform gcc aip googl api core except import notfound kfp dsl pipelin lookalik model train pipelin root pipelin root def pipelin sourc str project dataset tabl displai str displai project str project gcp region str europ west api endpoint str europ west aiplatform googleapi com threshold dict str str auprc dataset creat gcc aip tabulardatasetcreateop project project displai displai sourc sourc locat gcp region train gcc aip automltabulartrainingjobrunop project project displai displai optim predict type classif budget milli node hour locat gcp region predefin split column set column transform categor column agentid categor column postcod categor column ismobil categor column gender categor column timeofdai categor column set categor column sale dataset dataset creat output dataset target column sale compil compil compil pipelin func pipelin packag path tab classif pipelin json pipelin job aiplatform pipelinejob displai model prefix train templat path tab classif pipelin json pipelin root pipelin root paramet valu project project displai displai enabl cach true locat europ west pipelin job submit previous mention dataset get creat suspect issu lie train gcc aip automltabulartrainingjobrunop tri provid endpoint aiplatform googleapi com yield follow error googl api core except invalidargu list error field messag provid locat match endpoint valid locat central fail send metric rpc error code permissiondeni desc permiss monitor metricdescriptor creat deni resourc exist rpc error code permissiondeni desc permiss monitor timeseri creat deni resourc exist understand pass api endpoint method thought highlight error chang slightli know issu run gcc aip automltabulartrainingjobrunop europ west gener thank",
        "Question_preprocessed_content":"vertexai pipelin provid locat match endpoint successfulli run follow pipelin creat dataset train model deploi endpoint vertexai pipelin tool base chang region follow error error occur dataset creat model start train code previous mention dataset get creat suspect issu lie tri provid endpoint yield follow error understand pass method thought highlight error chang slightli know issu run thank",
        "Question_gpt_summary_original":"The user encountered an error when trying to run a pipeline that creates a dataset, trains a model, and deploys to an endpoint using VertexAI's pipeline tool. The error occurred when the user changed the region to europe-west2, and the error message indicated that the provided location ID did not match the endpoint. The user tried providing another endpoint, but the error persisted. The user is seeking assistance in resolving the issue and running the pipeline in europe-west2 or EU in general.",
        "Question_gpt_summary":"user encount error try run pipelin creat dataset train model deploi endpoint vertexai pipelin tool error occur user chang region europ west error messag indic provid locat match endpoint user tri provid endpoint error persist user seek assist resolv issu run pipelin europ west gener",
        "Answer_original_content":"try updat pipelin compon command pip instal forc reinstal googl cloud pipelin compon",
        "Answer_preprocessed_content":"try updat pipelin compon command",
        "Answer_gpt_summary_original":"Solution: The discussion suggests updating the pipeline component using the command \"pip3 install --force-reinstall google_cloud_pipeline_components==0.1.3\" to resolve the error encountered when running the pipeline in europe-west2 or EU in general.",
        "Answer_gpt_summary":"solut discuss suggest updat pipelin compon command pip instal forc reinstal googl cloud pipelin compon resolv error encount run pipelin europ west gener"
    },
    {
        "Question_title":"Vertex AI Training: Auto-packaged Custom Training Job Yields Very Large Docker Image",
        "Question_body":"Hello,I am trying to run a Custom Training Job in the Vertex AI Training service.The job is based on a tutorial for that fine-tuning a pre-trained BERT model (from HuggingFace).When I use the `gcloud` CLI tool to auto-package my training code into a Docker image and deploy it to the Vertex AI Training service like so:$BASE_GPU_IMAGE=\"us-docker.pkg.dev\/vertex-ai\/training\/pytorch-gpu.1-7:latest\"\n$BUCKET_NAME = \"my-bucket\"gcloud ai custom-jobs create `\n--region=us-central1 `\n--display-name=fine_tune_bert `\n--args=\"--job_dir=$BUCKET_NAME,--num-epochs=2,--model-name=finetuned-bert-classifier\" `\n--worker-pool-spec=\"machine-type=n1-standard-4,replica-count=1,accelerator-type=NVIDIA_TESLA_V100,executor-image-uri=$BASE_GPU_IMAGE,local-package-path=.,python-module=trainer.task\"... I end up with a Docker image that is roughly 18GB (!) and takes a very long time to upload to the GCP registry.Granted the base image is around 6.5GB but where do the additional >10GB come from? Is there a way for me to avoid incurring the added size increase?Please note that my job loads the training data using the `datasets` Python package at run time and AFAIK does not include it in the auto-packaged docker image. Thanks,\nurig",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1645930620000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":441.0,
        "Answer_body":"Hello Ismail,\n\n\u00a0\n\nThank you for your help.\n\nI've checked and to the best of my knowledge there are no data or log files being picked up into my custom docker image.\n\nAccording to an answer that I've received on stackoverflow.com, it's likely that the 18GB size that I'm seeing is the size of my image after extraction. Apparently the ~6.8GB size is for the image compressed.\n\n\u00a0\n\nCheers,\n\n@urig\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Vertex-AI-Training-Auto-packaged-Custom-Training-Job-Yields-Very\/td-p\/397685\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1650182580000,
        "Question_original_content":"train auto packag custom train job yield larg docker imag hello try run custom train job train servic job base tutori fine tune pre train bert model huggingfac us gcloud cli tool auto packag train code docker imag deploi train servic like base gpu imag docker pkg dev vertex train pytorch gpu latest bucket bucket gcloud custom job creat region central displai fine tune bert arg job dir bucket num epoch model finetun bert classifi worker pool spec machin type standard replica count acceler type nvidia tesla executor imag uri base gpu imag local packag path python modul trainer task end docker imag roughli take long time upload gcp registri grant base imag addit come wai avoid incur ad size increas note job load train data dataset python packag run time afaik includ auto packag docker imag thank urig",
        "Question_preprocessed_content":"train custom train job yield larg docker imag hello try run custom train job train job base tutori bert model us cli tool train code docker imag deploi train servic like creat end docker imag roughli take long time upload gcp base imag addit come wai avoid incur ad size increas note job load train data python packag run time afaik includ docker imag thank urig",
        "Question_gpt_summary_original":"The user is facing a challenge while running a Custom Training Job in the Vertex AI Training service. The auto-packaged Docker image created by the `gcloud` CLI tool is very large, around 18GB, which takes a long time to upload to the GCP registry. The user is unsure where the additional >10GB comes from and is looking for a way to avoid incurring the added size increase. The job loads the training data using the `datasets` Python package at run time and does not include it in the auto-packaged Docker image.",
        "Question_gpt_summary":"user face challeng run custom train job train servic auto packag docker imag creat gcloud cli tool larg take long time upload gcp registri user unsur addit come look wai avoid incur ad size increas job load train data dataset python packag run time includ auto packag docker imag",
        "Answer_original_content":"hello ismail thank help check best knowledg data log file pick custom docker imag accord answer receiv stackoverflow com like size see size imag extract appar size imag compress cheer urig view solut origin post",
        "Answer_preprocessed_content":"hello ismail thank help check best knowledg data log file pick custom docker imag accord answer receiv like size see size imag extract appar size imag compress cheer view solut origin post",
        "Answer_gpt_summary_original":"Solution: No solution provided. The user received an answer on stackoverflow.com that the 18GB size is likely the size of the image after extraction, and the ~6.8GB size is for the compressed image.",
        "Answer_gpt_summary":"solut solut provid user receiv answer stackoverflow com size like size imag extract size compress imag"
    },
    {
        "Question_title":"Azure Machine Learning execute R script - Could not find function \"rowid\" error",
        "Question_body":"<p>I am trying to run R script in <strong>Azure ML studio<\/strong> that transposes\/reshapes the dataframe from long to wide format (<a href=\"https:\/\/stackoverflow.com\/questions\/11322801\/transpose-reshape-dataframe-without-timevar-from-long-to-wide-format\">example<\/a>). My script runs very fine in Rstudio. But the same does not run in Azure ML studio and throws the following error - could not find function &quot;rowid&quot;. It would be great to know how can I get rid of this and what exactly is causing this error despite it being good enough to run neatly in Rstudio.<\/p>\n<pre><code>#Error: Error 0063: The following error occurred during evaluation of R script:\n# ---------- Start of error message from R ----------\n      could not find function &quot;rowid&quot;\n# ----------- End of error message from R -----------\n<\/code><\/pre>\n<p>I've tried the code in both R versions <em>CRAN R 3.1.0<\/em> &amp; <em>Microsoft R open 3.2.2<\/em>.\nThank you very much in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_created_time":1513098832883,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1612471487528,
        "Question_score":1.0,
        "Question_view_count":803.0,
        "Answer_body":"<p>Hi I had the same problem 2 days ago with the function <code>pull()<\/code>, always of the package <code>dplyr<\/code>.\nThe problem is that the both version of R (CRAN R 3.1.0 and Microsoft R open 3.2.2) supported by Azure Machine Learning Studio, does not support the version <code>0.7.4<\/code> of package <code>dplyr<\/code>.\nIf you read the <a href=\"https:\/\/cran.r-project.org\/web\/packages\/dplyr\/dplyr.pdf\" rel=\"nofollow noreferrer\">documentation<\/a> related to the package <code>dplyr<\/code> you can see that the package is installable only for R versions >= 3.1.2.<\/p>\n\n<p>Then you must wait for the R version used by Azure Machine Learning Studio be updated, or find an alternative solution to your function.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47778076",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1513962447063,
        "Question_original_content":"execut script function rowid error try run script studio transpos reshap datafram long wide format exampl script run fine rstudio run studio throw follow error function rowid great know rid exactli caus error despit good run neatli rstudio error error follow error occur evalu script start error messag function rowid end error messag tri code version cran microsoft open thank advanc",
        "Question_preprocessed_content":"execut script function rowid error try run script studio datafram long wide format script run fine rstudio run studio throw follow error function rowid great know rid exactli caus error despit good run neatli rstudio tri code version cran microsoft open thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error while trying to run an R script in Azure ML studio that transposes\/reshapes a dataframe from long to wide format. The error message states that the function \"rowid\" cannot be found. The script runs fine in Rstudio, but not in Azure ML studio. The user has tried running the code in both CRAN R 3.1.0 and Microsoft R open 3.2.2.",
        "Question_gpt_summary":"user encount error try run script studio transpos reshap datafram long wide format error messag state function rowid script run fine rstudio studio user tri run code cran microsoft open",
        "Answer_original_content":"problem dai ago function pull packag dplyr problem version cran microsoft open support studio support version packag dplyr read document relat packag dplyr packag instal version wait version studio updat altern solut function",
        "Answer_preprocessed_content":"problem dai ago function packag problem version support studio support version packag read document relat packag packag instal version wait version studio updat altern solut function",
        "Answer_gpt_summary_original":"Solution: The error is caused by the fact that the version of R used by Azure Machine Learning Studio does not support the version of the package \"dplyr\" used in the script. The user can either wait for the R version used by Azure Machine Learning Studio to be updated or find an alternative solution to the function. No specific alternative solution is provided in the discussion.",
        "Answer_gpt_summary":"solut error caus fact version studio support version packag dplyr script user wait version studio updat altern solut function specif altern solut provid discuss"
    },
    {
        "Question_title":"SageMaker Managed Spot Training with Object Detection algorithm",
        "Question_body":"<p>I'm trying to train an object detection model starting from an existing model using the new Managed Spot Training feature,  The paramters used when creating my Estimator are as follows:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>od_model = sagemaker.estimator.Estimator(get_image_uri(sagemaker.Session().boto_region_name, 'object-detection', repo_version=\"latest\"),\n                                         Config['role'],\n                                         train_instance_count = 1,\n                                         train_instance_type = 'ml.p3.16xlarge',\n                                         train_volume_size = 50,\n                                         train_max_run = (48 * 60 * 60),\n                                         train_use_spot_instances = True,\n                                         train_max_wait = (72 * 60 * 60),\n                                         input_mode = 'File',\n                                         checkpoint_s3_uri = Config['train_checkpoint_uri'],\n                                         output_path = Config['s3_output_location'],\n                                         sagemaker_session = sagemaker.Session()\n                                         )\n<\/code><\/pre>\n\n<p>(The references to <code>Config<\/code> in the above are a config data structure I'm using to extract\/centralise some parameters)<\/p>\n\n<p>When I run the above, I get the following exception:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: MaxWaitTimeInSeconds above 3600 is not supported for the given algorithm.<\/p>\n<\/blockquote>\n\n<p>If I change <code>train_max_wait<\/code> to 3600 I get this exception instead:<\/p>\n\n<blockquote>\n  <p>botocore.exceptions.ClientError: An error occurred (ValidationException) when calling the CreateTrainingJob operation: Invalid MaxWaitTimeInSeconds. It must be present and be greater than or equal to MaxRuntimeInSeconds<\/p>\n<\/blockquote>\n\n<p>However changing <code>max_run_time<\/code> to 3600 or less isn't going to work for me as I expect this model to take several days to train (large data set), in fact a single epoch takes more than an hour.<\/p>\n\n<p>The <a href=\"https:\/\/aws.amazon.com\/blogs\/aws\/managed-spot-training-save-up-to-90-on-your-amazon-sagemaker-training-jobs\/\" rel=\"nofollow noreferrer\">AWS blog post on Managed Spot Training<\/a> say that <code>MaxWaitTimeInSeconds<\/code> is limited to an 60 minutes for:<\/p>\n\n<blockquote>\n  <p>For built-in algorithms and AWS Marketplace algorithms that don\u2019t use checkpointing, we\u2019re enforcing a maximum training time of 60 minutes (MaxWaitTimeInSeconds parameter).<\/p>\n<\/blockquote>\n\n<p>Earlier, the same blog post says:<\/p>\n\n<blockquote>\n  <p>Built-in algorithms: computer vision algorithms support checkpointing (Object Detection, Semantic Segmentation, and very soon Image Classification).<\/p>\n<\/blockquote>\n\n<p>So I don't think it's that my algorithm doesn't support Checkpointing.  In fact that blog post uses object detection and max run times of 48 hours.  So I don't think it's an algorithm limitation.<\/p>\n\n<p>As you can see above, I've set up a S3 URL for the checkpoints.  The S3 bucket does exist, and the training container has access to it (it's the same bucket that the training data and model outputs are placed, and I had no problems with access to those before turning on spot training.<\/p>\n\n<p>My boto and sagemaker libraries are current versions:<\/p>\n\n<pre><code>boto3 (1.9.239)\nbotocore (1.12.239)\nsagemaker (1.42.3)\n<\/code><\/pre>\n\n<p>As best I can tell from reading various docs, I've got everything set up correctly.  My use case is almost exactly what's described in the blog post linked above, but I'm using the SageMaker Python SDK instead of the console.<\/p>\n\n<p>I'd really like to try Managed Spot Training to save some money, as I have a very long training run coming up.  But limiting timeouts to an hour isn't going to work for my use case.  Any suggestions?<\/p>\n\n<p><strong>Update:<\/strong>  If I comment out the <code>train_use_spot_instances<\/code> and <code>train_max_wait<\/code> options to train on regular on-demand instances my training job is created successfully.  If I then try to use the console to clone the job and turn on Spot instances on the clone I get the same ValidationException.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1569898766770,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1569900383487,
        "Question_score":4.0,
        "Question_view_count":1234.0,
        "Answer_body":"<p>I ran my script again today and it worked fine, no <code>botocore.exceptions.ClientError<\/code> exceptions.  Given that this issue affected both the Python SDK for Sagemaker and the console, I suspect it might have been an issue with the backend API and not my client code.<\/p>\n<p>Either way, it's working now.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1646271695950,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58177548",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1570492274472,
        "Question_original_content":"manag spot train object detect algorithm try train object detect model start exist model new manag spot train featur paramt creat estim follow model estim estim imag uri session boto region object detect repo version latest config role train instanc count train instanc type xlarg train volum size train max run train us spot instanc true train max wait input mode file checkpoint uri config train checkpoint uri output path config output locat session session refer config config data structur extract centralis paramet run follow except botocor except clienterror error occur validationexcept call createtrainingjob oper maxwaittimeinsecond support given algorithm chang train max wait except instead botocor except clienterror error occur validationexcept call createtrainingjob oper invalid maxwaittimeinsecond present greater equal maxruntimeinsecond chang max run time isn go work expect model dai train larg data set fact singl epoch take hour aw blog post manag spot train maxwaittimeinsecond limit minut built algorithm aw marketplac algorithm dont us checkpoint enforc maximum train time minut maxwaittimeinsecond paramet earlier blog post sai built algorithm vision algorithm support checkpoint object detect semant segment soon imag classif think algorithm support checkpoint fact blog post us object detect max run time hour think algorithm limit set url checkpoint bucket exist train contain access bucket train data model output place problem access turn spot train boto librari current version boto botocor best tell read doc got set correctli us case exactli describ blog post link python sdk instead consol like try manag spot train save monei long train run come limit timeout hour isn go work us case suggest updat comment train us spot instanc train max wait option train regular demand instanc train job creat successfulli try us consol clone job turn spot instanc clone validationexcept",
        "Question_preprocessed_content":"manag spot train object detect algorithm try train object detect model start exist model new manag spot train featur paramt creat estim follow refer config data structur paramet run follow except error occur call createtrainingjob oper maxwaittimeinsecond support given algorithm chang except instead error occur call createtrainingjob oper invalid maxwaittimeinsecond present greater equal maxruntimeinsecond chang isn go work expect model dai train fact singl epoch take hour aw blog post manag spot train limit minut algorithm aw marketplac algorithm dont us checkpoint enforc maximum train time minut earlier blog post sai algorithm vision algorithm support checkpoint think algorithm support checkpoint fact blog post us object detect max run time hour think algorithm limit set url checkpoint bucket exist train contain access bucket train data model output place problem access turn spot train boto librari current version best tell read doc got set correctli us case exactli describ blog post link python sdk instead consol like try manag spot train save monei long train run come limit timeout hour isn go work us case suggest updat comment option train regular instanc train job creat successfulli try us consol clone job turn spot instanc clone validationexcept",
        "Question_gpt_summary_original":"The user is encountering challenges when trying to use the SageMaker Managed Spot Training feature with an object detection algorithm. They are receiving a ValidationException error when setting the train_max_wait parameter to a value greater than 3600, and when they set it to 3600 or less, they receive another ValidationException error. The user needs to train their model for several days, and the algorithm they are using supports checkpointing. The AWS blog post on Managed Spot Training states that the MaxWaitTimeInSeconds parameter is limited to 60 minutes for built-in algorithms and AWS Marketplace algorithms that do not use checkpointing, but the user's algorithm supports checkpointing. The user has checked that their S3 bucket exists and that the training container has access to it. When they comment out the train_use_spot_instances and train_max_wait options, they can create a training job successfully on regular on-demand instances. However, when they try to clone the job and turn on Spot instances on the clone, they receive the",
        "Question_gpt_summary":"user encount challeng try us manag spot train featur object detect algorithm receiv validationexcept error set train max wait paramet valu greater set receiv validationexcept error user need train model dai algorithm support checkpoint aw blog post manag spot train state maxwaittimeinsecond paramet limit minut built algorithm aw marketplac algorithm us checkpoint user algorithm support checkpoint user check bucket exist train contain access comment train us spot instanc train max wait option creat train job successfulli regular demand instanc try clone job turn spot instanc clone receiv",
        "Answer_original_content":"ran script todai work fine botocor except clienterror except given issu affect python sdk consol suspect issu backend api client code wai work",
        "Answer_preprocessed_content":"ran script todai work fine except given issu affect python sdk consol suspect issu backend api client code wai work",
        "Answer_gpt_summary_original":"There are no solutions provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Using external libraries for model training in aws sagemaker",
        "Question_body":"<p>I am just getting started with aws sagemaker and realized it doesn't have a random forest classifier. I found this github tutorial on creating your own and deploying it in sagemaker: <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker-python-sdk\/scikit_learn_randomforest\/Sklearn_on_SageMaker_end2end.ipynb<\/a>.<\/p>\n<p>I am using the python sdk and was more or less curious to see if anyone actually uses this or any external libraries for training with sagemaker. It seems that if you aren't using the built in algorithms then it is very involved to create your own and the functionality of the model and ability to interpret it is very limited once you do get it trained.<\/p>\n<p>For example after deploying the model to an aws endpoint and pulling down the artifacts I could only call the <code>predict<\/code> method (no <code>predict_probab<\/code> as is possible in the actual <code>sklearn randomforestclassifier<\/code>). I also haven't been able to find anything like what you get in <code>sklearn.metrics<\/code> such as <code>accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix<\/code> etc so I'm assuming one would need to build equivalents to these from scratch to be able to interpret their model.<\/p>\n<p>I have spent a week or so researching this and trying to get this rigged up and it just seems that importing external libraries for model training in sagemaker is not very popular or well-documented online. Interested to know if I'm just unaware of more functionality or if there are alternatives that people prefer or if I should just stick with the built in xgboost classifier if I am looking for a tree-based option. Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1655494198873,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":50.0,
        "Answer_body":"<p>It is possible to add extra packages for training when using any of the Framework containers such as SKLearn. Kindly see this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#using-third-party-libraries\" rel=\"nofollow noreferrer\">link<\/a> for more information.<\/p>\n<p>From a hosting perspective, you do have the ability to provide a custom entry_point \/ inference.py script that you can use to control model loading, pre and post processing. Please see this <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">link<\/a> for more information<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72663991",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1657063869132,
        "Question_original_content":"extern librari model train get start realiz random forest classifi github tutori creat deploi http github com aw amazon exampl blob main python sdk scikit learn randomforest sklearn endend ipynb python sdk curiou actual us extern librari train aren built algorithm involv creat function model abil interpret limit train exampl deploi model aw endpoint pull artifact predict method predict probab possibl actual sklearn randomforestclassifi haven abl like sklearn metric accuraci score roc auc score precis score recal score score confus matrix assum need build equival scratch abl interpret model spent week research try rig import extern librari model train popular document onlin interest know unawar function altern peopl prefer stick built xgboost classifi look tree base option thank",
        "Question_preprocessed_content":"extern librari model train get start realiz random forest classifi github tutori creat deploi python sdk curiou actual us extern librari train aren built algorithm involv creat function model abil interpret limit train exampl deploi model aw endpoint pull artifact method haven abl like assum need build equival scratch abl interpret model spent week research try rig import extern librari model train popular onlin interest know unawar function altern peopl prefer stick built xgboost classifi look option thank",
        "Question_gpt_summary_original":"The user is facing challenges while using external libraries for model training in AWS Sagemaker. They found a Github tutorial on creating their own random forest classifier and deploying it in Sagemaker, but they realized that it is very involved to create their own model and the functionality of the model and ability to interpret it is very limited once it is trained. The user is unable to find anything like what they get in sklearn.metrics, such as accuracy_score, roc_auc_score, precision_score, recall_score, f1_score, confusion_matrix, etc. The user has spent a week researching this and trying to get it rigged up, but it seems that importing external libraries for model training in Sagemaker is not very popular or well-documented online.",
        "Question_gpt_summary":"user face challeng extern librari model train github tutori creat random forest classifi deploi realiz involv creat model function model abil interpret limit train user unabl like sklearn metric accuraci score roc auc score precis score recal score score confus matrix user spent week research try rig import extern librari model train popular document onlin",
        "Answer_original_content":"possibl add extra packag train framework contain sklearn kindli link inform host perspect abil provid custom entri point infer script us control model load pre post process link inform",
        "Answer_preprocessed_content":"possibl add extra packag train framework contain sklearn kindli link inform host perspect abil provid custom script us control model load pre post process link inform",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include adding extra packages for training when using Framework containers such as SKLearn and providing a custom entry_point\/inference.py script to control model loading, pre and post processing.",
        "Answer_gpt_summary":"possibl solut mention discuss includ ad extra packag train framework contain sklearn provid custom entri point infer script control model load pre post process"
    },
    {
        "Question_title":"Sagemaker Studio is not opening after deleting lifecycle configuration",
        "Question_body":"I was working on a sagemaker studio for ML work, I attached Lifecycle Configuration with it, which was creating problem. Then I deleted the lifecycle configuration without detaching it, and this problem is happening. Can't start sagemaker studio notebook and this is shown.\n\nAny suggestion to fix this ?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1667756083758,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":23.0,
        "Answer_body":"You can try detaching the LCC script using the CLI. You can use the CloudShell from console, since your console role is able to perform updates on the domain.\n\nUse the update-domain CLI call, and provide an empty configuration for the default user settings, something like-\n\naws sagemaker update-domain --domain-id d-abc123 \\\n--default-user-settings '{\n\"JupyterServerAppSettings\": {\n  \"DefaultResourceSpec\": {\n    \"InstanceType\": \"system\"\n   },\n}}'",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU91ywEwTsRRqmHKZJ1yVrrA\/sagemaker-studio-is-not-opening-after-deleting-lifecycle-configuration",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1667785787444,
        "Question_original_content":"studio open delet lifecycl configur work studio work attach lifecycl configur creat problem delet lifecycl configur detach problem happen start studio notebook shown suggest fix",
        "Question_preprocessed_content":"studio open delet lifecycl configur work studio work attach lifecycl configur creat problem delet lifecycl configur detach problem happen start studio notebook shown suggest fix",
        "Question_gpt_summary_original":"The user is facing challenges with opening Sagemaker Studio after deleting a Lifecycle Configuration without detaching it. The notebook is not starting and the user is seeking suggestions to fix the issue.",
        "Question_gpt_summary":"user face challeng open studio delet lifecycl configur detach notebook start user seek suggest fix issu",
        "Answer_original_content":"try detach lcc script cli us cloudshel consol consol role abl perform updat domain us updat domain cli provid configur default user set like updat domain domain abc default user set jupyterserverappset defaultresourcespec instancetyp",
        "Answer_preprocessed_content":"try detach lcc script cli us cloudshel consol consol role abl perform updat domain us cli provid configur default user set like",
        "Answer_gpt_summary_original":"Solution: The user can try detaching the LCC script using the CLI. They can use the CloudShell from console and perform updates on the domain using the update-domain CLI call, and provide an empty configuration for the default user settings.",
        "Answer_gpt_summary":"solut user try detach lcc script cli us cloudshel consol perform updat domain updat domain cli provid configur default user set"
    },
    {
        "Question_title":"Automated machine learning model deployment issue",
        "Question_body":"So I'm having an issue with setting up an endpoint for a machine learning model which was trained using Azure AutoML. When I try to test the deployed model, I get an error saying that the service is temporarily unavailable. After looking online, I found that this might happen because of an error in the run() function in the entry script.\n\nWhen I try to test the entry script on a notebook in Azure ML studio, on a fresh compute instance, there are two problems:\nFirst I get the error: AttributeError: 'MSIAuthentication' object has no attribute 'get_token'\nWhich is solved by running: pip install azureml-core\n\nThen I get the error: ModuleNotFoundError: No module named 'azureml.automl.runtime'\nWhich I try to solve using: pip install azureml-automl-runtime\nBut this throws a lot of incompatibility errors during the installation. When I then try to run the entry script I get an error with the message: \"Failed while applying learned transformations.\"\n\nSo I setup a new virtual environment on my local machine in which I only installed azure-automl-runtime. Using that setup the entry script works perfectly fine. So I created a custom environment in Azure ML studio using the conda file of that local virtual environment. Unfortunatly I still get the error \"service temporarily unavailable\" when trying to test the endpoint.\n\nI have a feeling the default Azure ML containers are incompatible with azureml-automl-runtime, since installing this on a ML studio notebook also throws a lot of errors.\n\nI feel like there should be an elegant way to deploy an AutoML model, am I doing something wrong here?\n\n\n\n\n\nUpdate: I found out I didn't change the environment for the endpoint, so that is why I was getting the same error probably. When using the custom environment I got errors from gunicorn, so I also added that package to the environment. Now I get the following error:\n\n       File \"\/var\/azureml-server\/entry.py\", line 1, in <module>\n     import create_app\n   File \"\/var\/azureml-server\/create_app.py\", line 4, in <module>\n     from routes_common import main\n   File \"\/var\/azureml-server\/routes_common.py\", line 39, in <module>\n     from azure.ml.api.exceptions.ClientSideException import ClientSideException\n ModuleNotFoundError: No module named 'azure.ml'\n\n\n\n\nSo what do I install to fix this? Is there a list somewhere of required packages for an ML model endpoint?",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_created_time":1627371604967,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"I managed to fix the issue with the environment by just adding everything that would throw an error. Then I found out the return value has to be a json\/dict object, which if not done throws the exact same 'service temporarily unavailable' error.\n\nBut my issue with the confusing curated environments and azureml-automl-runtime in ML studio notebooks remain. Maybe this is worth looking into @ramr-msft .",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/490809\/automated-machine-learning-model-deployment-issue.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1627463996087,
        "Question_original_content":"autom machin learn model deploy issu have issu set endpoint machin learn model train azur automl try test deploi model error sai servic temporarili unavail look onlin happen error run function entri script try test entri script notebook studio fresh comput instanc problem error attributeerror msiauthent object attribut token solv run pip instal core error modulenotfounderror modul name automl runtim try solv pip instal automl runtim throw lot incompat error instal try run entri script error messag fail appli learn transform setup new virtual environ local machin instal azur automl runtim setup entri script work perfectli fine creat custom environ studio conda file local virtual environ unfortunatli error servic temporarili unavail try test endpoint feel default contain incompat automl runtim instal studio notebook throw lot error feel like eleg wai deploi automl model wrong updat chang environ endpoint get error probabl custom environ got error gunicorn ad packag environ follow error file var server entri line import creat app file var server creat app line rout common import main file var server rout common line azur api except clientsideexcept import clientsideexcept modulenotfounderror modul name azur instal fix list requir packag model endpoint",
        "Question_preprocessed_content":"autom machin learn model deploy issu have issu set endpoint machin learn model train azur automl try test deploi model error sai servic temporarili unavail look onlin happen error run function entri script try test entri script notebook studio fresh comput instanc problem error attributeerror msiauthent object attribut solv run pip instal core error modulenotfounderror modul name try solv pip instal throw lot incompat error instal try run entri script error messag fail appli learn setup new virtual environ local machin instal setup entri script work perfectli fine creat custom environ studio conda file local virtual environ unfortunatli error servic temporarili unavail try test endpoint feel default contain incompat instal studio notebook throw lot error feel like eleg wai deploi automl model wrong updat chang environ endpoint get error probabl custom environ got error gunicorn ad packag environ follow error file line import file line import main file line import clientsideexcept modulenotfounderror modul name instal fix list requir packag model endpoint",
        "Question_gpt_summary_original":"The user is facing challenges in setting up an endpoint for a machine learning model trained using Azure AutoML. The deployed model is showing a \"service temporarily unavailable\" error, which may be due to an error in the run() function in the entry script. The user encountered errors while testing the entry script on a notebook in Azure ML studio, which were solved by installing azureml-core and azureml-automl-runtime. However, the user still faced compatibility issues and errors while testing the endpoint. The user is unsure if they are doing something wrong and is seeking guidance on the required packages for an ML model endpoint.",
        "Question_gpt_summary":"user face challeng set endpoint machin learn model train azur automl deploi model show servic temporarili unavail error error run function entri script user encount error test entri script notebook studio solv instal core automl runtim user face compat issu error test endpoint user unsur wrong seek guidanc requir packag model endpoint",
        "Answer_original_content":"manag fix issu environ ad throw error return valu json dict object throw exact servic temporarili unavail error issu confus curat environ automl runtim studio notebook remain mayb worth look ramr msft",
        "Answer_preprocessed_content":"manag fix issu environ ad throw error return valu object throw exact servic temporarili unavail error issu confus curat environ studio notebook remain mayb worth look",
        "Answer_gpt_summary_original":"Solution: The user fixed the issue with the environment by adding all the packages that would throw an error. They also found out that the return value has to be a json\/dict object, which if not done throws the same 'service temporarily unavailable' error. However, there were no solutions provided for the confusion with curated environments and azureml-automl-runtime in ML studio notebooks.",
        "Answer_gpt_summary":"solut user fix issu environ ad packag throw error return valu json dict object throw servic temporarili unavail error solut provid confus curat environ automl runtim studio notebook"
    },
    {
        "Question_title":"why SageMaker Ground Truth automated labelling?",
        "Question_body":"Hi, I'm confused with the automated labelling feature of SM. Usually when people label things it is to train their own models afterwards. Is the goal of this feature to replace the downstream ML model that would use the labelled dataset? some sort of code free computer vision system?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1544611168000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":52.0,
        "Answer_body":"Automated data labeling is labeling of data using machine learning. Amazon SageMaker Ground Truth will first select a random sample of data and send it to humans to be labeled. The results are then used to train a labeling model that attempts to label a new sample of raw data automatically. The labels are committed when the model can label the data with a confidence score that meets or exceeds a high threshold. Where the confidence score falls below this threshold, the data is sent to human labelers. Some of the data labeled by humans is used to generate a new training dataset for the labeling model, and the model is automatically retrained to improve its accuracy. This process repeats with each sample of raw data to be labeled. The labeling model becomes more capable of automatically labeling raw data with each iteration, and less data is routed to humans.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUp6wm80kUT0GZJp8meQtgTg\/why-sage-maker-ground-truth-automated-labelling",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1544632081000,
        "Question_original_content":"ground truth autom label confus autom label featur usual peopl label thing train model goal featur replac downstream model us label dataset sort code free vision",
        "Question_preprocessed_content":"ground truth autom label confus autom label featur usual peopl label thing train model goal featur replac downstream model us label dataset sort code free vision",
        "Question_gpt_summary_original":"The user is confused about the purpose of SageMaker Ground Truth's automated labelling feature and whether it is meant to replace downstream ML models or serve as a code-free computer vision system.",
        "Question_gpt_summary":"user confus purpos ground truth autom label featur meant replac downstream model serv code free vision",
        "Answer_original_content":"autom data label label data machin learn ground truth select random sampl data send human label result train label model attempt label new sampl raw data automat label commit model label data confid score meet exce high threshold confid score fall threshold data sent human label data label human gener new train dataset label model model automat retrain improv accuraci process repeat sampl raw data label label model capabl automat label raw data iter data rout human",
        "Answer_preprocessed_content":"autom data label label data machin learn ground truth select random sampl data send human label result train label model attempt label new sampl raw data automat label commit model label data confid score meet exce high threshold confid score fall threshold data sent human label data label human gener new train dataset label model model automat retrain improv accuraci process repeat sampl raw data label label model capabl automat label raw data iter data rout human",
        "Answer_gpt_summary_original":"Solution: The purpose of SageMaker Ground Truth's automated labeling feature is to label data using machine learning. It is not meant to replace downstream ML models or serve as a code-free computer vision system. The process involves selecting a random sample of data and sending it to humans to be labeled. The results are then used to train a labeling model that attempts to label a new sample of raw data automatically. The labels are committed when the model can label the data with a confidence score that meets or exceeds a high threshold. Where the confidence score falls below this threshold, the data is sent to human labelers. Some of the data labeled by humans is used to generate a new training dataset for the labeling model, and the model is automatically retrained to improve",
        "Answer_gpt_summary":"solut purpos ground truth autom label featur label data machin learn meant replac downstream model serv code free vision process involv select random sampl data send human label result train label model attempt label new sampl raw data automat label commit model label data confid score meet exce high threshold confid score fall threshold data sent human label data label human gener new train dataset label model model automat retrain improv"
    },
    {
        "Question_title":"Training with TensorFlow on Sagemaker No module named 'tf_container'",
        "Question_body":"<p>I'm trying to training TensorFlow model on AWS Sagemaker.\nI created container with external lib for that (Use Your Own Algorithms or Models with Amazon SageMaker).<\/p>\n\n<p>we run a training job with TensorFlow API<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tensorflow import TensorFlow\nestimator = TensorFlow(\n  entry_point=\"entry.py\",             # entry script\n  role=role,\n  framework_version=\"1.13.0\",   \n  py_version='py3',\n  hyperparameters=hyperparameters,\n  train_instance_count=1,                   # \"The number of GPUs instances to use\"\n  train_instance_type=train_instance_type,\n  image_name=my_image\n\n)\nestimator.fit({'train': train_s3, 'eval': eval_s3})\n<\/code><\/pre>\n\n<p>and got an error:<\/p>\n\n<pre><code>09:06:46\n2019-07-23 09:06:45,463 INFO - root - running container entrypoint \uf141\n09:06:46\n2019-07-23 09:06:45,463 INFO - root - starting train task \uf141\n09:06:46\n2019-07-23 09:06:45,476 INFO - container_support.training - Training starting \uf141\n09:06:46\n2019-07-23 09:06:45,479 ERROR - container_support.training - uncaught exception during training: No module named 'tf_container'\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 136, in load_framework return importlib.import_module('mxnet_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, in import_module return _bootstrap._gcd_import(name[level:], package, level) File \"&lt;frozen importlib._bootstrap&gt;\", line 994, in _gcd_i \uf141\n09:06:46\nModuleNotFoundError: No module named 'mxnet_container'\n\uf141\n09:06:46\nDuring handling of the above exception, another exception occurred:\n\uf141\n09:06:46\nTraceback (most recent call last): File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/training.py\", line 35, in start fw = TrainingEnvironment.load_framework() File \"\/usr\/local\/lib\/python3.6\/dist-packages\/container_support\/environment.py\", line 138, in load_framework return importlib.import_module('tf_container') File \"\/usr\/lib\/python3.6\/importlib\/__init__.py\", line 126, \uf141\n09:06:46\nModuleNotFoundError: No module named 'tf_container'\n<\/code><\/pre>\n\n<p>What can I do to solve this issue? how can I debug this case?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1563885123080,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1191.0,
        "Answer_body":"<p>I'm guessing that you used your own TF container, not the SageMaker one at <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-container\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-container<\/a><\/p>\n\n<p>If that's the case, your container is missing the support code needed to use the TensorFlow estimator ('tf_container' package).<\/p>\n\n<p>The solution is to start from the SageMaker container, customize it, push it back to ECR and pass the image name to the SageMaker estimator with the 'image_name' parameter.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57164290",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1563901268600,
        "Question_original_content":"train tensorflow modul name contain try train tensorflow model creat contain extern lib us algorithm model run train job tensorflow api tensorflow import tensorflow estim tensorflow entri point entri entri script role role framework version version hyperparamet hyperparamet train instanc count number gpu instanc us train instanc type train instanc type imag imag estim fit train train eval eval got error info root run contain entrypoint info root start train task info contain support train train start error contain support train uncaught except train modul name contain traceback recent file usr local lib python dist packag contain support environ line load framework return importlib import modul mxnet contain file usr lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd modulenotfounderror modul name mxnet contain handl except except occur traceback recent file usr local lib python dist packag contain support train line start trainingenviron load framework file usr local lib python dist packag contain support environ line load framework return importlib import modul contain file usr lib python importlib init line modulenotfounderror modul name contain solv issu debug case",
        "Question_preprocessed_content":"train tensorflow modul name try train tensorflow model creat contain extern lib run train job tensorflow api got error solv issu debug case",
        "Question_gpt_summary_original":"The user is encountering an error while trying to train a TensorFlow model on AWS Sagemaker. The error message indicates that there is no module named 'tf_container'. The user is seeking advice on how to solve this issue and how to debug the problem.",
        "Question_gpt_summary":"user encount error try train tensorflow model error messag indic modul name contain user seek advic solv issu debug problem",
        "Answer_original_content":"guess contain http github com aw tensorflow contain case contain miss support code need us tensorflow estim contain packag solut start contain custom push ecr pass imag estim imag paramet",
        "Answer_preprocessed_content":"guess contain case contain miss support code need us tensorflow estim solut start contain custom push ecr pass imag estim paramet",
        "Answer_gpt_summary_original":"Solution: The user needs to start from the SageMaker container, customize it, push it back to ECR, and pass the image name to the SageMaker estimator with the 'image_name' parameter.",
        "Answer_gpt_summary":"solut user need start contain custom push ecr pass imag estim imag paramet"
    },
    {
        "Question_title":"Functions of sagemaker service from js-sdk not recognize by AWS Lambda",
        "Question_body":"<p>I'm writing a lambda (in node.js 6.10) to update an endpoint SageMaker. To do so I have to create a new HyperParamterTuningJob (and then describe it).\nI succeeded to call all functions of the service SageMaker from the sdk (like listModels, createTrainingJob, ...) (<a href=\"https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/AWSJavaScriptSDK\/latest\/AWS\/SageMaker.html<\/a>) except some of them.<\/p>\n\n<p>All the functions that are related to HyperParameterTuningJob \n(createHyperParameterTuningJob, describeHyperParameterTuningJob, listHyperParameterTuningJobs and stopHyperParameterTuningJob) \nare not known in the sdk by the lambda.<\/p>\n\n<p>I have attached the policy 'AmazonSageMakerFullAccess' to the role IAM used (where all these functions are allowed). So the error can't come from a problem of authorization.<\/p>\n\n<p>I have already created a HyperParameterTuningJob (by the interface of AWS) called 'myTuningJob'.\nI have an error everytime I use the function describeHyperParamterTuningJob.\nHere is my lambda code : <\/p>\n\n<pre><code>const AWS = require('aws-sdk');\nconst sagemaker = new AWS.SageMaker({region: 'eu-west-1', apiVersion: '2017-07-24'});\nvar role = 'arn:aws:iam::xxxxxxxxxxxx:role\/service-role\/AmazonSageMaker-ExecutionRole-xxxxxxxxxxxxxxx';\n\nexports.handler = (event, context, callback) =&gt; {\n    var params = {\n        HyperParameterTuningJobName: 'myTuningJob'\n    };\n\n    sagemaker.describeHyperParameterTuningJob(params, function(err, data) {\n        if (err) console.log(err, err.stack);\n        else console.log(data);\n    });\n};\n<\/code><\/pre>\n\n<p>When I try to test this code in AWS lambda, it returns this result in the console :<\/p>\n\n<pre><code>Function Logs:\nSTART RequestId: 6e79aaa4-9a18-11e8-8dcd-d58423b413c1 Version: $LATEST\n2018-08-07T08:03:56.336Z    6e79aaa4-9a18-11e8-8dcd-d58423b413c1    TypeError: sagemaker.describeHyperParameterTuningJob is not a function\nat exports.handler (\/var\/task\/index.js:10:15)\nEND RequestId: 6e79aaa4-9a18-11e8-8dcd-d58423b413c1\nREPORT RequestId: 6e79aaa4-9a18-11e8-8dcd-d58423b413c1  Duration: 50.00 ms   \nBilled Duration: 100 ms Memory Size: 128 MB Max Memory Used: 32 MB  \nRequestId: 6e79aaa4-9a18-11e8-8dcd-d58423b413c1 Process exited before completing request\n<\/code><\/pre>\n\n<p>When I call all other functions of the SageMaker service from the sdk, it runs correctly, whitout any error.<\/p>\n\n<p>I don't find any explanation in the documentation of why these functions related to HyperParameterTuningJob are not recognized as functions in the sdk.<\/p>\n\n<p>Does anyone have any idea of why it doesn't work ? Or any solutions to call theses functions ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1533631633087,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":212.0,
        "Answer_body":"<p>In AWS Lambda, only the sdk that have a sable version are available.\nThe sdk of the SageMaker service is not stable yet, so functions related to HyperParameterTuningJob are not in the version of the sdk included in AWS Lambda.<\/p>\n\n<p>To use theses functions, you need to install the latest version of the sdk on local on your machine (with npm install aws-sdk). \nThen zip the node_modules folder and your script (called index.js), then upload this zip folder into the AWS lambda.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51722555",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1533728073932,
        "Question_original_content":"function servic sdk recogn aw lambda write lambda node updat endpoint creat new hyperparamtertuningjob succeed function servic sdk like listmodel createtrainingjob http doc aw amazon com awsjavascriptsdk latest aw html function relat hyperparametertuningjob createhyperparametertuningjob describehyperparametertuningjob listhyperparametertuningjob stophyperparametertuningjob known sdk lambda attach polici amazonfullaccess role iam function allow error come problem author creat hyperparametertuningjob interfac aw call mytuningjob error everytim us function describehyperparamtertuningjob lambda code const aw requir aw sdk const new aw region west apivers var role arn aw iam role servic role amazon executionrol export handler event context callback var param hyperparametertuningjobnam mytuningjob describehyperparametertuningjob param function err data err consol log err err stack consol log data try test code aw lambda return result consol function log start requestid dcd dbc version latest dcd dbc typeerror describehyperparametertuningjob function export handler var task index end requestid dcd dbc report requestid dcd dbc durat bill durat memori size max memori requestid dcd dbc process exit complet request function servic sdk run correctli whitout error explan document function relat hyperparametertuningjob recogn function sdk idea work solut these function",
        "Question_preprocessed_content":"function servic recogn aw lambda write lambda updat endpoint creat new hyperparamtertuningjob succeed function servic sdk function relat hyperparametertuningjob createhyperparametertuningjob describehyperparametertuningjob listhyperparametertuningjob stophyperparametertuningjob known sdk lambda attach polici amazonfullaccess role iam error come problem author creat hyperparametertuningjob call mytuningjob error everytim us function describehyperparamtertuningjob lambda code try test code aw lambda return result consol function servic sdk run correctli whitout error explan document function relat hyperparametertuningjob recogn function sdk idea work solut these function",
        "Question_gpt_summary_original":"The user is facing challenges while writing a lambda function in node.js 6.10 to update an endpoint SageMaker. Although the user has successfully called all functions of the service SageMaker from the sdk, some functions related to HyperParameterTuningJob are not recognized in the sdk by the lambda. The user has attached the policy 'AmazonSageMakerFullAccess' to the role IAM used, where all these functions are allowed. The user is unable to find any explanation in the documentation of why these functions related to HyperParameterTuningJob are not recognized as functions in the sdk.",
        "Question_gpt_summary":"user face challeng write lambda function node updat endpoint user successfulli call function servic sdk function relat hyperparametertuningjob recogn sdk lambda user attach polici amazonfullaccess role iam function allow user unabl explan document function relat hyperparametertuningjob recogn function sdk",
        "Answer_original_content":"aw lambda sdk sabl version avail sdk servic stabl function relat hyperparametertuningjob version sdk includ aw lambda us these function need instal latest version sdk local machin npm instal aw sdk zip node modul folder script call index upload zip folder aw lambda",
        "Answer_preprocessed_content":"aw lambda sdk sabl version avail sdk servic stabl function relat hyperparametertuningjob version sdk includ aw lambda us these function need instal latest version sdk local machin zip folder script upload zip folder aw lambda",
        "Answer_gpt_summary_original":"Solution: The user can install the latest version of the sdk on their local machine using npm install aws-sdk and then zip the node_modules folder and their script (called index.js) and upload the zip folder into AWS Lambda to use the functions related to HyperParameterTuningJob.",
        "Answer_gpt_summary":"solut user instal latest version sdk local machin npm instal aw sdk zip node modul folder script call index upload zip folder aw lambda us function relat hyperparametertuningjob"
    },
    {
        "Question_title":"Azure ML Python with Script Bundle cannot import module",
        "Question_body":"<p>In Azure ML, I'm trying to execute a Python module that needs to import the module pyxdameraulevenshtein (<a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>).<\/p>\n\n<p>I followed the usual way, which is to create a zip file and then import it; however for this specific module, it seems to never be able to find it. The error message is as usual:<\/p>\n\n<p><em>ImportError: No module named 'pyxdameraulevenshtein'<\/em><\/p>\n\n<p>Has anyone included this pyxdameraulevenshtein module in Azure ML with success ?<\/p>\n\n<p>(I took the package from <a href=\"https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein\" rel=\"nofollow noreferrer\">https:\/\/pypi.python.org\/pypi\/pyxDamerauLevenshtein<\/a>.)<\/p>\n\n<p>Thanks for any help you can provide,<\/p>\n\n<p>PH<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1496235911927,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1496278915287,
        "Question_score":2.0,
        "Question_view_count":2395.0,
        "Answer_body":"<p>I viewed the <code>pyxdameraulevenshtein<\/code> module page, there are two packages you can download which include a wheel file for MacOS and a source code tar file. I don't think you can directly use the both on Azure ML, because the MacOS one is just a share library <code>.so<\/code> file for darwin which is not compatible with Azure ML, and the other you need to first compile it.<\/p>\n\n<p>So my suggestion is as below for using <code>pyxdameraulevenshtein<\/code>.<\/p>\n\n<ol>\n<li>First, compile the source code of <code>pyxdameraulevenshtein<\/code> to a DLL file on Windows, please refer to the document for Python <a href=\"https:\/\/docs.python.org\/2\/extending\/windows.html\" rel=\"nofollow noreferrer\">2<\/a>\/<a href=\"https:\/\/docs.python.org\/3\/extending\/windows.html\" rel=\"nofollow noreferrer\">3<\/a> or search for doing this.<\/li>\n<li>Write a Python script using the DLL you compiled to implement your needs, please refer to the SO thread <a href=\"https:\/\/stackoverflow.com\/questions\/252417\/how-can-i-use-a-dll-file-from-python\">How can I use a DLL file from Python?<\/a> for how to use DLL from Python and refer to the Azure offical <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-execute-python-scripts\" rel=\"nofollow noreferrer\">tutorial<\/a> to write your Python script<\/li>\n<li>Package your Python script and DLL file as a zip file, then to upload the zip file to use it in <code>Execute Python script<\/code> model of Azure ML.<\/li>\n<\/ol>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44285641",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1496307870776,
        "Question_original_content":"python script bundl import modul try execut python modul need import modul pyxdameraulevenshtein http pypi python org pypi pyxdameraulevenshtein follow usual wai creat zip file import specif modul abl error messag usual importerror modul name pyxdameraulevenshtein includ pyxdameraulevenshtein modul success took packag http pypi python org pypi pyxdameraulevenshtein thank help provid",
        "Question_preprocessed_content":"python script bundl import modul try execut python modul need import modul pyxdameraulevenshtein follow usual wai creat zip file import specif modul abl error messag usual importerror modul name pyxdameraulevenshtein includ pyxdameraulevenshtein modul success took packag thank help provid",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to execute a Python module that needs to import the module pyxdameraulevenshtein in Azure ML. Despite following the usual way of creating a zip file and importing it, the module seems to never be found, resulting in an ImportError. The user is seeking help to include this module in Azure ML successfully.",
        "Question_gpt_summary":"user encount challeng try execut python modul need import modul pyxdameraulevenshtein despit follow usual wai creat zip file import modul result importerror user seek help includ modul successfulli",
        "Answer_original_content":"view pyxdameraulevenshtein modul page packag download includ wheel file maco sourc code tar file think directli us maco share librari file darwin compat need compil suggest pyxdameraulevenshtein compil sourc code pyxdameraulevenshtein dll file window refer document python search write python script dll compil implement need refer thread us dll file python us dll python refer azur offic tutori write python script packag python script dll file zip file upload zip file us execut python script model hope help",
        "Answer_preprocessed_content":"view modul page packag download includ wheel file maco sourc code tar file think directli us maco share librari file darwin compat need compil suggest compil sourc code dll file window refer document python search write python script dll compil implement need refer thread us dll file python us dll python refer azur offic tutori write python script packag python script dll file zip file upload zip file us model hope help",
        "Answer_gpt_summary_original":"Solution:\n- Compile the source code of pyxdameraulevenshtein to a DLL file on Windows.\n- Write a Python script using the DLL you compiled to implement your needs.\n- Package your Python script and DLL file as a zip file, then upload the zip file to use it in Execute Python script model of Azure ML.",
        "Answer_gpt_summary":"solut compil sourc code pyxdameraulevenshtein dll file window write python script dll compil implement need packag python script dll file zip file upload zip file us execut python script model"
    },
    {
        "Question_title":"probability on azure recommendations api",
        "Question_body":"<p>I am using the azure recommendation api on <a href=\"http:\/\/recommendations.azurewebsites.net\/\" rel=\"nofollow noreferrer\">http:\/\/recommendations.azurewebsites.net\/<\/a>.\nI prepared the catalog to be like <code>&lt;Item Id&gt;<\/code>, <code>&lt;Item Name&gt;<\/code>, <code>&lt;Item Category&gt;<\/code>, <code>&lt;Features list&gt;<\/code> and the usage file : <code>&lt;userId&gt;<\/code>, <code>&lt;ItemId&gt;<\/code>.\nNow when I test the recommender, I always get a probability of 0.5 for all items, so I had to presume something is not right.\nIn order to know what's the problem I added two items to the catalog \none with same features as an other item but with different name and id,\nand an other item with different id and one different feature.\nI still get the 0.5 probability and now i'm sure something is not right but I still can figure out what the problem.<\/p>\n\n<p>here is a screenshot of what I get when I add the item to the cart<\/p>\n\n<p><img src=\"https:\/\/i.stack.imgur.com\/zhwiq.png\" alt=\"\"><\/p>\n\n<p>Is there any possibility to use the azure ml matchbox recommender with features and without ratings? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1472144085380,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1491136060303,
        "Question_score":0.0,
        "Question_view_count":98.0,
        "Answer_body":"<p>Tayehi, <\/p>\n\n<p>Nice to meet you. I am the program manager in charge of the recommendations API.\n2 things:<\/p>\n\n<ol>\n<li><p>If you get a 0.5 probability you are most likely getting \"default recommendations\". This usually means that you do not have enough training data or that there are not enough co-occurrences for the item you are testing in the data. To describe the extreme case, imagine an item A that only gets purchased with an item B only one or two times -- it would be hard to say with confidence (statistical significance) that someone that likes item A is also likely to like item B.<\/p><\/li>\n<li><p>It looks like you are still using the old recommendations API. I would like to encourage you to use our newer version (the Recommendations API cognitive service). Please take a look at <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/cognitive-services-migration-from-dm\/to\" rel=\"nofollow\">https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/cognitive-services-migration-from-dm\/to<\/a> help you in this process.<\/p><\/li>\n<\/ol>\n\n<p>Thanks!\nLuis Cabrera\nCortana Intelligence Applications.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1476476017407,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/39150834",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1476373208683,
        "Question_original_content":"probabl azur recommend api azur recommend api http recommend azurewebsit net prepar catalog like usag file test recommend probabl item presum right order know problem ad item catalog featur item differ item differ differ featur probabl sure right figur problem screenshot add item cart possibl us matchbox recommend featur rate",
        "Question_preprocessed_content":"probabl azur recommend api azur recommend api prepar catalog like usag file test recommend probabl item presum right order know problem ad item catalog featur item differ item differ differ featur probabl sure right figur problem screenshot add item cart possibl us matchbox recommend featur rate",
        "Question_gpt_summary_original":"The user is encountering challenges with the Azure Recommendation API, as they are consistently receiving a probability of 0.5 for all items, despite having prepared the catalog and usage file correctly. The user has attempted to troubleshoot the issue by adding new items to the catalog, but the problem persists. The user is also inquiring about the possibility of using the Azure ML Matchbox Recommender without ratings.",
        "Question_gpt_summary":"user encount challeng azur recommend api consist receiv probabl item despit have prepar catalog usag file correctli user attempt troubleshoot issu ad new item catalog problem persist user inquir possibl matchbox recommend rate",
        "Answer_original_content":"tayehi nice meet program manag charg recommend api thing probabl like get default recommend usual mean train data occurr item test data extrem case imagin item get purchas item time hard confid statist signific like item like like item look like old recommend api like encourag us newer version recommend api cognit servic look http azur microsoft com document articl cognit servic migrat help process thank lui cabrera cortana intellig applic",
        "Answer_preprocessed_content":"tayehi nice meet program manag charg recommend api thing probabl like get default recommend usual mean train data item test data extrem case imagin item get purchas item time hard confid like item like like item look like old recommend api like encourag us newer version look help process thank lui cabrera cortana intellig applic",
        "Answer_gpt_summary_original":"Solutions provided:\n- The user is most likely getting \"default recommendations\" due to insufficient training data or co-occurrences for the item being tested in the data.\n- The program manager suggests using the newer version of the Recommendations API cognitive service.",
        "Answer_gpt_summary":"solut provid user like get default recommend insuffici train data occurr item test data program manag suggest newer version recommend api cognit servic"
    },
    {
        "Question_title":"How to reuse successfully built docker images in Azure ML?",
        "Question_body":"<p>In our company I use Azure ML and I have the following issue. I specify a <em>conda_requirements.yaml<\/em> file with the PyTorch estimator class, like so (... are placeholders so that I do not have to type everything out):<\/p>\n\n<pre><code>from azureml.train.dnn import PyTorch\nest = PyTorch(source_directory=\u2019.\u2019, script_params=..., compute_target=..., entry_script=..., conda_dependencies_file_path=\u2019conda_requirements.yaml\u2019, environment_variables=..., framework_version=\u20191.1\u2019)\n<\/code><\/pre>\n\n<p>The <em>conda_requirements.yaml<\/em> (shortened version of the pip part) looks like this:<\/p>\n\n<pre><code>dependencies:\n  -  conda=4.5.11\n  -  conda-package-handling=1.3.10\n  -  python=3.6.2\n  -  cython=0.29.10\n  -  scikit-learn==0.21.2\n  -  anaconda::cloudpickle==1.2.1\n  -  anaconda::cffi==1.12.3\n  -  anaconda::mxnet=1.1.0\n  -  anaconda::psutil==5.6.3\n  -  anaconda::pip=19.1.1\n  -  anaconda::six==1.12.0\n  -  anaconda::mkl==2019.4\n  -  conda-forge::openmpi=3.1.2\n  -  conda-forge::pycparser==2.19\n  -  tensorboard==1.13.1\n  -  tensorflow==1.13.1\n  -  pip:\n        - torch==1.1.0\n        - torchvision==0.2.1\n<\/code><\/pre>\n\n<p>This successfully builds on Azure. Now in order to reuse the resulting docker image in that case, I use the <code>custom_docker_image<\/code> parameter to pass to the <\/p>\n\n<pre><code>from azureml.train.estimator import Estimator\nest = Estimator(source_directory=\u2019.\u2019, script_params=..., compute_target=..., entry_script=..., custom_docker_image=\u2019&lt;container registry name&gt;.azurecr.io\/azureml\/azureml_c3a4f...\u2019, environment_variables=...)\n<\/code><\/pre>\n\n<p>But now Azure somehow seems to rebuild the image again and when I run the experiment it cannot install torch. So it seems to only install the conda dependencies and not the pip dependencies, but actually I do not want Azure to rebuild the image. Can I solve this somehow?<\/p>\n\n<p>I attempted to somehow build a docker image from my Docker file and then add to the registry. I can do az login and according to <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-authentication<\/a> I then should also be able to do an acr login and push. This does not work. \nEven using the credentials from<\/p>\n\n<pre><code>az acr credential show \u2013name &lt;container registry name&gt;\n<\/code><\/pre>\n\n<p>and then doing a <\/p>\n\n<pre><code>docker login &lt;container registry name&gt;.azurecr.io \u2013u &lt;username from credentials above&gt; -p &lt;password from credentials above&gt;\n<\/code><\/pre>\n\n<p>does not work.\nThe error message is <em>authentication required<\/em> even though I used <\/p>\n\n<pre><code>az login\n<\/code><\/pre>\n\n<p>successfully. Would also be happy if someone could explain that to me in addition to how to reuse docker images when using Azure ML.\nThank you!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1565379277197,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":604.0,
        "Answer_body":"<p>AzureML should actually cache your docker image once it was created. The service will hash the base docker info and the contents of the conda.yaml file and will use that as the hash key -- unless you change any of that information, the docker should come from the ACR. <\/p>\n\n<p>As for the custom docker usage, did you set the parameter <code>user_managed=True<\/code>? Otherwise, AzureML will consider your docker to be a base image on top of which it will create the conda environment per your yaml file.<br>\nThere is an example of how to use a custom docker image in this notebook:\n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57436140",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1565501907287,
        "Question_original_content":"reus successfulli built docker imag compani us follow issu specifi conda requir yaml file pytorch estim class like placehold type train dnn import pytorch est pytorch sourc directori script param comput target entri script conda depend file path conda requir yaml environ variabl framework version conda requir yaml shorten version pip look like depend conda conda packag handl python cython scikit learn anaconda cloudpickl anaconda cffi anaconda mxnet anaconda psutil anaconda pip anaconda anaconda mkl conda forg openmpi conda forg pycpars tensorboard tensorflow pip torch torchvis successfulli build azur order reus result docker imag case us custom docker imag paramet pass train estim import estim est estim sourc directori script param comput target entri script custom docker imag azurecr caf environ variabl azur rebuild imag run experi instal torch instal conda depend pip depend actual want azur rebuild imag solv attempt build docker imag docker file add registri login accord http doc microsoft com azur contain registri contain registri authent abl acr login push work credenti acr credenti docker login azurecr work error messag authent requir login successfulli happi explain addit reus docker imag thank",
        "Question_preprocessed_content":"reus successfulli built docker imag compani us follow issu specifi file pytorch estim class like look like successfulli build azur order reus result docker imag case us paramet pass azur rebuild imag run experi instal torch instal conda depend pip depend actual want azur rebuild imag solv attempt build docker imag docker file add registri login accord abl acr login push work credenti work error messag authent requir successfulli happi explain addit reus docker imag thank",
        "Question_gpt_summary_original":"The user is facing challenges in reusing a successfully built docker image in Azure ML. They have specified a conda_requirements.yaml file with the PyTorch estimator class, which successfully builds on Azure. However, when attempting to reuse the resulting docker image, Azure seems to rebuild the image again and cannot install torch. The user has attempted to build a docker image from their Docker file and add it to the registry, but authentication is required even though they have successfully logged in.",
        "Question_gpt_summary":"user face challeng reus successfulli built docker imag specifi conda requir yaml file pytorch estim class successfulli build azur attempt reus result docker imag azur rebuild imag instal torch user attempt build docker imag docker file add registri authent requir successfulli log",
        "Answer_original_content":"actual cach docker imag creat servic hash base docker info content conda yaml file us hash kei chang inform docker come acr custom docker usag set paramet user manag true consid docker base imag creat conda environ yaml file exampl us custom docker imag notebook http github com azur machinelearningnotebook blob aeddedebdbabafbdce us train deep learn us estim us estim ipynb",
        "Answer_preprocessed_content":"actual cach docker imag creat servic hash base docker info content file us hash kei chang inform docker come acr custom docker usag set paramet consid docker base imag creat conda environ yaml file exampl us custom docker imag notebook",
        "Answer_gpt_summary_original":"Solution 1: AzureML caches the docker image once it is created, so if the base docker info and the contents of the conda.yaml file remain the same, the docker should come from the ACR.\n\nSolution 2: Set the parameter \"user_managed=True\" to use a custom docker image. There is an example of how to use a custom docker image in this notebook: https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/4170a394edd36413edebdbab347afb0d833c94ee\/how-to-use-azureml\/training-with-deep-learning\/how-to-use-estimator\/how-to-use-estimator.ipynb",
        "Answer_gpt_summary":"solut cach docker imag creat base docker info content conda yaml file remain docker come acr solut set paramet user manag true us custom docker imag exampl us custom docker imag notebook http github com azur machinelearningnotebook blob aeddedebdbabafbdce us train deep learn us estim us estim ipynb"
    },
    {
        "Question_title":"Azure Machine Learning with on premise SQL Server",
        "Question_body":"Hi is there a way for Azure Machine Learning to be able to perform analytics using data from an on premise SQL Server?\n\nOnly found the below article which is for Azure Machine Learning Studio (classic):\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/use-data-from-an-on-premises-sql-server\n\nThanks.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1592874857830,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@conrad Here is the link to connect with the Azure SQL server.\nhttps:\/\/stackoverflow.com\/questions\/61806350\/database-communication-link-error-occurded-on-azure-ml-service-used-azure-sql-s\/61950481#61950481",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/38894\/azure-machine-learning-with-on-premise-sql-server-1.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1592900268572,
        "Question_original_content":"premis sql server wai abl perform analyt data premis sql server articl studio classic http doc microsoft com azur machin learn studio us data premis sql server thank",
        "Question_preprocessed_content":"premis sql server wai abl perform analyt data premis sql server articl studio thank",
        "Question_gpt_summary_original":"The user is looking for a way to use Azure Machine Learning to perform analytics using data from an on-premise SQL Server. They have only found an article for Azure Machine Learning Studio (classic) and are seeking further assistance.",
        "Question_gpt_summary":"user look wai us perform analyt data premis sql server articl studio classic seek assist",
        "Answer_original_content":"conrad link connect azur sql server http stackoverflow com question databas commun link error occurd azur servic azur sql",
        "Answer_preprocessed_content":"link connect azur sql server",
        "Answer_gpt_summary_original":"Solution: In the discussion, a solution is provided in the form of a link to connect with the Azure SQL server. The link provides guidance on how to resolve a communication link error that occurred when using Azure Machine Learning Service with Azure SQL Server.",
        "Answer_gpt_summary":"solut discuss solut provid form link connect azur sql server link provid guidanc resolv commun link error occur servic azur sql server"
    },
    {
        "Question_title":"Fisher Linear Discriminant Analysis Azure",
        "Question_body":"How is the output of Fisher Linear Discriminant Analysis experiment interpreted now that the column labels in the output are replaced with Col1, Col2, Col3.......etc? How can the model be used to predict clusters of other input data as deployed web service requires even the dependent valuable(the same same ones we wish to predict)?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1621855005240,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Are you referring to the categories generated from LDA module? If so, then that's expected. LDA is an unsupervised technique, it groups words into categories\/topics and it's up to the analyst to interpret it by observing the results and transforming the output dataset accordingly. Here's are some examples of LDA approach in Azure AI Gallery. Hope this helps.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/407053\/fisher-linear-discriminant-analysis-azure.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1621895240423,
        "Question_original_content":"fisher linear discrimin analysi azur output fisher linear discrimin analysi experi interpret column label output replac col col col model predict cluster input data deploi web servic requir depend valuabl on wish predict",
        "Question_preprocessed_content":"fisher linear discrimin analysi azur output fisher linear discrimin analysi experi interpret column label output replac col col model predict cluster input data deploi web servic requir depend valuabl",
        "Question_gpt_summary_original":"The user is facing challenges in interpreting the output of a Fisher Linear Discriminant Analysis experiment after the column labels were replaced with generic names. Additionally, the user is unsure how to use the model to predict clusters of other input data as the deployed web service requires the same dependent variables that are being predicted.",
        "Question_gpt_summary":"user face challeng interpret output fisher linear discrimin analysi experi column label replac gener name addition user unsur us model predict cluster input data deploi web servic requir depend variabl predict",
        "Answer_original_content":"refer categori gener lda modul expect lda unsupervis techniqu group word categori topic analyst interpret observ result transform output dataset accordingli exampl lda approach azur galleri hope help",
        "Answer_preprocessed_content":"refer categori gener lda modul expect lda unsupervis techniqu group word analyst interpret observ result transform output dataset accordingli exampl lda approach azur galleri hope help",
        "Answer_gpt_summary_original":"Solutions provided: \n- The user is advised to observe the results and transform the output dataset accordingly as LDA is an unsupervised technique. \n- The user is provided with examples of LDA approach in Azure AI Gallery.",
        "Answer_gpt_summary":"solut provid user advis observ result transform output dataset accordingli lda unsupervis techniqu user provid exampl lda approach azur galleri"
    },
    {
        "Question_title":"Using AWS services to perform pet face recognition",
        "Question_body":"I want to build a camera that automatically recognizes a specific pet with image or video recognition. What AWS service can I use to identify an individual pet (not just the pet type). I've tried to use AWS Rekognition, but it can only differentiate between animal types, race, or color. Amazon SageMaker could be another option to create a completely new mode, but is very costly. What other AWS services can I use to identify specific pets?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1601645649000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":132.0,
        "Answer_body":"You can use Amazon Rekognition Custom Labels to use single class object detection to identify or classify a specific animal. However, note that Amazon Rekognition Custom Labels do not perform animal face recognition. It only classifies the image or object.\n\nFor example, you can train your detection model to identify an animal based on the images you provide for that label. For more information about using Amazon Rekognition Custom Labels, see this blog: https:\/\/aws.amazon.com\/blogs\/machine-learning\/training-a-custom-single-class-object-detection-model-with-amazon-rekognition-custom-labels\/.\n\nTo use frames from a video with Custom Labels, see Video analysis .",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU2S-z85hhSr-XVecDxE3ihw\/using-aws-services-to-perform-pet-face-recognition",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1601648944000,
        "Question_original_content":"aw servic perform pet face recognit want build camera automat recogn specif pet imag video recognit aw servic us identifi individu pet pet type tri us aw rekognit differenti anim type race color option creat complet new mode costli aw servic us identifi specif pet",
        "Question_preprocessed_content":"aw servic perform pet face recognit want build camera automat recogn specif pet imag video recognit aw servic us identifi individu pet tri us aw rekognit differenti anim type race color option creat complet new mode costli aw servic us identifi specif pet",
        "Question_gpt_summary_original":"The user is facing challenges in identifying individual pets using AWS services. They have tried using AWS Rekognition, but it can only differentiate between animal types, race, or color. The user is also hesitant to use Amazon SageMaker due to its high cost. They are seeking advice on other AWS services that can help them identify specific pets.",
        "Question_gpt_summary":"user face challeng identifi individu pet aw servic tri aw rekognit differenti anim type race color user hesit us high cost seek advic aw servic help identifi specif pet",
        "Answer_original_content":"us amazon rekognit custom label us singl class object detect identifi classifi specif anim note amazon rekognit custom label perform anim face recognit classifi imag object exampl train detect model identifi anim base imag provid label inform amazon rekognit custom label blog http aw amazon com blog machin learn train custom singl class object detect model amazon rekognit custom label us frame video custom label video analysi",
        "Answer_preprocessed_content":"us amazon rekognit custom label us singl class object detect identifi classifi specif anim note amazon rekognit custom label perform anim face recognit classifi imag object exampl train detect model identifi anim base imag provid label inform amazon rekognit custom label blog us frame video custom label video analysi",
        "Answer_gpt_summary_original":"Solution: The user can use Amazon Rekognition Custom Labels to identify or classify a specific animal. They can train their detection model to identify an animal based on the images they provide for that label. However, note that Amazon Rekognition Custom Labels do not perform animal face recognition. It only classifies the image or object. Another solution is to use frames from a video with Custom Labels. No solution was provided for the high cost of Amazon SageMaker.",
        "Answer_gpt_summary":"solut user us amazon rekognit custom label identifi classifi specif anim train detect model identifi anim base imag provid label note amazon rekognit custom label perform anim face recognit classifi imag object solut us frame video custom label solut provid high cost"
    },
    {
        "Question_title":"Jupyter notebook seems to remember previous path (!pwd) after being moved to a different directory?",
        "Question_body":"<p>I initially had a notebook in one directory in AWS SageMaker JupyterLab, say <code>\/A<\/code>, but then moved it into <code>\/A\/B<\/code>. However, when I run <code>!pwd<\/code> in a jupyter notebook cell, I still get <code>\/A<\/code>. This happens even when I press 'restart kernel'. How does the notebook remember this, and is there a way to prevent or reset this?<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1597055996810,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597057789608,
        "Question_score":0.0,
        "Question_view_count":118.0,
        "Answer_body":"<p>I was actually using AWS SageMaker, and restarting the kernel from the toolbar was not enough. I needed to restart the kernel session, by pressing 'shut down' in the &quot;Running terminals and kernels&quot; section on the left navigation.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/934Fe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/934Fe.png\" alt=\"Shut down button seen on left navigation of JupterLab\" \/><\/a><\/p>\n<p>They are currently <a href=\"https:\/\/github.com\/jupyterlab\/jupyterlab\/issues\/5989\" rel=\"nofollow noreferrer\">discussing<\/a> warning users about the need to restart the kernel when a notebook is moved.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63338577",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1597057792807,
        "Question_original_content":"jupyt notebook rememb previou path pwd move differ directori initi notebook directori jupyterlab move run pwd jupyt notebook cell happen press restart kernel notebook rememb wai prevent reset thank",
        "Question_preprocessed_content":"jupyt notebook rememb previou path move differ directori initi notebook directori jupyterlab move run jupyt notebook cell happen press restart kernel notebook rememb wai prevent reset thank",
        "Question_gpt_summary_original":"The user has encountered a challenge where their Jupyter notebook in AWS SageMaker JupyterLab still remembers the previous path after being moved to a different directory. Even after restarting the kernel, the notebook still displays the previous path when the user runs the !pwd command. The user is seeking a solution to prevent or reset this issue.",
        "Question_gpt_summary":"user encount challeng jupyt notebook jupyterlab rememb previou path move differ directori restart kernel notebook displai previou path user run pwd command user seek solut prevent reset issu",
        "Answer_original_content":"actual restart kernel toolbar need restart kernel session press shut run termin kernel section left navig current discuss warn user need restart kernel notebook move",
        "Answer_preprocessed_content":"actual restart kernel toolbar need restart kernel session press shut run termin kernel section left navig current discuss warn user need restart kernel notebook move",
        "Answer_gpt_summary_original":"Solution: The user can try restarting the kernel session by pressing the 'shut down' button in the \"Running terminals and kernels\" section on the left navigation of AWS SageMaker JupyterLab. It is also being discussed to warn users about the need to restart the kernel when a notebook is moved.",
        "Answer_gpt_summary":"solut user try restart kernel session press shut button run termin kernel section left navig jupyterlab discuss warn user need restart kernel notebook move"
    },
    {
        "Question_title":"How to use AWS SageMaker and S3 for Object Detection?",
        "Question_body":"<p>I am looking to run a pre-trained object detection model onto a folder of ~400k images which is about 1.5GB. When I've tried running locally, it was estimated to take ~8 days to complete (with keras yolov3). Thus, I am looking to use AWS SageMaker and S3.<\/p>\n<p>When I have uploaded the zip folder of my images in the SageMaker jupyter notebook and tried to unzip by using bash command, an error pops ups saying that I have insufficient space. The volume assigned to my notebook is 5GB EBS, I do have other heavy datasets in my jupyter notebook space which could be causing this issue.<\/p>\n<p>To tackle that, I am looking for a way where I can upload my data to S3 and run SageMaker to read the images hosted and run an object detection model over. However, it does not look like there's a method to unzip folders on S3 without using an additional service (read that AWS Lambda may help) as these services are paid by my school.<\/p>\n<p>I could possibly re-run my code to extract my images from URL. In this case, how can I save these images to S3 directly in this case? Also, does anyone know if I am able to run yolov3 on SageMaker or if there is a better model I can look to use. Appreciate any advice that may help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_created_time":1604600896053,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":256.0,
        "Answer_body":"<p>yes u are right u can upload thousands of images using aws cli using $aws s3 cp  ; or $aws s3 sync<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64703268",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1604629425800,
        "Question_original_content":"us object detect look run pre train object detect model folder imag tri run local estim dai complet kera yolov look us upload zip folder imag jupyt notebook tri unzip bash command error pop up sai insuffici space volum assign notebook eb heavi dataset jupyt notebook space caus issu tackl look wai upload data run read imag host run object detect model look like method unzip folder addit servic read aw lambda help servic paid school possibl run code extract imag url case save imag directli case know abl run yolov better model look us appreci advic help",
        "Question_preprocessed_content":"us object detect look run object detect model folder imag tri run local estim dai complet look us upload zip folder imag jupyt notebook tri unzip bash command error pop up sai insuffici space volum assign notebook eb heavi dataset jupyt notebook space caus issu tackl look wai upload data run read imag host run object detect model look like method unzip folder addit servic servic paid school possibl code extract imag url case save imag directli case know abl run yolov better model look us appreci advic help",
        "Question_gpt_summary_original":"The user is facing challenges in running a pre-trained object detection model on a folder of ~400k images locally, which is estimated to take ~8 days to complete. To solve this issue, the user is looking to use AWS SageMaker and S3, but encountered an error of insufficient space when trying to unzip the uploaded zip folder of images in the SageMaker jupyter notebook. The user is now looking for a way to upload the data to S3 and run SageMaker to read the images hosted and run an object detection model over. However, there is no method to unzip folders on S3 without using an additional paid service. The user is also considering re-running the code to extract images from URL and save them directly to S3. The user is seeking advice on whether it is possible to run yolov3 on SageMaker or if there is a better model to use.",
        "Question_gpt_summary":"user face challeng run pre train object detect model folder imag local estim dai complet solv issu user look us encount error insuffici space try unzip upload zip folder imag jupyt notebook user look wai upload data run read imag host run object detect model method unzip folder addit paid servic user consid run code extract imag url save directli user seek advic possibl run yolov better model us",
        "Answer_original_content":"ye right upload thousand imag aw cli aw aw sync",
        "Answer_preprocessed_content":"ye right upload thousand imag aw cli aw aw sync",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use AWS CLI to upload the images to S3 using the command \"$aws s3 cp\" or \"$aws s3 sync\". However, no solution was provided for the issue of insufficient space when unzipping the uploaded zip folder of images in the SageMaker jupyter notebook. The user is also considering re-running the code to extract images from URL and save them directly to S3. Additionally, there was a question raised about whether it is possible to run yolov3 on SageMaker or if there is a better model to use.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us aw cli upload imag command aw aw sync solut provid issu insuffici space unzip upload zip folder imag jupyt notebook user consid run code extract imag url save directli addition question rais possibl run yolov better model us"
    },
    {
        "Question_title":"AWS Sagemaker custom user algorithms: how to take advantage of extra instances",
        "Question_body":"<p>This is a fundamental AWS Sagemaker question. When I run training with one of Sagemaker's built in algorithms I am able to take advantage of the massive speedup from distributing the job to many instances by increasing the instance_count argument of the training algorithm. However, when I package my own custom algorithm then increasing the instance count seems to just duplicate the training on every instance, leading to no speedup. <\/p>\n\n<p>I suspect that when I am packaging my own algorithm there is something special I need to do to control how it handles the training differently for a particular instance inside of the my custom train() function (otherwise, how would it know how the job should be distributed?), but I have not been able to find any discussion of how to do this online. <\/p>\n\n<p>Does anyone know how to handle this? Thank you very much in advance.<\/p>\n\n<p>Specific examples:\n=> It works well in a standard algorithm: I verified that increasing train_instance_count in the first documented sagemaker example speeds things up here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-train-model-create-training-job.html<\/a><\/p>\n\n<p>=> It does not work in my custom algorithm. I tried taking the standard sklearn build-your-own-model example and adding a few extra sklearn variants inside of the training and then printing out results to compare. When I increase the train_instance_count that is passed to the Estimator object, it runs the same training on every instance, so the output gets duplicated across each instance (the printouts of the results are duplicated) and there is no speedup.\nThis is the sklearn example base: <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a> . The third argument of the Estimator object partway down in this notebook is what lets you control the number of training instances.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1517249392277,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1369.0,
        "Answer_body":"<p>Distributed training requires having a way to sync the results of the training between the training workers. Most of the traditional libraries, such as scikit-learn are designed to work with a single worker, and can't just be used in a distributed environment. Amazon SageMaker is distributing the data across the workers, but it is up to you to make sure that the algorithm can benefit from the multiple workers. Some algorithms, such as Random Forest, are easier to take advantage of the distribution, as each worker can build a different part of the forest, but other algorithms need more help. <\/p>\n\n<p>Spark MLLib has distributed implementations of popular algorithms such as k-means, logistic regression, or PCA, but these implementations are not good enough for some cases. Most of them were too slow and some even crushed when a lot of data was used for the training. The Amazon SageMaker team reimplemented many of these algorithms from scratch to benefit from the scale and economics of the cloud (20 hours of one instance costs the same as 1 hour of 20 instances, just 20 times faster). Many of these algorithms are now more stable and much faster beyond the linear scalability. See more details here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/algos.html<\/a><\/p>\n\n<p>For the deep learning frameworks (TensorFlow and MXNet) SageMaker is using the built-in parameters server that each one is using, but it is taking the heavy lifting of the building the cluster and configuring the instances to communicate with it. <\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48507471",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1517706762272,
        "Question_original_content":"custom user algorithm advantag extra instanc fundament question run train built algorithm abl advantag massiv speedup distribut job instanc increas instanc count argument train algorithm packag custom algorithm increas instanc count duplic train instanc lead speedup suspect packag algorithm special need control handl train differ particular instanc insid custom train function know job distribut abl discuss onlin know handl thank advanc specif exampl work standard algorithm verifi increas train instanc count document exampl speed thing http doc aw amazon com latest train model creat train job html work custom algorithm tri take standard sklearn build model exampl ad extra sklearn variant insid train print result compar increas train instanc count pass estim object run train instanc output get duplic instanc printout result duplic speedup sklearn exampl base http github com awslab amazon exampl blob master advanc function scikit bring scikit bring ipynb argument estim object partwai notebook let control number train instanc",
        "Question_preprocessed_content":"custom user algorithm advantag extra instanc fundament question run train built algorithm abl advantag massiv speedup distribut job instanc increas argument train algorithm packag custom algorithm increas instanc count duplic train instanc lead speedup suspect packag algorithm special need control handl train differ particular instanc insid custom train function abl discuss onlin know handl thank advanc specif exampl work standard algorithm verifi increas document exampl speed thing work custom algorithm tri take standard sklearn exampl ad extra sklearn variant insid train print result compar increas pass estim object run train instanc output get duplic instanc speedup sklearn exampl base argument estim object partwai notebook let control number train instanc",
        "Question_gpt_summary_original":"The user is facing challenges in taking advantage of the speedup from distributing the job to many instances when using their own custom algorithm in AWS Sagemaker. Increasing the instance count duplicates the training on every instance, leading to no speedup. The user suspects that they need to control how their custom train() function handles the training differently for a particular instance, but they have not been able to find any discussion of how to do this online. The user has provided specific examples of how it works well in a standard algorithm but not in their custom algorithm.",
        "Question_gpt_summary":"user face challeng take advantag speedup distribut job instanc custom algorithm increas instanc count duplic train instanc lead speedup user suspect need control custom train function handl train differ particular instanc abl discuss onlin user provid specif exampl work standard algorithm custom algorithm",
        "Answer_original_content":"distribut train requir have wai sync result train train worker tradit librari scikit learn design work singl worker distribut environ distribut data worker sure algorithm benefit multipl worker algorithm random forest easier advantag distribut worker build differ forest algorithm need help spark mllib distribut implement popular algorithm mean logist regress pca implement good case slow crush lot data train team reimplement algorithm scratch benefit scale econom cloud hour instanc cost hour instanc time faster algorithm stabl faster linear scalabl detail http doc aw amazon com latest algo html deep learn framework tensorflow mxnet built paramet server take heavi lift build cluster configur instanc commun",
        "Answer_preprocessed_content":"distribut train requir have wai sync result train train worker tradit librari design work singl worker distribut environ distribut data worker sure algorithm benefit multipl worker algorithm random forest easier advantag distribut worker build differ forest algorithm need help spark mllib distribut implement popular algorithm logist regress pca implement good case slow crush lot data train team reimplement algorithm scratch benefit scale econom cloud algorithm stabl faster linear scalabl detail deep learn framework paramet server take heavi lift build cluster configur instanc commun",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion for the user's specific challenge of controlling how their custom train() function handles the training differently for a particular instance in AWS Sagemaker. However, the discussion provides information on how distributed training works in general and how some algorithms are easier to take advantage of the distribution than others. It also mentions that Amazon SageMaker has re-implemented many popular algorithms from scratch to benefit from the scale and economics of the cloud, and provides links to more details on this. Additionally, the discussion mentions that for deep learning frameworks, SageMaker is using the built-in parameters server that each one is using, but it is taking the heavy lifting of building the cluster and configuring the instances to communicate with it.",
        "Answer_gpt_summary":"solut provid discuss user specif challeng control custom train function handl train differ particular instanc discuss provid inform distribut train work gener algorithm easier advantag distribut mention implement popular algorithm scratch benefit scale econom cloud provid link detail addition discuss mention deep learn framework built paramet server take heavi lift build cluster configur instanc commun"
    },
    {
        "Question_title":"Deploy SageMaker model to IoT Greengrass in different account?",
        "Question_body":"Is it possible to deploy a model created by SageMaker in one account to an IoT Greengrass device in a different account?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1556295446000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":"For IoT Greengrass 1.x, this is possible but not trivial. From the console this is not possible, as you can only select buckets or SageMaker jobs from the same account, but you can refer to resources in other accounts if you use the CLI or the API.\n\nYou have to create a new Resource Definition Version with the correct data specifying the model resource and then add it to your group definition. For permissions in the source account, you must set up the S3 bucket policy to allow access from the destination account. For permissions in the destination account, you must update the IoT Greengrass service role policy to access the model resource in the source account.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJha7KbxOTXuhMRGbMYGC0g\/deploy-sage-maker-model-to-io-t-greengrass-in-different-account",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1556529654000,
        "Question_original_content":"deploi model iot greengrass differ account possibl deploi model creat account iot greengrass devic differ account",
        "Question_preprocessed_content":"deploi model iot greengrass differ account possibl deploi model creat account iot greengrass devic differ account",
        "Question_gpt_summary_original":"The user is facing a challenge of deploying a SageMaker model to an IoT Greengrass device in a different account. They are unsure if this is possible and seeking clarification.",
        "Question_gpt_summary":"user face challeng deploi model iot greengrass devic differ account unsur possibl seek clarif",
        "Answer_original_content":"iot greengrass possibl trivial consol possibl select bucket job account refer resourc account us cli api creat new resourc definit version correct data specifi model resourc add group definit permiss sourc account set bucket polici allow access destin account permiss destin account updat iot greengrass servic role polici access model resourc sourc account",
        "Answer_preprocessed_content":"iot greengrass possibl trivial consol possibl select bucket job account refer resourc account us cli api creat new resourc definit version correct data specifi model resourc add group definit permiss sourc account set bucket polici allow access destin account permiss destin account updat iot greengrass servic role polici access model resourc sourc account",
        "Answer_gpt_summary_original":"Solution: It is possible to deploy a SageMaker model to an IoT Greengrass device in a different account, but it is not trivial. From the console, it is not possible, but it can be done using the CLI or the API. You have to create a new Resource Definition Version with the correct data specifying the model resource and then add it to your group definition. For permissions in the source account, you must set up the S3 bucket policy to allow access from the destination account. For permissions in the destination account, you must update the IoT Greengrass service role policy to access the model resource in the source account.",
        "Answer_gpt_summary":"solut possibl deploi model iot greengrass devic differ account trivial consol possibl cli api creat new resourc definit version correct data specifi model resourc add group definit permiss sourc account set bucket polici allow access destin account permiss destin account updat iot greengrass servic role polici access model resourc sourc account"
    },
    {
        "Question_title":"XGBoost Model in AWS-Sagemaker Fails with no error message",
        "Question_body":"<p>I'm trying to get a model using the XGBoost classifier in AWS-Sagemaker.  I'm following the abalone example, but when I run it to build the training job it states InProgress 3 times and then just states Failed.  Where do I go to find why it failed?  <\/p>\n\n<p>I've double checked the parameters and made sure the input and output files and directories in S3 were correct.  I know there is permission to read and write because when setting up the data for train\/validate\/test I read and write to S3 with no problems.<\/p>\n\n<pre><code>print(status)\nwhile status !='Completed' and status!='Failed':\n    time.sleep(60)\n    status = client.describe_training_job(TrainingJobName=job_name)['TrainingJobStatus']\n    print(status)\n<\/code><\/pre>\n\n<p>That is the code where the print statements come from.  Is there something I can add to receive a better error message?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1562946273023,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":204.0,
        "Answer_body":"<p>The problem occurred was that the file sent for predictions was csv but the XGBoost settings were set to receive libsvm.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57010184",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1564494294316,
        "Question_original_content":"xgboost model aw fail error messag try model xgboost classifi aw follow abalon exampl run build train job state inprogress time state fail fail doubl check paramet sure input output file directori correct know permiss read write set data train valid test read write problem print statu statu complet statu fail time sleep statu client train job trainingjobnam job trainingjobstatu print statu code print statement come add receiv better error messag",
        "Question_preprocessed_content":"xgboost model aw fail error messag try model xgboost classifi follow abalon exampl run build train job state inprogress time state fail fail doubl check paramet sure input output file directori correct know permiss read write set data read write problem code print statement come add receiv better error messag",
        "Question_gpt_summary_original":"The user is facing challenges while trying to build a training job using the XGBoost classifier in AWS-Sagemaker. The job fails without any error message, and the user is unable to determine the reason for the failure. The user has checked the parameters and ensured that the input and output files and directories in S3 are correct. The user is also able to read and write to S3 without any problems. The user is seeking help to receive a better error message.",
        "Question_gpt_summary":"user face challeng try build train job xgboost classifi aw job fail error messag user unabl determin reason failur user check paramet ensur input output file directori correct user abl read write problem user seek help receiv better error messag",
        "Answer_original_content":"problem occur file sent predict csv xgboost set set receiv libsvm",
        "Answer_preprocessed_content":"problem occur file sent predict csv xgboost set set receiv libsvm",
        "Answer_gpt_summary_original":"Solution: The issue was identified as a mismatch between the file format sent for predictions and the XGBoost settings. The XGBoost classifier was set to receive libsvm format, but the file sent was in csv format. The solution is to ensure that the file format matches the XGBoost settings.",
        "Answer_gpt_summary":"solut issu identifi mismatch file format sent predict xgboost set xgboost classifi set receiv libsvm format file sent csv format solut ensur file format match xgboost set"
    },
    {
        "Question_title":"How to write Azure machine learning batch scoring results to data lake?",
        "Question_body":"<p>I'm trying to write the output of batch scoring into datalake:<\/p>\n<pre><code>    parallel_step_name = &quot;batchscoring-&quot; + datetime.now().strftime(&quot;%Y%m%d%H%M&quot;)\n    \n    output_dir = PipelineData(name=&quot;scores&quot;, \n                              datastore=def_ADL_store,\n                              output_mode=&quot;upload&quot;,\n                              output_path_on_compute=&quot;path in data lake&quot;)\n\nparallel_run_config = ParallelRunConfig(\n    environment=curated_environment,\n    entry_script=&quot;use_model.py&quot;,\n    source_directory=&quot;.\/&quot;,\n    output_action=&quot;append_row&quot;,\n    mini_batch_size=&quot;20&quot;,\n    error_threshold=1,\n    compute_target=compute_target,\n    process_count_per_node=2,\n    node_count=2\n)\n    \n    batch_score_step = ParallelRunStep(\n        name=parallel_step_name,\n        inputs=[test_data.as_named_input(&quot;test_data&quot;)],\n        output=output_dir,\n        parallel_run_config=parallel_run_config,\n        allow_reuse=False\n    )\n<\/code><\/pre>\n<p>However I meet the error: &quot;code&quot;: &quot;UserError&quot;,\n&quot;message&quot;: &quot;User program failed with Exception: Missing argument --output or its value is empty.&quot;<\/p>\n<p>How can I write results of batch score to data lake?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1596780750297,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":277.0,
        "Answer_body":"<p>I don\u2019t think ADLS is supported for <code>PipelineData<\/code>. My suggestion is to use the workspace\u2019s default blob store for the <code>PipelineData<\/code>, then use a <code>DataTransferStep<\/code> for after the <code>ParallelRunStep<\/code> is completed.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63296185",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1596781453976,
        "Question_original_content":"write batch score result data lake try write output batch score datalak parallel step batchscor datetim strftime output dir pipelinedata score datastor def adl store output mode upload output path comput path data lake parallel run config parallelrunconfig environ curat environ entri script us model sourc directori output action append row mini batch size error threshold comput target comput target process count node node count batch score step parallelrunstep parallel step input test data name input test data output output dir parallel run config parallel run config allow reus fals meet error code usererror messag user program fail except miss argument output valu write result batch score data lake",
        "Question_preprocessed_content":"write batch score result data lake try write output batch score datalak meet error code usererror messag user program fail except miss argument valu write result batch score data lake",
        "Question_gpt_summary_original":"The user is encountering an error while trying to write the output of batch scoring into a data lake. The error message indicates that the user is missing an argument or its value is empty. The user is seeking guidance on how to write the results of batch scoring to the data lake.",
        "Question_gpt_summary":"user encount error try write output batch score data lake error messag indic user miss argument valu user seek guidanc write result batch score data lake",
        "Answer_original_content":"dont think adl support pipelinedata suggest us workspac default blob store pipelinedata us datatransferstep parallelrunstep complet",
        "Answer_preprocessed_content":"dont think adl support suggest us workspac default blob store us complet",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use the workspace's default blob store for the PipelineData and then use a DataTransferStep after the ParallelRunStep is completed. It is suggested that ADLS is not supported for PipelineData.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us workspac default blob store pipelinedata us datatransferstep parallelrunstep complet suggest adl support pipelinedata"
    },
    {
        "Question_title":"Azure Machine Learning - Access uri_folder dataset (ML v2) from notebook (not job)",
        "Question_body":"Hi all,\n\nI've registered in Azure Machine Learning a Data Lake Gen2 datastore that point to a container with a hierarchy of folders that contain avro files and on top of it I registered a folder_uri dataset (ML v2).\n\nNow I want to access to these folders from a notebook, convert them in a pandas dataframe in order to do some data exploration.\n\nI search on the documentation, and I only found examples that run job and using this type of dataset as input, but I need to be able to explore it using notebook.\n\nIs it possible? How can I do it?\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1666794361980,
        "Question_favorite_count":18.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@GCocci Thanks for the question. Currently it's not supported to access the avro files. Here is the document for accessing the datastore using folder_uri dataset.\nhttps:\/\/learn.microsoft.com\/en-us\/azure\/machine-learning\/migrate-to-v2-resource-datastore\n\nMapping Data Flow supports AVRO as a source type https:\/\/learn.microsoft.com\/en-us\/azure\/data-factory\/data-flow-source#supported-sources",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1063867\/azure-machine-learning-access-uri-folder-dataset-m.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1666874705140,
        "Question_original_content":"access uri folder dataset notebook job regist data lake gen datastor point contain hierarchi folder contain avro file regist folder uri dataset want access folder notebook convert panda datafram order data explor search document exampl run job type dataset input need abl explor notebook possibl thank",
        "Question_preprocessed_content":"access dataset notebook regist data lake gen datastor point contain hierarchi folder contain avro file regist dataset want access folder notebook convert panda datafram order data explor search document exampl run job type dataset input need abl explor notebook possibl thank",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing a folder_uri dataset (ML v2) from a notebook in Azure Machine Learning. They have registered a Data Lake Gen2 datastore that points to a container with a hierarchy of folders containing avro files. The user wants to convert the folders into a pandas dataframe for data exploration but has only found examples that run jobs using this type of dataset as input. The user is seeking guidance on how to explore the dataset using a notebook.",
        "Question_gpt_summary":"user face challeng access folder uri dataset notebook regist data lake gen datastor point contain hierarchi folder contain avro file user want convert folder panda datafram data explor exampl run job type dataset input user seek guidanc explor dataset notebook",
        "Answer_original_content":"gcocci thank question current support access avro file document access datastor folder uri dataset http learn microsoft com azur machin learn migrat resourc datastor map data flow support avro sourc type http learn microsoft com azur data factori data flow sourc support sourc",
        "Answer_preprocessed_content":"thank question current support access avro file document access datastor dataset map data flow support avro sourc type",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to use Mapping Data Flow, which supports AVRO as a source type. Another solution is to access the Data Lake Gen2 datastore using the folder_uri dataset, as described in the provided documentation. However, it is currently not supported to access the AVRO files directly.",
        "Answer_gpt_summary":"solut solut mention discuss us map data flow support avro sourc type solut access data lake gen datastor folder uri dataset describ provid document current support access avro file directli"
    },
    {
        "Question_title":"How to disable logs in Kedro",
        "Question_body":"<p>I have been unsuccessful in disabling kedro logs.  I have tried adding <code>disable_existing_loggers: True<\/code> to the logging.yml file as well as <code>disable:True<\/code> to all of the existing logs and it still appears to be saving log files.  Any suggestions?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1573137628147,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":410.0,
        "Answer_body":"<p>If you want <code>kedro<\/code> to stop logging you can override the <code>_setup_logging<\/code> in <code>ProjectContext<\/code> in <code>src\/&lt;package-name&gt;\/run.py<\/code> as per the <a href=\"https:\/\/kedro.readthedocs.io\/en\/latest\/04_user_guide\/07_logging.html#configure-logging\" rel=\"nofollow noreferrer\">documentation<\/a>. For example:<\/p>\n\n<pre><code>class ProjectContext(KedroContext):\n    \"\"\"Users can override the remaining methods from the parent class here, or create new ones\n    (e.g. as required by plugins)\n\n    \"\"\"\n\n    project_name = \"&lt;PACKGE-NAME&gt;\"\n    project_version = \"0.15.4\"\n\n    def _get_pipelines(self) -&gt; Dict[str, Pipeline]:\n        return create_pipelines()\n\n    def _setup_logging(self) -&gt; None:\n        import logging\n        logging.disable()\n<\/code><\/pre>\n\n<p>If you want it to still log to the console, but not save to <code>logs\/info.log<\/code> then you can do <code>def _setup_logging(self) -&gt; None: pass<\/code>.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1573142581883,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58751122",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1573141908796,
        "Question_original_content":"disabl log unsuccess disabl log tri ad disabl exist logger true log yml file disabl true exist log appear save log file suggest",
        "Question_preprocessed_content":"disabl log unsuccess disabl log tri ad file exist log appear save log file suggest",
        "Question_gpt_summary_original":"The user is facing challenges in disabling logs in Kedro despite trying to add \"disable_existing_loggers: True\" to the logging.yml file and \"disable:True\" to all existing logs. The user is seeking suggestions to resolve the issue.",
        "Question_gpt_summary":"user face challeng disabl log despit try add disabl exist logger true log yml file disabl true exist log user seek suggest resolv issu",
        "Answer_original_content":"want stop log overrid setup log projectcontext src run document exampl class projectcontext context user overrid remain method parent class creat new on requir plugin project project version def pipelin self dict str pipelin return creat pipelin def setup log self import log log disabl want log consol save log info log def setup log self pass",
        "Answer_preprocessed_content":"want stop log overrid document exampl want log consol save",
        "Answer_gpt_summary_original":"Solution:\n- Override the `_setup_logging` method in `ProjectContext` in `src\/<package-name>\/run.py` and add `logging.disable()` to disable logging in Kedro. \n- If you want to still log to the console, but not save to `logs\/info.log`, then add `def _setup_logging(self) -> None: pass`.",
        "Answer_gpt_summary":"solut overrid setup log method projectcontext src run add log disabl disabl log want log consol save log info log add def setup log self pass"
    },
    {
        "Question_title":"CI Credentials Not Tied to User",
        "Question_body":"<p>Hi there.<\/p>\n<p>We currently use wandb artifacts for model versioning during experiments. We\u2019d also like to integrate this into our production pipeline so that we can automatically pull specific model versions during builds.<\/p>\n<p>I am wondering if it\u2019s possible to get credentials that are not tied to a specific wandb user so that they don\u2019t expire if the team member that implements this happens to leave our company.<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1667918249037,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":170.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/willjstone\">@willjstone<\/a> thank you for writing in! You can do this using a <code>service account<\/code>, the steps to add this account type to your team are explained in our documentation <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\/general#what-is-a-service-account-and-why-is-it-useful\">here<\/a>. Would this work for your use case?<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/ci-credentials-not-tied-to-user\/3388",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1667922870256,
        "Question_original_content":"credenti ti user current us artifact model version experi wed like integr product pipelin automat pull specif model version build wonder possibl credenti ti specif user dont expir team member implement happen leav compani thank",
        "Question_preprocessed_content":"credenti ti user current us artifact model version experi wed like integr product pipelin automat pull specif model version build wonder possibl credenti ti specif user dont expir team member implement happen leav compani thank",
        "Question_gpt_summary_original":"The user is facing a challenge of getting credentials that are not tied to a specific wandb user so that they don't expire if the team member that implements this happens to leave the company. They want to integrate wandb artifacts for model versioning into their production pipeline to automatically pull specific model versions during builds.",
        "Question_gpt_summary":"user face challeng get credenti ti specif user expir team member implement happen leav compani want integr artifact model version product pipelin automat pull specif model version build",
        "Answer_original_content":"willjston thank write servic account step add account type team explain document work us case",
        "Answer_preprocessed_content":"thank write step add account type team explain document work us case",
        "Answer_gpt_summary_original":"Solution: A possible solution mentioned in the discussion is to use a service account, which is not tied to a specific wandb user and can be added to the team. The steps to add this account type are explained in the wandb documentation.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us servic account ti specif user ad team step add account type explain document"
    },
    {
        "Question_title":"How to access file system in Sagemaker notebook instance from outside of that instance (ie via Python Sagemaker Estimator training call)",
        "Question_body":"Hi,\n\nI have large image dataset stored in a Sagemaker notebook instance, in the file system. I was hoping to learn how I could access this data from outside of that particular notebook instance. I have done quite a bit of researching but can't seem to find much - I am relatively new to this.\n\nI want to be able to access the data in that notebook in a fast manner as I will be using the data to train an AI model. Is there any recommended way to do this?\n\nI originally uploaded the data within that notebook instance to train a model within that instance in exactly the same file system. Note that it is a reasonably large dataset which I had to do some preprocessing on within Sagemaker.\n\nWhat is the best way to store data when using the Sagemaker estimators from training AI models?\n\nMany thanks\n\nTim",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1638914293851,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":690.0,
        "Answer_body":"Hi Tim, when you create a sagemaker training job using the estimator, the general best practice is to store your data on S3 and the training job will launch instances as requested by the training job configuration. As now we support fast file mode, which allows faster training job start compared to the file mode (which downloads the data from s3 to the training instance). But when you say you used sagemaker notebook instance to train the model, I assume you were not using SageMaker Training jobs but rather running the notebook (.ipynb) on the SageMaker notebook instance. Please note that as SageMaker is a fully managed service, the notebook instance (also training instances, hosting instances etc.) are launched in the service account, so you will not have directly access to those instance. The SageMaker notebook instance use EBS to store data and the EBS volume is mounted to the \/home\/ec2-user\/SageMaker. Please note that the EBS volume used by a SageMaker notebook instance can only be increased but not decrease. If you want to reduce the EBS volume, you need to create a new notebook instance with a smaller volume and move your data from the previous instance via s3. You will not be able to access that EBS volume from outside of the SageMaker notebook instance. The general best practice is to store large dataset on s3 and only use sample data on the SageMaker notebook instance (reduce the storage). Then use that small amount of sample data to test\/build your code. Then when you are ready to train on the whole dataset, you can launch a SageMaker training job and use the whole dataset stored on s3. Note that, running the training on the whole dataset on a SageMaker notebook instance will require you to use a big instance with enough computing power and also will not be able to perform distributed training with multiple instances. Comparatively, if you run the training job use SageMaker training instances, it gives you more flexibility of choosing the instance type and allow you to run on multiple instances for distributed training. Lastly, once the SageMaker training job is done, all the resources will be terminated which will save cost compared to continue using the big instance with a SageMaker notebook instance. Hope this has helped answer your question",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU3yXAL7d7Sl--kKO3TTZf1g\/how-to-access-file-system-in-sagemaker-notebook-instance-from-outside-of-that-instance-ie-via-python-sagemaker-estimator-training-call",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1638917636668,
        "Question_original_content":"access file notebook instanc outsid instanc python estim train larg imag dataset store notebook instanc file hope learn access data outsid particular notebook instanc bit research rel new want abl access data notebook fast manner data train model recommend wai origin upload data notebook instanc train model instanc exactli file note reason larg dataset preprocess best wai store data estim train model thank tim",
        "Question_preprocessed_content":"access file notebook instanc outsid instanc larg imag dataset store notebook instanc file hope learn access data outsid particular notebook instanc bit research rel new want abl access data notebook fast manner data train model recommend wai origin upload data notebook instanc train model instanc exactli file note reason larg dataset preprocess best wai store data estim train model thank tim",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing a large image dataset stored in a Sagemaker notebook instance from outside of that instance. They are looking for a recommended way to access the data in a fast manner as they will be using it to train an AI model. The user is also seeking advice on the best way to store data when using Sagemaker estimators for training AI models.",
        "Question_gpt_summary":"user face challeng access larg imag dataset store notebook instanc outsid instanc look recommend wai access data fast manner train model user seek advic best wai store data estim train model",
        "Answer_original_content":"tim creat train job estim gener best practic store data train job launch instanc request train job configur support fast file mode allow faster train job start compar file mode download data train instanc notebook instanc train model assum train job run notebook ipynb notebook instanc note fulli manag servic notebook instanc train instanc host instanc launch servic account directli access instanc notebook instanc us eb store data eb volum mount home user note eb volum notebook instanc increas decreas want reduc eb volum need creat new notebook instanc smaller volum data previou instanc abl access eb volum outsid notebook instanc gener best practic store larg dataset us sampl data notebook instanc reduc storag us small sampl data test build code readi train dataset launch train job us dataset store note run train dataset notebook instanc requir us big instanc comput power abl perform distribut train multipl instanc compar run train job us train instanc give flexibl choos instanc type allow run multipl instanc distribut train lastli train job resourc termin save cost compar continu big instanc notebook instanc hope help answer question",
        "Answer_preprocessed_content":"tim creat train job estim gener best practic store data train job launch instanc request train job configur support fast file mode allow faster train job start compar file mode notebook instanc train model assum train job run notebook notebook instanc note fulli manag servic notebook instanc launch servic account directli access instanc notebook instanc us eb store data eb volum mount note eb volum notebook instanc increas decreas want reduc eb volum need creat new notebook instanc smaller volum data previou instanc abl access eb volum outsid notebook instanc gener best practic store larg dataset us sampl data notebook instanc us small sampl data code readi train dataset launch train job us dataset store note run train dataset notebook instanc requir us big instanc comput power abl perform distribut train multipl instanc compar run train job us train instanc give flexibl choos instanc type allow run multipl instanc distribut train lastli train job resourc termin save cost compar continu big instanc notebook instanc hope help answer question",
        "Answer_gpt_summary_original":"Solution:\n- The recommended way to access large image datasets stored in a Sagemaker notebook instance from outside of that instance is to store the data on S3 and launch a SageMaker training job to access the data. \n- The best way to store data when using Sagemaker estimators for training AI models is to store the large dataset on S3 and only use sample data on the SageMaker notebook instance. \n- It is not possible to access the EBS volume used by a SageMaker notebook instance from outside of the instance. \n- Running the training on the whole dataset on a SageMaker notebook instance will require a big instance with enough computing power and will not be able to perform distributed training with multiple instances. \n- Running the",
        "Answer_gpt_summary":"solut recommend wai access larg imag dataset store notebook instanc outsid instanc store data launch train job access data best wai store data estim train model store larg dataset us sampl data notebook instanc possibl access eb volum notebook instanc outsid instanc run train dataset notebook instanc requir big instanc comput power abl perform distribut train multipl instanc run"
    },
    {
        "Question_title":"Assigning name to AWS SageMaker Training job",
        "Question_body":"<p>This may be a very basic question (I imagine it is)<\/p>\n<pre><code>estimator = PyTorch(entry_point='train.py',\n                   source_dir = 'code',\n                    role = role,\n                   framework_version = '1.5.0',\n                   py_version = 'py3',\n                   instance_count = 2,\n                   instance_type = 'ml.g4dn.2xlarge',\n                   hyperparameters={&quot;epochs&quot;: 2,\n                                     &quot;num_labels&quot;: 46,\n                                     &quot;backend&quot;: &quot;gloo&quot;,    \n                                    },\n                   profiler_config=profiler_config,\n                    debugger_hook_config=debugger_hook_config,\n                    rules=rules\n                   )\n<\/code><\/pre>\n<p>I declare my estimator as above, and put this into training using fit().<br \/>\nI have done several of these on my sagemaker, and there are several training jobs in the aws training job log.<br \/>\nBut they all appear in the form 'pytorch-training-2021 ....'. <br \/>\nIs there anyway I could declare the name of the training job like 'custom-model-xgboost-ver1' ?<br \/>\nI thought it would be possible as one of the parameter of estimator, but i couldn't find it.<\/p>\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1626249953723,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":445.0,
        "Answer_body":"<p>When you call <code>fit()<\/code> you can pass this parameter <code>job_name=yourJobName<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68374280",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1626250161860,
        "Question_original_content":"assign train job basic question imagin estim pytorch entri point train sourc dir code role role framework version version instanc count instanc type gdn xlarg hyperparamet epoch num label backend gloo profil config profil config debugg hook config debugg hook config rule rule declar estim train fit train job aw train job log appear form pytorch train declar train job like custom model xgboost ver thought possibl paramet estim couldn thank advanc",
        "Question_preprocessed_content":"assign train job basic question declar estim train fit train job aw train job log appear form declar train job like thought possibl paramet estim couldn thank advanc",
        "Question_gpt_summary_original":"The user is facing a challenge in assigning a custom name to their AWS SageMaker training job. They have declared their estimator and put it into training using fit(), but all the training jobs appear in the form 'pytorch-training-2021....' and they cannot find a parameter in the estimator to declare a custom name for the training job.",
        "Question_gpt_summary":"user face challeng assign custom train job declar estim train fit train job appear form pytorch train paramet estim declar custom train job",
        "Answer_original_content":"fit pass paramet job yourjobnam",
        "Answer_preprocessed_content":"pass paramet",
        "Answer_gpt_summary_original":"Solution: The user can pass the parameter 'job_name=yourJobName' when calling the 'fit()' function to assign a custom name to their AWS SageMaker training job.",
        "Answer_gpt_summary":"solut user pass paramet job yourjobnam call fit function assign custom train job"
    },
    {
        "Question_title":"Sagemaker how to pass tuning step's best hyperparameter into another estimator?",
        "Question_body":"<p>I'm running a <strong>Sagemaker pipeline<\/strong> with 2 steps, tuning and then training. The purpose is the get the best hyperparameter with tuning, and then use those hyperparameters in the next training step.\nI am aware that I can use <code>HyperparameterTuningJobAnalytics<\/code> to retrieve the tuning job specs after the tuning. However, I want to be able to use the hyperparameters like dependency and pass them directly to next trainingStep's estimator, see code below:\n<code>hyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,<\/code>\nBut this doesn't work with this error msg: <code>AttributeError: 'PropertiesMap' object has no attribute 'update'<\/code><\/p>\n<pre><code>tf_estimator_final = TensorFlow(entry_point='.\/train.py',\n                          role=role,\n                          sagemaker_session=sagemaker_session,\n                          code_location=code_location,\n                          instance_count=1,\n                          instance_type=&quot;ml.p3.16xlarge&quot;,\n                          framework_version='2.4',\n                          py_version=&quot;py37&quot;,\n                          base_job_name=base_job_name,\n                          output_path=model_path, # if output_path not specified,\nhyperparameters=step_tuning.properties.BestTrainingJob.TunedHyperParameters,\n                          model_dir=&quot;\/opt\/ml\/model&quot;,\n                          script_mode=True\n                          )\n\nstep_train = TrainingStep(\n    name=base_job_name,\n    estimator=tf_estimator_final,\n    inputs={\n        &quot;train&quot;: TrainingInput(\n            s3_data=train_s3\n        )\n    },\n    depends_on = [step_tuning]\n)\n\npipeline = Pipeline(\n    name=jobname,\n    steps=[\n        step_tuning,\n        step_train\n    ],\n    sagemaker_session=sagemaker_session\n)\n\njson.loads(pipeline.definition())\n<\/code><\/pre>\n<p>Any suggestions?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660154954560,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1660167642767,
        "Question_score":1.0,
        "Question_view_count":79.0,
        "Answer_body":"<p>This can't be done in SageMaker Pipelines at the moment.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73310895",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1661377323147,
        "Question_original_content":"pass tune step best hyperparamet estim run pipelin step tune train purpos best hyperparamet tune us hyperparamet train step awar us hyperparametertuningjobanalyt retriev tune job spec tune want abl us hyperparamet like depend pass directli trainingstep estim code hyperparamet step tune properti besttrainingjob tunedhyperparamet work error msg attributeerror propertiesmap object attribut updat estim final tensorflow entri point train role role session session code locat code locat instanc count instanc type xlarg framework version version base job base job output path model path output path specifi hyperparamet step tune properti besttrainingjob tunedhyperparamet model dir opt model script mode true step train trainingstep base job estim estim final input train traininginput data train depend step tune pipelin pipelin jobnam step step tune step train session session json load pipelin definit suggest",
        "Question_preprocessed_content":"pass tune step best hyperparamet estim run pipelin step tune train purpos best hyperparamet tune us hyperparamet train step awar us retriev tune job spec tune want abl us hyperparamet like depend pass directli trainingstep estim code work error msg suggest",
        "Question_gpt_summary_original":"The user is facing a challenge in passing the best hyperparameters obtained from a Sagemaker tuning step to the next training step. They have tried to retrieve the hyperparameters using HyperparameterTuningJobAnalytics but encountered an error when trying to pass them to the next estimator. The user is seeking suggestions on how to pass the hyperparameters directly to the next trainingStep's estimator.",
        "Question_gpt_summary":"user face challeng pass best hyperparamet obtain tune step train step tri retriev hyperparamet hyperparametertuningjobanalyt encount error try pass estim user seek suggest pass hyperparamet directli trainingstep estim",
        "Answer_original_content":"pipelin moment",
        "Answer_preprocessed_content":"pipelin moment",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Azure Automated ML(interface) choosing primary metrics to handle imbalanced data",
        "Question_body":"I figured out that there are some primary metrics I can choose when I run an automated ML experiment. Yet the number of primary metrics is fewer than the run metrics in the result page. I want to deal with imbalanced data(10:1 or 20:1) and\n\nlooked up the links below:\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-manage-ml-pitfalls#identify-models-with-imbalanced-data\nand\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\n\nIt seems F1 score is recommended to evaluate each model with imbalanced data.\n\nHere are my questions:\n\nIs there any way to set F1 score or multiple measures as a primary metric?\n\n\nIf there is no such way, should I do it manually?\n\n\nOf all the given primary metrics, which primary metric is the most appropriate(to build a Classification model with imbalanced data)?\n\n\n\n\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1593398061863,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"For imbalanced data, it is preferred to choose AUC Weighted. Also user should then choose a metric that is appropriate to work well for imbalance. E.g. F1, micro averaged AUC, balanced accuracy for model evaluation. For primary metric (metric used for model optimization) the user should preferably choose AUC Weighted instead of accuracy.\nCurrently from the ml.azure.com the following metrics are supported. To add F1 score metric forwarded to product team to check on this.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#primary-metric",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/40792\/azure-automated-mlinterface-choosing-primary-metri.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1593508812347,
        "Question_original_content":"azur autom interfac choos primari metric handl imbalanc data figur primari metric choos run autom experi number primari metric fewer run metric result page want deal imbalanc data look link http doc microsoft com azur machin learn concept manag pitfal identifi model imbalanc data http doc microsoft com azur machin learn configur auto train score recommend evalu model imbalanc data question wai set score multipl measur primari metric wai manual given primari metric primari metric appropri build classif model imbalanc data thank",
        "Question_preprocessed_content":"azur autom choos primari metric handl imbalanc data figur primari metric choos run autom experi number primari metric fewer run metric result page want deal imbalanc data look link score recommend evalu model imbalanc data question wai set score multipl measur primari metric wai manual given primari metric primari metric appropri thank",
        "Question_gpt_summary_original":"The user is facing challenges in handling imbalanced data while using Azure Automated ML interface. They have found that the number of primary metrics available is fewer than the run metrics in the result page. They have researched and found that F1 score is recommended to evaluate models with imbalanced data. The user is seeking advice on whether they can set F1 score or multiple measures as a primary metric and which primary metric is the most appropriate for building a classification model with imbalanced data.",
        "Question_gpt_summary":"user face challeng handl imbalanc data azur autom interfac number primari metric avail fewer run metric result page research score recommend evalu model imbalanc data user seek advic set score multipl measur primari metric primari metric appropri build classif model imbalanc data",
        "Answer_original_content":"imbalanc data prefer choos auc weight user choos metric appropri work imbal micro averag auc balanc accuraci model evalu primari metric metric model optim user prefer choos auc weight instead accuraci current azur com follow metric support add score metric forward product team check http doc microsoft com azur machin learn configur auto train primari metric",
        "Answer_preprocessed_content":"imbalanc data prefer choos auc weight user choos metric appropri work imbal micro averag auc balanc accuraci model evalu primari metric user prefer choos auc weight instead accuraci current follow metric support add score metric forward product team check",
        "Answer_gpt_summary_original":"The discussion suggests that for imbalanced data, it is recommended to choose AUC Weighted as the primary metric for model optimization. Additionally, the user should choose a metric appropriate for working well with imbalanced data, such as F1, micro averaged AUC, or balanced accuracy for model evaluation. However, currently, only a limited number of metrics are supported on ml.azure.com, and the user has forwarded a request to the product team to add F1 score as a metric.",
        "Answer_gpt_summary":"discuss suggest imbalanc data recommend choos auc weight primari metric model optim addition user choos metric appropri work imbalanc data micro averag auc balanc accuraci model evalu current limit number metric support azur com user forward request product team add score metric"
    },
    {
        "Question_title":"How to bypass ID column without being used in the training model but have it as output - Azure ML",
        "Question_body":"<p>The input data in the model includes column ControlNo.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/eqULH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqULH.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But I don't want this column being part of learning process so I'm using <code>Select Columns in Dataset<\/code> to exclude <code>ControlNo<\/code> column.<\/p>\n<p>But as a output I want those columns:<\/p>\n<pre><code>ControlNo, Score Label, Score Probability\n<\/code><\/pre>\n<p>So basically I need NOT to include column <code>ControlNo<\/code> into learning process,\nbut have it as output along with <code>Score Label<\/code> column.<\/p>\n<p>How can I do that?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/iPlpa.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1519761299607,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1592644375060,
        "Question_score":2.0,
        "Question_view_count":582.0,
        "Answer_body":"<p>Instead of removing the ControlNo column from the dataset, you can use the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/edit-metadata\" rel=\"nofollow noreferrer\">Edit Metadata<\/a> module to clear its \"Feature\" flag - just select the column and set <strong>Fields<\/strong> to <strong>Clear feature<\/strong>. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/EUy9A.png\" alt=\"Edit Metadata settings\"><\/a><\/p>\n\n<p>This will cause the Azure ML Studio algorithms to ignore it during training, and you'll be able to return it as part of your output. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49016896",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1533111465720,
        "Question_original_content":"bypass column train model output input data model includ column controlno want column learn process select column dataset exclud controlno column output want column controlno score label score probabl basic need includ column controlno learn process output score label column",
        "Question_preprocessed_content":"bypass column train model output input data model includ column controlno want column learn process exclud column output want column basic need includ column learn process output column",
        "Question_gpt_summary_original":"The user wants to exclude the \"ControlNo\" column from the learning process in Azure ML but still have it as an output along with the \"Score Label\" column. They are using \"Select Columns in Dataset\" to exclude the \"ControlNo\" column, but are unsure how to include it as an output.",
        "Question_gpt_summary":"user want exclud controlno column learn process output score label column select column dataset exclud controlno column unsur includ output",
        "Answer_original_content":"instead remov controlno column dataset us edit metadata modul clear featur flag select column set field clear featur caus studio algorithm ignor train abl return output",
        "Answer_preprocessed_content":"instead remov controlno column dataset us edit metadata modul clear featur flag select column set field clear featur caus studio algorithm ignor train abl return output",
        "Answer_gpt_summary_original":"Solution: The user can use the \"Edit Metadata\" module to clear the \"Feature\" flag of the \"ControlNo\" column, which will cause Azure ML Studio algorithms to ignore it during training. This way, the user can still include the \"ControlNo\" column as an output along with the \"Score Label\" column.",
        "Answer_gpt_summary":"solut user us edit metadata modul clear featur flag controlno column caus studio algorithm ignor train wai user includ controlno column output score label column"
    },
    {
        "Question_title":"Workflow for running an ensemble of experiments with different initial conditions",
        "Question_body":"<p>Any ideas on the right workflow to run sweeps\/groups with a whole bunch of different variations on initial conditions to see an ensemble of results?  I think that the  <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/advanced\/grouping\" class=\"inline-onebox\">Group Runs - Documentation<\/a>  seems a natural candidate for this but I am not sure the right approach or how it overlays with sweeps in this sort of usecase.<\/p>\n<p>To setup the scenario I have in mind: I have a script  I want to run hundred times on my local machine with pretty much all parameters fixed except the neural network initial conditions.  I can control that by doing things like incrementing a <code>--seed<\/code> argument  or just not establishing a default seed.  After running those experiments, it is nice to see pretty pictures of distribtions in wandb but I also want to be able to later collect the results\/assets as a group. and do things like plot a histogram of <code>val_loss<\/code> to put in a research paper.<\/p>\n<p>Is the way to do this with a combination of sweeps and run_groups?  Forr example, can I run a bunch of these in a sweep with after setting the <code>WANDB_RUN_GROUP<\/code> environment variable?  For example, maybe setup a sweep file like<\/p>\n<pre data-code-wrap=\"yaml\"><code class=\"lang-nohighlight\">program: train.py\nmethod: grid\nparameters:\n  seed:\n    min: 2\n    max: 102\n<\/code><\/pre>\n<p>Where <code>--seed<\/code> is used internally to set the seed for the experiment?  Any better approaches<\/p>\n<p>If that works, ,  then do I just need to set <code>WANDB_RUN_GROUP<\/code> environment variable on every machine that I will run an agent on and then it can be grouped?  Then I can pull down all of the assets for these with the <code>WAND_RUN_GROUP<\/code>?  I couldn\u2019t figure it out from the docs how to get all of the logged results (and the artifacts if there are any) for a group.<\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1662490156459,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":131.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/jlperla\">@jlperla<\/a> thank you for the detailed information, and great to hear that the grouping issue has been now resolved. Regarding your question using the API to filter runs, you could do that indeed with the following command:<br>\n<code>runs = api.runs(\"entity\/project\", filters={\"sweep\": \"sweep_id\"})<\/code><br>\nAlternatively you can use <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/tags#how-to-add-tags\">API to tag all your runs<\/a> based on <code>my_sweep_name<\/code> identifier and then query runs as follows:<br>\n<code>runs = api.runs(\"entity\/project\", filters={\"tags\": \"my_sweep_name\"})<\/code><br>\nIs my_sweep_name defined in your config? In that case you could do <code>filters={\"config.sweep_name\": \"my_sweep_name\"}<\/code>.<\/p>\n<p>Would any of these work for you? Please let me know if you have any further questions or issues with this!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/workflow-for-running-an-ensemble-of-experiments-with-different-initial-conditions\/3074",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1663850808295,
        "Question_original_content":"workflow run ensembl experi differ initi condit idea right workflow run sweep group bunch differ variat initi condit ensembl result think group run document natur candid sure right approach overlai sweep sort usecas setup scenario mind script want run time local machin pretti paramet fix neural network initi condit control thing like increment seed argument establish default seed run experi nice pretti pictur distribt want abl later collect result asset group thing like plot histogram val loss research paper wai combin sweep run group forr exampl run bunch sweep set run group environ variabl exampl mayb setup sweep file like program train method grid paramet seed min max seed intern set seed experi better approach work need set run group environ variabl machin run agent group pull asset wand run group figur doc log result artifact group",
        "Question_preprocessed_content":"workflow run ensembl experi differ initi condit idea right workflow run bunch differ variat initi condit ensembl result think group run document natur candid sure right approach overlai sweep sort usecas setup scenario mind script want run time local machin pretti paramet fix neural network initi condit control thing like increment argument establish default seed run experi nice pretti pictur distribt want abl later collect group thing like plot histogram research paper wai combin sweep forr exampl run bunch sweep set environ variabl exampl mayb setup sweep file like intern set seed experi better approach work need set environ variabl machin run agent group pull asset figur doc log result group",
        "Question_gpt_summary_original":"The user is seeking advice on the appropriate workflow to run multiple experiments with different initial conditions and collect the results as a group. They are considering using a combination of sweeps and run groups, but are unsure of the best approach and how to collect the results and assets for a group. The user wants to be able to plot a histogram of val_loss for their research paper.",
        "Question_gpt_summary":"user seek advic appropri workflow run multipl experi differ initi condit collect result group consid combin sweep run group unsur best approach collect result asset group user want abl plot histogram val loss research paper",
        "Answer_original_content":"jlperla thank detail inform great hear group issu resolv question api filter run follow command run api run entiti project filter sweep sweep altern us api tag run base sweep identifi queri run follow run api run entiti project filter tag sweep sweep defin config case filter config sweep sweep work let know question issu",
        "Answer_preprocessed_content":"thank detail inform great hear group issu resolv question api filter run follow command altern us api tag run base identifi queri run follow defin config case work let know question issu",
        "Answer_gpt_summary_original":"Solution: The discussion does not provide a solution to the challenge.",
        "Answer_gpt_summary":"solut discuss provid solut challeng"
    },
    {
        "Question_title":"Output model metrics to Cloudwatch",
        "Question_body":"<p>I am following the mnist-2 guide from the aws github documentation to implement my own training job <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_script_mode_training_and_serving<\/a>. I have wrote my code using a similar structure, but I would like to visualise the training and validation metrics from Cloudwatch while the job is running. Do I need to manually specify the metrics I am trying to observe? The AWS guide states &quot;<em>SageMaker automatically parses the logs for metrics that built-in algorithms emit and sends those metrics to CloudWatch.<\/em>&quot; I am only using Tensorflow's training and validation accuracy and loss metrics, which I am not sure if they are built-in, or if I need to call them manually.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1616769637850,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":475.0,
        "Answer_body":"<p>If you are not using a built-in algorithm, like in the example you linked, you have to define your metrics when you create the training job. You have to define regex expressions to grab from the logs the metric values, then cloudwatch will plot for you. The x axis will be the timestamp, you cannot change it.\nBasically just run your traning job and observe how the metrics are outputted, then you can build the appropriate regex. For example, since I am using coco metrics in tensorflow which periodically produce this:<\/p>\n<pre><code>INFO:tensorflow:Saving dict for global step 1109: DetectionBoxes_Precision\/mAP = 0.111895345, DetectionBoxes_Precision\/mAP (large) = 0.12102994, DetectionBoxes_Precision\/mAP (medium) = 0.050807837, DetectionBoxes_Precision\/mAP (small) = -1.0, DetectionBoxes_Precision\/mAP@.50IOU = 0.33130914, DetectionBoxes_Precision\/mAP@.75IOU = 0.03787096, DetectionBoxes_Recall\/AR@1 = 0.18493989, DetectionBoxes_Recall\/AR@10 = 0.36792925, DetectionBoxes_Recall\/AR@100 = 0.48543888, DetectionBoxes_Recall\/AR@100 (large) = 0.5131599, DetectionBoxes_Recall\/AR@100 (medium) = 0.21598063, DetectionBoxes_Recall\/AR@100 (small) = -1.0, Loss\/classification_loss = 0.8041124, Loss\/localization_loss = 0.35313264, Loss\/regularization_loss = 0.15211834, Loss\/total_loss = 1.30936, global_step = 1109, learning_rate = 0.28119853, loss = 1.30936\n<\/code><\/pre>\n<p>I use to grab the total loss for example:<\/p>\n<pre><code>INFO.*Loss\\\/total_loss = ([0-9\\.]+) \n<\/code><\/pre>\n<p>That's it, cloudwatch automatically plot the total_loss in time.<\/p>\n<p>You can define metrics either in the console or in the notebook, like this (just an example from my code):<\/p>\n<pre><code>metrics = [{'Name': 'Loss', 'Regex': 'loss: ([0-9\\.]+)'},\n           {'Name': 'Accuracy', 'Regex': 'acc: ([0-9\\.]+)'},\n           {'Name': 'Epoch', 'Regex': 'Epoch ([0-9\\.]+)'},\n           {'Name': 'Validation_Acc', 'Regex': 'val_acc: ([0-9\\.]+)'},\n           {'Name': 'Validation_Loss', 'Regex': 'val_loss: ([0-9\\.]+)'}]\n\ntf_estimator = TensorFlow(entry_point='training.py', \n                          role=get_execution_role(),\n                          train_instance_count=1, \n                          train_instance_type='ml.p2.xlarge',\n                          train_max_run=172800,\n                          output_path=s3_output_location,\n                          framework_version='1.12',\n                          py_version='py3',\n                          metric_definitions = metrics,\n                          hyperparameters = hyperparameters)\n<\/code><\/pre>\n<p>In order to test your regex, you can use a tool like <a href=\"https:\/\/regex101.com\/\" rel=\"nofollow noreferrer\">this<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66819026",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1616771138300,
        "Question_original_content":"output model metric cloudwatch follow mnist guid aw github document implement train job http github com aw amazon exampl tree master python sdk tensorflow script mode train serv wrote code similar structur like visualis train valid metric cloudwatch job run need manual specifi metric try observ aw guid state automat pars log metric built algorithm emit send metric cloudwatch tensorflow train valid accuraci loss metric sure built need manual",
        "Question_preprocessed_content":"output model metric cloudwatch follow guid aw github document implement train job wrote code similar structur like visualis train valid metric cloudwatch job run need manual specifi metric try observ aw guid state automat pars log metric algorithm emit send metric tensorflow train valid accuraci loss metric sure need manual",
        "Question_gpt_summary_original":"The user is following a guide to implement their own training job using AWS SageMaker and TensorFlow. They want to visualize the training and validation metrics from Cloudwatch while the job is running, but they are unsure if they need to manually specify the metrics they want to observe. They are only using TensorFlow's built-in training and validation accuracy and loss metrics, but they are not sure if they need to call them manually.",
        "Question_gpt_summary":"user follow guid implement train job tensorflow want visual train valid metric cloudwatch job run unsur need manual specifi metric want observ tensorflow built train valid accuraci loss metric sure need manual",
        "Answer_original_content":"built algorithm like exampl link defin metric creat train job defin regex express grab log metric valu cloudwatch plot axi timestamp chang basic run trane job observ metric output build appropri regex exampl coco metric tensorflow period produc info tensorflow save dict global step detectionbox precis map detectionbox precis map larg detectionbox precis map medium detectionbox precis map small detectionbox precis map iou detectionbox precis map iou detectionbox recal detectionbox recal detectionbox recal detectionbox recal larg detectionbox recal medium detectionbox recal small loss classif loss loss local loss loss regular loss loss total loss global step learn rate loss us grab total loss exampl info loss total loss cloudwatch automat plot total loss time defin metric consol notebook like exampl code metric loss regex loss accuraci regex acc epoch regex epoch valid acc regex val acc valid loss regex val loss estim tensorflow entri point train role execut role train instanc count train instanc type xlarg train max run output path output locat framework version version metric definit metric hyperparamet hyperparamet order test regex us tool like",
        "Answer_preprocessed_content":"algorithm like exampl link defin metric creat train job defin regex express grab log metric valu cloudwatch plot axi timestamp chang basic run trane job observ metric output build appropri regex exampl coco metric tensorflow period produc us grab total loss exampl cloudwatch automat plot time defin metric consol notebook like order test regex us tool like",
        "Answer_gpt_summary_original":"The solution provided is to define regex expressions to grab the metric values from the logs and then Cloudwatch will plot them automatically. The user can observe how the metrics are outputted during the training job and build the appropriate regex. The metrics can be defined either in the console or in the notebook. A tool like regex101 can be used to test the regex.",
        "Answer_gpt_summary":"solut provid defin regex express grab metric valu log cloudwatch plot automat user observ metric output train job build appropri regex metric defin consol notebook tool like regex test regex"
    },
    {
        "Question_title":"Can I make Amazon SageMaker deliver a recommendation based on historic data instead of a probability score?",
        "Question_body":"<p>We have a huge set of data in CSV format, containing a few numeric elements, like this:<\/p>\n\n<pre><code>Year,BinaryDigit,NumberToPredict,JustANumber, ...other stuff\n1954,1,762,16, ...other stuff\n1965,0,142,16, ...other stuff\n1977,1,172,16, ...other stuff\n<\/code><\/pre>\n\n<p>The thing here is that there is a strong correlation between the third column and the columns before that. So I have pre-processed the data and it's now available in a format I think is perfect:<\/p>\n\n<pre><code>1954,1,762\n1965,0,142\n1977,1,172\n<\/code><\/pre>\n\n<p>What I want is a predicition on the value in the third column, using the first two as input. So in the case above, I want the input 1965,0 to return 142. In real life this file is thousands of rows, but since there's a pattern, I'd like to retrieve the most possible value.<\/p>\n\n<p>So far I've setup a train job on the CSV file using the L<em>inear Learner<\/em> algorithm, with the following settings:<\/p>\n\n<pre><code>label_size = 1\nfeature_dim = 2\npredictor_type = regression\n<\/code><\/pre>\n\n<p>I've also created a model from it, and setup an endpoint. When I invoke it, I get a score in return.<\/p>\n\n<pre><code>    response = runtime.invoke_endpoint(EndpointName=ENDPOINT_NAME,\n                                   ContentType='text\/csv',\n                                   Body=payload)\n<\/code><\/pre>\n\n<p>My goal here is to get the third column prediction instead. How can I achieve that? I have read a lot of the documentation regarding this, but since I'm not very familiar with AWS, I might as well have used the wrong algorithms for what I am trying to do.<\/p>\n\n<p>(Please feel free to edit this question to better suit AWS terminology)<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1552553455710,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1552895653107,
        "Question_score":1.0,
        "Question_view_count":271.0,
        "Answer_body":"<p>For csv input, the label should be in the first column, as mentioned <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/cdf-training.html\" rel=\"nofollow noreferrer\">here<\/a>:  So you should preprocess your data to put the label (the column you want to predict) on the left.<\/p>\n\n<p>Next, you need to decide whether this is a regression problem or a classification problem. <\/p>\n\n<p>If you want to predict a number that's as close as possible to the true number, that's regression. For example, the truth might be 4, and the model might predict 4.15. If you need an integer prediction, you could round the model's output.<\/p>\n\n<p>If you want the prediction to be one of a few categories, then you have a classification problem. For example, we might encode 'North America' = 0, 'Europe' = 1, 'Africa' = 2, and so on. In this case, a fractional prediction wouldn't make sense. <\/p>\n\n<p>For regression, use <code>'predictor_type' = 'regressor'<\/code> and for classification with more than 2 classes, use <code>'predictor_type' = 'multiclass_classifier'<\/code> as documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ll_hyperparameters.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>The output of regression will contain only a <code>'score'<\/code> field, which is the model's prediction. The output of multiclass classification will contain a <code>'predicted_label'<\/code> field, which is the model's prediction, as well as a <code>'score'<\/code> field, which is a vector of probabilities representing the model's confidence. The index with the highest probability will be the one that's predicted as the <code>'predicted_label'<\/code>. The output formats are documented <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/LL-in-formats.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55158307",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1553880598532,
        "Question_original_content":"deliv recommend base histor data instead probabl score huge set data csv format contain numer element like year binarydigit numbertopredict justanumb stuff stuff stuff stuff thing strong correl column column pre process data avail format think perfect want predicit valu column input case want input return real life file thousand row pattern like retriev possibl valu far setup train job csv file linear learner algorithm follow set label size featur dim predictor type regress creat model setup endpoint invok score return respons runtim invok endpoint endpointnam endpoint contenttyp text csv bodi payload goal column predict instead achiev read lot document familiar aw wrong algorithm try feel free edit question better suit aw terminolog",
        "Question_preprocessed_content":"deliv recommend base histor data instead probabl score huge set data csv format contain numer element like thing strong correl column column data avail format think perfect want predicit valu column input case want input return real life file thousand row pattern like retriev possibl valu far setup train job csv file linear learner algorithm follow set creat model setup endpoint invok score return goal column predict instead achiev read lot document familiar aw wrong algorithm try feel free edit question better suit aw terminolog",
        "Question_gpt_summary_original":"The user has a large dataset in CSV format with a strong correlation between the third column and the columns before it. They have pre-processed the data and want to predict the value in the third column using the first two columns as input. The user has set up a train job using the Linear Learner algorithm with regression settings and created a model with an endpoint. However, when invoking the endpoint, they receive a score instead of the desired prediction for the third column. The user is seeking guidance on how to achieve their goal.",
        "Question_gpt_summary":"user larg dataset csv format strong correl column column pre process data want predict valu column column input user set train job linear learner algorithm regress set creat model endpoint invok endpoint receiv score instead desir predict column user seek guidanc achiev goal",
        "Answer_original_content":"csv input label column mention preprocess data label column want predict left need decid regress problem classif problem want predict number close possibl true number regress exampl truth model predict need integ predict round model output want predict categori classif problem exampl encod north america europ africa case fraction predict wouldn sens regress us predictor type regressor classif class us predictor type multiclass classifi document output regress contain score field model predict output multiclass classif contain predict label field model predict score field vector probabl repres model confid index highest probabl predict predict label output format document",
        "Answer_preprocessed_content":"csv input label column mention preprocess data label left need decid regress problem classif problem want predict number close possibl true number regress exampl truth model predict need integ predict round model output want predict categori classif problem exampl encod north america europ africa case fraction predict wouldn sens regress us classif class us document output regress contain field model predict output multiclass classif contain field model predict field vector probabl repres model confid index highest probabl predict output format document",
        "Answer_gpt_summary_original":"The user should preprocess their data to put the label (the column they want to predict) on the left. They need to decide whether this is a regression problem or a classification problem. For regression, use 'predictor_type' = 'regressor' and for classification with more than 2 classes, use 'predictor_type' = 'multiclass_classifier'. The output of regression will contain only a 'score' field, which is the model's prediction. The output of multiclass classification will contain a 'predicted_label' field, which is the model's prediction, as well as a 'score' field, which is a vector of probabilities representing the model's confidence.",
        "Answer_gpt_summary":"user preprocess data label column want predict left need decid regress problem classif problem regress us predictor type regressor classif class us predictor type multiclass classifi output regress contain score field model predict output multiclass classif contain predict label field model predict score field vector probabl repres model confid"
    },
    {
        "Question_title":"Filter GPU curves in the system panel",
        "Question_body":"<p>Hi,<br>\nMe and a colleague are sharing a remote server with 8 GPUs. We split them, 4 GPUs each. In the system panel at the WANDB page I currently see data of all 8 GPUs. Is it possible to filter some of those curves, so I\u2019ll only see the GPUs I\u2019m using?<\/p>\n<p>Many thanks<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1663566341242,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":174.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/yonatan-shimoni\">@yonatan-shimoni<\/a> thank you for writing in! Just to clarify here, is this for the System view that you can access from the left panel of an individual Run, or is it at the System panels section in your Project\u2019s Workspace? You can select which GPUs to visualise there by editing the Chart (pencil icon) as in the attached screenshot. Would this help?<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818.png\" data-download-href=\"\/uploads\/short-url\/uFNI8njAc6A8srPIjwcR86x4AZi.png?dl=1\" title=\"Screenshot 2022-09-20 at 16.50.40\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_476x500.png\" alt=\"Screenshot 2022-09-20 at 16.50.40\" data-base62-sha1=\"uFNI8njAc6A8srPIjwcR86x4AZi\" width=\"476\" height=\"500\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_476x500.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_714x750.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_952x1000.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/d\/d6fae44c542194daa3cd8f4afb453e59ecbdc818_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screenshot 2022-09-20 at 16.50.40<\/span><span class=\"informations\">1098\u00d71153 76.4 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/filter-gpu-curves-in-the-system-panel\/3152",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1663689312779,
        "Question_original_content":"filter gpu curv panel colleagu share remot server gpu split gpu panel page current data gpu possibl filter curv ill gpu thank",
        "Question_preprocessed_content":"filter gpu curv panel colleagu share remot server gpu split gpu panel page current data gpu possibl filter curv ill gpu thank",
        "Question_gpt_summary_original":"The user is sharing a remote server with a colleague and has access to 8 GPUs, split between them. They are encountering a challenge in the system panel at the WANDB page, where they are seeing data for all 8 GPUs instead of just the ones they are using. The user is seeking a solution to filter out the curves for the unused GPUs.",
        "Question_gpt_summary":"user share remot server colleagu access gpu split encount challeng panel page see data gpu instead on user seek solut filter curv unus gpu",
        "Answer_original_content":"yonatan shimoni thank write clarifi view access left panel individu run panel section project workspac select gpu visualis edit chart pencil icon attach screenshot help screenshot",
        "Answer_preprocessed_content":"thank write clarifi view access left panel individu run panel section project workspac select gpu visualis edit chart attach screenshot help screenshot",
        "Answer_gpt_summary_original":"Solution: The colleague suggested that the user can select which GPUs to visualize by editing the chart in the System panels section of the Project's Workspace. The colleague also provided a screenshot to help the user with the process.",
        "Answer_gpt_summary":"solut colleagu suggest user select gpu visual edit chart panel section project workspac colleagu provid screenshot help user process"
    },
    {
        "Question_title":"How to pass environment variables in sagemaker tuner job",
        "Question_body":"Sagemaker training jobs support setting environment variables on-the-fly in the training job:\n\n \"Environment\": { \n      \"string\" : \"string\" \n   },\n\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateTrainingJob.html\n\nI did not find an equivalent for the tuner jobs:\n\nhttps:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateHyperParameterTuningJob.html\n\nAccording to my testing, the SagemakerTuner in the python SDK simply ignores the environment variables set in the passed estimator.\n\nIs there any way to pass environment variables to the training jobs started by a tuner job programmatically, or is that currently unsupported?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1669725280762,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":9.0,
        "Answer_body":"Thanks for raising this. Yes, as you point out the Environment collection is not supported in the underlying CreateHyperparameterTuningJob API and therefore the SageMaker Python SDK can't make use of it when running a tuner.\n\nAs discussed on the SM Py SDK GitHub issue here, you might consider using hyperparameters instead to pass parameters through to the job?\n\nIf you specifically need environment variables for some other process\/library, you could also explore setting the variables from your Python script (perhaps to map from hyperparam to env var?).\n\nOr another option could be to customize your container image to bake in the variable via the ENV command? For example to customize an existing AWS Deep Learning Container (framework container), you could:\n\nUse sagemaker.image_uris.retrieve(...) to find the base image URI for your given framework, version, region, etc. You'll need to authenticate Docker to this registry as well as your own Amazon ECR account.\nCreate a Dockerfile that takes this base image URI as an arg and builds FROM it, something like this example\nAdd the required ENV commands to bake in the (static) environment variables you need\ndocker build your custom container (passing in the base image URI as a --build-arg), upload it to Amazon ECR, and use in your SageMaker training job.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU5aMFxhnLQeqMY39mlmYHjA\/how-to-pass-environment-variables-in-sagemaker-tuner-job",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1669736547972,
        "Question_original_content":"pass environ variabl tuner job train job support set environ variabl fly train job environ string string http doc aw amazon com latest apirefer api createtrainingjob html equival tuner job http doc aw amazon com latest apirefer api createhyperparametertuningjob html accord test tuner python sdk simpli ignor environ variabl set pass estim wai pass environ variabl train job start tuner job programmat current unsupport",
        "Question_preprocessed_content":"pass environ variabl tuner job train job support set environ variabl train job environ equival tuner job accord test tuner python sdk simpli ignor environ variabl set pass estim wai pass environ variabl train job start tuner job programmat current unsupport",
        "Question_gpt_summary_original":"The user is facing a challenge in passing environment variables to Sagemaker tuner jobs programmatically. While Sagemaker training jobs support setting environment variables on-the-fly, the user did not find an equivalent for the tuner jobs. The SagemakerTuner in the python SDK also ignores the environment variables set in the passed estimator. The user is seeking a solution to pass environment variables to the training jobs started by a tuner job programmatically.",
        "Question_gpt_summary":"user face challeng pass environ variabl tuner job programmat train job support set environ variabl fly user equival tuner job tuner python sdk ignor environ variabl set pass estim user seek solut pass environ variabl train job start tuner job programmat",
        "Answer_original_content":"thank rais ye point environ collect support underli createhyperparametertuningjob api python sdk us run tuner discuss sdk github issu consid hyperparamet instead pass paramet job specif need environ variabl process librari explor set variabl python script map hyperparam env var option custom contain imag bake variabl env command exampl custom exist aw deep learn contain framework contain us imag uri retriev base imag uri given framework version region need authent docker registri amazon ecr account creat dockerfil take base imag uri arg build like exampl add requir env command bake static environ variabl need docker build custom contain pass base imag uri build arg upload amazon ecr us train job",
        "Answer_preprocessed_content":"thank rais ye point environ collect support underli createhyperparametertuningjob api python sdk us run tuner discuss sdk github issu consid hyperparamet instead pass paramet job specif need environ variabl explor set variabl python script option custom contain imag bake variabl env command exampl custom exist aw deep learn contain us base imag uri given framework version region need authent docker registri amazon ecr account creat dockerfil take base imag uri arg build like exampl add requir env command bake environ variabl need docker build custom contain upload amazon ecr us train job",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use hyperparameters instead of environment variables to pass parameters through to the job.\n2. Set the environment variables from the Python script.\n3. Customize the container image to bake in the variable via the ENV command.\n\nNo personal opinions or biases are included in the response.",
        "Answer_gpt_summary":"possibl solut mention discuss us hyperparamet instead environ variabl pass paramet job set environ variabl python script custom contain imag bake variabl env command person opinion bias includ respons"
    },
    {
        "Question_title":"How to Curl an Amazon Sagemaker Endpoint",
        "Question_body":"<p>What is a is the curl command to make a POST request to sage-maker and receive a ML inference?<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":1,
        "Question_created_time":1513373969900,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":11.0,
        "Question_view_count":6889.0,
        "Answer_body":"<p>Rather than using curl, it's recommended that you use the SageMaker Runtime client to send data and get back inferences from a SageMaker Endpoint:<\/p>\n\n<p><a href=\"http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html\" rel=\"nofollow noreferrer\">http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47840209",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1514326523572,
        "Question_original_content":"curl endpoint curl command post request sage maker receiv infer",
        "Question_preprocessed_content":"curl endpoint curl command post request receiv infer",
        "Question_gpt_summary_original":"The user is facing a challenge in understanding how to use the curl command to make a POST request to Amazon Sagemaker and receive a machine learning inference.",
        "Question_gpt_summary":"user face challeng understand us curl command post request receiv machin learn infer",
        "Answer_original_content":"curl recommend us runtim client send data infer endpoint http doc aw amazon com latest api runtim invokeendpoint html",
        "Answer_preprocessed_content":"curl recommend us runtim client send data infer endpoint",
        "Answer_gpt_summary_original":"Solution: It is recommended to use the SageMaker Runtime client instead of curl to send data and receive inferences from a SageMaker Endpoint. The documentation for using the SageMaker Runtime client can be found at http:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_runtime_InvokeEndpoint.html. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut recommend us runtim client instead curl send data receiv infer endpoint document runtim client http doc aw amazon com latest api runtim invokeendpoint html solut mention discuss"
    },
    {
        "Question_title":"SageMaker PIPE Mode vs FSx ?",
        "Question_body":"Hi, SageMaker supports training data streaming via PIPE mode, and also reading from FSx distributed file system. Those options seem to provide same value: low latency, high throughput.\n\nWhat are the reasons for using one or the other?\nDo we have any benchmark of PIPE vs FSx for SageMaker, in terms of costs and speed?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1579692074000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":124.0,
        "Answer_body":"I can think of the following scenarios\n\nPipemode cons\n\n** UPDATED**\n\nData Shuffling - In pipe mode you are working with streaming data and hence you cannot perform data shuffle operations unless you are prepared to shuffle within batches (as in wait to read a batch of records and shuffle within the batch in Pipe mode). Of if your data is distributed across multiples files, then you could use Sagemaker data shuffle to perform file level shuffle\n\nData readers - There are default data readers for pipemode that come with Tensorflow for formats like csv, tfrecord etc. But if you have custom data formats or using a different deep leaning framework, yYou would have to use custom data readers to deal with the raw bytes and understand the logical end of record. You could also use ml-io to see if any of the built-in pipe mode readers work for your usecase\n\nPIPE mode streams the data for each epoch from S3 and hence will be slower than FSX when you run a few epochs\n\nFSX:\n\nFSX works by lazy loading the s3 file and hence it has a start up delay but gets faster during repeated training.\n\nThere is no dependency on the framework and your existing code will work as is..\n\nThe only con of using FSX is the additional storage costs, but I would almost prefer FSX to pipe mode in most cases.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUyS6bjxG4R4qtnrXzA3uSeg\/sage-maker-pipe-mode-vs-f-sx",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1579732148000,
        "Question_original_content":"pipe mode fsx support train data stream pipe mode read fsx distribut file option provid valu low latenc high throughput reason benchmark pipe fsx term cost speed",
        "Question_preprocessed_content":"pipe mode fsx support train data stream pipe mode read fsx distribut file option provid valu low latenc high throughput reason benchmark pipe fsx term cost speed",
        "Question_gpt_summary_original":"The user is seeking information on the differences between SageMaker PIPE mode and FSx for training data streaming, as both options offer low latency and high throughput. The user is looking for benchmarks comparing the costs and speed of each option.",
        "Question_gpt_summary":"user seek inform differ pipe mode fsx train data stream option offer low latenc high throughput user look benchmark compar cost speed option",
        "Answer_original_content":"think follow scenario pipemod con updat data shuffl pipe mode work stream data perform data shuffl oper prepar shuffl batch wait read batch record shuffl batch pipe mode data distribut multipl file us data shuffl perform file level shuffl data reader default data reader pipemod come tensorflow format like csv tfrecord custom data format differ deep lean framework yyou us custom data reader deal raw byte understand logic end record us built pipe mode reader work usecas pipe mode stream data epoch slower fsx run epoch fsx fsx work lazi load file start delai get faster repeat train depend framework exist code work fsx addit storag cost prefer fsx pipe mode case",
        "Answer_preprocessed_content":"think follow scenario pipemod con updat data shuffl pipe mode work stream data perform data shuffl oper prepar shuffl batch data distribut multipl file us data shuffl perform file level shuffl data reader default data reader pipemod come tensorflow format like csv tfrecord custom data format differ deep lean framework yyou us custom data reader deal raw byte understand logic end record us pipe mode reader work usecas pipe mode stream data epoch slower fsx run epoch fsx fsx work lazi load file start delai get faster repeat train depend framework exist code work fsx addit storag cost prefer fsx pipe mode case",
        "Answer_gpt_summary_original":"Summary:\n\n- SageMaker PIPE mode: \n    - Cons: \n        - Cannot perform data shuffle operations unless shuffling within batches or using Sagemaker data shuffle for file level shuffle.\n        - Custom data readers required for custom data formats or different deep learning frameworks.\n        - Slower than FSX when running a few epochs.\n    - Pros: \n        - Default data readers for formats like csv, tfrecord, etc.\n- FSx: \n    - Cons: \n        - Additional storage costs.\n        - Start-up delay, but gets faster during repeated training.\n    - Pros: \n        - No dependency on the framework, existing code will work as is.",
        "Answer_gpt_summary":"summari pipe mode con perform data shuffl oper shuffl batch data shuffl file level shuffl custom data reader requir custom data format differ deep learn framework slower fsx run epoch pro default data reader format like csv tfrecord fsx con addit storag cost start delai get faster repeat train pro depend framework exist code work"
    },
    {
        "Question_title":"AWS Sagemaker inference endpoint not utilizing all vCPUs",
        "Question_body":"<p>I have deployed a custom model on sagemaker inference endpoint (single instance) and while I was load testing, I have observed that CPU utilization metric is maxing out at 100% but according to <a href=\"https:\/\/aws.amazon.com\/premiumsupport\/knowledge-center\/sagemaker-cpu-gpu-utilization-100\/\" rel=\"nofollow noreferrer\">this post<\/a> it should max out at #vCPU*100 %. I have confirmed that the inference endpoint is not using all cores in clowdwatch logs.<\/p>\n<p>So if one prediction call requires one second to be processed to give response, the deployed model is only able to handle one API call per second which could have been increased to 8 calls per second if all vCPUs would have been used.<\/p>\n<p>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/p>\n<p>Or could we use multiprocessing python package inside <code>inference.py<\/code> file while deploying such that each call comes to the default core and from there all calculations\/prediction is done in any other core whichever is empty at that instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1624366085347,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1628443120180,
        "Question_score":3.0,
        "Question_view_count":661.0,
        "Answer_body":"<p>UPDATE<\/p>\n<ul>\n<li><p>Set three environment variables<\/p>\n<ol>\n<li>ENABLE_MULTI_MODEL as &quot;true&quot; (make sure it is string and not bool) and set <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L74\" rel=\"nofollow noreferrer\">SAGEMAKER_HANDLER<\/a> as custom model handler python module path if custom service else dont define it. Also make sure model name <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L94\" rel=\"nofollow noreferrer\">model.mar<\/a>, before compressing it as tar ball and storing in s3<\/li>\n<li>TS_DEFAULT_WORKERS_PER_MODEL as number of vcpus<\/li>\n<li>First environment variable makes sure torch serve env_vars are enabled and second one uses first setting and loads requested number of workers<\/li>\n<li>Setting can be done by passing env dictionary argument to <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">PyTorch function<\/a>. Below is explanation as to why it works<\/li>\n<\/ol>\n<\/li>\n<li><p>From the looks of it, sagemaker deployment for pytorch model as given in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-an-estimator\" rel=\"nofollow noreferrer\">Sagemaker SDK guide<\/a>, uses <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu\" rel=\"nofollow noreferrer\">this dockerfile<\/a>. In this docker, entrypoint is <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> as in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/1.8\/py3\/Dockerfile.cpu#L124\" rel=\"nofollow noreferrer\">Dockerfile line#124<\/a>.<\/p>\n<\/li>\n<li><p>This <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">torchserve-entrypoint.py<\/a> calls <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/pytorch\/inference\/docker\/build_artifacts\/torchserve-entrypoint.py\" rel=\"nofollow noreferrer\">serving.main()<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py\" rel=\"nofollow noreferrer\">serving.py<\/a>. Which ends up calling <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/serving.py#L34\" rel=\"nofollow noreferrer\">torchserve.start_torchserve(handler_service=HANDLER_SERVICE)<\/a> from <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py\" rel=\"nofollow noreferrer\">torchserve.py<\/a>.<\/p>\n<\/li>\n<li><p><a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L34\" rel=\"nofollow noreferrer\">At line 34 in torchserve.py<\/a> it defines &quot;\/etc\/default-ts.properties&quot; as DEFAULT_TS_CONFIG_FILE. This file is located <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties\" rel=\"nofollow noreferrer\">here<\/a>. In this file <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/etc\/default-ts.properties#L2\" rel=\"nofollow noreferrer\">enable_envvars_config=true<\/a> is set. It will use this file setting IFF Environment variable &quot;ENABLE_MULTI_MODEL&quot; is set to &quot;false&quot; as refered <a href=\"https:\/\/github.com\/aws\/sagemaker-pytorch-inference-toolkit\/blob\/master\/src\/sagemaker_pytorch_serving_container\/torchserve.py#L167\" rel=\"nofollow noreferrer\">here<\/a>. If it is set to &quot;true&quot; then it will use \/etc\/mme-ts.properties<\/p>\n<\/li>\n<\/ul>\n<hr \/>\n<p>As for the question <code>Are there any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency?<\/code>\nThere are various settings you can use\nFor models you can set <code>default_workers_per_model<\/code> in config.properties <code>TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all)<\/code> in environment variables. Environment variables take top priority.<\/p>\n<p>Other than that for each model, you can set the number of workers by using management API, but sadly it is not possible to curl to management API in sagemaker. SO TS_DEFAULT_WORKERS_PER_MODEL is the best bet.\nSetting this should make sure all cores are used.<\/p>\n<p>But if you are using docker file then in entrypoint you can setup scripts which wait for model loading and curl to it to set number of workers<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code># load the model\ncurl -X POST localhost:8081\/models?url=model_1.mar&amp;batch_size=8&amp;max_batch_delay=50\n# after loading the model it is possible to set min_worker, etc\ncurl -v -X PUT http:\/\/localhost:8081\/models\/model_1?min_worker=1\n<\/code><\/pre>\n<p>About the other issue that logs confirm that not all cores are used, I face the same issue and believe that is a problem in the logging system. Please look at this issue <a href=\"https:\/\/github.com\/pytorch\/serve\/issues\/782\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pytorch\/serve\/issues\/782<\/a>. The community itself agrees that if threads are not set, then by default then it prints 0, even if by default it uses 2*num_cores.<\/p>\n<p><strong>For an exhaustive set of all configs possible<\/strong><\/p>\n<pre class=\"lang-py prettyprint-override\"><code># Reference: https:\/\/github.com\/pytorch\/serve\/blob\/master\/docs\/configuration.md\n# Variables that can be configured through config.properties and Environment Variables\n# NOTE: Variables which can be configured through environment variables **SHOULD** have a\n# &quot;TS_&quot; prefix\n# debug\ninference_address=http:\/\/0.0.0.0:8080\nmanagement_address=http:\/\/0.0.0.0:8081\nmetrics_address=http:\/\/0.0.0.0:8082\nmodel_store=\/opt\/ml\/model\nload_models=model_1.mar\n# blacklist_env_vars\n# default_workers_per_model\n# default_response_timeout\n# unregister_model_timeout\n# number_of_netty_threads\n# netty_client_threads\n# job_queue_size\n# number_of_gpu\n# async_logging\n# cors_allowed_origin\n# cors_allowed_methods\n# cors_allowed_headers\n# decode_input_request\n# keystore\n# keystore_pass\n# keystore_type\n# certificate_file\n# private_key_file\n# max_request_size\n# max_response_size\n# default_service_handler\n# service_envelope\n# model_server_home\n# snapshot_store\n# prefer_direct_buffer\n# allowed_urls\n# install_py_dep_per_model\n# metrics_format\n# enable_metrics_api\n# initial_worker_port\n\n# Configuration which are not documented or enabled through environment variables\n\n# When below variable is set true, then the variables set in environment have higher precedence.\n# For example, the value of an environment variable overrides both command line arguments and a property in the configuration file. The value of a command line argument overrides a value in the configuration file.\n# When set to false, environment variables are not used at all\n# use_native_io=\n# io_ratio=\n# metric_time_interval=\nenable_envvars_config=true\n# model_snapshot=\n# version=\n<\/code><\/pre>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1624470584452,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68083831",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1624416005592,
        "Question_original_content":"infer endpoint util vcpu deploi custom model infer endpoint singl instanc load test observ cpu util metric max accord post max vcpu confirm infer endpoint core clowdwatch log predict requir second process respons deploi model abl handl api second increas call second vcpu set deploy us vcpu increas concurr us multiprocess python packag insid infer file deploi come default core calcul predict core whichev instanc",
        "Question_preprocessed_content":"infer endpoint util vcpu deploi custom model infer endpoint load test observ cpu util metric max accord post max vcpu confirm infer endpoint core clowdwatch log predict requir second process respons deploi model abl handl api second increas call second vcpu set deploy us vcpu increas concurr us multiprocess python packag insid file deploi come default core core whichev instanc",
        "Question_gpt_summary_original":"The user has encountered a challenge with their AWS Sagemaker inference endpoint, as it is not utilizing all vCPUs. Despite load testing, the CPU utilization metric is maxing out at 100%, which is not in line with AWS's guidelines. As a result, the deployed model is only able to handle one API call per second, instead of eight. The user is seeking advice on whether there are any settings in AWS Sagemaker deployment to use all vCPUs to increase concurrency or if they could use the multiprocessing python package inside the inference.py file to distribute the workload across multiple cores.",
        "Question_gpt_summary":"user encount challeng infer endpoint util vcpu despit load test cpu util metric max line aw guidelin result deploi model abl handl api second instead user seek advic set deploy us vcpu increas concurr us multiprocess python packag insid infer file distribut workload multipl core",
        "Answer_original_content":"updat set environ variabl enabl multi model true sure string bool set handler custom model handler python modul path custom servic dont defin sure model model mar compress tar ball store default worker model number vcpu environ variabl make sure torch serv env var enabl second us set load request number worker set pass env dictionari argument pytorch function explan work look deploy pytorch model given sdk guid us dockerfil docker entrypoint torchserv entrypoint dockerfil line torchserv entrypoint call serv main serv end call torchserv start torchserv handler servic handler servic torchserv line torchserv defin default properti default config file file locat file enabl envvar config true set us file set iff environ variabl enabl multi model set fals refer set true us mme properti question set deploy us vcpu increas concurr set us model set default worker model config properti default worker model nproc environ variabl environ variabl prioriti model set number worker manag api sadli possibl curl manag api default worker model best bet set sure core docker file entrypoint setup script wait model load curl set number worker load model curl post localhost model url model mar batch size max batch delai load model possibl set min worker curl http localhost model model min worker issu log confirm core face issu believ problem log look issu http github com pytorch serv issu commun agre thread set default print default us num core exhaust set config possibl refer http github com pytorch serv blob master doc configur variabl configur config properti environ variabl note variabl configur environ variabl prefix debug infer address http manag address http metric address http model store opt model load model model mar blacklist env var default worker model default respons timeout unregist model timeout number netti thread netti client thread job queue size number gpu async log cor allow origin cor allow method cor allow header decod input request keystor keystor pass keystor type certif file privat kei file max request size max respons size default servic handler servic envelop model server home snapshot store prefer direct buffer allow url instal dep model metric format enabl metric api initi worker port configur document enabl environ variabl variabl set true variabl set environ higher preced exampl valu environ variabl overrid command line argument properti configur file valu command line argument overrid valu configur file set fals environ variabl us nativ ratio metric time interv enabl envvar config true model snapshot version",
        "Answer_preprocessed_content":"updat set environ variabl true set custom model handler python modul path custom servic dont defin sure model compress tar ball store number vcpu environ variabl make sure torch serv enabl second us set load request number worker set pass env dictionari argument pytorch function explan work look deploy pytorch model given sdk guid us dockerfil docker entrypoint dockerfil line call end call line defin file locat file set us file set iff environ variabl set fals refer set true us question set us model set environ variabl environ variabl prioriti model set number worker manag api sadli possibl curl manag api best bet set sure core docker file entrypoint setup script wait model load curl set number worker issu log confirm core face issu believ problem log look issu commun agre thread set default print default us exhaust set config possibl",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Set three environment variables: ENABLE_MULTI_MODEL as \"true\", TS_DEFAULT_WORKERS_PER_MODEL as the number of vCPUs, and SAGEMAKER_HANDLER as a custom model handler python module path if custom service else don't define it. This setting can be done by passing env dictionary argument to PyTorch function. \n\n2. For models, set default_workers_per_model in config.properties TS_DEFAULT_WORKERS_PER_MODEL=$(nproc --all) in environment variables. Other than that, for each model, set the number of workers by using management API, but it is not possible to curl to management API in Sagemaker. \n\n3. If using a docker file, in entrypoint",
        "Answer_gpt_summary":"possibl solut mention discuss set environ variabl enabl multi model true default worker model number vcpu handler custom model handler python modul path custom servic defin set pass env dictionari argument pytorch function model set default worker model config properti default worker model nproc environ variabl model set number worker manag api possibl curl manag api docker file entrypoint"
    },
    {
        "Question_title":"AWS send image to Sagemaker from Lambda: how to set content handling?",
        "Question_body":"<p>similar question to\n<a href=\"https:\/\/stackoverflow.com\/a\/66683538\/6896705\">AWS Lambda send image file to Amazon Sagemaker<\/a><\/p>\n<p>I try to make simple-mnist work (the model was built by referring to <a href=\"https:\/\/sagemaker-immersionday.workshop.aws\/en\/lab3\/option1.html\" rel=\"nofollow noreferrer\">aws tutorial<\/a>)<\/p>\n<p>Then I am using API gateway (REST API w\/ proxy integration) to post image data to lambda, and would like to send it to sagemaker endpoint and make an inference.<\/p>\n<p>In lambda function, I wrote the code(.py) like this.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>runtime = boto3.Session().client('sagemaker-runtime')\n\nendpoint_name = 'tensorflow-training-YYYY-mm-dd-...'\nres = runtime.invoke_endpoint(EndpointName=endpoint_name,\n                              Body=Image,\n                              ContentType='image\/jpeg',\n                              Accept='image\/jpeg')\n<\/code><\/pre>\n<p>However, when I send image to lambda via API gateway, this error occurs.<\/p>\n<blockquote>\n<p>[ERROR] ModelError: An error occurred (ModelError) when calling the\nInvokeEndpoint operation: Received client error (415) from model with\nmessage &quot; {\n&quot;error&quot;: &quot;Unsupported Media Type: image\/jpeg&quot; }<\/p>\n<\/blockquote>\n<p>I think I need to do something referring to <a href=\"https:\/\/docs.aws.amazon.com\/apigateway\/latest\/developerguide\/api-gateway-payload-encodings.html\" rel=\"nofollow noreferrer\">Working with binary media types for REST APIs\n<\/a><\/p>\n<p>But since I am very new, I have no idea about the appropriate thing to do, on which page (maybe API Gateway page?) or how...<\/p>\n<p>I need some clues to solve this problem. Thank you in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1626929018517,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1626941375923,
        "Question_score":1.0,
        "Question_view_count":390.0,
        "Answer_body":"<p>Looking <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/adapt-inference-container.html\" rel=\"nofollow noreferrer\">here<\/a> you can see that only some specific content types are supported by default, and images are not in this list. I think you have to either implement your <code>input_fn<\/code> function or adapt your data to one of the supported content types.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68479297",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1626967022752,
        "Question_original_content":"aw send imag lambda set content handl similar question aw lambda send imag file try simpl mnist work model built refer aw tutori api gatewai rest api proxi integr post imag data lambda like send endpoint infer lambda function wrote code like runtim boto session client runtim endpoint tensorflow train re runtim invok endpoint endpointnam endpoint bodi imag contenttyp imag jpeg accept imag jpeg send imag lambda api gatewai error occur error modelerror error occur modelerror call invokeendpoint oper receiv client error model messag error unsupport media type imag jpeg think need refer work binari media type rest api new idea appropri thing page mayb api gatewai page need clue solv problem thank advanc",
        "Question_preprocessed_content":"aw send imag lambda set content handl similar question aw lambda send imag file try work api gatewai post imag data lambda like send endpoint infer lambda function wrote like send imag lambda api gatewai error occur error modelerror error occur call invokeendpoint oper receiv client error model messag think need refer work binari media type rest api new idea appropri thing page need clue solv problem thank advanc",
        "Question_gpt_summary_original":"The user is facing a challenge in sending an image from Lambda to Sagemaker endpoint for inference. The user is using API Gateway to post image data to Lambda, but when the image is sent to Lambda, an error occurs stating that the media type is unsupported. The user needs guidance on how to handle binary media types for REST APIs to solve the problem.",
        "Question_gpt_summary":"user face challeng send imag lambda endpoint infer user api gatewai post imag data lambda imag sent lambda error occur state media type unsupport user need guidanc handl binari media type rest api solv problem",
        "Answer_original_content":"look specif content type support default imag list think implement input function adapt data support content type",
        "Answer_preprocessed_content":"look specif content type support default imag list think implement function adapt data support content type",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are to either implement the `input_fn` function or adapt the data to one of the supported content types to handle binary media types for REST APIs.",
        "Answer_gpt_summary":"possibl solut mention discuss implement input function adapt data support content type handl binari media type rest api"
    },
    {
        "Question_title":"How do I stop a Google Cloud's AutoML (now VertexAI) batch prediction job using the web GUI?",
        "Question_body":"<p>I started a batch prediction job in AutoML (now VertexAI) for a small csv in one of my buckets, using a classification model, then I noticed the csv had an error but was unable to find a way to cancel the job using the web GUI, it just says &quot;running&quot; but I see no &quot;stop&quot; or &quot;cancel&quot; button.<\/p>\n<p>Fortunately, it was done after 20 minutes, but I need to know how to stop a job since I will require predictions for way bigger files and can't risk having to wait until the job ends by itself. It was kind of desperating being able to watch the log throwing error after error and not being able to stop the job. I tried to delete the job but it said it can't be deleted while its running.<\/p>\n<p>I found a related question, but it was not answered, the job just finished itself after a couple of days. I can't risk that.\n<a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a><\/p>\n<p>I will greatly appreciate any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1625608160277,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1625612012432,
        "Question_score":0.0,
        "Question_view_count":619.0,
        "Answer_body":"<p>Unfortunately the cancel\/stop feature is not yet available in the Vertex AI UI. As per <a href=\"https:\/\/stackoverflow.com\/questions\/68077606\/how-do-i-force-batch-prediction-to-stop\">How do I force &quot;batch prediction&quot; to stop?<\/a>, the OP sent a feedback. You can ask if there was a public issue tracker created for this so you can monitor the progress of the feature request there.<\/p>\n<p>But there is a workaround for this, just send a request <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/reference\/rest\/v1\/projects.locations.batchPredictionJobs\/cancel\" rel=\"nofollow noreferrer\">projects.locations.batchPredictionJobs.cancel<\/a> via REST.<\/p>\n<p>To do this you can send a request via curl. In this example the model and endpoint are located in <code>us-central1<\/code> thus the location defined in the request.<\/p>\n<p>Just supply your <code>project-id<\/code> and the <code>batch-prediction-id<\/code> on the request. To get the <code>batch-prediction-id<\/code> you can get it via UI:<\/p>\n<p>Get <code>batch-prediction-id<\/code> via UI:<\/p>\n<ul>\n<li>Open &quot;Batch Predictions&quot; tab in the Vertex AI UI<\/li>\n<li>Click on the job you want to cancel<\/li>\n<li>Job information will be displayed and the 1st entry will contain the Job ID<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lKJdT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>To cancel the job send a cancel request via curl. If requests is successful, the response body is empty.<\/p>\n<pre><code>curl -X POST -H &quot;Content-Type: application\/json&quot; \\\n-H &quot;Authorization: Bearer &quot;$(gcloud auth application-default print-access-token) \\\nhttps:\/\/us-central1-aiplatform.googleapis.com\/v1\/projects\/your-project-id\/locations\/us-central1\/batchPredictionJobs\/batch-prediction-job-id:cancel\n<\/code><\/pre>\n<p>Check in Vertex AI UI if the job was canceled.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/atSqt.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/atSqt.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68277691",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1625628028808,
        "Question_original_content":"stop googl cloud automl vertexai batch predict job web gui start batch predict job automl vertexai small csv bucket classif model notic csv error unabl wai cancel job web gui sai run stop cancel button fortun minut need know stop job requir predict wai bigger file risk have wait job end kind desper abl watch log throw error error abl stop job tri delet job said delet run relat question answer job finish coupl dai risk forc batch predict stop greatli appreci help",
        "Question_preprocessed_content":"stop googl cloud automl batch predict job web gui start batch predict job automl small csv bucket classif model notic csv error unabl wai cancel job web gui sai run stop cancel button fortun minut need know stop job requir predict wai bigger file risk have wait job end kind desper abl watch log throw error error abl stop job tri delet job said delet run relat question answer job finish coupl dai risk forc batch predict stop greatli appreci help",
        "Question_gpt_summary_original":"The user encountered a challenge in stopping a batch prediction job in Google Cloud's AutoML (now VertexAI) using the web GUI. The user was unable to find a way to cancel the job using the web GUI and was forced to wait until the job ended by itself. The user needs to know how to stop a job since they will require predictions for larger files and cannot risk waiting for the job to end by itself. The user tried to delete the job but was unable to do so while it was running.",
        "Question_gpt_summary":"user encount challeng stop batch predict job googl cloud automl vertexai web gui user unabl wai cancel job web gui forc wait job end user need know stop job requir predict larger file risk wait job end user tri delet job unabl run",
        "Answer_original_content":"unfortun cancel stop featur avail forc batch predict stop sent feedback ask public issu tracker creat monitor progress featur request workaround send request project locat batchpredictionjob cancel rest send request curl exampl model endpoint locat central locat defin request suppli project batch predict request batch predict batch predict open batch predict tab click job want cancel job inform displai entri contain job cancel job send cancel request curl request success respons bodi curl post content type applic json author bearer gcloud auth applic default print access token http central aiplatform googleapi com project project locat central batchpredictionjob batch predict job cancel check job cancel",
        "Answer_preprocessed_content":"unfortun featur avail forc batch predict stop sent feedback ask public issu tracker creat monitor progress featur request workaround send request rest send request curl exampl model endpoint locat locat defin request suppli request open batch predict tab click job want cancel job inform displai entri contain job cancel job send cancel request curl request success respons bodi check job cancel",
        "Answer_gpt_summary_original":"Solution: The cancel\/stop feature is not yet available in the Vertex AI UI. However, there is a workaround for this, which is to send a request via REST using the projects.locations.batchPredictionJobs.cancel API. To do this, the user can send a request via curl, supplying their project-id and the batch-prediction-id on the request. The batch-prediction-id can be obtained via the UI by opening the \"Batch Predictions\" tab, clicking on the job to be canceled, and obtaining the Job ID from the job information displayed. If the request is successful, the response body is empty.",
        "Answer_gpt_summary":"solut cancel stop featur avail workaround send request rest project locat batchpredictionjob cancel api user send request curl suppli project batch predict request batch predict obtain open batch predict tab click job cancel obtain job job inform displai request success respons bodi"
    },
    {
        "Question_title":"Python boto3 load model tar file from s3 and unpack it",
        "Question_body":"<p>I am using Sagemaker and have a bunch of model.tar.gz files that I need to unpack and load in sklearn. I've been testing using list_objects with delimiter to get to the tar.gz files:<\/p>\n\n<pre><code>response = s3.list_objects(\nBucket = bucket,\nPrefix = 'aleks-weekly\/models\/',\nDelimiter = '.csv'\n)\n\n\nfor i in response['Contents']:\n    print(i['Key'])\n<\/code><\/pre>\n\n<p>And then I plan to extract with<\/p>\n\n<pre><code>import tarfile\ntf = tarfile.open(model.read())\ntf.extractall()\n<\/code><\/pre>\n\n<p>But how do I get to the actual tar.gz file from s3 instead of a some boto3 object? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1565806893303,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":4787.0,
        "Answer_body":"<p>You can download objects to files using <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/s3.html#S3.Client.download_file\" rel=\"nofollow noreferrer\"><code>s3.download_file()<\/code><\/a>. This will make your code look like:<\/p>\n\n<pre><code>s3 = boto3.client('s3')\nbucket = 'my-bukkit'\nprefix = 'aleks-weekly\/models\/'\n\n# List objects matching your criteria\nresponse = s3.list_objects(\n    Bucket = bucket,\n    Prefix = prefix,\n    Delimiter = '.csv'\n)\n\n# Iterate over each file found and download it\nfor i in response['Contents']:\n    key = i['Key']\n    dest = os.path.join('\/tmp',key)\n    print(\"Downloading file\",key,\"from bucket\",bucket)\n    s3.download_file(\n        Bucket = bucket,\n        Key = key,\n        Filename = dest\n    )\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57500105",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1566337639600,
        "Question_original_content":"python boto load model tar file unpack bunch model tar file need unpack load sklearn test list object delimit tar file respons list object bucket bucket prefix alek weekli model delimit csv respons content print kei plan extract import tarfil tarfil open model read extractal actual tar file instead boto object",
        "Question_preprocessed_content":"python boto load model tar file unpack bunch file need unpack load sklearn test delimit file plan extract actual file instead boto object",
        "Question_gpt_summary_original":"The user is facing a challenge in unpacking and loading model.tar.gz files in sklearn using Sagemaker. They have been able to use list_objects with delimiter to get to the tar.gz files, but they are unsure of how to extract the actual tar.gz file from s3 instead of a boto3 object.",
        "Question_gpt_summary":"user face challeng unpack load model tar file sklearn abl us list object delimit tar file unsur extract actual tar file instead boto object",
        "Answer_original_content":"download object file download file code look like boto client bucket bukkit prefix alek weekli model list object match criteria respons list object bucket bucket prefix prefix delimit csv iter file download respons content kei kei dest path join tmp kei print download file kei bucket bucket download file bucket bucket kei kei filenam dest",
        "Answer_preprocessed_content":"download object file code look like",
        "Answer_gpt_summary_original":"Solution: The user can download objects to files using s3.download_file(). They can iterate over each file found and download it using the code provided in the discussion.",
        "Answer_gpt_summary":"solut user download object file download file iter file download code provid discuss"
    },
    {
        "Question_title":"ClientError: Annotation value 10 found in labels. This is greater than number of classes., exit code: 2",
        "Question_body":"<p>My total number of classes is 10<\/p>\n<p>1-&quot;Button_damage&quot;\n2-&quot;Cracks&quot;\n3-&quot;Edge_damage&quot;\n4-&quot;Frame_damage&quot;\n5-&quot;Hinge_damage&quot;\n6-&quot;Screen_damage&quot;\n7-&quot;Good_Button&quot;\n8-&quot;Good_Hinge&quot;\n9-&quot;Good_screen&quot;\n10-&quot;Good_frame&quot;<\/p>\n<p>When I give num_classes=10 in hyperparameter it throws a error.<\/p>\n<p>ClientError: Annotation value 10 found in labels. This is greater than number of classes., exit code: 2<\/p>\n<p>I changed num_classes=9 and tried, it shows same error.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1663588593177,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":7.0,
        "Answer_body":"<p>It was worked when I changed the num_classes=11. Because it adds one label &quot;0&quot; by default.<\/p>\n<p>0-&quot;BACKGROUND&quot;\n1-&quot;Button_damage&quot;\n2-&quot;Cracks&quot;\n3-&quot;Edge_damage&quot;\n4-&quot;Frame_damage&quot;\n5-&quot;Hinge_damage&quot;\n6-&quot;Screen_damage&quot;\n7-&quot;Good_Button&quot;\n8-&quot;Good_Hinge&quot;\n9-&quot;Good_screen&quot;\n10-&quot;Good_frame&quot;<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73773006",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1663588593176,
        "Question_original_content":"clienterror annot valu label greater number class exit code total number class button damag crack edg damag frame damag hing damag screen damag good button good hing good screen good frame num class hyperparamet throw error clienterror annot valu label greater number class exit code chang num class tri show error",
        "Question_preprocessed_content":"clienterror annot valu label greater number exit code total number class crack hyperparamet throw error clienterror annot valu label greater number exit code chang tri show error",
        "Question_gpt_summary_original":"The user encountered a challenge with their hyperparameters when trying to set the number of classes to 10. They received an error message stating that the annotation value 10 found in labels is greater than the number of classes. Even when they changed the number of classes to 9, they still received the same error message.",
        "Question_gpt_summary":"user encount challeng hyperparamet try set number class receiv error messag state annot valu label greater number class chang number class receiv error messag",
        "Answer_original_content":"work chang num class add label default background button damag crack edg damag frame damag hing damag screen damag good button good hing good screen good frame",
        "Answer_preprocessed_content":"work chang add label default background crack",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to change the `num_classes` to 11 instead of 10, as it adds a default label \"0\" which can resolve the error message.",
        "Answer_gpt_summary":"solut possibl solut mention discuss chang num class instead add default label resolv error messag"
    },
    {
        "Question_title":"How to pass region to the SKLearnProcessor - botocore.exceptions.NoRegionError: You must specify a region",
        "Question_body":"<p>I'm using the following code to do a sklearn transformation job in sagemaker:<\/p>\n<pre><code>region = boto3.session.Session().region_name\nrole = sagemaker.get_execution_role()\nsklearn_processor = SKLearnProcessor(\n    framework_version=&quot;1.0-1&quot;, role=role,\n    instance_type=&quot;ml.m5.xlarge&quot;, instance_count=1,\n    # sagemaker_session = Session()\n)\nout_path = os.path.join(bucket, prefix, f'test_transform\/data.csv')\nsklearn_processor.run(\n    code=&quot;preprocess.py&quot;,\n    inputs = [\n        ProcessingInput(source = 'my_package\/', destination = '\/opt\/ml\/processing\/input\/code\/my_package\/')\n    ],\n    outputs=[\n        ProcessingOutput(output_name=&quot;test_transform_data&quot;, \n                         source = '\/opt\/ml\/processing\/output\/test_transform',\n                         destination = out_path),\n    ],\n    arguments=[&quot;--time-slot-minutes&quot;, &quot;30min&quot;]\n)\n<\/code><\/pre>\n<p>Within the above code, it's running preprocess.py, and (within) preprocess.py loads the data from snowflake database using the credentials saved in aws secretsmanager:<\/p>\n<pre><code>region = boto3.Session().region_name\nsecrets_client = boto3.client(service_name='secretsmanager', region_name=region)\n<\/code><\/pre>\n<p>So here's where error happen: first line above returns region as None, so the the second line of code raises <code>botocore.exceptions.NoRegionError: You must specify a region<\/code><\/p>\n<p>In this case, how can I pass the region to SKLearnProcessor or is there any other way to make the code working within the processing job instance?<\/p>\n<p>FYI:\nthe source of input <code>'my_package\/'<\/code> is in the structure below to install packages and include py dependencies used in <code>preprocess.py<\/code><\/p>\n<pre><code>\u251c\u2500\u2500 my_package\n\u2502   \u251c\u2500\u2500 file1.py\n\u2502   \u251c\u2500\u2500 file2.py\n\u2502   \u2514\u2500\u2500 requirements.txt\n\u2514\u2500\u2500 preprocess.py\n<\/code><\/pre>\n<p>Thanks<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1663640822130,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":20.0,
        "Answer_body":"<p>set following in the code preprocess.py solved the issue:<\/p>\n<pre><code>os.environ['AWS_DEFAULT_REGION'] = 'us-west-2' \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73781018",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1663678383900,
        "Question_original_content":"pass region sklearnprocessor botocor except noregionerror specifi region follow code sklearn transform job region boto session session region role execut role sklearn processor sklearnprocessor framework version role role instanc type xlarg instanc count session session path path join bucket prefix test transform data csv sklearn processor run code preprocess input processinginput sourc packag destin opt process input code packag output processingoutput output test transform data sourc opt process output test transform destin path argument time slot minut min code run preprocess preprocess load data snowflak databas credenti save aw secretsmanag region boto session region secret client boto client servic secretsmanag region region error happen line return region second line code rais botocor except noregionerror specifi region case pass region sklearnprocessor wai code work process job instanc fyi sourc input packag structur instal packag includ depend preprocess packag file file requir txt preprocess thank",
        "Question_preprocessed_content":"pass region sklearnprocessor specifi region follow code sklearn transform job code run load data snowflak databas credenti save aw secretsmanag error happen line return region second line code rais case pass region sklearnprocessor wai code work process job instanc fyi sourc input structur instal packag includ depend thank",
        "Question_gpt_summary_original":"The user is encountering an error while running a sklearn transformation job in Sagemaker. The error is caused by the region being returned as None, which is required to load data from a Snowflake database using AWS Secrets Manager. The user is seeking a solution to pass the region to SKLearnProcessor or find an alternative way to make the code work within the processing job instance.",
        "Question_gpt_summary":"user encount error run sklearn transform job error caus region return requir load data snowflak databas aw secret manag user seek solut pass region sklearnprocessor altern wai code work process job instanc",
        "Answer_original_content":"set follow code preprocess solv issu environ aw default region west",
        "Answer_preprocessed_content":"set follow code solv issu",
        "Answer_gpt_summary_original":"Solution: Setting the AWS_DEFAULT_REGION environment variable to the desired region (in this case, 'us-west-2') in the code (preprocess.py) can solve the issue.",
        "Answer_gpt_summary":"solut set aw default region environ variabl desir region case west code preprocess solv issu"
    },
    {
        "Question_title":"SageMaker Script Mode Serving",
        "Question_body":"<p>I've trained a tensorflow.keras model using SageMaker Script Mode like this:<\/p>\n\n<pre><code>import os\n\nimport sagemaker\nfrom sagemaker.tensorflow import TensorFlow\n\nestimator = TensorFlow(entry_point='train.py',\n                       source_dir='src',\n                       train_instance_type=train_instance_type,\n                       train_instance_count=1,\n                       hyperparameters=hyperparameters,\n                       role=sagemaker.get_execution_role(),\n                       framework_version='1.12.0',\n                       py_version='py3', \n                       script_mode=True)\n<\/code><\/pre>\n\n<p>However, how do I specify what the serving code is when I call <code>estimator.deploy()<\/code>? And what is it by default? Also is there any way to modify the nginx.conf using Script Mode?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1548625393003,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":1352.0,
        "Answer_body":"<p>With script mode the default serving method is the TensorFlow Serving-based one:\n<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/estimator.py#L393\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/estimator.py#L393<\/a>\nCustom script is not allowed with the TFS based container. You can use serving_input_receiver_fn to specify how the input data is processed as described here: <a href=\"https:\/\/www.tensorflow.org\/guide\/saved_model\" rel=\"nofollow noreferrer\">https:\/\/www.tensorflow.org\/guide\/saved_model<\/a><\/p>\n\n<p>As for modifying the ngnix.conf, there are no supported ways of doing that. Depends on what you want to change in the config file you can hack the sagemaker-python-sdk to pass in different values for these environment variables: <a href=\"https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/3fd736aac4b0d97df5edaea48d37c49a1688ad6e\/container\/sagemaker\/serve.py#L29\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-tensorflow-serving-container\/blob\/3fd736aac4b0d97df5edaea48d37c49a1688ad6e\/container\/sagemaker\/serve.py#L29<\/a><\/p>\n\n<p>Here is where you can override the environment variables: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L130\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/serving.py#L130<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54393158",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1548883525400,
        "Question_original_content":"script mode serv train tensorflow kera model script mode like import import tensorflow import tensorflow estim tensorflow entri point train sourc dir src train instanc type train instanc type train instanc count hyperparamet hyperparamet role execut role framework version version script mode true specifi serv code estim deploi default wai modifi nginx conf script mode",
        "Question_preprocessed_content":"script mode serv train model script mode like specifi serv code default wai modifi script mode",
        "Question_gpt_summary_original":"The user is facing challenges in specifying the serving code when calling estimator.deploy() after training a tensorflow.keras model using SageMaker Script Mode. The user is also unsure about the default serving code and whether it is possible to modify the nginx.conf using Script Mode.",
        "Question_gpt_summary":"user face challeng specifi serv code call estim deploi train tensorflow kera model script mode user unsur default serv code possibl modifi nginx conf script mode",
        "Answer_original_content":"script mode default serv method tensorflow serv base http github com aw python sdk blob master src tensorflow estim custom script allow tf base contain us serv input receiv specifi input data process describ http tensorflow org guid save model modifi ngnix conf support wai depend want chang config file hack python sdk pass differ valu environ variabl http github com aw tensorflow serv contain blob fdaacbddfedaeadcaad contain serv overrid environ variabl http github com aw python sdk blob master src tensorflow serv",
        "Answer_preprocessed_content":"script mode default serv method tensorflow custom script allow tf base contain us specifi input data process describ modifi support wai depend want chang config file hack pass differ valu environ variabl overrid environ variabl",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- The default serving method with Script Mode is TensorFlow Serving-based. The user can use serving_input_receiver_fn to specify how the input data is processed.\n- There are no supported ways of modifying the nginx.conf file. However, the user can hack the sagemaker-python-sdk to pass in different values for the environment variables. The environment variables can be overridden in the sagemaker-python-sdk.",
        "Answer_gpt_summary":"possibl solut mention discuss default serv method script mode tensorflow serv base user us serv input receiv specifi input data process support wai modifi nginx conf file user hack python sdk pass differ valu environ variabl environ variabl overridden python sdk"
    },
    {
        "Question_title":"Need to do a Face detection on rtsp stream using Azure services",
        "Question_body":"I would like to use some of the Azure service for my project. Idea is to stream encoded video stream (h264), expecting Azure to decode and perform face detection on it in real time. What are the best services to use it.\nOne option I explore is to use computer vision\/face Api with media services, however computed vision API requires image in jpeg\/bif format which requires reencoding of decoding stream, or I need to do screen grab, which I dont want to do as it will increase latency.\nIf there is service which does ML operations on raw frame decoded by media services would help here.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1631162999540,
        "Question_favorite_count":14.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@SaurabhSinghSengar-7709 The service that offers the capability to enable identification of faces in real time with streaming video is Azure Video Analyzer. It should support HLS and other popular formats, you can take a look at the limitations and quotas from the documentation.\n\nWith computer vision or face API you need to capture the frame or a screen shot and pass the same to the API to get the results.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/545213\/need-to-do-a-face-detection-on-rtsp-stream-using-a.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1631183295583,
        "Question_original_content":"need face detect rtsp stream azur servic like us azur servic project idea stream encod video stream expect azur decod perform face detect real time best servic us option explor us vision face api media servic comput vision api requir imag jpeg bif format requir reencod decod stream need screen grab dont want increas latenc servic oper raw frame decod media servic help",
        "Question_preprocessed_content":"need face detect rtsp stream azur servic like us azur servic project idea stream encod video stream expect azur decod perform face detect real time best servic us option explor us api media servic comput vision api requir imag format requir reencod decod stream need screen grab dont want increas latenc servic oper raw frame decod media servic help",
        "Question_gpt_summary_original":"The user is facing challenges in performing real-time face detection on an encoded video stream using Azure services. They are exploring options such as using computer vision\/face API with media services, but are encountering issues with the required image format and latency. They are seeking a service that can perform ML operations on raw frames decoded by media services.",
        "Question_gpt_summary":"user face challeng perform real time face detect encod video stream azur servic explor option vision face api media servic encount issu requir imag format latenc seek servic perform oper raw frame decod media servic",
        "Answer_original_content":"saurabhsinghsengar servic offer capabl enabl identif face real time stream video azur video analyz support hl popular format look limit quota document vision face api need captur frame screen shot pass api result forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"servic offer capabl enabl identif face real time stream video azur video analyz support hl popular format look limit quota document vision face api need captur frame screen shot pass api result forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_gpt_summary_original":"Solution: The user can use Azure Video Analyzer service to enable real-time face detection on a video stream. This service supports HLS and other popular formats. Using computer vision or face API requires capturing the frame or a screenshot and passing it to the API to get the results, which may not be suitable for real-time face detection. No other solutions are mentioned in the discussion.",
        "Answer_gpt_summary":"solut user us azur video analyz servic enabl real time face detect video stream servic support hl popular format vision face api requir captur frame screenshot pass api result suitabl real time face detect solut mention discuss"
    },
    {
        "Question_title":"Workspace Authentication: More than one token matches the criteria",
        "Question_body":"<p>I constantly run into problems when working on Azure Compute Instances and trying to connect from the Jupyter Lab to the workspace.<\/p>\n<p>With InteractiveLoginAuthentication I get the following message:<\/p>\n<pre><code>AuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<p>With a Service Principal this one (SP is owner in the ML Workspace):<\/p>\n<pre><code>WorkspaceException: WorkspaceException:\n    Message: No workspaces found with name=xxx in all the subscriptions that you have access to.\n    InnerException None\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;message&quot;: &quot;No workspaces found with name=xxx in all the subscriptions that you have access to.&quot;\n    }\n}\n<\/code><\/pre>\n<p>I had another workspace in a different subscription where I could resolve it by giving the tennant as an extra input to the InteractiveLoginAuthentication. This time, no chance.<\/p>\n<p>The funny thing is, though, that I can login to the workspace via InteractiveLoginAuthentication when doing it from my local computer.<\/p>\n<p>I supsected that some old tokens are cached somewhere so I tried to use the &quot;Private browsing&quot; function of my browser. Furthermore, I deleted <code>\/home\/azureuser\/.azure\/accessTokens.json<\/code> but no effect.<\/p>\n<p>Maybe some of you had this problem before and have an idea?<\/p>\n<p>For reference some sites I checked:<\/p>\n<ul>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-setup-authentication<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/manage-azureml-service\/authentication-in-azureml\/authentication-in-azureml.ipynb<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/4618\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/4618<\/a><\/li>\n<li><a href=\"https:\/\/github.com\/Azure\/azure-cli\/issues\/6147\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-cli\/issues\/6147<\/a><\/li>\n<\/ul>\n<h1>Update<\/h1>\n<p>When I run this code:<\/p>\n<pre><code>from azureml.core.authentication import InteractiveLoginAuthentication\ninteractive_auth = InteractiveLoginAuthentication(tenant_id='xxx')\n\nws = Workspace.get(name='xxx',\n                   subscription_id='xxx',\n                   resource_group='xxx',\n                   auth=interactive_auth)\n<\/code><\/pre>\n<p>I get the following trace:<\/p>\n<pre><code>---------------------------------------------------------------------------\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    288                     module_logger.debug(&quot;{} acquired lock in {} s.&quot;.format(type(self).__name__, duration))\n--&gt; 289                 return test_function(self, *args, **kwargs)\n    290             except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n\nDuring handling of the above exception, another exception occurred:\n\nAdalError                                 Traceback (most recent call last)\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1820         auth, _, _ = profile_object.get_login_credentials(resource)\n-&gt; 1821         access_token = auth._token_retriever()[1]\n   1822         if (_get_exp_time(access_token) - time.time()) &lt; _TOKEN_REFRESH_THRESHOLD_SEC:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in _retrieve_token()\n    525                     return self._creds_cache.retrieve_token_for_user(username_or_sp_id,\n--&gt; 526                                                                      account[_TENANT_ID], resource)\n    527                 use_cert_sn_issuer = account[_USER_ENTITY].get(_SERVICE_PRINCIPAL_CERT_SN_ISSUER_AUTH)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_vendor\/azure_cli_core\/_profile.py in retrieve_token_for_user(self, username, tenant, resource)\n    889         context = self._auth_ctx_factory(self._cloud_type, tenant, cache=self.adal_token_cache)\n--&gt; 890         token_entry = context.acquire_token(resource, username, _CLIENT_ID)\n    891         if not token_entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in acquire_token(self, resource, user_id, client_id)\n    144 \n--&gt; 145         return self._acquire_token(token_func)\n    146 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in _acquire_token(self, token_func, correlation_id)\n    127         self.authority.validate(self._call_context)\n--&gt; 128         return token_func(self)\n    129 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/authentication_context.py in token_func(self)\n    142             token_request = TokenRequest(self._call_context, self, client_id, resource)\n--&gt; 143             return token_request.get_token_from_cache_with_refresh(user_id)\n    144 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in get_token_from_cache_with_refresh(self, user_id)\n    346         self._user_id = user_id\n--&gt; 347         return self._find_token_from_cache()\n    348 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/token_request.py in _find_token_from_cache(self)\n    126         cache_query = self._create_cache_query()\n--&gt; 127         return self._cache_driver.find(cache_query)\n    128 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in find(self, query)\n    195                         {&quot;query&quot;: log.scrub_pii(query)})\n--&gt; 196         entry, is_resource_tenant_specific = self._load_single_entry_from_cache(query)\n    197         if entry:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/adal\/cache_driver.py in _load_single_entry_from_cache(self, query)\n    123             else:\n--&gt; 124                 raise AdalError('More than one token matches the criteria. The result is ambiguous.')\n    125 \n\nAdalError: More than one token matches the criteria. The result is ambiguous.\n\nDuring handling of the above exception, another exception occurred:\n\nAuthenticationException                   Traceback (most recent call last)\n&lt;ipython-input-2-fd1276999d15&gt; in &lt;module&gt;\n      5                    subscription_id='00c983e5-d766-480b-be75-abf95d1a46c3',\n      6                    resource_group='BusinessIntelligence',\n----&gt; 7                    auth=interactive_auth)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in get(name, auth, subscription_id, resource_group)\n    547 \n    548         result_dict = Workspace.list(\n--&gt; 549             subscription_id, auth=auth, resource_group=resource_group)\n    550         result_dict = {k.lower(): v for k, v in result_dict.items()}\n    551         name = name.lower()\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in list(subscription_id, auth, resource_group)\n    637         elif subscription_id and resource_group:\n    638             workspaces_list = Workspace._list_legacy(\n--&gt; 639                 auth, subscription_id=subscription_id, resource_group_name=resource_group)\n    640 \n    641             Workspace._process_autorest_workspace_list(\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1373                 return None\n   1374             else:\n-&gt; 1375                 raise e\n   1376 \n   1377     @staticmethod\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/workspace.py in _list_legacy(auth, subscription_id, resource_group_name, ignore_error)\n   1367             # azureml._base_sdk_common.workspace.models.workspace.Workspace\n   1368             workspace_autorest_list = _commands.list_workspace(\n-&gt; 1369                 auth, subscription_id=subscription_id, resource_group_name=resource_group_name)\n   1370             return workspace_autorest_list\n   1371         except Exception as e:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/_project\/_commands.py in list_workspace(auth, subscription_id, resource_group_name)\n    386         if resource_group_name:\n    387             list_object = WorkspacesOperations.list_by_resource_group(\n--&gt; 388                 auth._get_service_client(AzureMachineLearningWorkspaces, subscription_id).workspaces,\n    389                 resource_group_name)\n    390             workspace_list = list_object.value\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_service_client(self, client_class, subscription_id, subscription_bound, base_url)\n    155         # in the multi-tenant case, which causes confusion.\n    156         if subscription_id:\n--&gt; 157             all_subscription_list, tenant_id = self._get_all_subscription_ids()\n    158             self._check_if_subscription_exists(subscription_id, all_subscription_list, tenant_id)\n    159 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_all_subscription_ids(self)\n    497         :rtype: list, str\n    498         &quot;&quot;&quot;\n--&gt; 499         arm_token = self._get_arm_token()\n    500         return self._get_all_subscription_ids_internal(arm_token)\n    501 \n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in wrapper(self, *args, **kwargs)\n    293                     InteractiveLoginAuthentication(force=True, tenant_id=self._tenant_id)\n    294                     # Try one more time\n--&gt; 295                     return test_function(self, *args, **kwargs)\n    296                 else:\n    297                     raise e\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token(self)\n    473             return self._ambient_auth._get_arm_token()\n    474         else:\n--&gt; 475             return self._get_arm_token_using_interactive_auth()\n    476 \n    477     @_login_on_failure_decorator(_interactive_auth_lock)\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_using_interactive_auth(self, force_reload, resource)\n    588         profile_object = Profile(async_persist=False, cloud_type=cloud_type)\n    589         arm_token = _get_arm_token_with_refresh(profile_object, cloud_type, ACCOUNT, CONFIG, SESSION,\n--&gt; 590                                                 get_config_dir(), force_reload=force_reload, resource=resource)\n    591         # If a user has specified a tenant id then we need to check if this token is for that tenant.\n    592         if self._tenant_id and fetch_tenantid_from_aad_token(arm_token) != self._tenant_id:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in connection_aborted_wrapper(*args, **kwargs)\n    324             while True:\n    325                 try:\n--&gt; 326                     return function(*args, **kwargs)\n    327                 except AuthenticationException as e:\n    328                     if &quot;Connection aborted.&quot; in str(e) and attempt &lt;= retries:\n\n\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages\/azureml\/core\/authentication.py in _get_arm_token_with_refresh(profile_object, cloud_type, account_object, config_object, session_object, config_directory, force_reload, resource)\n   1828         if not token_about_to_expire:\n   1829             raise AuthenticationException(&quot;Could not retrieve user token. Please run 'az login'&quot;,\n-&gt; 1830                                           inner_exception=e)\n   1831 \n   1832     try:\n\nAuthenticationException: AuthenticationException:\n    Message: Could not retrieve user token. Please run 'az login'\n    InnerException More than one token matches the criteria. The result is ambiguous.\n    ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;Authentication&quot;\n        },\n        &quot;message&quot;: &quot;Could not retrieve user token. Please run 'az login'&quot;\n    }\n}\n<\/code><\/pre>\n<ul>\n<li><code>azureml-sdk<\/code> is on version 1.9.0<\/li>\n<li>I can connect an authenticate from my local machine. Problems only occur when I want to work on a compute instance.<\/li>\n<\/ul>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1591616299803,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1594308517016,
        "Question_score":3.0,
        "Question_view_count":2737.0,
        "Answer_body":"<p>Okay, here is the answer:<\/p>\n<ul>\n<li>You work for company A which is on Azure.<\/li>\n<li>You get access to company B's subscription.<\/li>\n<li>Problem is: You are associated to A's AAD in ML-Studio.<\/li>\n<li>You need to specify the tenant ID in the <code>InteractiveLoginAuthentication<\/code> like so:<\/li>\n<\/ul>\n<pre><code>interactive_auth = InteractiveLoginAuthentication(tenant_id=tenant_id)\n\nworkspace = Workspace.get(name=workspace_name,\n                          subscription_id=subscription_id,\n                          resource_group=resource_group,\n                          auth=interactive_auth)\n<\/code><\/pre>\n<ul>\n<li>Now the <strong>important<\/strong> part: You need to use company B's <code>tenant_id<\/code> (I used company A's all the time since I thought that was my authentication point)<\/li>\n<li>Of course, this is obvious while you read it...as it is to me now :)<\/li>\n<\/ul>\n<p>Hope this helps you. Took me some time but learned a lot ;)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62261222",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1596034370720,
        "Question_original_content":"workspac authent token match criteria constantli run problem work azur comput instanc try connect jupyt lab workspac interactiveloginauthent follow messag authenticationexcept authenticationexcept messag retriev user token run login innerexcept token match criteria result ambigu errorrespons error code usererror inner error code authent messag retriev user token run login servic princip owner workspac workspaceexcept workspaceexcept messag workspac subscript access innerexcept errorrespons error messag workspac subscript access workspac differ subscript resolv give tennant extra input interactiveloginauthent time chanc funni thing login workspac interactiveloginauthent local supsect old token cach tri us privat brows function browser furthermor delet home azureus azur accesstoken json effect mayb problem idea refer site check http doc microsoft com azur machin learn setup authent http github com azur machinelearningnotebook blob master us manag servic authent authent ipynb http github com azur azur cli issu http github com azur azur cli issu updat run code core authent import interactiveloginauthent interact auth interactiveloginauthent tenant workspac subscript resourc group auth interact auth follow trace adalerror traceback recent anaconda env lib python site packag core authent arm token refresh profil object cloud type account object config object session object config directori forc reload resourc auth profil object login credenti resourc access token auth token retriev exp time access token time time account tenant resourc us cert issuer account user entiti servic princip cert issuer auth anaconda env lib python site packag vendor azur cli core profil retriev token user self usernam tenant resourc context self auth ctx factori self cloud type tenant cach self adal token cach token entri context acquir token resourc usernam client token entri anaconda env lib python site packag adal authent context acquir token self resourc user client return self acquir token token func anaconda env lib python site packag adal authent context acquir token self token func correl self author valid self context return token func self anaconda env lib python site packag adal authent context token func self token request tokenrequest self context self client resourc return token request token cach refresh user anaconda env lib python site packag adal token request token cach refresh self user self user user return self token cach anaconda env lib python site packag adal token request token cach self cach queri self creat cach queri return self cach driver cach queri anaconda env lib python site packag adal cach driver self queri queri log scrub pii queri entri resourc tenant specif self load singl entri cach queri entri anaconda env lib python site packag adal cach driver load singl entri cach self queri rais adalerror token match criteria result ambigu adalerror token match criteria result ambigu handl except except occur authenticationexcept traceback recent anaconda env lib python site packag core authent wrapper self arg kwarg modul logger debug acquir lock format type self durat return test function self arg kwarg except anaconda env lib python site packag core authent arm token self return self arm token interact auth anaconda env lib python site packag core authent arm token interact auth self forc reload resourc arm token arm token refresh profil object cloud type account config session config dir forc reload forc reload resourc resourc user specifi tenant need check token tenant anaconda env lib python site packag core authent connect abort wrapper arg kwarg try return function arg kwarg authenticationexcept anaconda env lib python site packag core authent arm token refresh profil object cloud type account object config object session object config directori forc reload resourc rais authenticationexcept retriev user token run login inner except authenticationexcept authenticationexcept messag retriev user token run login innerexcept token match criteria result ambigu errorrespons error code usererror inner error code authent messag retriev user token run login handl except except occur adalerror traceback recent anaconda env lib python site packag core authent arm token refresh profil object cloud type account object config object session object config directori forc reload resourc auth profil object login credenti resourc access token auth token retriev exp time access token time time account tenant resourc us cert issuer account user entiti servic princip cert issuer auth anaconda env lib python site packag vendor azur cli core profil retriev token user self usernam tenant resourc context self auth ctx factori self cloud type tenant cach self adal token cach token entri context acquir token resourc usernam client token entri anaconda env lib python site packag adal authent context acquir token self resourc user client return self acquir token token func anaconda env lib python site packag adal authent context acquir token self token func correl self author valid self context return token func self anaconda env lib python site packag adal authent context token func self token request tokenrequest self context self client resourc return token request token cach refresh user anaconda env lib python site packag adal token request token cach refresh self user self user user return self token cach anaconda env lib python site packag adal token request token cach self cach queri self creat cach queri return self cach driver cach queri anaconda env lib python site packag adal cach driver self queri queri log scrub pii queri entri resourc tenant specif self load singl entri cach queri entri anaconda env lib python site packag adal cach driver load singl entri cach self queri rais adalerror token match criteria result ambigu adalerror token match criteria result ambigu handl except except occur authenticationexcept traceback recent subscript abfdac resourc group businessintellig auth interact auth anaconda env lib python site packag core workspac auth subscript resourc group result dict workspac list subscript auth auth resourc group resourc group result dict lower result dict item lower anaconda env lib python site packag core workspac list subscript auth resourc group elif subscript resourc group workspac list workspac list legaci auth subscript subscript resourc group resourc group workspac process autorest workspac list anaconda env lib python site packag core workspac list legaci auth subscript resourc group ignor error return rais staticmethod anaconda env lib python site packag core workspac list legaci auth subscript resourc group ignor error base sdk common workspac model workspac workspac workspac autorest list command list workspac auth subscript subscript resourc group resourc group return workspac autorest list except anaconda env lib python site packag project command list workspac auth subscript resourc group resourc group list object workspacesoper list resourc group auth servic client azuremachinelearningworkspac subscript workspac resourc group workspac list list object valu anaconda env lib python site packag core authent servic client self client class subscript subscript bound base url multi tenant case caus confus subscript subscript list tenant self subscript id self check subscript exist subscript subscript list tenant anaconda env lib python site packag core authent subscript id self rtype list str arm token self arm token return self subscript id intern arm token anaconda env lib python site packag core authent wrapper self arg kwarg interactiveloginauthent forc true tenant self tenant try time return test function self arg kwarg rais anaconda env lib python site packag core authent arm token self return self ambient auth arm token return self arm token interact auth login failur decor interact auth lock anaconda env lib python site packag core authent arm token interact auth self forc reload resourc profil object profil async persist fals cloud type cloud type arm token arm token refresh profil object cloud type account config session config dir forc reload forc reload resourc resourc user specifi tenant need check token tenant self tenant fetch tenantid aad token arm token self tenant anaconda env lib python site packag core authent connect abort wrapper arg kwarg true try return function arg kwarg authenticationexcept connect abort str attempt inner except try authenticationexcept authenticationexcept messag retriev user token run login innerexcept token match criteria result ambigu errorrespons error code usererror inner error code authent messag retriev user token run login sdk version connect authent local machin problem occur want work comput instanc",
        "Question_preprocessed_content":"workspac authent token match criteria constantli run problem work azur comput instanc try connect jupyt lab workspac interactiveloginauthent follow messag servic princip workspac differ subscript resolv give tennant extra input interactiveloginauthent time chanc funni thing login workspac interactiveloginauthent local supsect old token cach tri us privat brows function browser furthermor delet effect mayb problem idea refer site check updat run code follow trace version connect authent local machin problem occur want work comput instanc",
        "Question_gpt_summary_original":"the user is encountering challenges when trying to connect from the jupyter lab to the workspace, with authentication exceptions due to more than one token matching the criteria.",
        "Question_gpt_summary":"user encount challeng try connect jupyt lab workspac authent except token match criteria",
        "Answer_original_content":"okai answer work compani azur access compani subscript problem associ aad studio need specifi tenant interactiveloginauthent like interact auth interactiveloginauthent tenant tenant workspac workspac workspac subscript subscript resourc group resourc group auth interact auth import need us compani tenant compani time thought authent point cours obviou read hope help took time learn lot",
        "Answer_preprocessed_content":"okai answer work compani azur access compani subscript problem associ aad need specifi tenant like import need us compani cours obviou read hope help took time learn lot",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to specify the tenant ID in the InteractiveLoginAuthentication and use the correct tenant ID associated with company B instead of company A's tenant ID.",
        "Answer_gpt_summary":"solut solut provid discuss specifi tenant interactiveloginauthent us correct tenant associ compani instead compani tenant"
    },
    {
        "Question_title":"How do I get tensorboard to work with sagemaker studio?",
        "Question_body":"<p>Currently playing around with tensorboard on Sagemaker studio. According to aws, it is supposedly possible. However, I keep encountering error code 500 after launching tensorboard and changing the file path to .....\/proxy\/{port number}<\/p>\n<p>Great if somebody can assists on this topic :)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1657288188783,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":22.0,
        "Answer_body":"<p>See this step by step guide on how to enable TensorBoard on SageMaker Studio here: <a href=\"https:\/\/github.com\/anoop-ml\/smstudio_tensorboard_sample\" rel=\"nofollow noreferrer\">https:\/\/github.com\/anoop-ml\/smstudio_tensorboard_sample<\/a><\/p>\n<p>Did you follow those instructions?<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72912418",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1657296138023,
        "Question_original_content":"tensorboard work studio current plai tensorboard studio accord aw supposedli possibl encount error code launch tensorboard chang file path proxi port number great somebodi assist topic",
        "Question_preprocessed_content":"tensorboard work studio current plai tensorboard studio accord aw supposedli possibl encount error code launch tensorboard chang file path number great somebodi assist topic",
        "Question_gpt_summary_original":"The user is facing challenges in getting Tensorboard to work with Sagemaker Studio, despite AWS claiming it is possible. The user encounters an error code 500 after launching Tensorboard and changing the file path to ...\/proxy\/{port number}. The user is seeking assistance to resolve this issue.",
        "Question_gpt_summary":"user face challeng get tensorboard work studio despit aw claim possibl user encount error code launch tensorboard chang file path proxi port number user seek assist resolv issu",
        "Answer_original_content":"step step guid enabl tensorboard studio http github com anoop smstudio tensorboard sampl follow instruct",
        "Answer_preprocessed_content":"step step guid enabl tensorboard studio follow instruct",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to a step-by-step guide on how to enable TensorBoard on SageMaker Studio. The user is advised to follow those instructions to resolve the issue.",
        "Answer_gpt_summary":"solut discuss provid link step step guid enabl tensorboard studio user advis follow instruct resolv issu"
    },
    {
        "Question_title":"Failing to read data from s3 to a spark dataframe in Sagemaker",
        "Question_body":"<p>I'm trying to read a csv file on an s3 bucket (for which the sagemaker notebook has full access to) into a spark dataframe however I am hitting the following issue where <code>sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar<\/code> can't be found. Any tips on how to resolve this is appreciate!<\/p>\n\n<pre><code>bucket = \"mybucket\"\nprefix = \"folder\/file.csv\"\ndf = spark.read.csv(\"s3:\/\/{}\/{}\/\".format(bucket,prefix))\n\nPy4JJavaError: An error occurred while calling o388.csv.\n: java.util.ServiceConfigurationError: org.apache.spark.sql.sources.DataSourceRegister: Error reading configuration file\nat java.util.ServiceLoader.fail(ServiceLoader.java:232)\nat java.util.ServiceLoader.parse(ServiceLoader.java:309)\nat java.util.ServiceLoader.access$200(ServiceLoader.java:185)\nat java.util.ServiceLoader$LazyIterator.hasNextService(ServiceLoader.java:357)\nat java.util.ServiceLoader$LazyIterator.hasNext(ServiceLoader.java:393)\nat java.util.ServiceLoader$1.hasNext(ServiceLoader.java:474)\nat scala.collection.convert.Wrappers$JIteratorWrapper.hasNext(Wrappers.scala:42)\nat scala.collection.Iterator$class.foreach(Iterator.scala:893)\nat scala.collection.AbstractIterator.foreach(Iterator.scala:1336)\nat scala.collection.IterableLike$class.foreach(IterableLike.scala:72)\nat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\nat scala.collection.TraversableLike$class.filterImpl(TraversableLike.scala:247)\nat scala.collection.TraversableLike$class.filter(TraversableLike.scala:259)\nat scala.collection.AbstractTraversable.filter(Traversable.scala:104)\nat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:614)\nat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:190)\nat org.apache.spark.sql.DataFrameReader.csv(DataFrameReader.scala:596)\nat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\nat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\nat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\nat java.lang.reflect.Method.invoke(Method.java:498)\nat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\nat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\nat py4j.Gateway.invoke(Gateway.java:282)\nat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\nat py4j.commands.CallCommand.execute(CallCommand.java:79)\nat py4j.GatewayConnection.run(GatewayConnection.java:238)\nat java.lang.Thread.run(Thread.java:745)\nCaused by: java.io.FileNotFoundException: \/home\/ec2-user\/anaconda3\/envs\/python3\/lib\/python3.6\/site-packages\/sagemaker_pyspark\/jars\/sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar (No such file or directory)\n    at java.util.zip.ZipFile.open(Native Method)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:219)\n    at java.util.zip.ZipFile.&lt;init&gt;(ZipFile.java:149)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:166)\n    at java.util.jar.JarFile.&lt;init&gt;(JarFile.java:103)\n    at sun.net.www.protocol.jar.URLJarFile.&lt;init&gt;(URLJarFile.java:93)\n    at sun.net.www.protocol.jar.URLJarFile.getJarFile(URLJarFile.java:69)\n    at sun.net.www.protocol.jar.JarFileFactory.get(JarFileFactory.java:84)\n    at sun.net.www.protocol.jar.JarURLConnection.connect(JarURLConnection.java:122)\n    at sun.net.www.protocol.jar.JarURLConnection.getInputStream(JarURLConnection.java:150)\n    at java.net.URL.openStream(URL.java:1045)\n    at java.util.ServiceLoader.parse(ServiceLoader.java:304)\n    ... 26 more\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1532387294417,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1532387921180,
        "Question_score":0.0,
        "Question_view_count":2283.0,
        "Answer_body":"<p>(Making comment to the original question as answer)<\/p>\n\n<p>It looks like a jupyter kernel issue. I had a similar issue and I used <code>Sparkmagic (pyspark)<\/code> kernel instead of <code>Sparkmagic (pyspark3)<\/code> and it is working fine. Follow instructions on this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/build-amazon-sagemaker-notebooks-backed-by-spark-in-amazon-emr\/\" rel=\"nofollow noreferrer\">blog<\/a> and see if it helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51488308",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1532493818792,
        "Question_original_content":"fail read data spark datafram try read csv file bucket notebook access spark datafram hit follow issu spark spark jar tip resolv appreci bucket mybucket prefix folder file csv spark read csv format bucket prefix pyjjavaerror error occur call csv java util serviceconfigurationerror org apach spark sql sourc datasourceregist error read configur file java util serviceload fail serviceload java java util serviceload pars serviceload java java util serviceload access serviceload java java util serviceload lazyiter hasnextservic serviceload java java util serviceload lazyiter hasnext serviceload java java util serviceload hasnext serviceload java scala collect convert wrapper jiteratorwrapp hasnext wrapper scala scala collect iter class foreach iter scala scala collect abstractiter foreach iter scala scala collect iterablelik class foreach iterablelik scala scala collect abstractiter foreach iter scala scala collect traversablelik class filterimpl traversablelik scala scala collect traversablelik class filter traversablelik scala scala collect abstracttravers filter travers scala org apach spark sql execut datasourc datasourc lookupdatasourc datasourc scala org apach spark sql dataframeread load dataframeread scala org apach spark sql dataframeread csv dataframeread scala sun reflect nativemethodaccessorimpl invok nativ method sun reflect nativemethodaccessorimpl invok nativemethodaccessorimpl java sun reflect delegatingmethodaccessorimpl invok delegatingmethodaccessorimpl java java lang reflect method invok method java pyj reflect methodinvok invok methodinvok java pyj reflect reflectionengin invok reflectionengin java pyj gatewai invok gatewai java pyj command abstractcommand invokemethod abstractcommand java pyj command callcommand execut callcommand java pyj gatewayconnect run gatewayconnect java java lang thread run thread java caus java filenotfoundexcept home user anaconda env python lib python site packag pyspark jar spark spark jar file directori java util zip zipfil open nativ method java util zip zipfil zipfil java java util zip zipfil zipfil java java util jar jarfil jarfil java java util jar jarfil jarfil java sun net protocol jar urljarfil urljarfil java sun net protocol jar urljarfil getjarfil urljarfil java sun net protocol jar jarfilefactori jarfilefactori java sun net protocol jar jarurlconnect connect jarurlconnect java sun net protocol jar jarurlconnect getinputstream jarurlconnect java java net url openstream url java java util serviceload pars serviceload java",
        "Question_preprocessed_content":"fail read data spark datafram try read csv file bucket spark datafram hit follow issu tip resolv appreci",
        "Question_gpt_summary_original":"The user is encountering an error while trying to read a CSV file from an S3 bucket into a Spark dataframe in Sagemaker. The error message indicates that the sagemaker-spark_2.11-spark_2.2.0-1.1.1.jar file cannot be found, which is causing the issue. The user is seeking tips on how to resolve this issue.",
        "Question_gpt_summary":"user encount error try read csv file bucket spark datafram error messag indic spark spark jar file caus issu user seek tip resolv issu",
        "Answer_original_content":"make comment origin question answer look like jupyt kernel issu similar issu sparkmag pyspark kernel instead sparkmag pyspark work fine follow instruct blog help",
        "Answer_preprocessed_content":"look like jupyt kernel issu similar issu kernel instead work fine follow instruct blog help",
        "Answer_gpt_summary_original":"Solution: The user can try using the Sparkmagic (pyspark) kernel instead of Sparkmagic (pyspark3) to resolve the issue. They can follow the instructions provided in the blog mentioned in the discussion.",
        "Answer_gpt_summary":"solut user try sparkmag pyspark kernel instead sparkmag pyspark resolv issu follow instruct provid blog mention discuss"
    },
    {
        "Question_title":"How do I add many CSV files to the catalog in Kedro?",
        "Question_body":"<p>I have hundreds of CSV files that I want to process similarly. For simplicity, we can assume that they are all in <code>.\/data\/01_raw\/<\/code> (like <code>.\/data\/01_raw\/1.csv<\/code>, <code>.\/data\/02_raw\/2.csv<\/code>) etc. I would much rather not give each file a different name and keep track of them individually when building my pipeline. I would like to know if there is any way to read all of them in bulk by specifying something in the <code>catalog.yml<\/code> file?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1588799145203,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1588803043230,
        "Question_score":4.0,
        "Question_view_count":806.0,
        "Answer_body":"<p>You are looking for <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a>. In your example, the <code>catalog.yml<\/code> might look like this:<\/p>\n<pre><code>my_partitioned_dataset:\n  type: &quot;PartitionedDataSet&quot;\n  path: &quot;data\/01_raw&quot;\n  dataset: &quot;pandas.CSVDataSet&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1600445892403,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61645397",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1588804881827,
        "Question_original_content":"add csv file catalog hundr csv file want process similarli simplic assum data raw like data raw csv data raw csv file differ track individu build pipelin like know wai read bulk specifi catalog yml file",
        "Question_preprocessed_content":"add csv file catalog hundr csv file want process similarli simplic assum file differ track individu build pipelin like know wai read bulk specifi file",
        "Question_gpt_summary_original":"The user is facing the challenge of adding hundreds of CSV files to the catalog in Kedro and wants to know if there is a way to read all of them in bulk by specifying something in the catalog.yml file, instead of giving each file a different name and keeping track of them individually when building the pipeline.",
        "Question_gpt_summary":"user face challeng ad hundr csv file catalog want know wai read bulk specifi catalog yml file instead give file differ keep track individu build pipelin",
        "Answer_original_content":"look partitioneddataset exampl catalog yml look like partit dataset type partitioneddataset path data raw dataset panda csvdataset",
        "Answer_preprocessed_content":"look partitioneddataset exampl look like",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to use PartitionedDataSet in Kedro. The user can specify the path to the directory containing all the CSV files in the catalog.yml file and use the pandas.CSVDataSet to read them in bulk. This way, the user does not have to give each file a different name and keep track of them individually when building the pipeline.",
        "Answer_gpt_summary":"solut solut suggest discuss us partitioneddataset user specifi path directori contain csv file catalog yml file us panda csvdataset read bulk wai user file differ track individu build pipelin"
    },
    {
        "Question_title":"Azure Machine Learning Studio vs. Workbench",
        "Question_body":"<p>What is the difference between <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-studio\/\" rel=\"noreferrer\">Azure Machine Learning Studio<\/a> and <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-services\/\" rel=\"noreferrer\">Azure Machine Learning Workbench<\/a>?  What is the <em>intended<\/em> difference? And is it expected that Workbench is heading towards deprecation in favor of Studio?<\/p>\n\n<p>I have gathered an assorted collection of differences:<\/p>\n\n<ul>\n<li>Studio has a hard limit of 10 GB total input of training data per module, whereas Workbench has a variable limit by price.<\/li>\n<li>Studio appears to have a more fully-featured GUI and user-friendly deployment tools, whereas Workbench appears to have more powerful \/ customizable deployment tools.<\/li>\n<li>etc.<\/li>\n<\/ul>\n\n<p>However, I have also found several scattered references claiming that Studio is a renamed updated of Workbench, even though both services appear to still be offered.<\/p>\n\n<p>For a fresh Data Scientist looking to adopt the Microsoft stack (potentially on an enterprise scale within the medium-term and for the long-term), which offering should I prefer?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1522638099293,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":8.0,
        "Question_view_count":3387.0,
        "Answer_body":"<p>Azure Machine Learning Workbench is a preview downloadable application. It provides a UI for many of the Azure Machine Learning CLI commands, particularly around experimentation submission for Python based jobs to DSVM or HDI. The Azure Machine Learning CLI is made up of many key functions, such as job submisison, and creation of real time web services. The workbench installer provided a way to install everything required to participate in the preview. <\/p>\n\n<p>Azure Machine Learning Studio is an older product, and provides a drag and drop interface for creating simply machine learning processes. It has limitations about the size of the data that can be handled (about 10gigs of processing). Learning and customer requests have based on this service have contributed to the design of the new Azure Machine Learning CLI mentioned above.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49604773",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1524806701632,
        "Question_original_content":"studio workbench differ studio workbench intend differ expect workbench head deprec favor studio gather assort collect differ studio hard limit total input train data modul workbench variabl limit price studio appear fulli featur gui user friendli deploy tool workbench appear power customiz deploy tool scatter refer claim studio renam updat workbench servic appear offer fresh data scientist look adopt microsoft stack potenti enterpris scale medium term long term offer prefer",
        "Question_preprocessed_content":"studio workbench differ studio workbench intend differ expect workbench head deprec favor studio gather assort collect differ studio hard limit total input train data modul workbench variabl limit price studio appear gui deploy tool workbench appear power customiz deploy tool scatter refer claim studio renam updat workbench servic appear offer fresh data scientist look adopt microsoft stack offer prefer",
        "Question_gpt_summary_original":"The user is trying to understand the differences between Azure Machine Learning Studio and Azure Machine Learning Workbench, and is facing challenges in deciding which one to use. The user has found differences such as Studio having a limit of 10 GB input of training data per module, while Workbench has a variable limit by price. Studio has a more user-friendly GUI and deployment tools, while Workbench has more powerful and customizable deployment tools. However, the user has also found scattered references claiming that Studio is a renamed and updated version of Workbench, which is causing confusion.",
        "Question_gpt_summary":"user try understand differ studio workbench face challeng decid us user differ studio have limit input train data modul workbench variabl limit price studio user friendli gui deploy tool workbench power customiz deploy tool user scatter refer claim studio renam updat version workbench caus confus",
        "Answer_original_content":"workbench preview download applic provid cli command particularli experiment submiss python base job dsvm hdi cli kei function job submisison creation real time web servic workbench instal provid wai instal requir particip preview studio older product provid drag drop interfac creat simpli machin learn process limit size data handl gig process learn custom request base servic contribut design new cli mention",
        "Answer_preprocessed_content":"workbench preview download applic provid cli command particularli experiment submiss python base job dsvm hdi cli kei function job submisison creation real time web servic workbench instal provid wai instal requir particip preview studio older product provid drag drop interfac creat simpli machin learn process limit size data handl learn custom request base servic contribut design new cli mention",
        "Answer_gpt_summary_original":"Solutions provided: \n- Azure Machine Learning Workbench provides a UI for many of the Azure Machine Learning CLI commands, particularly around experimentation submission for Python based jobs to DSVM or HDI. It has more powerful and customizable deployment tools.\n- Azure Machine Learning Studio provides a drag and drop interface for creating simple machine learning processes. It has a more user-friendly GUI and deployment tools. However, it has limitations about the size of the data that can be handled (about 10gigs of processing).\n- The Azure Machine Learning CLI is made up of many key functions, such as job submission, and creation of real-time web services. It is a newer product that has been designed based on learning and customer requests from Azure Machine Learning Studio.",
        "Answer_gpt_summary":"solut provid workbench provid cli command particularli experiment submiss python base job dsvm hdi power customiz deploy tool studio provid drag drop interfac creat simpl machin learn process user friendli gui deploy tool limit size data handl gig process cli kei function job submiss creation real time web servic newer product design base learn custom request studio"
    },
    {
        "Question_title":"Azure Data Factory : How to pass DataPath as a parameter to Azure ML Pipeline activity?",
        "Question_body":"Hello All,\n\nHow to pass a Datapath as a parameter in Azure ML Pipeline activity?\n\nMore details here : Have opened an issue here : https:\/\/github.com\/Azure\/Azure-DataFactory\/issues\/216\n\n\n\n\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_created_time":1599771191990,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Thanks @SriramNarayanan-6939 for your patience!\n\nI discussed with the Product team and they confirmed that there is no datatype supported for \"DataPath\" parameter today in Azure Data Factory(ADF). However, there is a feature already raised for the same and work is in progress for it.\n\nI would recommend you also to submit an idea in feedback forum. The ideas in this forum are closely monitored by data factory product team and will prioritize implementing them in future releases.\n\nSorry for the inconvenience!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/91785\/azure-data-factory-how-to-pass-datapath-as-a-param.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1601023399256,
        "Question_original_content":"azur data factori pass datapath paramet pipelin activ hello pass datapath paramet pipelin activ detail open issu http github com azur azur datafactori issu thank",
        "Question_preprocessed_content":"azur data factori pass datapath paramet pipelin activ hello pass datapath paramet pipelin activ detail open issu thank",
        "Question_gpt_summary_original":"The user is facing a challenge in passing a Datapath as a parameter to Azure ML Pipeline activity and has opened an issue on GitHub seeking a solution.",
        "Question_gpt_summary":"user face challeng pass datapath paramet pipelin activ open issu github seek solut",
        "Answer_original_content":"thank sriramnarayanan patienc discuss product team confirm datatyp support datapath paramet todai azur data factori adf featur rais work progress recommend submit idea feedback forum idea forum close monitor data factori product team priorit implement futur releas sorri inconveni",
        "Answer_preprocessed_content":"thank patienc discuss product team confirm datatyp support datapath paramet todai azur data factori featur rais work progress recommend submit idea feedback forum idea forum close monitor data factori product team priorit implement futur releas sorri inconveni",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The product team confirmed that there is no datatype supported for \"DataPath\" parameter in Azure Data Factory, but there is a feature already raised for the same and work is in progress for it. The user is recommended to submit an idea in the feedback forum.",
        "Answer_gpt_summary":"solut solut provid discuss product team confirm datatyp support datapath paramet azur data factori featur rais work progress user recommend submit idea feedback forum"
    },
    {
        "Question_title":"R code on Azure Machine Learning is slow compared to local execution time",
        "Question_body":"<p>Getting straight to it:\nWhy is my R code doing fine on my local CPU (under one minute), but tens of times slower on Azure Machine Learning, using one R script block (over 18 minutes)?<\/p>\n\n<p>I assume that it has to do with the resources allocated to the experiment, but how can I be sure? Can I obtain details about the resource allocated to the R script block from somwehere hidden in the Azure-ML Studio machinery?<\/p>\n\n<p>Thank you, Flo<\/p>\n\n<p>Later Edit:\nAs it often happens, I did finally find some information, which still doesn't solve my issue. According to <a href=\"https:\/\/msdn.microsoft.com\/library\/en-us\/Dn905952.aspx#Technical%20Notes\" rel=\"nofollow noreferrer\">https:\/\/msdn.microsoft.com\/library\/en-us\/Dn905952.aspx#Technical%20Notes<\/a> \"User-specified R code is run by a 64-bit R interpreter that runs in Azure using an A8 virtual machine with 56 GB of RAM.\"<\/p>\n\n<p>This is more than my local machine has, the R code is still much slower on the Azure-ML studio.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_created_time":1480336059453,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1480338194648,
        "Question_score":0.0,
        "Question_view_count":733.0,
        "Answer_body":"<p>Consider using rbenchmark or other benchmarking tools to get an idea of the runtime and complexity of your code. In general for loops tend to be slow.<\/p>\n\n<p>It's very possible that the server has less resources available (ram, cpu) or that you have to wait in a que before you get served. Without any more code it's hard to comment further on this issue.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40844351",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1480339657187,
        "Question_original_content":"code slow compar local execut time get straight code fine local cpu minut ten time slower script block minut assum resourc alloc experi sure obtain detail resourc alloc script block somweher hidden azur studio machineri thank flo later edit happen final inform solv issu accord http msdn microsoft com librari aspx technic note user specifi code run bit interpret run azur virtual machin ram local machin code slower azur studio",
        "Question_preprocessed_content":"code slow compar local execut time get straight code fine local cpu ten time slower script block assum resourc alloc experi sure obtain detail resourc alloc script block somweher hidden studio machineri thank flo later edit happen final inform solv issu accord code run interpret run azur virtual machin local machin code slower studio",
        "Question_gpt_summary_original":"The user is experiencing slow execution time for their R code on Azure Machine Learning compared to their local CPU, despite Azure having more resources allocated to it. The user is seeking information on how to obtain details about the resource allocation for the R script block in Azure-ML Studio.",
        "Question_gpt_summary":"user experienc slow execut time code compar local cpu despit azur have resourc alloc user seek inform obtain detail resourc alloc script block azur studio",
        "Answer_original_content":"consid rbenchmark benchmark tool idea runtim complex code gener loop tend slow possibl server resourc avail ram cpu wait que serv code hard comment issu",
        "Answer_preprocessed_content":"consid rbenchmark benchmark tool idea runtim complex code gener loop tend slow possibl server resourc avail wait que serv code hard comment issu",
        "Answer_gpt_summary_original":"Solutions provided in the discussion include using benchmarking tools such as rbenchmark to get an idea of the runtime and complexity of the code, and considering that the server may have less resources available or that there may be a queue before being served. No solution is provided for obtaining details about the resource allocation for the R script block in Azure-ML Studio.",
        "Answer_gpt_summary":"solut provid discuss includ benchmark tool rbenchmark idea runtim complex code consid server resourc avail queue serv solut provid obtain detail resourc alloc script block azur studio"
    },
    {
        "Question_title":"Executing R script from Azure function",
        "Question_body":"<p>I want to execute a R script every time an azure function is triggered. The R script executes perfectly on Azure machine learning Studio. But I am failing to execute through azure function.\nIs there any way to execute it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1612522682103,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":640.0,
        "Answer_body":"<p>AFAIK you'll have to create your own Runtime as <code>R<\/code> isn't supported natively.<\/p>\n<p>Have you already tried <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/azure-functions\/functions-create-function-linux-custom-image?tabs=bash%2Cportal&amp;pivots=programming-language-other\" rel=\"nofollow noreferrer\">&quot;Create a function on Linux using a custom container&quot;<\/a>? Interestingly they have given <code>R<\/code> as the example of custom runtime, so hopefully that answers your question.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66062015",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1612564671216,
        "Question_original_content":"execut script azur function want execut script time azur function trigger script execut perfectli studio fail execut azur function wai execut",
        "Question_preprocessed_content":"execut script azur function want execut script time azur function trigger script execut perfectli studio fail execut azur function wai execut",
        "Question_gpt_summary_original":"The user is facing challenges in executing an R script through Azure function despite it working perfectly on Azure machine learning Studio. They are seeking a solution to execute the script through Azure function.",
        "Question_gpt_summary":"user face challeng execut script azur function despit work perfectli studio seek solut execut script azur function",
        "Answer_original_content":"afaik creat runtim isn support nativ tri creat function linux custom contain interestingli given exampl custom runtim hopefulli answer question",
        "Answer_preprocessed_content":"afaik creat runtim isn support nativ tri creat function linux custom contain interestingli given exampl custom runtim hopefulli answer question",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to create a custom container using the \"Create a function on Linux using a custom container\" feature in Azure Functions. This feature allows for the use of custom runtimes, including R, which is not natively supported.",
        "Answer_gpt_summary":"solut possibl solut mention discuss creat custom contain creat function linux custom contain featur azur function featur allow us custom runtim includ nativ support"
    },
    {
        "Question_title":"Can machine learning rewrite\/recognize text to one truth",
        "Question_body":"Hi there,\n\nMy dataset has a lot of productnames, all the product of the shops are not written by the same.\nSo i want azure can recognize if it's the same:\n\nSo if the productgroup is X and productname looks like\/contains tomato. The product is tomato.\nExample: Tomatoes, tomato, bunch of tomatoes, a bag of tomatoes, small tomatoes = new colom tomato.\n\nHopefully someone can help me with this?\n\nThanks a lot.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1604082069767,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@Borget-8487 Thanks for the details. With a variety of data inputs, Can you try fuzzy matching\/regex\u2019s or Azure Search would be a complete Information Retrieval engine. Azure Search works well for this.\nbut using full Lucene syntax you can do fuzzy and proximity search.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/search\/search-query-lucene-examples",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/146531\/can-machine-learning-rewriterecognize-text-to-one.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1604310668223,
        "Question_original_content":"machin learn rewrit recogn text truth dataset lot productnam product shop written want azur recogn productgroup productnam look like contain tomato product tomato exampl tomato tomato bunch tomato bag tomato small tomato new colom tomato hopefulli help thank lot",
        "Question_preprocessed_content":"machin learn text truth dataset lot productnam product shop written want azur recogn productgroup productnam look tomato product tomato exampl tomato tomato bunch tomato bag tomato small tomato new colom tomato hopefulli help thank lot",
        "Question_gpt_summary_original":"The user is facing a challenge with recognizing and categorizing product names in a dataset, as the names are not consistent across all shops. They are seeking assistance in using Azure to recognize and group similar product names together.",
        "Question_gpt_summary":"user face challeng recogn categor product name dataset name consist shop seek assist azur recogn group similar product name",
        "Answer_original_content":"borget thank detail varieti data input try fuzzi match regex azur search complet inform retriev engin azur search work lucen syntax fuzzi proxim search http doc microsoft com azur search search queri lucen exampl",
        "Answer_preprocessed_content":"thank detail varieti data input try fuzzi azur search complet inform retriev engin azur search work lucen syntax fuzzi proxim search",
        "Answer_gpt_summary_original":"Solution: The discussion suggests two possible solutions for the challenge. The first solution is to use fuzzy matching\/regex to recognize and group similar product names together. The second solution is to use Azure Search, which is a complete Information Retrieval engine that works well for this task. Additionally, the discussion recommends using full Lucene syntax to perform fuzzy and proximity search.",
        "Answer_gpt_summary":"solut discuss suggest possibl solut challeng solut us fuzzi match regex recogn group similar product name second solut us azur search complet inform retriev engin work task addition discuss recommend lucen syntax perform fuzzi proxim search"
    },
    {
        "Question_title":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker?",
        "Question_body":"Anybody ever successfully ran multi-node gradient boosting on Amazon SageMaker? I'm looking for a SageMaker-compatible multi-node training solution for either Catboost, LightGBM or XGBoost. Knowing if it's ever been done would be nice, having a public demo link would be even better :)",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1607968009000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":44.0,
        "Answer_body":"We do have an example of distributed training of XGBoost in the sagemaker-examples repo. You can find it here: https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone_dist_script_mode.ipynb",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUGJGFHP76S0izgf1xfM6aIg\/anybody-ever-successfully-ran-multi-node-gradient-boosting-on-amazon-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1607969193000,
        "Question_original_content":"anybodi successfulli ran multi node gradient boost anybodi successfulli ran multi node gradient boost look compat multi node train solut catboost lightgbm xgboost know nice have public demo link better",
        "Question_preprocessed_content":"anybodi successfulli ran gradient boost anybodi successfulli ran gradient boost look compat train solut catboost lightgbm xgboost know nice have public demo link better",
        "Question_gpt_summary_original":"The user is seeking information on whether anyone has successfully run multi-node gradient boosting on Amazon SageMaker, specifically for Catboost, LightGBM, or XGBoost. They are looking for evidence of successful implementation and potentially a public demo link.",
        "Question_gpt_summary":"user seek inform successfulli run multi node gradient boost specif catboost lightgbm xgboost look evid success implement potenti public demo link",
        "Answer_original_content":"exampl distribut train xgboost exampl repo http github com aw amazon exampl blob master introduct amazon algorithm xgboost abalon xgboost abalon dist script mode ipynb",
        "Answer_preprocessed_content":"exampl distribut train xgboost exampl repo",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution in the form of a link to an example of distributed training of XGBoost in the sagemaker-examples repository. The example demonstrates successful implementation and can potentially serve as a reference for the user. No other solutions were mentioned.",
        "Answer_gpt_summary":"solut discuss provid solut form link exampl distribut train xgboost exampl repositori exampl demonstr success implement potenti serv refer user solut mention"
    },
    {
        "Question_title":"Beginners guide to Sagemaker",
        "Question_body":"<p>I have followed an Amazon tutorial for using SageMaker and have used it to create the model in the tutorial (<a href=\"https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/tutorials\/build-train-deploy-machine-learning-model-sagemaker\/<\/a>).<\/p>\n\n<p>This is my first time using SageMaker, so my question may be stupid.<\/p>\n\n<p>How do you actually view the model that it has created? I want to be able to see a) the final formula created with the parameters etc. b) graphs of plotted factors etc. as if I was reviewing a GLM for example.<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1557237604320,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":748.0,
        "Answer_body":"<p>If you followed the SageMaker tutorial you must have trained an XGBoost model. SageMaker places the model artifacts in a bucket that you own, check the output S3 location in the AWS SageMaker console. <\/p>\n\n<p>For more information about XGBoost you can check the AWS SageMaker documentation <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html#xgboost-sample-notebooks<\/a> and the example notebooks, e.g. <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/xgboost_abalone\/xgboost_abalone.ipynb<\/a><\/p>\n\n<p>To consume the XGBoost artifact generated by SageMaker, check out the official documentation, which contains the following code:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># SageMaker XGBoost uses the Python pickle module to serialize\/deserialize \n# the model, which can be used for saving\/loading the model.\n# To use a model trained with SageMaker XGBoost in open source XGBoost\n# Use the following Python code:\n\nimport pickle as pkl \nmodel = pkl.load(open(model_file_path, 'rb'))\n# prediction with test data\npred = model.predict(dtest)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56024351",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1559693969323,
        "Question_original_content":"beginn guid follow amazon tutori creat model tutori http aw amazon com get start tutori build train deploi machin learn model time question stupid actual view model creat want abl final formula creat paramet graph plot factor review glm exampl thank advanc",
        "Question_preprocessed_content":"beginn guid follow amazon tutori creat model tutori time question stupid actual view model creat want abl final formula creat paramet graph plot factor review glm exampl thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in viewing the model created using SageMaker. They are unsure of how to view the final formula and graphs of plotted factors.",
        "Question_gpt_summary":"user face challeng view model creat unsur view final formula graph plot factor",
        "Answer_original_content":"follow tutori train xgboost model place model artifact bucket check output locat consol inform xgboost check document http doc aw amazon com latest xgboost html xgboost sampl notebook exampl notebook http github com awslab amazon exampl blob master introduct amazon algorithm xgboost abalon xgboost abalon ipynb consum xgboost artifact gener check offici document contain follow code xgboost us python pickl modul serial deseri model save load model us model train xgboost open sourc xgboost us follow python code import pickl pkl model pkl load open model file path predict test data pred model predict dtest",
        "Answer_preprocessed_content":"follow tutori train xgboost model place model artifact bucket check output locat consol inform xgboost check document exampl notebook consum xgboost artifact gener check offici document contain follow code",
        "Answer_gpt_summary_original":"The discussion provides a solution to the challenge. The user can check the output S3 location in the AWS SageMaker console to find the model artifacts. They can also refer to the AWS SageMaker documentation and example notebooks for more information about XGBoost. To consume the XGBoost artifact generated by SageMaker, the user can use the Python pickle module to serialize\/deserialize the model.",
        "Answer_gpt_summary":"discuss provid solut challeng user check output locat consol model artifact refer document exampl notebook inform xgboost consum xgboost artifact gener user us python pickl modul serial deseri model"
    },
    {
        "Question_title":"Is it possible to use DistributedDataParallel with PyTorch Estimator",
        "Question_body":"<p>We know that Horovod is suppported. Is there an example script which uses DistributedDataParallel and Pytorch estimator?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1573859552880,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":98.0,
        "Answer_body":"<p>You should be able to specify nccl or gloo as distributed data parallel backend, in addition to MPI with Horovod. See the <em>distributed_training<\/em> parameter of <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.pytorch?view=azure-ml-py\" rel=\"nofollow noreferrer\">PyTorch Estimator<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58885873",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1574100412503,
        "Question_original_content":"possibl us distributeddataparallel pytorch estim know horovod suort exampl script us distributeddataparallel pytorch estim",
        "Question_preprocessed_content":"possibl us distributeddataparallel pytorch estim know horovod suort exampl script us distributeddataparallel pytorch estim",
        "Question_gpt_summary_original":"The user is seeking an example script that uses DistributedDataParallel with PyTorch estimator, as they are aware that Horovod is supported but are unsure about the former.",
        "Question_gpt_summary":"user seek exampl script us distributeddataparallel pytorch estim awar horovod support unsur",
        "Answer_original_content":"abl specifi nccl gloo distribut data parallel backend addit mpi horovod distribut train paramet pytorch estim",
        "Answer_preprocessed_content":"abl specifi nccl gloo distribut data parallel backend addit mpi horovod paramet pytorch estim",
        "Answer_gpt_summary_original":"Solution: The user can specify nccl or gloo as distributed data parallel backend in addition to MPI with Horovod by using the \"distributed_training\" parameter of PyTorch Estimator.",
        "Answer_gpt_summary":"solut user specifi nccl gloo distribut data parallel backend addit mpi horovod distribut train paramet pytorch estim"
    },
    {
        "Question_title":"Logging Metrics for each sample per epoch",
        "Question_body":"<p>Hey everyone,<br>\nLet\u2019s say that I have a dataset with 50000 samples and I am training my model for 10 epochs. Now, in each epoch, I am recording the <em>per sample loss<\/em> (i.e. loss of each sample - Not the average loss of all samples). This means that there are 50000 loss values per epoch. I want to log these values for <em>each epoch<\/em>, so that I can later perform some analysis on how the loss values for the samples change as training progresses (And, if possible, observe the loss values of a particular sample across epochs). For reference, <a href=\"https:\/\/proceedings.neurips.cc\/paper\/2020\/file\/62000dee5a05a6a71de3a6127a68778a-Paper.pdf\" rel=\"noopener nofollow ugc\">this<\/a> paper tracks such  statistics. Here are two ways I can think of doing this -<\/p>\n<ul>\n<li>A simple way to do this is to update the values in a 50000x10 array, then log the array as a table at the end of training (I would obviously need to track which indices belong to which samples). However, I need to wait for the training to end in this scenario.<\/li>\n<li>I can also log each sample\u2019s statistic with wandb.log (Maybe put them under \u201csample_statistics\/\u201d to pull them more easily). This ensures that the metrics are logged as and when they are observed, however, I am not sure if this is the most optimal solution.<\/li>\n<\/ul>\n<p>Is there any other way in which I can do this so that I can analyse the resulting data effectively? Open to all suggestions!<br>\nThank you!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1656534480479,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":89.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/tataganesh\">@tataganesh<\/a> ,<\/p>\n<p>Thank you for writing in with your question.   Ideally what would be best in your case here is to create an Empty table and log per sample loss values per epoch and be able to see your data live in the UI. However, we currently,  don\u2019t support adding new rows to existing tables that you\u2019ve already logged. We are working on adding this functionality.<\/p>\n<p>In the meantime here are two approaches<\/p>\n<ol>\n<li>Keep the wandb.Table locally holding all the data in memory and logging it once.<\/li>\n<li>Keep logging the same table at each step, and just add new rows to it. The final table you log will have all the rows you want, and you\u2019ll be able to see the latest table logged in the UI. This would be risky if you have large table sizes.<\/li>\n<\/ol>\n<p>Please Note: If you were to look through our docs and come across the<a href=\"https:\/\/docs.wandb.ai\/guides\/data-vis\/log-tables#add-data\"> Add Data Incrementally<\/a> to  Tables doc, this functionality is currently broken and we are working on an active fix.  There is github issues thread <a href=\"https:\/\/github.com\/wandb\/client\/issues\/2981\" rel=\"noopener nofollow ugc\">here<\/a> where community members have posted workarounds for this, you may find it helpful.<\/p>\n<p>Please let me know if you have any questions.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/logging-metrics-for-each-sample-per-epoch\/2678",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1656718364004,
        "Question_original_content":"log metric sampl epoch hei let dataset sampl train model epoch epoch record sampl loss loss sampl averag loss sampl mean loss valu epoch want log valu epoch later perform analysi loss valu sampl chang train progress possibl observ loss valu particular sampl epoch refer paper track statist wai think simpl wai updat valu arrai log arrai tabl end train obvious need track indic belong sampl need wait train end scenario log sampl statist log mayb sampl statist pull easili ensur metric log observ sure optim solut wai analys result data effect open suggest thank",
        "Question_preprocessed_content":"log metric sampl epoch hei let dataset sampl train model epoch epoch record sampl loss mean loss valu epoch want log valu epoch later perform analysi loss valu sampl chang train progress refer paper track statist wai think simpl wai updat valu arrai log arrai tabl end train need wait train end scenario log sampl statist log ensur metric log observ sure optim solut wai analys result data effect open suggest thank",
        "Question_gpt_summary_original":"The user is facing the challenge of logging per sample loss for each epoch while training a model on a dataset with 50,000 samples for 10 epochs. They want to analyze how the loss values for the samples change as training progresses and observe the loss values of a particular sample across epochs. The user is considering two ways to log the data, but they are open to other suggestions for effective analysis.",
        "Question_gpt_summary":"user face challeng log sampl loss epoch train model dataset sampl epoch want analyz loss valu sampl chang train progress observ loss valu particular sampl epoch user consid wai log data open suggest effect analysi",
        "Answer_original_content":"tataganesh thank write question ideal best case creat tabl log sampl loss valu epoch abl data live current dont support ad new row exist tabl youv log work ad function meantim approach tabl local hold data memori log log tabl step add new row final tabl log row want youll abl latest tabl log riski larg tabl size note look doc come add data increment tabl doc function current broken work activ fix github issu thread commun member post workaround help let know question regard mohammad",
        "Answer_preprocessed_content":"thank write question ideal best case creat tabl log sampl loss valu epoch abl data live current dont support ad new row exist tabl youv log work ad function meantim approach tabl local hold data memori log log tabl step add new row final tabl log row want youll abl latest tabl log riski larg tabl size note look doc come add data increment tabl doc function current broken work activ fix github issu thread commun member post workaround help let know question regard mohammad",
        "Answer_gpt_summary_original":"Two possible solutions were suggested in the discussion for the challenge of logging per sample loss for each epoch while training a model on a dataset with 50,000 samples for 10 epochs. The first solution is to keep the wandb.Table locally holding all the data in memory and logging it once. The second solution is to keep logging the same table at each step and just add new rows to it. However, it was noted that the second solution would be risky if the table size is large. The discussion also mentioned that the functionality of adding new rows to existing tables is currently not supported, but the team is working on adding this feature.",
        "Answer_gpt_summary":"possibl solut suggest discuss challeng log sampl loss epoch train model dataset sampl epoch solut tabl local hold data memori log second solut log tabl step add new row note second solut riski tabl size larg discuss mention function ad new row exist tabl current support team work ad featur"
    },
    {
        "Question_title":"Why do I get 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS error for Vertex AI init?",
        "Question_body":"<p>I am trying to set up mlops for Vertex AI, following <a href=\"https:\/\/github.com\/GoogleCloudPlatform\/mlops-with-vertex-ai\/blob\/main\/01-dataset-management.ipynb\" rel=\"nofollow noreferrer\">this notebook<\/a>. It works until, near the end, I try:<\/p>\n<pre><code>vertex_ai.init(\nproject=PROJECT,\n    location=REGION)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code> module 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS\n<\/code><\/pre>\n<p>I am using <code>us-central1<\/code> which is supported. I wondered if maybe <code>from google.cloud import aiplatform as vertex_ai<\/code> has been changed but don't know how to find out. Any help is much appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1652110521003,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":298.0,
        "Answer_body":"<p>I followed the same Notebook as you, even though I didn't have any issue. What could be happening to you is that you are using an <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform\/releases\" rel=\"nofollow noreferrer\">older version of the library<\/a>.<\/p>\n<p>You can use the command to upgrade the library that is the following one: <code>pip3 install google-cloud-aiplatform --upgrade<\/code>.<\/p>\n<p>Sometimes this happens with the basic installation of the library; the problems could be in the <a href=\"https:\/\/github.com\/googleapis\/python-aiplatform#installation\" rel=\"nofollow noreferrer\">dependencies, versions and indirectly permissions<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72174602",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1652193787552,
        "Question_original_content":"googl cloud aiplatform constant attribut support region error init try set mlop follow notebook work near end try vertex init project project locat region give modul googl cloud aiplatform constant attribut support region central support wonder mayb googl cloud import aiplatform vertex chang know help appreci",
        "Question_preprocessed_content":"attribut error init try set mlop follow notebook work near end try give support wonder mayb chang know help appreci",
        "Question_gpt_summary_original":"The user is encountering an error while setting up mlops for Vertex AI. Specifically, when trying to initialize Vertex AI with a project and region, the user receives an error message stating that 'google.cloud.aiplatform.constants' has no attribute 'SUPPORTED_REGIONS'. The user is unsure why this error is occurring, as the region they are using (us-central1) is supported. The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user encount error set mlop specif try initi project region user receiv error messag state googl cloud aiplatform constant attribut support region user unsur error occur region central support user seek assist resolv issu",
        "Answer_original_content":"follow notebook issu happen older version librari us command upgrad librari follow pip instal googl cloud aiplatform upgrad happen basic instal librari problem depend version indirectli permiss",
        "Answer_preprocessed_content":"follow notebook issu happen older version librari us command upgrad librari follow happen basic instal librari problem depend version indirectli permiss",
        "Answer_gpt_summary_original":"Solution: The user can try upgrading the library by using the command 'pip3 install google-cloud-aiplatform --upgrade'. This error could be occurring due to an older version of the library being used, which may have issues with dependencies, versions, or permissions.",
        "Answer_gpt_summary":"solut user try upgrad librari command pip instal googl cloud aiplatform upgrad error occur older version librari issu depend version permiss"
    },
    {
        "Question_title":"Azure Machine Learning - Memory Error while creating dataframe",
        "Question_body":"<p>I am getting memory error while creating simple dataframe read from CSV file on Azure Machine Learning using notebook VM as compute instance. The VM has config of DS 13 56gb RAM, 8vcpu, 112gb storage on Ubuntu (Linux (ubuntu 16.04). CSV file is 5gb file. <\/p>\n\n<pre><code>blob_service = BlockBlobService(account_name,account_key)\nblobstring = blob_service.get_blob_to_text(container,filepath).content\ndffinaldata = pd.read_csv(StringIO(blobstring), sep=',')\n<\/code><\/pre>\n\n<p>What I am doing wrong here ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1579544749467,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1579556126092,
        "Question_score":1.0,
        "Question_view_count":507.0,
        "Answer_body":"<p>you need to provide the right encoding when calling get_blob_to_text, please refer to the <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\/blob\/master\/samples\/blob\/block_blob_usage.py#L390\" rel=\"nofollow noreferrer\">sample<\/a>.<\/p>\n\n<p>The code below is what  normally use for reading data file in blob storages. Basically, you can use blob\u2019s url along with sas token and use a request method. However, You might want to edit the \u2018for loop\u2019 depending what types of data you have (e.g. csv, jpg, and etc).<\/p>\n\n<p>-- Python code below --<\/p>\n\n<pre><code>import requests\nfrom azure.storage.blob import BlockBlobService, BlobPermissions\nfrom azure.storage.blob.baseblobservice import BaseBlobService\nfrom datetime import datetime, timedelta\n\naccount_name = '&lt;account_name&gt;'\naccount_key = '&lt;account_key&gt;'\ncontainer_name = '&lt;container_name&gt;'\n\nblob_service=BlockBlobService(account_name,account_key)\ngenerator = blob_service.list_blobs(container_name)\n\nfor blob in generator:\n    url = f\"https:\/\/{account_name}.blob.core.windows.net\/{container_name}\"\n    service = BaseBlobService(account_name=account_name, account_key=account_key)\n    token = service.generate_blob_shared_access_signature(container_name, img_name, permission=BlobPermissions.READ, expiry=datetime.utcnow() + timedelta(hours=1),)\n    url_with_sas = f\"{url}?{token}\"\n    response = requests.get(url_with_sas)\n<\/code><\/pre>\n\n<p>Please follow the below link to read data on Azure Blob Storage.\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1579586200472,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59829017",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1579583849896,
        "Question_original_content":"memori error creat datafram get memori error creat simpl datafram read csv file notebook comput instanc config ram vcpu storag ubuntu linux ubuntu csv file file blob servic blockblobservic account account kei blobstr blob servic blob text contain filepath content dffinaldata read csv stringio blobstr sep wrong",
        "Question_preprocessed_content":"memori error creat datafram get memori error creat simpl datafram read csv file notebook comput instanc config ram vcpu storag ubuntu csv file file wrong",
        "Question_gpt_summary_original":"The user is encountering a memory error while creating a dataframe from a 5GB CSV file on Azure Machine Learning using a notebook VM as a compute instance with a configuration of DS 13 56gb RAM, 8vcpu, 112gb storage on Ubuntu (Linux (ubuntu 16.04). The user is seeking assistance in identifying the issue.",
        "Question_gpt_summary":"user encount memori error creat datafram csv file notebook comput instanc configur ram vcpu storag ubuntu linux ubuntu user seek assist identifi issu",
        "Answer_original_content":"need provid right encod call blob text refer sampl code normal us read data file blob storag basic us blob url sa token us request method want edit loop depend type data csv jpg python code import request azur storag blob import blockblobservic blobpermiss azur storag blob baseblobservic import baseblobservic datetim import datetim timedelta account account kei contain blob servic blockblobservic account account kei gener blob servic list blob contain blob gener url http account blob core window net contain servic baseblobservic account account account kei account kei token servic gener blob share access signatur contain img permiss blobpermiss read expiri datetim utcnow timedelta hour url sa url token respons request url sa follow link read data azur blob storag http doc microsoft com azur machin learn access data",
        "Answer_preprocessed_content":"need provid right encod call refer sampl code normal us read data file blob storag basic us blob url sa token us request method want edit loop depend type data python code follow link read data azur blob storag",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting that the user needs to provide the right encoding when calling get_blob_to_text. The discussion also provides a Python code that can be used for reading data files in blob storages. The code uses blob's URL along with SAS token and a request method. However, the user might need to edit the 'for loop' depending on the types of data they have. The discussion also provides a link to the Microsoft documentation on how to access data on Azure Blob Storage.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest user need provid right encod call blob text discuss provid python code read data file blob storag code us blob url sa token request method user need edit loop depend type data discuss provid link microsoft document access data azur blob storag"
    },
    {
        "Question_title":"Can you share success stories of AWS customers performing ML CI\/CD?",
        "Question_body":"I want to create simple templates for scientists so that they can fit their models easily into a continuous integration\/continuous delivery (CI\/CD) pipeline. I want to know about success stories of AWS customers performing CI\/CD on machine learning pipelines.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1592577511000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Answer_body":"Amazon has released the [Amazon SageMaker Pipelines][1] that are the first purpose-built CI\/CD service for machine learning: [1]: https:\/\/aws.amazon.com\/sagemaker\/pipelines\/\n\nFor more information, see [New \u2013 Amazon SageMaker Pipelines brings DevOps capabilities to your machine learning projects] [2] [2]: https:\/\/aws.amazon.com\/blogs\/aws\/amazon-sagemaker-pipelines-brings-devops-to-machine-learning-projects\/\n\nAdditionally, we have a case-study where a customer created one on their own for model development using Airflow. For more information, see [NerdWallet uses machine learning on AWS to power recommendations platform][3] and [Using Amazon SageMaker to build a machine learning platform with just three engineers][4]. [3]: https:\/\/aws.amazon.com\/solutions\/case-studies\/nerdwallet-case-study\/ [4]: https:\/\/www.nerdwallet.com\/blog\/engineering\/machine-learning-platform-amazon-sagemaker\/",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUwLq6HNRZSOK7ODKKc_lC3Q\/can-you-share-success-stories-of-aws-customers-performing-ml-ci-cd",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1592579742000,
        "Question_original_content":"share success stori aw custom perform want creat simpl templat scientist fit model easili continu integr continu deliveri pipelin want know success stori aw custom perform machin learn pipelin",
        "Question_preprocessed_content":"share success stori aw custom perform want creat simpl templat scientist fit model easili continu deliveri pipelin want know success stori aw custom perform machin learn pipelin",
        "Question_gpt_summary_original":"The user is seeking success stories of AWS customers who have implemented continuous integration\/continuous delivery (CI\/CD) on machine learning pipelines. They aim to create simple templates for scientists to easily fit their models into a CI\/CD pipeline.",
        "Question_gpt_summary":"user seek success stori aw custom implement continu integr continu deliveri machin learn pipelin aim creat simpl templat scientist easili fit model pipelin",
        "Answer_original_content":"amazon releas pipelin purpos built servic machin learn http aw amazon com pipelin inform new pipelin bring devop capabl machin learn project http aw amazon com blog aw amazon pipelin bring devop machin learn project addition case studi custom creat model develop airflow inform nerdwallet us machin learn aw power recommend platform build machin learn platform engin http aw amazon com solut case studi nerdwallet case studi http nerdwallet com blog engin machin learn platform amazon",
        "Answer_preprocessed_content":"amazon releas servic machin learn inform addition custom creat model develop airflow inform",
        "Answer_gpt_summary_original":"Solution: Amazon has released the Amazon SageMaker Pipelines, which is the first purpose-built CI\/CD service for machine learning. There is also a case-study where a customer created their own CI\/CD pipeline for model development using Airflow.",
        "Answer_gpt_summary":"solut amazon releas pipelin purpos built servic machin learn case studi custom creat pipelin model develop airflow"
    },
    {
        "Question_title":"ipydatagrid widget does not display in SageMaker Studio",
        "Question_body":"<h2>The problem<\/h2>\n<p>I have a jupyter notebook with an <code>ipydatagrid<\/code> widget that displays a dataframe. This notebook works correctly when run locally, but not when run in AWS SageMaker Studio. When run in SageMaker Studio, instead of showing the widget it simply shows the text <code>Loading widget...<\/code><\/p>\n<p>How does one use a <code>ipydatagrid<\/code> widget in the SageMaker Studio environment?<\/p>\n<h2>Details<\/h2>\n<p>Python version:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ python --version\nPython 3.7.10\n<\/code><\/pre>\n<p>Run at start:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>$ pip install -r requirements.txt\n$ jupyter nbextension enable --py --sys-prefix widgetsnbextension\n$ jupyter nbextension install --py --symlink --sys-prefix ipydatagrid\n$ jupyter nbextension enable --py --sys-prefix ipydatagrid\n<\/code><\/pre>\n<p>File <code>requirements.txt<\/code>:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>ipydatagrid==1.1.11\npandas==1.0.1\n<\/code><\/pre>\n<p>Notebook contents:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># %%\nimport pandas as pd\nfrom ipydatagrid import DataGrid\nfrom IPython.display import display\nimport ipywidgets\n\n# %%\ndata = [\n    (&quot;potato&quot;, 1.2, True),\n    (&quot;sweet potato&quot;, 0.8, False),\n    (&quot;french fries&quot;, 4.5, True),\n    (&quot;waffle fries&quot;, 4.9, True)\n]\ndf = pd.DataFrame(\n    data,\n    columns=[&quot;food&quot;, &quot;stars&quot;, &quot;is_available&quot;]\n)\n\n# %%\ngrid = DataGrid(df)\n\n# %%\ndisplay(grid)\n\n# %%\n# SANITY CHECK:\nbutton = ipywidgets.Button(\n    description=&quot;Button&quot;,\n    disabled=False\n)\ndef on_click(b):\n    print(&quot;CLICK&quot;)\nbutton.on_click(on_click)\ndisplay(button)\n<\/code><\/pre>\n<h3>Error messages<\/h3>\n<p>If I use the Google Chrome developer tools, I can see more logs in the browser that give some error messages, most of which are repeated:<\/p>\n<pre class=\"lang-none prettyprint-override\"><code>manager.js:305 Uncaught (in promise) Error: Module ipydatagrid, semver range ^1.1.11 is not registered as a widget module\n    at C.loadClass (manager.js:305:19)\n    at C.&lt;anonymous&gt; (manager-base.js:263:46)\n    at l (manager-base.js:44:23)\n    at Object.next (manager-base.js:25:53)\n    at manager-base.js:19:71\n    at new Promise (&lt;anonymous&gt;)\n    at Rtm6.k (manager-base.js:15:12)\n    at C.e._make_model (manager-base.js:257:16)\n    at C.&lt;anonymous&gt; (manager-base.js:246:45)\n    at l (manager-base.js:44:23)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>utils.js:119 Error: Could not create a model.\n    at n (utils.js:119:27)\n    at async C._handleCommOpen (manager.js:61:51)\n    at async v._handleCommOpen (default.js:994:100)\n    at async v._handleMessage (default.js:1100:43)\n<\/code><\/pre>\n<pre class=\"lang-none prettyprint-override\"><code>manager-base.js:273 Could not instantiate widget\n<\/code><\/pre>\n<p>However, there is no overt error message that's immediately obvious to the user, including in the log where <code>print<\/code> statements send their output.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1648246951157,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":312.0,
        "Answer_body":"<p>SageMaker Studio currently runs JupyterLab v1.2 (as confirmed by <em>Help &gt; About JupyterLab<\/em>), and per the <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid#Installation\" rel=\"nofollow noreferrer\">ipydatagrid installation instructions<\/a>, current\/recent versions of this widget require v3+... So I think this is most likely your problem - as there were breaking changes in the interfaces for extensions between these major versions.<\/p>\n<p>I had a quick look at the past releases of <code>ipydatagrid<\/code> to see if using an older version would be possible, and it seems like the documented JLv3 requirement gets added between <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/0.2.16\" rel=\"nofollow noreferrer\">v0.2.16<\/a> and <a href=\"https:\/\/github.com\/bloomberg\/ipydatagrid\/tree\/1.0.1\" rel=\"nofollow noreferrer\">v1.0.1<\/a> (which are adjacent on GitHub).<\/p>\n<p>However, the old install instructions documented on 0.2 don't seem to work anymore: I get <code>ValueError: &quot;jupyter-datagrid&quot; is not a valid npm package<\/code> and also note that versions &lt;1.0 don't seem to be present <a href=\"https:\/\/libraries.io\/pypi\/ipydatagrid\/versions\" rel=\"nofollow noreferrer\">on PyPI<\/a>.<\/p>\n<p>So unfortunately I think (unless\/until SM Studio gets a JupyterLab version upgrade), this widget's not likely to work unless you dive in to building it from an old source code version.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71623732",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1648549758096,
        "Question_original_content":"ipydatagrid widget displai studio problem jupyt notebook ipydatagrid widget displai datafram notebook work correctli run local run studio run studio instead show widget simpli show text load widget us ipydatagrid widget studio environ detail python version python version python run start pip instal requir txt jupyt nbextens enabl sy prefix widgetsnbextens jupyt nbextens instal symlink sy prefix ipydatagrid jupyt nbextens enabl sy prefix ipydatagrid file requir txt ipydatagrid panda notebook content import panda ipydatagrid import datagrid ipython displai import displai import ipywidget data potato true sweet potato fals french fri true waffl fri true datafram data column food star avail grid datagrid displai grid saniti check button ipywidget button descript button disabl fals def click print click button click click displai button error messag us googl chrome develop tool log browser error messag repeat manag uncaught promis error modul ipydatagrid semver rang regist widget modul loadclass manag manag base manag base object manag base manag base new promis rtm manag base model manag base manag base manag base util error creat model util async handlecommopen manag async handlecommopen default async handlemessag default manag base instanti widget overt error messag immedi obviou user includ log print statement send output",
        "Question_preprocessed_content":"ipydatagrid widget displai studio problem jupyt notebook widget displai datafram notebook work correctli run local run studio run studio instead show widget simpli show text us widget studio environ detail python version run start file notebook content error messag us googl chrome develop tool log browser error messag repeat overt error messag immedi obviou user includ log statement send output",
        "Question_gpt_summary_original":"The user is facing a challenge where the ipydatagrid widget is not displaying in AWS SageMaker Studio, despite working correctly when run locally. The user has installed the necessary requirements and enabled the widget extensions, but the widget only shows the text \"Loading widget...\". The user has also encountered error messages in the browser console, but there is no clear error message for the user to address.",
        "Question_gpt_summary":"user face challeng ipydatagrid widget displai studio despit work correctli run local user instal necessari requir enabl widget extens widget show text load widget user encount error messag browser consol clear error messag user address",
        "Answer_original_content":"studio current run jupyterlab confirm help jupyterlab ipydatagrid instal instruct current recent version widget requir think like problem break chang interfac extens major version quick look past releas ipydatagrid older version possibl like document jlv requir get ad adjac github old instal instruct document work anymor valueerror jupyt datagrid valid npm packag note version present pypi unfortun think studio get jupyterlab version upgrad widget like work dive build old sourc code version",
        "Answer_preprocessed_content":"studio current run jupyterlab ipydatagrid instal instruct version widget requir think like problem break chang interfac extens major version quick look past releas older version possibl like document jlv requir get ad old instal instruct document work anymor note version present pypi unfortun think widget like work dive build old sourc code version",
        "Answer_gpt_summary_original":"Solution: The user is facing an issue where the ipydatagrid widget is not displaying in AWS SageMaker Studio. The widget only shows the text \"Loading widget...\". The user has installed the necessary requirements and enabled the widget extensions, but the widget is not working. The issue is that SageMaker Studio currently runs JupyterLab v1.2, and the current\/recent versions of this widget require v3+. The solution is to upgrade the JupyterLab version to v3+ or build the widget from an old source code version. However, unless\/until SM Studio gets a JupyterLab version upgrade, this widget is not likely to work.",
        "Answer_gpt_summary":"solut user face issu ipydatagrid widget displai studio widget show text load widget user instal necessari requir enabl widget extens widget work issu studio current run jupyterlab current recent version widget requir solut upgrad jupyterlab version build widget old sourc code version studio get jupyterlab version upgrad widget like work"
    },
    {
        "Question_title":"Failed precondition: Table not initialized. on deployed universal sentence encoder from aws sagemaker",
        "Question_body":"<p>I have deployed a the universal_sentence_encoder_large_3 to an aws sagemaker.  When I am attempting to predict with the deployed model I get <code>Failed precondition: Table not initialized.<\/code> as an error. I have included the part where I save my model below:<\/p>\n\n<pre><code>import tensorflow as tf\nimport tensorflow_hub as hub\nimport numpy as np\ndef tfhub_to_savedmodel(model_name, export_path):\n\n    model_path = '{}\/{}\/00000001'.format(export_path, model_name)\n    tfhub_uri = 'http:\/\/tfhub.dev\/google\/universal-sentence-encoder-large\/3'\n\n    with tf.Session() as sess:\n        module = hub.Module(tfhub_uri)\n        sess.run([tf.global_variables_initializer(), tf.tables_initializer()])\n        input_params = module.get_input_info_dict()\n        dtype = input_params['text'].dtype\n        shape = input_params['text'].get_shape()\n\n        # define the model inputs\n        inputs = {'text': tf.placeholder(dtype, shape, 'text')}\n        output = module(inputs['text'])\n        outputs = {\n            'vector': output,\n        }\n\n        # export the model\n        tf.saved_model.simple_save(\n            sess,\n            model_path,\n            inputs=inputs,\n            outputs=outputs)  \n\n    return model_path\n<\/code><\/pre>\n\n<p>I have seen other people ask this problem but no solution has been ever posted.  It seems to be a common problem with tensorflow_hub sentence encoders<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1563993432753,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":365.0,
        "Answer_body":"<p>I was running into this exact issue earlier this week while trying to modify this example <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_serving_container\/tensorflow_serving_container.ipynb\" rel=\"nofollow noreferrer\">Sagemaker notebook<\/a>. Particularly the part where serving the model. That is, running <code>predictor.predict()<\/code> on the Sagemaker Tensorflow Estimator.<\/p>\n\n<p>The solution outlined in the issue worked perfectly for me- <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/issues\/773#issuecomment-509433290<\/a><\/p>\n\n<p>I think it's just because <code>tf.tables_initializer()<\/code> only runs for training but it needs to be specified through the <code>legacy_init_op<\/code> if you want to run it during prediction.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57189292",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1580113898207,
        "Question_original_content":"fail precondit tabl initi deploi univers sentenc encod deploi univers sentenc encod larg attempt predict deploi model fail precondit tabl initi error includ save model import tensorflow import tensorflow hub hub import numpi def tfhub savedmodel model export path model path format export path model tfhub uri http tfhub dev googl univers sentenc encod larg session sess modul hub modul tfhub uri sess run global variabl initi tabl initi input param modul input info dict dtype input param text dtype shape input param text shape defin model input input text placehold dtype shape text output modul input text output vector output export model save model simpl save sess model path input input output output return model path seen peopl ask problem solut post common problem tensorflow hub sentenc encod",
        "Question_preprocessed_content":"fail precondit tabl initi deploi univers sentenc encod deploi attempt predict deploi model error includ save model seen peopl ask problem solut post common problem sentenc encod",
        "Question_gpt_summary_original":"The user has encountered an error message \"Failed precondition: Table not initialized\" while attempting to predict with the deployed universal_sentence_encoder_large_3 model on AWS Sagemaker. The error seems to be a common problem with TensorFlow Hub sentence encoders, and the user has not found a solution yet.",
        "Question_gpt_summary":"user encount error messag fail precondit tabl initi attempt predict deploi univers sentenc encod larg model error common problem tensorflow hub sentenc encod user solut",
        "Answer_original_content":"run exact issu earlier week try modifi exampl notebook particularli serv model run predictor predict tensorflow estim solut outlin issu work perfectli http github com awslab amazon exampl issu issuecom think tabl initi run train need specifi legaci init want run predict",
        "Answer_preprocessed_content":"run exact issu earlier week try modifi exampl notebook particularli serv model run tensorflow estim solut outlin issu work perfectli think run train need specifi want run predict",
        "Answer_gpt_summary_original":"Solution: The user can try specifying <code>legacy_init_op=tf.tables_initializer()<\/code> when creating the TensorFlow estimator to run <code>tf.tables_initializer()<\/code> during prediction. This solution was suggested in an issue on the AWS Sagemaker examples GitHub repository and worked for the person who encountered the same error.",
        "Answer_gpt_summary":"solut user try specifi legaci init tabl initi creat tensorflow estim run tabl initi predict solut suggest issu exampl github repositori work person encount error"
    },
    {
        "Question_title":"Contextual version conflict error, Microsoft Azure Machine Learning Studio",
        "Question_body":"<p>I'm trying to run this 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio<\/p>\n<p><a href=\"https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing\" rel=\"nofollow noreferrer\">https:\/\/colab.research.google.com\/drive\/1o_-QIR8yVphfnbNZGYemyEr111CHHxSv?usp=sharing<\/a><\/p>\n<p>When I get to this step:<\/p>\n<pre><code>import gradio as gr\nimport tensorflow as tf\nfrom transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n<\/code><\/pre>\n<p>I get this error:<\/p>\n<pre><code>ContextualVersionConflict: (Flask 1.0.3 (\/anaconda\/envs\/azureml_py36\/lib\/python3.6\/site-packages), Requirement.parse('Flask&gt;=1.1.1'), {'gradio'})\n<\/code><\/pre>\n<p>I tried to install the Flask 1.1.1 version but I get more errors. Any idea what I should do to get past this step in Azure ML Studio?<\/p>\n<pre><code>!pip install \u2013force-reinstall Flask==1.1.1\n\/\/ More errors\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1630103458310,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1630103807136,
        "Question_score":2.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>The issue is because <code>gradio<\/code> package using existing Flask package (version 1.0.3). But as your application required Flask&gt;=1.1.1, therefore it is showing error. You need to uninstall the existing Flask package and then install the latest required version.<\/p>\n<p>To uninstall the existing package:\n<code>!pip uninstall Flask -y<\/code><\/p>\n<p>To install latest package:\n<code>!pip install Flask&gt;=1.1.1<\/code><\/p>\n<p><strong>Then, make sure to restart your runtime to pick up the new Flask using the Runtime -&gt; Restart runtime menu.<\/strong><\/p>\n<p>Finally, import gradio.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68959934",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1630303052768,
        "Question_original_content":"contextu version conflict error studio try run line ipynb file googl colab studio http colab research googl com drive qiryvphfnbnzgyemyerchhxsv usp share step import gradio import tensorflow transform import tfgptlmheadmodel gpttoken error contextualversionconflict flask anaconda env lib python site packag requir pars flask gradio tri instal flask version error idea past step studio pip instal forc reinstal flask error",
        "Question_preprocessed_content":"contextu version conflict error studio try run line ipynb file googl colab studio step error tri instal flask version error idea past step studio",
        "Question_gpt_summary_original":"The user is encountering a ContextualVersionConflict error while trying to run a 10 line .ipynb file from Google Colab in Microsoft Azure Machine Learning Studio. The error occurs when importing gradio, tensorflow, and transformers. The user attempted to install Flask 1.1.1 version but encountered more errors. The user is seeking advice on how to resolve the issue and proceed with the step in Azure ML Studio.",
        "Question_gpt_summary":"user encount contextualversionconflict error try run line ipynb file googl colab studio error occur import gradio tensorflow transform user attempt instal flask version encount error user seek advic resolv issu proce step studio",
        "Answer_original_content":"issu gradio packag exist flask packag version applic requir flask show error need uninstal exist flask packag instal latest requir version uninstal exist packag pip uninstal flask instal latest packag pip instal flask sure restart runtim pick new flask runtim restart runtim menu final import gradio",
        "Answer_preprocessed_content":"issu packag exist flask packag applic requir show error need uninstal exist flask packag instal latest requir version uninstal exist packag instal latest packag sure restart runtim pick new flask runtim restart runtim menu final import gradio",
        "Answer_gpt_summary_original":"Solution:\n- Uninstall the existing Flask package using \"!pip uninstall Flask -y\" command.\n- Install the latest required version of Flask using \"!pip install Flask>=1.1.1\" command.\n- Restart the runtime to pick up the new Flask using the Runtime -> Restart runtime menu.\n- Import gradio.",
        "Answer_gpt_summary":"solut uninstal exist flask packag pip uninstal flask command instal latest requir version flask pip instal flask command restart runtim pick new flask runtim restart runtim menu import gradio"
    },
    {
        "Question_title":"AWS Sagemaker - Blazingtext BatchTransform no output",
        "Question_body":"<p>I have trained a blazingText model and followed this guide.\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html<\/a><\/p>\n\n<p>\"Sample JSON request\" The Invoke end point is working perfectly. So I switched to,\nBatch Transform Job with \"content-type: application\/jsonlines\" and created a file in S3 with the following format data:<\/p>\n\n<pre><code>{\"source\": \"source_0\"}\n<\/code><\/pre>\n\n<p>The job ran success. But the output did not sent to S3. Also In the cloud logs,<\/p>\n\n<pre><code>\" [79] [INFO] Booting worker with pid: 79\"\n<\/code><\/pre>\n\n<p>This is is the last response. Did anyone know what went wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1543474625430,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":317.0,
        "Answer_body":"<p>I have found the issue. The batchtransform select the folder as input and the s3 source should be S3Prefix instead of manifest.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53533434",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1543477871070,
        "Question_original_content":"blazingtext batchtransform output train blazingtext model follow guid http doc aw amazon com latest blazingtext html sampl json request invok end point work perfectli switch batch transform job content type applic jsonlin creat file follow format data sourc sourc job ran success output sent cloud log info boot worker pid respons know went wrong",
        "Question_preprocessed_content":"blazingtext batchtransform output train blazingtext model follow guid sampl json request invok end point work perfectli switch batch transform job creat file follow format data job ran success output sent cloud log respons know went wrong",
        "Question_gpt_summary_original":"The user encountered a challenge with AWS Sagemaker's Blazingtext BatchTransform, where the job ran successfully but the output was not sent to S3. The cloud logs showed the last response as \"Booting worker with pid: 79\". The user is seeking assistance to identify the issue.",
        "Question_gpt_summary":"user encount challeng blazingtext batchtransform job ran successfulli output sent cloud log show respons boot worker pid user seek assist identifi issu",
        "Answer_original_content":"issu batchtransform select folder input sourc sprefix instead manifest",
        "Answer_preprocessed_content":"issu batchtransform select folder input sourc prefix instead manifest",
        "Answer_gpt_summary_original":"Solution: The issue with AWS Sagemaker's Blazingtext BatchTransform not sending output to S3 was identified as the batchtransform selecting the folder as input and the S3 source should be S3Prefix instead of manifest.",
        "Answer_gpt_summary":"solut issu blazingtext batchtransform send output identifi batchtransform select folder input sourc sprefix instead manifest"
    },
    {
        "Question_title":"How is the entry point to the code specified in AWS sagemaker bring your own code?",
        "Question_body":"<p>I'm trying out the sample notebooks in AWS Sagemaker, currently in the mxnet mnist example which demonstrates bringing your own code. The entry point parameter passed in when instantiating an estimator instance, only mentions the source file (mnist.py) and not a method name or any other point inside the source file.<\/p>\n\n<p>So how does aws sagemaker figure out which method to send the training data to? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1519726485703,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1519802441487,
        "Question_score":1.0,
        "Question_view_count":3327.0,
        "Answer_body":"<p>Your python script should implement a few methods like train, model_fn, transform_fn, input_fn etc. SagaMaker would call appropriate method when needed. <\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/mxnet-training-inference-code-template.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49006174",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1520316895088,
        "Question_original_content":"entri point code specifi bring code try sampl notebook current mxnet mnist exampl demonstr bring code entri point paramet pass instanti estim instanc mention sourc file mnist method point insid sourc file figur method send train data",
        "Question_preprocessed_content":"entri point code specifi bring code try sampl notebook current mxnet mnist exampl demonstr bring code entri point paramet pass instanti estim instanc mention sourc file method point insid sourc file figur method send train data",
        "Question_gpt_summary_original":"The user is facing a challenge in AWS Sagemaker while trying to specify the entry point to the code for the mxnet mnist example. The entry point parameter only mentions the source file and not a method name or any other point inside the source file, which makes it difficult for the user to understand how AWS Sagemaker figures out which method to send the training data to.",
        "Question_gpt_summary":"user face challeng try specifi entri point code mxnet mnist exampl entri point paramet mention sourc file method point insid sourc file make difficult user understand figur method send train data",
        "Answer_original_content":"python script implement method like train model transform input sagamak appropri method need http doc aw amazon com latest mxnet train infer code templat html",
        "Answer_preprocessed_content":"python script implement method like train sagamak appropri method need",
        "Answer_gpt_summary_original":"Solution: The user can implement methods like train, model_fn, transform_fn, input_fn, etc. in their Python script, and AWS Sagemaker will call the appropriate method when needed. The documentation provides a code template that can be used as a reference.",
        "Answer_gpt_summary":"solut user implement method like train model transform input python script appropri method need document provid code templat refer"
    },
    {
        "Question_title":"Vertex AI - how to monitor training progress?",
        "Question_body":"<h1>Question<\/h1>\n<p>Is there a way to monitor the console output of model training progress during the Vertex AI training?<\/p>\n<h2>Background<\/h2>\n<p>Suppose we have a Tensorflow\/Keras model training code:<\/p>\n<pre><code>model = keras.Sequential([\n    layers.Dense(64, activation='relu', input_shape=[len(train_dataset.keys())]),\n    layers.Dense(64, activation='relu'),\n    layers.Dense(1)\n])\n\noptimizer = tf.keras.optimizers.RMSprop(0.001)\nmodel.compile(\n    loss='mse',\n    optimizer=optimizer,\n    metrics=['mae', 'mse']\n)\n\nEPOCHS = 1000\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10)\n\nearly_history = model.fit(normed_train_data, train_labels, \n                    epochs=EPOCHS, validation_split = 0.2, \n                    callbacks=[early_stop])\n<\/code><\/pre>\n<p>When run the model training from the command line, we can see the progress in the console.<\/p>\n<pre><code>Epoch 1\/1000\nOMP: Info #211: KMP_AFFINITY: decoding x2APIC ids.\nOMP: Info #209: KMP_AFFINITY: Affinity capable, using global cpuid leaf 11 info\nOMP: Info #154: KMP_AFFINITY: Initial OS proc set respected: 0-3\nOMP: Info #156: KMP_AFFINITY: 4 available OS procs\nOMP: Info #157: KMP_AFFINITY: Uniform topology\nOMP: Info #179: KMP_AFFINITY: 1 packages x 2 cores\/pkg x 2 threads\/core (2 total cores)\nOMP: Info #213: KMP_AFFINITY: OS proc to physical thread map:\nOMP: Info #171: KMP_AFFINITY: OS proc 0 maps to package 0 core 0 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 2 maps to package 0 core 0 thread 1 \nOMP: Info #171: KMP_AFFINITY: OS proc 1 maps to package 0 core 1 thread 0 \nOMP: Info #171: KMP_AFFINITY: OS proc 3 maps to package 0 core 1 thread 1 \nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 0 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 17 thread 1 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 28 thread 2 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 29 thread 3 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 30 thread 4 bound to OS proc set 0\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 18 thread 5 bound to OS proc set 1\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 31 thread 6 bound to OS proc set 2\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 32 thread 7 bound to OS proc set 3\nOMP: Info #249: KMP_AFFINITY: pid 1 tid 33 thread 8 bound to OS proc set 0\n8\/8 [==============================] - 2s 31ms\/step - loss: 579.6393 - mae: 22.7661 - mse: 579.6393 - val_loss: 571.7239 - val_mae: 22.5494 - val_mse: 571.7239\nEpoch 2\/1000\n8\/8 [==============================] - 0s 7ms\/step - loss: 527.9056 - mae: 21.6268 - mse: 527.9056 - val_loss: 520.5531 - val_mae: 21.3917 - val_mse: 520.5531\n...\n<\/code><\/pre>\n<p>However, if we run the training in the Vertex AI training, there looks to be no menu\/option to see the console output. Not sure if it is logged in Log Explorer. Please help understand how to monitor the training progress realtime.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tbaBD.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1651193148017,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":339.0,
        "Answer_body":"<p>You may view training logs in the <strong>GCP Logs Explorer<\/strong> by using below query.<\/p>\n<pre><code>resource.type=&quot;ml_job&quot;\nresource.labels.job_id=&quot;your-training-custom-job-ID&quot;\n<\/code><\/pre>\n<p>The <em><strong>your-training-custom-job-ID<\/strong><\/em> can be found on the ongoing Vertex AI Training in GCP console as seen on the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Ks4Yf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Below is the screenshot of the logs for the Vertex AI training in GCP logs explorer using the above query.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/waHu6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/waHu6.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>You may click on <strong>Jump to now<\/strong> to immediately view the latest logs. Also, you may use <strong>Stream Logs<\/strong> option to view <strong>REAL TIME<\/strong> log data which you can also adjust the buffer window in which has certain trade offs. You may refer to this <a href=\"https:\/\/cloud.google.com\/logging\/docs\/view\/streaming-live-tailing\" rel=\"nofollow noreferrer\">documentation<\/a> for more information on streaming logs in GCP logs explorer.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1651563481836,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72051655",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1651562908648,
        "Question_original_content":"monitor train progress question wai monitor consol output model train progress train background suppos tensorflow kera model train code model kera sequenti layer dens activ relu input shape len train dataset kei layer dens activ relu layer dens optim kera optim rmsprop model compil loss mse optim optim metric mae mse epoch earli stop kera callback earlystop monitor val loss patienc earli histori model fit norm train data train label epoch epoch valid split callback earli stop run model train command line progress consol epoch omp info kmp affin decod xapic id omp info kmp affin affin capabl global cpuid leaf info omp info kmp affin initi proc set respect omp info kmp affin avail proc omp info kmp affin uniform topolog omp info kmp affin packag core pkg thread core total core omp info kmp affin proc physic thread map omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin proc map packag core thread omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set omp info kmp affin pid tid thread bound proc set step loss mae mse val loss val mae val mse epoch step loss mae mse val loss val mae val mse run train train look menu option consol output sure log log explor help understand monitor train progress realtim",
        "Question_preprocessed_content":"monitor train progress question wai monitor consol output model train progress train background suppos model train code run model train command line progress consol run train train look consol output sure log log explor help understand monitor train progress realtim",
        "Question_gpt_summary_original":"The user is facing a challenge in monitoring the console output of model training progress during the Vertex AI training. They are unsure if the output is logged in Log Explorer and are seeking help to understand how to monitor the training progress in real-time.",
        "Question_gpt_summary":"user face challeng monitor consol output model train progress train unsur output log log explor seek help understand monitor train progress real time",
        "Answer_original_content":"view train log gcp log explor queri resourc type job resourc label job train custom job train custom job ongo train gcp consol seen screenshot screenshot log train gcp log explor queri click jump immedi view latest log us stream log option view real time log data adjust buffer window certain trade off refer document inform stream log gcp log explor",
        "Answer_preprocessed_content":"view train log gcp log explor queri ongo train gcp consol seen screenshot screenshot log train gcp log explor queri click jump immedi view latest log us stream log option view real time log data adjust buffer window certain trade off refer document inform stream log gcp log explor",
        "Answer_gpt_summary_original":"Solution: The user can monitor the console output of model training progress during the Vertex AI training by viewing the training logs in the GCP Logs Explorer using a specific query. The query is \"resource.type=ml_job resource.labels.job_id=your-training-custom-job-ID\" where \"your-training-custom-job-ID\" can be found on the ongoing Vertex AI Training in GCP console. The user can click on \"Jump to now\" to immediately view the latest logs or use the \"Stream Logs\" option to view real-time log data. The user can adjust the buffer window, but it has certain trade-offs. The documentation on streaming logs in GCP logs explorer can be referred to for more information.",
        "Answer_gpt_summary":"solut user monitor consol output model train progress train view train log gcp log explor specif queri queri resourc type job resourc label job train custom job train custom job ongo train gcp consol user click jump immedi view latest log us stream log option view real time log data user adjust buffer window certain trade off document stream log gcp log explor refer inform"
    },
    {
        "Question_title":"How to get status of Azure machine learning service pipeline run using Rest Api?",
        "Question_body":"<p>I have created an Azure Machine Learning Service Pipeline which i am invoking externally using its rest endpoint.\nBut i also need to monitor its run , whether it got completed or failed, periodically.\n<strong>Is there a methodinside a machine learning pipeline's rest endpoint, which i can hit to check its run status?<\/strong>\nI have tried the steps mentioned in the link here \n<a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/machine-learning-pipelines\/pipeline-batch-scoring\/pipeline-batch-scoring.ipynb<\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1569502183603,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1569502283203,
        "Question_score":1.0,
        "Question_view_count":898.0,
        "Answer_body":"<p>For getting status of run, you can use REST APIs described here <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/tree\/master\/specification\/machinelearningservices\/data-plane<\/a> <\/p>\n\n<p>Specifically you need <a href=\"https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Azure\/azure-rest-api-specs\/blob\/master\/specification\/machinelearningservices\/data-plane\/Microsoft.MachineLearningServices\/preview\/2019-08-01\/runHistory.json<\/a><\/p>\n\n<p>use this call to get run information including status:<\/p>\n\n<blockquote>\n  <p>\/history\/v1.0\/subscriptions\/{subscriptionId}\/resourceGroups\/{resourceGroupName}\/providers\/Microsoft.MachineLearningServices\/workspaces\/{workspaceName}\/experiments\/{experimentName}\/runs\/{runId}\/details<\/p>\n<\/blockquote>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58117200",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1569516992870,
        "Question_original_content":"statu servic pipelin run rest api creat servic pipelin invok extern rest endpoint need monitor run got complet fail period methodinsid machin learn pipelin rest endpoint hit check run statu tri step mention link http github com azur machinelearningnotebook blob master us machin learn pipelin pipelin batch score pipelin batch score ipynb",
        "Question_preprocessed_content":"statu servic pipelin run rest api creat servic pipelin invok extern rest endpoint need monitor run got complet fail period methodinsid machin learn pipelin rest endpoint hit check run statu tri step mention link",
        "Question_gpt_summary_original":"The user is facing a challenge in monitoring the status of an Azure Machine Learning Service Pipeline run using its Rest API. They are seeking a method to check the run status periodically and have tried the steps mentioned in a provided link without success.",
        "Question_gpt_summary":"user face challeng monitor statu servic pipelin run rest api seek method check run statu period tri step mention provid link success",
        "Answer_original_content":"get statu run us rest api describ http github com azur azur rest api spec tree master specif machinelearningservic data plane specif need http github com azur azur rest api spec blob master specif machinelearningservic data plane microsoft machinelearningservic preview runhistori json us run inform includ statu histori subscript subscriptionid resourcegroup resourcegroupnam provid microsoft machinelearningservic workspac workspacenam experi experimentnam run runid detail",
        "Answer_preprocessed_content":"get statu run us rest api describ specif need us run inform includ statu",
        "Answer_gpt_summary_original":"Solution: The solution suggested is to use the REST APIs provided by Azure Machine Learning Service to get the status of the run. The user can use the specific call mentioned in the discussion to get the run information, including its status.",
        "Answer_gpt_summary":"solut solut suggest us rest api provid servic statu run user us specif mention discuss run inform includ statu"
    },
    {
        "Question_title":"SageMaker Studio Enterprise Deployment guidelines",
        "Question_body":"Do we have guidelines on requirements gathering\/designing the provisioning of SageMaker Studio domains across large global enterprises with many business units?\n\nI've seen discussions where topics like number of users\/domain, org\/team structure, collaboration patterns, resource needs, classes of ML problems, framework\/library usage, security and others were raised when defining requirements and boundaries. Customer is starting their first Studio deployment and they are asking for guidance on how to scope and design that so that they can have a scalable process.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1612447142000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Answer_body":"You should guide your customer based on the general principles of multi account best practices that we provide for other services.\n\nHere are some high level boundaries.\n\nOne studio domain per account and region. No cross region AWS SSO configuration provided.\n\nMaximum numbers of users allowed in studio vary between 60 - 200 users. Although AWS SSO can support many more users, there are some considerations around other dependencies such as EFS among others.\n\nIf you need to isolate any model artifacts produced by SageMaker, you may want to have them use a separate account. Even if you use tag based access control, you can still technically list those artifacts.\n\nSageMaker feature store should follow the data lake pattern closely. As a general rule, you want to write in one account and can read from many other accounts perhaps using Lake formation to expose datasets into other accounts. Teams can create their own offline \/ online feature store for non production use cases.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU0QiBS-GdSIKZdXxSf1NUYA\/sage-maker-studio-enterprise-deployment-guidelines",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1614090886000,
        "Question_original_content":"studio enterpris deploy guidelin guidelin requir gather design provis studio domain larg global enterpris busi unit seen discuss topic like number user domain org team structur collabor pattern resourc need class problem framework librari usag secur rais defin requir boundari custom start studio deploy ask guidanc scope design scalabl process",
        "Question_preprocessed_content":"studio enterpris deploy guidelin guidelin requir provis studio domain larg global enterpris busi unit seen discuss topic like number structur collabor pattern resourc need class problem usag secur rais defin requir boundari custom start studio deploy ask guidanc scope design scalabl process",
        "Question_gpt_summary_original":"The user is seeking guidelines on how to provision SageMaker Studio domains across large global enterprises with many business units. They are facing challenges in scoping and designing the deployment process to ensure scalability. Factors such as number of users\/domain, org\/team structure, collaboration patterns, resource needs, classes of ML problems, framework\/library usage, and security need to be considered when defining requirements and boundaries.",
        "Question_gpt_summary":"user seek guidelin provis studio domain larg global enterpris busi unit face challeng scope design deploy process ensur scalabl factor number user domain org team structur collabor pattern resourc need class problem framework librari usag secur need consid defin requir boundari",
        "Answer_original_content":"guid custom base gener principl multi account best practic provid servic high level boundari studio domain account region cross region aw sso configur provid maximum number user allow studio vari user aw sso support user consider depend ef need isol model artifact produc want us separ account us tag base access control technic list artifact featur store follow data lake pattern close gener rule want write account read account lake format expos dataset account team creat offlin onlin featur store non product us case",
        "Answer_preprocessed_content":"guid custom base gener principl multi account best practic provid servic high level boundari studio domain account region cross region aw sso configur provid maximum number user allow studio vari user aw sso support user consider depend ef need isol model artifact produc want us separ account us tag base access control technic list artifact featur store follow data lake pattern close gener rule want write account read account lake format expos dataset account team creat offlin onlin featur store non product us case",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include:\n\n- Provisioning one SageMaker Studio domain per account and region.\n- Limiting the maximum number of users allowed in a studio domain to between 60-200 users.\n- Considering isolating model artifacts produced by SageMaker in a separate account.\n- Following the data lake pattern closely when using SageMaker feature store.\n- Allowing teams to create their own offline\/online feature store for non-production use cases.\n\nNo personal opinions or biases were included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss includ provis studio domain account region limit maximum number user allow studio domain user consid isol model artifact produc separ account follow data lake pattern close featur store allow team creat offlin onlin featur store non product us case person opinion bias includ summari"
    },
    {
        "Question_title":"Create a model that predicts an event based on other time series events and properties of an object",
        "Question_body":"<p>I have the following data:<\/p>\n\n<ul>\n<li>Identifier of a person<\/li>\n<li>Days in location (starts at 1 and runs until event)<\/li>\n<li>Age of person in months at that time (so this increases as the days in location increase too).<\/li>\n<li>Smoker (boolean), doesn't change over time in our case<\/li>\n<li>Sex, doesn't change over time<\/li>\n<li>Fall (boolean) this is an event that may never happen, or can happen multiple times during the complete period for a certain person<\/li>\n<li>Number of wounds: (this can go from 0 to 8), a wound mostly doesn't heal immediately so it mostly stays open for a certain period of time<\/li>\n<li>Event we want to predict (boolean), only the last row of a person will have value true for this<\/li>\n<\/ul>\n\n<p>I have this data for 1500 people (in total 1500000 records so on average about 1000 records per person). For some people the event I want to predict takes place after a couple of days, for some after 10 years.  For everybody in the dataset the event will take place, so the last record for a certain identifier will always have the event we want to predict as 1.<\/p>\n\n<p>I'm new to this and all the documentation I have found so far doesn't demonstrate time series for multiple persons or objects. When I for example split the data in the machine learning studio, I want to keep records of the same person over time together.<\/p>\n\n<p>Would it be possible to feed the system after the model is trained with new records and for each day that passes it would give the estimate of the event taking place in the next 5 days?<\/p>\n\n<p>Edit: sample data of 2 persons: <a href=\"http:\/\/pastebin.com\/KU4bjKwJ\" rel=\"nofollow\">http:\/\/pastebin.com\/KU4bjKwJ<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1477382328947,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1477400304008,
        "Question_score":0.0,
        "Question_view_count":596.0,
        "Answer_body":"<p>sounds like very similar to this sample:<\/p>\n\n<p><a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/df7c518dcba7407fb855377339d6589f\" rel=\"nofollow\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/df7c518dcba7407fb855377339d6589f<\/a><\/p>\n\n<p>Unfortunately there is going to be a bit of R code involved. Yes you should be able to retrain the model with new data.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40234432",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1477404012396,
        "Question_original_content":"creat model predict event base time seri event properti object follow data identifi person dai locat start run event ag person month time increas dai locat increas smoker boolean chang time case sex chang time fall boolean event happen happen multipl time complet period certain person number wound wound heal immedi stai open certain period time event want predict boolean row person valu true data peopl total record averag record person peopl event want predict take place coupl dai year everybodi dataset event place record certain identifi event want predict new document far demonstr time seri multipl person object exampl split data machin learn studio want record person time possibl feed model train new record dai pass estim event take place dai edit sampl data person http pastebin com kubjkwj",
        "Question_preprocessed_content":"creat model predict event base time seri event properti object follow data identifi person dai locat ag person month time smoker chang time case sex chang time fall event happen happen multipl time complet period certain person number wound wound heal immedi stai open certain period time event want predict row person valu true data peopl peopl event want predict take place coupl dai year everybodi dataset event place record certain identifi event want predict new document far demonstr time seri multipl person object exampl split data machin learn studio want record person time possibl feed model train new record dai pass estim event take place dai edit sampl data person",
        "Question_gpt_summary_original":"The user has encountered challenges in creating a time series model that predicts an event based on other time series events and properties of an object. The user has data for 1500 people, with each person having an average of 1000 records. The event to be predicted occurs at different times for different people. The user is struggling to find documentation on time series for multiple persons or objects and is unsure how to keep records of the same person over time together when splitting the data in the machine learning studio. The user also wants to know if it is possible to feed the system with new records and get an estimate of the event taking place in the next 5 days.",
        "Question_gpt_summary":"user encount challeng creat time seri model predict event base time seri event properti object user data peopl person have averag record event predict occur differ time differ peopl user struggl document time seri multipl person object unsur record person time split data machin learn studio user want know possibl feed new record estim event take place dai",
        "Answer_original_content":"sound like similar sampl http galleri cortanaintellig com experi dfcdcbafbdf unfortun go bit code involv ye abl retrain model new data",
        "Answer_preprocessed_content":"sound like similar sampl unfortun go bit code involv ye abl retrain model new data",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to a sample experiment that is similar to the user's challenge. The solution involves using R code and retraining the model with new data. The discussion also confirms that it is possible to feed the system with new records and get an estimate of the event taking place in the next 5 days.",
        "Answer_gpt_summary":"solut discuss provid link sampl experi similar user challeng solut involv code retrain model new data discuss confirm possibl feed new record estim event take place dai"
    },
    {
        "Question_title":"Non-interactive login to registered dataset",
        "Question_body":"I'm trying to tune hyperparameters similar to the following guide: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-tune-hyperparameters\n\nMy PyTorch Dataset in train.py contains:\n\n ws = Workspace.from_config()\n ds = Dataset.get_by_name(ws, 'train')\n df = ds.to_pandas_dataframe()\n\nThis code works fine when run from the command-line, but when I submit a hyperparam tuning job to each node of a training cluster, I get the following error:\n\nWe could not find config.json in: \/mnt\/batch\/tasks\/shared\/LS_root\/jobs\/adamml\/azureml\/hd_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3\/mounts\/workspaceblobstore\/azureml\/HD_ba15bb39-f0fe-47a7-afbc-d2f9968e9687_3 or in its parent directories. Please provide the full path to the config file or ensure that config.json exists in the parent directories.\n\nIf I manually pass my subscription id, resource group, and workspace id, I don't get this error, but now every single hyperparam tuning experiment requires me to log in through the web portal. Is there a way to do a non-interactive login?",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_created_time":1595346764937,
        "Question_favorite_count":36.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"If I read the post correctly, you were trying to get an registered dataset within a submitted run. There, Workspace.from_config() won't work since there is no config.json file as the error suggested.\n\nAnd when you created an auth object which is InteractiveLoginAuth, it is expected to perform interactive login.\n\nWithin a run the recommended way to connect to current workspace it via:\n\nfrom azureml.core import Run\nrun = Run.get_context().experiment.workspace\n\n\n\nMeanwhile, there is way to pass in an dataset object to a run without involving register and workspace signin. If that fit your scenario better, please refer to the example in this document https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-datasets#access-and-explore-input-datasets\n\nfrom azureml.core import Dataset, Run\nrun = Run.get_context()\n# get the input dataset by name\ndataset = run.input_datasets['titanic']\n# load the TabularDataset to pandas DataFrame\ndf = dataset.to_pandas_dataframe()",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/50386\/non-interactive-login-to-registered-dataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1595534742240,
        "Question_original_content":"non interact login regist dataset try tune hyperparamet similar follow guid http doc microsoft com azur machin learn tune hyperparamet pytorch dataset train contain workspac config dataset train panda datafram code work fine run command line submit hyperparam tune job node train cluster follow error config json mnt batch task share root job adamml babb ffe afbc dfe mount workspaceblobstor babb ffe afbc dfe parent directori provid path config file ensur config json exist parent directori manual pass subscript resourc group workspac error singl hyperparam tune experi requir log web portal wai non interact login",
        "Question_preprocessed_content":"login regist dataset try tune hyperparamet similar follow guid pytorch dataset contain train code work fine run submit hyperparam tune job node train cluster follow error parent directori provid path config file ensur exist parent directori manual pass subscript resourc group workspac error singl hyperparam tune experi requir log web portal wai login",
        "Question_gpt_summary_original":"The user is encountering an error when submitting a hyperparameter tuning job to a training cluster due to the inability to find config.json in the specified directory. Manually passing subscription id, resource group, and workspace id resolves the issue but requires logging in through the web portal for every hyperparameter tuning experiment. The user is seeking a solution for non-interactive login.",
        "Question_gpt_summary":"user encount error submit hyperparamet tune job train cluster inabl config json specifi directori manual pass subscript resourc group workspac resolv issu requir log web portal hyperparamet tune experi user seek solut non interact login",
        "Answer_original_content":"read post correctli try regist dataset submit run workspac config won work config json file error suggest creat auth object interactiveloginauth expect perform interact login run recommend wai connect current workspac core import run run run context experi workspac wai pass dataset object run involv regist workspac signin fit scenario better refer exampl document http doc microsoft com azur machin learn train dataset access explor input dataset core import dataset run run run context input dataset dataset run input dataset titan load tabulardataset panda datafram dataset panda datafram",
        "Answer_preprocessed_content":"read post correctli try regist dataset submit run won work file error suggest creat auth object interactiveloginauth expect perform interact login run recommend wai connect current workspac core import run run wai pass dataset object run involv regist workspac signin fit scenario better refer exampl document core import dataset run run input dataset dataset load tabulardataset panda datafram",
        "Answer_gpt_summary_original":"Solution: The recommended way to connect to the current workspace within a run is via \"from azureml.core import Run; run = Run.get_context().experiment.workspace\". Another solution is to pass in a dataset object to a run without involving register and workspace signin. This can be done by using \"from azureml.core import Dataset, Run; run = Run.get_context(); dataset = run.input_datasets['titanic']; df = dataset.to_pandas_dataframe()\".",
        "Answer_gpt_summary":"solut recommend wai connect current workspac run core import run run run context experi workspac solut pass dataset object run involv regist workspac signin core import dataset run run run context dataset run input dataset titan dataset panda datafram"
    },
    {
        "Question_title":"SageMaker TensorFlow serving stack comparisons",
        "Question_body":"<p>SageMaker seems to give examples of using two different serving stacks for serving custom docker images:<\/p>\n\n<ol>\n<li>NGINX + Gunicorn + Flask<\/li>\n<li>NGINX + TensorFlow Serving<\/li>\n<\/ol>\n\n<p>Could someone explain to me at a very high level (I have very little knowledge of network engineering) what responsibilities these different components have? And since the second stack has only two components instead of one, can I rightly assume that TensorFlow Serving does the job (whatever that may be) of both Gunicorn and Flask? <\/p>\n\n<p>Lastly, I've read that it's possible to use Flask and TensorFlow serving at the same time. Would this then be NGINX -> Gunicorn -> Flask -> TensorFlow Serving? And what are there advantages of this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1547670814743,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1547802223467,
        "Question_score":2.0,
        "Question_view_count":741.0,
        "Answer_body":"<p>I'll try to answer your question on a high level. Disclaimer: I'm not at an expert across the full stack of what you describe, and I would welcome corrections or additions from people who are. <\/p>\n\n<p>I'll go over the different components from bottom to top:<\/p>\n\n<p><strong>TensorFlow Serving<\/strong> is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions. The idea is to train models with TensorFlow, export them to the SavedModel format and serve them with TF Serving. You can set up a TF Server to accept requests via HTTP and\/or RPC. One advantage of RPC is that the request message is compressed, which can be useful when sending large payloads, for instance with image data.<\/p>\n\n<p><strong>Flask<\/strong> is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures. <\/p>\n\n<p>Now, the combination of Flask and TensorFlow serving should make sense. You could write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood. The user uses the API to transmit some data (<strong>1<\/strong>), the Flask app perhaps transform the data (for example, wrap it in numpy arrays), calls the TF Server to get a model prediction (<strong>2<\/strong>)(<strong>3<\/strong>), perhaps transforms the prediction (for example convert a predicted probability that is larger than 0.5 to a class label of 1), and returns the prediction to the user (<strong>4<\/strong>). You could visualize this as follows:<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/67EXW.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/67EXW.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Gunicorn<\/strong> is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems. As the name says, it's the interface between a web server and a web application. When you are developing a Flask app, you can run it locally to test it. In production, gunicorn will run the app for you.<\/p>\n\n<p>TF Serving will host your model as a functional application. Therefore, you do not need gunicorn to run the TF Server application for you. <\/p>\n\n<p><strong>Nginx<\/strong> is the actual web server, which will host your application, handle requests from the outside and pass them to the application server (gunicorn). Nginx cannot talk directly to Flask applications, which is why gunicorn is there. <\/p>\n\n<p><a href=\"https:\/\/serverfault.com\/questions\/331256\/why-do-i-need-nginx-and-something-like-gunicorn\">This answer<\/a> might be helpful as well. <\/p>\n\n<p>Finally, if you are working on a cloud platform, the web server part will probably be handled for you, so you will either need to write the Flask app and host it with gunicorn, or setup the TF Serving server. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1547816279183,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54224934",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1547801639500,
        "Question_original_content":"tensorflow serv stack comparison exampl differ serv stack serv custom docker imag nginx gunicorn flask nginx tensorflow serv explain high level littl knowledg network engin respons differ compon second stack compon instead rightli assum tensorflow serv job gunicorn flask lastli read possibl us flask tensorflow serv time nginx gunicorn flask tensorflow serv advantag",
        "Question_preprocessed_content":"tensorflow serv stack comparison exampl differ serv stack serv custom docker imag nginx gunicorn flask nginx tensorflow serv explain high level respons differ compon second stack compon instead rightli assum tensorflow serv job gunicorn flask lastli read possibl us flask tensorflow serv time nginx gunicorn flask tensorflow serv advantag",
        "Question_gpt_summary_original":"The user is seeking an explanation of the differences between two serving stacks for custom docker images in SageMaker: NGINX + Gunicorn + Flask and NGINX + TensorFlow Serving. They are also asking if TensorFlow Serving does the job of both Gunicorn and Flask, and if it's possible to use Flask and TensorFlow serving at the same time. The user is seeking to understand the advantages of using both components together.",
        "Question_gpt_summary":"user seek explan differ serv stack custom docker imag nginx gunicorn flask nginx tensorflow serv ask tensorflow serv job gunicorn flask possibl us flask tensorflow serv time user seek understand advantag compon",
        "Answer_original_content":"try answer question high level disclaim expert stack welcom correct addit peopl differ compon tensorflow serv librari deploi host tensorflow model model server accept request input data return model predict idea train model tensorflow export savedmodel format serv serv set server accept request http rpc advantag rpc request messag compress us send larg payload instanc imag data flask python framework write web applic gener purpos serv wide build web servic instanc microservic architectur combin flask tensorflow serv sens write flask web applic expos api user call model host serv hood user us api transmit data flask app transform data exampl wrap numpi arrai call server model predict transform predict exampl convert predict probabl larger class label return predict user visual follow gunicorn web server gatewai interfac wsgi commonli host flask applic product system sai interfac web server web applic develop flask app run local test product gunicorn run app serv host model function applic need gunicorn run server applic nginx actual web server host applic handl request outsid pass applic server gunicorn nginx talk directli flask applic gunicorn answer help final work cloud platform web server probabl handl need write flask app host gunicorn setup serv server",
        "Answer_preprocessed_content":"try answer question high level disclaim expert stack welcom correct addit peopl differ compon tensorflow serv librari deploi host tensorflow model model server accept request input data return model predict idea train model tensorflow export savedmodel format serv serv set server accept request http rpc advantag rpc request messag compress us send larg payload instanc imag data flask python framework write web applic serv wide build web servic instanc microservic architectur combin flask tensorflow serv sens write flask web applic expos api user call model host serv hood user us api transmit data flask app transform data call server model predict transform predict return predict user visual follow gunicorn web server gatewai interfac commonli host flask applic product system sai interfac web server web applic develop flask app run local test product gunicorn run app serv host model function applic need gunicorn run server applic nginx actual web server host applic handl request outsid pass applic server nginx talk directli flask applic gunicorn answer help final work cloud platform web server probabl handl need write flask app host gunicorn setup serv server",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- TensorFlow Serving is a library for deploying and hosting TensorFlow models as model servers that accept requests with input data and return model predictions.\n- Flask is a python framework for writing web applications. It's much more general-purpose than TF Serving and is widely used to build web services, for instance in microservice architectures.\n- Flask and TensorFlow serving can be used together to write a Flask web application that exposes an API to the user and calls a TF model hosted with TF Serving under the hood.\n- Gunicorn is a Web Server Gateway Interface (WSGI) that is commonly used to host Flask applications in production systems.\n- Nginx is the actual web server, which will host your application, handle requests from the",
        "Answer_gpt_summary":"possibl solut mention discuss tensorflow serv librari deploi host tensorflow model model server accept request input data return model predict flask python framework write web applic gener purpos serv wide build web servic instanc microservic architectur flask tensorflow serv write flask web applic expos api user call model host serv hood gunicorn web server gatewai interfac wsgi commonli host flask applic product system nginx actual web server host applic handl request"
    },
    {
        "Question_title":"How to create Parallel Coordinates plot without sweeps",
        "Question_body":"<p>It would be great to plot hparams without doing sweeps, most of the time I\u2019m doing experiments and I would love the plot to be across runs and not as a sweep. It might be complex to make this feature automated, but I\u2019m fine if it\u2019s within one run, would be great to have something like <code>wandb.plots.ParallelCoordinates<\/code><\/p>",
        "Question_answer_count":6,
        "Question_comment_count":0,
        "Question_created_time":1671394517195,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":146.0,
        "Answer_body":"<p>Hi Faris!<\/p>\n<p>You absolutely can use Parallel coordinates plots without sweeps. The web UI has an option to add additional plots on the top right of the graph section which contains the Parallel Coordinates Plot.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-create-parallel-coordinates-plot-without-sweeps\/3566",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1671425374288,
        "Question_original_content":"creat parallel coordin plot sweep great plot hparam sweep time experi love plot run sweep complex featur autom fine run great like plot parallelcoordin",
        "Question_preprocessed_content":"creat parallel coordin plot sweep great plot hparam sweep time experi love plot run sweep complex featur autom fine run great like",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a Parallel Coordinates plot without doing sweeps. They would like to plot hparams across runs and not as a sweep, but find it complex to make this feature automated. They suggest having something like \"wandb.plots.ParallelCoordinates\" within one run.",
        "Question_gpt_summary":"user face challeng creat parallel coordin plot sweep like plot hparam run sweep complex featur autom suggest have like plot parallelcoordin run",
        "Answer_original_content":"fari absolut us parallel coordin plot sweep web option add addit plot right graph section contain parallel coordin plot thank ramit",
        "Answer_preprocessed_content":"fari absolut us parallel coordin plot sweep web option add addit plot right graph section contain parallel coordin plot thank ramit",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to use the web UI option to add additional plots on the top right of the graph section which contains the Parallel Coordinates Plot. This allows the user to plot hparams across runs without doing sweeps.",
        "Answer_gpt_summary":"solut solut provid discuss us web option add addit plot right graph section contain parallel coordin plot allow user plot hparam run sweep"
    },
    {
        "Question_title":"How to log a confusion matrix to azureml platform using python",
        "Question_body":"<p>Hello Stackoverflowers,<\/p>\n\n<p>I'm using azureml and I'm wondering if it is possible to log a confusion matrix of the xgboost model I'm training, together with the other metrics I'm already logging. Here's a sample of the code I'm using:<\/p>\n\n<pre><code>from azureml.core.model import Model\nfrom azureml.core import Workspace\nfrom azureml.core.experiment import Experiment\nfrom azureml.core.authentication import ServicePrincipalAuthentication\nimport json\n\nwith open('.\/azureml.config', 'r') as f:\n    config = json.load(f)\n\nsvc_pr = ServicePrincipalAuthentication(\n   tenant_id=config['tenant_id'],\n   service_principal_id=config['svc_pr_id'],\n   service_principal_password=config['svc_pr_password'])\n\n\nws = Workspace(workspace_name=config['workspace_name'],\n                        subscription_id=config['subscription_id'],\n                        resource_group=config['resource_group'],\n                        auth=svc_pr)\n\ny_pred = model.predict(dtest)\n\nacc = metrics.accuracy_score(y_test, (y_pred&gt;.5).astype(int))\nrun.log(\"accuracy\",  acc)\nf1 = metrics.f1_score(y_test, (y_pred&gt;.5).astype(int), average='binary')\nrun.log(\"f1 score\",  f1)\n\n\ncmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\nrun.log_confusion_matrix('Confusion matrix', cmtx)\n<\/code><\/pre>\n\n<p>The above code raises this kind of error:<\/p>\n\n<pre><code>TypeError: Object of type ndarray is not JSON serializable\n<\/code><\/pre>\n\n<p>I already tried to transform the matrix in a simpler one, but another error occurred as before I logged a \"manual\" version of it (<code>cmtx = [[30000, 50],[40, 2000]]<\/code>).<\/p>\n\n<pre><code>run.log_confusion_matrix('Confusion matrix', [list([int(y) for y in x]) for x in cmtx])\n\nAzureMLException: AzureMLException:\n    Message: UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-    c5103b205379\/Confusion matrix already exists.\n    InnerException None\n    ErrorResponse \n{\n    \"error\": {\n        \"message\": \"UserError: Resource Conflict: ArtifactId ExperimentRun\/dcid.3196bf92-4952-4850-9a8a-c5103b205379\/Confusion matrix already exists.\"\n    }\n}\n<\/code><\/pre>\n\n<p>This makes me think that I'm not properly handling the command <code>run.log_confusion_matrix()<\/code>. So, again, which is the best way I can log a confusion matrix to my azureml experiments?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1591960207257,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":1418.0,
        "Answer_body":"<p>I eventually found a solution thanks to colleague of mine. I'm hence answering myself, in order to close the question and, maybe, help somebody else.<\/p>\n<p>You can find the proper function in this link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----\" rel=\"noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#log-confusion-matrix-name--value--description----<\/a>.<\/p>\n<p>Anyway, you also have to consider that, apparently, Azure doesn't work with the standard confusion matrix format returned by sklearn. It accepts indeed ONLY list of list, instead of numpy array, populated with numpy.int64 elements. So you also have to transform the matrix in a simpler format (for the sake of simplicity I used the nested list comprehension in the command below:<\/p>\n<pre><code>cmtx = metrics.confusion_matrix(y_test,(y_pred&gt;.5).astype(int))\ncmtx = {\n\n&quot;schema_type&quot;: &quot;confusion_matrix&quot;,\n&quot;parameters&quot;: params,\n &quot;data&quot;: {&quot;class_labels&quot;: [&quot;0&quot;, &quot;1&quot;],\n          &quot;matrix&quot;: [[int(y) for y in x] for x in cmtx]}\n}\nrun.log_confusion_matrix('Confusion matrix - error rate', cmtx)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62343056",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1593519812260,
        "Question_original_content":"log confus matrix platform python hello stackoverflow wonder possibl log confus matrix xgboost model train metric log sampl code core model import model core import workspac core experi import experi core authent import serviceprincipalauthent import json open config config json load svc serviceprincipalauthent tenant config tenant servic princip config svc servic princip password config svc password workspac workspac config workspac subscript config subscript resourc group config resourc group auth svc pred model predict dtest acc metric accuraci score test pred astyp int run log accuraci acc metric score test pred astyp int averag binari run log score cmtx metric confus matrix test pred astyp int run log confus matrix confus matrix cmtx code rais kind error typeerror object type ndarrai json serializ tri transform matrix simpler error occur log manual version cmtx run log confus matrix confus matrix list int cmtx except except messag usererror resourc conflict artifactid experimentrun dcid confus matrix exist innerexcept errorrespons error messag usererror resourc conflict artifactid experimentrun dcid confus matrix exist make think properli handl command run log confus matrix best wai log confus matrix experi",
        "Question_preprocessed_content":"log confus matrix platform python hello stackoverflow wonder possibl log confus matrix xgboost model train metric log sampl code code rais kind error tri transform matrix simpler error occur log manual version make think properli handl command best wai log confus matrix experi",
        "Question_gpt_summary_original":"The user is trying to log a confusion matrix of an xgboost model they are training on the AzureML platform using Python. However, they are encountering errors when trying to log the matrix using the <code>run.log_confusion_matrix()<\/code> command. The errors include a TypeError and a Resource Conflict error. The user is seeking advice on the best way to log a confusion matrix to their AzureML experiments.",
        "Question_gpt_summary":"user try log confus matrix xgboost model train platform python encount error try log matrix run log confus matrix command error includ typeerror resourc conflict error user seek advic best wai log confus matrix experi",
        "Answer_original_content":"eventu solut thank colleagu answer order close question mayb help somebodi proper function link http doc microsoft com python api core core run run view azur log confus matrix valu descript consid appar azur work standard confus matrix format return sklearn accept list list instead numpi arrai popul numpi int element transform matrix simpler format sake simplic nest list comprehens command cmtx metric confus matrix test pred astyp int cmtx schema type confus matrix paramet param data class label matrix int cmtx run log confus matrix confus matrix error rate cmtx",
        "Answer_preprocessed_content":"eventu solut thank colleagu answer order close question mayb help somebodi proper function link consid appar azur work standard confus matrix format return sklearn accept list list instead numpi arrai popul element transform matrix simpler format sake simplic nest list comprehens command",
        "Answer_gpt_summary_original":"Solution:\nThe user found a solution to their problem by using the <code>run.log_confusion_matrix()<\/code> function provided in the AzureML Python SDK. However, they also discovered that AzureML only accepts confusion matrices in a specific format, which is a list of lists populated with numpy.int64 elements. Therefore, the user transformed their confusion matrix into this format using nested list comprehension before logging it to their AzureML experiment.",
        "Answer_gpt_summary":"solut user solut problem run log confus matrix function provid python sdk discov accept confus matric specif format list list popul numpi int element user transform confus matrix format nest list comprehens log experi"
    },
    {
        "Question_title":"Do Amazon SageMaker manifest files enable dataset versioning?",
        "Question_body":"Some Amazon SageMaker algorithms can train with a manifest JSON file that stores the mapping between images and their Amazon S3 ARNs and metadata, such as labels. This is a great option, because the manifest file is much smaller than the dataset itself. Because the manifest files are small, they can be used easily in versioning tools or saved as part of the model artifact. This appears to be the best construct enabling exact dataset versioning within SageMaker. i.e., if we exclude the creation of a unique training set hard copy per training job that can't be scaled to large datasets. Is my understanding accurate?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1607681930000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":65.0,
        "Answer_body":"If you create the conditions for immutability of the assets the manifest points to, then manifest enables exact dataset versioning with SageMaker. You can have a data store in Amazon S3 with all versions of the data assets and use the manifest files for creating and versioning datasets for specific usage.\n\nIf you don't guarantee immutability for the assets that the manifest points to, then your manifest becomes invalid.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq44kZCYWTiOnwXblHSQSTA\/do-amazon-sage-maker-manifest-files-enable-dataset-versioning",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1607684202000,
        "Question_original_content":"manifest file enabl dataset version algorithm train manifest json file store map imag amazon arn metadata label great option manifest file smaller dataset manifest file small easili version tool save model artifact appear best construct enabl exact dataset version exclud creation uniqu train set hard copi train job scale larg dataset understand accur",
        "Question_preprocessed_content":"manifest file enabl dataset version algorithm train manifest json file store map imag amazon arn metadata label great option manifest file smaller dataset manifest file small easili version tool save model artifact appear best construct enabl exact dataset version exclud creation uniqu train set hard copi train job scale larg dataset understand accur",
        "Question_gpt_summary_original":"The user is trying to determine if Amazon SageMaker manifest files enable dataset versioning. They have found that manifest files are a good option because they are smaller than the dataset itself and can be easily used in versioning tools or saved as part of the model artifact. The user is questioning if this is the best way to enable exact dataset versioning within SageMaker, as creating a unique training set hard copy per training job is not scalable for large datasets.",
        "Question_gpt_summary":"user try determin manifest file enabl dataset version manifest file good option smaller dataset easili version tool save model artifact user question best wai enabl exact dataset version creat uniqu train set hard copi train job scalabl larg dataset",
        "Answer_original_content":"creat condit immut asset manifest point manifest enabl exact dataset version data store amazon version data asset us manifest file creat version dataset specif usag guarante immut asset manifest point manifest invalid",
        "Answer_preprocessed_content":"creat condit immut asset manifest point manifest enabl exact dataset version data store amazon version data asset us manifest file creat version dataset specif usag guarante immut asset manifest point manifest invalid",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that Amazon SageMaker manifest files can enable exact dataset versioning if the assets that the manifest points to are immutable. To ensure immutability, a data store in Amazon S3 can be used to store all versions of the data assets, and the manifest files can be used for creating and versioning datasets for specific usage. However, if the assets are not immutable, the manifest becomes invalid.",
        "Answer_gpt_summary":"solut discuss suggest manifest file enabl exact dataset version asset manifest point immut ensur immut data store amazon store version data asset manifest file creat version dataset specif usag asset immut manifest invalid"
    },
    {
        "Question_title":"How to get the public ip of amazon sagemaker's notebook instance? Is it possible?",
        "Question_body":"<p>Is it possible to get the public-ip of an amazon <code>sagemaker<\/code> notebook instance?<\/p>\n\n<p>I was wondering if I can ssh into it using the public ip for remote debugging purposes.<\/p>\n\n<p>I tried getting the public ip using the below curl command<\/p>\n\n<pre><code>$curl http:\/\/169.254.169.254\/latest\/meta-data\n<\/code><\/pre>\n\n<p>This just lists the local ip and not the public ip.<\/p>\n\n<p>I also tried the below command.<\/p>\n\n<pre><code>$curl ifconfig.me\n<\/code><\/pre>\n\n<p>This returns an ip address like <code>13.232.96.15<\/code>. If I try ssh into this it doesnt work.<\/p>\n\n<p>Is there any other way we can do this?<\/p>\n\n<p>Note : The ssh port 22 is open already in the security group<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1563361386390,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":6.0,
        "Question_view_count":4461.0,
        "Answer_body":"<p>I don't think you can ssh to notebook instances. You can either use open them from the console, or grab the url with an API, re: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-access-ws.html\" rel=\"noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-access-ws.html<\/a><\/p>\n\n<p>If you need a terminal, then you can open one from Jupyter.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57074382",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1563405774440,
        "Question_original_content":"public notebook instanc possibl possibl public notebook instanc wonder ssh public remot debug purpos tri get public curl command curl http latest meta data list local public tri command curl ifconfig return address like try ssh doesnt work wai note ssh port open secur group",
        "Question_preprocessed_content":"public notebook instanc possibl possibl amazon notebook instanc wonder ssh public remot debug purpos tri get public curl command list local public tri command return address like try ssh doesnt work wai note ssh port open secur group",
        "Question_gpt_summary_original":"The user is facing challenges in obtaining the public IP of an Amazon Sagemaker notebook instance for remote debugging purposes. The user attempted to retrieve the public IP using curl commands but was only able to obtain the local IP. The user also tried to use the IP address obtained from ifconfig.me but was unable to SSH into it. The user is seeking alternative methods to obtain the public IP.",
        "Question_gpt_summary":"user face challeng obtain public notebook instanc remot debug purpos user attempt retriev public curl command abl obtain local user tri us address obtain ifconfig unabl ssh user seek altern method obtain public",
        "Answer_original_content":"think ssh notebook instanc us open consol grab url api http doc aw amazon com latest howitwork access html need termin open jupyt",
        "Answer_preprocessed_content":"think ssh notebook instanc us open consol grab url api need termin open jupyt",
        "Answer_gpt_summary_original":"Solutions provided:\n- It is not possible to SSH into notebook instances. Instead, the user can either open them from the console or grab the URL with an API.\n- If the user needs a terminal, they can open one from Jupyter.",
        "Answer_gpt_summary":"solut provid possibl ssh notebook instanc instead user open consol grab url api user need termin open jupyt"
    },
    {
        "Question_title":"How to see all SageMaker service quota limits?",
        "Question_body":"<p>I believe there are different limits for SageMaker training, vs CreateTransformJob, spot vs not dedicated. Where can I see the current service limits for sagemaker services? Is there a place to check all SageMaker service quotas?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1648517982110,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":215.0,
        "Answer_body":"<p><a href=\"https:\/\/docs.aws.amazon.com\/general\/latest\/gr\/sagemaker.html\" rel=\"nofollow noreferrer\">Here<\/a> in the documentation you can see the default sagemaker service quotas. Unfortunately, it's not yet possible to see the current quotas according to this <a href=\"https:\/\/repost.aws\/questions\/QUweO83CSlTu-3Zn2RxdESWg\/how-do-i-check-my-current-sage-maker-service-quotas\" rel=\"nofollow noreferrer\">post<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71655510",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1648538467292,
        "Question_original_content":"servic quota limit believ differ limit train createtransformjob spot dedic current servic limit servic place check servic quota",
        "Question_preprocessed_content":"servic quota limit believ differ limit train createtransformjob spot dedic current servic limit servic place check servic quota",
        "Question_gpt_summary_original":"The user is facing challenges in finding the current service limits for different SageMaker services such as training, CreateTransformJob, spot, and not dedicated. They are seeking information on where to check all SageMaker service quotas.",
        "Question_gpt_summary":"user face challeng find current servic limit differ servic train createtransformjob spot dedic seek inform check servic quota",
        "Answer_original_content":"document default servic quota unfortun possibl current quota accord post",
        "Answer_preprocessed_content":"document default servic quota unfortun possibl current quota accord post",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to the default SageMaker service quotas in the documentation. However, there is currently no solution mentioned for finding the current quotas for different SageMaker services.",
        "Answer_gpt_summary":"solut discuss provid link default servic quota document current solut mention find current quota differ servic"
    },
    {
        "Question_title":"Use tensorboard with object detection API in sagemaker",
        "Question_body":"<p>with <a href=\"https:\/\/github.com\/svpino\/tensorflow-object-detection-sagemaker\" rel=\"nofollow noreferrer\">this<\/a> I successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container. Now I'd like to monitor the training job using sagemaker, but cannot find anything explaining how to do it. I don't use a sagemaker notebook.\nI think I can do it by saving the logs into a S3 bucket and point there a local tensorboard instance .. but don't know how to tell the tensorflow object detection API where to save the logs (is there any command line argument for this ?).\nSomething like <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/keras_script_mode_pipe_mode_horovod\/tensorflow_keras_CIFAR10.ipynb\" rel=\"nofollow noreferrer\">this<\/a>, but the script <code>generate_tensorboard_command.py<\/code> fails because my training job don't have the <code>sagemaker_submit_directory<\/code> parameter..<\/p>\n<p>The fact is when I start the training job nothing is created on my s3 until the job finish and upload everything. There should be a way tell tensorflow where to save the logs (s3) during the training, hopefully without modifying the API source code..<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>I can finally make it works with the accepted solution (tensorflow natively supports read\/write to s3), there are however additional steps to do:<\/p>\n<ol>\n<li>Disable network isolation in the training job configuration<\/li>\n<li>Provide credentials to the docker image to write to S3 bucket<\/li>\n<\/ol>\n<p>The only thing is that Tensorflow continuously polls filesystem (i.e. looking for an updated model in serving mode) and this cause useless requests to S3, that you will have to pay (together with a buch of errors in the console). I opened a new question <a href=\"https:\/\/stackoverflow.com\/q\/64969198\/4267439\">here<\/a> for this. At least it works.<\/p>\n<p><strong>Edit 2<\/strong><\/p>\n<p>I was wrong, TF just write logs, is not polling so it's an expected behavior and the extra costs are minimal.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1590408759343,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1606323198012,
        "Question_score":2.0,
        "Question_view_count":311.0,
        "Answer_body":"<p>Looking through the example you posted, it looks as though the <code>model_dir<\/code> passed to the TensorFlow Object Detection package is configured to <code>\/opt\/ml\/model<\/code>:<\/p>\n<pre><code># These are the paths to where SageMaker mounts interesting things in your container.\nprefix = '\/opt\/ml\/'\ninput_path = os.path.join(prefix, 'input\/data')\noutput_path = os.path.join(prefix, 'output')\nmodel_path = os.path.join(prefix, 'model')\nparam_path = os.path.join(prefix, 'input\/config\/hyperparameters.json')\n<\/code><\/pre>\n<p>During the training process, tensorboard logs will be written to <code>\/opt\/ml\/model<\/code>, and then uploaded to s3 as a final model artifact AFTER training: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-envvariables.html<\/a>.<\/p>\n<p>You <em>might<\/em> be able to side-step the SageMaker artifact upload step and point the <code>model_dir<\/code> of TensorFlow Object Detection API directly at an s3 location during training:<\/p>\n<pre><code>model_path = &quot;s3:\/\/your-bucket\/path\/here\n<\/code><\/pre>\n<p>This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of it's container. Assuming the underlying TensorFlow Object Detection code can write directly to S3 (something you'll have to verify), you should be able to see the tensorboard logs and checkpoints there in realtime.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62002183",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1594137982056,
        "Question_original_content":"us tensorboard object detect api successfulli creat train job tensorflow object detect api docker contain like monitor train job explain us notebook think save log bucket point local tensorboard instanc know tell tensorflow object detect api save log command line argument like script gener tensorboard command fail train job submit directori paramet fact start train job creat job finish upload wai tell tensorflow save log train hopefulli modifi api sourc code edit final work accept solut tensorflow nativ support read write addit step disabl network isol train job configur provid credenti docker imag write bucket thing tensorflow continu poll filesystem look updat model serv mode caus useless request pai buch error consol open new question work edit wrong write log poll expect behavior extra cost minim",
        "Question_preprocessed_content":"us tensorboard object detect api successfulli creat train job tensorflow object detect api docker contain like monitor train job explain us notebook think save log bucket point local tensorboard instanc know tell tensorflow object detect api save log like script fail train job fact start train job creat job finish upload wai tell tensorflow save log train hopefulli modifi api sourc edit final work accept solut addit step disabl network isol train job configur provid credenti docker imag write bucket thing tensorflow continu poll filesystem caus useless request pai open new question work edit wrong write log poll expect behavior extra cost minim",
        "Question_gpt_summary_original":"The user successfully created a training job on sagemaker using the Tensorflow Object Detection API in a docker container, but is facing challenges in monitoring the training job using sagemaker. The user is unable to find any information on how to do it and is considering saving the logs into an S3 bucket and pointing to a local tensorboard instance. However, the user does not know how to tell the Tensorflow Object Detection API where to save the logs. The user has tried using a script but it failed because the training job does not have the required parameter. The user was able to make it work with the accepted solution, but had to disable network isolation in the training job configuration and provide credentials to the docker image to write to the S3 bucket. The user also discovered that Tensorflow continuously writes logs and does not poll the filesystem, which results in minimal extra costs.",
        "Question_gpt_summary":"user successfulli creat train job tensorflow object detect api docker contain face challeng monitor train job user unabl inform consid save log bucket point local tensorboard instanc user know tell tensorflow object detect api save log user tri script fail train job requir paramet user abl work accept solut disabl network isol train job configur provid credenti docker imag write bucket user discov tensorflow continu write log poll filesystem result minim extra cost",
        "Answer_original_content":"look exampl post look model dir pass tensorflow object detect packag configur opt model path mount interest thing contain prefix opt input path path join prefix input data output path path join prefix output model path path join prefix model param path path join prefix input config hyperparamet json train process tensorboard log written opt model upload final model artifact train http doc aw amazon com latest algorithm train algo envvari html abl step artifact upload step point model dir tensorflow object detect api directli locat train model path bucket path mean tensorflow librari job directli write instead filesystem insid contain assum underli tensorflow object detect code write directli verifi abl tensorboard log checkpoint realtim",
        "Answer_preprocessed_content":"look exampl post look pass tensorflow object detect packag configur train process tensorboard log written upload final model artifact train abl artifact upload step point tensorflow object detect api directli locat train mean tensorflow librari job directli write instead filesystem insid contain assum underli tensorflow object detect code write directli abl tensorboard log checkpoint realtim",
        "Answer_gpt_summary_original":"Solution: The user can configure the `model_dir` passed to the TensorFlow Object Detection package to `\/opt\/ml\/model`, where tensorboard logs will be written during the training process and then uploaded to S3 as a final model artifact after training. Alternatively, the user can point the `model_dir` of TensorFlow Object Detection API directly at an S3 location during training by setting `model_path = \"s3:\/\/your-bucket\/path\/here\"`. This means that the TensorFlow library within the SageMaker job is directly writing to S3 instead of the filesystem inside of its container. Assuming the underlying TensorFlow Object Detection code can write directly to S3, the user should be able to see the tensorboard logs and checkpoints there in real-time.",
        "Answer_gpt_summary":"solut user configur model dir pass tensorflow object detect packag opt model tensorboard log written train process upload final model artifact train altern user point model dir tensorflow object detect api directli locat train set model path bucket path mean tensorflow librari job directli write instead filesystem insid contain assum underli tensorflow object detect code write directli user abl tensorboard log checkpoint real time"
    },
    {
        "Question_title":"AWS Sagemaker Studio, cannot load pickle files",
        "Question_body":"<p>I'm a newbie in Sagemaker and i'm trying to load a pickle dataset into sagemaker notebook.\nI'm using the Python 3 (Data Science) kernel and ml.t3.medium instance.\nEither i load the pickle from S3 or I upload it directly from the studio like this:<\/p>\n<pre><code>import pickle5\nwith open('filename', 'rb') as f:\n    x = pickle.load(f)\n<\/code><\/pre>\n<p><strong>I get this Error:<\/strong><\/p>\n<pre><code>---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n\/opt\/conda\/lib\/python3.7\/site-packages\/IPython\/core\/formatters.py in __call__(self, obj)\n    700                 type_pprinters=self.type_printers,\n    701                 deferred_pprinters=self.deferred_printers)\n--&gt; 702             printer.pretty(obj)\n    703             printer.flush()\n    704             return stream.getvalue()\n\n..................... more errors here\n\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\npandas\/_libs\/properties.pyx in pandas._libs.properties.AxisProperty.__get__()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pandas\/core\/generic.py in __getattr__(self, name)\n   5268             or name in self._accessors\n   5269         ):\n-&gt; 5270             return object.__getattribute__(self, name)\n   5271         else:\n   5272             if self._info_axis._can_hold_identifiers_and_holds_name(name):\n\nAttributeError: 'DataFrame' object has no attribute '_data'\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1619992853797,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":237.0,
        "Answer_body":"<p>Can you check your Pandas versions? This error typically occurs when the pickled file was written in an old Pandas version. Your Sagemaker notebook probably runs Pandas &gt; 1.1 where as the Pandas in which the dataframe was pickled is probably &lt; 1.1<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67361483",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1620025682632,
        "Question_original_content":"studio load pickl file newbi try load pickl dataset notebook python data scienc kernel medium instanc load pickl upload directli studio like import pickl open filenam pickl load error attributeerror traceback recent opt conda lib python site packag ipython core formatt self obj type pprinter self type printer defer pprinter self defer printer printer pretti obj printer flush return stream getvalu error opt conda lib python site packag panda core gener getattr self self accessor return object getattribut self self info axi hold identifi hold panda lib properti pyx panda lib properti axisproperti opt conda lib python site packag panda core gener getattr self self accessor return object getattribut self self info axi hold identifi hold attributeerror datafram object attribut data",
        "Question_preprocessed_content":"studio load pickl file newbi try load pickl dataset notebook python kernel instanc load pickl upload directli studio like error",
        "Question_gpt_summary_original":"The user is facing challenges while trying to load a pickle dataset into Sagemaker notebook using Python 3 (Data Science) kernel and ml.t3.medium instance. The user is encountering an error message that says 'DataFrame' object has no attribute '_data'.",
        "Question_gpt_summary":"user face challeng try load pickl dataset notebook python data scienc kernel medium instanc user encount error messag sai datafram object attribut data",
        "Answer_original_content":"check panda version error typic occur pickl file written old panda version notebook probabl run panda panda datafram pickl probabl",
        "Answer_preprocessed_content":"check panda version error typic occur pickl file written old panda version notebook probabl run panda panda datafram pickl probabl",
        "Answer_gpt_summary_original":"Solution: The error message that says 'DataFrame' object has no attribute '_data' while trying to load a pickle dataset into Sagemaker notebook using Python 3 (Data Science) kernel and ml.t3.medium instance can be resolved by checking the Pandas versions. This error typically occurs when the pickled file was written in an old Pandas version. The Sagemaker notebook probably runs Pandas > 1.1 where as the Pandas in which the dataframe was pickled is probably < 1.1.",
        "Answer_gpt_summary":"solut error messag sai datafram object attribut data try load pickl dataset notebook python data scienc kernel medium instanc resolv check panda version error typic occur pickl file written old panda version notebook probabl run panda panda datafram pickl probabl"
    },
    {
        "Question_title":"Why package is not updated even the lifecycle script has been executed successfully in SageMaker?",
        "Question_body":"<p>I wanted to update pandas version in 'conda-python3' in SageMaker, I've followed the steps in this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">page<\/a>, and linked the new configuration to my instance, CloudWatch log shows me the script has been executed successfully, but when I restart my instance and print out the panda version, it's still showing the old version 0.24.2, I don't understand why?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/fR82t.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fR82t.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This is the script in the lifecycle configuration:<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1593901036277,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":106.0,
        "Answer_body":"<p>You are not activating any conda environment such as <a href=\"https:\/\/stackoverflow.com\/questions\/60036916\/sagemaker-lifecycle-configuration-for-installing-pandas-not-working\">python3<\/a>.<\/p>\n<pre><code>#!\/bin\/bash\nsudo -u ec2-user -i &lt;&lt;'EOF'\n\n# This will affect only the Jupyter kernel called &quot;conda_python3&quot;.\nsource activate python3\n\npip install pandas\n\nconda update pandas\n\nsource deactivate\n\nEOF\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1605563749572,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62734994",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1594359260763,
        "Question_original_content":"packag updat lifecycl script execut successfulli want updat panda version conda python follow step page link new configur instanc cloudwatch log show script execut successfulli restart instanc print panda version show old version understand script lifecycl configur bin bash sudo user eof pip instal panda conda updat panda sourc deactiv eof",
        "Question_preprocessed_content":"packag updat lifecycl script execut successfulli want updat panda version follow step page link new configur instanc cloudwatch log show script execut successfulli restart instanc print panda version show old version understand script lifecycl configur",
        "Question_gpt_summary_original":"The user encountered a challenge in updating the pandas version in 'conda-python3' in SageMaker. The user followed the steps in the provided page and linked the new configuration to their instance. The CloudWatch log showed that the script was executed successfully, but upon restarting the instance and checking the pandas version, it still showed the old version. The user is unsure why this is happening.",
        "Question_gpt_summary":"user encount challeng updat panda version conda python user follow step provid page link new configur instanc cloudwatch log show script execut successfulli restart instanc check panda version show old version user unsur happen",
        "Answer_original_content":"activ conda environ python bin bash sudo user eof affect jupyt kernel call conda python sourc activ python pip instal panda conda updat panda sourc deactiv eof",
        "Answer_preprocessed_content":"activ conda environ python",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to activate the conda environment such as 'python3' before installing and updating pandas. The provided script should be modified to include the command 'source activate python3' before installing and updating pandas, and 'source deactivate' after completing the installation and update.",
        "Answer_gpt_summary":"solut possibl solut mention discuss activ conda environ python instal updat panda provid script modifi includ command sourc activ python instal updat panda sourc deactiv complet instal updat"
    },
    {
        "Question_title":"Azure ML, DevOps: Switching between branches keeps some files from another branch",
        "Question_body":"<p>I use Azure Machine Learning Workspace Notebooks, connected to a DevOps Repository - using terminal git commands to manage my code. I work on different branches, often has to switch back and forth between them.<\/p>\n<p>I reviewed this thread before: <a href=\"https:\/\/stackoverflow.com\/questions\/18615428\/switching-branches-keeps-new-files-from-other-branch\">switching branches keeps new files from other branch<\/a><\/p>\n<p>In my case it does not only keep the files that should be ignored with the use of the gitignore file, but others too.<\/p>\n<p>I tested it with a totally empty branch, that should not have any files in it, checked it out, and it still has files from the branch that I worked with previously. When I check it manually on DevOps, in the repo, the empty branch is actually empty there.<\/p>\n<p>Has anyone seen similar issues?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1622716713767,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":189.0,
        "Answer_body":"<p>Some files that are tracked in a branch could be not tracked in another. So when you switch back to the &quot;non tracking&quot; branch, that files remain in the file system. Git does not clean stuff that does not track directly. Do not exchange the term not tracked by ignored. Files are not tracked until we &quot;add&quot; them in stage and commit.\nYou could cleanup the working git by running <code>git clean -f -d<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67819912",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1622721091067,
        "Question_original_content":"devop switch branch keep file branch us workspac notebook connect devop repositori termin git command manag code work differ branch switch forth review thread switch branch keep new file branch case file ignor us gitignor file test total branch file check file branch work previous check manual devop repo branch actual seen similar issu",
        "Question_preprocessed_content":"devop switch branch keep file branch us workspac notebook connect devop repositori termin git command manag code work differ branch switch forth review thread switch branch keep new file branch case file ignor us gitignor file test total branch file check file branch work previous check manual devop repo branch actual seen similar issu",
        "Question_gpt_summary_original":"The user is encountering challenges when switching between branches in Azure Machine Learning Workspace Notebooks connected to a DevOps Repository. Despite using the gitignore file, files from other branches are being kept, including in an empty branch that should not have any files. The user is seeking advice on similar issues.",
        "Question_gpt_summary":"user encount challeng switch branch workspac notebook connect devop repositori despit gitignor file file branch kept includ branch file user seek advic similar issu",
        "Answer_original_content":"file track branch track switch non track branch file remain file git clean stuff track directli exchang term track ignor file track add stage commit cleanup work git run git clean",
        "Answer_preprocessed_content":"file track branch track switch non track branch file remain file git clean stuff track directli exchang term track ignor file track add stage commit cleanup work git run",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to clean up the working git by running the command \"git clean -f -d\". This will remove any untracked files from the file system.",
        "Answer_gpt_summary":"solut possibl solut mention discuss clean work git run command git clean remov untrack file file"
    },
    {
        "Question_title":"Azure ML Online Endpoint deployment DriverFileNotFound Error",
        "Question_body":"<p>When running the Azure ML Online endpoint commands, it works locally. But when I try to deploy it to Azure I get this error.\nCommand - <code>az ml online-deployment create --name blue --endpoint &quot;unique-name&quot; -f endpoints\/online\/managed\/sample\/blue-deployment.yml --all-traffic<\/code><\/p>\n<pre><code>{\n    &quot;status&quot;: &quot;Failed&quot;,\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;DriverFileNotFound&quot;,\n        &quot;message&quot;: &quot;Driver file with name score.py not found in provided dependencies. Please check the name of your file.&quot;,\n        &quot;details&quot;: [\n            {\n                &quot;code&quot;: &quot;DriverFileNotFound&quot;,\n                &quot;message&quot;: &quot;Driver file with name score.py not found in provided dependencies. Please check the name of your file.\\nThe build log is available in the workspace blob store \\&quot;coloraiamlsa\\&quot; under the path \\&quot;\/azureml\/ImageLogs\/1673692e-e30b-4306-ab81-2eed9dfd4020\/build.log\\&quot;&quot;,\n                &quot;details&quot;: [],\n                &quot;additionalInfo&quot;: []\n            }\n        ],\n        \n<\/code><\/pre>\n<p>This is the deployment YAML taken straight from <a href=\"https:\/\/github.com\/Azure\/azureml-examples\" rel=\"nofollow noreferrer\">azureml-examples<\/a> repo<\/p>\n<pre><code>$schema: https:\/\/azuremlschemas.azureedge.net\/latest\/managedOnlineDeployment.schema.json\nname: blue\nendpoint_name: my-endpoint\nmodel:\n  local_path: ..\/..\/model-1\/model\/sklearn_regression_model.pkl\ncode_configuration:\n  code: \n    local_path: ..\/..\/model-1\/onlinescoring\/\n  scoring_script: score.py\nenvironment: \n  conda_file: ..\/..\/model-1\/environment\/conda.yml\n  image: mcr.microsoft.com\/azureml\/openmpi3.1.2-ubuntu18.04:20210727.v1\ninstance_type: Standard_F2s_v2\ninstance_count: 1\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1642055150693,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":130.0,
        "Answer_body":"<p>Finally after lot of head banging, I have been able to consistently repro this bug in another Azure ML Workspace.<\/p>\n<p>I tried deploying the same sample in a brand new Azure ML workspace created and it went smoothly.<\/p>\n<p>At this point I remembered that I had upgraded the Storage Account of my previous AML Workspace to DataLake Gen2.<\/p>\n<p>So I did the same upgrade in this new workspace\u2019s storage account. After the upgrade, when I try to deploy the same endpoint, I get the same <code>DriverFileNotFoundError<\/code>!<\/p>\n<p>It seems Azure ML does not support Storage Account with DataLake Gen2 capabilities although the support page says otherwise. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#supported-data-storage-service-types<\/a>.<\/p>\n<p>At this point my only option is to recreate a new workspace and deploy my code there. Hope Azure team fixes this soon.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70692270",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1642392913590,
        "Question_original_content":"onlin endpoint deploy driverfilenotfound error run onlin endpoint command work local try deploi azur error command onlin deploy creat blue endpoint uniqu endpoint onlin manag sampl blue deploy yml traffic statu fail error code driverfilenotfound messag driver file score provid depend check file detail code driverfilenotfound messag driver file score provid depend check file nthe build log avail workspac blob store coloraiamlsa path imagelog eeddfd build log detail additionalinfo deploy yaml taken straight exampl repo schema http schema azureedg net latest managedonlinedeploy schema json blue endpoint endpoint model local path model model sklearn regress model pkl code configur code local path model onlinescor score script score environ conda file model environ conda yml imag mcr microsoft com openmpi ubuntu instanc type standard instanc count",
        "Question_preprocessed_content":"onlin endpoint deploy driverfilenotfound error run onlin endpoint command work local try deploi azur error command deploy yaml taken straight exampl repo",
        "Question_gpt_summary_original":"The user is encountering a \"DriverFileNotFound\" error when trying to deploy Azure ML Online endpoint commands to Azure. The error message indicates that the driver file with the name \"score.py\" is not found in the provided dependencies. The deployment YAML used is taken from the Azure ML examples repository.",
        "Question_gpt_summary":"user encount driverfilenotfound error try deploi onlin endpoint command azur error messag indic driver file score provid depend deploy yaml taken exampl repositori",
        "Answer_original_content":"final lot head bang abl consist repro bug workspac tri deploi sampl brand new workspac creat went smoothli point rememb upgrad storag account previou aml workspac datalak gen upgrad new workspac storag account upgrad try deploi endpoint driverfilenotfounderror support storag account datalak gen capabl support page sai http doc microsoft com azur machin learn access data support data storag servic type point option recreat new workspac deploi code hope azur team fix soon",
        "Answer_preprocessed_content":"final lot head bang abl consist repro bug workspac tri deploi sampl brand new workspac creat went smoothli point rememb upgrad storag account previou aml workspac datalak gen upgrad new workspac storag account upgrad try deploi endpoint support storag account datalak gen capabl support page sai point option recreat new workspac deploi code hope azur team fix soon",
        "Answer_gpt_summary_original":"Solution: The user found that upgrading the Storage Account of their previous Azure ML Workspace to DataLake Gen2 caused the \"DriverFileNotFound\" error. They tried deploying the same sample in a brand new Azure ML workspace created and it went smoothly. It seems Azure ML does not support Storage Account with DataLake Gen2 capabilities although the support page says otherwise. The only option for the user is to recreate a new workspace and deploy their code there. No other solutions were mentioned.",
        "Answer_gpt_summary":"solut user upgrad storag account previou workspac datalak gen caus driverfilenotfound error tri deploi sampl brand new workspac creat went smoothli support storag account datalak gen capabl support page sai option user recreat new workspac deploi code solut mention"
    },
    {
        "Question_title":"ClearML SSH port forwarding fileserver not available in WEB Ui",
        "Question_body":"<p>Trying to use clearml-server on own Ubuntu 18.04.5 with SSH Port Forwarding and not beeing able to see my debug samples.<\/p>\n<p>My setup:<\/p>\n<ul>\n<li>ClearML server on hostA<\/li>\n<li>SSH Tunnel connections to access Web App from working machine via localhost:18080<\/li>\n<li>Web App: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<li>Fileserver: <code>ssh -N -L 18081:127.0.0.1:8081 user@hostA<\/code><\/li>\n<\/ul>\n<p>In Web App under Task-&gt;Results-&gt;Debug Samples the Images are still refrenced by localhost:8081<\/p>\n<p>Where can I set the fileserver URL to be localhost:18081 in Web App?\nI tried ~\/clearml.conf, but this did not work ( I think it is for my python script ).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1610384778623,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":121.0,
        "Answer_body":"<p>Disclaimer: I'm a member of the ClearML team (formerly Trains)<\/p>\n<p>In ClearML, debug images' URL is registered once they are uploaded to the fileserver. The WebApp doesn't actually decide on the URL for each debug image, but rather obtains it for each debug image from the server. This allows you to potentially upload debug images to a variety of storage targets, ClearML File Server simply being the most convenient, built-in option.<\/p>\n<p>So, the WebApp will always look for <code>localhost:8008<\/code> for debug images that have already been uploaded to the fileserver and contain <code>localhost:8080<\/code> in their URL.\nA possible solution is to simply add another tunnel in the form of <code>ssh -N -L 8081:127.0.0.1:8081 user@hostA<\/code>.<\/p>\n<p>For future experiments, you can choose to keep using <code>8081<\/code> (and keep using this new tunnel), or to change the default fileserver URL in <code>clearml.conf<\/code> to point to port <code>localhost:18081<\/code>, assuming you're running your experiments from the same machine where the tunnel to <code>18081<\/code> exists.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65671395",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1610390345336,
        "Question_original_content":"ssh port forward fileserv avail web try us server ubuntu ssh port forward bee abl debug sampl setup server hosta ssh tunnel connect access web app work machin localhost web app ssh user hosta fileserv ssh user hosta web app task result debug sampl imag refrenc localhost set fileserv url localhost web app tri conf work think python script",
        "Question_preprocessed_content":"ssh port forward fileserv avail web try us server ubuntu ssh port forward bee abl debug sampl setup server hosta ssh tunnel connect access web app work machin localhost web app fileserv web app sampl imag refrenc localhost set fileserv url localhost web app tri work",
        "Question_gpt_summary_original":"The user is facing challenges while using ClearML server on their Ubuntu 18.04.5 with SSH Port Forwarding. They are unable to see their debug samples and are trying to set the fileserver URL to localhost:18081 in the Web App, but are unable to find the correct location to do so. The images are still referenced by localhost:8081 in the Web App under Task->Results->Debug Samples.",
        "Question_gpt_summary":"user face challeng server ubuntu ssh port forward unabl debug sampl try set fileserv url localhost web app unabl correct locat imag referenc localhost web app task result debug sampl",
        "Answer_original_content":"disclaim member team train debug imag url regist upload fileserv webapp actual decid url debug imag obtain debug imag server allow potenti upload debug imag varieti storag target file server simpli conveni built option webapp look localhost debug imag upload fileserv contain localhost url possibl solut simpli add tunnel form ssh user hosta futur experi choos new tunnel chang default fileserv url conf point port localhost assum run experi machin tunnel exist",
        "Answer_preprocessed_content":"disclaim member team debug imag url regist upload fileserv webapp actual decid url debug imag obtain debug imag server allow potenti upload debug imag varieti storag target file server simpli conveni option webapp look debug imag upload fileserv contain url possibl solut simpli add tunnel form futur experi choos chang default fileserv url point port assum run experi machin tunnel exist",
        "Answer_gpt_summary_original":"Solution:\n- The WebApp obtains the URL for each debug image from the server, so changing the fileserver URL to localhost:18081 in the Web App will not work.\n- A possible solution is to add another tunnel in the form of `ssh -N -L 8081:127.0.0.1:8081 user@hostA`.\n- For future experiments, the user can choose to keep using 8081 (and keep using the new tunnel), or change the default fileserver URL in `clearml.conf` to point to port localhost:18081, assuming they are running their experiments from the same machine where the tunnel to 18081 exists.",
        "Answer_gpt_summary":"solut webapp obtain url debug imag server chang fileserv url localhost web app work possibl solut add tunnel form ssh user hosta futur experi user choos new tunnel chang default fileserv url conf point port localhost assum run experi machin tunnel exist"
    },
    {
        "Question_title":"Watching over SageMaker while it is training",
        "Question_body":"<p>I am using Amazon SageMaker to train a model with a lot of data. \nThis takes a lot of time - hours or even days. During this time, I would like be able to query the trainer and see its current status, particularly:<\/p>\n\n<ul>\n<li>How many iterations it already did, and how many iterations it still needs to do? (the training algorithm is deep learning - it is based on iterations).<\/li>\n<li>How much time does it need to complete the training?<\/li>\n<li>Ideally, I would like to classify a test-sample using the model of the current iteration, to see its current performance.<\/li>\n<\/ul>\n\n<p>One way to do this is to explicitly tell the trainer to print debug messages after each iteration. However, these messages will be availble only at the console from which I run the trainer. Since training takes so much time, I would like to be able to query the trainer status remotely, from different computers.<\/p>\n\n<p>Is there a way to remotely query the status of a running trainer?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1537104812430,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1537105180103,
        "Question_score":3.0,
        "Question_view_count":504.0,
        "Answer_body":"<p>All logs are available in Amazon Cloudwatch. You can query CloudWatch programmatically or via an API to parse the logs.<\/p>\n\n<p>Are you using built-in algorithms or a Framework like MXNet or TensorFlow? For TensorFlow you can monitor your job with <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_resnet_cifar10_with_tensorboard\/tensorflow_resnet_cifar10_with_tensorboard.ipynb\" rel=\"nofollow noreferrer\">TensorBoard<\/a>.<\/p>\n\n<p>Additionally, you can see high level job status using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/API_DescribeTrainingJob.html\" rel=\"nofollow noreferrer\">describe training job<\/a> API call:<\/p>\n\n<pre><code>import sagemaker\nsm_client = sagemaker.Session().sagemaker_client\nprint(sm_client.describe_training_job(TrainingJobName='You job name here'))\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52354671",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1537169575087,
        "Question_original_content":"watch train train model lot data take lot time hour dai time like abl queri trainer current statu particularli iter iter need train algorithm deep learn base iter time need complet train ideal like classifi test sampl model current iter current perform wai explicitli tell trainer print debug messag iter messag availbl consol run trainer train take time like abl queri trainer statu remot differ comput wai remot queri statu run trainer",
        "Question_preprocessed_content":"watch train train model lot data take lot time hour dai time like abl queri trainer current statu particularli iter iter need time need complet train ideal like classifi model current iter current perform wai explicitli tell trainer print debug messag iter messag availbl consol run trainer train take time like abl queri trainer statu remot differ comput wai remot queri statu run trainer",
        "Question_gpt_summary_original":"The user is facing challenges while using Amazon SageMaker to train a model with a large amount of data. They want to be able to remotely query the trainer's status, including the number of iterations completed and remaining, the time needed to complete training, and the ability to classify a test-sample using the model of the current iteration. The user is looking for a way to access this information from different computers, as the training process takes a lot of time.",
        "Question_gpt_summary":"user face challeng train model larg data want abl remot queri trainer statu includ number iter complet remain time need complet train abil classifi test sampl model current iter user look wai access inform differ comput train process take lot time",
        "Answer_original_content":"log avail amazon cloudwatch queri cloudwatch programmat api pars log built algorithm framework like mxnet tensorflow tensorflow monitor job tensorboard addition high level job statu train job api import client session client print client train job trainingjobnam job",
        "Answer_preprocessed_content":"log avail amazon cloudwatch queri cloudwatch programmat api pars log algorithm framework like mxnet tensorflow tensorflow monitor job tensorboard addition high level job statu train job api",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Querying CloudWatch programmatically or via an API to parse the logs.\n- Monitoring the job with TensorBoard if using TensorFlow.\n- Using the \"describe training job\" API call to see high-level job status.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss queri cloudwatch programmat api pars log monitor job tensorboard tensorflow train job api high level job statu person opinion bias includ summari"
    },
    {
        "Question_title":"Pipeline can not be built using a HyperdriveStep inside a Pipeline",
        "Question_body":"Hei, I'm trying to build a pipeline including a HyperdriveStep to tuen the hyperparameters.\nThe pipeline should later on run automatically and be tuned at each pipeline run.\n\nThe pipeline consists of three steps: a preparation step resulting in a PipelineData Object, the HyperdriveStep and a final PythonRegisterStep, where the best model should be registered.\n\nHowever, when creating the pipeline object I'm getting an error I can not relate to.\n\n\n\n\nTraceback (most recent call last):\n\n       File \"\/Users\/xxx\/Desktop\/azure_test\/pipeline-folder\/azure_pipeline_wrapper1.py\", line 168, in <module>\n         pipeline = Pipeline(workspace=ws, steps=pipeline_steps, description=\"Pipeline for hyperparameter tuning\")\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/core\/_experiment_method.py\", line 104, in wrapper\n         return init_func(self, *args, **kwargs)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/pipeline.py\", line 177, in __init__\n         self._graph = self._graph_builder.build(self._name, steps, finalize=False)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1481, in build\n         graph = self.construct(name, steps)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1503, in construct\n         self.process_collection(steps)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1539, in process_collection\n         builder.process_collection(collection)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1830, in process_collection\n         self._base_builder.process_collection(item)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1533, in process_collection\n         return self.process_step(collection)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/core\/builder.py\", line 1577, in process_step\n         node = step.create_node(self._graph, self._default_datastore, self._context)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py\", line 270, in create_node\n         hyperdrive_config, reuse_hashable_config = self._get_hyperdrive_config(context._workspace,\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/pipeline\/steps\/hyper_drive_step.py\", line 346, in _get_hyperdrive_config\n         hyperdrive_dto = _search._create_experiment_dto(self._hyperdrive_config, workspace,\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/train\/hyperdrive\/_search.py\", line 38, in _create_experiment_dto\n         platform_config = hyperdrive_config._get_platform_config(workspace, experiment_name, **kwargs)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/train\/hyperdrive\/runconfig.py\", line 672, in _get_platform_config\n         platform_config.update(self._get_platform_config_data_from_run_config(workspace))\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/train\/hyperdrive\/runconfig.py\", line 686, in _get_platform_config_data_from_run_config\n         run_config = get_run_config_from_script_run(self.run_config)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/site-packages\/azureml\/core\/script_run_config.py\", line 84, in get_run_config_from_script_run\n         run_config.arguments = deepcopy(script_run_config.arguments)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 146, in deepcopy\n         y = copier(x, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 205, in _deepcopy_list\n         append(deepcopy(a, memo))\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 172, in deepcopy\n         y = _reconstruct(x, memo, *rv)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 270, in _reconstruct\n         state = deepcopy(state, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 146, in deepcopy\n         y = copier(x, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 230, in _deepcopy_dict\n         y[deepcopy(key, memo)] = deepcopy(value, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 172, in deepcopy\n         y = _reconstruct(x, memo, *rv)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 270, in _reconstruct\n         state = deepcopy(state, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 146, in deepcopy\n         y = copier(x, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 230, in _deepcopy_dict\n         y[deepcopy(key, memo)] = deepcopy(value, memo)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 172, in deepcopy\n         y = _reconstruct(x, memo, *rv)\n        \n       File \"\/Users\/xxx\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copy.py\", line 264, in _reconstruct\n         y = func(*args)\n        \n       File \"\/Users\/xxxr\/opt\/anaconda3\/envs\/azure_env\/lib\/python3.8\/copyreg.py\", line 91, in __newobj__\n         return cls.__new__(cls, *args)\n        \n     TypeError: __new__() missing 2 required positional arguments: 'workspace' and 'name'\n\n\n\n\n\nMy Code:\n\n # Connect to workspace \n ws = Workspace.from_config()\n print(ws.name, \"loaded\")\n    \n # Set compute target\n cluster_name = \"compcluster234\"\n pipeline_cluster = ComputeTarget(workspace=ws, name=cluster_name)\n    \n # Create new environment\n sklearn_env = Environment(\"sklearn_env\")\n # Adds dependencies to PythonSection of sklaern_env\n env_packages = CondaDependencies.create(conda_packages=['scikit-learn'])\n sklearn_env.docker.enabled = True\n sklearn_env.python.conda_dependencies = env_packages\n # Register the environment\n sklearn_env.register(workspace=ws)\n    \n # =============================================================================\n # Run Configuration\n # =============================================================================\n    \n # Create Run configuration \n # Pipeline_folder\n pipeline_folder = path + '\/pipeline-folder'\n # Create a new runconfig object for the pipeline\n pipeline_run_config = RunConfiguration()\n # Use the compute you created above. \n pipeline_run_config.target = pipeline_cluster\n # Assign the environment to the run configuration\n # In comparison to the ScriptRunCnfig object, the RunConfig is more generous\n pipeline_run_config.environment = sklearn_env\n print (\"Run configuration created.\")\n    \n # =============================================================================\n # DataPath\n # =============================================================================\n    \n # Get the default datastore\n default_ds = ws.get_default_datastore()\n # Create a DataPath object \n datapath = DataPath(datastore = default_ds,\n                      path_on_datastore = 'cancer-data')\n # Make the datapath a PipelineParameter\n datapath_pipeline_param = PipelineParameter(name='input-data',   \n                                             default_value=datapath)\n datapath_input = (datapath_pipeline_param, \n                    DataPathComputeBinding(mode = 'mount'))\n    \n # =============================================================================\n # PipelineData\n # =============================================================================\n    \n # Create a PipelineData (temporary Data Reference) for the preppared data folder\n prepped_data_folder = PipelineData(name=\"prepped_data_folder\",\n                                    datastore=ws.get_default_datastore())\n    \n # Create PipelineData objects for the Metrics and the saved model\n metrics_output_name = 'metrics_output'\n metrics_data = PipelineData(name='metrics_data',\n                             datastore=default_ds,\n                             pipeline_output_name=metrics_output_name,\n                             training_output=TrainingOutput(\"Metrics\"))\n    \n model_output_name = 'model_output'\n saved_model = PipelineData(name='saved_model',\n                            datastore=default_ds,\n                            pipeline_output_name=model_output_name,\n                            training_output=TrainingOutput(\"Model\",\n                                                           model_file=\"outputs\/model\/cancer_model.pkl\"))\n    \n # =============================================================================\n # Pipeline Steps\n # =============================================================================\n    \n # Step 1, Run the data prep script\n prep_step = PythonScriptStep(name = \"prepare_data\",\n                                 source_directory = pipeline_folder,\n                                 script_name = \"cancer_pipeline_preprocessing.py\",\n                                 arguments = ['--input-data', datapath_input,\n                                              '--prepped-data', prepped_data_folder],\n                                 inputs=[datapath_input],\n                                 outputs=[prepped_data_folder],\n                                 compute_target = pipeline_cluster,\n                                 runconfig = pipeline_run_config,\n                                 allow_reuse = False)\n    \n # Define the search strategy and parameter space for hyperparameter tuning\n ps = GridParameterSampling({ '--max_depth': choice(1,2,3)})\n # Define a early stopping criteria\n early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n # Define a ScriptRunConfig for the Training script\n # The ScriptRunConfig is based on the RunConfig of the Pipeline\n script_run_config = ScriptRunConfig(script=\"cancer_pipeline_tuning.py\",\n                                     source_directory=pipeline_folder,\n                                     # Add non-hyperparameter arguments -in this case, the training dataset\n                                     arguments = ['--training_folder', prepped_data_folder],\n                                     run_config=pipeline_run_config)\n # Define a HyperDriveConfiguration\n # The primary_metric_name must be completely idential to the metric name logged during training (inside the training script)\n hd_config = HyperDriveConfig(run_config=script_run_config, \n                              hyperparameter_sampling=ps,\n                              policy=early_termination_policy,\n                              primary_metric_name='Accuracy', \n                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                              max_total_runs=3,\n                              max_concurrent_runs=2)\n    \n # Step 2b, define a HyperDriveStep\n # HyperDriveStep can be used to run HyperDrive job as a step in pipeline.\n # No arguments need to be set as they are already set inside the ScriptRunConfig\n hyperdrive_step = HyperDriveStep(name=\"tune_hyperparameters\",\n                                  hyperdrive_config=hd_config,\n                                  inputs=[prepped_data_folder],\n                                  outputs=[metrics_data, saved_model])\n    \n hyperdrive_step.run_after(prep_step)    \n    \n # Step 3, Run the model registration step\n register_step = PythonScriptStep(name=\"register_model\",\n                                        script_name='cancer_pipeline_register1.py',\n                                        source_directory = pipeline_folder,\n                                        arguments=[\"--saved_model\", saved_model],\n                                        inputs=[saved_model],\n                                        compute_target = pipeline_cluster,\n                                        runconfig=pipeline_run_config,\n                                        allow_reuse = False)\n    \n register_step.run_after(hyperdrive_step)    \n print(\"Pipeline steps defined\")\n    \n    \n # Construct the pipeline\n pipeline_steps = [prep_step, hyperdrive_step, register_step]\n pipeline = Pipeline(workspace=ws, steps=pipeline_steps, description=\"Pipeline for hyperparameter tuning\")\n print(\"Pipeline is built.\")",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_created_time":1621515534017,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Solved the issue!\n\nHad to remove the arguments argument of the ScriptRunConfig and instead set the values to the Hyperdrive Steps estimator_entry_script_arguments argument.\n\n # Step 1, Run the data prep script\n prep_step = PythonScriptStep(name = \"prepare_data\",\n                                 source_directory = pipeline_folder,\n                                 script_name = \"cancer_pipeline_preprocessing.py\",\n                                 arguments = ['--input-data', datapath_input,\n                                              '--prepped-data', prepped_data_folder],\n                                 inputs=[datapath_input],\n                                 outputs=[prepped_data_folder],\n                                 compute_target = pipeline_cluster,\n                                 runconfig = pipeline_run_config,\n                                 allow_reuse=False)\n    \n # Define the search strategy and parameter space for hyperparameter tuning\n ps = GridParameterSampling({'--max_depth': choice(1,2,3),\n                             '--n_estimators': choice(100,300)})\n # Define a early stopping criteria\n early_termination_policy = BanditPolicy(evaluation_interval=2, slack_factor=0.1)\n # Define a ScriptRunConfig for the Training script\n # The ScriptRunConfig is based on the RunConfig of the Pipeline\n script_run_config = ScriptRunConfig(script=\"cancer_pipeline_tuning.py\",\n                                     source_directory=pipeline_folder,\n                                     run_config=pipeline_run_config)\n # Define a HyperDriveConfiguration\n # The primary_metric_name must be completely idential to the metric name logged during training (inside the training script)\n hd_config = HyperDriveConfig(run_config=script_run_config, \n                              hyperparameter_sampling=ps,\n                              policy=None,\n                              primary_metric_name=\"Accuracy\", \n                              primary_metric_goal=PrimaryMetricGoal.MAXIMIZE, \n                              max_total_runs=6,\n                              max_concurrent_runs=2)\n    \n # Step 2b, define a HyperDriveStep\n # HyperDriveStep can be used to run HyperDrive job as a step in pipeline.\n # No arguments need to be set as they are already set inside the ScriptRunConfig\n hyperdrive_step = HyperDriveStep(name=\"tune_hyperparameters\",\n                                  hyperdrive_config=hd_config,\n                                  # Add non-hyperparameter arguments -in this case, the training dataset\n                                  # IMPORTANT: Don't add them already in the ScriptRunConfig\n                                  estimator_entry_script_arguments=['--training_folder', prepped_data_folder],\n                                  inputs=[prepped_data_folder],\n                                  outputs=[metrics_data, saved_model],\n                                  allow_reuse=False)",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/403018\/pipeline-can-not-be-built-using-a-hyperdrivestep-i.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1622098105463,
        "Question_original_content":"pipelin built hyperdrivestep insid pipelin hei try build pipelin includ hyperdrivestep tuen hyperparamet pipelin later run automat tune pipelin run pipelin consist step prepar step result pipelinedata object hyperdrivestep final pythonregisterstep best model regist creat pipelin object get error relat traceback recent file user desktop azur test pipelin folder azur pipelin wrapper line pipelin pipelin workspac step pipelin step descript pipelin hyperparamet tune file user opt anaconda env azur env lib python site packag core experi method line wrapper return init func self arg kwarg file user opt anaconda env azur env lib python site packag pipelin core pipelin line init self graph self graph builder build self step final fals file user opt anaconda env azur env lib python site packag pipelin core builder line build graph self construct step file user opt anaconda env azur env lib python site packag pipelin core builder line construct self process collect step file user opt anaconda env azur env lib python site packag pipelin core builder line process collect builder process collect collect file user opt anaconda env azur env lib python site packag pipelin core builder line process collect self base builder process collect item file user opt anaconda env azur env lib python site packag pipelin core builder line process collect return self process step collect file user opt anaconda env azur env lib python site packag pipelin core builder line process step node step creat node self graph self default datastor self context file user opt anaconda env azur env lib python site packag pipelin step hyper drive step line creat node hyperdr config reus hashabl config self hyperdr config context workspac file user opt anaconda env azur env lib python site packag pipelin step hyper drive step line hyperdr config hyperdr dto search creat experi dto self hyperdr config workspac file user opt anaconda env azur env lib python site packag train hyperdr search line creat experi dto platform config hyperdr config platform config workspac experi kwarg file user opt anaconda env azur env lib python site packag train hyperdr runconfig line platform config platform config updat self platform config data run config workspac file user opt anaconda env azur env lib python site packag train hyperdr runconfig line platform config data run config run config run config script run self run config file user opt anaconda env azur env lib python site packag core script run config line run config script run run config argument deepcopi script run config argument file user opt anaconda env azur env lib python copi line deepcopi copier memo file user opt anaconda env azur env lib python copi line deepcopi list append deepcopi memo file user opt anaconda env azur env lib python copi line deepcopi reconstruct memo file user opt anaconda env azur env lib python copi line reconstruct state deepcopi state memo file user opt anaconda env azur env lib python copi line deepcopi copier memo file user opt anaconda env azur env lib python copi line deepcopi dict deepcopi kei memo deepcopi valu memo file user opt anaconda env azur env lib python copi line deepcopi reconstruct memo file user opt anaconda env azur env lib python copi line reconstruct state deepcopi state memo file user opt anaconda env azur env lib python copi line deepcopi copier memo file user opt anaconda env azur env lib python copi line deepcopi dict deepcopi kei memo deepcopi valu memo file user opt anaconda env azur env lib python copi line deepcopi reconstruct memo file user opt anaconda env azur env lib python copi line reconstruct func arg file user opt anaconda env azur env lib python copyreg line newobj return cl new cl arg typeerror new miss requir posit argument workspac code connect workspac workspac config print load set comput target cluster compclust pipelin cluster computetarget workspac cluster creat new environ sklearn env environ sklearn env add depend pythonsect sklaern env env packag condadepend creat conda packag scikit learn sklearn env docker enabl true sklearn env python conda depend env packag regist environ sklearn env regist workspac run configur creat run configur pipelin folder pipelin folder path pipelin folder creat new runconfig object pipelin pipelin run config runconfigur us comput creat pipelin run config target pipelin cluster assign environ run configur comparison scriptruncnfig object runconfig gener pipelin run config environ sklearn env print run configur creat datapath default datastor default default datastor creat datapath object datapath datapath datastor default path datastor cancer data datapath pipelineparamet datapath pipelin param pipelineparamet input data default valu datapath datapath input datapath pipelin param datapathcomputebind mode mount pipelinedata creat pipelinedata temporari data refer preppar data folder prep data folder pipelinedata prep data folder datastor default datastor creat pipelinedata object metric save model metric output metric output metric data pipelinedata metric data datastor default pipelin output metric output train output trainingoutput metric model output model output save model pipelinedata save model datastor default pipelin output model output train output trainingoutput model model file output model cancer model pkl pipelin step step run data prep script prep step pythonscriptstep prepar data sourc directori pipelin folder script cancer pipelin preprocess argument input data datapath input prep data prep data folder input datapath input output prep data folder comput target pipelin cluster runconfig pipelin run config allow reus fals defin search strategi paramet space hyperparamet tune gridparametersampl max depth choic defin earli stop criteria earli termin polici banditpolici evalu interv slack factor defin scriptrunconfig train script scriptrunconfig base runconfig pipelin script run config scriptrunconfig script cancer pipelin tune sourc directori pipelin folder add non hyperparamet argument case train dataset argument train folder prep data folder run config pipelin run config defin hyperdriveconfigur primari metric complet identi metric log train insid train script config hyperdriveconfig run config script run config hyperparamet sampl polici earli termin polici primari metric accuraci primari metric goal primarymetricgo maxim max total run max concurr run step defin hyperdrivestep hyperdrivestep run hyperdr job step pipelin argument need set set insid scriptrunconfig hyperdr step hyperdrivestep tune hyperparamet hyperdr config config input prep data folder output metric data save model hyperdr step run prep step step run model registr step regist step pythonscriptstep regist model script cancer pipelin regist sourc directori pipelin folder argument save model save model input save model comput target pipelin cluster runconfig pipelin run config allow reus fals regist step run hyperdr step print pipelin step defin construct pipelin pipelin step prep step hyperdr step regist step pipelin pipelin workspac step pipelin step descript pipelin hyperparamet tune print pipelin built",
        "Question_preprocessed_content":"pipelin built hyperdrivestep insid pipelin hei try build pipelin includ hyperdrivestep tuen hyperparamet pipelin later run automat tune pipelin run pipelin consist step prepar step result pipelinedata object hyperdrivestep final pythonregisterstep best model regist creat pipelin object get error relat traceback file line pipelin pipelin file line wrapper return arg kwarg file line step final fals file line build graph step file line construct file line file line file line return file line node file line file line workspac file line kwarg file line file line file line file line deepcopi copier file line append file line deepcopi memo file line state deepcopi file line deepcopi copier file line deepcopi file line deepcopi memo file line state deepcopi file line deepcopi copier file line deepcopi file line deepcopi memo file line func file line return arg typeerror miss requir posit argument workspac code connect workspac load set comput target compclust computetarget creat new environ add depend pythonsect true regist environ run configur creat run configur path creat new runconfig object pipelin runconfigur us comput creat assign environ run configur comparison scriptruncnfig object runconfig gener print datapath default datastor creat datapath object datapath datapath datapath pipelineparamet pipelinedata creat pipelinedata preppar data folder creat pipelinedata object metric save model pipelin step step run data prep script pythonscriptstep defin search strategi paramet space hyperparamet tune gridparametersampl defin earli stop criteria defin scriptrunconfig train script scriptrunconfig base runconfig pipelin add argument case train dataset argument defin hyperdriveconfigur complet identi metric log train step defin hyperdrivestep hyperdrivestep run hyperdr job step pipelin argument need set set insid scriptrunconfig step run model registr step fals print construct pipelin pipelin pipelin print",
        "Question_gpt_summary_original":"The user is encountering an error while trying to build a pipeline that includes a HyperdriveStep to tune hyperparameters. The pipeline consists of three steps: a preparation step, the HyperdriveStep, and a final PythonRegisterStep. The error occurs when creating the pipeline object, and the user is unsure of the cause. The error traceback suggests that there is an issue with the __new__() function, which is missing two required positional arguments: 'workspace' and 'name'.",
        "Question_gpt_summary":"user encount error try build pipelin includ hyperdrivestep tune hyperparamet pipelin consist step prepar step hyperdrivestep final pythonregisterstep error occur creat pipelin object user unsur caus error traceback suggest issu new function miss requir posit argument workspac",
        "Answer_original_content":"solv issu remov argument argument scriptrunconfig instead set valu hyperdr step estim entri script argument argument step run data prep script prep step pythonscriptstep prepar data sourc directori pipelin folder script cancer pipelin preprocess argument input data datapath input prep data prep data folder input datapath input output prep data folder comput target pipelin cluster runconfig pipelin run config allow reus fals defin search strategi paramet space hyperparamet tune gridparametersampl max depth choic estim choic defin earli stop criteria earli termin polici banditpolici evalu interv slack factor defin scriptrunconfig train script scriptrunconfig base runconfig pipelin script run config scriptrunconfig script cancer pipelin tune sourc directori pipelin folder run config pipelin run config defin hyperdriveconfigur primari metric complet identi metric log train insid train script config hyperdriveconfig run config script run config hyperparamet sampl polici primari metric accuraci primari metric goal primarymetricgo maxim max total run max concurr run step defin hyperdrivestep hyperdrivestep run hyperdr job step pipelin argument need set set insid scriptrunconfig hyperdr step hyperdrivestep tune hyperparamet hyperdr config config add non hyperparamet argument case train dataset import add scriptrunconfig estim entri script argument train folder prep data folder input prep data folder output metric data save model allow reus fals",
        "Answer_preprocessed_content":"solv issu remov argument argument scriptrunconfig instead set valu hyperdr step argument step run data prep script pythonscriptstep defin search strategi paramet space hyperparamet tune choic choic defin earli stop criteria defin scriptrunconfig train script scriptrunconfig base runconfig pipelin defin hyperdriveconfigur complet identi metric log train polici step defin hyperdrivestep hyperdrivestep run hyperdr job step pipelin argument need set set insid scriptrunconfig add argument case train dataset import add scriptrunconfig",
        "Answer_gpt_summary_original":"Solution: The user solved the issue by removing the arguments argument of the ScriptRunConfig and instead setting the values to the Hyperdrive Steps estimator_entry_script_arguments argument.",
        "Answer_gpt_summary":"solut user solv issu remov argument argument scriptrunconfig instead set valu hyperdr step estim entri script argument argument"
    },
    {
        "Question_title":"Recommendations without ratings (Azure ML)",
        "Question_body":"<p>I'm trying to build an experiment to create recommendations (using the Movie Ratings sample database), but without using the ratings. I simply consider that if a user has rated certain movies, then he would be interested by other movies that have been rated by users that have also rated his movies.<\/p>\n\n<p>I can consider, for instance, that ratings are 1 (exists in the database) or 0 (does not exist), but in that case, how do I transform the initial data to reflect this?<\/p>\n\n<p>I couldn't find any kind of examples or tutorials about this kind of scenario, and I don't really know how to proceed. Should I transform the data before injecting it into an algorithm? And\/or is there any kind of specific algorithm that I should use?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_created_time":1469698571557,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1469701089488,
        "Question_score":1.0,
        "Question_view_count":910.0,
        "Answer_body":"<p>If you're hoping to use the Matchbox Recommender in AML, you're correct that you need to identify some user-movie pairs that <em>are<\/em> not present in the raw dataset, and add these in with a rating of zero. (I'll assume that you have already set all of the real user-movie pairs to have a rating of one, as you described above.)<\/p>\n\n<p>I would recommend generating some random candidate pairs and confirming their absence from the training data in an Execute R (or Python) Script module. I don't know the names of your dataset's features, but here is some pseudocode in R to do that:<\/p>\n\n<pre><code>library(dplyr)\ndf &lt;- maml.mapInputPort(1)  # input dataset of observed user-movie pairs\nall_movies &lt;- unique(df[['movie']])\nall_users &lt;- unique(df[['user']])\nn &lt;- 30  # number of random pairs to start with\n\nnegative_observations &lt;- data.frame(movie = sample(all_movies, n, replace=TRUE),\n                                    user = sample(all_users, n, replace=TRUE),\n                                    rating = rep(0, n))          \nacceptable_negative_observations &lt;- anti_join(unique(negative_observations), df, by=c('movie', 'user'))\ndf &lt;- rbind(df, acceptable_negative_observations)\nmaml.mapOutputPort(\"df\");\n<\/code><\/pre>\n\n<p>Alternatively, you could try a method like <a href=\"https:\/\/en.wikipedia.org\/wiki\/Association_rule_learning\" rel=\"nofollow\">association rule learning<\/a> which would not require you to add in the fake zero ratings. Martin Machac has posted a <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Frequently-bought-together-market-basket-analyses-using-ARULES-1\" rel=\"nofollow\">nice example<\/a> of how to do this in R\/AML in the Cortana Intelligence Gallery.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38632533",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1471354453808,
        "Question_original_content":"recommend rate try build experi creat recommend movi rate sampl databas rate simpli consid user rate certain movi interest movi rate user rate movi consid instanc rate exist databas exist case transform initi data reflect couldn kind exampl tutori kind scenario know proce transform data inject algorithm kind specif algorithm us",
        "Question_preprocessed_content":"recommend rate try build experi creat recommend rate simpli consid user rate certain movi interest movi rate user rate movi consid instanc rate case transform initi data reflect couldn kind exampl tutori kind scenario know proce transform data inject algorithm kind specif algorithm us",
        "Question_gpt_summary_original":"The user is facing challenges in building an experiment to create recommendations without using ratings in Azure ML. They are unsure of how to transform the initial data to reflect this and are seeking guidance on whether to transform the data before injecting it into an algorithm and if there is a specific algorithm to use. They have not found any examples or tutorials on this scenario.",
        "Question_gpt_summary":"user face challeng build experi creat recommend rate unsur transform initi data reflect seek guidanc transform data inject algorithm specif algorithm us exampl tutori scenario",
        "Answer_original_content":"hope us matchbox recommend aml correct need identifi user movi pair present raw dataset add rate zero assum set real user movi pair rate describ recommend gener random candid pair confirm absenc train data execut python script modul know name dataset featur pseudocod librari dplyr maml mapinputport input dataset observ user movi pair movi uniqu movi user uniqu user number random pair start neg observ data frame movi sampl movi replac true user sampl user replac true rate rep accept neg observ anti join uniqu neg observ movi user rbind accept neg observ maml mapoutputport altern try method like associ rule learn requir add fake zero rate martin machac post nice exampl aml cortana intellig galleri",
        "Answer_preprocessed_content":"hope us matchbox recommend aml correct need identifi pair present raw dataset add rate zero recommend gener random candid pair confirm absenc train data execut script modul know name dataset featur pseudocod altern try method like associ rule learn requir add fake zero rate martin machac post nice exampl cortana intellig galleri",
        "Answer_gpt_summary_original":"Two possible solutions were discussed in the conversation. The first solution involves generating random candidate pairs and confirming their absence from the training data in an Execute R (or Python) Script module. The second solution involves using association rule learning, which would not require adding in fake zero ratings. A nice example of how to do this in R\/AML is available in the Cortana Intelligence Gallery.",
        "Answer_gpt_summary":"possibl solut discuss convers solut involv gener random candid pair confirm absenc train data execut python script modul second solut involv associ rule learn requir ad fake zero rate nice exampl aml avail cortana intellig galleri"
    },
    {
        "Question_title":"Does Amazon SageMaker RL support heterogenous clusters?",
        "Question_body":"Does SageMaker RL support heterogenous clusters? I'd like to have our training to run on GPU and and SageMaker RL, and our inferences to run on CPUs.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1593179712000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":27.0,
        "Answer_body":"Yes, Amazon SageMaker RL allows you to define training workers separately from inference workers.\n\nFor more information, see Amazon SageMaker RL \u2013 Managed reinforcement learning with Amazon SageMaker on the AWS News Blog. Also, the Build and Train Reinforcement Models with Amazon SageMaker RL AWS online tech talk (see minute 26).",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUE59_Oro0SGKaOIdZNmySiw\/does-amazon-sage-maker-rl-support-heterogenous-clusters",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1593376503000,
        "Question_original_content":"support heterogen cluster support heterogen cluster like train run gpu infer run cpu",
        "Question_preprocessed_content":"support heterogen cluster support heterogen cluster like train run gpu infer run cpu",
        "Question_gpt_summary_original":"The user is inquiring about whether Amazon SageMaker RL supports heterogenous clusters, as they want their training to run on GPUs and their inferences to run on CPUs.",
        "Question_gpt_summary":"user inquir support heterogen cluster want train run gpu infer run cpu",
        "Answer_original_content":"ye allow defin train worker separ infer worker inform manag reinforc learn aw new blog build train reinforc model aw onlin tech talk minut",
        "Answer_preprocessed_content":"ye allow defin train worker separ infer worker inform manag reinforc learn aw new blog build train reinforc model aw onlin tech talk",
        "Answer_gpt_summary_original":"Solution: Amazon SageMaker RL allows the user to define training workers separately from inference workers, which means that they can run their training on GPUs and their inferences on CPUs. More information can be found on the AWS News Blog and the AWS online tech talk.",
        "Answer_gpt_summary":"solut allow user defin train worker separ infer worker mean run train gpu infer cpu inform aw new blog aw onlin tech talk"
    },
    {
        "Question_title":"Unable to upload data using sagemaker_session.upload_data in s3 bucket that I created, it is getting stored in default s3 bucket",
        "Question_body":"<p>So I am trying iris to get acquainted with was sagemaker I am following simple tutorials from <a href=\"https:\/\/towardsdatascience.com\/training-and-deploying-custom-tensorflow-models-with-aws-sagemaker-72027722ad76\" rel=\"nofollow noreferrer\">link<\/a>. I have created a bucket named &quot;tf-practise-iris-data&quot; and gave the IAM role of Sagemaker access to the s3 bucket as mentioned in the tutorial. I also tried creating a new bucket with a different name thinking there might be some problem with a bucket but still it is having the same issue, this is the snippet of my code <a href=\"https:\/\/i.stack.imgur.com\/FgjVK.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FgjVK.png\" alt=\"enter image description here\" \/><\/a>. And I have turned off Block all public access from the bucket but still nothing.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1642443300117,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":242.0,
        "Answer_body":"<p>Solved it!!!!!!!!!!!!<\/p>\n<pre><code>prefix = &quot;checking-with-new-bucket&quot;\ntraining_input_path = sagemaker_session.upload_data('train.csv', bucket = 'checking-with-new-bucket',key_prefix = prefix + &quot;\/training&quot;)\ntraining_input_path\n<\/code><\/pre>\n<p>Which gave output as<\/p>\n<pre><code>'s3:\/\/checking-with-new-bucket\/checking-with-new-bucket\/training\/train.csv'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70745798",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1642446326470,
        "Question_original_content":"unabl upload data session upload data bucket creat get store default bucket try iri acquaint follow simpl tutori link creat bucket name practis iri data gave iam role access bucket mention tutori tri creat new bucket differ think problem bucket have issu snippet code turn block public access bucket",
        "Question_preprocessed_content":"unabl upload data bucket creat get store default bucket try iri acquaint follow simpl tutori link creat bucket name gave iam role access bucket mention tutori tri creat new bucket differ think problem bucket have issu snippet code turn block public access bucket",
        "Question_gpt_summary_original":"The user is facing challenges in uploading data using sagemaker_session.upload_data in an s3 bucket that they created. Despite creating a new bucket and giving Sagemaker access to the bucket, the data is still getting stored in the default s3 bucket. The user has turned off Block all public access from the bucket but the issue persists.",
        "Question_gpt_summary":"user face challeng upload data session upload data bucket creat despit creat new bucket give access bucket data get store default bucket user turn block public access bucket issu persist",
        "Answer_original_content":"solv prefix check new bucket train input path session upload data train csv bucket check new bucket kei prefix prefix train train input path gave output check new bucket check new bucket train train csv",
        "Answer_preprocessed_content":"solv gave output",
        "Answer_gpt_summary_original":"Solution: The user was able to solve the issue by specifying the bucket name in the key_prefix parameter while using the sagemaker_session.upload_data function. The code used was:\n\nprefix = \"checking-with-new-bucket\"\ntraining_input_path = sagemaker_session.upload_data('train.csv', bucket = 'checking-with-new-bucket',key_prefix = prefix + \"\/training\")\ntraining_input_path\n\nThis solution ensured that the data was uploaded to the correct bucket and the issue was resolved.",
        "Answer_gpt_summary":"solut user abl solv issu specifi bucket kei prefix paramet session upload data function code prefix check new bucket train input path session upload data train csv bucket check new bucket kei prefix prefix train train input path solut ensur data upload correct bucket issu resolv"
    },
    {
        "Question_title":"ValueError: no SavedModel bundles found! when trying to deploy a TF2.0 model to SageMaker",
        "Question_body":"<p>I'm trying to deploy a TF2.0 model to SageMaker. So far, I managed to train the model and save it into an S3 bucket but when I'm calling the <code>.deploy()<\/code> method, I get the following error from cloud Watch <\/p>\n\n<p><code>ValueError: no SavedModel bundles found!<\/code><\/p>\n\n<p>Here is my training script: <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tf_model\"), save_format=\"tf\")\n\ndef model_fn(model_dir):\n    classifier = tf.keras.models.load_model(os.path.join(model_dir, \"tf_model\"))\n    return classifier\n<\/code><\/pre>\n\n<p>And here is the code that I wrote into Colab <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='tensorflow_estimator.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='ml.m5.large',\n                          framework_version='2.0.0', \n                          sagemaker_session=sagemaker_session,\n                          output_path=s3_output_location,\n                          hyperparameters={'epochs': 1,\n                                           'batch_size': 30,\n                                           'learning_rate': 0.001},\n                          py_version='py3')\n\n\ntf_estimator.fit({\"train\":train_data})\n\nfrom sagemaker.tensorflow.serving import Model\n\nmodel = Model(model_data='s3:\/\/path\/to\/model.tar.gz', \n              role=role,\n              framework_version=\"2.0.0\",\n              sagemaker_session=sagemaker_session)\n\npredictor = model.deploy(initial_instance_count=1, instance_type='ml.m5.large')\n<\/code><\/pre>\n\n<p>I already tried to look at <a href=\"https:\/\/stackoverflow.com\/questions\/57172147\/no-savedmodel-bundles-found-on-tensorflow-hub-model-deployment-to-aws-sagemak\">this thread<\/a> but I actually don't have the problem of versions in my tar.gz file as the structure is the following : <\/p>\n\n<pre><code>\u251c\u2500\u2500 assets\n\u251c\u2500\u2500 saved_model.pb\n\u2514\u2500\u2500 variables\n    \u251c\u2500\u2500 variables.data-00000-of-00001\n    \u2514\u2500\u2500 variables.index\n<\/code><\/pre>\n\n<p>I feel I might be wrong when defining <code>model_fn()<\/code> in my training script but definitely don't what to replace that with. Would you guys have an idea? <\/p>\n\n<p>Thanks a lot for your help!  <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1579796394240,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":943.0,
        "Answer_body":"<p>I actually tried to modify my training script to the following : <\/p>\n\n<pre><code>### Code to add in a tensorflow_estimator.py file\n\nimport argparse\nimport os\nimport pathlib\nimport tensorflow as tf\n\n\nif __name__ == '__main__':\n\n\n    parser = argparse.ArgumentParser()\n\n    # hyperparameters sent by the client are passed as command-line arguments to the script.\n    parser.add_argument('--epochs', type=int, default=10)\n    parser.add_argument('--batch_size', type=int, default=100)\n    parser.add_argument('--learning_rate', type=float, default=0.1)\n\n    # Data, model, and output directories\n    parser.add_argument('--output-data-dir', type=str, default=os.environ.get('SM_OUTPUT_DATA_DIR'))\n    parser.add_argument('--model-dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--test', type=str, default=os.environ.get('SM_CHANNEL_TEST'))\n\n    args, _ = parser.parse_known_args()\n\n    print(\"##### ARGS ##### \\n{}\".format(args))\n\n    # Get files \n    path = pathlib.Path(args.train)\n\n    # Print out folder content \n    for item in path.iterdir():\n        print(\"##### DIRECTORIES ##### \\n{}\".format(item))\n\n    # Get all images \n    all_images = list(path.glob(\"*\/*\"))\n    all_image_paths = [str(path) for path in list(path.glob(\"*\/*\"))]\n\n    # Transform images into tensors\n    def preprocess_and_load_images(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=3)\n        image = tf.image.resize(image, [192, 192])\n        return image\n\n    # Apply preprocessing function\n    ds_paths = tf.data.Dataset.from_tensor_slices(all_image_paths)\n    ds_images = ds_paths.map(preprocess_and_load_images)\n\n    # Map Labels\n    labels = []\n    for data in path.iterdir():  \n        if data.is_dir():               \n            labels += [data.name]    \n\n    labels_index = {}\n    for i,label in enumerate(labels):\n        labels_index[label]=i\n\n    print(\"##### Label Index ##### \\n{}\".format(labels_index))\n\n    all_image_labels = [labels_index[path.parent.name] for path in list(path.glob(\"*\/*\"))]\n\n    # Create a tf Dataset\n    labels_ds = tf.data.Dataset.from_tensor_slices(all_image_labels)\n\n    # Zip train and labeled dataset\n    full_ds = tf.data.Dataset.zip((ds_images, labels_ds))\n\n    # Shuffle Dataset and batch it \n    full_ds = full_ds.shuffle(len(all_images)).batch(args.batch_size)\n\n    # Create a pre-trained model \n    base_model = tf.keras.applications.InceptionV3(input_shape=(192,192,3), \n                                               include_top=False,\n                                               weights = \"imagenet\"\n                                               )\n\n    base_model.trainable = False\n    model = tf.keras.Sequential([\n                base_model,\n                tf.keras.layers.GlobalAveragePooling2D(),\n                tf.keras.layers.Dense(len(labels), activation=\"softmax\")\n            ])\n\n    initial_learning_rate = args.learning_rate\n\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate,\n        decay_steps=1000,\n        decay_rate=0.96,\n        staircase=True) \n\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate = lr_schedule),\n              loss = tf.keras.losses.SparseCategoricalCrossentropy(),\n              metrics = [tf.keras.metrics.SparseCategoricalAccuracy()])\n\n    # Train the model\n    model.fit(full_ds, epochs=args.epochs)\n\n    # Save the model \n    model.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n\n<\/code><\/pre>\n\n<p>It seems that it's important to have a numerical name for your folder:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code># Save the model\nmodel.save(os.path.join(args.model_dir, \"tensorflow_model\/1\"), save_format=\"tf\")\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1590696546172,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59882941",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1579799847967,
        "Question_original_content":"valueerror savedmodel bundl try deploi model try deploi model far manag train model save bucket call deploi method follow error cloud watch valueerror savedmodel bundl train script code add tensorflow estim file import argpars import import pathlib import tensorflow main parser argpars argumentpars hyperparamet sent client pass command line argument script parser add argument epoch type int default parser add argument batch size type int default parser add argument learn rate type float default data model output directori parser add argument output data dir type str default environ output data dir parser add argument model dir type str default environ model dir parser add argument train type str default environ channel train parser add argument test type str default environ channel test arg parser pars known arg print arg format arg file path pathlib path arg train print folder content item path iterdir print directori format item imag imag list path glob imag path str path path list path glob transform imag tensor def preprocess load imag path imag read file path imag imag decod jpeg imag channel imag imag resiz imag return imag appli preprocess function path data dataset tensor slice imag path imag path map preprocess load imag map label label data path iterdir data dir label data label index label enumer label label index label print label index format label index imag label label index path parent path list path glob creat dataset label data dataset tensor slice imag label zip train label dataset data dataset zip imag label shuffl dataset batch shuffl len imag batch arg batch size creat pre train model base model kera applic inceptionv input shape includ fals weight imagenet base model trainabl fals model kera sequenti base model kera layer globalaveragepoolingd kera layer dens len label activ softmax initi learn rate arg learn rate schedul kera optim schedul exponentialdecai initi learn rate decai step decai rate staircas true model compil optim kera optim adam learn rate schedul loss kera loss sparsecategoricalcrossentropi metric kera metric sparsecategoricalaccuraci train model model fit epoch arg epoch save model model save path join arg model dir model save format def model model dir classifi kera model load model path join model dir model return classifi code wrote colab tensorflow import tensorflow estim tensorflow entri point tensorflow estim role role train instanc count train instanc type larg framework version session session output path output locat hyperparamet epoch batch size learn rate version estim fit train train data tensorflow serv import model model model model data path model tar role role framework version session session predictor model deploi initi instanc count instanc type larg tri look thread actual problem version tar file structur follow asset save model variabl variabl data variabl index feel wrong defin model train script definit replac gui idea thank lot help",
        "Question_preprocessed_content":"valueerror savedmodel bundl try deploi model try deploi model far manag train model save bucket call method follow error cloud watch train script code wrote colab tri look thread actual problem version file structur follow feel wrong defin train script definit replac gui idea thank lot help",
        "Question_gpt_summary_original":"The user is encountering an error when trying to deploy a TensorFlow 2.0 model to SageMaker. The error message \"ValueError: no SavedModel bundles found!\" is displayed when calling the .deploy() method. The user has already trained the model and saved it to an S3 bucket. The user has also tried to look for solutions on Stack Overflow but has not found a solution. The user suspects that the issue may be with the model_fn() function in the training script but is unsure of how to fix it.",
        "Question_gpt_summary":"user encount error try deploi tensorflow model error messag valueerror savedmodel bundl displai call deploi method user train model save bucket user tri look solut stack overflow solut user suspect issu model function train script unsur fix",
        "Answer_original_content":"actual tri modifi train script follow code add tensorflow estim file import argpars import import pathlib import tensorflow main parser argpars argumentpars hyperparamet sent client pass command line argument script parser add argument epoch type int default parser add argument batch size type int default parser add argument learn rate type float default data model output directori parser add argument output data dir type str default environ output data dir parser add argument model dir type str default environ model dir parser add argument train type str default environ channel train parser add argument test type str default environ channel test arg parser pars known arg print arg format arg file path pathlib path arg train print folder content item path iterdir print directori format item imag imag list path glob imag path str path path list path glob transform imag tensor def preprocess load imag path imag read file path imag imag decod jpeg imag channel imag imag resiz imag return imag appli preprocess function path data dataset tensor slice imag path imag path map preprocess load imag map label label data path iterdir data dir label data label index label enumer label label index label print label index format label index imag label label index path parent path list path glob creat dataset label data dataset tensor slice imag label zip train label dataset data dataset zip imag label shuffl dataset batch shuffl len imag batch arg batch size creat pre train model base model kera applic inceptionv input shape includ fals weight imagenet base model trainabl fals model kera sequenti base model kera layer globalaveragepoolingd kera layer dens len label activ softmax initi learn rate arg learn rate schedul kera optim schedul exponentialdecai initi learn rate decai step decai rate staircas true model compil optim kera optim adam learn rate schedul loss kera loss sparsecategoricalcrossentropi metric kera metric sparsecategoricalaccuraci train model model fit epoch arg epoch save model model save path join arg model dir tensorflow model save format import numer folder save model model save path join arg model dir tensorflow model save format",
        "Answer_preprocessed_content":"actual tri modifi train script follow import numer folder",
        "Answer_gpt_summary_original":"Solution:\n- The user modified the training script to include a numerical name for the folder where the model is saved.",
        "Answer_gpt_summary":"solut user modifi train script includ numer folder model save"
    },
    {
        "Question_title":"Error while detaching AKS cluster through Azure ML SDK extension",
        "Question_body":"<p>I created an AKS cluster using Azure Machine Learning SDK extension and I attached to the workspace created. When the cluster is created and attached, I doesn't show any error. When I am trying to detach it from workspace, it is not accepting the operations.<\/p>\n<p>I would like to detach the existing AKS cluster from workspace either by program manner, using CLI or even using Azure portal.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1659308799773,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":47.0,
        "Answer_body":"<p>If we are using any <strong>extensions of SDK or Azure CLI<\/strong> for machine learning to detach AKS cluster, it <strong>will not work<\/strong> and it will not get deleted or detached. Instead, we need to use <strong>Azure CLI with AKS<\/strong>. There are two types of implementations we can perform.<\/p>\n<p><strong>Python:<\/strong><\/p>\n<pre><code>Aks_target.detach()\n<\/code><\/pre>\n<p><strong>Azure CLI:<\/strong><\/p>\n<p>Before performing this step, we need to get the details of the working AKS cluster name attached to our workspace. Resource Group details and workspace name<\/p>\n<pre><code>az ml computertarget detach -n youraksname -g yourresourcegroup -w yourworkspacename\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73187536",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659333006088,
        "Question_original_content":"error detach ak cluster sdk extens creat ak cluster sdk extens attach workspac creat cluster creat attach error try detach workspac accept oper like detach exist ak cluster workspac program manner cli azur portal",
        "Question_preprocessed_content":"error detach ak cluster sdk extens creat ak cluster sdk extens attach workspac creat cluster creat attach error try detach workspac accept oper like detach exist ak cluster workspac program manner cli azur portal",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to detach an AKS cluster from a workspace created using Azure Machine Learning SDK extension. The cluster was created and attached without any errors, but detaching it from the workspace is not working. The user is seeking solutions to detach the cluster either programmatically, using CLI, or through the Azure portal.",
        "Question_gpt_summary":"user encount challeng try detach ak cluster workspac creat sdk extens cluster creat attach error detach workspac work user seek solut detach cluster programmat cli azur portal",
        "Answer_original_content":"extens sdk azur cli machin learn detach ak cluster work delet detach instead need us azur cli ak type implement perform python ak target detach azur cli perform step need detail work ak cluster attach workspac resourc group detail workspac computertarget detach youraksnam yourresourcegroup yourworkspacenam",
        "Answer_preprocessed_content":"extens sdk azur cli machin learn detach ak cluster work delet detach instead need us azur cli ak type implement perform python azur cli perform step need detail work ak cluster attach workspac resourc group detail workspac",
        "Answer_gpt_summary_original":"The discussion suggests that detaching an AKS cluster from a workspace created using Azure Machine Learning SDK extension cannot be done programmatically or using Azure CLI with machine learning extensions. Instead, we need to use Azure CLI with AKS. Two possible solutions are provided: using Python with Aks_target.detach() or using Azure CLI with the command \"az ml computertarget detach -n youraksname -g yourresourcegroup -w yourworkspacename\" after obtaining the details of the working AKS cluster name attached to the workspace, resource group details, and workspace name.",
        "Answer_gpt_summary":"discuss suggest detach ak cluster workspac creat sdk extens programmat azur cli machin learn extens instead need us azur cli ak possibl solut provid python ak target detach azur cli command computertarget detach youraksnam yourresourcegroup yourworkspacenam obtain detail work ak cluster attach workspac resourc group detail workspac"
    },
    {
        "Question_title":"Unable to create Device Fleet",
        "Question_body":"<p>I am following this <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/edge-device-fleet-create.html#edge-device-fleet-create-console\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/edge-device-fleet-create.html#edge-device-fleet-create-console<\/a> docs to create device fleet. In this console, Role ARN is optional but it throws <code>RoleARN is required<\/code>. If I provide proper RoleArn it throws <code>Failed to create\/modify RoleAlias. Check your IAM role permission<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/84UEW.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/84UEW.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/wKuTV.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/wKuTV.jpg\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I have no idea what is going wrong. Any hint would be appreciable.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1607600991993,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":48.0,
        "Answer_body":"<p>Mohamed, this means that Sagemaker Edge Manager was unable to use the RoleAlias you provided to take the necessary actions when creating a DeviceFleet. It needs to have the AmazonSageMakerEdgeDeviceFleetPolicy attached (or have similar permissions granted) and it needs to trust both SageMaker and IoT Core.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65233943",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1614723172687,
        "Question_original_content":"unabl creat devic fleet follow http doc aw amazon com latest edg devic fleet creat html edg devic fleet creat consol doc creat devic fleet consol role arn option throw rolearn requir provid proper rolearn throw fail creat modifi rolealia check iam role permiss idea go wrong hint appreci",
        "Question_preprocessed_content":"unabl creat devic fleet follow doc creat devic fleet consol role arn option throw provid proper rolearn throw idea go wrong hint appreci",
        "Question_gpt_summary_original":"The user is facing challenges while creating a device fleet using AWS SageMaker. The console is throwing an error message stating that Role ARN is required, even though it is optional. When the user provides the proper RoleArn, it throws another error message stating that the creation or modification of RoleAlias has failed, and the user is unsure of what is going wrong.",
        "Question_gpt_summary":"user face challeng creat devic fleet consol throw error messag state role arn requir option user provid proper rolearn throw error messag state creation modif rolealia fail user unsur go wrong",
        "Answer_original_content":"moham mean edg manag unabl us rolealia provid necessari action creat devicefleet need amazonedgedevicefleetpolici attach similar permiss grant need trust iot core",
        "Answer_preprocessed_content":"moham mean edg manag unabl us rolealia provid necessari action creat devicefleet need amazonedgedevicefleetpolici attach need trust iot core",
        "Answer_gpt_summary_original":"Solution: The user needs to attach the AmazonSageMakerEdgeDeviceFleetPolicy to the RoleArn and ensure that it trusts both SageMaker and IoT Core to resolve the error messages while creating a device fleet using AWS SageMaker.",
        "Answer_gpt_summary":"solut user need attach amazonedgedevicefleetpolici rolearn ensur trust iot core resolv error messag creat devic fleet"
    },
    {
        "Question_title":"Connecting to existing sweep from Python",
        "Question_body":"<p>I have a few questions regarding the hyperparameter sweeps from Python.<br>\nI am wanting to essentially start a few tmux sessions on my server, and connect them all to the same sweep agent, but no keyword in the sweep_config (that i have found) allow me to connect to a specific sweep ID, and rather just a sweep name that doesnt connect to the same sweep, but just makes multiple sweeps of the same name.  If this possible or strongly advised against due to computational usage or similar?<\/p>\n<p>Furthermore, sweeps take up a great deal of storage requirements due to saving all the models, is it possible to store the model file from the best model only, while keeping the statistics from all the models for plots and interpretation? This would allow me to keep the great information gathered from sweeps, while not taking up 100+ GB from a single sweep.<\/p>\n<p>Thanks!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1641567235002,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":215.0,
        "Answer_body":"<p>I found the issue, i was trying to create a new wandb.sweep(config, project, entity) and pass the ID into the config dictionary, but instead i just needed to take the ID directly, and just do sweep_id = sweep_id_string which worked.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/connecting-to-existing-sweep-from-python\/1721",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1641593349991,
        "Question_original_content":"connect exist sweep python question hyperparamet sweep python want essenti start tmux session server connect sweep agent keyword sweep config allow connect specif sweep sweep doesnt connect sweep make multipl sweep possibl strongli advis comput usag similar furthermor sweep great deal storag requir save model possibl store model file best model keep statist model plot interpret allow great inform gather sweep take singl sweep thank",
        "Question_preprocessed_content":"connect exist sweep python question hyperparamet sweep python want essenti start tmux session server connect sweep agent keyword allow connect specif sweep sweep doesnt connect sweep make multipl sweep possibl strongli advis comput usag similar furthermor sweep great deal storag requir save model possibl store model file best model keep statist model plot interpret allow great inform gather sweep take singl sweep thank",
        "Question_gpt_summary_original":"The user is facing two challenges related to hyperparameter sweeps from Python. Firstly, they are unable to connect to a specific sweep ID and can only connect to a sweep name, which creates multiple sweeps of the same name instead of connecting to the same sweep. Secondly, the user is concerned about the storage requirements of sweeps as they take up a lot of space due to saving all the models. They are looking for a way to store only the model file from the best model while keeping the statistics from all the models for plots and interpretation.",
        "Question_gpt_summary":"user face challeng relat hyperparamet sweep python firstli unabl connect specif sweep connect sweep creat multipl sweep instead connect sweep secondli user concern storag requir sweep lot space save model look wai store model file best model keep statist model plot interpret",
        "Answer_original_content":"issu try creat new sweep config project entiti pass config dictionari instead need directli sweep sweep string work",
        "Answer_preprocessed_content":"issu try creat new sweep pass config dictionari instead need directli work",
        "Answer_gpt_summary_original":"Solution: The user found a solution to their first challenge by directly taking the ID instead of creating a new wandb.sweep and passing the ID into the config dictionary. However, no solution was provided for the second challenge of reducing storage requirements while keeping statistics from all models.",
        "Answer_gpt_summary":"solut user solut challeng directli take instead creat new sweep pass config dictionari solut provid second challeng reduc storag requir keep statist model"
    },
    {
        "Question_title":"Cloud Translation Permission",
        "Question_body":"So I'm pulling my hair out over this and reaching out here for help. I'm trying to set up a service account with Cloud Translation, and Text-to-speech enabled, but we keep getting this response:I have confirmed that the service account has the \"cloudtranslate.generalModels.predict\" permission, and showing the \"Cloud Translation API User\" role. We've also confirmed that it works with a different Service account that my colleague set up in his personal Google console profile. But, we need this setup with an account through our org. I did verify that the service account has the permission from the https:\/\/console.cloud.google.com\/iam-admin\/troubleshooter so and that my organization's admin sees that the service account is granted access through ancestor policies.  So what else can we check? ",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1668497880000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":53.0,
        "Answer_body":"Ok, turned out we had a hard-coded value for resource location, which was set to the wrong project. So of course it was coming back as permission denied.\u00a0\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Cloud-Translation-Permission\/td-p\/489632\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1668501000000,
        "Question_original_content":"cloud translat permiss pull hair reach help try set servic account cloud translat text speech enabl get respons confirm servic account cloudtransl generalmodel predict permiss show cloud translat api user role confirm work differ servic account colleagu set person googl consol profil need setup account org verifi servic account permiss http consol cloud googl com iam admin troubleshoot organ admin see servic account grant access ancestor polici check",
        "Question_preprocessed_content":"cloud translat permiss pull hair reach help try set servic account cloud translat enabl get respons confirm servic account permiss show cloud translat api user role confirm work differ servic account colleagu set person googl consol profil need setup account org verifi servic account permiss organ admin see servic account grant access ancestor polici check",
        "Question_gpt_summary_original":"The user is facing challenges while setting up a service account with Cloud Translation and Text-to-speech enabled. Despite having the necessary permissions and roles, the user is receiving an error message. The user has confirmed that the service account works with a different account but needs to set it up with an account through their organization. The user has verified that the service account has the required permission and access through ancestor policies. The user is seeking help to identify the issue.",
        "Question_gpt_summary":"user face challeng set servic account cloud translat text speech enabl despit have necessari permiss role user receiv error messag user confirm servic account work differ account need set account organ user verifi servic account requir permiss access ancestor polici user seek help identifi issu",
        "Answer_original_content":"turn hard code valu resourc locat set wrong project cours come permiss deni view solut origin post",
        "Answer_preprocessed_content":"turn valu resourc locat set wrong project cours come permiss deni view solut origin post",
        "Answer_gpt_summary_original":"Solution: The issue was identified as a hard-coded value for resource location that was set to the wrong project, resulting in a permission denied error. The solution was to update the resource location to the correct project.",
        "Answer_gpt_summary":"solut issu identifi hard code valu resourc locat set wrong project result permiss deni error solut updat resourc locat correct project"
    },
    {
        "Question_title":"How to get multiple lines exported to wandb",
        "Question_body":"<p>I am using the library weights and  biases. My model outputs a curve (a time series). I'd like to see how this curve changes throughout training. So, I'd need some kind of slider where I can select epoch and it shows me the curve for that epoch. It could be something very similar to what it's done with histograms (it shows an image of the histograms across epochs and when you hover it display the histogram corresponding to that epoch). Is there a way to do this or something similar using <code>wandb<\/code>?<\/p>\n<p>Currently my code looks like this:<\/p>\n<pre><code>for epoch in range(epochs):\n   output = model(input)\n   #output is shape (37,40) (lenght 40 and I have 37 samples)\n   #it's enough to plot the first sample\n   xs = torch.arange(40).unsqueeze(dim=1)\n   ys = output[0,:].unsqueeze(dim=1)\n   wandb.log({&quot;line&quot;: wandb.plot.line_series(xs=xs, ys=ys,title=&quot;Out&quot;)}, step=epoch)\n<\/code><\/pre>\n<p>I'd appreciate any help! Thanks!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1619035214083,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":840.0,
        "Answer_body":"<p>You can use <code>wandb.log()<\/code> with matplotlib. Create your plot using matplotlib:<\/p>\n<pre><code>import matplotlib.pyplot as plt\nimport numpy as np\nx = np.linspace(0, 50)\nfor i in range(1, 4):\n    fig, ax = plt.subplots()\n    y = x ** i\n    ax.plot(x, y)\n    wandb.log({'chart': ax})\n<\/code><\/pre>\n<p>Then when you look on your wandb dashboard for the run, you will see the plot rendered as plotly plot. Click the gear in the upper left hand corner to see a slider that lets you slide over training steps and see the plot at each step.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67202711",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_closed_time":1620426900467,
        "Question_original_content":"multipl line export librari weight bias model output curv time seri like curv chang train need kind slider select epoch show curv epoch similar histogram show imag histogram epoch hover displai histogram correspond epoch wai similar current code look like epoch rang epoch output model input output shape lenght sampl plot sampl torch arang unsqueez dim output unsqueez dim log line plot line seri titl step epoch appreci help thank",
        "Question_preprocessed_content":"multipl line export librari weight bias model output curv like curv chang train need kind slider select epoch show curv epoch similar histogram wai similar current code look like appreci help thank",
        "Question_gpt_summary_original":"The user is facing a challenge in visualizing the changes in a time series curve throughout the training of their model using the library weights and biases. They are looking for a way to create a slider that allows them to select an epoch and view the corresponding curve. The user has provided their current code, which logs the curve for each epoch, but they are seeking a more interactive visualization method.",
        "Question_gpt_summary":"user face challeng visual chang time seri curv train model librari look wai creat slider allow select epoch view correspond curv user provid current code log curv epoch seek interact visual method",
        "Answer_original_content":"us log matplotlib creat plot matplotlib import matplotlib pyplot plt import numpi linspac rang fig plt subplot plot log chart look dashboard run plot render plotli plot click gear upper left hand corner slider let slide train step plot step",
        "Answer_preprocessed_content":"us matplotlib creat plot matplotlib look dashboard run plot render plotli plot click gear upper left hand corner slider let slide train step plot step",
        "Answer_gpt_summary_original":"Solution: The user can use wandb.log() with matplotlib to create a plot for each epoch. Then, when they look at their wandb dashboard, they will see the plot rendered as a plotly plot with a slider that allows them to slide over training steps and view the plot at each step.",
        "Answer_gpt_summary":"solut user us log matplotlib creat plot epoch look dashboard plot render plotli plot slider allow slide train step view plot step"
    },
    {
        "Question_title":"how to set path of bucket in amazonsagemaker jupyter notebook?",
        "Question_body":"<p>I'm new to the aws how to set path of my bucket and access file of that bucket?<\/p>\n\n<p>Is there anything i need to change with prefix ?<\/p>\n\n<pre><code>import os\nimport boto3\nimport re\nimport copy\nimport time\nfrom time import gmtime, strftime\nfrom sagemaker import get_execution_role\n\nrole = get_execution_role()\n\nregion = boto3.Session().region_name\n\nbucket='ltfs1' # Replace with your s3 bucket name\nprefix = 'sagemaker\/ltfs1' # Used as part of the path in the bucket where you store data\n# bucket_path = 'https:\/\/s3-{}.amazonaws.com\/{}'.format(region,bucket) # The URL to access the bucket\n<\/code><\/pre>\n\n<p>I'm using the above code but it's showing file not found error<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1562134154127,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":852.0,
        "Answer_body":"<p>If the file you are accessing is in the root directory of your s3 bucket, you can access the file like this:<\/p>\n\n<pre><code>import pandas as pd\n\nbucket='ltfs1'\ndata_key = 'data.csv'\ndata_location = 's3:\/\/{}\/{}'.format(bucket, data_key)\ntraining_data = pd.read_csv(data_location)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56863907",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1562166882852,
        "Question_original_content":"set path bucket amazon jupyt notebook new aw set path bucket access file bucket need chang prefix import import boto import import copi import time time import gmtime strftime import execut role role execut role region boto session region bucket ltf replac bucket prefix ltf path bucket store data bucket path http amazonaw com format region bucket url access bucket code show file error",
        "Question_preprocessed_content":"set path bucket amazon jupyt notebook new aw set path bucket access file bucket need chang prefix code show file error",
        "Question_gpt_summary_original":"The user is facing challenges in setting the path of their bucket in Amazon SageMaker Jupyter Notebook and accessing files from that bucket. They have provided a code snippet that they are using, but it is resulting in a \"file not found\" error. The user is seeking guidance on whether they need to change anything with the prefix.",
        "Question_gpt_summary":"user face challeng set path bucket jupyt notebook access file bucket provid code snippet result file error user seek guidanc need chang prefix",
        "Answer_original_content":"file access root directori bucket access file like import panda bucket ltf data kei data csv data locat format bucket data kei train data read csv data locat",
        "Answer_preprocessed_content":"file access root directori bucket access file like",
        "Answer_gpt_summary_original":"Solution: The user can access the file in the root directory of their s3 bucket by using the code snippet provided in the discussion. They need to replace the 'ltfs1' with their own bucket name and 'data.csv' with the name of the file they want to access.",
        "Answer_gpt_summary":"solut user access file root directori bucket code snippet provid discuss need replac ltf bucket data csv file want access"
    },
    {
        "Question_title":"Sagemaker multi-model endpoints with unsupported built-in algorithms",
        "Question_body":"<p>I am aware that Sagemaker <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/multi-model-endpoints.html#multi-model-support\" rel=\"nofollow noreferrer\">does not support multi-model endpoints for their built-in image classification algorithm<\/a>. However, in the documentation they hint at building a custom container to use &quot;any other framework or algorithm&quot; with the multi-model endpoint functionality:<\/p>\n<blockquote>\n<p>To use any other framework or algorithm, use the SageMaker inference toolkit to build a container that supports multi-model endpoints. For information, see <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/build-multi-model-build-container.html\" rel=\"nofollow noreferrer\">Build Your Own Container with Multi Model Server<\/a>.<\/p>\n<\/blockquote>\n<p>Ideally, I would like to deploy many (20+) image classification models I have already trained to a single endpoint to save on costs. However, after reading the &quot;Build Your Own Container&quot; guide it is still not exactly clear to me how to build a custom inference container for the models produced by a non-custom algorithm. Most of the tutorials and example notebooks refer to using Pytorch or Sklearn. It is not clear to me that I could make inferences using these libraries on the models I've created with the built-in image classification algorithm.<\/p>\n<p><em>Is<\/em> it possible to create a container to support multi-model endpoints for unsupported built-in Sagemaker algorithms? If so, would somebody be able to hint at how this might be done?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1611192699733,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":524.0,
        "Answer_body":"<p>yes, it is possible to deploy the built in image classification models as a SageMaker multi model endpoint. The key is that the image classification uses <a href=\"https:\/\/mxnet.apache.org\/versions\/1.7.0\/\" rel=\"nofollow noreferrer\">Apache MXNet<\/a>. You can extract the model artifacts (SageMaker stores them in a zip file named model.tar.gz in S3), then load them in to MXNet. The SageMaker MXNet container supports multi model endpoints, so you can use that to deploy the model.<\/p>\n<p>If you unzip the model.tar.gz from this algorithm, you'll find three files:<\/p>\n<p>image-classification-****.params<\/p>\n<p>image-classification-symbol.json<\/p>\n<p>model-shapes.json<\/p>\n<p>The MxNet container expects these files to be named <strong>image-classification-0000.params, model-symbol.json, and model-shapes.json<\/strong>. So I unzipped the zip file, renamed the files and rezipped them. For more information on the MXNet container check out the <a href=\"https:\/\/github.com\/aws\/sagemaker-mxnet-inference-toolkit\" rel=\"nofollow noreferrer\">GitHub repository<\/a>.<\/p>\n<p>After that you can deploy the model as a single MXNet endpoint using the SageMaker SDK with the following code:<\/p>\n<pre><code>from sagemaker import get_execution_role\nfrom sagemaker.mxnet.model import MXNetModel\n\nrole = get_execution_role()\n\nmxnet_model = MXNetModel(model_data=s3_model, role=role, \n                         entry_point='built_in_image_classifier.py', \n                         framework_version='1.4.1',\n                         py_version='py3')\n\npredictor = mxnet_model.deploy(instance_type='ml.c4.xlarge', initial_instance_count=1)\n<\/code><\/pre>\n<p>The entry point Python script can be an empty Python file for now. We will be using the default inference handling provided by the MXNet container.<\/p>\n<p>The default MXNet container only accepts JSON, CSV, and Numpy arrays as valid input. So you will have to format your input in to one of these three formats. The code below demonstrates how I did it with Numpy arrays:<\/p>\n<pre><code>import cv2\nimport io\n\nnp_array = cv2.imread(filename=img_filename)\nnp_array = np_array.transpose((2,0,1))\nnp_array = np.expand_dims(np_array, axis=0)\n\nbuffer = io.BytesIO()\nnp.save(buffer, np_array)\n\nresponse = sm.invoke_endpoint(EndpointName='Your_Endpoint_name', Body=buffer.getvalue(), ContentType='application\/x-npy')\n<\/code><\/pre>\n<p>Once you have a single endpoint working with MXNet container, you should be able to get it running in multi model endpoint using the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/multi_data_model.html\" rel=\"nofollow noreferrer\">SageMaker MultiDataModel constructor<\/a>.<\/p>\n<p>If you want to use a different input data type so you don't have to do the preprocessing in your application code, you can overwrite the input_fn method in the MxNet container by providing it in the entry_point script. <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/mxnet\/using_mxnet.html\" rel=\"nofollow noreferrer\">See here<\/a> for more information. If you do this, you could pass the image bytes directly to SageMaker, without formatting the numpy arrays.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65819978",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1612378891027,
        "Question_original_content":"multi model endpoint unsupport built algorithm awar support multi model endpoint built imag classif algorithm document hint build custom contain us framework algorithm multi model endpoint function us framework algorithm us infer toolkit build contain support multi model endpoint inform build contain multi model server ideal like deploi imag classif model train singl endpoint save cost read build contain guid exactli clear build custom infer contain model produc non custom algorithm tutori exampl notebook refer pytorch sklearn clear infer librari model creat built imag classif algorithm possibl creat contain support multi model endpoint unsupport built algorithm somebodi abl hint",
        "Question_preprocessed_content":"endpoint unsupport algorithm awar support endpoint imag classif algorithm document hint build custom contain us framework algorithm endpoint function us framework algorithm us infer toolkit build contain support endpoint inform build contain multi model server ideal like deploi imag classif model train singl endpoint save cost read build contain guid exactli clear build custom infer contain model produc algorithm tutori exampl notebook refer pytorch sklearn clear infer librari model creat imag classif algorithm possibl creat contain support endpoint unsupport algorithm somebodi abl hint",
        "Question_gpt_summary_original":"The user wants to deploy multiple image classification models to a single endpoint in Sagemaker to save on costs, but the built-in image classification algorithm does not support multi-model endpoints. The documentation suggests building a custom container using the SageMaker inference toolkit, but the user is unsure how to do this for models produced by a non-custom algorithm. The user is seeking guidance on whether it is possible to create a container for unsupported built-in Sagemaker algorithms and how to do it.",
        "Question_gpt_summary":"user want deploi multipl imag classif model singl endpoint save cost built imag classif algorithm support multi model endpoint document suggest build custom contain infer toolkit user unsur model produc non custom algorithm user seek guidanc possibl creat contain unsupport built algorithm",
        "Answer_original_content":"ye possibl deploi built imag classif model multi model endpoint kei imag classif us apach mxnet extract model artifact store zip file name model tar load mxnet mxnet contain support multi model endpoint us deploi model unzip model tar algorithm file imag classif param imag classif symbol json model shape json mxnet contain expect file name imag classif param model symbol json model shape json unzip zip file renam file rezip inform mxnet contain check github repositori deploi model singl mxnet endpoint sdk follow code import execut role mxnet model import mxnetmodel role execut role mxnet model mxnetmodel model data model role role entri point built imag classifi framework version version predictor mxnet model deploi instanc type xlarg initi instanc count entri point python script python file default infer handl provid mxnet contain default mxnet contain accept json csv numpi arrai valid input format input format code demonstr numpi arrai import import arrai imread filenam img filenam arrai arrai transpos arrai expand dim arrai axi buffer bytesio save buffer arrai respons invok endpoint endpointnam endpoint bodi buffer getvalu contenttyp applic npy singl endpoint work mxnet contain abl run multi model endpoint multidatamodel constructor want us differ input data type preprocess applic code overwrit input method mxnet contain provid entri point script inform pass imag byte directli format numpi arrai",
        "Answer_preprocessed_content":"ye possibl deploi built imag classif model multi model endpoint kei imag classif us apach mxnet extract model artifact load mxnet mxnet contain support multi model endpoint us deploi model unzip algorithm file mxnet contain expect file name unzip zip file renam file rezip inform mxnet contain check github repositori deploi model singl mxnet endpoint sdk follow code entri point python script python file default infer handl provid mxnet contain default mxnet contain accept json csv numpi arrai valid input format input format code demonstr numpi arrai singl endpoint work mxnet contain abl run multi model endpoint multidatamodel constructor want us differ input data type preprocess applic code overwrit method mxnet contain provid script inform pass imag byte directli format numpi arrai",
        "Answer_gpt_summary_original":"Solution: It is possible to deploy the built-in image classification models as a SageMaker multi-model endpoint by extracting the model artifacts and loading them into MXNet. The MXNet container supports multi-model endpoints, so it can be used to deploy the model. The user can deploy the model as a single MXNet endpoint using the SageMaker SDK and format the input into JSON, CSV, or Numpy arrays. Once a single endpoint is working with the MXNet container, it can be run in a multi-model endpoint using the SageMaker MultiDataModel constructor. If the user wants to use a different input data type, they can overwrite the input_fn method in the MXNet container by providing it in the entry_point script.",
        "Answer_gpt_summary":"solut possibl deploi built imag classif model multi model endpoint extract model artifact load mxnet mxnet contain support multi model endpoint deploi model user deploi model singl mxnet endpoint sdk format input json csv numpi arrai singl endpoint work mxnet contain run multi model endpoint multidatamodel constructor user want us differ input data type overwrit input method mxnet contain provid entri point script"
    },
    {
        "Question_title":"Optuna hyperparameter search not reproducible with interrupted \/ resumed studies",
        "Question_body":"<p>For big ML models with many parameters, it is helpful if one can interrupt and resume the hyperparameter optimization search.\nOptuna allows doing that with the RDB backend, which stores the study in a SQlite database (<a href=\"https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/001_rdb.html#sphx-glr-tutorial-20-recipes-001-rdb-py\" rel=\"nofollow noreferrer\">https:\/\/optuna.readthedocs.io\/en\/stable\/tutorial\/20_recipes\/001_rdb.html#sphx-glr-tutorial-20-recipes-001-rdb-py<\/a>).<\/p>\n<p>However, when interrupting and resuming a study, the results are not the same as that of an uninterrupted study.<\/p>\n<p><strong>Expect<\/strong>: For a fixed seed, the results from an optimization run with <code>n_trials = x<\/code> are identical to a study with <code>n_trials = x\/5<\/code>, that is resumed 5 times and a study, that is interrupted with <code>KeyboardInterrupt<\/code> 5 times and resumed 5 times until <code>n_trials = x<\/code>.<\/p>\n<p><strong>Actual<\/strong>: The results are equal up to the point of the first interruption. From then on, they differ.<\/p>\n<p>The <a href=\"https:\/\/i.stack.imgur.com\/ijIzl.png\" rel=\"nofollow noreferrer\">figures<\/a> show the optimization history of all trials in a study. The left-most figure (A) shows the uninterrupted run, the center one shows a run interrupted by keyboard (B), the right-most figure shows the run interrupted by <code>n_iter<\/code> (C). In B and C, the red dotted line shows the point where the first study was first interrupted. Left of the line, the results are equal to the uninterrupted study, to the right they differ.<\/p>\n<p>Is it possible to interrupt and resume a study, so that another study with the same seed that has not been interrupted generates exactly the same result?\n(Obviously assuming that the objective function behaves in a non-deterministic way.)<\/p>\n<p>Minimal working example to reproduce:<\/p>\n<pre><code>import optuna\nimport logging\nimport sys\nimport numpy as np\n\ndef objective(trial):\n    x = trial.suggest_float(&quot;x&quot;, -10, 10)\n    return (x - 4) ** 2\n\ndef set_study(db_name, \n                study_name, \n                seed, \n                direction=&quot;minimize&quot;):\n    '''\n    Creates a new study in a sqlite database located in results\/ .\n    The study can be resumed after keyboard interrupt by simple creating it\n    using the same command used for the initial creation.\n    '''\n\n    # Add stream handler of stdout to show the messages\n    optuna.logging.get_logger(&quot;optuna&quot;).addHandler(logging.StreamHandler(sys.stdout))\n    sampler = optuna.samplers.TPESampler(seed = seed, n_startup_trials = 0)\n    storage_name = f&quot;sqlite:\/\/\/{db_name}.db&quot;\n    storage = optuna.storages.RDBStorage(storage_name, heartbeat_interval=1)\n\n    study = optuna.create_study(storage=storage, \n                                study_name=study_name, \n                                sampler=sampler, \n                                direction=direction, \n                                load_if_exists=True)\n    return study\n\n\nstudy = set_study('optuna_test', 'optuna_test_study', 1)\n\ntry:\n    # Press CTRL+C to stop the optimization.\n    study.optimize(objective, n_trials=100)  \nexcept KeyboardInterrupt:\n    pass\n\n\ndf = study.trials_dataframe(attrs=(&quot;number&quot;, &quot;value&quot;, &quot;params&quot;, &quot;state&quot;))\n\nprint(df)\n\nprint(&quot;Best params: &quot;, study.best_params)\nprint(&quot;Best value: &quot;, study.best_value)\nprint(&quot;Best Trial: &quot;, study.best_trial)\n# print(&quot;Trials: &quot;, study.trials)\n\n\nfig = optuna.visualization.plot_optimization_history(study)\nfig.show()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660225615907,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1661591436152,
        "Question_score":0.0,
        "Question_view_count":92.0,
        "Answer_body":"<p>Found whats causing the problem: The random number generator in the sampler is initialized using the seed, but of course it returns a different number if the study is interrupted and resumed (it is then reinitialised)\nThis is especially bad using random search with fixed seed, as the search then basically starts from new.<\/p>\n<p>If one really needs reproducible runs, one can simply extract the rng into a binary file after a run or keyboard interrupt, and resume by overwriting the newly generated rng of the sampler with the saved one.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73321808",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Question_closed_time":1661932276243,
        "Question_original_content":"hyperparamet search reproduc interrupt resum studi big model paramet help interrupt resum hyperparamet optim search allow rdb backend store studi sqlite databas http readthedoc stabl tutori recip rdb html sphx glr tutori recip rdb interrupt resum studi result uninterrupt studi expect fix seed result optim run trial ident studi trial resum time studi interrupt keyboardinterrupt time resum time trial actual result equal point interrupt differ figur optim histori trial studi left figur show uninterrupt run center show run interrupt keyboard right figur show run interrupt iter red dot line show point studi interrupt left line result equal uninterrupt studi right differ possibl interrupt resum studi studi seed interrupt gener exactli result obvious assum object function behav non determinist wai minim work exampl reproduc import import log import sy import numpi def object trial trial suggest float return def set studi studi seed direct minim creat new studi sqlite databas locat result studi resum keyboard interrupt simpl creat command initi creation add stream handler stdout messag log logger addhandl log streamhandl sy stdout sampler sampler tpesampl seed seed startup trial storag sqlite storag storag rdbstorag storag heartbeat interv studi creat studi storag storag studi studi sampler sampler direct direct load exist true return studi studi set studi test test studi try press ctrl stop optim studi optim object trial keyboardinterrupt pass studi trial datafram attr number valu param state print print best param studi best param print best valu studi best valu print best trial studi best trial print trial studi trial fig visual plot optim histori studi fig",
        "Question_preprocessed_content":"hyperparamet search reproduc interrupt resum studi big model paramet help interrupt resum hyperparamet optim search allow rdb backend store studi sqlite databas interrupt resum studi result uninterrupt studi expect fix seed result optim run ident studi resum time studi interrupt time resum time actual result equal point interrupt differ figur optim histori trial studi figur show uninterrupt run center show run interrupt keyboard figur show run interrupt red dot line show point studi interrupt left line result equal uninterrupt studi right differ possibl interrupt resum studi studi seed interrupt gener exactli result obvious assum object function behav minim work exampl reproduc",
        "Question_gpt_summary_original":"The user is facing challenges with Optuna hyperparameter search not being reproducible with interrupted\/resumed studies. Although Optuna allows interrupting and resuming the hyperparameter optimization search, the results are not the same as that of an uninterrupted study. The figures show that the results are equal up to the point of the first interruption, but from then on, they differ. The user is looking for a way to interrupt and resume a study so that another study with the same seed that has not been interrupted generates exactly the same result.",
        "Question_gpt_summary":"user face challeng hyperparamet search reproduc interrupt resum studi allow interrupt resum hyperparamet optim search result uninterrupt studi figur result equal point interrupt differ user look wai interrupt resum studi studi seed interrupt gener exactli result",
        "Answer_original_content":"what caus problem random number gener sampler initi seed cours return differ number studi interrupt resum reinitialis especi bad random search fix seed search basic start new need reproduc run simpli extract rng binari file run keyboard interrupt resum overwrit newli gener rng sampler save",
        "Answer_preprocessed_content":"what caus problem random number gener sampler initi seed cours return differ number studi interrupt resum especi bad random search fix seed search basic start new need reproduc run simpli extract rng binari file run keyboard interrupt resum overwrit newli gener rng sampler save",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to extract the random number generator (rng) into a binary file after a run or keyboard interrupt and resume by overwriting the newly generated rng of the sampler with the saved one. This can ensure reproducibility of the hyperparameter optimization search even with interrupted\/resumed studies.",
        "Answer_gpt_summary":"solut solut mention discuss extract random number gener rng binari file run keyboard interrupt resum overwrit newli gener rng sampler save ensur reproduc hyperparamet optim search interrupt resum studi"
    },
    {
        "Question_title":"Where should I report to when encounter a trouble at SageMaker Canvas?",
        "Question_body":"When I was building model for analyzing in SageMaker Canvas, it just run for 1h 2m and then I got this notification:\n\nModel building failed: Failed to run Neo compilation or generate explainability report. client_request_id is f76d5bf7-6780-4257-9631-500101632b1e\n\nWhere should I contact the admin for this issue? Thank you so much!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1644763229468,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":63.0,
        "Answer_body":"Hi Ailee, you can submit a ticket here and engineering will get back to you: https:\/\/t.corp.amazon.com\/create\/templates\/20b56bae-3fca-4281-9b94-69b6e50128cd. Thanks!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUpsGPPvV7SbuofE1fnwC5AA\/where-should-i-report-to-when-encounter-a-trouble-at-sage-maker-canvas",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1644863324414,
        "Question_original_content":"report encount troubl canva build model analyz canva run got notif model build fail fail run neo compil gener explain report client request fdbf contact admin issu thank",
        "Question_preprocessed_content":"report encount troubl canva build model analyz canva run got notif model build fail fail run neo compil gener explain report contact admin issu thank",
        "Question_gpt_summary_original":"The user encountered an issue while building a model in SageMaker Canvas, receiving a notification that the model building failed due to a problem with Neo compilation or generating an explainability report. The user is seeking guidance on where to report the issue to the admin.",
        "Question_gpt_summary":"user encount issu build model canva receiv notif model build fail problem neo compil gener explain report user seek guidanc report issu admin",
        "Answer_original_content":"aile submit ticket engin http corp amazon com creat templat bbae fca becd thank",
        "Answer_preprocessed_content":"aile submit ticket engin thank",
        "Answer_gpt_summary_original":"Solution: The user can submit a ticket to report the issue to the engineering team by using the provided link.",
        "Answer_gpt_summary":"solut user submit ticket report issu engin team provid link"
    },
    {
        "Question_title":"how to use sagemaker inside pyspark",
        "Question_body":"<p>I have a simple requirement, I need to run sagemaker prediction inside a spark job<\/p>\n<p>am trying to run the below<\/p>\n<pre><code>ENDPOINT_NAME = &quot;MY-ENDPOINT_NAME&quot;\nfrom sagemaker_pyspark import SageMakerModel\nfrom sagemaker_pyspark import EndpointCreationPolicy\nfrom sagemaker_pyspark.transformation.serializers import ProtobufRequestRowSerializer\nfrom sagemaker_pyspark.transformation.deserializers import ProtobufResponseRowDeserializer\n\nattachedModel = SageMakerModel(\n    existingEndpointName=ENDPOINT_NAME,\n    endpointCreationPolicy=EndpointCreationPolicy.DO_NOT_CREATE,\n    endpointInstanceType=None,  # Required\n    endpointInitialInstanceCount=None,  # Required\n    requestRowSerializer=ProtobufRequestRowSerializer(\n        featuresColumnName=&quot;featureCol&quot;\n    ),  # Optional: already default value\n    responseRowDeserializer= ProtobufResponseRowDeserializer(schema=ouput_schema),\n)\n\ntransformedData2 = attachedModel.transform(df)\ntransformedData2.show()\n<\/code><\/pre>\n<p>I get the following error <code>TypeError: 'JavaPackage' object is not callable<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1662667047127,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":18.0,
        "Answer_body":"<p>this was solved by ...<\/p>\n<pre><code>classpath = &quot;:&quot;.join(sagemaker_pyspark.classpath_jars())\nconf = SparkConf() \\\n    .set(&quot;spark.driver.extraClassPath&quot;, classpath)\nsc = SparkContext(conf=conf)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73654460",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1662672827940,
        "Question_original_content":"us insid pyspark simpl requir need run predict insid spark job try run endpoint endpoint pyspark import model pyspark import endpointcreationpolici pyspark transform serial import protobufrequestrowseri pyspark transform deseri import protobufresponserowdeseri attachedmodel model existingendpointnam endpoint endpointcreationpolici endpointcreationpolici creat endpointinstancetyp requir endpointinitialinstancecount requir requestrowseri protobufrequestrowseri featurescolumnnam featurecol option default valu responserowdeseri protobufresponserowdeseri schema ouput schema transformeddata attachedmodel transform transformeddata follow error typeerror javapackag object callabl",
        "Question_preprocessed_content":"us insid pyspark simpl requir need run predict insid spark job try run follow error",
        "Question_gpt_summary_original":"The user is facing a challenge in running sagemaker prediction inside a spark job using the provided code. They are encountering a TypeError with the message \"'JavaPackage' object is not callable\".",
        "Question_gpt_summary":"user face challeng run predict insid spark job provid code encount typeerror messag javapackag object callabl",
        "Answer_original_content":"solv classpath join pyspark classpath jar conf sparkconf set spark driver extraclasspath classpath sparkcontext conf conf",
        "Answer_preprocessed_content":"solv",
        "Answer_gpt_summary_original":"Solution: The challenge was solved by setting the classpath and SparkConf in the provided code as shown above.",
        "Answer_gpt_summary":"solut challeng solv set classpath sparkconf provid code shown"
    },
    {
        "Question_title":"Saving data with DataCatalog",
        "Question_body":"<p>I was looking at <code>iris<\/code> project example provided by kedro. Apart from logging the accuracy I also wanted to save the <code>predictions<\/code> and <code>test_y<\/code> as a csv.<\/p>\n<p>This is the example node provided by kedro.<\/p>\n<pre><code>def report_accuracy(predictions: np.ndarray, test_y: pd.DataFrame) -&gt; None:\n    &quot;&quot;&quot;Node for reporting the accuracy of the predictions performed by the\n    previous node. Notice that this function has no outputs, except logging.\n    &quot;&quot;&quot;\n    # Get true class index\n    target = np.argmax(test_y.to_numpy(), axis=1)\n    # Calculate accuracy of predictions\n    accuracy = np.sum(predictions == target) \/ target.shape[0]\n    # Log the accuracy of the model\n    log = logging.getLogger(__name__)\n    log.info(&quot;Model accuracy on test set: %0.2f%%&quot;, accuracy * 100)\n<\/code><\/pre>\n<p>I added the following to save the data.<\/p>\n<pre><code>data = pd.DataFrame({&quot;target&quot;: target , &quot;prediction&quot;: predictions})\ndata_set = CSVDataSet(filepath=&quot;data\/test.csv&quot;)\ndata_set.save(data)\n<\/code><\/pre>\n<p>This works as intended, however, my question is &quot;is it the kedro way of doing thing&quot; ? Can I provide the <code>data_set <\/code> in <code>catalog.yml<\/code> and later save <code>data<\/code> to it? If I want to do it, how do I access the <code>data_set<\/code> from <code>catalog.yml<\/code> inside a node.<\/p>\n<p>Is there a way to save data without creating a catalog inside a node like this <code>data_set = CSVDataSet(filepath=&quot;data\/test.csv&quot;)<\/code> ? I want this in <code>catalog.yml<\/code>, if possible and if it follows kedro convention!.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1629897723887,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1629897818943,
        "Question_score":0.0,
        "Question_view_count":333.0,
        "Answer_body":"<p>Kedro actually abstracts this part for you. You don't need to access the datasets via their Python API.<\/p>\n<p>Your <code>report_accuracy<\/code> method does need to be tweaked to return the <code>DataFrame<\/code> instead of <code>None<\/code>.<\/p>\n<p>Your node needs to be defined as such:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>node(\n  func=report_accuracy,\n  inputs='dataset_a',\n  outputs='dataset_b'\n)\n<\/code><\/pre>\n<p>Kedro then looks at your catalog and will load\/save <code>dataset_a<\/code> and <code>dataset_b<\/code> as required:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset_a:\n   type: pandas.CSVDataSet\n   path: xxxx.csv\n\ndataset_b:\n   type: pandas.ParquetDataSet\n   path: yyyy.pq\n<\/code><\/pre>\n<p>As you run the node\/pipeline Kedro will handle the load\/save operations for you. You also don't need to save every dataset if it's only used mid-way in a pipeline, you can read about <code>MemoryDataSet<\/code>s <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/11_tools_integration\/01_pyspark.html#use-memorydataset-for-intermediary-dataframe\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1643038306147,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68923747",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1629898742263,
        "Question_original_content":"save data datacatalog look iri project exampl provid apart log accuraci want save predict test csv exampl node provid def report accuraci predict ndarrai test datafram node report accuraci predict perform previou node notic function output log true class index target argmax test numpi axi calcul accuraci predict accuraci sum predict target target shape log accuraci model log log getlogg log info model accuraci test set accuraci ad follow save data data datafram target target predict predict data set csvdataset filepath data test csv data set save data work intend question wai thing provid data set catalog yml later save data want access data set catalog yml insid node wai save data creat catalog insid node like data set csvdataset filepath data test csv want catalog yml possibl follow convent",
        "Question_preprocessed_content":"save data datacatalog look project exampl provid apart log accuraci want save csv exampl node provid ad follow save data work intend question wai thing provid later save want access insid node wai save data creat catalog insid node like want possibl follow convent",
        "Question_gpt_summary_original":"The user encountered a challenge in saving data in the Kedro project example provided by Kedro. They were able to save the data as intended, but they are unsure if it is the \"Kedro way\" of doing things. They are looking for a way to provide the data set in catalog.yml and access it from a node, without creating a catalog inside a node.",
        "Question_gpt_summary":"user encount challeng save data project exampl provid abl save data intend unsur wai thing look wai provid data set catalog yml access node creat catalog insid node",
        "Answer_original_content":"actual abstract need access dataset python api report accuraci method need tweak return datafram instead node need defin node func report accuraci input dataset output dataset look catalog load save dataset dataset requir dataset type panda csvdataset path csv dataset type panda parquetdataset path run node pipelin handl load save oper need save dataset mid wai pipelin read memorydataset",
        "Answer_preprocessed_content":"actual abstract need access dataset python api method need tweak return instead node need defin look catalog requir run handl oper need save dataset pipelin read",
        "Answer_gpt_summary_original":"Solution: The user can define their node with inputs and outputs in the catalog.yml file and Kedro will handle the load\/save operations for them. They can also use MemoryDataSet for intermediary dataframes if the dataset is only used mid-way in a pipeline.",
        "Answer_gpt_summary":"solut user defin node input output catalog yml file handl load save oper us memorydataset intermediari datafram dataset mid wai pipelin"
    },
    {
        "Question_title":"Access to NCasT4_v3-series and ND A100 v4-series VMs",
        "Question_body":"How could I request quota for the NCasT4_v3-series and ND A100 v4-series VMs for Machine Learning services and as regular VMs\n\nThey both do not appear as an option on the usual form to request quota increase in any of the 4 US regions I looked\n\nThanks\n\nManuel",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1629921673763,
        "Question_favorite_count":22.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi @ManuelReyesGomez-1028 ,\n\nthe VM series NCasT4_v3 and ND A100 v4 are only available in 3 US regions (both series together)\n\nSource: https:\/\/azure.microsoft.com\/en-us\/global-infrastructure\/services\/?products=virtual-machines&regions=us-central,us-east,us-east-2,us-north-central,us-south-central,us-west-central,us-west,us-west-2,us-west-3\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/528034\/access-to-ncast4-v3-series-and-nd-a100-v4-series-v.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1629922551630,
        "Question_original_content":"access ncast seri seri vm request quota ncast seri seri vm machin learn servic regular vm appear option usual form request quota increas region look thank manuel",
        "Question_preprocessed_content":"access vm request quota vm machin learn servic regular vm appear option usual form request quota increas region look thank manuel",
        "Question_gpt_summary_original":"The user is facing challenges in requesting quota for NCasT4_v3-series and ND A100 v4-series VMs for machine learning services and regular VMs as they do not appear as an option on the usual form to request quota increase in any of the 4 US regions.",
        "Question_gpt_summary":"user face challeng request quota ncast seri seri vm machin learn servic regular vm appear option usual form request quota increas region",
        "Answer_original_content":"manuelreyesgomez seri ncast avail region seri sourc http azur microsoft com global infrastructur servic product virtual machin region central east east north central south central west central west west west repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_preprocessed_content":"seri avail region sourc repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_gpt_summary_original":"Solution: The VM series NCasT4_v3 and ND A100 v4 are only available in 3 US regions. Therefore, they cannot be requested in the usual form to request quota increase in any of the 4 US regions.",
        "Answer_gpt_summary":"solut seri ncast avail region request usual form request quota increas region"
    },
    {
        "Question_title":"failing to create image in azure ml workspace",
        "Question_body":"<p>I am able to create image and run azure ml service in one env but when I am moving to another env its not able to create image and failing with this error -<\/p>\n<p>Message: Received bad response from Model Management Service:\nResponse Code: 500\n{&quot;code&quot;:&quot;InternalServerError&quot;,&quot;statusCode&quot;:500,&quot;message&quot;:&quot;An internal server error occurred. Please try again. If the problem persists, contact support.&quot;,&quot;correlation&quot;:{&quot;RequestId&quot;:&quot;8667981d-ef71-4e7c-a735-c43ef07b51b8&quot;}}'<\/p>\n<p>these logs are not helpful to find issue<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1627079442010,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":110.0,
        "Answer_body":"<p>As the error message said, this issue is an internal issue, please raise a support ticket to assign a support engineer to investigate it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1629745090063,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68505595",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1628037949483,
        "Question_original_content":"fail creat imag workspac abl creat imag run servic env move env abl creat imag fail error messag receiv bad respons model manag servic respons code code internalservererror statuscod messag intern server error occur try problem persist contact support correl requestid cefbb log help issu",
        "Question_preprocessed_content":"fail creat imag workspac abl creat imag run servic env move env abl creat imag fail error messag receiv bad respons model manag servic respons code code internalservererror statuscod messag intern server error occur try problem persist contact log help issu",
        "Question_gpt_summary_original":"The user is facing challenges in creating an image in Azure ML workspace. They are able to create an image and run Azure ML service in one environment, but when moving to another environment, it fails with an internal server error message. The logs provided are not helpful in identifying the issue.",
        "Question_gpt_summary":"user face challeng creat imag workspac abl creat imag run servic environ move environ fail intern server error messag log provid help identifi issu",
        "Answer_original_content":"error messag said issu intern issu rais support ticket assign support engin investig",
        "Answer_preprocessed_content":"error messag said issu intern issu rais support ticket assign support engin investig",
        "Answer_gpt_summary_original":"Solution: The suggested solution is to raise a support ticket to assign a support engineer to investigate the internal server error message. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut suggest solut rais support ticket assign support engin investig intern server error messag solut mention discuss"
    },
    {
        "Question_title":"Why is AWS Sagemaker notebook instance designed to only persist data under ~\/Sagemaker?",
        "Question_body":"<p>In my current job we use AWS managed notebooks on Sagemaker EC2. I am largely okay with the user experience but the lack of data persistency outside <code>~\/Sagemaker<\/code> has been quite inconvenient. Every time should the instance need restarting, I'd lose all the settings and python packages. Wonder why AWS would make this particular decision for Sagemaker. Have used Google Cloud's AI platform before and it does not have such settings and my configurations would always persist.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1625466236780,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":266.0,
        "Answer_body":"<p>I faced a similar issue on other AWS services. Usually for managed services AWS uses read-only containers approach and leave just one folder of the filesystem for read\/write that persist across the stop\/restart cycle.\nReguarding the packages installation the seems to be to install your custom environment on the notebook instance's Amazon EBS volume, as described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-add-external.html\" rel=\"nofollow noreferrer\">here<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68251533",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1625487892607,
        "Question_original_content":"notebook instanc design persist data current job us aw manag notebook larg okai user experi lack data persist outsid inconveni time instanc need restart lose set python packag wonder aw particular decis googl cloud platform set configur persist",
        "Question_preprocessed_content":"notebook instanc design persist data current job us aw manag notebook larg okai user experi lack data persist outsid inconveni time instanc need restart lose set python packag wonder aw particular decis googl cloud platform set configur persist",
        "Question_gpt_summary_original":"The user is facing challenges with AWS Sagemaker notebook instance as it is designed to only persist data under ~\/Sagemaker, causing inconvenience every time the instance needs restarting and resulting in loss of settings and python packages. The user wonders why AWS made this decision for Sagemaker, as compared to Google Cloud's AI platform which does not have such limitations.",
        "Question_gpt_summary":"user face challeng notebook instanc design persist data caus inconveni time instanc need restart result loss set python packag user wonder aw decis compar googl cloud platform limit",
        "Answer_original_content":"face similar issu aw servic usual manag servic aw us read contain approach leav folder filesystem read write persist stop restart cycl reguard packag instal instal custom environ notebook instanc amazon eb volum describ",
        "Answer_preprocessed_content":"face similar issu aw servic usual manag servic aw us contain approach leav folder filesystem persist cycl reguard packag instal instal custom environ notebook instanc amazon eb volum describ",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include using the read-only containers approach for managed services and installing custom environment on the notebook instance's Amazon EBS volume to persist packages across stop\/restart cycles.",
        "Answer_gpt_summary":"possibl solut mention discuss includ read contain approach manag servic instal custom environ notebook instanc amazon eb volum persist packag stop restart cycl"
    },
    {
        "Question_title":"Azure for students showing no usage despite using it",
        "Question_body":"Hello,\n\nI've created an azure for students account wiht $100 free credit and started using Azure Notebooks to train some ML models. I've created a GPU instance which costs $1.20\/hr. I've been using it for at least 1.5h now and what's weird is that no usage is being shown on my dashboard, and on the sponsorship page it's showing that it is not active and that I haven't used any of my credit:\n\n\n\n\nOn the other hand when I go to my subscriptions it says it's active:\n\nIs something wrong or does it take a while to see usage statistics\/credit spending?\n\nThanks in advance.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1662893566663,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi,\n\nUsually it is every 4 hours the data\/cost is updated so check after sometime, you can check and download the data by using and following the steps over here - download-azure-daily-usage\n\n\n\n\n\n==\nPlease \"Accept the answer\" if the information helped you. This will help us and others in the community as well.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1002201\/azure-for-students-showing-no-usage-despite-using.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1662904704207,
        "Question_original_content":"azur student show usag despit hello creat azur student account wiht free credit start azur notebook train model creat gpu instanc cost weird usag shown dashboard sponsorship page show activ haven credit hand subscript sai activ wrong usag statist credit spend thank advanc",
        "Question_preprocessed_content":"azur student show usag despit hello creat azur student account wiht free credit start azur notebook train model creat gpu instanc cost weird usag shown dashboard sponsorship page show activ haven credit hand subscript sai activ wrong usag spend thank advanc",
        "Question_gpt_summary_original":"The user has encountered a challenge with their Azure for students account, where they have been using Azure Notebooks to train ML models on a GPU instance that costs $1.20\/hr, but no usage is being shown on their dashboard and the sponsorship page shows that it is not active and they haven't used any credit. However, the subscriptions page shows that it is active, and the user is unsure if something is wrong or if it takes time to see usage statistics\/credit spending.",
        "Question_gpt_summary":"user encount challeng azur student account azur notebook train model gpu instanc cost usag shown dashboard sponsorship page show activ haven credit subscript page show activ user unsur wrong take time usag statist credit spend",
        "Answer_original_content":"usual hour data cost updat check check download data follow step download azur daili usag accept answer inform help help commun",
        "Answer_preprocessed_content":"usual hour updat check check download data follow step accept answer inform help help commun",
        "Answer_gpt_summary_original":"Solution: The user is advised to wait for 4 hours as the data\/cost is updated every 4 hours. They can check and download the data by following the steps provided in the link - download-azure-daily-usage. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user advis wait hour data cost updat hour check download data follow step provid link download azur daili usag solut mention discuss"
    },
    {
        "Question_title":"Amazon Sagemaker: write your own inference",
        "Question_body":"<p>I was evaluating what is needed to write your own Estimator in Sagemaker. I was following this example <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\/container\" rel=\"nofollow noreferrer\">here<\/a> and it's well explained and quite simple.<\/p>\n<p>My question is regarding the inference <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py\" rel=\"nofollow noreferrer\">here<\/a>. I see an example in which we can feed the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/e648e9a6f596263c7683635d1a55f1729b08277d\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L60\" rel=\"nofollow noreferrer\">invocations endpoint<\/a> a CSV. What if I want to just post a string or even individual parameters? What's the best practise for that? I see there is a condition like:<\/p>\n<pre><code>if flask.request.content_type == &quot;text\/csv&quot;:\n<\/code><\/pre>\n<p>Should we add more like those to support different formats or should we create a new endpoint?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1630420918507,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":168.0,
        "Answer_body":"<p>You need to add support for more content types.<\/p>\n<p>Since you would like to pass a string or a parameter, I suggest you add support for &quot;application\/json&quot; MIME media type (<a href=\"https:\/\/stackoverflow.com\/questions\/477816\/what-is-the-correct-json-content-type\">What is the correct JSON content type?<\/a>). Then your users will call the API with a Json that you can parse and extract parameters from in the backend.<\/p>\n<p>For example, if you have two parameters <code>age<\/code> and <code>gender<\/code> you want to pass to your model. You can put them in the following Json datastructure:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n &quot;age&quot;: ...,\n &quot;gender&quot;: ...\n}\n<\/code><\/pre>\n<p>Then add support for loading the Json and extracting the parameters in the backend as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>if flask.request.content_type == &quot;application\/json&quot;:\n    data = flask.request.data.decode(&quot;utf-8&quot;)\n    data = json.loads(data)\n    parameter1 = data['age']\n    parameter2 = data['gender']\n    ...\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1631097716312,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69000752",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1631096060112,
        "Question_original_content":"write infer evalu need write estim follow exampl explain simpl question infer exampl feed invoc endpoint csv want post string individu paramet best practis condit like flask request content type text csv add like support differ format creat new endpoint",
        "Question_preprocessed_content":"write infer evalu need write estim follow exampl explain simpl question infer exampl feed invoc endpoint csv want post string individu paramet best practis condit like add like support differ format creat new endpoint",
        "Question_gpt_summary_original":"The user is evaluating how to write their own Estimator in Amazon Sagemaker and is following a well-explained example. However, they have a question regarding the inference and how to post a string or individual parameters instead of a CSV. They are unsure whether to add more conditions to support different formats or create a new endpoint.",
        "Question_gpt_summary":"user evalu write estim follow explain exampl question infer post string individu paramet instead csv unsur add condit support differ format creat new endpoint",
        "Answer_original_content":"need add support content type like pass string paramet suggest add support applic json mime media type correct json content type user api json pars extract paramet backend exampl paramet ag gender want pass model follow json datastructur ag gender add support load json extract paramet backend follow flask request content type applic json data flask request data decod utf data json load data paramet data ag paramet data gender",
        "Answer_preprocessed_content":"need add support content type like pass string paramet suggest add support mime media type user api json pars extract paramet backend exampl paramet want pass model follow json datastructur add support load json extract paramet backend follow",
        "Answer_gpt_summary_original":"Solution: The solution suggested is to add support for \"application\/json\" MIME media type to pass a string or individual parameters. The user can parse and extract parameters from the Json data structure in the backend. The code snippet provided shows how to load the Json and extract the parameters in the backend.",
        "Answer_gpt_summary":"solut solut suggest add support applic json mime media type pass string individu paramet user pars extract paramet json data structur backend code snippet provid show load json extract paramet backend"
    },
    {
        "Question_title":"How to install Vowpal Wabbit on Amazon EC2 or SageMaker? (amazon linux)",
        "Question_body":"<p>Can someone explain or help me install <a href=\"https:\/\/github.com\/VowpalWabbit\/vowpal_wabbit\/tree\/master\/python\" rel=\"nofollow noreferrer\">vowpalwabbit<\/a> (I'm interested in the python bindings) on an Amazon linux machine, either EC2 or SageMaker?\nfor some reason it is very hard and I can't find anything about it online...<\/p>\n\n<p>a <code>pip install vowpalwabbit<\/code> returns a <\/p>\n\n<pre><code>Using cached https:\/\/files.pythonhosted.org\/packages\/d1\/5a\/9fcd64fd52ad22e2d1821b2ef871e8783c324b37e2103e7ddefa776c2ed7\/vowpalwabbit-8.8.0.tar.gz\nBuilding wheels for collected packages: vowpalwabbit\n  Building wheel for vowpalwabbit (setup.py) ... error\n  ERROR: Command errored out with exit status 1:\n   command: \/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/python -u -c 'import sys, setuptools, tokenize; sys.argv[0]= '\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"'; __file__='\"'\"'\/tmp\/pip-install-tvp1174t\/vowpalwabbit\/setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' bdist_wheel -d \/tmp\/pip-wheel-x0j85ac_ --python-tag cp36\n       cwd: \/tmp\/pip-install-tvp1174t\/vowpalwabbit\/\n<\/code><\/pre>\n\n<p>lower in the error I can also see a:<\/p>\n\n<pre><code>CMake Error at \/usr\/lib64\/python3.6\/dist-packages\/cmake\/data\/share\/cmake-3.13\/Modules\/FindBoost.cmake:2100 (message):\n    Unable to find the requested Boost libraries.\n\n    Boost version: 1.53.0\n\n    Boost include path: \/usr\/include\n\n    Could not find the following Boost libraries:\n\n            boost_python3\n\n    Some (but not all) of the required Boost libraries were found.  You may\n    need to install these additional Boost libraries.  Alternatively, set\n    BOOST_LIBRARYDIR to the directory containing Boost libraries or BOOST_ROOT\n    to the location of Boost.\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1580567889550,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":367.0,
        "Answer_body":"<p>Tested again 1.5 years later, and a <code>pip install vowpalwabbit<\/code> works fine on notebook instance. In training job, adding vowpalwabbit in a <code>requirements.txt<\/code> send to an AWS-managed Scikit learn container (<code>141502667606.dkr.ecr.eu-west-1.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3<\/code>) also installs successfully. Both tested with vowpalwabbit-8.11.0<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60017893",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1634653018680,
        "Question_original_content":"instal vowpal wabbit amazon amazon linux explain help instal vowpalwabbit interest python bind amazon linux machin reason hard onlin pip instal vowpalwabbit return cach http file pythonhost org packag fcdfdadedbefecbeeddefac vowpalwabbit tar build wheel collect packag vowpalwabbit build wheel vowpalwabbit setup error error command error exit statu command home user anaconda env jupytersystemenv bin python import sy setuptool token sy argv tmp pip instal tvpt vowpalwabbit setup file tmp pip instal tvpt vowpalwabbit setup getattr token open open file code read replac close exec compil code file exec bdist wheel tmp pip wheel xjac python tag cwd tmp pip instal tvpt vowpalwabbit lower error cmake error usr lib python dist packag cmake data share cmake modul findboost cmake messag unabl request boost librari boost version boost includ path usr includ follow boost librari boost python requir boost librari need instal addit boost librari altern set boost librarydir directori contain boost librari boost root locat boost",
        "Question_preprocessed_content":"instal vowpal wabbit amazon explain help instal vowpalwabbit amazon linux machin reason hard return lower error",
        "Question_gpt_summary_original":"The user is facing challenges in installing Vowpal Wabbit on an Amazon Linux machine, either EC2 or SageMaker. The user tried to install it using \"pip install vowpalwabbit\" but encountered an error related to CMake and Boost libraries. The user is seeking help to install Vowpal Wabbit with Python bindings.",
        "Question_gpt_summary":"user face challeng instal vowpal wabbit amazon linux machin user tri instal pip instal vowpalwabbit encount error relat cmake boost librari user seek help instal vowpal wabbit python bind",
        "Answer_original_content":"test year later pip instal vowpalwabbit work fine notebook instanc train job ad vowpalwabbit requir txt send aw manag scikit learn contain dkr ecr west amazonaw com scikit learn cpu instal successfulli test vowpalwabbit",
        "Answer_preprocessed_content":"test year later work fine notebook instanc train job ad vowpalwabbit send scikit learn contain instal successfulli test",
        "Answer_gpt_summary_original":"Solutions provided: \n- The user can try installing Vowpal Wabbit using \"pip install vowpalwabbit\" on an Amazon Linux machine, either EC2 or SageMaker. This solution was tested again 1.5 years later and worked fine on a notebook instance. \n- In a training job, adding vowpalwabbit in a requirements.txt file and sending it to an AWS-managed Scikit learn container also installs successfully.",
        "Answer_gpt_summary":"solut provid user try instal vowpal wabbit pip instal vowpalwabbit amazon linux machin solut test year later work fine notebook instanc train job ad vowpalwabbit requir txt file send aw manag scikit learn contain instal successfulli"
    },
    {
        "Question_title":"Vertex AI - Viewing Pipeline Output",
        "Question_body":"<p>I have followed <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/schedule-cloud-scheduler\" rel=\"nofollow noreferrer\">this<\/a> tutorial to create my first scheduled Vertex AI Pipeline to run every minute. The only thing it does is prints <code>&quot;Hello, &lt;any-greet-string&gt;&quot;<\/code> and also returns this same string. I can see that it is running because the last run time updates and the last run result is &quot;Success&quot; every time.<\/p>\n<p>My question is very simple: Where can I see this string printed and the output of my pipeline?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1639659363547,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1639726542407,
        "Question_score":2.0,
        "Question_view_count":346.0,
        "Answer_body":"<p>The output of the <code>print()<\/code> statements in the pipeline can be found in &quot;Cloud Logging&quot; with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click &quot;<strong>VIEW LOGS<\/strong>&quot; in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Refer to the below screenshot.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/xCDnf.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>I ran a sample <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#4\" rel=\"nofollow noreferrer\">pipeline<\/a> from this codelab, <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro#0\" rel=\"nofollow noreferrer\">Intro to Vertex Pipelines<\/a> and below is the output for one of the <code>print()<\/code> statements in the pipeline.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/T0C3S.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<br \/>\n<h4>UPDATE:<\/h4>\n<p>Every component in a pipeline run is deployed as an individual Vertex AI custom job. Corresponding to the sample pipeline consisting 3 components, there are 3 entries in the &quot;CUSTOM JOBS&quot; section as shown below.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/1OD4q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Therefore, to view the logs on the run level, we would need to query the log entries with the respective <code>job_id<\/code>s of the pipeline components and the <code>job_id<\/code> of the Cloud Scheduler job. The query would look like this.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.labels.job_id=(&quot;JOB_ID_1&quot; OR &quot;JOB_ID_2&quot; [OR &quot;JOB_ID_N&quot;...])\nseverity&gt;=DEFAULT\n<\/code><\/pre>\n<p>If there are no simultaneous pipeline runs, a simpler query like below can be used.<\/p>\n<pre class=\"lang-sql prettyprint-override\"><code>resource.type=(&quot;cloud_scheduler_job&quot; OR &quot;ml_job&quot;)\nseverity&gt;=DEFAULT\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1639750456896,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70379395",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1639667359983,
        "Question_original_content":"view pipelin output follow tutori creat schedul pipelin run minut thing print hello return string run run time updat run result success time question simpl string print output pipelin",
        "Question_preprocessed_content":"view pipelin output follow tutori creat schedul pipelin run minut thing print return string run run time updat run result success time question simpl string print output pipelin",
        "Question_gpt_summary_original":"The user has successfully created a scheduled Vertex AI Pipeline that prints a greeting string and returns the same string. However, the user is facing a challenge in locating the output of the pipeline and the printed string.",
        "Question_gpt_summary":"user successfulli creat schedul pipelin print greet string return string user face challeng locat output pipelin print string",
        "Answer_original_content":"output print statement pipelin cloud log appropri filter check log compon pipelin click respect compon consol click view log right pane new pane log open pipelin page allow output compon refer screenshot ran sampl pipelin codelab intro vertex pipelin output print statement pipelin updat compon pipelin run deploi individu custom job correspond sampl pipelin consist compon entri custom job section shown view log run level need queri log entri respect job id pipelin compon job cloud schedul job queri look like resourc label job job job job sever default simultan pipelin run simpler queri like resourc type cloud schedul job job sever default",
        "Answer_preprocessed_content":"output statement pipelin cloud log appropri filter check log compon pipelin click respect compon consol click view log right pane new pane log open pipelin page allow output compon refer screenshot ran sampl pipelin codelab intro vertex pipelin output statement pipelin updat compon pipelin run deploi individu custom job correspond sampl pipelin consist compon entri custom job section shown view log run level need queri log entri respect pipelin compon cloud schedul job queri look like simultan pipelin run simpler queri like",
        "Answer_gpt_summary_original":"The solution to locating the output of the pipeline and the printed string is to check the Cloud Logging with the appropriate filters. To check logs for each component in the pipeline, click on the respective component in the console and click \"VIEW LOGS\" in the right pane. A new pane with the logs will open in the pipeline page which will allow us to see the output of the component. Additionally, every component in a pipeline run is deployed as an individual Vertex AI custom job, so to view the logs on the run level, we would need to query the log entries with the respective job_ids of the pipeline components and the job_id of the Cloud Scheduler job.",
        "Answer_gpt_summary":"solut locat output pipelin print string check cloud log appropri filter check log compon pipelin click respect compon consol click view log right pane new pane log open pipelin page allow output compon addition compon pipelin run deploi individu custom job view log run level need queri log entri respect job id pipelin compon job cloud schedul job"
    },
    {
        "Question_title":"RunHistory finalization failed: ServiceException: Code: 400",
        "Question_body":"I am trying to log my model metrics and hyperparam space in the run metrics.\nI tried to also reduce the variable lengths throughout but still get consistently the following error:\ni find in the documentation that we can only have 15 columns for every row in the run metric.\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-limits-quotas-capacity . But is there a way to increase this capacity? Thanks\n\nIs there a way to increase this capacity?\n\n RunHistory finalization failed: ServiceException:\n  Code: 400\n  Message: (UserError) A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  Details:\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Dto.Columns\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Value.Data\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Value.Data\/Count, Limit=15, Size=16. See https:\/\/aka.ms\/azure-machine-learning-limits for service limits documentation.\n  A field of the entity is over the size limit. FieldName=MetricV2Value.Data\/Count, Limit=15, Size=16. See",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1643880887487,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@AntaraDas-4298 Some of the limits are soft limits which can be increased for a subscription or a workspace. Usually these limits can be increased by using a support case with appropriate usage scenario mentioned in the details of the case. Once the case is submitted it is reviewed by the service team and the limits are increased if it is possible to do so.\n\nPlease create a support case from Azure portal and use the following settings from the drop downs and mention the summary detail as \"Increase limit of columns per metric row\"\n\nIf you do not have a valid support subscription we could help you with a one time free support case that could help you to create one for this scenario.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/721282\/runhistory-finalization-failed-serviceexception-co.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1643903062356,
        "Question_original_content":"runhistori final fail serviceexcept code try log model metric hyperparam space run metric tri reduc variabl length consist follow error document column row run metric http doc microsoft com azur machin learn resourc limit quota capac wai increas capac thank wai increas capac runhistori final fail serviceexcept code messag usererror field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document detail field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvdto column count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvvalu data count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvvalu data count limit size http aka azur machin learn limit servic limit document field entiti size limit fieldnam metricvvalu data count limit size",
        "Question_preprocessed_content":"runhistori final fail serviceexcept code try log model metric hyperparam space run metric tri reduc variabl length consist follow error document column row run metric wai increas capac thank wai increas capac runhistori final fail serviceexcept code messag field entiti size limit limit size servic limit document detail field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size servic limit document field entiti size limit limit size",
        "Question_gpt_summary_original":"The user is encountering an error message \"RunHistory finalization failed: ServiceException: Code: 400\" while trying to log model metrics and hyperparameter space in the run metrics. The error message indicates that the user has exceeded the limit of 15 columns for every row in the run metric. The user is seeking a way to increase this capacity.",
        "Question_gpt_summary":"user encount error messag runhistori final fail serviceexcept code try log model metric hyperparamet space run metric error messag indic user exceed limit column row run metric user seek wai increas capac",
        "Answer_original_content":"antarada limit soft limit increas subscript workspac usual limit increas support case appropri usag scenario mention detail case case submit review servic team limit increas possibl creat support case azur portal us follow set drop down mention summari increas limit column metric row valid support subscript help time free support case help creat scenario answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"limit soft limit increas subscript workspac usual limit increas support case appropri usag scenario mention detail case case submit review servic team limit increas possibl creat support case azur portal us follow set drop down mention summari increas limit column metric row valid support subscript help time free support case help creat scenario answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Solution: The user can increase the limit of columns per metric row by creating a support case from the Azure portal and mentioning the summary detail as \"Increase limit of columns per metric row\". If the user does not have a valid support subscription, they can get a one-time free support case to create one for this scenario. The support team will review the case and increase the limits if possible. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user increas limit column metric row creat support case azur portal mention summari increas limit column metric row user valid support subscript time free support case creat scenario support team review case increas limit possibl solut mention discuss"
    },
    {
        "Question_title":"Azure ML Text Classification Import Text Files",
        "Question_body":"Hi,\n\nI'm trying to create an Azure Machine Learning model to classify text files. I have hundreds of text files that have been organized into a subfolder named its correct label. Similar to how you train Image classification.\n\nHow would I get this data into a data set. I have been trying to use the python sdk since I was able to successfully get the Image classification to work.\n\nThanks,\nKyle",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1659040614790,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@DomsohnKyle-2610 Thanks for the question. Here is the sample to import text files and explore azure ml text classification.\n\nhttps:\/\/github.com\/microsoft\/nlp-recipes\/blob\/master\/examples\/text_classification\/tc_bert_azureml.ipynb",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/946690\/azure-ml-text-classification-import-text-files.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1659586063576,
        "Question_original_content":"text classif import text file try creat model classifi text file hundr text file organ subfold name correct label similar train imag classif data data set try us python sdk abl successfulli imag classif work thank kyle",
        "Question_preprocessed_content":"text classif import text file try creat model classifi text file hundr text file organ subfold name correct label similar train imag classif data data set try us python sdk abl successfulli imag classif work thank kyle",
        "Question_gpt_summary_original":"The user is facing challenges in importing text files into Azure Machine Learning to create a text classification model. They have organized the text files into subfolders based on their correct label and are seeking guidance on how to get this data into a dataset using the Python SDK.",
        "Question_gpt_summary":"user face challeng import text file creat text classif model organ text file subfold base correct label seek guidanc data dataset python sdk",
        "Answer_original_content":"domsohnkyl thank question sampl import text file explor text classif http github com microsoft nlp recip blob master exampl text classif bert ipynb",
        "Answer_preprocessed_content":"thank question sampl import text file explor text classif",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to a sample code on GitHub that demonstrates how to import text files and explore Azure ML text classification using the Python SDK.",
        "Answer_gpt_summary":"solut discuss provid link sampl code github demonstr import text file explor text classif python sdk"
    },
    {
        "Question_title":"How to plot multiline in one plot with smoothing features?",
        "Question_body":"<p>I\u2019m trying to plot the figure as in [W&amp;B Smoothing Features], but it didn\u2019t provide any code:<\/p>\n<p>                    <a href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9b0793db422eac619667bd11a7c56351d9d69149.png\" target=\"_blank\" rel=\"noopener nofollow ugc\" class=\"onebox\">\n            <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/9b0793db422eac619667bd11a7c56351d9d69149.png\" width=\"690\" height=\"301\">\n          <\/a>\n\n<\/p>\n<p>Tutorials I could find by searching <code>wandb multiline in one plot<\/code> is [Custom Multi-Line Plots] which introduces <code>wandb.plot.line_series()<\/code>.  So I tried the code following<\/p>\n<pre><code class=\"lang-python\">import wandb\nimport numpy as np\n\n\nwandb.init(project=\"test\", entity=\"xxxx\")\n\nwandb.log({\"my_custom_id\":\n           wandb.plot.line_series(\n               xs=range(100),\n               ys=[range(100), np.random.randint(100, size=100)],\n               keys=[\"y1\", \"y2\"],\n               title=\"Multiline\",\n               xname=\"steps\"\n           )})\n<\/code><\/pre>\n<p>It gives me the following pic after choosing <code>Edit panel<\/code><\/p>\n<p>                    <a href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/4e32b0d8ec1fb96df36dd2a7609f129875cb9e8e.png\" target=\"_blank\" rel=\"noopener nofollow ugc\" class=\"onebox\">\n            <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/4e32b0d8ec1fb96df36dd2a7609f129875cb9e8e.png\" width=\"690\" height=\"351\">\n          <\/a>\n\n<\/p>\n<p>Unlike the first picture:<\/p>\n<ol>\n<li>It <strong>doesn\u2019t<\/strong> have <code>Data<\/code>, <code>Group<\/code> etc tabs.<\/li>\n<li>There are <strong>two types<\/strong> of legend <code>name<\/code> and <code>lineKey<\/code> rather than one type.<\/li>\n<\/ol>\n<p>My question is how to plot exactly the same as the first picture with same function supported in wandb web?<\/p>",
        "Question_answer_count":5,
        "Question_comment_count":0,
        "Question_created_time":1639275305880,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":274.0,
        "Answer_body":"<p>First I need to log the data I want<\/p>\n<pre><code class=\"lang-python\">import random\nimport wandb\n\nwandb.init(project=\"test\", entity=\"xxxx\")\nfor i in range(100):\n    wandb.log({\"y1\": random.random(), \"y2\": random.random(), \"x\": i})\n<\/code><\/pre>\n<p>Then, I need to mannually choosing y1 and y2 on Y Axis.<\/p>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c.jpeg\" data-download-href=\"\/uploads\/short-url\/3mBq6eTgdFLO5BAFCPEQmOUHYoQ.jpeg?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_690x262.jpeg\" alt=\"image\" data-base62-sha1=\"3mBq6eTgdFLO5BAFCPEQmOUHYoQ\" width=\"690\" height=\"262\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_690x262.jpeg, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_1035x393.jpeg 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_1380x524.jpeg 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/1794a161b9d71b6745d9a5c7f4beff76003a4d5c_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1807\u00d7687 214 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/how-to-plot-multiline-in-one-plot-with-smoothing-features\/1512",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1639528370760,
        "Question_original_content":"plot multilin plot smooth featur try plot figur smooth featur didnt provid code tutori search multilin plot custom multi line plot introduc plot line seri tri code follow import import numpi init project test entiti log custom plot line seri rang rang random randint size kei titl multilin xname step give follow pic choos edit panel unlik pictur doesnt data group tab type legend linekei type question plot exactli pictur function support web",
        "Question_preprocessed_content":"plot multilin plot smooth featur try plot figur didnt provid code tutori search introduc tri code follow give follow pic choos unlik pictur doesnt tab type legend type question plot exactli pictur function support web",
        "Question_gpt_summary_original":"The user is trying to plot a multiline graph with smoothing features using wandb.plot.line_series() function. However, the resulting graph does not have the same features as the desired graph, such as the Data and Group tabs and a single type of legend. The user is seeking guidance on how to plot the graph with the same features as the desired graph.",
        "Question_gpt_summary":"user try plot multilin graph smooth featur plot line seri function result graph featur desir graph data group tab singl type legend user seek guidanc plot graph featur desir graph",
        "Answer_original_content":"need log data want import random import init project test entiti rang log random random random random need mannual choos axi imag",
        "Answer_preprocessed_content":"need log data want need mannual choos axi imag",
        "Answer_gpt_summary_original":"Solution:\n- Log the data using wandb.log() function.\n- Manually choose y1 and y2 on Y Axis to plot the graph with the desired features.",
        "Answer_gpt_summary":"solut log data log function manual choos axi plot graph desir featur"
    },
    {
        "Question_title":"Azure MachineLearning WebService Not Using Passed .ilearner Model",
        "Question_body":"<p>We are currently working on making an Azure MachineLearning Studio experiment operational.<\/p>\n\n<p>Our most recent iteration has a webjob that accepts a queue message, gets some data to train the model, and consumes the ML Experiment webservice to put a trained model in a blob location.<\/p>\n\n<p>A second webjob accepts a queue message, pulls the data to be used in the predictive experiment, gets the location path of the trained .ilearner model, and then consumes THAT ML Experiment webservice.<\/p>\n\n<p>The data used to make the predictions is passed in as an input parameter, and the storage account name, key, and .ilearner path are all passed in as global parameters--Dictionary objects defined according to what the data scientist provided.<\/p>\n\n<p>Everything <em>appears<\/em> to work correctly--except in some cases, the predictive experiment fails, and the error message makes it clear the wrong .ilearner file is being used.<\/p>\n\n<p>When a non-existent blob path is passed to the experiment webservice, the error message reflects there is no such blob, so it's clear the webservice is at least validating the .ilearner's existence. <\/p>\n\n<p>The data scientist can run it locally, but has to change the name of the .ilearner file when he exports it locally through PowerShell. Ensuring each trained model has a unique file name did not resolve this issue.<\/p>\n\n<p>All files, when I view them in the Azure Storage Explorer, appear to be getting updated as expected based on last-modified dates. It's almost like there's a cached version of the .ilearner somewhere that isn't being overridden properly.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1535139575903,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":65.0,
        "Answer_body":"<p>After ruling out all possibility of passing in the wrong file, our data scientist took a closer look at the experiment itself. He discovered that it was defaulting to one hardcoded .ilearner path he had been using in development.<\/p>\n\n<p>At one point in time, he had created webservice parameters to override this value (hence why I had them defined in my webservice call), but they had been removed during one of the redesigns of the experiment with anyone noticing, because the webservice will apparently accept superfluous arguments.<\/p>\n\n<p><strong>The webservice was accepting my global parameters<\/strong>, and apparently even validating them. But since they weren't wired to anything inside <strong>the experiment the passed .ilearner file info was never applied to anything<\/strong>--the hardcoded .ilearner was being applied no matter what.<\/p>\n\n<p>We were all very surprised there was no exception thrown about passing in parameters to the webservice that weren't actually defined. Had <em>that<\/em> happened, we would have gotten to the bottom of it much more quickly.<\/p>\n\n<p>tl\/dr: The experiment wasn't properly configured to accept an .ilearner file path (or Account Name, or Account Key) as a parameter, and the webservice was happily accepting and ignoring the parameter arguments without raising any alarm since it had the hardcoded value to run with.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52010761",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1535139575903,
        "Question_original_content":"azur machinelearn webservic pass ilearn model current work make azur machinelearn studio experi oper recent iter webjob accept queue messag get data train model consum experi webservic train model blob locat second webjob accept queue messag pull data predict experi get locat path train ilearn model consum experi webservic data predict pass input paramet storag account kei ilearn path pass global paramet dictionari object defin accord data scientist provid appear work correctli case predict experi fail error messag make clear wrong ilearn file non exist blob path pass experi webservic error messag reflect blob clear webservic valid ilearn exist data scientist run local chang ilearn file export local powershel ensur train model uniqu file resolv issu file view azur storag explor appear get updat expect base modifi date like cach version ilearn isn overridden properli",
        "Question_preprocessed_content":"azur machinelearn webservic pass ilearn model current work make azur machinelearn studio experi oper recent iter webjob accept queue messag get data train model consum experi webservic train model blob locat second webjob accept queue messag pull data predict experi get locat path train ilearn model consum experi webservic data predict pass input paramet storag account kei ilearn path pass global object defin accord data scientist provid appear work case predict experi fail error messag make clear wrong ilearn file blob path pass experi webservic error messag reflect blob clear webservic valid ilearn exist data scientist run local chang ilearn file export local powershel ensur train model uniqu file resolv issu file view azur storag explor appear get updat expect base date like cach version ilearn isn overridden properli",
        "Question_gpt_summary_original":"The user is facing challenges with the Azure MachineLearning Studio experiment, where the predictive experiment fails due to the wrong .ilearner file being used. The webservice is validating the .ilearner's existence, but the issue persists even after ensuring each trained model has a unique file name. The user suspects that there might be a cached version of the .ilearner somewhere that isn't being overridden properly.",
        "Question_gpt_summary":"user face challeng azur machinelearn studio experi predict experi fail wrong ilearn file webservic valid ilearn exist issu persist ensur train model uniqu file user suspect cach version ilearn isn overridden properli",
        "Answer_original_content":"rule possibl pass wrong file data scientist took closer look experi discov default hardcod ilearn path develop point time creat webservic paramet overrid valu defin webservic remov redesign experi notic webservic appar accept superflu argument webservic accept global paramet appar valid weren wire insid experi pass ilearn file info appli hardcod ilearn appli matter surpris except thrown pass paramet webservic weren actual defin happen gotten quickli experi wasn properli configur accept ilearn file path account account kei paramet webservic happili accept ignor paramet argument rais alarm hardcod valu run",
        "Answer_preprocessed_content":"rule possibl pass wrong file data scientist took closer look experi discov default hardcod ilearn path develop point time creat webservic paramet overrid valu remov redesign experi notic webservic appar accept superflu argument webservic accept global paramet appar valid weren wire insid experi pass ilearn file info appli hardcod ilearn appli matter surpris except thrown pass paramet webservic weren actual defin happen gotten quickli experi wasn properli configur accept ilearn file path paramet webservic happili accept ignor paramet argument rais alarm hardcod valu run",
        "Answer_gpt_summary_original":"Solution: The issue was caused by the experiment defaulting to a hardcoded .ilearner path that was being used in development. The webservice was accepting global parameters, but they were not wired to anything inside the experiment, so the passed .ilearner file info was never applied to anything. The experiment was not properly configured to accept an .ilearner file path as a parameter, and the webservice was accepting and ignoring the parameter arguments without raising any alarm since it had the hardcoded value to run with.",
        "Answer_gpt_summary":"solut issu caus experi default hardcod ilearn path develop webservic accept global paramet wire insid experi pass ilearn file info appli experi properli configur accept ilearn file path paramet webservic accept ignor paramet argument rais alarm hardcod valu run"
    },
    {
        "Question_title":"How to use azureml.core.runconfig.DockerConfiguration class in azureml.core.Environment or azureml.core.ScriptRunConfig class",
        "Question_body":"<p>I use Microsoft Azure Machine Learning (Azure-ml) to run my (python) experiments.<\/p>\n<p>For specifying the VM and python environment I use:<\/p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core import ScriptRunConfig\n\n# Other imports and code...\n\n# Specify VM and Python environment:\nvm_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)\nvm_env.docker.enabled = True\nvm_env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'\n\n# Finally, use the environment in the ScriptRunConfig:\nsrc = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,\n                      script=SCRIPT_FILE_TO_EXECUTE,\n                      arguments=EXECUTE_ARGUMENTS,\n                      compute_target=compute_target,\n                      environment=vm_env)\n<\/code><\/pre>\n<p>I get the following warning for the line <code>vm_env.docker.enabled = True<\/code>:<\/p>\n<pre><code>'enabled' is deprecated. Please use the azureml.core.runconfig.DockerConfiguration object with the 'use_docker' param instead.\n<\/code><\/pre>\n<p>The documentation about the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.environment.dockersection?view=azure-ml-py\" rel=\"noreferrer\"><code>DockerSection Class<\/code><\/a> and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.runconfig.dockerconfiguration?view=azure-ml-py\" rel=\"noreferrer\"><code>DockerConfiguration Class<\/code><\/a> is not very clear about applying the <code>DockerConfiguration Class<\/code>.<\/p>\n<p><strong>I can't figure out how to use the <code>azureml.core.runconfig.DockerConfiguration<\/code> object. Can someone provide me with an example? Thank you!<\/strong><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1620140343337,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1620630619488,
        "Question_score":8.0,
        "Question_view_count":2662.0,
        "Answer_body":"<p>The <code>ScriptRunConfig<\/code> class now accepts a <code>docker_runtime_config<\/code> argument, which is where you pass the <code>DockerConfiguration<\/code> object.<\/p>\n<p>So, the code would look something like this:<\/p>\n<pre><code>from azureml.core import Environment\nfrom azureml.core import ScriptRunConfig\nfrom azureml.core.runconfig import DockerConfiguration\n\n# Other imports and code...\n\n# Specify VM and Python environment:\nvm_env = Environment.from_conda_specification(name='my-test-env', file_path=PATH_TO_YAML_FILE)\nvm_env.docker.base_image = 'mcr.microsoft.com\/azureml\/openmpi3.1.2-cuda10.2-cudnn7-ubuntu18.04'\n\ndocker_config = DockerConfiguration(use_docker=True)\n\n# Finally, use the environment in the ScriptRunConfig:\nsrc = ScriptRunConfig(source_directory=DEPLOY_CONTAINER_FOLDER_PATH,\n                      script=SCRIPT_FILE_TO_EXECUTE,\n                      arguments=EXECUTE_ARGUMENTS,\n                      compute_target=compute_target,\n                      environment=vm_env,\n                      docker_runtime_config=docker_config)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1620638535880,
        "Answer_score":12.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67387249",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1620311022350,
        "Question_original_content":"us core runconfig dockerconfigur class core environ core scriptrunconfig class us azur run python experi specifi python environ us core import environ core import scriptrunconfig import code specifi python environ env environ conda specif test env file path path yaml file env docker enabl true env docker base imag mcr microsoft com openmpi cuda cudnn ubuntu final us environ scriptrunconfig src scriptrunconfig sourc directori deploi contain folder path script script file execut argument execut argument comput target comput target environ env follow warn line env docker enabl true enabl deprec us core runconfig dockerconfigur object us docker param instead document dockersect class dockerconfigur class clear appli dockerconfigur class figur us core runconfig dockerconfigur object provid exampl thank",
        "Question_preprocessed_content":"us class class us run experi specifi python environ us follow warn line document clear appli figur us object provid exampl thank",
        "Question_gpt_summary_original":"The user is facing challenges in using the azureml.core.runconfig.DockerConfiguration class in azureml.core.Environment or azureml.core.ScriptRunConfig class. The user is getting a warning message for the line vm_env.docker.enabled = True and is unable to figure out how to use the azureml.core.runconfig.DockerConfiguration object. The documentation is not clear about applying the DockerConfiguration Class.",
        "Question_gpt_summary":"user face challeng core runconfig dockerconfigur class core environ core scriptrunconfig class user get warn messag line env docker enabl true unabl figur us core runconfig dockerconfigur object document clear appli dockerconfigur class",
        "Answer_original_content":"scriptrunconfig class accept docker runtim config argument pass dockerconfigur object code look like core import environ core import scriptrunconfig core runconfig import dockerconfigur import code specifi python environ env environ conda specif test env file path path yaml file env docker base imag mcr microsoft com openmpi cuda cudnn ubuntu docker config dockerconfigur us docker true final us environ scriptrunconfig src scriptrunconfig sourc directori deploi contain folder path script script file execut argument execut argument comput target comput target environ env docker runtim config docker config",
        "Answer_preprocessed_content":"class accept argument pass object code look like",
        "Answer_gpt_summary_original":"Solution: The user can use the `docker_runtime_config` argument in the `ScriptRunConfig` class to pass the `DockerConfiguration` object. The code snippet provided in the discussion shows how to use the `DockerConfiguration` object with the `ScriptRunConfig` class.",
        "Answer_gpt_summary":"solut user us docker runtim config argument scriptrunconfig class pass dockerconfigur object code snippet provid discuss show us dockerconfigur object scriptrunconfig class"
    },
    {
        "Question_title":"Commands in the Azure ML yml files",
        "Question_body":"<p>When reading the examples from Microsoft on azure ML CLI v2, they use the symbols:\n&quot;|&quot;, &quot;&gt;&quot;, etc., in their yml files.<\/p>\n<p>What do they mean, and where can I find explanations of possible syntax for the Azure CLI v2 engine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1649142429250,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":100.0,
        "Answer_body":"<p>| - This pipe symbol in YAML document is used for <em><strong>&quot;Multiple line statements&quot;<\/strong><\/em><\/p>\n<pre><code>description: |\n  # Azure Machine Learning &quot;hello world&quot; job\n\n  This is a &quot;hello world&quot; job running in the cloud via Azure Machine Learning!\n\n  ## Description\n\n  Markdown is supported in the studio for job descriptions! You can edit the description there or via CLI.\n<\/code><\/pre>\n<p>in the above example, we need to write some multiple line description. So, we need to use &quot;|&quot; symbol<\/p>\n<p>&quot;&gt;&quot; - This symbol is used to save some content directly to a specific location document.<\/p>\n<pre><code>command: echo &quot;hello world&quot; &gt; .\/outputs\/helloworld.txt\n<\/code><\/pre>\n<p>In this above command, we need to post <strong>&quot;hello world&quot;<\/strong> to <em><strong>&quot;helloworld.txt&quot;<\/strong><\/em><\/p>\n<p>Check the below link for complete documentation regarding YAML files.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-yaml-job-command<\/a><\/p>\n<p>All these symbols are the YAML job commands which are used to accomplish a specific task through CLI.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71747545",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1649231701843,
        "Question_original_content":"command yml file read exampl microsoft cli us symbol yml file mean explan possibl syntax azur cli engin",
        "Question_preprocessed_content":"command yml file read exampl microsoft cli us symbol yml file mean explan possibl syntax azur cli engin",
        "Question_gpt_summary_original":"The user is facing challenges in understanding the symbols used in yml files for Azure ML CLI v2 and is seeking explanations for the possible syntax used in the engine.",
        "Question_gpt_summary":"user face challeng understand symbol yml file cli seek explan possibl syntax engin",
        "Answer_original_content":"pipe symbol yaml document multipl line statement descript hello world job hello world job run cloud descript markdown support studio job descript edit descript cli exampl need write multipl line descript need us symbol symbol save content directli specif locat document command echo hello world output helloworld txt command need post hello world helloworld txt check link complet document yaml file http doc microsoft com azur machin learn refer yaml job command symbol yaml job command accomplish specif task cli",
        "Answer_preprocessed_content":"pipe symbol yaml document multipl line statement exampl need write multipl line descript need us symbol symbol save content directli specif locat document command need post hello world check link complet document yaml file symbol yaml job command accomplish specif task cli",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include using the pipe symbol \"|\" for multiple line statements in YAML documents and using the symbol \">\" to save content directly to a specific location in the document. The discussion also suggests referring to the complete documentation regarding YAML files for more information. No personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss includ pipe symbol multipl line statement yaml document symbol save content directli specif locat document discuss suggest refer complet document yaml file inform person opinion bias includ summari"
    },
    {
        "Question_title":"Fb-Prophet, Apache Spark in Colab and AWS SageMaker\/ Lambda",
        "Question_body":"<p>I am using <code>Google-Colab<\/code> for creating a model by using FbProphet and i am try to use Apache Spark in the <code>Google-Colab<\/code> itself. Now can i upload this <code>Google-colab<\/code> notebook in <code>aws Sagemaker\/Lambda<\/code> for free <code>(without charge for Apache Spark and only charge for AWS SageMaker)<\/code>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658906440657,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1660220920907,
        "Question_score":1.0,
        "Question_view_count":51.0,
        "Answer_body":"<p>In short, You can upload the notebook without any issue into SageMaker. Few things to keep in mind<\/p>\n<ol>\n<li>If you are using the pyspark library in colab and running spark locally,  you should be able to do the same by installing necessary pyspark libs in Sagemaker studio kernels. Here you will only pay for the underlying compute for the notebook instance. If you are experimenting then I would recommend you to use <a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a> to create a free account and try things out.<\/li>\n<li>If you had a separate spark cluster setup then you may need a similar setup in AWS using EMR so that you can connect to the cluster to execute the job.<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73133746",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1658964743500,
        "Question_original_content":"prophet apach spark colab lambda googl colab creat model fbprophet try us apach spark googl colab upload googl colab notebook lambda free charg apach spark charg",
        "Question_preprocessed_content":"apach spark colab lambda creat model fbprophet try us apach spark upload notebook free",
        "Question_gpt_summary_original":"The user is facing challenges in using FbProphet and Apache Spark in Google-Colab to create a model. They are also unsure if they can upload the Google-Colab notebook to AWS SageMaker\/Lambda for free, without being charged for Apache Spark.",
        "Question_gpt_summary":"user face challeng fbprophet apach spark googl colab creat model unsur upload googl colab notebook lambda free charg apach spark",
        "Answer_original_content":"short upload notebook issu thing mind pyspark librari colab run spark local abl instal necessari pyspark lib studio kernel pai underli comput notebook instanc experi recommend us http studiolab aw creat free account try thing separ spark cluster setup need similar setup aw emr connect cluster execut job",
        "Answer_preprocessed_content":"short upload notebook issu thing mind pyspark librari colab run spark local abl instal necessari pyspark lib studio kernel pai underli comput notebook instanc experi recommend us creat free account try thing separ spark cluster setup need similar setup aw emr connect cluster execut job",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- If the user is using the pyspark library in Google-Colab and running Spark locally, they can install necessary pyspark libraries in SageMaker studio kernels and upload the notebook to SageMaker without any issue. They will only pay for the underlying compute for the notebook instance.\n- If the user had a separate Spark cluster setup, they may need a similar setup in AWS using EMR so that they can connect to the cluster to execute the job.\n\nNo personal opinions or biases are included in the response.",
        "Answer_gpt_summary":"possibl solut mention discuss user pyspark librari googl colab run spark local instal necessari pyspark librari studio kernel upload notebook issu pai underli comput notebook instanc user separ spark cluster setup need similar setup aw emr connect cluster execut job person opinion bias includ respons"
    },
    {
        "Question_title":"Adding tags cause internal server error",
        "Question_body":"<p>Adding tags to any project in my account will cause an internal server error. Is there an outage or it\u2019s something specific to my account?<\/p><aside class=\"onebox allowlistedgeneric\" data-onebox-src=\"https:\/\/wandb.ai\/cchi\/tags_test_project\">\n  <header class=\"source\">\n      <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/18b592222b97bc09df3743831bb4929dec23611c.png\" class=\"site-icon\" width=\"32\" height=\"32\">\n\n      <a href=\"https:\/\/wandb.ai\/cchi\/tags_test_project\" target=\"_blank\" rel=\"noopener\">W&amp;B<\/a>\n  <\/header>\n\n  <article class=\"onebox-body\">\n    <img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/7\/7e3d01bfbb9f37b8c9af3076e9688c9bef1b6347.png\" class=\"thumbnail onebox-avatar\" width=\"120\" height=\"120\">\n\n<h3><a href=\"https:\/\/wandb.ai\/cchi\/tags_test_project\" target=\"_blank\" rel=\"noopener\">cchi<\/a><\/h3>\n\n  <p>Weights &amp; Biases, developer tools for machine learning<\/p>\n\n\n  <\/article>\n\n  <div class=\"onebox-metadata\">\n    \n    \n  <\/div>\n\n  <div style=\"clear: both\"><\/div>\n<\/aside>\n<p><div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051.png\" data-download-href=\"\/uploads\/short-url\/o8iWO0KA8b01xdqNjqP8vCU1eUh.png?dl=1\" title=\"Screen Shot 2022-11-01 at 5.32.04 PM\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051_2_690x258.png\" alt=\"Screen Shot 2022-11-01 at 5.32.04 PM\" data-base62-sha1=\"o8iWO0KA8b01xdqNjqP8vCU1eUh\" width=\"690\" height=\"258\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051_2_690x258.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/2X\/a\/a924869568d0c7ad73ea9c46361492d6a4827051.png 2x\" data-dominant-color=\"FBF3F4\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use href=\"#far-image\"><\/use><\/svg><span class=\"filename\">Screen Shot 2022-11-01 at 5.32.04 PM<\/span><span class=\"informations\">789\u00d7296 13.7 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1667338403355,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":213.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/cchi\">@cchi<\/a>,<\/p>\n<p>Could you try adding your tags once again? We were running into some site issues earlier today but they should be resolved now.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/adding-tags-cause-internal-server-error\/3363",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1667362849064,
        "Question_original_content":"ad tag caus intern server error ad tag project account caus intern server error outag specif account cchi develop tool machin learn screen shot",
        "Question_preprocessed_content":"ad tag caus intern server error ad tag project account caus intern server error outag specif account cchi develop tool machin learn screen shot",
        "Question_gpt_summary_original":"The user is encountering an internal server error when trying to add tags to any project in their account. They are unsure if this is due to an outage or if it is specific to their account.",
        "Question_gpt_summary":"user encount intern server error try add tag project account unsur outag specif account",
        "Answer_original_content":"cchi try ad tag run site issu earlier todai resolv thank ramit",
        "Answer_preprocessed_content":"try ad tag run site issu earlier todai resolv thank ramit",
        "Answer_gpt_summary_original":"Solution: The user is advised to try adding the tags again as the site issues that were causing the internal server error have been resolved.",
        "Answer_gpt_summary":"solut user advis try ad tag site issu caus intern server error resolv"
    },
    {
        "Question_title":"How to trigger Azure ML Pipeline from Power Automate",
        "Question_body":"<p>I have a published Azure ML Pipeline that I am trying to trigger from an Automate Flow I have that triggers when users edit a document. Since I have the REST Endpoint for the Published Pipeline, I figured I should be able to make a POST request using the HTTP module available in Power Automate to trigger the pipeline.<\/p>\n<p>However, when I actually try this, I get an authentication error. I assume this is because I need to include some access token with the REST Endpoint, but I can't find any documentation that will tell me where to get that token from. Please note that I do not need to pass any data to the Pipeline, it handles its own data collection, I literally just need a way to trigger it.<\/p>\n<p>Does anybody know how to trigger a Published Azure ML Pipeline using the REST Endpoint? Does it make sense to use the HTTP module, or is there a better way to achieve this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1624934847563,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1624937966803,
        "Question_score":1.0,
        "Question_view_count":305.0,
        "Answer_body":"<p>So I figured out how to do it by following the directions contained within this piece of Microsoft Documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-rest<\/a><\/p>\n<p>Specifically, it required performing two of the calls in the documentation;<\/p>\n<ul>\n<li>The first to get an AAD token using an Azure Service Principle that is authorised to access the Machine Learning Instance.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST <a href=\"https:\/\/login.microsoftonline.com\/\" rel=\"nofollow noreferrer\">https:\/\/login.microsoftonline.com\/<\/a>\/oauth2\/token -d &quot;grant_type=client_credentials&amp;resource=https%3A%2F%2Fmanagement.azure.com%2F&amp;client_id=&amp;client_secret=&quot;<\/p>\n<\/blockquote>\n<ul>\n<li>The second to use this token to trigger your pipeline from its rest endpoint. This one I had to figure out myself a little, but below is the basic structure I used.<\/li>\n<\/ul>\n<blockquote>\n<p>curl -X POST {PIPELINE_REST_ENDPOINT} -H &quot;Authorisation:Bearer {AAD_TOKEN}&quot; -H &quot;Content-Type: application\/json&quot; -d &quot;{&quot;ExperimentName&quot;: &quot;{EXPERIMENT_NAME}&quot;,&quot;ParameterAssignments&quot;: {}}&quot;<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68172002",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1630892629443,
        "Question_original_content":"trigger pipelin power autom publish pipelin try trigger autom flow trigger user edit document rest endpoint publish pipelin figur abl post request http modul avail power autom trigger pipelin actual try authent error assum need includ access token rest endpoint document tell token note need pass data pipelin handl data collect liter need wai trigger anybodi know trigger publish pipelin rest endpoint sens us http modul better wai achiev",
        "Question_preprocessed_content":"trigger pipelin power autom publish pipelin try trigger autom flow trigger user edit document rest endpoint publish pipelin figur abl post request http modul avail power autom trigger pipelin actual try authent error assum need includ access token rest endpoint document tell token note need pass data pipelin handl data collect liter need wai trigger anybodi know trigger publish pipelin rest endpoint sens us http modul better wai achiev",
        "Question_gpt_summary_original":"The user is facing challenges in triggering an Azure ML Pipeline from Power Automate. They are encountering an authentication error and are unsure of how to obtain the necessary access token to include with the REST Endpoint. The user does not need to pass any data to the Pipeline, they just need a way to trigger it. They are seeking advice on how to trigger the Pipeline using the REST Endpoint and whether there is a better way to achieve this.",
        "Question_gpt_summary":"user face challeng trigger pipelin power autom encount authent error unsur obtain necessari access token includ rest endpoint user need pass data pipelin need wai trigger seek advic trigger pipelin rest endpoint better wai achiev",
        "Answer_original_content":"figur follow direct contain piec microsoft document http doc microsoft com azur machin learn manag rest specif requir perform call document aad token azur servic principl authoris access machin learn instanc curl post http login microsoftonlin com oauth token grant type client credenti resourc http fmanag azur com client client secret second us token trigger pipelin rest endpoint figur littl basic structur curl post pipelin rest endpoint authoris bearer aad token content type applic json experimentnam experi parameterassign",
        "Answer_preprocessed_content":"figur follow direct contain piec microsoft document specif requir perform call document aad token azur servic principl authoris access machin learn instanc curl post second us token trigger pipelin rest endpoint figur littl basic structur curl post authoris bearer experimentnam",
        "Answer_gpt_summary_original":"Solution: The user can trigger an Azure ML Pipeline from Power Automate by following the directions contained within the Microsoft Documentation. The solution requires performing two calls, the first to get an AAD token using an Azure Service Principle that is authorized to access the Machine Learning Instance, and the second to use this token to trigger the Pipeline from its REST Endpoint. The user needs to include the necessary access token with the REST Endpoint.",
        "Answer_gpt_summary":"solut user trigger pipelin power autom follow direct contain microsoft document solut requir perform call aad token azur servic principl author access machin learn instanc second us token trigger pipelin rest endpoint user need includ necessari access token rest endpoint"
    },
    {
        "Question_title":"Azure Machine Learning - Uses invalid Pytorch version when training",
        "Question_body":"Hi, I am training my models via Azure Machine Learning.\n\nOn other day, my training is running with GPU support, however today I found my training is running on a CPU.\nI'm not modified training environment, only training script was modified.\nMy computing cluster is NC6v3 - have a GPU.\n\nI investigate a situation, and I found training script is running on PyTorch 1.6.0.\nOn other day, it ran on Pytorch 1.8.1.\nI think my \"don't use GPU\" problem is caused by the situation that CUDA toolkit version is not suitable for Pytorch version.\n\nThen, I output a installed package to the log.\nThe log says 'Pytorch 1.8.1 was installed, however uses 1.6.0'.\nI confused by this weird circumstances.\nCan someone tell me the solution?\n\n<My code snippet>\n<<conda_dependencies.yaml>>\n\nchannels:\n- conda-forge\n- pytorch\n- nvidia\ndependencies:\n- python=3.8.10\n- mesa-libgl-cos6-x86_64\n- cudatoolkit=11.1\n- pytorch==1.8.1\n- torchvision==0.9.1\n- tqdm\n- scikit-learn\n- matplotlib\n- pandas\n- pip < 20.3\n- pip:\n- azureml-defaults\n- opencv-python-headless\n- pillow==8.2.0\n\n<<Environment definition>>\nenvironment_definition_file = experiment_dir \/ 'conda_dependencies.yaml'\nenvironment_name = 'pytorch-1.8.1-gpu'\nbase_image_name = 'mcr.microsoft.com\/azureml\/openmpi4.1.0-cuda11.0.3-cudnn8-ubuntu18.04'\nenvironment = Environment.from_docker_image(environment_name, base_image_name, conda_specification = environment_definition_file)\ndocker_run_config = DockerConfiguration(use_docker=True)\n\nscript_run_config = ScriptRunConfig(\nsource_directory = experiment_dir,\nscript = SCRIPT_FILE_NAME,\narguments = arguments,\ncompute_target = compute_target,\ndocker_runtime_config = docker_run_config,\nenvironment = environment)\n\n<<Output a log in the training script>>\nimport torch\nimport pip\n\npip.main(['list'])\nprint(f'PyTorch version: {torch.version}')\n\n<My logs>\nPackage Version\n\n\nadal 1.2.7\napplicationinsights 0.11.10\n(omission)\ntorch 1.8.1\ntorchvision 0.9.0a0\n(omission)\n\nPyTorch version: 1.6.0",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1629119160593,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. These are the supported versions for PyTorch. Please refer to this document for creating a custom environment. As shown, you'll need to use versions <= 1.6.0. Hope this helps.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/515579\/azure-machine-learning-uses-invalid-pytorch-versio.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1629156299363,
        "Question_original_content":"us invalid pytorch version train train model dai train run gpu support todai train run cpu modifi train environ train script modifi comput cluster ncv gpu investig situat train script run pytorch dai ran pytorch think us gpu problem caus situat cuda toolkit version suitabl pytorch version output instal packag log log sai pytorch instal us confus weird circumst tell solut channel conda forg pytorch nvidia depend python mesa libgl co cudatoolkit pytorch torchvis tqdm scikit learn matplotlib panda pip environ definit file experi dir conda depend yaml environ pytorch gpu base imag mcr microsoft com openmpi cuda cudnn ubuntu environ environ docker imag environ base imag conda specif environ definit file docker run config dockerconfigur us docker true script run config scriptrunconfig sourc directori experi dir script script file argument argument comput target comput target docker runtim config docker run config environ environ import torch import pip pip main list print pytorch version torch version packag version adal applicationinsight omiss torch torchvis omiss pytorch version",
        "Question_preprocessed_content":"us invalid pytorch version train train model dai train run gpu support todai train run cpu modifi train environ train script modifi comput cluster gpu investig situat train script run pytorch dai ran pytorch think us gpu problem caus situat cuda toolkit version suitabl pytorch version output instal packag log log sai pytorch instal us confus weird circumst tell solut channel pytorch nvidia depend tqdm matplotlib panda pip environ scriptrunconfig import torch import pip print packag version adal applicationinsight omiss torch torchvis omiss pytorch version",
        "Question_gpt_summary_original":"The user is facing a challenge with Azure Machine Learning as their training script is running on PyTorch 1.6.0 instead of the required version of 1.8.1, causing their training to run on a CPU instead of a GPU. The user suspects that the issue is caused by an unsuitable CUDA toolkit version for the PyTorch version. The user has output a log in the training script and found that PyTorch 1.8.1 was installed but is using version 1.6.0. The user is seeking a solution to this issue.",
        "Question_gpt_summary":"user face challeng train script run pytorch instead requir version caus train run cpu instead gpu user suspect issu caus unsuit cuda toolkit version pytorch version user output log train script pytorch instal version user seek solut issu",
        "Answer_original_content":"thank reach support version pytorch refer document creat custom environ shown need us version hope help",
        "Answer_preprocessed_content":"thank reach support version pytorch refer document creat custom environ shown need us version hope help",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"azure ml designer: how to call a pipeline from another pipeline",
        "Question_body":"I'm using ML Designer and i have created a sub-pipeline that i want to use it in other pipelines. how do i call that sub-pipeline from the designer?\n\nThe purpose of the subpipeline is to transform data, so the output is a dataset.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1619288778083,
        "Question_favorite_count":7.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@javier-8889Thanks for the question. Can you please add more details about the pipeline steps. You can implement an AML pipeline with Python code, but also with the new AML designer which under the covers is creating an AML Pipeline defining that \u201cvisual workflow\u201d. Basically you can register a trained model in Designer bring it out with SDK\/CLI to deploy it. Currently only Data Drift Monitor (Data Drift->EventGrid->LogicApp->Trigger Pipeline) allows to trigger a pipeline when dataset drift has been detected.\n\nWhen designing a pipeline in Azure ML Designer, each step or module creates intermediate datasets that can be seen using the UI using Visualize option. Those datasets are persisted in the blob storage.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/370433\/azure-ml-designer-how-to-call-a-pipeline-from-anot.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1619449185636,
        "Question_original_content":"design pipelin pipelin design creat sub pipelin want us pipelin sub pipelin design purpos subpipelin transform data output dataset",
        "Question_preprocessed_content":"design pipelin pipelin design creat want us pipelin design purpos subpipelin transform data output dataset",
        "Question_gpt_summary_original":"The user is facing a challenge in calling a sub-pipeline created in Azure ML Designer from other pipelines. The sub-pipeline is designed to transform data and produce a dataset as output.",
        "Question_gpt_summary":"user face challeng call sub pipelin creat design pipelin sub pipelin design transform data produc dataset output",
        "Answer_original_content":"javier thank question add detail pipelin step implement aml pipelin python code new aml design cover creat aml pipelin defin visual workflow basic regist train model design bring sdk cli deploi current data drift monitor data drift eventgrid logicapp trigger pipelin allow trigger pipelin dataset drift detect design pipelin design step modul creat intermedi dataset seen visual option dataset persist blob storag",
        "Answer_preprocessed_content":"question add detail pipelin step implement aml pipelin python code new aml design cover creat aml pipelin defin visual workflow basic regist train model design bring deploi current data drift monitor allow trigger pipelin dataset drift detect design pipelin design step modul creat intermedi dataset seen visual option dataset persist blob storag",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How does google cloud speech to text api deals with invalid inputs ?",
        "Question_body":"I am using google cloud stream(AsyncStreamingRecognize) for speech to text conversion in my applications. I have gone through the below link to understand the structure of response returned by the apis :StreamingRecognizeResponse  I can have various scenarios where I can end up with various invalid scenarios and I do not understand what could be the responses. I can invalid scenarios like :- User speaks in a different language than what is passed in configuration .- User does not speak anything \/ no input- Only noise gets passed \/ Data lossIs there any parameter inside my response which can point to above scenarios ?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1664101920000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":59.0,
        "Answer_body":"1.- If a user speaks a different language you can use language recognition in audio requests. Speech-to-Text supports alternative language codes for all speech recognition methods. Also, one good practice is to show a phrase that can be used or advice on what language you select to be recognized by Speech-to-Text.\u00a0\n\n2.- There are multiple ways that Speech to text can return an empty response. The source of the problem could be the RecognitionConfig\u00a0or the audio itself.\n\n3.-To avoid that only the noise gets passed and the data is lost you can pre-process the audio just as the best practices doc\u00a0mentions.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-does-google-cloud-speech-to-text-api-deals-with-invalid\/td-p\/470810\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1664290560000,
        "Question_original_content":"googl cloud speech text api deal invalid input googl cloud stream asyncstreamingrecogn speech text convers applic gone link understand structur respons return api streamingrecognizerespons scenario end invalid scenario understand respons invalid scenario like user speak differ languag pass configur user speak input nois get pass data lossi paramet insid respons point scenario",
        "Question_preprocessed_content":"googl cloud speech text api deal invalid input googl cloud stream speech text convers applic gone link understand structur respons return api streamingrecognizerespons scenario end invalid scenario understand respons invalid scenario like user speak differ languag pass configur user speak input nois get pass data lossi paramet insid respons point scenario",
        "Question_gpt_summary_original":"The user is facing challenges with understanding how the Google Cloud Speech to Text API deals with invalid inputs. They are using the AsyncStreamingRecognize feature and have reviewed the StreamingRecognizeResponse structure, but are unsure of how the API will respond to scenarios such as the user speaking in a different language than configured, no input from the user, or only noise being passed. The user is seeking clarification on whether there is a parameter in the API response that can indicate these scenarios.",
        "Question_gpt_summary":"user face challeng understand googl cloud speech text api deal invalid input asyncstreamingrecogn featur review streamingrecognizerespons structur unsur api respond scenario user speak differ languag configur input user nois pass user seek clarif paramet api respons indic scenario",
        "Answer_original_content":"user speak differ languag us languag recognit audio request speech text support altern languag code speech recognit method good practic phrase advic languag select recogn speech text multipl wai speech text return respons sourc problem recognitionconfigor audio avoid nois get pass data lost pre process audio best practic docment view solut origin post",
        "Answer_preprocessed_content":"user speak differ languag us languag recognit audio request support altern languag code speech recognit method good practic phrase advic languag select recogn multipl wai speech text return respons sourc problem recognitionconfigor audio avoid nois get pass data lost audio best practic docment view solut origin post",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use language recognition in audio requests and show a phrase that can be used or advice on what language to select for recognition by Speech-to-Text.\n2. Check the RecognitionConfig or the audio itself if Speech to text returns an empty response.\n3. Pre-process the audio to avoid passing only noise and losing data, as mentioned in the best practices documentation.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss us languag recognit audio request phrase advic languag select recognit speech text check recognitionconfig audio speech text return respons pre process audio avoid pass nois lose data mention best practic document person opinion bias includ summari"
    },
    {
        "Question_title":"AWS uploading file into wrong bucket",
        "Question_body":"<p>I am using AWS Sagemaker and trying to upload a data folder into S3 from Sagemaker. I am trying to do is to upload my data into the s3_train_data directory (the directory exists in S3). However, it wouldn't upload it in that bucket, but in a default Bucket that has been created, and in turn creates a new folder directory with the S3_train_data variables.<\/p>\n\n<p>code to input in directory<\/p>\n\n<pre><code>import os\nimport sagemaker\nfrom sagemaker import get_execution_role\n\nsagemaker_session = sagemaker.Session()\nrole = get_execution_role()\n\nbucket = &lt;bucket name&gt;\nprefix = &lt;folders1\/folders2&gt;\nkey = &lt;input&gt;\n\n\ns3_train_data = 's3:\/\/{}\/{}\/{}\/'.format(bucket, prefix, key)\n\n\n#path 'data' is the folder in the Jupyter Instance, contains all the training data\ninputs = sagemaker_session.upload_data(path= 'data', key_prefix= s3_train_data)\n<\/code><\/pre>\n\n<p>Is the problem in the code or more in how I created the notebook?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1517943817340,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1525621837032,
        "Question_score":0.0,
        "Question_view_count":722.0,
        "Answer_body":"<p>You could look at the Sample notebooks, how to upload the data S3 bucket \nThere have many ways. I am just giving you hints to answer. \nAnd you forgot create a boto3 session to access the S3 bucket <\/p>\n\n<p><strong>It is one of the ways to do it.<\/strong> <\/p>\n\n<pre><code>import os \nimport urllib.request\nimport boto3\n\ndef download(url):\n    filename = url.split(\"\/\")[-1]\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n\ndef upload_to_s3(channel, file):\n    s3 = boto3.resource('s3')\n    data = open(file, \"rb\")\n    key = channel + '\/' + file\n    s3.Bucket(bucket).put_object(Key=key, Body=data)\n\n\n# caltech-256\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\nupload_to_s3('train', 'caltech-256-60-train.rec')\ndownload('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-val.rec')\nupload_to_s3('validation', 'caltech-256-60-val.rec')\n<\/code><\/pre>\n\n<p>link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb<\/a><\/p>\n\n<p><strong>Another way to do it.<\/strong> <\/p>\n\n<pre><code>bucket = '&lt;your_s3_bucket_name_here&gt;'# enter your s3 bucket where you will copy data and model artifacts\nprefix = 'sagemaker\/breast_cancer_prediction' # place to upload training files within the bucket\n# do some processing then prepare to push the data. \n\nf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(f, train_X.astype('float32'), train_y.astype('float32'))\nf.seek(0)\n\nboto3.Session().resource('s3').Bucket(bucket).Object(os.path.join(prefix, 'train', train_file)).upload_fileobj(f)\n<\/code><\/pre>\n\n<p>Link : <a href=\"https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb\" rel=\"nofollow noreferrer\">https:\/\/buildcustom.notebook.us-east-2.sagemaker.aws\/notebooks\/sample-notebooks\/introduction_to_applying_machine_learning\/breast_cancer_prediction\/Breast%20Cancer%20Prediction.ipynb<\/a><\/p>\n\n<p>Youtube link : <a href=\"https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo\" rel=\"nofollow noreferrer\">https:\/\/www.youtube.com\/watch?v=-YiHPIGyFGo<\/a> - how to pull the data in S3 bucket.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/48650152",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1518097617247,
        "Question_original_content":"aw upload file wrong bucket try upload data folder try upload data train data directori directori exist wouldn upload bucket default bucket creat turn creat new folder directori train data variabl code input directori import import import execut role session session role execut role bucket prefix kei train data format bucket prefix kei path data folder jupyt instanc contain train data input session upload data path data kei prefix train data problem code creat notebook",
        "Question_preprocessed_content":"aw upload file wrong bucket try upload data folder try upload data directori wouldn upload bucket default bucket creat turn creat new folder directori variabl code input directori problem code creat notebook",
        "Question_gpt_summary_original":"The user is facing a challenge while trying to upload a data folder into S3 from Sagemaker. The user is attempting to upload the data into the s3_train_data directory, but it is being uploaded into a default bucket instead, which creates a new folder directory with the S3_train_data variables. The user is unsure if the problem is in the code or in how the notebook was created.",
        "Question_gpt_summary":"user face challeng try upload data folder user attempt upload data train data directori upload default bucket instead creat new folder directori train data variabl user unsur problem code notebook creat",
        "Answer_original_content":"look sampl notebook upload data bucket wai give hint answer forgot creat boto session access bucket wai import import urllib request import boto def download url filenam url split path exist filenam urllib request urlretriev url filenam def upload channel file boto resourc data open file kei channel file bucket bucket object kei kei bodi data caltech download http data mxnet data caltech caltech train rec upload train caltech train rec download http data mxnet data caltech caltech val rec upload valid caltech val rec link http buildcustom notebook east aw notebook sampl notebook introduct amazon algorithm imageclassif caltech imag classif fulltrain ipynb wai bucket enter bucket copi data model artifact prefix breast cancer predict place upload train file bucket process prepar push data bytesio smac write numpi dens tensor train astyp float train astyp float seek boto session resourc bucket bucket object path join prefix train train file upload fileobj link http buildcustom notebook east aw notebook sampl notebook introduct appli machin learn breast cancer predict breast cancer predict ipynb youtub link http youtub com watch yihpigyfgo pull data bucket",
        "Answer_preprocessed_content":"look sampl notebook upload data bucket wai give hint answer forgot creat boto session access bucket wai link wai link youtub link pull data bucket",
        "Answer_gpt_summary_original":"Two possible solutions were provided in the discussion. The first solution involves creating a boto3 session to access the S3 bucket and then using the `upload_to_s3` function to upload the data. The second solution involves specifying the S3 bucket name and the prefix where the data needs to be uploaded and then using the `upload_fileobj` function to upload the data. Links to sample notebooks and a YouTube video were also provided for reference.",
        "Answer_gpt_summary":"possibl solut provid discuss solut involv creat boto session access bucket upload function upload data second solut involv specifi bucket prefix data need upload upload fileobj function upload data link sampl notebook youtub video provid refer"
    },
    {
        "Question_title":"Invoke aws sagemaker endpoint",
        "Question_body":"<p>I have some data in S3 and I want to create a lambda function to predict the output with my deployed aws sagemaker endpoint then I put the outputs in S3 again. Is it necessary in this case to create an api gateway like decribed in this <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/call-an-amazon-sagemaker-model-endpoint-using-amazon-api-gateway-and-aws-lambda\/\" rel=\"noreferrer\">link<\/a> ? and in the lambda function what I have to put. I expect to put (where to find the data, how to invoke the endpoint, where to put the data) <\/p>\n\n<pre><code>import boto3\nimport io\nimport json\nimport csv\nimport os\n\n\nclient = boto3.client('s3') #low-level functional API\n\nresource = boto3.resource('s3') #high-level object-oriented API\nmy_bucket = resource.Bucket('demo-scikit-byo-iris') #subsitute this for your s3 bucket name. \n\nobj = client.get_object(Bucket='demo-scikit-byo-iris', Key='foo.csv')\nlines= obj['Body'].read().decode('utf-8').splitlines()\nreader = csv.reader(lines)\n\nimport io\nfile = io.StringIO(lines)\n\n# grab environment variables\nruntime= boto3.client('runtime.sagemaker')\n\nresponse = runtime.invoke_endpoint(\n    EndpointName= 'nilm2',\n    Body = file.getvalue(),\n    ContentType='*\/*',\n    Accept = 'Accept')\n\noutput = response['Body'].read().decode('utf-8')\n<\/code><\/pre>\n\n<p>my data is a csv file of 2 columns of floats with no headers, the problem is that lines return a list of strings(each row is an element of this list:['11.55,65.23', '55.68,69.56'...]) the invoke work well but the response is also a string: output = '65.23\\n,65.23\\n,22.56\\n,...' <\/p>\n\n<p>So how to save this output to S3 as a csv file <\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1549885499183,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":3146.0,
        "Answer_body":"<p>If your Lambda function is scheduled, then you won't need an API Gateway. But if the predict action will be triggered by a user, by an application, for example, you will need.<\/p>\n\n<p>When you call the invoke endpoint, actually you are calling a SageMaker endpoint, which is not the same as an API Gateway endpoint. <\/p>\n\n<p>A common architecture with SageMaker is:<\/p>\n\n<ol>\n<li>API Gateway with receives a request then calls an authorizer, then\ninvoke your Lambda; <\/li>\n<li>A Lambda with does some parsing in your input data, then calls your SageMaker prediction endpoint, then, handles the result and returns to your application.<\/li>\n<\/ol>\n\n<p>By the situation you describe, I can't say if your task is some academic stuff or a production one.<\/p>\n\n<p>So, how you can save the data as a CSV file from your Lambda? <\/p>\n\n<p>I believe you can just parse the output, then just upload the file to S3. Here you will parse manually or with a lib, with boto3 you can upload the file. The output of your model depends on your implementation on SageMaker image. So, if you need the response data in another format, maybe you will need to use a <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/scikit_bring_your_own\" rel=\"nofollow noreferrer\">custom image<\/a>. I normally use a custom image, which I can define how I want to handle my data on requests\/responses.<\/p>\n\n<p>In terms of a production task, I certainly recommend you check Batch transform jobs from SageMaker. You can provide an input file (the S3 path) and also a destination file (another S3 path). The SageMaker will run the batch predictions and will persist a file with the results. Also, you won't need to deploy your model to an endpoint, when this job run, will create an instance of your endpoint, download your data to predict, do the predictions, upload the output, and shut down the instance. You only need a trained model.<\/p>\n\n<p>Here some info about Batch transform jobs:<\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/how-it-works-batch.html<\/a><\/p>\n\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-batch-transform.html<\/a><\/p>\n\n<p>I hope it helps, let me know if need more info.<\/p>\n\n<p>Regards.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54629890",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1550022296023,
        "Question_original_content":"invok endpoint data want creat lambda function predict output deploi endpoint output necessari case creat api gatewai like decrib link lambda function expect data invok endpoint data import boto import import json import csv import client boto client low level function api resourc boto resourc high level object orient api bucket resourc bucket demo scikit byo iri subsitut bucket obj client object bucket demo scikit byo iri kei foo csv line obj bodi read decod utf splitlin reader csv reader line import file stringio line grab environ variabl runtim boto client runtim respons runtim invok endpoint endpointnam nilm bodi file getvalu contenttyp accept accept output respons bodi read decod utf data csv file column float header problem line return list string row element list invok work respons string output save output csv file thank",
        "Question_preprocessed_content":"invok endpoint data want creat lambda function predict output deploi endpoint output necessari case creat api gatewai like decrib link lambda function expect data csv file column float header problem line return list string invok work respons string output save output csv file thank",
        "Question_gpt_summary_original":"The user wants to create a lambda function to predict the output with their deployed AWS SageMaker endpoint and put the outputs in S3 again. They are unsure if it is necessary to create an API gateway and what to put in the lambda function. The user's data is a CSV file of 2 columns of floats with no headers, and they are having trouble saving the output to S3 as a CSV file.",
        "Question_gpt_summary":"user want creat lambda function predict output deploi endpoint output unsur necessari creat api gatewai lambda function user data csv file column float header have troubl save output csv file",
        "Answer_original_content":"lambda function schedul won need api gatewai predict action trigger user applic exampl need invok endpoint actual call endpoint api gatewai endpoint common architectur api gatewai receiv request call author invok lambda lambda pars input data call predict endpoint handl result return applic situat task academ stuff product save data csv file lambda believ pars output upload file pars manual lib boto upload file output model depend implement imag need respons data format mayb need us custom imag normal us custom imag defin want handl data request respons term product task certainli recommend check batch transform job provid input file path destin file path run batch predict persist file result won need deploi model endpoint job run creat instanc endpoint download data predict predict upload output shut instanc need train model info batch transform job http doc aw amazon com latest work batch html http doc aw amazon com latest batch transform html hope help let know need info regard",
        "Answer_preprocessed_content":"lambda function schedul won need api gatewai predict action trigger user applic exampl need invok endpoint actual call endpoint api gatewai endpoint common architectur api gatewai receiv request call author invok lambda lambda pars input data call predict endpoint handl result return applic situat task academ stuff product save data csv file lambda believ pars output upload file pars manual lib boto upload file output model depend implement imag need respons data format mayb need us custom imag normal us custom imag defin want handl data term product task certainli recommend check batch transform job provid input file destin file run batch predict persist file result won need deploi model endpoint job run creat instanc endpoint download data predict predict upload output shut instanc need train model info batch transform job hope help let know need info regard",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- If the Lambda function is scheduled, then an API Gateway is not needed. But if the predict action will be triggered by a user, an API Gateway is needed.\n- A common architecture with SageMaker is to use an API Gateway that receives a request, calls an authorizer, and then invokes the Lambda function. The Lambda function does some parsing in the input data, calls the SageMaker prediction endpoint, handles the result, and returns it to the application.\n- To save the data as a CSV file from the Lambda function, the output can be parsed and uploaded to S3 using boto3. If the response data is needed in another format, a custom image can be used.\n- For a production",
        "Answer_gpt_summary":"possibl solut mention discuss lambda function schedul api gatewai need predict action trigger user api gatewai need common architectur us api gatewai receiv request call author invok lambda function lambda function pars input data call predict endpoint handl result return applic save data csv file lambda function output pars upload boto respons data need format custom imag product"
    },
    {
        "Question_title":"Help with Inference Script for Amazon Sagemaker Neo Compiled Models",
        "Question_body":"Hello everyone, I was trying to execute the example mentioned in the docs - https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/pytorch_torchvision_neo.html. I was able to successfully run this example but as soon as I changed the target_device to jetson_tx2, after which I ran the entire script again, keeping the rest of the code as it is, the model stopped working. I was not getting any inferences from the deployed model and it always errors out with the message:\n\nAn error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (0) from <users-sagemaker-endpoint> with message \"Your invocation timed out while waiting for a response from container model. Review the latency metrics for each container in Amazon CloudWatch, resolve the issue, and try again.\"                \n\n\nAccording to the troubleshoot docs https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/neo-troubleshooting-inference.html, this seems to be an issue of model_fn() function. The inference script used by this example is mentioned here https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/sagemaker_neo_compilation_jobs\/pytorch_torchvision\/code\/resnet18.py , which itself doesn't contain any model_fn() definition but it still worked for target device ml_c5. So could anyone please help me with the following questions:\n\nWhat changes does SageMaker Neo do to the model depending on target_device type? Since it seems the same model is loaded in a different way for different target device.\nIs there any way to determine how the model is expected to load for a certain target_device type so that I could define the model_fn() function myself in the same inference script mentioned above?\nAt-last, can anyone please help with the inference script for this very same model as mentioned in docs above which works for jetson_tx2 device as well.\n\nAny suggestions or links on how to resolve this issue would be really helpful.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1668430718144,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":32.0,
        "Answer_body":"As you mentioned, you changed the Neo compiling target from ml_c5 to jetson_tx2, the compiled model will require runtime from jetson_tx2. If you kept other code unchanged, the model will be deployed to a ml.c5.9xlarge EC2 instance, which doesn't provide Nvida Jeston.\n\nThe model can't be loaded and will error out since Jestion is a device Nvidia GPU structure while c5 is only equipped with CPU. No CUDA environment.\n\nIf you compile the model with jeston_tx2 as target, you should download the model and run the compiled model in a real Nvidia Jeston device.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUJvbkzp91TGSZO1GwW-r90w\/help-with-inference-script-for-amazon-sagemaker-neo-compiled-models",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1668766092280,
        "Question_original_content":"help infer script neo compil model hello try execut exampl mention doc http exampl readthedoc latest neo compil job pytorch torchvis pytorch torchvis neo html abl successfulli run exampl soon chang target devic jetson ran entir script keep rest code model stop work get infer deploi model error messag error occur modelerror call invokeendpoint oper receiv server error messag invoc time wait respons contain model review latenc metric contain amazon cloudwatch resolv issu try accord troubleshoot doc http doc aw amazon com latest neo troubleshoot infer html issu model function infer script exampl mention http github com aw amazon exampl blob main neo compil job pytorch torchvis code resnet contain model definit work target devic help follow question chang neo model depend target devic type model load differ wai differ target devic wai determin model expect load certain target devic type defin model function infer script mention help infer script model mention doc work jetson devic suggest link resolv issu help",
        "Question_preprocessed_content":"help infer script neo compil model hello try execut exampl mention doc abl successfulli run exampl soon chang ran entir script keep rest code model stop work get infer deploi model error messag error occur call invokeendpoint oper receiv server error messag invoc time wait respons contain model review latenc metric contain amazon cloudwatch resolv issu try accord troubleshoot doc issu function infer script exampl mention contain definit work target devic help follow question chang neo model depend type model load differ wai differ target devic wai determin model expect load certain type defin function infer script mention help infer script model mention doc work devic suggest link resolv issu help",
        "Question_gpt_summary_original":"The user encountered challenges while trying to execute an example in Amazon Sagemaker Neo Compiled Models. After changing the target_device to jetson_tx2, the model stopped working and the user was not getting any inferences from the deployed model. The troubleshoot docs suggest that this could be an issue with the model_fn() function. The user is seeking help with understanding the changes that SageMaker Neo does to the model depending on the target_device type, determining how the model is expected to load for a certain target_device type, and help with the inference script for the same model that works for jetson_tx2 device.",
        "Question_gpt_summary":"user encount challeng try execut exampl neo compil model chang target devic jetson model stop work user get infer deploi model troubleshoot doc suggest issu model function user seek help understand chang neo model depend target devic type determin model expect load certain target devic type help infer script model work jetson devic",
        "Answer_original_content":"mention chang neo compil target jetson compil model requir runtim jetson kept code unchang model deploi xlarg instanc provid nvida jeston model load error jestion devic nvidia gpu structur equip cpu cuda environ compil model jeston target download model run compil model real nvidia jeston devic",
        "Answer_preprocessed_content":"mention chang neo compil target compil model requir runtim kept code unchang model deploi instanc provid nvida jeston model load error jestion devic nvidia gpu structur equip cpu cuda environ compil model target download model run compil model real nvidia jeston devic",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to compile the model with the target_device set to jetson_tx2 and then download the compiled model to run it on a real Nvidia Jetson device. It is also mentioned that if the model is compiled with a target_device that is not available on the deployment instance, the model will not work and will error out. No other solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut solut suggest discuss compil model target devic set jetson download compil model run real nvidia jetson devic mention model compil target devic avail deploy instanc model work error solut provid discuss"
    },
    {
        "Question_title":"Kubeflow Pipeline Training Component Failing | Unknown return type: <class 'inspect._empty'>",
        "Question_body":"<p>I am running an ML pipeline and the training component\/step (see code below) continues to fail with the following error: &quot;RuntimeError: Unknown return type: &lt;class 'inspect._empty'&gt;. Must be one of <code>str<\/code>, <code>int<\/code>, <code>float<\/code>, a subclass of <code>Artifact<\/code>, or a NamedTuple collection of these types.&quot;<\/p>\n<p>Any ideas on what might be causing the issue\/error and how to resolve it?<\/p>\n<p>Thank you!<\/p>\n<ul>\n<li>RE<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>\n@component(\n    # this component builds an xgboost classifier with xgboost\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;\n)\n\ndef build_xgb_xgboost(project: str, \n                            bq_dataset: str, \n                            test_view_name: str,\n                            bq_location: str,\n                            metrics: Output[Metrics],\n                            model: Output[Model]\n\n):\n    from google.cloud import bigquery\n    import xgboost as xgb\n    import pandas as pd\n    from xgboost import XGBRegressor\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.metrics import mean_squared_error as MSE\n    from sklearn.metrics import mean_absolute_error\n    import joblib\n    import pyarrow\n    import db_dtypes\n     \n\n    client = bigquery.Client(project=project) \n\n    view_uri = f&quot;{project}.{bq_dataset}.{test_view_name}&quot; #replace view_name with test_view_name\n    \n    build_df_for_xgboost = '''\n    SELECT * FROM `{view_uri}`\n    '''.format(view_uri = view_uri)\n\n    job_config = bigquery.QueryJobConfig()\n    df_1 = client.query(build_df_for_xgboost).to_dataframe()\n    \n    #client.query(build_df_for_xgboost, job_config=job_config).to_dataframe()  \n    \n    df = df_1.drop(['int64_field_0'], axis=1)\n    \n    def onehot_encode(df, column):\n        df = df.copy()\n        dummies = pd.get_dummies(df[column], prefix=column)\n        df = pd.concat([df, dummies], axis=1)\n        df = df.drop(column, axis=1)\n    return df\n    \n    # Binary encoding\n    df['preferred_foot'] = df['preferred_foot'].replace({'left': 0, 'right': 1})\n    \n    # One-hot encoding\n    for column in ['attacking_work_rate', 'defensive_work_rate']:\n        df = onehot_encode(df, column=column)\n    \n    # Split df into X and y\n    y = df['overall_rating']\n    X = df.drop('overall_rating', axis=1)\n    \n    # Train-test split\n    X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, shuffle=True, random_state=1)\n    \n    # Scale X\n    scaler = StandardScaler()\n    scaler.fit(X_train)\n    X_train = pd.DataFrame(scaler.transform(X_train), index=X_train.index, columns=X_train.columns)\n    X_test = pd.DataFrame(scaler.transform(X_test), index=X_test.index, columns=X_test.columns)\n\n    #specify parameters\n    \n    #define your model \n    bst = XGBRegressor(\n    objective='reg:linear',\n    learning_rate = '.1',\n    alpha = '0.001'\n    )\n    \n    #fit your model\n    bst.fit(X_train, y_train)\n    \n    # Predict the model \n    y_pred = bst.predict(X_test)\n    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n    mae = mean_absolute_error(y_test, y_pred)\n    \n    metrics.log_metric(&quot;RMSE&quot;, rmse)\n    metrics.log_metric(&quot;framework&quot;, &quot;xgboost&quot;)\n    metrics.log_metric(&quot;dataset_size&quot;, len(df))\n    metrics.log_metric(&quot;MAE&quot;, mae)\n    \n    dump(bst, model.path + &quot;.joblib&quot;)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1655350561587,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1659256989790,
        "Question_score":0.0,
        "Question_view_count":107.0,
        "Answer_body":"<p>I think this might just be a bug in the version of KFP v2 SDK code you're using.<\/p>\n<p>I mostly use the stable KFPv1 methods to avoid problems.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.components import InputPath, OutputPath, create_component_from_func\n\n\ndef train_xgboost_model(\n    project: str, \n    bq_dataset: str, \n    test_view_name: str,\n    bq_location: str,\n    metrics_path: OutputPath(Metrics),\n    model_path: OutputPath(Model),\n):\n    import json\n    from pathlib import Path\n\n    metrics = {\n       ...\n    }\n    Path(metrics_path).write_text(json.dumps(metrics))\n\n    dump(bst, model_path)\n\ntrain_xgboost_model_op = create_component_from_func(\n    func=train_xgboost_model,\n    packages_to_install=[&quot;google-cloud-bigquery&quot;, &quot;xgboost&quot;, &quot;pandas&quot;, &quot;sklearn&quot;, &quot;joblib&quot;, &quot;pyarrow&quot;, &quot;db_dtypes&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;create_xgb_model_xgboost.yaml&quot;,\n)\n\n<\/code><\/pre>\n<p>You can also find many examples of real-world components in this repo: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/tree\/master\/components<\/a><\/p>\n<p>including an XGBoost trainer <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/d8c4cf5\/components\/XGBoost\/Train\/component.py<\/a><\/p>\n<p>and a full XGBoost pipeline: <a href=\"https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py\" rel=\"nofollow noreferrer\">https:\/\/github.com\/Ark-kun\/pipeline_components\/blob\/4f19be6f26eaaf85ba251110d10d103b17e54a17\/samples\/Google_Cloud_Vertex_AI\/Train_tabular_regression_model_using_XGBoost_and_import_to_Vertex_AI\/pipeline.py<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72640182",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659257407332,
        "Question_original_content":"kubeflow pipelin train compon fail unknown return type run pipelin train compon step code continu fail follow error runtimeerror unknown return type str int float subclass artifact namedtupl collect type idea caus issu error resolv thank compon compon build xgboost classifi xgboost packag instal googl cloud bigqueri xgboost panda sklearn joblib pyarrow dtype base imag python output compon file creat xgb model xgboost yaml def build xgb xgboost project str dataset str test view str locat str metric output metric model output model googl cloud import bigqueri import xgboost xgb import panda xgboost import xgbregressor sklearn model select import train test split sklearn preprocess import standardscal sklearn metric import mean squar error mse sklearn metric import mean absolut error import joblib import pyarrow import dtype client bigqueri client project project view uri project dataset test view replac view test view build xgboost select view uri format view uri view uri job config bigqueri queryjobconfig client queri build xgboost datafram client queri build xgboost job config job config datafram drop int field axi def onehot encod column copi dummi dummi column prefix column concat dummi axi drop column axi return binari encod prefer foot prefer foot replac left right hot encod column attack work rate defens work rate onehot encod column column split overal rate drop overal rate axi train test split train test train test train test split train size shuffl true random state scale scaler standardscal scaler fit train train datafram scaler transform train index train index column train column test datafram scaler transform test index test index column test column specifi paramet defin model bst xgbregressor object reg linear learn rate alpha fit model bst fit train train predict model pred bst predict test rmse sqrt mean test pred mae mean absolut error test pred metric log metric rmse rmse metric log metric framework xgboost metric log metric dataset size len metric log metric mae mae dump bst model path joblib",
        "Question_preprocessed_content":"kubeflow pipelin train compon fail unknown return type run pipelin train continu fail follow error runtimeerror unknown return type subclass namedtupl collect idea caus resolv thank",
        "Question_gpt_summary_original":"The user is encountering an error while running an ML pipeline's training component\/step. The error message states that the return type is unknown and must be one of the specified types. The user is seeking help to identify the cause of the issue and resolve it. The code for the training component is also provided.",
        "Question_gpt_summary":"user encount error run pipelin train compon step error messag state return type unknown specifi type user seek help identifi caus issu resolv code train compon provid",
        "Answer_original_content":"think bug version kfp sdk code us stabl kfpv method avoid problem kfp compon import inputpath outputpath creat compon func def train xgboost model project str dataset str test view str locat str metric path outputpath metric model path outputpath model import json pathlib import path metric path metric path write text json dump metric dump bst model path train xgboost model creat compon func func train xgboost model packag instal googl cloud bigqueri xgboost panda sklearn joblib pyarrow dtype base imag python output compon file creat xgb model xgboost yaml exampl real world compon repo http github com ark kun pipelin compon tree master compon includ xgboost trainer http github com ark kun pipelin compon blob dccf compon xgboost train compon xgboost pipelin http github com ark kun pipelin compon blob fbefeaafbaddbea sampl googl cloud vertex train tabular regress model xgboost import vertex pipelin",
        "Answer_preprocessed_content":"think bug version kfp sdk code us stabl kfpv method avoid problem exampl compon repo includ xgboost trainer xgboost pipelin",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Upload Saved ML Model in R (local) to Azure Machine Learning Studio",
        "Question_body":"<p>I am trying to reduce my development headaches for creating a ML Webservice on Azure ML Studio. One of the things that stuck me was can we just upload .rda files in the workbench and load it via an RScript (like in the figure below). <\/p>\n\n<p><img src=\"https:\/\/raw.githubusercontent.com\/pratos\/pratos.github.io\/master\/images\/stackb1model.png\" alt=\"Do\"><\/p>\n\n<p>But can't connect directly to the R Script block. There's another way to do it (works to upload packages that aren't available in Azure's R directories) -- using zip. But there isn't really any resource out there that I found to access the .rda file in .zip.<\/p>\n\n<p>I have 2 options here, make the .zip work or any other work around where I can directly use my .rda model. If someone could guide me about how to go forward it would appreciate it.<\/p>\n\n<p>Note: Currently, I'm creating models via the \"Create RModel\" block, training them and saving it, so that I can use it to make a predictive web service. But for models like Random Forest, not sure how the randomness might create models (local versions and Azure versions are different, the setting of seed also isn't very helpful). A bit tight on schedule, Azure ML seems boxed for creating iterations and automating the ML workflow (or maybe I'm doing it wrong).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1486101642917,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":730.0,
        "Answer_body":"<p>Here is an example of uploading a .rda file for scoring:\n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Womens-Health-Risk-Assessment-using-the-XGBoost-classification-algorithm-1<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42017727",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1486104439200,
        "Question_original_content":"upload save model local studio try reduc develop headach creat webservic studio thing stuck upload rda file workbench load rscript like figur connect directli script block wai work upload packag aren avail azur directori zip isn resourc access rda file zip option zip work work directli us rda model guid forward appreci note current creat model creat rmodel block train save us predict web servic model like random forest sure random creat model local version azur version differ set seed isn help bit tight schedul box creat iter autom workflow mayb wrong",
        "Question_preprocessed_content":"upload save model studio try reduc develop headach creat webservic studio thing stuck upload rda file workbench load rscript connect directli script block wai zip isn resourc access rda file zip option zip work work directli us rda model guid forward appreci note current creat model creat rmodel block train save us predict web servic model like random forest sure random creat model bit tight schedul box creat iter autom workflow",
        "Question_gpt_summary_original":"The user is facing challenges in uploading a saved ML model in R (local) to Azure Machine Learning Studio. They are unable to directly connect to the R Script block and are looking for a workaround to use their .rda model. They have tried using zip but are unable to access the .rda file in .zip. The user is also unsure about how the randomness in models like Random Forest might create differences between local and Azure versions. They are looking for guidance on how to proceed and are feeling constrained by the limitations of Azure ML for automating the ML workflow.",
        "Question_gpt_summary":"user face challeng upload save model local studio unabl directli connect script block look workaround us rda model tri zip unabl access rda file zip user unsur random model like random forest creat differ local azur version look guidanc proce feel constrain limit autom workflow",
        "Answer_original_content":"exampl upload rda file score http galleri cortanaintellig com experi women health risk assess xgboost classif algorithm",
        "Answer_preprocessed_content":"exampl upload rda file score",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution in the form of an example of uploading a .rda file for scoring. The user can refer to the provided link to learn how to upload their .rda model to Azure Machine Learning Studio. However, no solution is provided for the concern regarding the randomness in models like Random Forest creating differences between local and Azure versions.",
        "Answer_gpt_summary":"solut discuss provid solut form exampl upload rda file score user refer provid link learn upload rda model studio solut provid concern random model like random forest creat differ local azur version"
    },
    {
        "Question_title":"Training keras model in AWS Sagemaker",
        "Question_body":"<p>I have keras training script on my machine. I am experimenting to run my script on AWS sagemaker container. For that I have used below code.<\/p>\n<pre><code>from sagemaker.tensorflow import TensorFlow\nest = TensorFlow(\n    entry_point=&quot;caller.py&quot;,\n    source_dir=&quot;.\/&quot;,\n    role='role_arn',\n    framework_version=&quot;2.3.1&quot;,\n    py_version=&quot;py37&quot;,\n    instance_type='ml.m5.large',\n    instance_count=1,\n    hyperparameters={'batch': 8, 'epochs': 10},\n)\n\nest.fit()\n<\/code><\/pre>\n<p>here <code>caller.py<\/code> is my entry point. After executing the above code I am getting <code>keras is not installed<\/code>. Here is the stacktrace.<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;executor.py&quot;, line 14, in &lt;module&gt;\n    est.fit()\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 682, in fit\n    self.latest_training_job.wait(logs=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/estimator.py&quot;, line 1625, in wait\n    self.sagemaker_session.logs_for_job(self.job_name, wait=True, log_type=logs)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3681, in logs_for_job\n    self._check_job_status(job_name, description, &quot;TrainingJobStatus&quot;)\n  File &quot;\/home\/thasin\/Documents\/python\/venv\/lib\/python3.8\/site-packages\/sagemaker\/session.py&quot;, line 3240, in _check_job_status\n    raise exceptions.UnexpectedStatusException(\nsagemaker.exceptions.UnexpectedStatusException: Error for Training job tensorflow-training-2021-06-09-07-14-01-778: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand &quot;\/usr\/local\/bin\/python3.7 caller.py --batch 4 --epochs 10\n\nModuleNotFoundError: No module named 'keras'\n\n<\/code><\/pre>\n<ol>\n<li>Which instance has pre-installed keras?<\/li>\n<li>Is there any way I can install the python package to the AWS container? or any workaround for the issue?<\/li>\n<\/ol>\n<p>Note: I have tried with my own container uploading to ECR and successfully run my code. I am looking for AWS's existing container capability.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1623223568407,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1623224188496,
        "Question_score":0.0,
        "Question_view_count":316.0,
        "Answer_body":"<p>Keras is now part of tensorflow, so you can just reformat your code to use <code>tf.keras<\/code> instead of <code>keras<\/code>. Since version 2.3.0 of tensorflow they are in sync, so it should not be that difficult.\nYou container is <a href=\"https:\/\/aws.amazon.com\/releasenotes\/aws-deep-learning-containers-for-tensorflow-2-3-1-with-cuda-11-0\/\" rel=\"nofollow noreferrer\">this<\/a>, as you can see from the list of the packages, there is no <code>Keras<\/code>.\nIf you instead want to extend a pre-built container you can take a look <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/prebuilt-containers-extend.html\" rel=\"nofollow noreferrer\">here<\/a> but I don't recommend in this specific use-case, because also for future code maintainability you should go for <code>tf.keras<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67899421",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1623320398007,
        "Question_original_content":"train kera model kera train script machin experi run script contain code tensorflow import tensorflow est tensorflow entri point caller sourc dir role role arn framework version version instanc type larg instanc count hyperparamet batch epoch est fit caller entri point execut code get kera instal stacktrac traceback recent file executor line est fit file home thasin document python venv lib python site packag estim line fit self latest train job wait log log file home thasin document python venv lib python site packag estim line wait self session log job self job wait true log type log file home thasin document python venv lib python site packag session line log job self check job statu job descript trainingjobstatu file home thasin document python venv lib python site packag session line check job statu rais except unexpectedstatusexcept except unexpectedstatusexcept error train job tensorflow train fail reason algorithmerror executeuserscripterror command usr local bin python caller batch epoch modulenotfounderror modul name kera instanc pre instal kera wai instal python packag aw contain workaround issu note tri contain upload ecr successfulli run code look aw exist contain capabl",
        "Question_preprocessed_content":"train kera model kera train script machin experi run script contain code entri point execut code get stacktrac instanc kera wai instal python packag aw contain workaround issu note tri contain upload ecr successfulli run code look aw exist contain capabl",
        "Question_gpt_summary_original":"The user is facing challenges while training a Keras model in AWS Sagemaker. After executing the code, the user is getting an error message stating that Keras is not installed. The user is seeking answers to two questions: which instance has pre-installed Keras, and is there any way to install the Python package to the AWS container or any workaround for the issue? The user has tried running the code on their own container uploaded to ECR and it worked successfully, but they are looking for AWS's existing container capability.",
        "Question_gpt_summary":"user face challeng train kera model execut code user get error messag state kera instal user seek answer question instanc pre instal kera wai instal python packag aw contain workaround issu user tri run code contain upload ecr work successfulli look aw exist contain capabl",
        "Answer_original_content":"kera tensorflow reformat code us kera instead kera version tensorflow sync difficult contain list packag kera instead want extend pre built contain look recommend specif us case futur code maintain kera",
        "Answer_preprocessed_content":"kera tensorflow reformat code us instead version tensorflow sync difficult contain list packag instead want extend contain look recommend specif futur code maintain",
        "Answer_gpt_summary_original":"Solution 1: Reformat the code to use tf.keras instead of Keras since Keras is now part of tensorflow and they are in sync since version 2.3.0.\n\nSolution 2: Extend a pre-built container by following the instructions provided in the AWS Sagemaker documentation, but it is not recommended in this specific use-case as it is better to use tf.keras for future code maintainability. \n\nNote: No solution is provided for installing the Python package to the AWS container or any workaround for the issue.",
        "Answer_gpt_summary":"solut reformat code us kera instead kera kera tensorflow sync version solut extend pre built contain follow instruct provid document recommend specif us case better us kera futur code maintain note solut provid instal python packag aw contain workaround issu"
    },
    {
        "Question_title":"sagemaker - factorization machines - deserialize model",
        "Question_body":"<p>I estimated a factorization machine model in sagemaker and it saved a file <code>model.tar.gz<\/code> into an s3 folder.<\/p>\n\n<p>Is there a way I can load this file in Python and access the parameter of the model, i.e. the factors, directly?<\/p>\n\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1575827808930,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1575835104230,
        "Question_score":0.0,
        "Question_view_count":164.0,
        "Answer_body":"<p>As of April 2019: yes. An official AWS blog post was created to show how to open the SageMaker Factorization Machines artifact and extract its parameters: <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/blogs\/machine-learning\/extending-amazon-sagemaker-factorization-machines-algorithm-to-predict-top-x-recommendations\/<\/a><\/p>\n\n<p>That being said, be aware that Amazon SageMaker built-in algorithm are primarily built for deployment on AWS, and only <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/xgboost.html\" rel=\"nofollow noreferrer\">SageMaker XGBoost<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/blazingtext.html\" rel=\"nofollow noreferrer\">SageMaker BlazingText<\/a> are designed to produce artifacts interoperable with their open-source equivalent.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59238265",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1575844058683,
        "Question_original_content":"factor machin deseri model estim factor machin model save file model tar folder wai load file python access paramet model factor directli thank",
        "Question_preprocessed_content":"factor machin deseri model estim factor machin model save file folder wai load file python access paramet model factor directli thank",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing the parameters of a factorization machine model that was estimated in sagemaker and saved as a file in an s3 folder. They are seeking a way to load the file in Python and access the factors directly.",
        "Question_gpt_summary":"user face challeng access paramet factor machin model estim save file folder seek wai load file python access factor directli",
        "Answer_original_content":"april ye offici aw blog post creat open factor machin artifact extract paramet http aw amazon com blog machin learn extend amazon factor machin algorithm predict recommend said awar built algorithm primarili built deploy aw xgboost blazingtext design produc artifact interoper open sourc equival",
        "Answer_preprocessed_content":"april ye offici aw blog post creat open factor machin artifact extract paramet said awar algorithm primarili built deploy aw xgboost blazingtext design produc artifact interoper equival",
        "Answer_gpt_summary_original":"Solution: An official AWS blog post was created in April 2019 to show how to open the SageMaker Factorization Machines artifact and extract its parameters. However, it is important to note that Amazon SageMaker built-in algorithms are primarily built for deployment on AWS, and only SageMaker XGBoost and SageMaker BlazingText are designed to produce artifacts interoperable with their open-source equivalent.",
        "Answer_gpt_summary":"solut offici aw blog post creat april open factor machin artifact extract paramet import note built algorithm primarili built deploy aw xgboost blazingtext design produc artifact interoper open sourc equival"
    },
    {
        "Question_title":"Automating Azure Machine Learning",
        "Question_body":"<p>Is there a way of automating the calls to the Azure Machine Learning Service (AML)? <\/p>\n\n<p>I\u2019ve created the web service from AML. Now I have to do the calls the automated way. I\u2019m trying to build a system, that connects to a Raspberry Pi for sensor data and gets a prediction from the ML service to be saved with the data itself. <\/p>\n\n<p>Is there something in Azure to automate this or should I do it within the application?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1459095760807,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1459199614030,
        "Question_score":0.0,
        "Question_view_count":341.0,
        "Answer_body":"<p>I'm assuming you've created the webservice from the experiment and asking about the consumption of the webservice. You can consume the webservice from anything that can do an API call to the endpoint. I don't know the exact architecture of your solution but take a look at this as it might suit your scenario. <\/p>\n\n<p>Stream analytics on Azure has a new feature called Functions(just a heads-up, its still in preview) that can automate the usage of deployed ML services from your account.Since you are trying to gather info from IoT devices, you might use <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/event-hubs-csharp-ephcs-getstarted\/\" rel=\"nofollow\">Event Hubs<\/a> or <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/iot-hub-csharp-csharp-getstarted\/\" rel=\"nofollow\">IoT Hubs<\/a> to get the data and process it using Stream Analytics and during the process you can use the Webservice as Function in SA to achieve on-the-go ML results.<\/p>\n\n<p>Usage is relatively simple if you are familiar with Stream Analytics or SQL queries in general.This <a href=\"https:\/\/azure.microsoft.com\/en-us\/documentation\/articles\/stream-analytics-machine-learning-integration-tutorial\/\" rel=\"nofollow\">link<\/a> shows the step by step implementation and the usage is below;<\/p>\n\n<pre><code>    WITH subquery AS (  \n    SELECT text, \"webservicealias\"(text) as result from input  \n    )  \n\n    Select text, result.[Score]  \n    Into output  \n    From subquery  \n<\/code><\/pre>\n\n<p>Hope this helps!<\/p>\n\n<p>Mert<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/36249716",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1459099456052,
        "Question_original_content":"autom wai autom call servic aml iv creat web servic aml call autom wai try build connect raspberri sensor data get predict servic save data azur autom applic",
        "Question_preprocessed_content":"autom wai autom call servic iv creat web servic aml call autom wai try build connect raspberri sensor data get predict servic save data azur autom applic",
        "Question_gpt_summary_original":"The user is facing a challenge of automating calls to the Azure Machine Learning Service (AML) for a system that connects to a Raspberry Pi for sensor data and gets a prediction from the ML service to be saved with the data itself. The user is unsure whether to automate this within the application or if there is something in Azure to automate it.",
        "Question_gpt_summary":"user face challeng autom call servic aml connect raspberri sensor data get predict servic save data user unsur autom applic azur autom",
        "Answer_original_content":"assum creat webservic experi ask consumpt webservic consum webservic api endpoint know exact architectur solut look suit scenario stream analyt azur new featur call function head preview autom usag deploi servic account try gather info iot devic us event hub iot hub data process stream analyt process us webservic function achiev result usag rel simpl familiar stream analyt sql queri gener link show step step implement usag subqueri select text webservicealia text result input select text result score output subqueri hope help mert",
        "Answer_preprocessed_content":"assum creat webservic experi ask consumpt webservic consum webservic api endpoint know exact architectur solut look suit scenario stream analyt azur new featur call function autom usag deploi servic try gather info iot devic us event hub iot hub data process stream analyt process us webservic function achiev result usag rel simpl familiar stream analyt sql queri link show step step implement usag hope help mert",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using Stream Analytics on Azure with a new feature called Functions to automate the usage of deployed ML services from the user's account. The user can use Event Hubs or IoT Hubs to get the data and process it using Stream Analytics. During the process, the user can use the Webservice as Function in SA to achieve on-the-go ML results. The usage is relatively simple if the user is familiar with Stream Analytics or SQL queries in general.",
        "Answer_gpt_summary":"solut discuss suggest stream analyt azur new featur call function autom usag deploi servic user account user us event hub iot hub data process stream analyt process user us webservic function achiev result usag rel simpl user familiar stream analyt sql queri gener"
    },
    {
        "Question_title":"Can Azure calculate confidence interval for regressions?",
        "Question_body":"<p>I plan to try different regression methods provided by Azure ML Studio to predict numeric values. I wonder if it is possible to get the predictions together with corresponding confidence intervals. In other words, I would like the regression function to tell me not only the expected value (prediction) but also how confident it (the model) is about this value. Does Azure regression support this functionality?<\/p>\n\n<p><strong>ADDED<\/strong><\/p>\n\n<p>A related question. Can build in \"regressors\" estimate probability density functions? For example for a given case (a row in a data table) I would like to have not only a single number as a prediction (expected value) but also probabilities of all possible values.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1467121249227,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1467123550236,
        "Question_score":1.0,
        "Question_view_count":332.0,
        "Answer_body":"<p>Currently, you will have to use R or python within Azure ML for confidence interval <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38077884",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1467268176063,
        "Question_original_content":"azur calcul confid interv regress plan try differ regress method provid studio predict numer valu wonder possibl predict correspond confid interv word like regress function tell expect valu predict confid model valu azur regress support function ad relat question build regressor estim probabl densiti function exampl given case row data tabl like singl number predict expect valu probabl possibl valu",
        "Question_preprocessed_content":"azur calcul confid interv regress plan try differ regress method provid studio predict numer valu wonder possibl predict correspond confid interv word like regress function tell expect valu confid valu azur regress support function ad relat question build regressor estim probabl densiti function exampl given case like singl number predict probabl possibl valu",
        "Question_gpt_summary_original":"The user is wondering if Azure ML Studio's regression methods can provide confidence intervals for predicted numeric values and if it can estimate probability density functions for a given case.",
        "Question_gpt_summary":"user wonder studio regress method provid confid interv predict numer valu estim probabl densiti function given case",
        "Answer_original_content":"current us python confid interv",
        "Answer_preprocessed_content":"current us python confid interv",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that currently, the user will have to use R or Python within Azure ML to obtain confidence intervals for predicted numeric values. No solution is provided for estimating probability density functions for a given case.",
        "Answer_gpt_summary":"solut discuss suggest current user us python obtain confid interv predict numer valu solut provid estim probabl densiti function given case"
    },
    {
        "Question_title":"Understanding classification results",
        "Question_body":"<p>I've run a simple two-class neural network where I got this result in the end (eval):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/vw2jQ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vw2jQ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>I think I am going to be happy with the <code>True Positive<\/code> and <code>False Negative<\/code> results. But what does <code>False Positive<\/code> mean? <code>False Positive<\/code> means it did not correctly classify 2002 elements and missed them?<\/p>\n\n<p>The <code>Accuracy<\/code> is 66%, that's really bad right? Whats the difference between that and <code>AUC<\/code>?<\/p>\n\n<p><code>Precision<\/code> suffers because Accuracy also is bad (I hoping for a 80%+)?<\/p>\n\n<p>And how do I flip <code>Positive Label<\/code> and <code>Negative Label<\/code>? I really want to predict the classification where the target is to find <code>CANDIDATE<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1488042320197,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":81.0,
        "Answer_body":"<p>Basically, for the false\/true positives and false\/true negatives :\nYou have detected almost all the CANDIDATE samples in your dataset, 3420 of them were correctly predicted as TRUE and 31 of them were predicted as FALSE. This information is captured in the Recall ratio : 3420\/(3420+31) = 99.1%. It is very high, so very good. <\/p>\n\n<p>However, you have predicted <strong>too many<\/strong> CANDIDATE. Indeed, in all the TRUE values predicted by the model, 3420 were actually TRUE and 2002 were actually FALSE. This makes the Precision ratio bad : 3420\/(3420+2002)=63.1%. Which is not that good. <\/p>\n\n<p>F1 is a combinaison between Precision and Recall, it summarizes them into one value, some kind of weighted average. The formula is 2*(P*R)\/(P+R). So if one of Precision or Recall is bad : the F1score will capture it. <\/p>\n\n<p>You can see that you have a total of 5999 examples in your data set. Out of those, 3451 are really TRUE and 2548 are really FALSE. So you have 57% of your data that is TRUE. If you make a really stupid classifier that classifies everything as TRUE whatever the features are, then you will get 57% accuracy. Given that, 66.1% accuracy is not really good. \nIf you look at the second column of that table, you only predict 577 FALSE out of the 5999 samples. Your classifier is heavily biased towards TRUE predictions. <\/p>\n\n<p>For the AUC, it stands for Area Under the Curve. You can read <a href=\"http:\/\/fastml.com\/what-you-wanted-to-know-about-auc\/\" rel=\"nofollow noreferrer\">more detailed info about it here<\/a>. To summarize : when you predic a value, you don't really get True or False directly. You get a real number between 0 (False) and 1 (True). The way to classify a predicted value, say 0.2, is to use a Threshold. The threshold is by default set to 0.5. So if you predict 0.2, your model will predict to classify it as a False because 0.2&lt;0.5. But you could make that treshold move between 0 and 1. If the classifier is really good, if it discriminates really well the Falses and Trues predictions, then the AUC will be close to 1. If it's really bad, it will be close to 0.5. Refer to the link if you need more information. <\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1488063664327,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/42458982",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1488062187216,
        "Question_original_content":"understand classif result run simpl class neural network got result end eval think go happi true posit fals neg result fals posit mean fals posit mean correctli classifi element miss accuraci bad right what differ auc precis suffer accuraci bad hope flip posit label neg label want predict classif target candid",
        "Question_preprocessed_content":"understand classif result run simpl neural network got result end think go happi result mean mean correctli classifi element miss bad right what differ suffer accuraci bad flip want predict classif target",
        "Question_gpt_summary_original":"The user has encountered challenges in understanding the results of a two-class neural network, specifically the meaning of false positives, the low accuracy rate, the difference between accuracy and AUC, the impact of accuracy on precision, and how to flip positive and negative labels to predict the classification of finding \"CANDIDATE\".",
        "Question_gpt_summary":"user encount challeng understand result class neural network specif mean fals posit low accuraci rate differ accuraci auc impact accuraci precis flip posit neg label predict classif find candid",
        "Answer_original_content":"basic fals true posit fals true neg detect candid sampl dataset correctli predict true predict fals inform captur recal ratio high good predict candid true valu predict model actual true actual fals make precis ratio bad good combinaison precis recal summar valu kind weight averag formula precis recal bad fscore captur total exampl data set true fals data true stupid classifi classifi true featur accuraci given accuraci good look second column tabl predict fals sampl classifi heavili bias true predict auc stand area curv read detail info summar predic valu true fals directli real number fals true wai classifi predict valu us threshold threshold default set predict model predict classifi fals treshold classifi good discrimin fals true predict auc close bad close refer link need inform",
        "Answer_preprocessed_content":"basic posit neg detect candid sampl dataset correctli predict true predict fals inform captur recal ratio high good predict candid true valu predict model actual true actual fals make precis ratio bad good combinaison precis recal summar valu kind weight averag formula precis recal bad score captur total exampl data set true fals data true stupid classifi classifi true featur accuraci given accuraci good look second column tabl predict fals sampl classifi heavili bias true predict auc stand area curv read detail info summar predic valu true fals directli real number wai classifi predict valu us threshold threshold default set predict model predict classifi fals treshold classifi good discrimin fals true predict auc close bad close refer link need inform",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are as follows: \n- False\/true positives and false\/true negatives can be measured using recall ratio and precision ratio. \n- F1 score is a combination of precision and recall, which summarizes them into one value. \n- Accuracy rate can be calculated by dividing the number of correct predictions by the total number of predictions. \n- AUC (Area Under the Curve) is a measure of how well the classifier is able to discriminate between true and false predictions. \nHowever, no solution is provided for flipping positive and negative labels to predict the classification of finding \"CANDIDATE\".",
        "Answer_gpt_summary":"solut provid discuss follow fals true posit fals true neg measur recal ratio precis ratio score combin precis recal summar valu accuraci rate calcul divid number correct predict total number predict auc area curv measur classifi abl discrimin true fals predict solut provid flip posit neg label predict classif find candid"
    },
    {
        "Question_title":"The new languages are missing",
        "Question_body":"Google cloud translation have added new languages. About 24 new languages has been added to Google Translate. Very good job, well done. But they are not listed on this link.\nhttps:\/\/cloud.google.com\/translate\/docs\/languages\n\nI tried to access it using basic v2 API code, but no response came to my translation request. When will this new languages be available to be accessed by v2 APIs? ",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1652751780000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":89.0,
        "Answer_body":"These are the new 24 languages[1].\n\nIn that post there is a research paper[2] where you can see the codes it begins on page 57.\n\nThe document that you shared it is in an internal Work in Progress with no launch date yet.\n\n[1]https:\/\/blog.google\/products\/translate\/24-new-languages\/\u00a0\n\n[2]https:\/\/arxiv.org\/pdf\/2205.03983.pdf\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/The-new-languages-are-missing\/td-p\/423648\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1652889600000,
        "Question_original_content":"new languag miss googl cloud translat ad new languag new languag ad googl translat good job list link http cloud googl com translat doc languag tri access basic api code respons came translat request new languag avail access api",
        "Question_preprocessed_content":"new languag miss googl cloud translat ad new languag new languag ad googl translat good job list link tri access basic api code respons came translat request new languag avail access api",
        "Question_gpt_summary_original":"The user is facing challenges accessing the new languages added to Google Translate as they are not listed on the provided link and are not available to be accessed by v2 APIs.",
        "Question_gpt_summary":"user face challeng access new languag ad googl translat list provid link avail access api",
        "Answer_original_content":"new languag post research paper code begin page document share intern work progress launch date http blog googl product translat new languag http arxiv org pdf pdf view solut origin post",
        "Answer_preprocessed_content":"new languag post research paper code begin page document share intern work progress launch date view solut origin post",
        "Answer_gpt_summary_original":"Solutions provided: \n- The codes for the new languages can be found in a research paper linked in the discussion.\n- The new languages are not yet available for access through v2 APIs as the document shared is still a work in progress with no launch date.",
        "Answer_gpt_summary":"solut provid code new languag research paper link discuss new languag avail access api document share work progress launch date"
    },
    {
        "Question_title":"Restrict the number of nodes used by an Azure Machine Learning pipeine",
        "Question_body":"<p>I have written a pipeline that I want to run on a remote compute cluster within Azure Machine Learning. My aim is to process a large amount of historical data, and to do this I will need to run the pipeline on a large number of input parameter combinations.<\/p>\n\n<p>Is there a way to restrict the number of nodes that the pipeline uses on the cluster? By default it will use all the nodes available to the cluster, and I would like to restrict it so that it only uses a pre-defined maximum. This allows me to leave the rest of the cluster free for other users.<\/p>\n\n<p>My current code to start the pipeline looks like this:<\/p>\n\n<pre><code># Setup the pipeline\nsteps = [data_import_step] # Contains PythonScriptStep\npipeline = Pipeline(workspace=ws, steps=steps)\npipeline.validate()\n\n# Big long list of historical dates that I want to process data for\ndts = pd.date_range('2019-01-01', '2020-01-01', freq='6H', closed='left')\n# Submit the pipeline job\nfor dt in dts:\n    pipeline_run = Experiment(ws, 'my-pipeline-run').submit(\n        pipeline,\n        pipeline_parameters={\n            'import_datetime': dt.strftime('%Y-%m-%dT%H:00'),\n        }\n    )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1592308824897,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":274.0,
        "Answer_body":"<p>For me, the killer feature of Azure ML is not having to worry about load balancing like this. Our team has a compute target with <code>max_nodes=100<\/code> for every feature branch and we have <code>Hyperdrive<\/code> pipelines that result in 130 runs for each pipeline.<\/p>\n<p>We can submit multiple <code>PipelineRun<\/code>s back-to-back and the orchestrator does the heavy lifting of queuing, submitting, all the runs so that the <code>PipelineRun<\/code>s execute in the serial order I submitted them, and that the cluster is never overloaded. This works without issue for us 99% of the time.<\/p>\n<p>If what you're looking for is that you'd like the <code>PipelineRun<\/code>s to be executed in parallel, then you should check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-parallel-run-step#build-and-run-the-pipeline-containing-parallelrunstep\" rel=\"nofollow noreferrer\"><code>ParallelRunStep<\/code><\/a>.<\/p>\n<p>Another option is to isolate your computes. You can have up to 200 <code>ComputeTarget<\/code>s per workspace. Two 50-node <code>ComputeTarget<\/code>s cost the same as one 100-node <code>ComputeTarget<\/code>.<\/p>\n<p>On our team, we use <a href=\"https:\/\/www.pygit2.org\/\" rel=\"nofollow noreferrer\"><code>pygit2<\/code><\/a> to have a <code>ComputeTarget<\/code> created for each feature branch, so that, as data scientists, we can be confident that we're not stepping on our coworkers' toes.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1592842949400,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62407943",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1592597207007,
        "Question_original_content":"restrict number node pipein written pipelin want run remot comput cluster aim process larg histor data need run pipelin larg number input paramet combin wai restrict number node pipelin us cluster default us node avail cluster like restrict us pre defin maximum allow leav rest cluster free user current code start pipelin look like setup pipelin step data import step contain pythonscriptstep pipelin pipelin workspac step step pipelin valid big long list histor date want process data dt date rang freq close left submit pipelin job dt pipelin run experi pipelin run submit pipelin pipelin paramet import datetim strftime",
        "Question_preprocessed_content":"restrict number node pipein written pipelin want run remot comput cluster aim process larg histor data need run pipelin larg number input paramet combin wai restrict number node pipelin us cluster default us node avail cluster like restrict us maximum allow leav rest cluster free user current code start pipelin look like",
        "Question_gpt_summary_original":"The user wants to run a pipeline on a remote compute cluster within Azure Machine Learning to process a large amount of historical data. However, the user wants to restrict the number of nodes used by the pipeline on the cluster to a pre-defined maximum, leaving the rest of the cluster free for other users.",
        "Question_gpt_summary":"user want run pipelin remot comput cluster process larg histor data user want restrict number node pipelin cluster pre defin maximum leav rest cluster free user",
        "Answer_original_content":"killer featur have worri load balanc like team comput target max node featur branch hyperdr pipelin result run pipelin submit multipl pipelinerun orchestr heavi lift queu submit run pipelinerun execut serial order submit cluster overload work issu time look like pipelinerun execut parallel check parallelrunstep option isol comput computetarget workspac node computetarget cost node computetarget team us pygit computetarget creat featur branch data scientist confid step cowork toe",
        "Answer_preprocessed_content":"killer featur have worri load balanc like team comput target featur branch pipelin result run pipelin submit multipl orchestr heavi lift queu submit run execut serial order submit cluster overload work issu time look like execut parallel check option isol comput workspac cost team us creat featur branch data scientist confid step cowork toe",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Use Azure ML's built-in load balancing feature to submit multiple PipelineRuns back-to-back, which will be executed in serial order and ensure that the cluster is never overloaded. This can be achieved by setting the max_nodes parameter to the desired maximum number of nodes.\n- Use ParallelRunStep if parallel execution of PipelineRuns is desired.\n- Isolate computes by creating multiple ComputeTargets, each with a pre-defined maximum number of nodes. This will ensure that the user's pipeline does not use more nodes than the pre-defined maximum, leaving the rest of the cluster free for other users.\n- Use pygit2 to create a ComputeTarget for each feature branch, ensuring that data scientists do not step on each other",
        "Answer_gpt_summary":"possibl solut mention discuss us built load balanc featur submit multipl pipelinerun execut serial order ensur cluster overload achiev set max node paramet desir maximum number node us parallelrunstep parallel execut pipelinerun desir isol comput creat multipl computetarget pre defin maximum number node ensur user pipelin us node pre defin maximum leav rest cluster free user us pygit creat computetarget featur branch ensur data scientist step"
    },
    {
        "Question_title":"Remove multiple runs at the same time",
        "Question_body":"<p>Hello,<\/p>\n<p>I think it would be beneficial to select and delete several experiments at the same time.<br>\nNow I have to delete one by one and it is very time consuming.<\/p>\n<p>Thank you!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1652776776526,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":142.0,
        "Answer_body":"<p>Hey <a class=\"mention\" href=\"\/u\/lucasventura\">@lucasventura<\/a>, you can do it like <a href=\"https:\/\/docs.wandb.ai\/ref\/app\/features\/runs-table#filter-and-delete-unwanted-runs\">this<\/a>.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/remove-multiple-runs-at-the-same-time\/2435",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1652874850582,
        "Question_original_content":"remov multipl run time hello think benefici select delet experi time delet time consum thank",
        "Question_preprocessed_content":"remov multipl run time hello think benefici select delet experi time delet time consum thank",
        "Question_gpt_summary_original":"The user is facing a challenge of having to delete experiments one by one, which is time-consuming, and suggests the option to select and delete multiple experiments at the same time.",
        "Question_gpt_summary":"user face challeng have delet experi time consum suggest option select delet multipl experi time",
        "Answer_original_content":"hei lucasventura like",
        "Answer_preprocessed_content":"hei like",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting a link that shows how to filter and delete unwanted runs, which allows the user to select and delete multiple experiments at the same time.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest link show filter delet unwant run allow user select delet multipl experi time"
    },
    {
        "Question_title":"Reading Data in Vertex AI Pipelines",
        "Question_body":"<p>This is my first time using Google's Vertex AI Pipelines. I checked <a href=\"https:\/\/codelabs.developers.google.com\/vertex-pipelines-intro?hl=en#0\" rel=\"nofollow noreferrer\">this codelab<\/a> as well as <a href=\"https:\/\/towardsdatascience.com\/how-to-set-up-custom-vertex-ai-pipelines-step-by-step-467487f81cad\" rel=\"nofollow noreferrer\">this post<\/a> and <a href=\"https:\/\/medium.com\/google-cloud\/google-vertex-ai-the-easiest-way-to-run-ml-pipelines-3a41c5ed153\" rel=\"nofollow noreferrer\">this post<\/a>, on top of some links derived from the <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/pipelines\/introduction?hl=es-419\" rel=\"nofollow noreferrer\">official documentation<\/a>. I decided to put all that knowledge to work, in some toy example: I was planning to build a pipeline consisting of 2 components: &quot;get-data&quot; (which reads some .csv file stored in Cloud Storage) and &quot;report-data&quot; (which basically returns the shape of the .csv data read in the previous component). Furthermore, I was cautious to include <a href=\"https:\/\/stackoverflow.com\/questions\/71351821\/reading-file-from-vertex-ai-and-google-cloud-storage\">some suggestions<\/a> provided in this forum. The code I currently have, goes as follows:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>\nfrom kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom google.cloud import aiplatform\n\n# Components section   \n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    import pandas as pd\n    from google.cloud import storage\n    \n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # path = &quot;gs:\/\/my-bucket\/program_grouping_data.zip&quot;\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n):\n    import pandas as pd\n    df = pd.read_csv(inputd.path)\n    return df.shape\n\n\n# Pipeline section\n\n@pipeline(\n    # Default pipeline root. You can override it when submitting the pipeline.\n    pipeline_root=PIPELINE_ROOT,\n    # A name for the pipeline.\n    name=&quot;my-pipeline&quot;,\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n\n# Compilation section\n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n# Running and submitting job\n\nfrom datetime import datetime\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={&quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;, &quot;bucket&quot;: &quot;my-bucket&quot;},\n    enable_caching=True,\n)\n\nrun1.submit()\n<\/code><\/pre>\n<p>I was happy to see that the pipeline compiled with no errors, and managed to submit the job. However &quot;my happiness lasted short&quot;, as when I went to Vertex AI Pipelines, I stumbled upon some &quot;error&quot;, which goes like:<\/p>\n<blockquote>\n<p>The DAG failed because some tasks failed. The failed tasks are: [get-data].; Job (project_id = my-project, job_id = 4290278978419163136) is failed due to the above error.; Failed to handle the job: {project_number = xxxxxxxx, job_id = 4290278978419163136}<\/p>\n<\/blockquote>\n<p>I did not find any related info on the web, neither could I find any log or something similar, and I feel a bit overwhelmed that the solution to this (seemingly) easy example, is still eluding me.<\/p>\n<p>Quite obviously, I don't what or where I am mistaking. Any suggestion?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":5,
        "Question_created_time":1650588056410,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1650845389252,
        "Question_score":4.0,
        "Question_view_count":892.0,
        "Answer_body":"<p>With some suggestions provided in the comments, I think I managed to make my demo pipeline work. I will first include the updated code:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from kfp.v2 import compiler\nfrom kfp.v2.dsl import pipeline, component, Dataset, Input, Output\nfrom datetime import datetime\nfrom google.cloud import aiplatform\nfrom typing import NamedTuple\n\n\n# Importing 'COMPONENTS' of the 'PIPELINE'\n\n@component(\n    packages_to_install=[\n        &quot;google-cloud-storage&quot;,\n        &quot;pandas&quot;,\n    ],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;get_data.yaml&quot;\n)\ndef get_data(\n    bucket: str,\n    url: str,\n    dataset: Output[Dataset],\n):\n    &quot;&quot;&quot;Reads a csv file, from some location in Cloud Storage&quot;&quot;&quot;\n    import ast\n    import pandas as pd\n    from google.cloud import storage\n    \n    # 'Pulling' demo .csv data from a know location in GCS\n    storage_client = storage.Client(&quot;my-project&quot;)\n    bucket = storage_client.get_bucket(bucket)\n    blob = bucket.blob(url)\n    blob.download_to_filename('localdf.csv')\n    \n    # Reading the pulled demo .csv data\n    df = pd.read_csv('localdf.csv', compression='zip')\n    df['new_skills'] = df['new_skills'].apply(ast.literal_eval)\n    df.to_csv(dataset.path + &quot;.csv&quot; , index=False, encoding='utf-8-sig')\n\n\n@component(\n    packages_to_install=[&quot;pandas&quot;],\n    base_image=&quot;python:3.9&quot;,\n    output_component_file=&quot;report_data.yaml&quot;\n)\ndef report_data(\n    inputd: Input[Dataset],\n) -&gt; NamedTuple(&quot;output&quot;, [(&quot;rows&quot;, int), (&quot;columns&quot;, int)]):\n    &quot;&quot;&quot;From a passed csv file existing in Cloud Storage, returns its dimensions&quot;&quot;&quot;\n    import pandas as pd\n    \n    df = pd.read_csv(inputd.path+&quot;.csv&quot;)\n    \n    return df.shape\n\n\n# Building the 'PIPELINE'\n\n@pipeline(\n    # i.e. in my case: PIPELINE_ROOT = 'gs:\/\/my-bucket\/test_vertex\/pipeline_root\/'\n    # Can be overriden when submitting the pipeline\n    pipeline_root=PIPELINE_ROOT,\n    name=&quot;readcsv-pipeline&quot;,  # Your own naming for the pipeline.\n)\ndef my_pipeline(\n    url: str = &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n    bucket: str = &quot;my-bucket&quot;\n):\n    dataset_task = get_data(bucket, url)\n\n    dimensions = report_data(\n        dataset_task.output\n    )\n    \n\n# Compiling the 'PIPELINE'    \n\ncompiler.Compiler().compile(\n    pipeline_func=my_pipeline, package_path=&quot;pipeline_job.json&quot;\n)\n\n\n# Running the 'PIPELINE'\n\nTIMESTAMP = datetime.now().strftime(&quot;%Y%m%d%H%M%S&quot;)\n\nrun1 = aiplatform.PipelineJob(\n    display_name=&quot;my-pipeline&quot;,\n    template_path=&quot;pipeline_job.json&quot;,\n    job_id=&quot;mlmd-pipeline-small-{0}&quot;.format(TIMESTAMP),\n    parameter_values={\n        &quot;url&quot;: &quot;test_vertex\/pipeline_root\/program_grouping_data.zip&quot;,\n        &quot;bucket&quot;: &quot;my-bucket&quot;\n    },\n    enable_caching=True,\n)\n\n# Submitting the 'PIPELINE'\n\nrun1.submit()\n<\/code><\/pre>\n<p>Now, I will add some complementary comments, which in sum, managed to solve my problem:<\/p>\n<ul>\n<li>First, having the &quot;Logs Viewer&quot; (roles\/logging.viewer) enabled for your user, will greatly help to troubleshoot any existing error in your pipeline (Note: that role worked for me, however you might want to look for a better matching role for you own purposes <a href=\"https:\/\/cloud.google.com\/logging\/docs\/access-control?&amp;_ga=2.212939101.-833851896.1650314090&amp;_gac=1.182768084.1650632490.Cj0KCQjwpImTBhCmARIsAKr58cw9X0TGy2YeN-CFsM4RvEXDH_vL54Ce1ECrWh4_aJNoPEhgtXusUhwaAlIUEALw_wcB#permissions_and_roles\" rel=\"nofollow noreferrer\">here<\/a>). Those errors will appear as &quot;Logs&quot;, which can be accessed by clicking the corresponding button:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jpgfU.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<ul>\n<li>NOTE: In the picture above, when the &quot;Logs&quot; are displayed, it might be helpful to carefully check each log (close to the time when you created you pipeline), as generally each eof them corresponds with a single warning or error line:<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CpTkZ.png\" alt=\"Verte AI Pipelines Logs\" \/><\/a><\/p>\n<ul>\n<li>Second, the output of my pipeline was a tuple. In my original approach, I just returned the plain tuple, but it is advised to return a <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html#typing.NamedTuple\" rel=\"nofollow noreferrer\">NamedTuple<\/a> instead. In general, if you need to input \/ output one or more &quot;<em>small values<\/em>&quot; (int or str, for any reason), pick a NamedTuple to do so.<\/li>\n<li>Third, when the connection between your pipelines is <code>Input[Dataset]<\/code> or <code>Ouput[Dataset]<\/code>, adding the file extension is needed (and quite easy to forget). Take for instance the ouput of the <code>get_data<\/code> component, and notice how the data is recorded by specifically adding the file extension, i.e. <code>dataset.path + &quot;.csv&quot;<\/code>.<\/li>\n<\/ul>\n<p>Of course, this is a very tiny example, and projects can easily scale to huge projects, however as some sort of &quot;Hello Vertex AI Pipelines&quot; it will work well.<\/p>\n<p>Thank you.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1650870474732,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71962260",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1650657469576,
        "Question_original_content":"read data pipelin time googl pipelin check codelab post post link deriv offici document decid knowledg work toi exampl plan build pipelin consist compon data read csv file store cloud storag report data basic return shape csv data read previou compon furthermor cautiou includ suggest provid forum code current goe follow kfp import compil kfp dsl import pipelin compon dataset input output googl cloud import aiplatform compon section compon packag instal googl cloud storag panda base imag python output compon file data yaml def data bucket str url str dataset output dataset import panda googl cloud import storag storag client storag client project bucket storag client bucket bucket blob bucket blob url blob download filenam localdf csv path bucket program group data zip read csv localdf csv compress zip new skill new skill appli ast liter eval csv dataset path csv index fals encod utf sig compon packag instal panda base imag python output compon file report data yaml def report data inputd input dataset import panda read csv inputd path return shape pipelin section pipelin default pipelin root overrid submit pipelin pipelin root pipelin root pipelin pipelin def pipelin url str test vertex pipelin root program group data zip bucket str bucket dataset task data bucket url dimens report data dataset task output compil section compil compil compil pipelin func pipelin packag path pipelin job json run submit job datetim import datetim timestamp datetim strftime run aiplatform pipelinejob displai pipelin templat path pipelin job json job mlmd pipelin small format timestamp paramet valu url test vertex pipelin root program group data zip bucket bucket enabl cach true run submit happi pipelin compil error manag submit job happi last short went pipelin stumbl error goe like dag fail task fail fail task data job project project job fail error fail handl job project number job relat info web log similar feel bit overwhelm solut seemingli easi exampl elud obvious mistak suggest",
        "Question_preprocessed_content":"read data pipelin time googl pipelin check codelab post post link deriv offici document decid knowledg work toi exampl plan build pipelin consist compon furthermor cautiou includ suggest provid forum code current goe follow happi pipelin compil error manag submit job happi last short went pipelin stumbl error goe like dag fail task fail fail task job fail fail handl job relat info web log similar feel bit overwhelm solut easi exampl elud obvious mistak suggest",
        "Question_gpt_summary_original":"The user encountered challenges while using Google's Vertex AI Pipelines to build a pipeline consisting of two components: \"get-data\" and \"report-data\". Although the pipeline compiled with no errors, the user encountered an error when submitting the job, and the DAG failed because some tasks failed. The user was unable to find any related information on the web or any logs to identify the issue.",
        "Question_gpt_summary":"user encount challeng googl pipelin build pipelin consist compon data report data pipelin compil error user encount error submit job dag fail task fail user unabl relat inform web log identifi issu",
        "Answer_original_content":"suggest provid comment think manag demo pipelin work includ updat code kfp import compil kfp dsl import pipelin compon dataset input output datetim import datetim googl cloud import aiplatform type import namedtupl import compon pipelin compon packag instal googl cloud storag panda base imag python output compon file data yaml def data bucket str url str dataset output dataset read csv file locat cloud storag import ast import panda googl cloud import storag pull demo csv data know locat gc storag client storag client project bucket storag client bucket bucket blob bucket blob url blob download filenam localdf csv read pull demo csv data read csv localdf csv compress zip new skill new skill appli ast liter eval csv dataset path csv index fals encod utf sig compon packag instal panda base imag python output compon file report data yaml def report data inputd input dataset namedtupl output row int column int pass csv file exist cloud storag return dimens import panda read csv inputd path csv return shape build pipelin pipelin case pipelin root bucket test vertex pipelin root overriden submit pipelin pipelin root pipelin root readcsv pipelin name pipelin def pipelin url str test vertex pipelin root program group data zip bucket str bucket dataset task data bucket url dimens report data dataset task output compil pipelin compil compil compil pipelin func pipelin packag path pipelin job json run pipelin timestamp datetim strftime run aiplatform pipelinejob displai pipelin templat path pipelin job json job mlmd pipelin small format timestamp paramet valu url test vertex pipelin root program group data zip bucket bucket enabl cach true submit pipelin run submit add complementari comment sum manag solv problem have log viewer role log viewer enabl user greatli help troubleshoot exist error pipelin note role work want look better match role purpos error appear log access click correspond button note pictur log displai help carefulli check log close time creat pipelin gener eof correspond singl warn error line second output pipelin tupl origin approach return plain tupl advis return namedtupl instead gener need input output small valu int str reason pick namedtupl connect pipelin input dataset ouput dataset ad file extens need easi forget instanc ouput data compon notic data record specif ad file extens dataset path csv cours tini exampl project easili scale huge project sort hello pipelin work thank",
        "Answer_preprocessed_content":"suggest provid comment think manag demo pipelin work includ updat code add complementari comment sum manag solv problem have log viewer enabl user greatli help troubleshoot exist error pipelin error appear log access click correspond button note pictur log displai help carefulli check log gener eof correspond singl warn error line second output pipelin tupl origin approach return plain tupl advis return namedtupl instead gener need input output small valu pick namedtupl connect pipelin ad file extens need instanc ouput compon notic data record specif ad file extens cours tini exampl project easili scale huge project sort hello pipelin work thank",
        "Answer_gpt_summary_original":"Solutions provided:\n- Enabling the \"Logs Viewer\" role for the user to troubleshoot errors in the pipeline.\n- Returning a NamedTuple instead of a plain tuple when the output of the pipeline is a tuple.\n- Adding the file extension when using Input[Dataset] or Output[Dataset] in the pipeline.",
        "Answer_gpt_summary":"solut provid enabl log viewer role user troubleshoot error pipelin return namedtupl instead plain tupl output pipelin tupl ad file extens input dataset output dataset pipelin"
    },
    {
        "Question_title":"azure cosmos db as a datastore in ml",
        "Question_body":"Hi, I'm wondering if I can register azure cosmos db as a datastore in azure machine learning?\nFrom your documentation, it seems not https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore%28class%29?view=azure-ml-py\n\nDo you have a plan to implement the feature in near future?\n\n\nAny recommended alternative solutions for now?\n\nThanks.",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1597331768270,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. Currently, Cosmos DB isn't a supported datasource when using Azure ML datastores. However, the product team are aware of this request and will provide updates accordingly. An alternative for now will be to use Azure ML Studio (Classic) which supports Cosmos DB as data source. You can also try a heuristic approach via Execute Python Script module in Designer to import data using python. Hope this helps.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/66297\/azure-cosmos-db-as-a-datastore-in-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1597444998180,
        "Question_original_content":"azur cosmo datastor wonder regist azur cosmo datastor document http doc microsoft com python api core core datastor class view azur plan implement featur near futur recommend altern solut thank",
        "Question_preprocessed_content":"azur cosmo datastor wonder regist azur cosmo datastor document plan implement featur near futur recommend altern solut thank",
        "Question_gpt_summary_original":"The user is facing a challenge of not being able to register Azure Cosmos DB as a datastore in Azure Machine Learning, as it is not supported according to the documentation. The user is inquiring about any plans to implement this feature in the future and is also seeking alternative solutions for the time being.",
        "Question_gpt_summary":"user face challeng abl regist azur cosmo datastor support accord document user inquir plan implement featur futur seek altern solut time",
        "Answer_original_content":"thank reach current cosmo isn support datasourc datastor product team awar request provid updat accordingli altern us studio classic support cosmo data sourc try heurist approach execut python script modul design import data python hope help",
        "Answer_preprocessed_content":"thank reach current cosmo isn support datasourc datastor product team awar request provid updat accordingli altern us studio support cosmo data sourc try heurist approach execut python script modul design import data python hope help",
        "Answer_gpt_summary_original":"Solution:\n- Currently, Azure Cosmos DB is not a supported datastore in Azure Machine Learning.\n- The product team is aware of this request and will provide updates accordingly.\n- An alternative solution for now is to use Azure ML Studio (Classic) which supports Cosmos DB as a data source.\n- Another option is to use the Execute Python Script module in Designer to import data using Python.",
        "Answer_gpt_summary":"solut current azur cosmo support datastor product team awar request provid updat accordingli altern solut us studio classic support cosmo data sourc option us execut python script modul design import data python"
    },
    {
        "Question_title":"RuntimeError: Load model failed - Score machine learning models with PREDICT in serverless Apache Spark pools (Synapse & Azure Machine learning AML)",
        "Question_body":"Hi all,\n\nI am following the steps on this tutorial:\nTutorial: Score machine learning models with PREDICT in serverless Apache Spark pools tutorial-score-model-predict-spark-pool\nI tried to used a model created with AutoML and another from designer and I am getting this error: RuntimeError: Load model failed\n\n\n\n\nI am using the model according to this: https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/631200\/what-is-aml-model-uri-predict-in-serverless-apache.html?childToView=637754#comment-637754\n\nThank you for your help.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1638981543063,
        "Question_favorite_count":14.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @Anaid-6816,\n\nBefore running this script, update it with the URI for ADLS Gen2 data file along with model output return data type and ADLS\/AML URI for the model file.\n\n #Set model URI\n        #Set AML URI, if trained model is registered in AML\n           AML_MODEL_URI = \"<aml model uri>\" #In URI \":x\" signifies model version in AML. You can   choose which model version you want to run. If \":x\" is not provided then by default   latest version will be picked.\n    \n        #Set ADLS URI, if trained model is uploaded in ADLS\n           ADLS_MODEL_URI = \"abfss:\/\/<filesystemname>@<account name>.dfs.core.windows.net\/<model   mlflow folder path>\"\n\nModel URI from AML Workspace:\n\n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n AML_MODEL_URI_SKLEARN = \"aml:\/\/mlflow_sklearn:1\" #Here \":1\" signifies model version in AML. We can choose which version we want to run. If \":1\" is not provided then by default latest version will be picked\n RETURN_TYPES = \"INT\"\n RUNTIME = \"mlflow\"\n\nModel URI uploaded to ADLS Gen2:\n\n DATA_FILE = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/AML\/LengthOfStay_cooked_small.csv\"\n AML_MODEL_URI_SKLEARN = \"abfss:\/\/data@cheprasynapse.dfs.core.windows.net\/linear_regression\/linear_regression\" #Here \":1\" signifies model version in AML. We can choose which version we want to run. If \":1\" is not provided then by default latest version will be picked\n RETURN_TYPES = \"INT\"\n RUNTIME = \"mlflow\"\n\n\n\nHope this will help. Please let us know if any further queries.\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/656548\/what-is-aml-model-uri-predict-in-serverless-apache-1.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1639460160303,
        "Question_original_content":"runtimeerror load model fail score machin learn model predict serverless apach spark pool synaps aml follow step tutori tutori score machin learn model predict serverless apach spark pool tutori score model predict spark pool tri model creat automl design get error runtimeerror load model fail model accord http doc microsoft com answer question aml model uri predict serverless apach html childtoview comment thank help",
        "Question_preprocessed_content":"runtimeerror load model fail score machin learn model predict serverless apach spark pool follow step tutori tutori score machin learn model predict serverless apach spark pool tri model creat automl design get error runtimeerror load model fail model accord thank help",
        "Question_gpt_summary_original":"The user is encountering an error message \"RuntimeError: Load model failed\" while trying to use a machine learning model created with AutoML and another from designer in a tutorial on scoring machine learning models with PREDICT in serverless Apache Spark pools. The user is following the steps provided in the tutorial and using the model as per the instructions given in the documentation.",
        "Question_gpt_summary":"user encount error messag runtimeerror load model fail try us machin learn model creat automl design tutori score machin learn model predict serverless apach spark pool user follow step provid tutori model instruct given document",
        "Answer_original_content":"hello anaid run script updat uri adl gen data file model output return data type adl aml uri model file set model uri set aml uri train model regist aml aml model uri uri signifi model version aml choos model version want run provid default latest version pick set adl uri train model upload adl adl model uri abfss df core window net model uri aml workspac data file abfss data cheprasynaps df core window net aml lengthofstai cook small csv aml model uri sklearn aml sklearn signifi model version aml choos version want run provid default latest version pick return type int runtim model uri upload adl gen data file abfss data cheprasynaps df core window net aml lengthofstai cook small csv aml model uri sklearn abfss data cheprasynaps df core window net linear regress linear regress signifi model version aml choos version want run provid default latest version pick return type int runtim hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"hello run script updat uri adl gen data file model output return data type uri model file set model uri set aml uri train model regist aml uri signifi model version aml choos model version want run provid default latest version pick set adl uri train model upload adl model uri aml workspac signifi model version aml choos version want run provid default latest version pick int runtim model uri upload adl gen signifi model version aml choos version want run provid default latest version pick int runtim hope help let know queri forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the error message \"RuntimeError: Load model failed\" encountered by the user. The solution involves updating the script with the URI for ADLS Gen2 data file along with model output return data type and ADLS\/AML URI for the model file. The user can set the model URI from AML Workspace or uploaded to ADLS Gen2. The solution is provided in the form of code snippets and instructions.",
        "Answer_gpt_summary":"solut discuss provid solut error messag runtimeerror load model fail encount user solut involv updat script uri adl gen data file model output return data type adl aml uri model file user set model uri aml workspac upload adl gen solut provid form code snippet instruct"
    },
    {
        "Question_title":"How to configute WebServiceOutput?",
        "Question_body":"Hi, I trained and deployed a ML model via Auto ML. The result looks like this:\n\"\\\"{\\\\\\\"result\\\\\\\": [\\\\\\\"Test\\\\\\\"]}\\\"\"\n\nOnce I did the same with an endpoint created with the Azure ML Designer my result looks like this:\n\"{\\\"Results\\\": {\\\"WebServiceOutput0\\\": [{\\\"Scored Labels\\\": \\\"Test\\\"}]}}\"\n\n\n\n\nIs there a way to configure the response that it looks similar to the AutoML response?\n\nThanks :)",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1606819526077,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@27051995 Unfortunately, AutoML and AML Designer currently generates 2 different swagger format automatically, and there is no way to configure the output format. We are working on to address this inconsistency, and the Designer swagger format will be the converged format. Cheers!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/181635\/how-to-configute-webserviceoutput.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1606912260000,
        "Question_original_content":"configut webserviceoutput train deploi model auto result look like result test endpoint creat design result look like result webserviceoutput score label test wai configur respons look similar automl respons thank",
        "Question_preprocessed_content":"configut webserviceoutput train deploi model auto result look like endpoint creat design result look like wai configur respons look similar automl respons thank",
        "Question_gpt_summary_original":"The user is facing a challenge in configuring the WebServiceOutput to match the format of the AutoML response. They have trained and deployed a ML model via AutoML and the result is in a different format than the result obtained from an endpoint created with the Azure ML Designer. The user is seeking guidance on how to configure the response to match the AutoML format.",
        "Question_gpt_summary":"user face challeng configur webserviceoutput match format automl respons train deploi model automl result differ format result obtain endpoint creat design user seek guidanc configur respons match automl format",
        "Answer_original_content":"unfortun automl aml design current gener differ swagger format automat wai configur output format work address inconsist design swagger format converg format cheer",
        "Answer_preprocessed_content":"unfortun automl aml design current gener differ swagger format automat wai configur output format work address inconsist design swagger format converg format cheer",
        "Answer_gpt_summary_original":"Solution: Unfortunately, no solution is provided in the discussion. The user is informed that currently, there is no way to configure the output format to match the AutoML format. The team is working on addressing this inconsistency, and the Designer swagger format will be the converged format in the future.",
        "Answer_gpt_summary":"solut unfortun solut provid discuss user inform current wai configur output format match automl format team work address inconsist design swagger format converg format futur"
    },
    {
        "Question_title":"How do I version control Azure ML workspaces with custom environments and pipelines?",
        "Question_body":"<p>I'm trying to figure out how viable Azure ML in production; I would like to accomplish the following:<\/p>\n\n<ol>\n<li>Specify <em>custom environments<\/em> for my pipelines using a <em>pip file<\/em> and use them in a pipeline<\/li>\n<li><em>Declaratively<\/em> specify my workspace, environments and pipelines in an <em>Azure DevOps repo<\/em><\/li>\n<li><em>Reproducibly<\/em> deploy my Azure ML workspace to my subscription using an <em>Azure DevOps pipeline<\/em><\/li>\n<\/ol>\n\n<p>I found an <a href=\"https:\/\/stackoverflow.com\/questions\/60506398\/how-do-i-use-an-environment-in-an-ml-azure-pipeline\">explanation of how to specify environments using notebooks<\/a> but this seems ill-suited for the second and third requirements I have.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1583316022363,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1583348292107,
        "Question_score":1.0,
        "Question_view_count":659.0,
        "Answer_body":"<p>Currently, we have a python script, <code>pipeline.py<\/code> that uses the <code>azureml-sdk<\/code>to create, register and run all of our ML artifacts (envs, pipelines, models). We call this script in our Azure DevOps CI pipeline with a Python Script task after building the right pip env from the requirements file in our repo.<\/p>\n\n<p>However, it is worth noting there is YAML support for ML artifact definition. Though I don't know if the existing support will cover all of your bases (though that is the plan).<\/p>\n\n<p>Here's some great docs from MSFT to get you started:<\/p>\n\n<ul>\n<li><a href=\"https:\/\/github.com\/microsoft\/MLOpsPython\" rel=\"nofollow noreferrer\">GitHub Template repo of an end-to-end example of ML pipeline + deployment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments\" rel=\"nofollow noreferrer\">How to define\/create an environment (using Pip or Conda) and use it in a remote compute context<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/devops\/pipelines\/targets\/azure-machine-learning?context=azure%2Fmachine-learning%2Fservice%2Fcontext%2Fml-context&amp;view=azure-devops&amp;tabs=yaml\" rel=\"nofollow noreferrer\">Azure Pipelines guidance on CI\/CD for ML Service<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/reference-pipeline-yaml\" rel=\"nofollow noreferrer\">Defining ML pipelines in YAML<\/a><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1583340824232,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60523435",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1583340584892,
        "Question_original_content":"version control workspac custom environ pipelin try figur viabl product like accomplish follow specifi custom environ pipelin pip file us pipelin declar specifi workspac environ pipelin azur devop repo reproduc deploi workspac subscript azur devop pipelin explan specifi environ notebook ill suit second requir",
        "Question_preprocessed_content":"version control workspac custom environ pipelin try figur viabl product like accomplish follow specifi custom environ pipelin pip file us pipelin declar specifi workspac environ pipelin azur devop repo reproduc deploi workspac subscript azur devop pipelin explan specifi environ notebook second requir",
        "Question_gpt_summary_original":"The user is facing challenges in version controlling Azure ML workspaces with custom environments and pipelines. They want to specify custom environments for their pipelines using a pip file, declaratively specify their workspace, environments, and pipelines in an Azure DevOps repo, and reproducibly deploy their Azure ML workspace to their subscription using an Azure DevOps pipeline. The user found an explanation of how to specify environments using notebooks, but it is not suitable for their requirements.",
        "Question_gpt_summary":"user face challeng version control workspac custom environ pipelin want specifi custom environ pipelin pip file declar specifi workspac environ pipelin azur devop repo reproduc deploi workspac subscript azur devop pipelin user explan specifi environ notebook suitabl requir",
        "Answer_original_content":"current python script pipelin us sdkto creat regist run artifact env pipelin model script azur devop pipelin python script task build right pip env requir file repo worth note yaml support artifact definit know exist support cover base plan great doc msft start github templat repo end end exampl pipelin deploy defin creat environ pip conda us remot comput context azur pipelin guidanc servic defin pipelin yaml",
        "Answer_preprocessed_content":"current python script us creat regist run artifact script azur devop pipelin python script task build right pip env requir file repo worth note yaml support artifact definit know exist support cover base great doc msft start github templat repo exampl pipelin deploy environ us remot comput context azur pipelin guidanc servic defin pipelin yaml",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Using YAML support for ML artifact definition to declaratively specify workspace, environments, and pipelines in an Azure DevOps repo.\n- Creating and using environments (using Pip or Conda) in a remote compute context, as explained in the Microsoft documentation.\n- Following the Azure Pipelines guidance on CI\/CD for ML Service.\n- Defining ML pipelines in YAML, as explained in the Microsoft documentation.\n\nNo solution is mentioned that specifically addresses the challenge of specifying custom environments for pipelines using a pip file.",
        "Answer_gpt_summary":"possibl solut mention discuss yaml support artifact definit declar specifi workspac environ pipelin azur devop repo creat environ pip conda remot comput context explain microsoft document follow azur pipelin guidanc servic defin pipelin yaml explain microsoft document solut mention specif address challeng specifi custom environ pipelin pip file"
    },
    {
        "Question_title":"Connect to redshift using sagemaker notebook instances",
        "Question_body":"<p>I would like to connect to redshift using sagemaker notebook instances. I want to run Unload commands to unload data from redshift to s3 using IAM role and schedule the sagemaker notebook.\nI want to know how I can import db credentials in sagemaker without hardcoding.<\/p>\n<p>Thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1618603786843,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1504.0,
        "Answer_body":"<p>You can use <a href=\"https:\/\/docs.aws.amazon.com\/secretsmanager\/latest\/userguide\/intro.html\" rel=\"nofollow noreferrer\">AWS Secrets Manager<\/a> to store and access credentials. Your Sagemaker execution role should have permission to read from Secrets Manager (AFAIK AWS managed-role does have it). This is the same mechanism that's used by Sagemaker notebooks to get access to github repo, for example<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67131617",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1618908321396,
        "Question_original_content":"connect redshift notebook instanc like connect redshift notebook instanc want run unload command unload data redshift iam role schedul notebook want know import credenti hardcod thank",
        "Question_preprocessed_content":"connect redshift notebook instanc like connect redshift notebook instanc want run unload command unload data redshift iam role schedul notebook want know import credenti hardcod thank",
        "Question_gpt_summary_original":"The user is facing challenges in connecting to Redshift using Sagemaker notebook instances and wants to unload data from Redshift to S3 using IAM role and schedule the Sagemaker notebook. They are also looking for a way to import database credentials in Sagemaker without hardcoding.",
        "Question_gpt_summary":"user face challeng connect redshift notebook instanc want unload data redshift iam role schedul notebook look wai import databas credenti hardcod",
        "Answer_original_content":"us aw secret manag store access credenti execut role permiss read secret manag afaik aw manag role mechan notebook access github repo exampl",
        "Answer_preprocessed_content":"us aw secret manag store access credenti execut role permiss read secret manag mechan notebook access github repo exampl",
        "Answer_gpt_summary_original":"Solution: The user can use AWS Secrets Manager to store and access credentials. The Sagemaker execution role should have permission to read from Secrets Manager. This is the same mechanism used by Sagemaker notebooks to access Github repositories.",
        "Answer_gpt_summary":"solut user us aw secret manag store access credenti execut role permiss read secret manag mechan notebook access github repositori"
    },
    {
        "Question_title":"AWS Sagemaker Deploy fails",
        "Question_body":"<p>I am following the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ex1-deploy-model.html\" rel=\"noreferrer\">sage maker documentation<\/a> to train and deploy an ML model. I am using the high-level Python library provided by Amazon SageMaker to achieve this. <\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n\n<p>The deployment fails with error<\/p>\n\n<p>ResourceLimitExceeded: An error occurred (ResourceLimitExceeded) when calling the CreateEndpoint operation: The account-level service limit 'ml.c4.8xlarge for endpoint usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 1 Instances. <\/p>\n\n<p>Where am I going wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1543844726537,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":7.0,
        "Question_view_count":5932.0,
        "Answer_body":"<p>I resolved the issue by changing the instance type:<\/p>\n\n<pre><code>kmeans_predictor = kmeans.deploy(initial_instance_count=1,\n                                 instance_type='ml.t2.medium')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53595157",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1543906305888,
        "Question_original_content":"deploi fail follow sage maker document train deploi model high level python librari provid achiev kmean predictor kmean deploi initi instanc count instanc type xlarg deploy fail error resourcelimitexceed error occur resourcelimitexceed call createendpoint oper account level servic limit xlarg endpoint usag instanc current util instanc request delta instanc go wrong",
        "Question_preprocessed_content":"deploi fail follow sage maker document train deploi model python librari provid achiev deploy fail error resourcelimitexceed error occur call createendpoint oper servic limit endpoint usag instanc current util instanc request delta instanc go wrong",
        "Question_gpt_summary_original":"The user encountered a challenge while following the AWS Sagemaker documentation to train and deploy an ML model using the high-level Python library provided by Amazon SageMaker. The deployment failed with an error message indicating that the account-level service limit for endpoint usage has been exceeded. The user is seeking assistance to resolve the issue.",
        "Question_gpt_summary":"user encount challeng follow document train deploi model high level python librari provid deploy fail error messag indic account level servic limit endpoint usag exceed user seek assist resolv issu",
        "Answer_original_content":"resolv issu chang instanc type kmean predictor kmean deploi initi instanc count instanc type medium",
        "Answer_preprocessed_content":"resolv issu chang instanc type",
        "Answer_gpt_summary_original":"Solution: The user resolved the issue by changing the instance type to 'ml.t2.medium' while deploying the model.",
        "Answer_gpt_summary":"solut user resolv issu chang instanc type medium deploi model"
    },
    {
        "Question_title":"GCP Vertex AI Service Agent access to GCR image Error",
        "Question_body":"<p>I am getting the below error.\nDoes anyone have any idea how to solve it?<\/p>\n<pre><code>Failed to create pipeline job. Error: Vertex AI Service Agent \n'XXXXX@gcp-sa-aiplatform-cc.iam.gserviceaccount.com' should be granted\n access to the image gcr.io\/gcp-project-id\/application:latest\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_created_time":1643025892210,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1643085989932,
        "Question_score":3.0,
        "Question_view_count":707.0,
        "Answer_body":"<p><code>{PROJECT_NUMBER}@gcp-sa-aiplatform-cc.iam.gserviceaccount.com<\/code> is google's <a href=\"https:\/\/cloud.google.com\/iam\/docs\/service-agents\" rel=\"nofollow noreferrer\">AI Platform service agent<\/a>.\nThis Service agent requires access to read\/pull the docker image from your project's gcr to create container for pipeline run.<\/p>\n<p>If You have permission to edit <a href=\"https:\/\/cloud.google.com\/iam\/docs\/understanding-roles\" rel=\"nofollow noreferrer\">IAM roles<\/a>, You can try adding <a href=\"https:\/\/cloud.google.com\/artifact-registry\/docs\/access-control#roles\" rel=\"nofollow noreferrer\">Artifact Registry roles<\/a> to the above service agent.\nYou can start with adding <code>roles\/artifactregistry.reader<\/code>.<\/p>\n<p>Hope this helps :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70833594",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659424830192,
        "Question_original_content":"gcp servic agent access gcr imag error get error idea solv fail creat pipelin job error servic agent gcp aiplatform iam gserviceaccount com grant access imag gcr gcp project applic latest",
        "Question_preprocessed_content":"gcp servic agent access gcr imag error get error idea solv",
        "Question_gpt_summary_original":"The user is encountering an error while trying to create a pipeline job in GCP Vertex AI Service Agent. The error message indicates that the Service Agent does not have access to the GCR image 'gcr.io\/gcp-project-id\/application:latest'. The user is seeking help to resolve this issue.",
        "Question_gpt_summary":"user encount error try creat pipelin job gcp servic agent error messag indic servic agent access gcr imag gcr gcp project applic latest user seek help resolv issu",
        "Answer_original_content":"project number gcp aiplatform iam gserviceaccount com googl platform servic agent servic agent requir access read pull docker imag project gcr creat contain pipelin run permiss edit iam role try ad artifact registri role servic agent start ad role artifactregistri reader hope help",
        "Answer_preprocessed_content":"googl platform servic agent servic agent requir access docker imag project gcr creat contain pipelin run permiss edit iam role try ad artifact registri role servic agent start ad hope help",
        "Answer_gpt_summary_original":"Solution: The user can try adding Artifact Registry roles to the AI Platform service agent, starting with adding \"roles\/artifactregistry.reader\".",
        "Answer_gpt_summary":"solut user try ad artifact registri role platform servic agent start ad role artifactregistri reader"
    },
    {
        "Question_title":"How to get Sagemaker Batch Transform Job status printed out in my python notebook?",
        "Question_body":"<p>I am running a python notebook which is initiating a Batch Transform Job in Sagemaker. However, I want to also print the status &quot;Failed&quot;, &quot;In Progress&quot; and &quot;Completed&quot; once the job is complete running. As of now, I am only able to start the Batch Transform Job (rf=random forest) but I am not certain how to get the job status print outs. Can someone help with that given my script below?<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/SU0ln.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/SU0ln.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code>rf_transformer = rf.transformer(\n                                instance_count,\n                                instance_type,\n                                strategy=strategy,\n                                output_path=output_path,\n                                max_payload=max_payload)\n\nrf_transformer.transform(\n                                str('s3:\/\/batch_scoring\/rf_output),\n                                content_type='text\/csv',\n                                compression_type='Gzip'\n                         )\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1593925160127,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":612.0,
        "Answer_body":"<p>You can do it with:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>job_name = rf_transformer.latest_transform_job.name\nrf_transformer.sagemaker_session.describe_transform_job(job_name)['TransformJobStatus']\n<\/code><\/pre>\n<p>You can also use the AWS SDK directly, if you wish:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import boto3\n\nsagemaker_client = boto3.client('sagemaker')\nsagemaker_client.describe_transform_job(job_name)['TransformJobStatus']\n<\/code><\/pre>\n<p>API documentation: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62737008",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1594154231750,
        "Question_original_content":"batch transform job statu print python notebook run python notebook initi batch transform job want print statu fail progress complet job complet run abl start batch transform job random forest certain job statu print out help given script transform transform instanc count instanc type strategi strategi output path output path max payload max payload transform transform str batch score output content type text csv compress type gzip",
        "Question_preprocessed_content":"batch transform job statu print python notebook run python notebook initi batch transform job want print statu fail progress complet job complet run abl start batch transform job certain job statu print out help given script",
        "Question_gpt_summary_original":"The user is facing a challenge in getting the status of a Batch Transform Job in Sagemaker printed out in their Python notebook. They have successfully initiated the job but are unsure of how to obtain the status updates for \"Failed\", \"In Progress\", and \"Completed\". They have provided a script for starting the job using a random forest transformer.",
        "Question_gpt_summary":"user face challeng get statu batch transform job print python notebook successfulli initi job unsur obtain statu updat fail progress complet provid script start job random forest transform",
        "Answer_original_content":"job transform latest transform job transform session transform job job transformjobstatu us aw sdk directli wish import boto client boto client client transform job job transformjobstatu api document http doc aw amazon com latest apirefer api describetransformjob html",
        "Answer_preprocessed_content":"us aw sdk directli wish api document",
        "Answer_gpt_summary_original":"Solution: The user can obtain the status updates for a Batch Transform Job in Sagemaker by using the following script in their Python notebook:\n```\njob_name = rf_transformer.latest_transform_job.name\nrf_transformer.sagemaker_session.describe_transform_job(job_name)['TransformJobStatus']\n```\nAlternatively, they can use the AWS SDK directly by importing boto3 and using the following script:\n```\nimport boto3\nsagemaker_client = boto3.client('sagemaker')\nsagemaker_client.describe_transform_job(job_name)['TransformJobStatus']\n```\nThe API documentation for this can be found at https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_DescribeTransformJob.html.",
        "Answer_gpt_summary":"solut user obtain statu updat batch transform job follow script python notebook job transform latest transform job transform session transform job job transformjobstatu altern us aw sdk directli import boto follow script import boto client boto client client transform job job transformjobstatu api document http doc aw amazon com latest apirefer api describetransformjob html"
    },
    {
        "Question_title":"Image Classification in Azure Machine Learning",
        "Question_body":"<p>I'm preparing for the Azure Machine Learning exam, and here is a question confuses me.<\/p>\n<blockquote>\n<p>You are designing an Azure Machine Learning workflow. You have a\ndataset that contains two million large digital photographs. You plan\nto detect the presence of trees in the photographs. You need to ensure\nthat your model supports the following:<\/p>\n<p>Solution: You create a Machine\nLearning experiment that implements the Multiclass Decision Jungle\nmodule. Does this meet the goal?<\/p>\n<p>Solution: You create a Machine Learning experiment that implements the\nMulticlass Neural Network module. Does this meet the goal?<\/p>\n<\/blockquote>\n<p>The answer for the first question is No while for second is Yes, but I cannot understand why Multiclass Decision Jungle doesn't meet the goal since it is a classifier. Can someone explain to me the reason?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_created_time":1559676822253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1592644375060,
        "Question_score":1.0,
        "Question_view_count":538.0,
        "Answer_body":"<p>I suppose that this is part of a series of questions that present the same scenario. And there should be definitely some constraints in the scenario. \nMoreover if you have a look on the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/multiclass-neural-network\" rel=\"nofollow noreferrer\">Azure documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>However, recent research has shown that deep neural networks (DNN)\n  with many layers can be very effective in complex tasks such as image\n  or speech recognition. The successive layers are used to model\n  increasing levels of semantic depth.<\/p>\n<\/blockquote>\n\n<p>Thus, Azure recommends using Neural Networks for image classification. Remember, that the goal of the exam is to test your capacity to design data science solution <strong>using Azure<\/strong> so better to use their official documentation as a reference.<\/p>\n\n<p>And comparing to the other solutions:<\/p>\n\n<ol>\n<li>You create an Azure notebook that supports the Microsoft Cognitive\nToolkit.<\/li>\n<li>You create a Machine Learning experiment that implements\nthe Multiclass Decision Jungle module.<\/li>\n<li>You create an endpoint to the\nComputer vision API. <\/li>\n<li>You create a Machine Learning experiment that\nimplements the Multiclass Neural Network module.<\/li>\n<li>You create an Azure\nnotebook that supports the Microsoft Cognitive Toolkit.<\/li>\n<\/ol>\n\n<p>There are only 2 Azure ML Studio modules, and as the question is about constructing a <strong>workflow<\/strong> I guess we can only choose between them. (CNTK is actually the best solution as it allows constructing a deep neural network with ReLU whereas AML Studio doesn't, and API call is not about data science at all). <\/p>\n\n<p>Finally, I do agree with the other contributors that the question is absurd. Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56450223",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1559895314607,
        "Question_original_content":"imag classif prepar exam question confus design workflow dataset contain million larg digit photograph plan detect presenc tree photograph need ensur model support follow solut creat machin learn experi implement multiclass decis jungl modul meet goal solut creat machin learn experi implement multiclass neural network modul meet goal answer question second ye understand multiclass decis jungl meet goal classifi explain reason",
        "Question_preprocessed_content":"imag classif prepar exam question confus design workflow dataset contain million larg digit photograph plan detect presenc tree photograph need ensur model support follow solut creat machin learn experi implement multiclass decis jungl modul meet goal solut creat machin learn experi implement multiclass neural network modul meet goal answer question second ye understand multiclass decis jungl meet goal classifi explain reason",
        "Question_gpt_summary_original":"The user is preparing for the Azure Machine Learning exam and is confused about a question regarding the detection of trees in a dataset of two million digital photographs. They are unsure why the Multiclass Decision Jungle module does not meet the goal of detecting trees while the Multiclass Neural Network module does.",
        "Question_gpt_summary":"user prepar exam confus question detect tree dataset million digit photograph unsur multiclass decis jungl modul meet goal detect tree multiclass neural network modul",
        "Answer_original_content":"suppos seri question present scenario definit constraint scenario look azur document recent research shown deep neural network dnn layer effect complex task imag speech recognit success layer model increas level semant depth azur recommend neural network imag classif rememb goal exam test capac design data scienc solut azur better us offici document refer compar solut creat azur notebook support microsoft cognit toolkit creat machin learn experi implement multiclass decis jungl modul creat endpoint vision api creat machin learn experi implement multiclass neural network modul creat azur notebook support microsoft cognit toolkit studio modul question construct workflow guess choos cntk actual best solut allow construct deep neural network relu aml studio api data scienc final agre contributor question absurd hope help",
        "Answer_preprocessed_content":"suppos seri question present scenario definit constraint scenario look azur document recent research shown deep neural network layer effect complex task imag speech recognit success layer model increas level semant depth azur recommend neural network imag classif rememb goal exam test capac design data scienc solut azur better us offici document refer compar solut creat azur notebook support microsoft cognit toolkit creat machin learn experi implement multiclass decis jungl modul creat endpoint vision api creat machin learn experi implement multiclass neural network modul creat azur notebook support microsoft cognit toolkit studio modul question construct workflow guess choos final agre contributor question absurd hope help",
        "Answer_gpt_summary_original":"Solutions provided:\n- Azure documentation recommends using Neural Networks for image classification, which is why the Multiclass Neural Network module is more effective in detecting trees in the dataset.\n- There are only 2 Azure ML Studio modules available for constructing a workflow, and the Multiclass Decision Jungle module is not effective in this scenario. \n- CNTK is actually the best solution as it allows constructing a deep neural network with ReLU, whereas AML Studio doesn't. \n- API call is not about data science at all. \n\nNo personal opinions or biases were included in the summary.",
        "Answer_gpt_summary":"solut provid azur document recommend neural network imag classif multiclass neural network modul effect detect tree dataset studio modul avail construct workflow multiclass decis jungl modul effect scenario cntk actual best solut allow construct deep neural network relu aml studio api data scienc person opinion bias includ summari"
    },
    {
        "Question_title":"feature in new generation of classic studio",
        "Question_body":"hi:\n\nI have a question regarding to the new generation as I have already known that the classic old version of machine learning studio is retiring in August 2024.\n\nI wonder if all the features will be continouslty supported in the new generation of new version of studio in the future?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1648513803980,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @dontbelazy-2604\n\nThanks for reaching out to us here, I just answered your question under your previous thread. Not every feature in Studio (classic) will be back as the same, but most of the previous features\/ functions will be covered in the Azure Machine Learning Studio (V2).\n\nPlease go ahead to do the migration first, if you face any issue or you feel like some of the features missing, please let us know, we can help.\n\nGenerally, the new studio will provide a better experience.\n\nHope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/791041\/feature-in-new-generation-of-classic-studio.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1648525475910,
        "Question_original_content":"featur new gener classic studio question new gener known classic old version machin learn studio retir august wonder featur continouslti support new gener new version studio futur",
        "Question_preprocessed_content":"featur new gener classic studio question new gener known classic old version machin learn studio retir august wonder featur continouslti support new gener new version studio futur",
        "Question_gpt_summary_original":"The user is concerned about the retirement of the classic old version of machine learning studio in August 2024 and is unsure if all the features will be continuously supported in the new generation of the studio.",
        "Question_gpt_summary":"user concern retir classic old version machin learn studio august unsur featur continu support new gener studio",
        "Answer_original_content":"hello dontbelazi thank reach answer question previou thread featur studio classic previou featur function cover studio ahead migrat face issu feel like featur miss let know help gener new studio provid better experi hope help regard yutong kindli accept answer feel help thank",
        "Answer_preprocessed_content":"hello thank reach answer question previou thread featur studio previou featur function cover studio ahead migrat face issu feel like featur miss let know help gener new studio provid better experi hope help regard yutong kindli accept answer feel help thank",
        "Answer_gpt_summary_original":"Solution: The new generation of Azure Machine Learning Studio (V2) will cover most of the previous features and functions of the classic version. Users are advised to do the migration first and report any missing features or issues to receive help. The new studio is expected to provide a better experience.",
        "Answer_gpt_summary":"solut new gener studio cover previou featur function classic version user advis migrat report miss featur issu receiv help new studio expect provid better experi"
    },
    {
        "Question_title":"TensorFlow Serving send data as b64 instead of Numpy Array",
        "Question_body":"<p>I have a TensorFlow Serving container in a SageMaker endpoint. I'm able to take a batch of images as a Numpy array and get back predictions like this:<\/p>\n\n<pre><code>import numpy as np\nimport sagemaker\nfrom sagemaker.predictor import json_serializer, json_deserializer\n\nimage = np.random.uniform(low=-1.0, high=1.0, size=(1,128,128,3)).astype(np.float32)    \nimage = {'instances': image}\nimage = json_serializer(image)\n\nrequest_args = {}\nrequest_args['Body'] = image\nrequest_args['EndpointName'] = endpoint_name\nrequest_args['ContentType'] = 'application\/json'\nrequest_args['Accept'] = 'application\/json'\n\n# works successfully\nresponse = sagemaker_session.sagemaker_runtime_client.invoke_endpoint(**request_args)\nresponse_body = response['Body']\npredictions = json_deserializer(response_body, response['ContentType'])\n<\/code><\/pre>\n\n<p>The size of the <code>request_args<\/code> payload is large doing it this way. I'm wondering, is there a way to send this in a more compressed format? <\/p>\n\n<p>I've tried experimenting with <code>base64<\/code> and <code>json.dumps<\/code>, but can't get past <code>Invalid argument: JSON Value: ...<\/code> errors. Not sure if this isn't supported or if I'm just doing it incorrectly.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1548093365813,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":796.0,
        "Answer_body":"<p>I've talked to AWS support about this (see <a href=\"https:\/\/stackoverflow.com\/questions\/54090270\/more-efficient-way-to-send-a-request-than-json-to-deployed-tensorflow-model-in-s\">More efficient way to send a request than JSON to deployed tensorflow model in Sagemaker?<\/a>).<\/p>\n\n<p>They suggest that it is possible to pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).<\/p>\n\n<p>I'll be testing this soon and hopefully this stuff works since it would add a lot of flexibility to the input processing.<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54295445",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1548251290416,
        "Question_original_content":"tensorflow serv send data instead numpi arrai tensorflow serv contain endpoint abl batch imag numpi arrai predict like import numpi import predictor import json serial json deseri imag random uniform low high size astyp float imag instanc imag imag json serial imag request arg request arg bodi imag request arg endpointnam endpoint request arg contenttyp applic json request arg accept applic json work successfulli respons session runtim client invok endpoint request arg respons bodi respons bodi predict json deseri respons bodi respons contenttyp size request arg payload larg wai wonder wai send compress format tri experi base json dump past invalid argument json valu error sure isn support incorrectli",
        "Question_preprocessed_content":"tensorflow serv send data instead numpi arrai tensorflow serv contain endpoint abl batch imag numpi arrai predict like size payload larg wai wonder wai send compress format tri experi past error sure isn support incorrectli",
        "Question_gpt_summary_original":"The user is facing a challenge in sending a batch of images as a Numpy array to a TensorFlow Serving container in a SageMaker endpoint. The payload size of the request is large, and the user is looking for a way to send it in a more compressed format. The user has tried using base64 and json.dumps but is encountering errors.",
        "Question_gpt_summary":"user face challeng send batch imag numpi arrai tensorflow serv contain endpoint payload size request larg user look wai send compress format user tri base json dump encount error",
        "Answer_original_content":"talk aw support effici wai send request json deploi tensorflow model suggest possibl pass custom input serv contain unpack compress format protobuf test soon hopefulli stuff work add lot flexibl input process",
        "Answer_preprocessed_content":"talk aw support suggest possibl pass custom serv contain unpack compress format test soon hopefulli stuff work add lot flexibl input process",
        "Answer_gpt_summary_original":"Solution: The user can pass in a custom input_fn that will be used by the serving container where one can unpack a compressed format (such as protobuf).",
        "Answer_gpt_summary":"solut user pass custom input serv contain unpack compress format protobuf"
    },
    {
        "Question_title":"Read multiple CSV files in Azure ML Python Script",
        "Question_body":"<p>I have 4 csv files that are inputs to the python script in azure ML, but the widget has only 2 inputs for dataframes and the third for a zip file. I tried to put the csv files in a zipped folder and connect it to the third input for the script but that also did not work :\n<a href=\"https:\/\/i.stack.imgur.com\/FkxRE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FkxRE.png\" alt=\"Image of workspace\"><\/a><\/p>\n\n<p>I would like to know how to read multiple csv files in the python script.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1499844354733,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1499850837700,
        "Question_score":1.0,
        "Question_view_count":944.0,
        "Answer_body":"<p>Here's some more detail on the approach others have outlined above. Try replacing the code currently in the \"Execute Python Script\" module with the following:<\/p>\n\n<pre><code>import pandas as pd\nimport os\ndef azureml_main(dataframe1=None, dataframe2=None):\n    print(os.listdir('.'))\n    return(pd.DataFrame([]))\n<\/code><\/pre>\n\n<p>After running the experiment, click on the module. There should be a \"View output log\" link now in the right-hand bar. I get something like the following:<\/p>\n\n<pre><code>[Information]         Started in [C:\\temp]\n[Information]         Running in [C:\\temp]\n[Information]         Executing 4af67c05ba02417a980f6a16e84e61dc with inputs [] and generating outputs ['.maml.oport1']\n[Information]         Extracting Script Bundle.zip to .\\Script Bundle\n[Information]         File Name                                             Modified             Size\n[Information]         temp.csv                                       2016-05-06 13:16:56           52\n[Information]         [ READING ] 0:00:00\n[Information]         ['4af67c05ba02417a980f6a16e84e61dc.py', 'Script Bundle', 'Script Bundle.zip']\n<\/code><\/pre>\n\n<p>This tells me that the contents of my zip file have been extracted to the <code>C:\\temp\\Script Bundle<\/code> folder. In my case the zip file contained just one CSV file, <code>temp.csv<\/code>: your output would probably have four files. You may also have zipped a folder containing your four files, in which case the filepath would be one layer deeper. You can use the <code>os.listdir()<\/code> to explore your directory structure further if necessary.<\/p>\n\n<p>Once you think you know the full filepaths for your CSV files, edit your Execute Python Script module's code to load them, e.g.:<\/p>\n\n<pre><code>import pandas as pd\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    df = pd.read_csv('C:\/temp\/Script Bundle\/temp.csv')\n    # ...load other files and merge into a single dataframe...\n    return(df)\n<\/code><\/pre>\n\n<p>Hope that helps!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45051055",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1500412322168,
        "Question_original_content":"read multipl csv file python script csv file input python script widget input datafram zip file tri csv file zip folder connect input script work like know read multipl csv file python script",
        "Question_preprocessed_content":"read multipl csv file python script csv file input python script widget input datafram zip file tri csv file zip folder connect input script work like know read multipl csv file python script",
        "Question_gpt_summary_original":"The user is facing a challenge in reading multiple CSV files in a Python script in Azure ML. The widget has only 2 inputs for dataframes and the third for a zip file, but the user has 4 CSV files as inputs. The user tried to put the CSV files in a zipped folder and connect it to the third input for the script, but it did not work. The user is seeking guidance on how to read multiple CSV files in the Python script.",
        "Question_gpt_summary":"user face challeng read multipl csv file python script widget input datafram zip file user csv file input user tri csv file zip folder connect input script work user seek guidanc read multipl csv file python script",
        "Answer_original_content":"approach outlin try replac code current execut python script modul follow import panda import def main datafram datafram print listdir return datafram run experi click modul view output log link right hand bar like follow inform start temp inform run temp inform execut afcbaafaeedc input gener output maml oport inform extract script bundl zip script bundl inform file modifi size inform temp csv inform read inform afcbaafaeedc script bundl script bundl zip tell content zip file extract temp script bundl folder case zip file contain csv file temp csv output probabl file zip folder contain file case filepath layer deeper us listdir explor directori structur necessari think know filepath csv file edit execut python script modul code load import panda def main datafram datafram read csv temp script bundl temp csv load file merg singl datafram return hope help",
        "Answer_preprocessed_content":"approach outlin try replac code current execut python script modul follow run experi click modul view output log link bar like follow tell content zip file extract folder case zip file contain csv file output probabl file zip folder contain file case filepath layer deeper us explor directori structur necessari think know filepath csv file edit execut python script modul code load hope help",
        "Answer_gpt_summary_original":"Solution: The user can try to put the CSV files in a zipped folder and connect it to the third input for the script. After running the experiment, the user can click on the module and view the output log to see the file path where the contents of the zip file have been extracted. Once the user knows the full file paths for the CSV files, they can edit the Execute Python Script module's code to load them.",
        "Answer_gpt_summary":"solut user try csv file zip folder connect input script run experi user click modul view output log file path content zip file extract user know file path csv file edit execut python script modul code load"
    },
    {
        "Question_title":"Azure ML - Import Hive Query Failing - Hive over ADLS",
        "Question_body":"<p>We are working on Azure ML and ADLS combination. Since HDInsight Cluster is working over ADLS, we are trying to use Hive Query and HDFS route and running into problems. \nRequest your help in solving the problem of reading data from hive query and writing to HDFS. Below is the error URL for reference:<\/p>\n\n<p><a href=\"https:\/\/studioapi.azureml.net\/api\/sharedaccess?workspaceId=025ba20578874d7086e6c495cc49a3f2&amp;signature=ZMUCNMwRjlrksrrmsrx5SaGedSgwMmO%2FfSHvq190%2F1I%3D&amp;sharedAccessUri=https%3A%2F%2Fesprodussouth001.blob.core.windows.net%2Fexperimentoutput%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9.txt%3Fsv%3D2015-02-21%26sr%3Db%26sig%3DHkuFm8B2Ba1kEWWIwanqlv%2FcQPWVz0XYveSsZnEa0Wg%3D%26st%3D2017-10-16T18%3A31%3A06Z%26se%3D2017-10-17T18%3A36%3A06Z%26sp%3Dr\" rel=\"nofollow noreferrer\">https:\/\/studioapi.azureml.net\/api\/sharedaccess?workspaceId=025ba20578874d7086e6c495cc49a3f2&amp;signature=ZMUCNMwRjlrksrrmsrx5SaGedSgwMmO%2FfSHvq190%2F1I%3D&amp;sharedAccessUri=https%3A%2F%2Fesprodussouth001.blob.core.windows.net%2Fexperimentoutput%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9%2Fccf9a206-730d-4773-b44e-a2dd8c6e87b9.txt%3Fsv%3D2015-02-21%26sr%3Db%26sig%3DHkuFm8B2Ba1kEWWIwanqlv%2FcQPWVz0XYveSsZnEa0Wg%3D%26st%3D2017-10-16T18%3A31%3A06Z%26se%3D2017-10-17T18%3A36%3A06Z%26sp%3Dr<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1508806505617,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1510470314876,
        "Question_score":0.0,
        "Question_view_count":64.0,
        "Answer_body":"<p>Azure Machine Learning supports Hive but not over ADLS. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1509134225963,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46900593",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1508944940720,
        "Question_original_content":"import hive queri fail hive adl work adl combin hdinsight cluster work adl try us hive queri hdf rout run problem request help solv problem read data hive queri write hdf error url refer http studioapi net api sharedaccess workspaceid badecccaf signatur zmucnmwrjlrksrrmsrxsagedsgwmmo ffshvq sharedaccessuri http fesprodussouth blob core window net fexperimentoutput fccfa addceb fccfa addceb txt fsv sig dhkufmbbakewwiwanqlv fcqpwvzxyvesszneawg",
        "Question_preprocessed_content":"import hive queri fail hive adl work adl combin hdinsight cluster work adl try us hive queri hdf rout run problem request help solv problem read data hive queri write hdf error url refer",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to use Hive Query and HDFS route on Azure ML and ADLS combination. The HDInsight Cluster is working over ADLS, but the user is unable to read data from Hive Query and write to HDFS. The user is seeking help to solve the problem and has provided an error URL for reference.",
        "Question_gpt_summary":"user encount challeng try us hive queri hdf rout adl combin hdinsight cluster work adl user unabl read data hive queri write hdf user seek help solv problem provid error url refer",
        "Answer_original_content":"support hive adl",
        "Answer_preprocessed_content":"support hive adl",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"Reading File from Vertex AI and Google Cloud Storage",
        "Question_body":"<p>I am trying to set up a pipeline in GCP\/Vertex AI and am having a lot of trouble. The pipeline is being written using Kubeflow Pipelines and has many different components, one thing in particular is giving me trouble however. Eventually I want to launch this from a Cloud Function with the help of the Cloud Scheduler.<\/p>\n<p>The part that is giving me issues is fairly simple and I believe I just need some form of introduction to how I should be thinking about this setup. I simply want to read and write from files (might be .csv, .txt or similar). I imagine that the analog to the filesystem on my local machine in GCP is the Cloud Storage so this is where I have been trying to read from for the time being (please correct me if I'm wrong). The component I've built is a blatant rip-off of <a href=\"https:\/\/stackoverflow.com\/questions\/48279061\/gcs-read-a-text-file-from-google-cloud-storage-directly-into-python\">this<\/a> post and looks like this.<\/p>\n<pre><code>@component(\n    packages_to_install=[&quot;google-cloud&quot;],\n    base_image=&quot;python:3.9&quot;\n)\n\n\ndef main(\n):\n    import csv\n    from io import StringIO\n\n    from google.cloud import storage\n\n    BUCKET_NAME = &quot;gs:\/\/my_bucket&quot;\n\n    storage_client = storage.Client()\n    bucket = storage_client.get_bucket(BUCKET_NAME)\n\n    blob = bucket.blob('test\/test.txt')\n    blob = blob.download_as_string()\n    blob = blob.decode('utf-8')\n\n    blob = StringIO(blob)  #tranform bytes to string here\n\n    names = csv.reader(blob)  #then use csv library to read the content\n    for name in names:\n        print(f&quot;First Name: {name[0]}&quot;)\n<\/code><\/pre>\n<p>The error I'm getting looks like the following:<\/p>\n<pre><code>google.api_core.exceptions.NotFound: 404 GET https:\/\/storage.googleapis.com\/storage\/v1\/b\/gs:\/\/pipeline_dev?projection=noAcl&amp;prettyPrint=false: Not Found\n<\/code><\/pre>\n<p>What's going wrong in my brain? I get the feeling that it shouldn't be this difficult to read and write files. I must be missing something fundamental? Any help is highly appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_created_time":1646398826330,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1298.0,
        "Answer_body":"<p>Try specifying bucket name w\/o a gs:\/\/. This should fix the issue. One more stackoverflow post that says the same thing: <a href=\"https:\/\/stackoverflow.com\/questions\/53436615\/cloud-storage-python-client-fails-to-retrieve-bucket\">Cloud Storage python client fails to retrieve bucket<\/a><\/p>\n<p>any storage bucket you try to access in GCP has a unique address to access it. That address starts with a gs:\/\/ always which specifies that it is a cloud storage url. Now, GCS apis are designed such that they need the bucket name only to work with it. Hence, you just pass the bucket name. If you were accessing the bucket via browser you will need the complete address to access and hence the gs:\/\/ prefix as well.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71351821",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1646582537360,
        "Question_original_content":"read file googl cloud storag try set pipelin gcp have lot troubl pipelin written kubeflow pipelin differ compon thing particular give troubl eventu want launch cloud function help cloud schedul give issu fairli simpl believ need form introduct think setup simpli want read write file csv txt similar imagin analog filesystem local machin gcp cloud storag try read time correct wrong compon built blatant rip post look like compon packag instal googl cloud base imag python def main import csv import stringio googl cloud import storag bucket bucket storag client storag client bucket storag client bucket bucket blob bucket blob test test txt blob blob download string blob blob decod utf blob stringio blob tranform byte string name csv reader blob us csv librari read content name print error get look like follow googl api core except notfound http storag googleapi com storag pipelin dev project noacl prettyprint fals go wrong brain feel shouldn difficult read write file miss fundament help highli appreci",
        "Question_preprocessed_content":"read file googl cloud storag try set pipelin gcp have lot troubl pipelin written kubeflow pipelin differ compon thing particular give troubl eventu want launch cloud function help cloud schedul give issu fairli simpl believ need form introduct think setup simpli want read write file imagin analog filesystem local machin gcp cloud storag try read time compon built blatant post look like error get look like follow go wrong brain feel shouldn difficult read write file miss fundament help highli appreci",
        "Question_gpt_summary_original":"The user is facing challenges while setting up a pipeline in GCP\/Vertex AI using Kubeflow Pipelines. The user is having trouble reading and writing files from Cloud Storage and is getting a 404 error. The user is seeking guidance on how to properly set up the file reading and writing process.",
        "Question_gpt_summary":"user face challeng set pipelin gcp kubeflow pipelin user have troubl read write file cloud storag get error user seek guidanc properli set file read write process",
        "Answer_original_content":"try specifi bucket fix issu stackoverflow post sai thing cloud storag python client fail retriev bucket storag bucket try access gcp uniqu address access address start specifi cloud storag url gc api design need bucket work pass bucket access bucket browser need complet address access prefix",
        "Answer_preprocessed_content":"try specifi bucket fix issu stackoverflow post sai thing cloud storag python client fail retriev bucket storag bucket try access gcp uniqu address access address start specifi cloud storag url gc api design need bucket work pass bucket access bucket browser need complet address access prefix",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that the user should try specifying the bucket name without using \"gs:\/\/\" prefix. This should fix the issue of getting a 404 error while reading and writing files from Cloud Storage in GCP\/Vertex AI using Kubeflow Pipelines. The discussion also explains that GCS APIs are designed to work with the bucket name only, and not the complete address with \"gs:\/\/\" prefix.",
        "Answer_gpt_summary":"solut discuss suggest user try specifi bucket prefix fix issu get error read write file cloud storag gcp kubeflow pipelin discuss explain gc api design work bucket complet address prefix"
    },
    {
        "Question_title":"API definition for ModelBiasMonitor and ModelExplainabilityMonitor",
        "Question_body":"Where can I find the actual references to API definitions and descriptions for ModelBiasMonitor and ModelExplainabilityMonitor Classes?\n\nI can a find a few mentions in the Amazon SageMaker documentation in the following links. https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model_monitor.html https:\/\/sagemaker-examples.readthedocs.io\/en\/latest\/sagemaker_model_monitor\/fairness_and_explainability\/SageMaker-Model-Monitor-Fairness-and-Explainability.html\n\nWhere can I find the actual reference and the code implementation for these Classes?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1611507553000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":25.0,
        "Answer_body":"The actual reference to the classes can be found here: https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/model_monitor\/clarify_model_monitoring.py\nIt encapsulates the definitions and descriptions for all of SageMaker Clarify related monitoring classes.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU9SPQKzelSUu3dr-D4zaXHQ\/api-definition-for-model-bias-monitor-and-model-explainability-monitor",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1611571671000,
        "Question_original_content":"api definit modelbiasmonitor modelexplainabilitymonitor actual refer api definit descript modelbiasmonitor modelexplainabilitymonitor class mention document follow link http readthedoc stabl api infer model monitor html http exampl readthedoc latest model monitor fair explain model monitor fair explain html actual refer code implement class",
        "Question_preprocessed_content":"api definit modelbiasmonitor modelexplainabilitymonitor actual refer api definit descript modelbiasmonitor modelexplainabilitymonitor class mention document follow link actual refer code implement class",
        "Question_gpt_summary_original":"The user is looking for the API definitions and descriptions for the ModelBiasMonitor and ModelExplainabilityMonitor classes in Amazon SageMaker. They have found some mentions in the documentation but are unable to locate the actual references and code implementation for these classes.",
        "Question_gpt_summary":"user look api definit descript modelbiasmonitor modelexplainabilitymonitor class mention document unabl locat actual refer code implement class",
        "Answer_original_content":"actual refer class http github com aw python sdk blob master src model monitor clarifi model monitor encapsul definit descript clarifi relat monitor class",
        "Answer_preprocessed_content":"actual refer class encapsul definit descript clarifi relat monitor class",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to find the reference to the ModelBiasMonitor and ModelExplainabilityMonitor classes in the following link: https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/model_monitor\/clarify_model_monitoring.py. This link contains the definitions and descriptions for all of SageMaker Clarify related monitoring classes.",
        "Answer_gpt_summary":"solut solut provid discuss refer modelbiasmonitor modelexplainabilitymonitor class follow link http github com aw python sdk blob master src model monitor clarifi model monitor link contain definit descript clarifi relat monitor class"
    },
    {
        "Question_title":"How to convert jupyter notebook (ipython) to slideshow NOT using command line",
        "Question_body":"<p>I am new with <code>Jupyter<\/code>, and I use Amazon SageMaker so that everything is cloud based and not local.  I cannot use any resources locally, nor can I install <code>Jupyter<\/code> on this local computer that I want to do this on, so I cannot use the command line to put :<\/p>\n\n<pre><code>jupyter nbconvert Jupyter\\ Slides.ipynb --to slides --post serve\n<\/code><\/pre>\n\n<p>So, I am struggling to find a way to convert my notebook to a slideshow NOT using command line. Thanks in advance!<\/p>",
        "Question_answer_count":4,
        "Question_comment_count":0,
        "Question_created_time":1532649972710,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1532675039560,
        "Question_score":1.0,
        "Question_view_count":4610.0,
        "Answer_body":"<p>You can follow below steps to convert your notebook to slides on AWS Sagemaker (tried on sagemaker notebook instance) without installing any extensions.<\/p>\n\n<p><strong>Step 1:<\/strong> Follow this <a href=\"https:\/\/medium.com\/@mjspeck\/presenting-code-using-jupyter-notebook-slides-a8a3c3b59d67\" rel=\"nofollow noreferrer\">article<\/a> to chose which cells in your notebook can be presented or skipped.\n  - Go to View \u2192 Cell Toolbar \u2192 Slideshow\n  - A light gray bar will appear above each cell with a scroll down window on the top right\n  - Select type of slide each cell should be - regular slide, sub-slide, skip, notes<\/p>\n\n<p><strong>Step 2:<\/strong> Go to Sagemaker notebook home page and open terminal<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kDl3d.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 3:<\/strong> Change directory in the instance where your notebook exists<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/rA1lZ.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 4:<\/strong> Clone <code>reveal.js<\/code> in the directory where notebook exists from <a href=\"https:\/\/github.com\/hakimel\/reveal.js\" rel=\"nofollow noreferrer\">github<\/a>. <code>reveal.js<\/code> is used for rendering HTML file as presentation.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/dillF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/dillF.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p><strong>Step 5:<\/strong> Run the below command (same as in your question) to convert the notebook to slides without serving them (since there is no browser on the Sagemaker instance). This will just convert notebook to slides html.<\/p>\n\n<pre><code>jupyter nbconvert Image-classification-fulltraining.ipynb --to slides\n[NbConvertApp] Converting notebook Image-classification-fulltraining.ipynb to slides\n[NbConvertApp] Writing 346423 bytes to Image-classification-fulltraining.slides.html\n<\/code><\/pre>\n\n<p><strong>Step 6:<\/strong> Now open the html file from Sagemaker notebook file browser <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fykyl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fykyl.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Now you can see the notebook rendered as slides based on how setup each cell in your notebook in Step 1<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/9PLUA.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Hope it helps.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51549048",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1532732192903,
        "Question_original_content":"convert jupyt notebook ipython slideshow command line new jupyt us cloud base local us resourc local instal jupyt local want us command line jupyt nbconvert jupyt slide ipynb slide post serv struggl wai convert notebook slideshow command line thank advanc",
        "Question_preprocessed_content":"convert jupyt notebook slideshow command line new us cloud base local us resourc local instal local want us command line struggl wai convert notebook slideshow command line thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in converting a Jupyter notebook to a slideshow without using the command line. They are unable to use any resources locally or install Jupyter on their local computer as they are using Amazon SageMaker, which is cloud-based.",
        "Question_gpt_summary":"user face challeng convert jupyt notebook slideshow command line unabl us resourc local instal jupyt local cloud base",
        "Answer_original_content":"follow step convert notebook slide tri notebook instanc instal extens step follow articl chose cell notebook present skip view cell toolbar slideshow light grai bar appear cell scroll window right select type slide cell regular slide sub slide skip note step notebook home page open termin step chang directori instanc notebook exist step clone reveal directori notebook exist github reveal render html file present step run command question convert notebook slide serv browser instanc convert notebook slide html jupyt nbconvert imag classif fulltrain ipynb slide nbconvertapp convert notebook imag classif fulltrain ipynb slide nbconvertapp write byte imag classif fulltrain slide html step open html file notebook file browser notebook render slide base setup cell notebook step hope help",
        "Answer_preprocessed_content":"follow step convert notebook slide instal extens step follow articl chose cell notebook present skip view cell toolbar slideshow light grai bar appear cell scroll window right select type slide cell regular slide skip note step notebook home page open termin step chang directori instanc notebook exist step clone directori notebook exist github render html file present step run command convert notebook slide serv convert notebook slide html step open html file notebook file browser notebook render slide base setup cell notebook step hope help",
        "Answer_gpt_summary_original":"Solution: The user can follow the steps mentioned below to convert their Jupyter notebook to a slideshow on AWS SageMaker without using the command line or installing any extensions. \n\nStep 1: Choose which cells in the notebook can be presented or skipped by going to View \u2192 Cell Toolbar \u2192 Slideshow and selecting the type of slide each cell should be - regular slide, sub-slide, skip, notes.\n\nStep 2: Open the terminal on the SageMaker notebook home page.\n\nStep 3: Change the directory to where the notebook exists.\n\nStep 4: Clone reveal.js in the directory where the notebook exists from GitHub. reveal.js is used for rendering the HTML file as a presentation.\n\nStep 5: Run the command \"jupyter nbconvert",
        "Answer_gpt_summary":"solut user follow step mention convert jupyt notebook slideshow command line instal extens step choos cell notebook present skip go view cell toolbar slideshow select type slide cell regular slide sub slide skip note step open termin notebook home page step chang directori notebook exist step clone reveal directori notebook exist github reveal render html file present step run command jupyt nbconvert"
    },
    {
        "Question_title":"using transformers module with sagemaker studio project: ModuleNotFoundError: No module named 'transformers'",
        "Question_body":"So as mentioned in my other recent post, I'm trying to modify the sagemaker example abalone xgboost template to use tensorfow.\n\nMy current problem is that running the pipeline I get a failure and in the logs I see:\n\nModuleNotFoundError: No module named 'transformers'\n\n\nNOTE: I am importing 'transformers' in preprocess.py not in pipeline.py\n\nNow I have 'transformers' listed in various places as a dependency including:\n\nsetup.py - required_packages = [\"sagemaker==2.93.0\", \"sklearn\", \"transformers\", \"openpyxl\"]\npipelines.egg-info\/requires.txt - transformers (auto-generated from setup.py?)\n\nbut so I'm keen to understand, how can I ensure that additional dependencies are available in the pipline itself?\n\nMany thanks in advance\n\nADDITIONAL DETAILS ON HOW I ENCOUNTERED THE ERROR\n\nFrom one particular notebook (see previous post for more details) I have succesfully constructed the new topic\/tensorflow pipeline and run the following steps:\n\npipeline.upsert(role_arn=role)\nexecution = pipeline.start()\nexecution.describe()\n\n\nthe describe() method gives this output:\n\n{'PipelineArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example',\n 'PipelineExecutionArn': 'arn:aws:sagemaker:eu-west-1:398371982844:pipeline\/topicpipeline-example\/execution\/0aiczulkjoaw',\n 'PipelineExecutionDisplayName': 'execution-1664394415255',\n 'PipelineExecutionStatus': 'Executing',\n 'PipelineExperimentConfig': {'ExperimentName': 'topicpipeline-example',\n  'TrialName': '0aiczulkjoaw'},\n 'CreationTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'LastModifiedTime': datetime.datetime(2022, 9, 28, 19, 46, 55, 147000, tzinfo=tzlocal()),\n 'CreatedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'LastModifiedBy': {'UserProfileArn': 'arn:aws:sagemaker:eu-west-1:398371982844:user-profile\/d-5qgy6ubxlbdq\/sjoseph-reg-genome-com-273',\n  'UserProfileName': 'sjoseph-reg-genome-com-273',\n  'DomainId': 'd-5qgy6ubxlbdq'},\n 'ResponseMetadata': {'RequestId': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n  'HTTPStatusCode': 200,\n  'HTTPHeaders': {'x-amzn-requestid': 'f949d6f4-1865-4a01-b7a2-a96c42304071',\n   'content-type': 'application\/x-amz-json-1.1',\n   'content-length': '882',\n   'date': 'Wed, 28 Sep 2022 19:47:02 GMT'},\n  'RetryAttempts': 0}}\n\n\nWaiting for the execution I get:\n\n---------------------------------------------------------------------------\nWaiterError                               Traceback (most recent call last)\n<ipython-input-14-72be0c8b7085> in <module>\n----> 1 execution.wait()\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/workflow\/pipeline.py in wait(self, delay, max_attempts)\n    581             waiter_id, model, self.sagemaker_session.sagemaker_client\n    582         )\n--> 583         waiter.wait(PipelineExecutionArn=self.arn)\n    584 \n    585 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n     53     # method.\n     54     def wait(self, **kwargs):\n---> 55         Waiter.wait(self, **kwargs)\n     56 \n     57     wait.__doc__ = WaiterDocstring(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/botocore\/waiter.py in wait(self, **kwargs)\n    376                     name=self.name,\n    377                     reason=reason,\n--> 378                     last_response=response,\n    379                 )\n    380             if num_attempts >= max_attempts:\n\nWaiterError: Waiter PipelineExecutionComplete failed: Waiter encountered a terminal failure state: For expression \"PipelineExecutionStatus\" we matched expected path: \"Failed\"\n\n\nWhich I assume is corresponding to the failure I see in the logs:\n\nI did also run python setup.py build to ensure my build directory was up to date ... here's the terminal output of that command:\n\nsagemaker-user@studio$ python setup.py build\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/dist.py:771: UserWarning: Usage of dash-separated 'description-file' will not be supported in future versions. Please use the underscore name 'description_file' instead\n  warnings.warn(\n\/opt\/conda\/lib\/python3.9\/site-packages\/setuptools\/config\/setupcfg.py:508: SetuptoolsDeprecationWarning: The license_file parameter is deprecated, use license_files instead.\n  warnings.warn(msg, warning_class)\nrunning build\nrunning build_py\ncopying pipelines\/topic\/pipeline.py -> build\/lib\/pipelines\/topic\nrunning egg_info\nwriting pipelines.egg-info\/PKG-INFO\nwriting dependency_links to pipelines.egg-info\/dependency_links.txt\nwriting entry points to pipelines.egg-info\/entry_points.txt\nwriting requirements to pipelines.egg-info\/requires.txt\nwriting top-level names to pipelines.egg-info\/top_level.txt\nreading manifest file 'pipelines.egg-info\/SOURCES.txt'\nadding license file 'LICENSE'\nwriting manifest file 'pipelines.egg-info\/SOURCES.txt'\n\n\nIt seems like the dependencies are being written to pipelines.egg-info\/requires.txt but are these not being picked up by the pipeline?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1664396753855,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":81.0,
        "Answer_body":"Hi! There are two places where you need to install the dependencies \/ requirements:\n\nIn your environment where you execute pipeline.start() \u2013 can be Amazon SageMaker Studio, your local machine or CI\/CD pipeline executor, e. g. AWS CodeBuild. These dependencies are installed in setup.py.\nInside the SageMaker processing and training jobs as well as in inference endpoints. This is usually done via requirements.txt file that you submit as part of your source_dir.\n\nIn your example, I recommend you to use the TensorFlowProcessor. The way how to install dependencies into it is described in the corresponding section of the documentation, in particular:\n\nSageMaker Processing installs the dependencies in requirements.txt in the container for you.\n\nSame applies to your model training and to the TensorFlow estimator. See the section Use third-party libraries in the TensorFlow documentation of the SageMaker Python SDK, in particular:\n\nIf there are other packages you want to use with your script, you can use a requirements.txt to install other dependencies at runtime.\n\nHope it helps!",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUdd2zOBY0Q4CEG1ZdbgNsgA\/using-transformers-module-with-sagemaker-studio-project-module-not-found-error-no-module-named-transformers",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1664451755510,
        "Question_original_content":"transform modul studio project modulenotfounderror modul name transform mention recent post try modifi exampl abalon xgboost templat us tensorfow current problem run pipelin failur log modulenotfounderror modul name transform note import transform preprocess pipelin transform list place depend includ setup requir packag sklearn transform openpyxl pipelin egg info requir txt transform auto gener setup keen understand ensur addit depend avail piplin thank advanc addit detail encount error particular notebook previou post detail succesfulli construct new topic tensorflow pipelin run follow step pipelin upsert role arn role execut pipelin start execut method give output pipelinearn arn aw west pipelin topicpipelin exampl pipelineexecutionarn arn aw west pipelin topicpipelin exampl execut aiczulkjoaw pipelineexecutiondisplaynam execut pipelineexecutionstatu execut pipelineexperimentconfig experimentnam topicpipelin exampl trialnam aiczulkjoaw creationtim datetim datetim tzinfo tzlocal lastmodifiedtim datetim datetim tzinfo tzlocal createdbi userprofilearn arn aw west user profil qgyubxlbdq sjoseph reg genom com userprofilenam sjoseph reg genom com domainid qgyubxlbdq lastmodifiedbi userprofilearn arn aw west user profil qgyubxlbdq sjoseph reg genom com userprofilenam sjoseph reg genom com domainid qgyubxlbdq responsemetadata requestid fdf httpstatuscod httpheader amzn requestid fdf content type applic amz json content length date wed sep gmt retryattempt wait execut waitererror traceback recent execut wait opt conda lib python site packag workflow pipelin wait self delai max attempt waiter model self session client waiter wait pipelineexecutionarn self arn opt conda lib python site packag botocor waiter wait self kwarg method def wait self kwarg waiter wait self kwarg wait doc waiterdocstr opt conda lib python site packag botocor waiter wait self kwarg self reason reason respons respons num attempt max attempt waitererror waiter pipelineexecutioncomplet fail waiter encount termin failur state express pipelineexecutionstatu match expect path fail assum correspond failur log run python setup build ensur build directori date termin output command user studio python setup build opt conda lib python site packag setuptool dist userwarn usag dash separ descript file support futur version us underscor descript file instead warn warn opt conda lib python site packag setuptool config setupcfg setuptoolsdeprecationwarn licens file paramet deprec us licens file instead warn warn msg warn class run build run build copi pipelin topic pipelin build lib pipelin topic run egg info write pipelin egg info pkg info write depend link pipelin egg info depend link txt write entri point pipelin egg info entri point txt write requir pipelin egg info requir txt write level name pipelin egg info level txt read manifest file pipelin egg info sourc txt ad licens file licens write manifest file pipelin egg info sourc txt like depend written pipelin egg info requir txt pick pipelin",
        "Question_preprocessed_content":"transform modul studio project modulenotfounderror modul name transform mention recent post try modifi exampl abalon xgboost templat us tensorfow current problem run pipelin failur log modulenotfounderror modul name transform note import transform transform list place depend includ transform keen understand ensur addit depend avail piplin thank advanc addit detail encount error particular notebook succesfulli construct new pipelin run follow step execut method give output pipelinearn pipelineexecutionarn pipelineexecutiondisplaynam pipelineexecutionstatu execut pipelineexperimentconfig creationtim tzinfo tzlocal lastmodifiedtim tzinfo tzlocal createdbi lastmodifiedbi responsemetadata retryattempt wait execut waitererror traceback wait model wait method def wait kwarg waiterdocstr reason reason waitererror waiter pipelineexecutioncomplet fail waiter encount termin failur state express pipelineexecutionstatu match expect path fail assum correspond failur log run python build ensur build directori date termin output command user python build userwarn usag support futur version us underscor instead setuptoolsdeprecationwarn paramet deprec us instead run build run copi run write write write entri point write requir write name read manifest file ad licens file licens write manifest file like depend written pick pipelin",
        "Question_gpt_summary_original":"The user is encountering a ModuleNotFoundError while trying to use the transformers module with a SageMaker Studio project. The user has listed transformers as a dependency in various places, including setup.py and pipelines.egg-info\/requires.txt, but is unsure how to ensure that additional dependencies are available in the pipeline itself. The user has also encountered a failure while running the pipeline and is trying to understand why the dependencies are not being picked up by the pipeline.",
        "Question_gpt_summary":"user encount modulenotfounderror try us transform modul studio project user list transform depend place includ setup pipelin egg info requir txt unsur ensur addit depend avail pipelin user encount failur run pipelin try understand depend pick pipelin",
        "Answer_original_content":"place need instal depend requir environ execut pipelin start studio local machin pipelin executor aw codebuild depend instal setup insid process train job infer endpoint usual requir txt file submit sourc dir exampl recommend us tensorflowprocessor wai instal depend describ correspond section document particular process instal depend requir txt contain appli model train tensorflow estim section us parti librari tensorflow document python sdk particular packag want us script us requir txt instal depend runtim hope help",
        "Answer_preprocessed_content":"place need instal depend requir environ execut studio local machin pipelin executor aw codebuild depend instal insid process train job infer endpoint usual file submit exampl recommend us tensorflowprocessor wai instal depend describ correspond section document particular process instal depend contain appli model train tensorflow estim section us librari tensorflow document python sdk particular packag want us script us instal depend runtim hope help",
        "Answer_gpt_summary_original":"Solution:\n- Install the dependencies in the environment where pipeline.start() is executed, which can be Amazon SageMaker Studio, local machine, or CI\/CD pipeline executor, using setup.py.\n- Install dependencies inside the SageMaker processing and training jobs, as well as in inference endpoints, using requirements.txt file submitted as part of the source_dir.\n- Use TensorFlowProcessor and follow the instructions in the documentation to install dependencies in the container.\n- Use a requirements.txt file to install other dependencies at runtime for TensorFlow estimator.",
        "Answer_gpt_summary":"solut instal depend environ pipelin start execut studio local machin pipelin executor setup instal depend insid process train job infer endpoint requir txt file submit sourc dir us tensorflowprocessor follow instruct document instal depend contain us requir txt file instal depend runtim tensorflow estim"
    },
    {
        "Question_title":"ModuleNotFound Error - Azure ML with prebuilt docker image",
        "Question_body":"<p>I have developed a module which works perfectly when executed locally.<\/p>\n<p>I have created an environment on azure using a prebuilt docker image found here:\n<strong>&quot;azureml\/minimal-ubuntu18.04-py37-cpu-inference&quot;<\/strong> <a href=\"https:\/\/mcr.microsoft.com\/v2\/_catalog\" rel=\"nofollow noreferrer\">https:\/\/mcr.microsoft.com\/v2\/_catalog<\/a>\n. Also, Using pythonScriptStep, to run a pipeline. Here is how the step looks<\/p>\n<pre><code>StepPreprocessing = PythonScriptStep(\n    name=&quot;Preprocessing&quot;,\n    script_name=e.preprocess_script_path,\n    arguments=[\n        &quot;--config_path&quot;, e.preprocess_config_path,\n        &quot;--task&quot;, e.preprocess_task,\n    ],\n    inputs=None,\n    compute_target=aml_compute,\n    runconfig=run_config,\n    source_directory=e.sources_directory,\n    allow_reuse=False\n)\nprint(&quot;Step Preprocessing created&quot;)\n<\/code><\/pre>\n<p>This results in error:<\/p>\n<pre><code>Traceback (most recent call last):\n[stderr]  File &quot;Pipeline\/custom_pipeline.py&quot;, line 4, in &lt;module&gt;\n[stderr]    from Preprocess.logger import logger\n[stderr]ModuleNotFoundError: No module named 'Preprocess'\n<\/code><\/pre>\n<p>in the 1st line of entry script (<strong>custom_pipeline.py<\/strong>):<\/p>\n<pre><code>import sys\nsys.path.append(&quot;.&quot;) \nfrom Preprocess.logger import logger\n<\/code><\/pre>\n<p>The folder structure is as:<\/p>\n<pre><code>-Preprocess\n  -__init__.py\n  - Module1\n    -__init__.py\n    -somefile.py\n  - Module2\n    -__init__.py\n    -someOtherfile.py\n  - Pipeline\n    -__init__.py\n    -custom_pipeline.py\n  - logger\n    -__init__.py\n    -logger.py\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1644260100200,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":109.0,
        "Answer_body":"<p>I found out that the python script step copies everything inside the source_dir and therefore in my case it was copying the modules and not the root folder. So I had to put the dir Preprocess inside another dir and mention the new dir as source_dir.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71023918",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1644604906183,
        "Question_original_content":"modulenotfound error prebuilt docker imag develop modul work perfectli execut local creat environ azur prebuilt docker imag minim ubuntu cpu infer http mcr microsoft com catalog pythonscriptstep run pipelin step look steppreprocess pythonscriptstep preprocess script preprocess script path argument config path preprocess config path task preprocess task input comput target aml comput runconfig run config sourc directori sourc directori allow reus fals print step preprocess creat result error traceback recent stderr file pipelin custom pipelin line stderr preprocess logger import logger stderr modulenotfounderror modul name preprocess line entri script custom pipelin import sy sy path append preprocess logger import logger folder structur preprocess init modul init somefil modul init someotherfil pipelin init custom pipelin logger init logger",
        "Question_preprocessed_content":"modulenotfound error prebuilt docker imag develop modul work perfectli execut local creat environ azur prebuilt docker imag pythonscriptstep run pipelin step look result error line entri script folder structur",
        "Question_gpt_summary_original":"The user encountered a ModuleNotFound error when running a pipeline on Azure ML using a prebuilt docker image. The error occurred in the entry script, custom_pipeline.py, when importing a module named 'Preprocess'. The folder structure includes the 'Preprocess' folder containing subfolders and files, including the 'logger' module.",
        "Question_gpt_summary":"user encount modulenotfound error run pipelin prebuilt docker imag error occur entri script custom pipelin import modul name preprocess folder structur includ preprocess folder contain subfold file includ logger modul",
        "Answer_original_content":"python script step copi insid sourc dir case copi modul root folder dir preprocess insid dir mention new dir sourc dir",
        "Answer_preprocessed_content":"python script step copi insid case copi modul root folder dir preprocess insid dir mention new dir",
        "Answer_gpt_summary_original":"Solution: The user resolved the ModuleNotFound error by putting the 'Preprocess' folder inside another directory and specifying the new directory as the source directory in the python script step.",
        "Answer_gpt_summary":"solut user resolv modulenotfound error put preprocess folder insid directori specifi new directori sourc directori python script step"
    },
    {
        "Question_title":"Google Translate API and Serbian Latin script",
        "Question_body":"Hi there,In Serbia we are using 2 scripts side by side - Cyrillic and Latin script.I am heaving an issue with translation to Serbian Latin.\nBy default Google translate offer translation to Serbian Cyrillic , but bellow that default translation there is a translation to Serbian Latin.\nTake a look at this example:\nhttps:\/\/translate.google.com\/?sl=en&tl=sr&text=Hello%20world!&op=translateI have found this post from early 2019.\nhttps:\/\/support.google.com\/translate\/thread\/1836538?hl=enLike in that post my question is the same:\nI need it to support Serbian Latin, for some projects I don`t use the Cyrillic script. Also there is a problem with translating pages or similar plugins, e.g.: Google Language Translator for WordPress and some others CMS system like Kopage you can translate only to Serbian Cyrillic script.As I found this post on stackoverflow:\nhttps:\/\/stackoverflow.com\/questions\/73699065\/google-cloud-translate-serbian-latin-not-workingIt seems, according to the poster of that article, that there was a workaround.\nInstead of \"sr\" ISO-639 code you can put \"sr_Latn\" - and you will get translation into Serbian Latin script.\nBut that workaround stop working several weeks ago - according to the poster.Is there a workaround to translate into Serbian Latin characters rather into Serbian Cyrillic characters?Regards,\nBranislav",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1665455160000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":128.0,
        "Answer_body":"It appears that translation to the Serbian Latin Alphabet is not officially supported by the Cloud Translate API, as discussed in this recent issue. Therefore it\u2019s not assured that any possible workaround will be functional or reliable. You can see the list of supported language codes for translation here.\n\nYou can, however, submit a Feature Request to the public Google issue tracker for Cloud Translation API. The higher the number of users who bring attention to this request, the more likely it is for it to be eventually built into the API.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Google-Translate-API-and-Serbian-Latin-script\/td-p\/476723\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1665562320000,
        "Question_original_content":"googl translat api serbian latin script serbia script cyril latin script heav issu translat serbian latin default googl translat offer translat serbian cyril bellow default translat translat serbian latin look exampl http translat googl com text hello world translatei post earli http support googl com translat thread enlik post question need support serbian latin project us cyril script problem translat page similar plugin googl languag translat wordpress cm like kopag translat serbian cyril script post stackoverflow http stackoverflow com question googl cloud translat serbian latin workingit accord poster articl workaround instead iso code latn translat serbian latin script workaround stop work week ago accord poster workaround translat serbian latin charact serbian cyril charact regard branislav",
        "Question_preprocessed_content":"googl translat api serbian latin script serbia script cyril latin heav issu translat serbian latin default googl translat offer translat serbian cyril bellow default translat translat serbian latin look exampl post earli post question need support serbian latin project us cyril script problem translat page similar plugin googl languag translat wordpress cm like kopag translat serbian cyril post stackoverflow accord poster articl workaround instead code translat serbian latin script workaround stop work week ago accord workaround translat serbian latin charact serbian cyril charact regard branislav",
        "Question_gpt_summary_original":"The user is facing challenges with the Google Translate API when translating to Serbian Latin script. By default, the API offers translation to Serbian Cyrillic, and there is a problem with translating pages or plugins to Serbian Latin script. The user has found a workaround by using \"sr_Latn\" instead of \"sr\" ISO-639 code, but it has stopped working several weeks ago. The user is seeking a solution to translate into Serbian Latin characters rather than Serbian Cyrillic characters.",
        "Question_gpt_summary":"user face challeng googl translat api translat serbian latin script default api offer translat serbian cyril problem translat page plugin serbian latin script user workaround latn instead iso code stop work week ago user seek solut translat serbian latin charact serbian cyril charact",
        "Answer_original_content":"appear translat serbian latin alphabet offici support cloud translat api discuss recent issu assur possibl workaround function reliabl list support languag code translat submit featur request public googl issu tracker cloud translat api higher number user bring attent request like eventu built api view solut origin post",
        "Answer_preprocessed_content":"appear translat serbian latin alphabet offici support cloud translat api discuss recent issu assur possibl workaround function reliabl list support languag code translat submit featur request public googl issu tracker cloud translat api higher number user bring attent request like eventu built api view solut origin post",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include submitting a Feature Request to the public Google issue tracker for Cloud Translation API to add support for Serbian Latin script. However, it is not guaranteed that any possible workaround will be functional or reliable since translation to the Serbian Latin Alphabet is not officially supported by the Cloud Translate API.",
        "Answer_gpt_summary":"possibl solut mention discuss includ submit featur request public googl issu tracker cloud translat api add support serbian latin script guarante possibl workaround function reliabl translat serbian latin alphabet offici support cloud translat api"
    },
    {
        "Question_title":"Machine Learning studio Data Labeling Dataset",
        "Question_body":"Hi ,\n\nI have a have a dataset from the labelled data using the ML Data Labeling tool , my question is how can use the dataset to train a model ? , I tried Automated ML but I cannot make ant connection with the dataset .\n\nThanks for your help.",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_created_time":1623134368530,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@hernandoZ-8172 I can confirm that using labeling data in the designer is currently not supported. This is however part of the roadmap in the future releases of designer. You can consume the data with the SDK as mentioned above.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/426209\/machine-learning-studio-data-labeling-dataset.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1623240266352,
        "Question_original_content":"machin learn studio data label dataset dataset label data data label tool question us dataset train model tri autom ant connect dataset thank help",
        "Question_preprocessed_content":"machin learn studio data label dataset dataset label data data label tool question us dataset train model tri autom ant connect dataset thank help",
        "Question_gpt_summary_original":"The user is facing challenges in using a dataset labeled with the Machine Learning Data Labeling tool to train a model. They have attempted to use Automated ML but have been unable to connect with the dataset.",
        "Question_gpt_summary":"user face challeng dataset label machin learn data label tool train model attempt us autom unabl connect dataset",
        "Answer_original_content":"hernandoz confirm label data design current support roadmap futur releas design consum data sdk mention",
        "Answer_preprocessed_content":"confirm label data design current support roadmap futur releas design consum data sdk mention",
        "Answer_gpt_summary_original":"Solution: The discussion mentions that using labeling data in the designer is currently not supported, but it is part of the roadmap for future releases. In the meantime, the user can consume the data with the SDK. No other solutions are provided.",
        "Answer_gpt_summary":"solut discuss mention label data design current support roadmap futur releas meantim user consum data sdk solut provid"
    },
    {
        "Question_title":"VSCode cannot see packages on a GCP VM",
        "Question_body":"<p>I have an issue where vscode when connected to a VM on GCP cannot see packages installed in <code>\/opt\/conda\/lib\/python3.7\/site-packages.<\/code> I have created the VM using Vertex AI. When I open the jupyter notebook through the UI in a the browser I can see all the packages via <code>pip3 list<\/code>. But when I am connected to the VM through SSH in vscode I cannot see the packages installed such as nltk, spacy etc. and when I try to load it gives me <code>ModuleNotFoundError<\/code>. This error does not show up when I use the jupyter notebook from the Vertex AI UI. The site-packages folder is in my system path and the python that I am using is <code>\/opt\/conda\/bin\/python3<\/code>.<\/p>\n<p>Any help is appreciated. Please do let me know if my question is clear.<\/p>\n<p>EDIT: I figured out that my packages are running on a container in the VM. Is there a way for me to access those packages via jupyter notebook in vscode?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1638473538253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1638550660460,
        "Question_score":0.0,
        "Question_view_count":230.0,
        "Answer_body":"<p>Posting the answer as community wiki. As confirmed by @Abhishek, he was able to make it work by installing a docker extension on the VM then attach VS code to the container.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70205432",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1638843453688,
        "Question_original_content":"vscode packag gcp issu vscode connect gcp packag instal opt conda lib python site packag creat open jupyt notebook browser packag pip list connect ssh vscode packag instal nltk spaci try load give modulenotfounderror error us jupyt notebook site packag folder path python opt conda bin python help appreci let know question clear edit figur packag run contain wai access packag jupyt notebook vscode",
        "Question_preprocessed_content":"vscode packag gcp issu vscode connect gcp packag instal creat open jupyt notebook browser packag connect ssh vscode packag instal nltk spaci try load give error us jupyt notebook folder path python help appreci let know question clear edit figur packag run contain wai access packag jupyt notebook vscode",
        "Question_gpt_summary_original":"The user is facing a challenge where VSCode, when connected to a VM on GCP, cannot see packages installed in \/opt\/conda\/lib\/python3.7\/site-packages. The packages are visible when the user opens the Jupyter notebook through the UI in a browser, but not when connected to the VM through SSH in VSCode. The user is receiving a ModuleNotFoundError and has discovered that the packages are running on a container in the VM. The user is seeking help to access those packages via Jupyter notebook in VSCode.",
        "Question_gpt_summary":"user face challeng vscode connect gcp packag instal opt conda lib python site packag packag visibl user open jupyt notebook browser connect ssh vscode user receiv modulenotfounderror discov packag run contain user seek help access packag jupyt notebook vscode",
        "Answer_original_content":"post answer commun wiki confirm abhishek abl work instal docker extens attach code contain",
        "Answer_preprocessed_content":"post answer commun wiki confirm abl work instal docker extens attach code contain",
        "Answer_gpt_summary_original":"Solution: The user can install a docker extension on the VM and attach VSCode to the container to access the packages installed in \/opt\/conda\/lib\/python3.7\/site-packages. This solution has been confirmed to work by another user.",
        "Answer_gpt_summary":"solut user instal docker extens attach vscode contain access packag instal opt conda lib python site packag solut confirm work user"
    },
    {
        "Question_title":"how to set Quota at resource group level?",
        "Question_body":"Hi team,\nI am learning about the quota for machine learning service and I have a general doubt.\n\nI can see that quotas for CPU cores is set at subscription level. Now, lets say my subscription level total CPU cores quota is 10.\nAnd i have 2 resource groups under that subscription. Can I assign 5 -5 cores each to both of the resource groups.\n\nso that if all the cores are taken up by the resources under 1 resource group, the other resource_group (or the ML workspace under the other resource group) should not suffer.\n\nI am able to find out the- get details query but this one doesnt give me details specific to each resource-group or the workspace.\n\nHTTP query -> https:\/\/management.azure.com\/subscriptions\/{subs_id}\/providers\/Microsoft.MachineLearningServices\/locations\/eastus\/usages?api-version=2022-10-01",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1667069444447,
        "Question_favorite_count":21.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hi @JA-9673 ,\n\nquotas can be set on Azure Subscription level only.\nThere is no option to apply quotas for different Azure Resource Groups.\nThere are 2 options I can see for your requirement:\nUse 2 Azure Subscriptions for each Resource Group\nUse the 2 Resource Groups in 2 different regions. There is a quota for vCPUs per region within the same Subscription.\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/1067916\/how-to-set-quota-at-resource-group-level.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1667069947336,
        "Question_original_content":"set quota resourc group level team learn quota machin learn servic gener doubt quota cpu core set subscript level let subscript level total cpu core quota resourc group subscript assign core resourc group core taken resourc resourc group resourc group workspac resourc group suffer abl detail queri doesnt detail specif resourc group workspac http queri http manag azur com subscript sub provid microsoft machinelearningservic locat eastu usag api version",
        "Question_preprocessed_content":"set quota resourc group level team learn quota machin learn servic gener doubt quota cpu core set subscript level let subscript level total cpu core quota resourc group subscript assign core resourc group core taken resourc resourc group suffer abl detail queri doesnt detail specif workspac http queri",
        "Question_gpt_summary_original":"The user is trying to set quotas for machine learning services at the resource group level, but is facing challenges as the quotas for CPU cores are set at the subscription level. They want to assign 5 cores each to two resource groups under the same subscription to ensure that one resource group does not consume all the cores and affect the other resource group or the ML workspace under it. The user is unable to find specific details for each resource group or workspace using the provided HTTP query.",
        "Question_gpt_summary":"user try set quota machin learn servic resourc group level face challeng quota cpu core set subscript level want assign core resourc group subscript ensur resourc group consum core affect resourc group workspac user unabl specif detail resourc group workspac provid http queri",
        "Answer_original_content":"quota set azur subscript level option appli quota differ azur resourc group option requir us azur subscript resourc group us resourc group differ region quota vcpu region subscript repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_preprocessed_content":"quota set azur subscript level option appli quota differ azur resourc group option requir us azur subscript resourc group us resourc group differ region quota vcpu region subscript repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_gpt_summary_original":"Solutions provided:\n- Quotas can only be set at the Azure Subscription level, not at the Resource Group level.\n- Two possible solutions are suggested: \n  1. Use two Azure Subscriptions for each Resource Group.\n  2. Use the two Resource Groups in two different regions, as there is a quota for vCPUs per region within the same Subscription.",
        "Answer_gpt_summary":"solut provid quota set azur subscript level resourc group level possibl solut suggest us azur subscript resourc group us resourc group differ region quota vcpu region subscript"
    },
    {
        "Question_title":"Form recognizer to report on missing information in (near) real-time",
        "Question_body":"Hi community,\nI'm interested in what Azure Form Recogniser or another tool can do for us in terms of screening the correctness of uploaded applications. Think of applications for funding grants. I haven't built any models yet, just wondering how feasible the below is. A solution doesn't have to involve AI at all, but must be able to 'read' the uploaded documents.\n\n\n\n\nA client uploads a set of standard documents (usually scanned PDF's) using a file upload in our .net application.\nCan we:\n1) Use form recogniser to extract key value pairs, after training a custom model.\n2) Run a loop over these pairs to find missing information e.g. they forgot to add their date of birth, or didn't enter their income.\n3) Report back to the user the missing information so they can correct the document and reupload them?\nPreferably in real time? So they hit submit on the webpage, it extracts, analyses and provides a result in a few seconds?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1647481005860,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@AndrewRobertson-7835 Yes, you can use Azure form recognizer to analyze a document that is passed to the API and use the result of the analyze operation to report any missing fields in the form back to the user. This is the most widely used use case by most of the customers.\n\nForm recognizer comes with a set of prebuilt APIs where it can extract common information from invoices, business cards, receipts etc. If you have a form that does not conform to the prebuilt API standards you need to create a custom model to extract the text in the form of a tags and their key:value pairs. The custom models require some basic training with some test forms and if all the forms that need extraction follow the same layout or guidelines the extraction results will be good.\n\nIn the case of custom forms the results are provided in almost real time where the form is submitted or POST request is sent to the API and an operation id is returned to retrieve the results using GET. Depending on your pricing tier of your resource if you intend to perform these actions synchronously you might have to limit the rate of requests sent to the API to avoid any TPS errors. If you are using async operations with a slight delay to fetch the results then you can design an application that can take large number of documents and provide results to the users within a short span of time.\n\nI hope the above information is helpful.\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/775440\/form-recognizer-to-report-on-missing-information-i.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1647513011952,
        "Question_original_content":"form recogn report miss inform near real time commun interest azur form recognis tool term screen correct upload applic think applic fund grant haven built model wonder feasibl solut involv abl read upload document client upload set standard document usual scan pdf file upload net applic us form recognis extract kei valu pair train custom model run loop pair miss inform forgot add date birth enter incom report user miss inform correct document reupload prefer real time hit submit webpag extract analys provid result second",
        "Question_preprocessed_content":"form recogn report miss inform commun interest azur form recognis tool term screen correct upload applic think applic fund grant haven built model wonder feasibl solut involv abl read upload document client upload set standard document file upload net applic us form recognis extract kei valu pair train custom model run loop pair miss inform forgot add date birth enter incom report user miss inform correct document reupload prefer real time hit submit webpag extract analys provid result second",
        "Question_gpt_summary_original":"The user is seeking information on whether Azure Form Recognizer or another tool can be used to screen the correctness of uploaded applications, specifically for funding grants. They want to extract key value pairs using a custom model, find missing information, and report it back to the user in real-time so they can correct the document and re-upload it. The user is interested in a solution that can read uploaded documents and provide results within a few seconds.",
        "Question_gpt_summary":"user seek inform azur form recogn tool screen correct upload applic specif fund grant want extract kei valu pair custom model miss inform report user real time correct document upload user interest solut read upload document provid result second",
        "Answer_original_content":"andrewrobertson ye us azur form recogn analyz document pass api us result analyz oper report miss field form user wide us case custom form recogn come set prebuilt api extract common inform invoic busi card receipt form conform prebuilt api standard need creat custom model extract text form tag kei valu pair custom model requir basic train test form form need extract follow layout guidelin extract result good case custom form result provid real time form submit post request sent api oper return retriev result depend price tier resourc intend perform action synchron limit rate request sent api avoid tp error async oper slight delai fetch result design applic larg number document provid result user short span time hope inform help answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"ye us azur form recogn analyz document pass api us result analyz oper report miss field form user wide us case custom form recogn come set prebuilt api extract common inform invoic busi card receipt form conform prebuilt api standard need creat custom model extract text form tag kei valu pair custom model requir basic train test form form need extract follow layout guidelin extract result good case custom form result provid real time form submit post request sent api oper return retriev result depend price tier resourc intend perform action synchron limit rate request sent api avoid tp error async oper slight delai fetch result design applic larg number document provid result user short span time hope inform help answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Solution: Azure Form Recognizer can be used to analyze uploaded documents and extract key value pairs using prebuilt APIs or custom models. If a custom model is required, basic training with test forms is needed. The results are provided almost in real-time, and an operation ID is returned to retrieve the results using GET. Depending on the pricing tier, the rate of requests sent to the API might need to be limited to avoid TPS errors. An application can be designed to take a large number of documents and provide results to users within a short span of time.",
        "Answer_gpt_summary":"solut azur form recogn analyz upload document extract kei valu pair prebuilt api custom model custom model requir basic train test form need result provid real time oper return retriev result depend price tier rate request sent api need limit avoid tp error applic design larg number document provid result user short span time"
    },
    {
        "Question_title":"Sacred - pass all parameters as one",
        "Question_body":"<p>When using <code>Sacred<\/code> it is necessary to pass all variables from the experiment config, into the main function, for example<\/p>\n\n<pre><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(C, gamma):\n  iris = datasets.load_iris()\n  per = permutation(iris.target.size)\n  iris.data = iris.data[per]\n  iris.target = iris.target[per]\n  clf = svm.SVC(C, 'rbf', gamma=gamma)\n  clf.fit(iris.data[:90],\n          iris.target[:90])\n  return clf.score(iris.data[90:],\n                   iris.target[90:])\n<\/code><\/pre>\n\n<p>As you can see, in this experiment there are 2 variables, <code>C<\/code> and <code>gamma<\/code>, and they are passed into the main function.<\/p>\n\n<p>In real scenarios, there are dozens of experiment variables, and the passing all of them into the main function gets really cluttered.\nIs there a way to pass them all as a dictionary? Or maybe as an object with attributes? <\/p>\n\n<p>A good solution will result in something like follows:<\/p>\n\n<pre><code>@ex.automain\ndef run(config):\n    config.C      # Option 1\n    config['C']   # Option 2 \n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_created_time":1542890499740,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":735.0,
        "Answer_body":"<p>Yes, you can use the <a href=\"https:\/\/sacred.readthedocs.io\/en\/latest\/configuration.html#special-values\" rel=\"nofollow noreferrer\">special value<\/a> <code>_config<\/code> value for that:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>ex = Experiment('iris_rbf_svm')\n\n@ex.config\ndef cfg():\n  C = 1.0\n  gamma = 0.7\n\n@ex.automain\ndef run(_config):\n  C = _config['C']\n  gamma = _config['gamma']\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53431283",
        "Tool":"Sacred",
        "Platform":"Stack Overflow",
        "Question_closed_time":1551699854476,
        "Question_original_content":"pass paramet necessari pass variabl experi config main function exampl experi iri rbf svm config def cfg gamma automain def run gamma iri dataset load iri permut iri target size iri data iri data iri target iri target clf svm svc rbf gamma gamma clf fit iri data iri target return clf score iri data iri target experi variabl gamma pass main function real scenario dozen experi variabl pass main function get clutter wai pass dictionari mayb object attribut good solut result like follow automain def run config config option config option",
        "Question_preprocessed_content":"pass paramet necessari pass variabl experi config main function exampl experi variabl pass main function real scenario dozen experi variabl pass main function get clutter wai pass dictionari mayb object attribut good solut result like follow",
        "Question_gpt_summary_original":"The user is facing a challenge when using Sacred, as it is necessary to pass all variables from the experiment config into the main function, which can become cluttered when dealing with dozens of variables. The user is seeking a solution to pass all variables as a dictionary or an object with attributes.",
        "Question_gpt_summary":"user face challeng necessari pass variabl experi config main function clutter deal dozen variabl user seek solut pass variabl dictionari object attribut",
        "Answer_original_content":"ye us special valu config valu experi iri rbf svm config def cfg gamma automain def run config config gamma config gamma",
        "Answer_preprocessed_content":"ye us special valu valu",
        "Answer_gpt_summary_original":"Solution: The user can use the special value \"_config\" in Sacred to pass all variables as a dictionary. They can define the variables in the experiment config and access them in the main function using _config['variable_name'].",
        "Answer_gpt_summary":"solut user us special valu config pass variabl dictionari defin variabl experi config access main function config variabl"
    },
    {
        "Question_title":"Azure Machine Learning - Use personal container registry",
        "Question_body":"<p>I tried to run this code to create an ml workspace from the azure-ml-cli, referencing an existing azure container registry from another subscription:<\/p>\n<pre><code>az ml workspace create --workspace-name &quot;test-mlws&quot; --keyvault &quot;&lt;key-vault-service-id&gt;&quot; --container-registry &quot;&lt;container-registry-zervice-id&gt;&quot; --location westeurope\n<\/code><\/pre>\n<p>The deploy failed with this error code:<\/p>\n<pre><code>{'code': 'InternalServerError', 'message': 'Received 403 from a service request'}\n<\/code><\/pre>\n<p>I can't find any documentation about it, and I guess it's due to the container registry I used which belongs to another subscription. Anyone who knows if it's mandatory for the registry to be in the same subscription?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1617971298560,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1618194925572,
        "Question_score":0.0,
        "Question_view_count":773.0,
        "Answer_body":"<p>From <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace?tabs=python#limitations\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-manage-workspace?tabs=python#limitations<\/a><\/p>\n<blockquote>\n<p>If you want to use <strong>existing services from a different Azure\nsubscription<\/strong> than the workspace, you must register the Azure Machine\nLearning namespace in the subscription that contains those services.<\/p>\n<\/blockquote>\n<p>So, in order to use the ACR in that different subscription, you need to register resource provider <code>Microsoft.MachineLearningServices<\/code> in that subscription contains ACR. For information on how to see if it is registered and how to register it, see the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/azure-resource-manager\/management\/resource-providers-and-types\" rel=\"nofollow noreferrer\">Azure resource providers and types<\/a> article.<\/p>\n<p>To register a resource provider, use:<\/p>\n<pre><code>Register-AzResourceProvider -ProviderNamespace Microsoft.MachineLearningServices\n<\/code><\/pre>\n<p>To see information for a particular resource provider, use:<\/p>\n<pre><code>Get-AzResourceProvider -ProviderNamespace Microsoft.MachineLearningServices\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67021176",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1618194887616,
        "Question_original_content":"us person contain registri tri run code creat workspac azur cli referenc exist azur contain registri subscript workspac creat workspac test mlw keyvault contain registri locat westeurop deploi fail error code code internalservererror messag receiv servic request document guess contain registri belong subscript know mandatori registri subscript",
        "Question_preprocessed_content":"us person contain registri tri run code creat workspac referenc exist azur contain registri subscript deploi fail error code document guess contain registri belong subscript know mandatori registri subscript",
        "Question_gpt_summary_original":"The user encountered a challenge while trying to create an ml workspace from the azure-ml-cli, referencing an existing azure container registry from another subscription. The deploy failed with an error code 'InternalServerError' and a message 'Received 403 from a service request'. The user suspects that the error is due to the container registry belonging to another subscription and is unsure if it is mandatory for the registry to be in the same subscription.",
        "Question_gpt_summary":"user encount challeng try creat workspac azur cli referenc exist azur contain registri subscript deploi fail error code internalservererror messag receiv servic request user suspect error contain registri belong subscript unsur mandatori registri subscript",
        "Answer_original_content":"http doc microsoft com azur machin learn manag workspac tab python limit want us exist servic differ azur subscript workspac regist azur machin learn namespac subscript contain servic order us acr differ subscript need regist resourc provid microsoft machinelearningservic subscript contain acr inform regist regist azur resourc provid type articl regist resourc provid us regist azresourceprovid providernamespac microsoft machinelearningservic inform particular resourc provid us azresourceprovid providernamespac microsoft machinelearningservic",
        "Answer_preprocessed_content":"want us exist servic differ azur subscript workspac regist azur machin learn namespac subscript contain servic order us acr differ subscript need regist resourc provid subscript contain acr inform regist regist azur resourc provid type articl regist resourc provid us inform particular resourc provid us",
        "Answer_gpt_summary_original":"Solution: The user needs to register the Azure Machine Learning namespace in the subscription that contains the Azure Container Registry (ACR) to use existing services from a different Azure subscription than the workspace. To register the resource provider, the user can use the command \"Register-AzResourceProvider -ProviderNamespace Microsoft.MachineLearningServices\". To see information for a particular resource provider, the user can use the command \"Get-AzResourceProvider -ProviderNamespace Microsoft.MachineLearningServices\".",
        "Answer_gpt_summary":"solut user need regist namespac subscript contain azur contain registri acr us exist servic differ azur subscript workspac regist resourc provid user us command regist azresourceprovid providernamespac microsoft machinelearningservic inform particular resourc provid user us command azresourceprovid providernamespac microsoft machinelearningservic"
    },
    {
        "Question_title":"Consuming ML models on Power BI",
        "Question_body":"When I connect the ML model endpoint with Power BI, it doesn't show me the model attributes that match the PBI dataset. Can you please help with the expected data format in Power BI.",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1653284389440,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@Arjun-1257 Thanks, Can you try define the input schema and follow the below sample.\n\nHere is the sample to Deploy trained model to Azure ML and use Power BI to score new data.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/859568\/consuming-ml-models-on-power-bi.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1653287904968,
        "Question_original_content":"consum model power connect model endpoint power model attribut match pbi dataset help expect data format power",
        "Question_preprocessed_content":"consum model power connect model endpoint power model attribut match pbi dataset help expect data format power",
        "Question_gpt_summary_original":"The user is facing challenges in connecting their ML model endpoint with Power BI as the model attributes do not match the PBI dataset. They are seeking help with the expected data format in Power BI.",
        "Question_gpt_summary":"user face challeng connect model endpoint power model attribut match pbi dataset seek help expect data format power",
        "Answer_original_content":"arjun thank try defin input schema follow sampl sampl deploi train model us power score new data",
        "Answer_preprocessed_content":"thank try defin input schema follow sampl sampl deploi train model us power score new data",
        "Answer_gpt_summary_original":"Solution: The discussion suggests defining the input schema and following a provided sample to deploy the trained model to Azure ML and use Power BI to score new data. No other solutions were mentioned.",
        "Answer_gpt_summary":"solut discuss suggest defin input schema follow provid sampl deploi train model us power score new data solut mention"
    },
    {
        "Question_title":"ModuleNotFoundError: No module named 'allegroai'",
        "Question_body":"<p>I am trying to use <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/mlops\/mlops_first_steps\" rel=\"nofollow noreferrer\">ClearML<\/a>.<\/p>\n<p>The only line in my file is<\/p>\n<pre><code>from allegroai import Dataset, DatasetVersion\n<\/code><\/pre>\n<p>which yields<\/p>\n<pre><code>ModuleNotFoundError: No module named 'allegroai'\n<\/code><\/pre>\n<p>Looks like some pip package is missing, but I couldn't for the life of me find it in the docs.<\/p>\n<p>What should I pip install?<\/p>\n<p><strong>Not working:<\/strong><\/p>\n<ul>\n<li><code>pip install clearml-agent<\/code><\/li>\n<li><code>pip install clearml<\/code> and <code>clearml-init<\/code> as in <a href=\"https:\/\/clear.ml\/docs\/latest\/docs\/getting_started\/ds\/ds_first_steps\" rel=\"nofollow noreferrer\">here<\/a><\/li>\n<li><code>pip install allegroai<\/code><\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1655044800580,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":49.0,
        "Answer_body":"<p>Allegroai package should be taken from ClearML PyPi server.\nThis is only for paying customers (I think), and the way to retrieve it is by:<\/p>\n<ol>\n<li>Going to ClearML website (login with username\/company).<\/li>\n<li>Press the ? on the top right of the screen (next to your user icon) and choose the first one\n<a href=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B1rDo.png\" alt=\"1\" \/><\/a><\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72593187",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1657799581543,
        "Question_original_content":"modulenotfounderror modul name allegroai try us line file allegroai import dataset datasetvers yield modulenotfounderror modul name allegroai look like pip packag miss couldn life doc pip instal work pip instal agent pip instal init pip instal allegroai",
        "Question_preprocessed_content":"modulenotfounderror modul name allegroai try us line file yield look like pip packag miss couldn life doc pip instal work",
        "Question_gpt_summary_original":"The user is encountering a \"ModuleNotFoundError\" when trying to use ClearML due to a missing \"allegroai\" module. They have attempted to install various packages, including \"clearml-agent\" and \"allegroai\", but none have resolved the issue.",
        "Question_gpt_summary":"user encount modulenotfounderror try us miss allegroai modul attempt instal packag includ agent allegroai resolv issu",
        "Answer_original_content":"allegroai packag taken pypi server pai custom think wai retriev go websit login usernam compani press right screen user icon choos",
        "Answer_preprocessed_content":"allegroai packag taken pypi server pai custom wai retriev go websit press right screen choos",
        "Answer_gpt_summary_original":"Solution: The \"allegroai\" package should be taken from ClearML PyPi server, which is only available for paying customers. To retrieve it, the user needs to log in to the ClearML website, click on the \"?\" icon on the top right of the screen, and choose the first option. No other solutions were mentioned.",
        "Answer_gpt_summary":"solut allegroai packag taken pypi server avail pai custom retriev user need log websit click icon right screen choos option solut mention"
    },
    {
        "Question_title":"How to custom code an inference pipeline in AWS sagemaker?",
        "Question_body":"<p>I am building a time series usecase to automate the preprocess and retrain tasks.At first the data is preprocessed using numpy, pandas, statsmodels etc &amp; later a machine learning algorithm is applied to make predictions.\nThe reason for using inference pipeline is that it reuses the same preprocess code for training and inference. I have checked the examples given by AWS sagemaker team with spark and sci-kit learn. In both the examples they use a sci-kit learn container to fit &amp; transform their preprocess code. Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code? <\/p>\n\n<p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n\n<p><strong>Sources looked into:<\/strong><\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/scikit_learn_inference_pipeline<\/a>\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/advanced_functionality\/inference_pipeline_sparkml_blazingtext_dbpedia<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1574408498230,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":1186.0,
        "Answer_body":"<p>Apologies for the late response.<\/p>\n\n<p>Below is some documentation on inference pipelines:\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html<\/a>\n<a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipeline-real-time.html<\/a><\/p>\n\n<blockquote>\n  <p>Should I also have to create a container which is not needed in my use case as I am not using any sci-kit-learn code?<\/p>\n<\/blockquote>\n\n<p>Your container is an encapsulation of the environment needed for your custom code needed to run properly. Based on the requirements listed above, <code>numpy, pandas, statsmodels etc &amp; later a machine learning algorithm<\/code>, I would create a container if you wish to isolate your dependencies or modify an existing predefined SageMaker container, such as the scikit-learn one, and add your dependencies into that.<\/p>\n\n<blockquote>\n  <p>Can someone give me a custom example of using these pipelines? Any help is appreciated!<\/p>\n<\/blockquote>\n\n<p>Unfortunately, the two example notebooks referenced above are the only examples utilizing inference pipelines. The biggest hurdle most likely is creating containers that fulfill the preprocessing and prediction task you are seeking and then combining those two together into the inference pipeline.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1579974118672,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58989610",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1575505485728,
        "Question_original_content":"custom code infer pipelin build time seri usecas autom preprocess retrain task data preprocess numpi panda statsmodel later machin learn algorithm appli predict reason infer pipelin reus preprocess code train infer check exampl given team spark sci kit learn exampl us sci kit learn contain fit transform preprocess code creat contain need us case sci kit learn code custom exampl pipelin help appreci sourc look http github com awslab amazon exampl tree master python sdk scikit learn infer pipelin http github com awslab amazon exampl tree master advanc function infer pipelin sparkml blazingtext dbpedia",
        "Question_preprocessed_content":"custom code infer pipelin build time seri usecas autom preprocess retrain data preprocess numpi panda statsmodel later machin learn algorithm appli predict reason infer pipelin reus preprocess code train infer check exampl given team spark learn exampl us learn contain fit transform preprocess code creat contain need us case code custom exampl pipelin help appreci sourc look",
        "Question_gpt_summary_original":"The user is facing a challenge in custom coding an inference pipeline in AWS Sagemaker for a time series use case. They have checked examples provided by AWS Sagemaker team but are unsure if they need to create a container as they are not using any sci-kit-learn code. The user is seeking a custom example of using these pipelines.",
        "Question_gpt_summary":"user face challeng custom code infer pipelin time seri us case check exampl provid team unsur need creat contain sci kit learn code user seek custom exampl pipelin",
        "Answer_original_content":"apolog late respons document infer pipelin http doc aw amazon com latest infer pipelin html http doc aw amazon com latest infer pipelin real time html creat contain need us case sci kit learn code contain encapsul environ need custom code need run properli base requir list numpi panda statsmodel later machin learn algorithm creat contain wish isol depend modifi exist predefin contain scikit learn add depend custom exampl pipelin help appreci unfortun exampl notebook referenc exampl util infer pipelin biggest hurdl like creat contain fulfil preprocess predict task seek combin infer pipelin",
        "Answer_preprocessed_content":"apolog late respons document infer pipelin creat contain need us case code contain encapsul environ need custom code need run properli base requir list creat contain wish isol depend modifi exist predefin contain add depend custom exampl pipelin help appreci unfortun exampl notebook referenc exampl util infer pipelin biggest hurdl like creat contain fulfil preprocess predict task seek combin infer pipelin",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- The user can refer to the AWS Sagemaker documentation on inference pipelines to understand how to create them for their time series use case.\n- The user may need to create a container to encapsulate the environment needed for their custom code to run properly, even if they are not using any sci-kit-learn code.\n- The user can modify an existing predefined SageMaker container, such as the scikit-learn one, and add their dependencies into that.\n- Unfortunately, no custom example of using these pipelines is provided in the discussion.",
        "Answer_gpt_summary":"possibl solut mention discuss user refer document infer pipelin understand creat time seri us case user need creat contain encapsul environ need custom code run properli sci kit learn code user modifi exist predefin contain scikit learn add depend unfortun custom exampl pipelin provid discuss"
    },
    {
        "Question_title":"AzureML schema \"list index out of range\" error",
        "Question_body":"<p>I developed a machine learning model using Azure ML's clustering. Few of the requests made from the cluster are triggering 404 HTTP error. I followed the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">document<\/a> to do modifications in my swagger.json file. Finally ended up with &quot;list index out of range&quot; error. It seems to be having issue with the global parameter but I am no sure about it. I am using the API from postman with some default headers like mentioned in the body below<\/p>\n<pre><code>{\n    &quot;Inputs&quot;: {\n         &quot;input_1&quot; : &quot;content&quot;\n         &quot;input_2: : &quot;content&quot;\n         ......\n    },\n    &quot;GlobalParameters&quot;: 0\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1651094225790,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1651098753480,
        "Question_score":1.0,
        "Question_view_count":78.0,
        "Answer_body":"<p>Change the &quot;GlobalParameter&quot; value to any floating number other than 1.0 or even you can remove it and execute. Sometimes, Global parameter will cause the issue. Check the below documentation.<\/p>\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/746784\/azure-ml-studio-error-while-testing-real-time-endp.html<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72035391",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1651111084096,
        "Question_original_content":"schema list index rang error develop machin learn model cluster request cluster trigger http error follow document modif swagger json file final end list index rang error have issu global paramet sure api postman default header like mention bodi input input content input content globalparamet",
        "Question_preprocessed_content":"schema list index rang error develop machin learn model cluster request cluster trigger http error follow document modif file final end list index rang error have issu global paramet sure api postman default header like mention bodi",
        "Question_gpt_summary_original":"The user encountered a \"list index out of range\" error while modifying the swagger.json file to resolve 404 HTTP errors triggered by requests made from the Azure ML clustering machine learning model. The error seems to be related to the global parameter, but the user is unsure. The API is being used from Postman with default headers.",
        "Question_gpt_summary":"user encount list index rang error modifi swagger json file resolv http error trigger request cluster machin learn model error relat global paramet user unsur api postman default header",
        "Answer_original_content":"chang globalparamet valu float number remov execut global paramet caus issu check document http doc microsoft com answer question azur studio error test real time endp html",
        "Answer_preprocessed_content":"chang globalparamet valu float number remov execut global paramet caus issu check document",
        "Answer_gpt_summary_original":"Solution: The user can change the \"GlobalParameter\" value to any floating number other than 1.0 or remove it and execute. Sometimes, the Global parameter can cause the issue. The user can refer to the documentation provided in the discussion for more information.",
        "Answer_gpt_summary":"solut user chang globalparamet valu float number remov execut global paramet caus issu user refer document provid discuss inform"
    },
    {
        "Question_title":"Does the Azure CLI ML \"service run\" command work?",
        "Question_body":"<p>I have deployed a Model to an ACI container and have an endpoint that I can hit in Postman or using python SDK. I use Python to hit the endpoint as well as Postman and I get a response and the Container Instance logging records the event. I now what to use the AZ ML CLI to run the service and pass in some hardcoded JSON:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/service?view=azure-cli-latest#ext-azure-cli-ml-az-ml-service-run\" rel=\"nofollow noreferrer\">From the Azure ML CLI docs<\/a>:  <\/p>\n\n<pre><code>az ml service run --name (-n) --input-data (-d)\n<\/code><\/pre>\n\n<p>I run this <\/p>\n\n<pre><code>az ml service run -n \"rj-aci-5\" -d {\\\"input_df\\\": [{\\\"width\\\": 50, \\\"shoe_size\\\": 28}]}\n<\/code><\/pre>\n\n<p>There is no output or error. The logs do not record any invocation. Has anyone used the Azure CLI ML extensions to run a service in the manner above?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1570495889007,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":63.0,
        "Answer_body":"<p>The az cli is likely failing to parse the provided data input. If I attempt to run the same command I see the following error:<\/p>\n\n<p><code>az: error: unrecognized arguments: [{\"width\": 50, \"shoe_size\": 28}]}<\/code><\/p>\n\n<p>You need to wrap the input in quotes for it to appropriately be taken as a single input parameter:<\/p>\n\n<p><code>az ml service run -n \"rj-aci-5\" -d \"{\\\"input_df\\\": [{\\\"width\\\": 50, \\\"shoe_size\\\": 28}]}\"<\/code><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58278844",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1570637042412,
        "Question_original_content":"azur cli servic run command work deploi model aci contain endpoint hit postman python sdk us python hit endpoint postman respons contain instanc log record event us cli run servic pass hardcod json cli doc servic run input data run servic run aci input width shoe size output error log record invoc azur cli extens run servic manner",
        "Question_preprocessed_content":"azur cli servic run command work deploi model aci contain endpoint hit postman python sdk us python hit endpoint postman respons contain instanc log record event us cli run servic pass hardcod json cli doc run output error log record invoc azur cli extens run servic manner",
        "Question_gpt_summary_original":"The user is facing a challenge with the Azure CLI ML \"service run\" command. They have deployed a model to an ACI container and can hit the endpoint using Postman or Python SDK. However, when they try to use the AZ ML CLI to run the service and pass in some hardcoded JSON, there is no output or error, and the logs do not record any invocation. The user is seeking help from others who have used the Azure CLI ML extensions to run a service in a similar manner.",
        "Question_gpt_summary":"user face challeng azur cli servic run command deploi model aci contain hit endpoint postman python sdk try us cli run servic pass hardcod json output error log record invoc user seek help azur cli extens run servic similar manner",
        "Answer_original_content":"cli like fail pars provid data input attempt run command follow error error unrecogn argument width shoe size need wrap input quot appropri taken singl input paramet servic run aci input width shoe size",
        "Answer_preprocessed_content":"cli like fail pars provid data input attempt run command follow error need wrap input quot appropri taken singl input paramet",
        "Answer_gpt_summary_original":"Solution: The user needs to wrap the input in quotes for it to be taken as a single input parameter. The correct command should be: \n\n`az ml service run -n \"rj-aci-5\" -d \"{\\\"input_df\\\": [{\\\"width\\\": 50, \\\"shoe_size\\\": 28}]}\"` \n\nThis will allow the az cli to parse the provided data input correctly.",
        "Answer_gpt_summary":"solut user need wrap input quot taken singl input paramet correct command servic run aci input width shoe size allow cli pars provid data input correctli"
    },
    {
        "Question_title":"Azure Machine Learning - error during the creation Create a control script",
        "Question_body":"Hello, I am reproducing this tutorial https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-1st-experiment-hello-world \/ Create a control script. The next observations appear in the console.\n\nI will thank you if some ideas are shared with me to face this issue.\n\nRegards",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1643753509637,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@Anth0nyCamp0s I believe the error is because in the config command you are using the script parameter hello.py which is in .\/src directory but because you are already in .\/src directory on the terminal and the source_directory parameter also mentions to use .\/src as the path to the file the following error is indicated in the message.\n\n\n\n\nIf you navigate back to get-started directory in your terminal and run the script run-hello.py your experiment should be created successfully.\n\n\n\n\nIf an answer is helpful, please click on  or upvote  which might help other community members reading this thread.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/718873\/azure-machine-learning-error-during-the-creation-c.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1643789056328,
        "Question_original_content":"error creation creat control script hello reproduc tutori http doc microsoft com azur machin learn tutori experi hello world creat control script observ appear consol thank idea share face issu regard",
        "Question_preprocessed_content":"error creation creat control script hello reproduc tutori creat control script observ appear consol thank idea share face issu regard",
        "Question_gpt_summary_original":"The user encountered an error while following a tutorial on creating a control script in Azure Machine Learning. They are seeking ideas to resolve the issue.",
        "Question_gpt_summary":"user encount error follow tutori creat control script seek idea resolv issu",
        "Answer_original_content":"anthnycamp believ error config command script paramet hello src directori src directori termin sourc directori paramet mention us src path file follow error indic messag navig start directori termin run script run hello experi creat successfulli answer help click upvot help commun member read thread",
        "Answer_preprocessed_content":"believ error config command script paramet directori directori termin paramet mention us path file follow error indic messag navig directori termin run script experi creat successfulli answer help click upvot help commun member read thread",
        "Answer_gpt_summary_original":"Solution: The error is caused by using the script parameter hello.py which is in the .\/src directory while already being in the .\/src directory on the terminal. To resolve the issue, the user should navigate back to the get-started directory in the terminal and run the script run-hello.py.",
        "Answer_gpt_summary":"solut error caus script paramet hello src directori src directori termin resolv issu user navig start directori termin run script run hello"
    },
    {
        "Question_title":"Sagemaker Batch Transform entry point",
        "Question_body":"<p>Before the AWS Sagemaker batch transform I need to do some transform. is it possible to have an custom script and associate as entry point to BatchTransformer?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1645114921967,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":230.0,
        "Answer_body":"<p>The inference code and requirement.txt should be stored as part of model.gz while training.  They will be used in the batch transform!!<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71161777",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1646173336163,
        "Question_original_content":"batch transform entri point batch transform need transform possibl custom script associ entri point batchtransform",
        "Question_preprocessed_content":"batch transform entri point batch transform need transform possibl custom script associ entri point batchtransform",
        "Question_gpt_summary_original":"The user is facing a challenge with AWS Sagemaker batch transform and is seeking to use a custom script as an entry point to the BatchTransformer.",
        "Question_gpt_summary":"user face challeng batch transform seek us custom script entri point batchtransform",
        "Answer_original_content":"infer code requir txt store model train batch transform",
        "Answer_preprocessed_content":"infer code store train batch transform",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to store the inference code and requirement.txt as part of model.gz while training. This will ensure that they are used in the batch transform.",
        "Answer_gpt_summary":"solut solut mention discuss store infer code requir txt model train ensur batch transform"
    },
    {
        "Question_title":"How to extract a dataset from azureml.core.model.Model Class?",
        "Question_body":"<p>Azure Machine Learning Service's Model Artifact has the ability to store references to the Datasets associated with the model. We can use <code>azureml.core.model.Model.add_dataset_references([('relation-as-a-string', Dataset)])<\/code> to add these dataset references.\nHow do we retrieve a Dataset from the references stored in this Model class by using a reference to the Model Class?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1623658430013,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":225.0,
        "Answer_body":"<p>Consider that a Dataset was added as a reference to a Model with the name <code>'training_dataset'<\/code><\/p>\n<p>In order to get a reference to this Dataset we use:<\/p>\n<pre><code>model = Model(workspace, name)\ndataset_id = next(dictionary['id'] for dictionary in model.serialize()['datasets'] if dictionary[&quot;name&quot;] == 'training_dataset')\ndataset_reference = Dataset.get_by_id(workspace, dataset_id )\n<\/code><\/pre>\n<p>After this step we can use <code>dataset_reference<\/code> as any other AzureML Dataset Class object.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67966905",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1624008862167,
        "Question_original_content":"extract dataset core model model class servic model artifact abil store refer dataset associ model us core model model add dataset refer relat string dataset add dataset refer retriev dataset refer store model class refer model class",
        "Question_preprocessed_content":"extract dataset class servic model artifact abil store refer dataset associ model us add dataset refer retriev dataset refer store model class refer model class",
        "Question_gpt_summary_original":"The user is facing a challenge in retrieving a dataset from the references stored in the Azure Machine Learning Service's Model Artifact by using a reference to the Model Class.",
        "Question_gpt_summary":"user face challeng retriev dataset refer store servic model artifact refer model class",
        "Answer_original_content":"consid dataset ad refer model train dataset order refer dataset us model model workspac dataset dictionari dictionari model serial dataset dictionari train dataset dataset refer dataset workspac dataset step us dataset refer dataset class object",
        "Answer_preprocessed_content":"consid dataset ad refer model order refer dataset us step us dataset class object",
        "Answer_gpt_summary_original":"Solution: The solution to retrieve a dataset from the references stored in the Azure Machine Learning Service's Model Artifact by using a reference to the Model Class is to use the code provided in the discussion. The code involves using the Model Class to get the dataset ID and then using the Dataset Class to get the dataset reference.",
        "Answer_gpt_summary":"solut solut retriev dataset refer store servic model artifact refer model class us code provid discuss code involv model class dataset dataset class dataset refer"
    },
    {
        "Question_title":"How can I choose the right number of epochs per trial in Optuna?",
        "Question_body":"<p>Is there a rule of thumb for how to choose the number of epochs per trial in <a href=\"https:\/\/optuna.org\/\" rel=\"nofollow noreferrer\">Optuna<\/a>?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1612428847203,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":253.0,
        "Answer_body":"<p>I would imagine epochs are directly correlated with your computational costs, but perhaps that's a parameter worth optimizing. If you aren't sure, start with your best guess and run a few optimization studies with different epoch values. Once you confirm the importance of your epochs, you can conduct separate studies on just the epoch value.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66042246",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Question_closed_time":1612636645883,
        "Question_original_content":"choos right number epoch trial rule thumb choos number epoch trial",
        "Question_preprocessed_content":"choos right number epoch trial rule thumb choos number epoch trial",
        "Question_gpt_summary_original":"The user is facing a challenge in determining the appropriate number of epochs per trial in Optuna and is seeking guidance on whether there is a rule of thumb to follow.",
        "Question_gpt_summary":"user face challeng determin appropri number epoch trial seek guidanc rule thumb follow",
        "Answer_original_content":"imagin epoch directli correl comput cost paramet worth optim aren sure start best guess run optim studi differ epoch valu confirm import epoch conduct separ studi epoch valu",
        "Answer_preprocessed_content":"imagin epoch directli correl comput cost paramet worth optim aren sure start best guess run optim studi differ epoch valu confirm import epoch conduct separ studi epoch valu",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that there is no specific rule of thumb to determine the appropriate number of epochs per trial in Optuna. However, it is recommended to start with a best guess and run optimization studies with different epoch values to determine their importance. Once the importance of epochs is confirmed, separate studies can be conducted to determine the optimal epoch value.",
        "Answer_gpt_summary":"solut discuss suggest specif rule thumb determin appropri number epoch trial recommend start best guess run optim studi differ epoch valu determin import import epoch confirm separ studi conduct determin optim epoch valu"
    },
    {
        "Question_title":"Mount a datalake storage in azure ML studio",
        "Question_body":"<p>I created a file dataset from a data lake folder on Azure ML Studio,  at the moment I\u00b4m able to download the data from the dataset to the compute instance with this code:<\/p>\n<pre><code>subscription_id = 'xxx'\nresource_group = 'luisdatapipelinetest'\nworkspace_name = 'ml-pipelines'\nworkspace = Workspace(subscription_id, resource_group, workspace_name)\ndataset = Dataset.get_by_name(workspace, name='files_test')\npath = &quot;\/mnt\/batch\/tasks\/shared\/LS_root\/mounts\/clusters\/demo1231\/code\/Users\/luis.rramirez\/test\/&quot;\ndataset.download(target_path=path, overwrite=True)\n<\/code><\/pre>\n<p>With that I'm able to access the files from the notebook.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/8q8y2.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But copying the data from the data lake to the compute instance is not efficient, how can I mount the data lake directory in the vm instead of copying the data each time?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1630008530263,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1630044624416,
        "Question_score":1.0,
        "Question_view_count":258.0,
        "Answer_body":"<p>MOUNTING ADLS2 to AML so you can save files into your mountPoint directly. <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"nofollow noreferrer\">Here<\/a> is the example of registering the storage and <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.data.file_dataset.filedataset?view=azure-ml-py#mount-mount-point-none----kwargs-\" rel=\"nofollow noreferrer\">here<\/a> shows how to mount your registered datastore.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68944750",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1630298993132,
        "Question_original_content":"mount datalak storag studio creat file dataset data lake folder studio moment abl download data dataset comput instanc code subscript resourc group luisdatapipelinetest workspac pipelin workspac workspac subscript resourc group workspac dataset dataset workspac file test path mnt batch task share root mount cluster demo code user lui rramirez test dataset download target path path overwrit true abl access file notebook copi data data lake comput instanc effici mount data lake directori instead copi data time",
        "Question_preprocessed_content":"mount datalak storag studio creat file dataset data lake folder studio moment abl download data dataset comput instanc code abl access file notebook copi data data lake comput instanc effici mount data lake directori instead copi data time",
        "Question_gpt_summary_original":"The user has created a file dataset from a data lake folder on Azure ML Studio and is able to download the data from the dataset to the compute instance. However, the user is facing the challenge of copying the data from the data lake to the compute instance each time, which is not efficient. The user is seeking a solution to mount the data lake directory in the VM instead of copying the data each time.",
        "Question_gpt_summary":"user creat file dataset data lake folder studio abl download data dataset comput instanc user face challeng copi data data lake comput instanc time effici user seek solut mount data lake directori instead copi data time",
        "Answer_original_content":"mount adl aml save file mountpoint directli exampl regist storag show mount regist datastor",
        "Answer_preprocessed_content":"mount adl aml save file mountpoint directli exampl regist storag show mount regist datastor",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to mount the data lake directory in the VM instead of copying the data each time. The user can register the storage and mount the registered datastore using the Azure ML Studio. The example of registering the storage and how to mount the registered datastore is provided in the discussion.",
        "Answer_gpt_summary":"solut solut suggest discuss mount data lake directori instead copi data time user regist storag mount regist datastor studio exampl regist storag mount regist datastor provid discuss"
    },
    {
        "Question_title":"How to prevent storing data in Jupyter project tree when writing data from Sagemaker to S3",
        "Question_body":"<p>I am new to AWS Sagemaker and I wrote data to my S3 bucket.\nBut these datasets also appear in the working tree of my jupyter instance.<\/p>\n<p>How can I move data directly to S3 without saving it &quot;locally&quot;?<\/p>\n<p>My code:<\/p>\n<pre><code>import os\nimport pandas as pd\n\nimport sagemaker, boto3\nfrom sagemaker import get_execution_role\nfrom sagemaker.inputs import TrainingInput\nfrom sagemaker.serializers import CSVSerializer\n\n# please provide your own bucket and folder path of your bucket here\nbucket = &quot;test-bucket2342343&quot;\nsm_sess = sagemaker.Session(default_bucket=bucket)\nfile_path = &quot;Use Cases\/Sagemaker Demo\/xgboost&quot;\n\n# data \ndf_train = pd.DataFrame({'X':[0,100,200,400,450,  550,600,800,1600],\n                         'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\ndf_test = pd.DataFrame({'X':[10,90,240,459,120,  650,700,1800,1300],\n                        'y':[0,0,  0,  0,  0,    1,  1,  1,  1]})\n\n# move to S3 \ndf_train[['y','X']].to_csv('train.csv', header=False, index=False)\n\ndf_val = df_test.copy()\ndf_val[['y','X']].to_csv('val.csv', header=False, index=False)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;train.csv&quot;)).upload_file(&quot;train.csv&quot;)\n\nboto3.Session().resource(&quot;s3&quot;).Bucket(bucket) \\\n.Object(os.path.join(file_path, &quot;val.csv&quot;)).upload_file(&quot;val.csv&quot;)\n\n<\/code><\/pre>\n<p>It successfully appears in my S3 bucket.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/d1yCy.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>But it also appears here:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/RjGZr.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1661336683943,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1661337201248,
        "Question_score":0.0,
        "Question_view_count":17.0,
        "Answer_body":"<p>with Pandas you can save to S3 directly (<a href=\"https:\/\/stackoverflow.com\/a\/56275519\/121956\">relevant answer<\/a>). For example:<\/p>\n<pre><code>import pandas as pd\ndf = pd.DataFrame( [ [1, 1, 1], [2, 2, 2] ], columns=['a', 'b', 'c'])\ndf.to_csv('s3:\/\/test-bucket2342343\/\/tmp.csv', index=False)\n<\/code><\/pre>\n<p>Or, use what you currently do and delete the local files:<\/p>\n<pre><code>import os\nos.remove('train.csv')\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73471486",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1661447960212,
        "Question_original_content":"prevent store data jupyt project tree write data new wrote data bucket dataset appear work tree jupyt instanc data directli save local code import import panda import boto import execut role input import traininginput serial import csvserial provid bucket folder path bucket bucket test bucket sess session default bucket bucket file path us case demo xgboost data train datafram test datafram train csv train csv header fals index fals val test copi val csv val csv header fals index fals boto session resourc bucket bucket object path join file path train csv upload file train csv boto session resourc bucket bucket object path join file path val csv upload file val csv successfulli appear bucket appear",
        "Question_preprocessed_content":"prevent store data jupyt project tree write data new wrote data bucket dataset appear work tree jupyt instanc data directli save local code successfulli appear bucket appear",
        "Question_gpt_summary_original":"The user is facing a challenge of data being stored in the Jupyter project tree when writing data from Sagemaker to S3. They are seeking a solution to move data directly to S3 without saving it locally. The user has successfully uploaded the data to their S3 bucket, but it also appears in the working tree of their Jupyter instance.",
        "Question_gpt_summary":"user face challeng data store jupyt project tree write data seek solut data directli save local user successfulli upload data bucket appear work tree jupyt instanc",
        "Answer_original_content":"panda save directli relev answer exampl import panda datafram column csv test bucket tmp csv index fals us current delet local file import remov train csv",
        "Answer_preprocessed_content":"panda save directli exampl us current delet local file",
        "Answer_gpt_summary_original":"Solution 1: Use Pandas to save data directly to S3 without saving it locally. The user can import Pandas and use the to_csv() function to save the data to their S3 bucket. \n\nSolution 2: Continue with the current method of saving data locally and then delete the local files using the os.remove() function.",
        "Answer_gpt_summary":"solut us panda save data directli save local user import panda us csv function save data bucket solut continu current method save data local delet local file remov function"
    },
    {
        "Question_title":"Where can I find guidance for getting a customer started with SageMaker sizing and cost?",
        "Question_body":"A customer wants to use SageMaker, but doesn't know how to get started with instance sizes or how to forecast the cost for it. I've looked at the SageMaker TCO PDF we have online, but that appears more marketing than helpful, i.e. more price comparison than guidance.\n\nI know that the SageMaker cost is really the underlying EC2 and storage pieces, not SageMaker itself. However, I feel it is incorrect to say that they start with (say) t3.medium and see if that fits and scale up if they need more power behind it. As well, that doesn't help them to forecast either.\n\nAny thoughts here?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1603285551000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":33.0,
        "Answer_body":"See the performance efficiency and cost optimization pillars in Machine Learning Lens. Additionally this is an EC2 based right sizing best practices guide.\nOverall, it's better to start small, then increase instance size as needed (as those that start large, never bother reduce the size), or apply auto scaling for SageMaker hosting.\nAssuming a CPU ML predictions: When choosing ml.t2.medium instances the customer will need to keep an eye on the instance CPU credits. If they lack the knowledge, just start with M5.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUq-Kaj1bLStK6Bs2gCUZ1Iw\/where-can-i-find-guidance-for-getting-a-customer-started-with-sage-maker-sizing-and-cost",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1603286522000,
        "Question_original_content":"guidanc get custom start size cost custom want us know start instanc size forecast cost look tco pdf onlin appear market help price comparison guidanc know cost underli storag piec feel incorrect start medium fit scale need power help forecast thought",
        "Question_preprocessed_content":"guidanc get custom start size cost custom want us know start instanc size forecast cost look tco pdf onlin appear market help price comparison guidanc know cost underli storag piec feel incorrect start fit scale need power help forecast thought",
        "Question_gpt_summary_original":"The user is facing challenges in guiding a customer who wants to use SageMaker but is unsure about instance sizes and cost forecasting. The available SageMaker TCO PDF is not helpful and appears to be more of a marketing tool than guidance. The user is aware that the cost is based on underlying EC2 and storage components, but suggesting starting with a specific instance size and scaling up if needed is not a helpful solution. The user is seeking advice on how to provide better guidance to the customer.",
        "Question_gpt_summary":"user face challeng guid custom want us unsur instanc size cost forecast avail tco pdf help appear market tool guidanc user awar cost base underli storag compon suggest start specif instanc size scale need help solut user seek advic provid better guidanc custom",
        "Answer_original_content":"perform effici cost optim pillar machin learn len addition base right size best practic guid overal better start small increas instanc size need start larg bother reduc size appli auto scale host assum cpu predict choos medium instanc custom need ey instanc cpu credit lack knowledg start",
        "Answer_preprocessed_content":"perform effici cost optim pillar machin learn len addition base right size best practic guid overal better start small increas instanc size need appli auto scale host assum cpu predict choos instanc custom need ey instanc cpu credit lack knowledg start",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include referring to the performance efficiency and cost optimization pillars in Machine Learning Lens, using an EC2-based right sizing best practices guide, starting with a small instance size and scaling up as needed, applying auto scaling for SageMaker hosting, and choosing an appropriate instance size based on the type of ML predictions needed. Specifically, for CPU ML predictions, it is suggested to start with M5 instances or keep an eye on the CPU credits when using ml.t2.medium instances.",
        "Answer_gpt_summary":"possibl solut mention discuss includ refer perform effici cost optim pillar machin learn len base right size best practic guid start small instanc size scale need appli auto scale host choos appropri instanc size base type predict need specif cpu predict suggest start instanc ey cpu credit medium instanc"
    },
    {
        "Question_title":"Loading Pretrained Keras to Sagemaker - local classification works but sagemaker classification changes",
        "Question_body":"<p>EDIT: Found a solution, see bottom of post.<\/p>\n<p>I have a pre-trained keras model (model.h5) which is a CNN for image classification. My goal is to deploy the model on sagemaker and use a lambda function to interface with the sagemaker endpoint and make predictions. When I predict with the model on my local machine using the following code, I get results I would expect:<\/p>\n<pre><code>model = load_model(r'model.h5')\nphoto_fp = r'\/path\/to\/photo.jpg'\n\nimg = Image.open(photo_fp).resize((128,128))\nimage_array = np.array(img) \/ 255.\nimg_batch = np.expand_dims(image_array, axis=0)\n\nprint(model.predict(img_batch))\n# [[9.9984562e-01 1.5430539e-04 2.2775747e-14 9.5851349e-16]]\n<\/code><\/pre>\n<p>However, when I deploy the model as an endpoint on sagemaker, I get different results. Below is my code to deploy the model as an endpoint:<\/p>\n<pre><code>model = load_model(r'model.h5')\n\nimport tensorflow as tf\nfrom tensorflow import keras\nimport sagemaker\nimport boto3, re\nfrom sagemaker import get_execution_role\ndef convert_h5_to_aws(loaded_model):\n    # Interpreted from 'Data Liam'\n    from tensorflow.python.saved_model import builder\n    from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n    from tensorflow.python.saved_model import tag_constants\n    \n    model_version = '1'\n    export_dir = 'export\/Servo\/' + model_version\n    \n    # Build the Protocol Buffer SavedModel at 'export_dir'\n    builder = builder.SavedModelBuilder(export_dir)\n    \n    # Create prediction signature to be used by TensorFlow Serving Predict API\n    signature = predict_signature_def(\n        inputs={&quot;inputs&quot;: loaded_model.input}, outputs={&quot;score&quot;: loaded_model.output})\n\n    with tf.compat.v1.Session() as sess:\n        init = tf.global_variables_initializer()\n        sess.run(init)\n        # Save the meta graph and variables\n        builder.add_meta_graph_and_variables(\n            sess=sess, tags=[tag_constants.SERVING], signature_def_map={&quot;serving_default&quot;: signature})\n        builder.save()\n    \n    #create a tarball\/tar file and zip it\n    import tarfile\n    with tarfile.open('model.tar.gz', mode='w:gz') as archive:\n        archive.add('export', recursive=True)\n        \nconvert_h5_to_aws(model)\n\nsagemaker_session = sagemaker.Session()\ninputs = sagemaker_session.upload_data(path='model.tar.gz', key_prefix='model')\n\n!touch train.py # from notebook\n# the (default) IAM role\nrole = get_execution_role()\nframework_version = tf.__version__\n\n# Create Sagemaker model\nfrom sagemaker.tensorflow.model import TensorFlowModel\nsagemaker_model = TensorFlowModel(model_data = 's3:\/\/' + sagemaker_session.default_bucket() + '\/model\/model.tar.gz',\n                                  role = role,\n                                  framework_version = framework_version,\n                                  entry_point = 'train.py')\n\npredictor = sagemaker_model.deploy(initial_instance_count=1,\n                                   instance_type='ml.m4.xlarge')\n<\/code><\/pre>\n<p>This deploys fine and saves as an endpoint. Then, I invoke the endpoint:<\/p>\n<pre><code>runtime = boto3.client('runtime.sagemaker')\nendpoint_name = 'endpoint-name-for-stackoverflow'\n\nimg = Image.open(photo_fp).resize((128,128))\nimage_array = np.array(img) \/ 255.\nimg_batch = np.expand_dims(image_array, axis=0)\npredictor = TensorFlowPredictor(endpoint_name)\nresult = predictor.predict(data=img_batch)\nprint(result)\n# {'predictions': [[0.199595317, 0.322404563, 0.209394112, 0.268606]]}\n<\/code><\/pre>\n<p>As you can see, the classifier is predicting all of the outputs as nearly equal probabilities, which is not what was predicted on the local machine. This leads me to believe that something is going wrong in my deployment.<\/p>\n<p>I have tried loading the model weights and json model structure to sagemaker rather than the entire h5 model but that yielded the same results. I also used invoke endpoint instead of the predictor API with the following code:<\/p>\n<pre><code>payload = json.dumps(img_batch.tolist())\nresponse = runtime.invoke_endpoint(EndpointName=endpoint_name,\n                                   ContentType='application\/json',\n                                   Body=payload)\nresult = json.loads(response['Body'].read().decode())\nprint(result)\n# {'predictions': [[0.199595317, 0.322404563, 0.209394112, 0.268606]]}\n<\/code><\/pre>\n<p>But yet again, the same results.<\/p>\n<p>Any ideas why I'm getting different results with the sagemaker than on my local machine with the same model?\nThanks!<\/p>\n<p>EDIT: Found a solution. The problem was with the TensorflowModel framework version argument. I changed the framework_version to '1.12' and installed version 1.12 in the Sagemaker Jupyter instance and retrained my model locally using TF 1.12. I'm not totally sure why this works but all of the blogs I found (e.g. <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/deploy-trained-keras-or-tensorflow-models-using-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">this one<\/a>) used 1.12. Hope this helps.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1620830223953,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1621607684923,
        "Question_score":0.0,
        "Question_view_count":169.0,
        "Answer_body":"<p>For the benefit of community providing solution in answer section<\/p>\n<blockquote>\n<p>The problem was with the <code>TensorflowModel<\/code> framework version argument. After\nchanging the <code>framework_version<\/code> to <code>1.12<\/code> and installed version <code>TF 1.12<\/code> in\nthe <code>Sagemaker Jupyter<\/code> instance and retrained model locally using <code>TF 1.12<\/code> got same results. (paraphrased from Peter Van Katwyk)<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67505781",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1621926225083,
        "Question_original_content":"load pretrain kera local classif work classif chang edit solut post pre train kera model model cnn imag classif goal deploi model us lambda function interfac endpoint predict predict model local machin follow code result expect model load model model photo path photo jpg img imag open photo resiz imag arrai arrai img img batch expand dim imag arrai axi print model predict img batch deploi model endpoint differ result code deploi model endpoint model load model model import tensorflow tensorflow import kera import import boto import execut role def convert aw load model interpret data liam tensorflow python save model import builder tensorflow python save model signatur def util import predict signatur def tensorflow python save model import tag constant model version export dir export servo model version build protocol buffer savedmodel export dir builder builder savedmodelbuild export dir creat predict signatur tensorflow serv predict api signatur predict signatur def input input load model input output score load model output compat session sess init global variabl initi sess run init save meta graph variabl builder add meta graph variabl sess sess tag tag constant serv signatur def map serv default signatur builder save creat tarbal tar file zip import tarfil tarfil open model tar mode archiv archiv add export recurs true convert aw model session session input session upload data path model tar kei prefix model touch train notebook default iam role role execut role framework version version creat model tensorflow model import tensorflowmodel model tensorflowmodel model data session default bucket model model tar role role framework version framework version entri point train predictor model deploi initi instanc count instanc type xlarg deploi fine save endpoint invok endpoint runtim boto client runtim endpoint endpoint stackoverflow img imag open photo resiz imag arrai arrai img img batch expand dim imag arrai axi predictor tensorflowpredictor endpoint result predictor predict data img batch print result predict classifi predict output nearli equal probabl predict local machin lead believ go wrong deploy tri load model weight json model structur entir model yield result invok endpoint instead predictor api follow code payload json dump img batch tolist respons runtim invok endpoint endpointnam endpoint contenttyp applic json bodi payload result json load respons bodi read decod print result predict result idea get differ result local machin model thank edit solut problem tensorflowmodel framework version argument chang framework version instal version jupyt instanc retrain model local total sure work blog hope help",
        "Question_preprocessed_content":"load pretrain kera local classif work classif chang edit solut post kera model cnn imag classif goal deploi model us lambda function interfac endpoint predict predict model local machin follow code result expect deploi model endpoint differ result code deploi model endpoint deploi fine save endpoint invok endpoint classifi predict output nearli equal probabl predict local machin lead believ go wrong deploy tri load model weight json model structur entir model yield result invok endpoint instead predictor api follow code result idea get differ result local machin model thank edit solut problem tensorflowmodel framework version argument chang instal version jupyt instanc retrain model local total sure work blog hope help",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a pre-trained Keras model on Sagemaker and using a lambda function to interface with the Sagemaker endpoint to make predictions. While the model predicts results as expected on the local machine, it produces different results on Sagemaker. The user has tried loading the model weights and json model structure to Sagemaker, but it yielded the same results. The user is seeking help to understand why the Sagemaker is producing different results and how to resolve the issue. The user later found a solution by changing the framework version to '1.12' and retraining the model locally using TF 1.12.",
        "Question_gpt_summary":"user face challeng deploi pre train kera model lambda function interfac endpoint predict model predict result expect local machin produc differ result user tri load model weight json model structur yield result user seek help understand produc differ result resolv issu user later solut chang framework version retrain model local",
        "Answer_original_content":"benefit commun provid solut answer section problem tensorflowmodel framework version argument chang framework version instal version jupyt instanc retrain model local got result paraphras peter van katwyk",
        "Answer_preprocessed_content":"benefit commun provid solut answer section problem framework version argument chang instal version instanc retrain model local got result",
        "Answer_gpt_summary_original":"Solution: The solution to the problem was to change the framework version to '1.12' and retrain the model locally using TF 1.12. The user had to install version TF 1.12 in the Sagemaker Jupyter instance and change the framework_version argument to '1.12' in the TensorflowModel. After making these changes, the user was able to get the same results as on the local machine.",
        "Answer_gpt_summary":"solut solut problem chang framework version retrain model local user instal version jupyt instanc chang framework version argument tensorflowmodel make chang user abl result local machin"
    },
    {
        "Question_title":"Azure machine learning data export module failure",
        "Question_body":"I want to export the data from my batch flow with the use of the data export module. Tried multiple file shares but get the following error.\n\n\n\n\nUser program failed with UserError: ScriptExecutionException was caused by WriteStreamsException.\nWriteStreamsException was caused by UnexpectedException.\nUnexpected exception while writing files with writer 'delimited'.\nStreamAccessException was caused by NotFoundException.\nFile Share '[REDACTED]' does not exist at '[REDACTED]'.\n| session_id=e1ee8699-3a94-4ea6-ab5d-f5bb945d56f3\n\n\n\n\nThe file share name is the Azure named file share name or is this something else. Cannot find an eample.\n\n\n\n\n\n\nThe export works to local ML workspace, but this doesn't accept folders creation.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1602687603067,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello,\n\nThanks for reaching out to us. The file share name is the name of FILE_SHARE_CONTAINER\n\nPlease refer to below document for more details:\n\nExport data module: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/algorithm-module-reference\/export-data\n\nData storage - Azure File Share: https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-file-share\n\n\n\n\nPlease let me know if you have more questions.\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/126502\/azure-machine-learning-data-export-module-failure.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1602726430747,
        "Question_original_content":"data export modul failur want export data batch flow us data export modul tri multipl file share follow error user program fail usererror scriptexecutionexcept caus writestreamsexcept writestreamsexcept caus unexpectedexcept unexpect except write file writer delimit streamaccessexcept caus notfoundexcept file share redact exist redact session eee abd fbbdf file share azur name file share eampl export work local workspac accept folder creation",
        "Question_preprocessed_content":"data export modul failur want export data batch flow us data export modul tri multipl file share follow error user program fail usererror scriptexecutionexcept caus writestreamsexcept writestreamsexcept caus unexpectedexcept unexpect except write file writer delimit streamaccessexcept caus notfoundexcept file share exist file share azur name file share eampl export work local workspac accept folder creation",
        "Question_gpt_summary_original":"The user is facing challenges with exporting data from their batch flow using the data export module. They have tried multiple file shares but encountered an error due to the file share not existing. The user is unsure if the file share name is the Azure named file share name or something else. Additionally, the export works to the local ML workspace but does not accept folder creation.",
        "Question_gpt_summary":"user face challeng export data batch flow data export modul tri multipl file share encount error file share exist user unsur file share azur name file share addition export work local workspac accept folder creation",
        "Answer_original_content":"hello thank reach file share file share contain refer document detail export data modul http doc microsoft com azur machin learn algorithm modul refer export data data storag azur file share http doc microsoft com azur machin learn access data azur file share let know question regard yutong",
        "Answer_preprocessed_content":"hello thank reach file share refer document detail export data modul data storag azur file share let know question regard yutong",
        "Answer_gpt_summary_original":"Solution: The solution provided is to use the name of FILE_SHARE_CONTAINER as the file share name. The user can refer to the Export data module and Data storage - Azure File Share documents for more details.",
        "Answer_gpt_summary":"solut solut provid us file share contain file share user refer export data modul data storag azur file share document detail"
    },
    {
        "Question_title":"Training Job is Stopping in Sagemaker",
        "Question_body":"<p>Recently, I have changed account on AWS and faced with weird error in Sagemaker.<\/p>\n<p>Basically, I'm just checking <code>xgboost<\/code> algo with some toy dataset in this manner:<\/p>\n<pre><code>from sagemaker import image_uris\n\nxgb_image_uri = image_uris.retrieve(&quot;xgboost&quot;, boto3.Session().region_name, &quot;1&quot;)\n\nclf = sagemaker.estimator.Estimator(xgb_image_uri,\n                   role, 1, 'ml.c4.2xlarge',\n                   output_path=&quot;s3:\/\/{}\/output&quot;.format(session.default_bucket()),\n                   sagemaker_session=session)\n\nclf.fit(location_data)\n<\/code><\/pre>\n<p>Then the training job is starting to be executed but for some reason, on downloading data step it stops the training job and displays the following message:<\/p>\n<pre><code>2021-10-21 17:33:27 Downloading - Downloading input data\n2021-10-21 17:33:27 Stopping - Stopping the training job\n2021-10-21 17:33:27 Stopped - Training job stopped\nProfilerReport-1634837444: Stopping\n..\nJob ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n<\/code><\/pre>\n<p>Also, when I'm trying to go back to training jobs section and check for logs in cloudwatch there is nothing to be displayed. Is it common issue and who had faced with that? Are there any workarounds?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1634838048463,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":361.0,
        "Answer_body":"<p>The problem was most likely with templates for sagemaker that was runned before creating the instance.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69666500",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1634844298240,
        "Question_original_content":"train job stop recent chang account aw face weird error basic check xgboost algo toi dataset manner import imag uri xgb imag uri imag uri retriev xgboost boto session region clf estim estim xgb imag uri role xlarg output path output format session default bucket session session clf fit locat data train job start execut reason download data step stop train job displai follow messag download download input data stop stop train job stop train job stop profilerreport stop job end statu stop complet mean job time stop earli reason consid check complet expect try train job section check log cloudwatch displai common issu face workaround",
        "Question_preprocessed_content":"train job stop recent chang account aw face weird error basic check algo toi dataset manner train job start execut reason download data step stop train job displai follow messag try train job section check log cloudwatch displai common issu face workaround",
        "Question_gpt_summary_original":"The user has encountered an issue with a training job in Sagemaker after changing their AWS account. The training job stops during the downloading data step and displays a message indicating that it has been stopped. The user has also been unable to find any logs in Cloudwatch. The user is seeking advice on whether this is a common issue and if there are any workarounds.",
        "Question_gpt_summary":"user encount issu train job chang aw account train job stop download data step displai messag indic stop user unabl log cloudwatch user seek advic common issu workaround",
        "Answer_original_content":"problem like templat run creat instanc",
        "Answer_preprocessed_content":"problem like templat run creat instanc",
        "Answer_gpt_summary_original":"Solution: The discussion does not provide any specific solution to the issue.",
        "Answer_gpt_summary":"solut discuss provid specif solut issu"
    },
    {
        "Question_title":"Why are shh keys lost on reboot of AWS ec2 instance (sage maker)?",
        "Question_body":"<p>I have an AWS SageMaker notebook running some ML stuff for work, and I have a private github repo with some of my commonly used functions which is formatted in such a way to be pip install-able, so I set up an SSH key by doing this:<\/p>\n<pre><code>ssh-keygen \n\n-t rsa -b 4096 -C &quot;danielwarfield1@gmail.com&quot;\n<\/code><\/pre>\n<p>enter, enter, enter (default save location no password)<\/p>\n<pre><code>eval $(ssh-agent -s)\nssh-add ~\/.ssh\/id_rs\n<\/code><\/pre>\n<p>then I copy the public key into github, then I run this to install my library<\/p>\n<pre><code>$PWD\/pip install git+ssh:\/\/git@github.com\/...\n<\/code><\/pre>\n<p>where <code>$PWD<\/code> is the directory containing pip for the conda env I'm using (tensorflow2_p36 specifically, the one that AWS provides)<\/p>\n<p>this works fine, until I restart the EC2, then it appears my shh key (along with all my other installs) are lost, and I have to repeat the process. I expect the modules to be lost, I know SageMaker manages the environments, but me loosing my ssh key seems peculiar, is there a place I can save my ssh key wher it wont get lost, but I can still find it when I pip install?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1596230563253,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":278.0,
        "Answer_body":"<p>The <code>\/home\/ec2-user\/SageMaker<\/code> location is persisted even when you switch down the notebook instance, you can try saving things here to get them persisted. Things saved elsewhere will be lost when you switch off the instance<\/p>\n<p>Regarding private git integration, you can use the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/nbi-git-repo.html\" rel=\"nofollow noreferrer\">SageMaker git Notebook integration<\/a>, which uses Secrets Manager to safely handle your credentials<\/p>\n<p>You can perform steps automatically when the notebook starts with a <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebook-lifecycle-config.html\" rel=\"nofollow noreferrer\">lifecycle configuration<\/a>. This is useful for example to standardise and automatise copying of data and environment customization<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63199239",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1596232881236,
        "Question_original_content":"shh kei lost reboot aw instanc sage maker notebook run stuff work privat github repo commonli function format wai pip instal abl set ssh kei ssh keygen rsa danielwarfield gmail com enter enter enter default save locat password eval ssh agent ssh add ssh copi public kei github run instal librari pwd pip instal git ssh git github com pwd directori contain pip conda env tensorflow specif aw provid work fine restart appear shh kei instal lost repeat process expect modul lost know manag environ loos ssh kei peculiar place save ssh kei wher wont lost pip instal",
        "Question_preprocessed_content":"shh kei lost reboot aw instanc notebook run stuff work privat github repo commonli function format wai pip set ssh kei enter enter enter copi public kei github run instal librari directori contain pip conda env work fine restart appear shh kei lost repeat process expect modul lost know manag environ loos ssh kei peculiar place save ssh kei wher wont lost pip instal",
        "Question_gpt_summary_original":"The user is encountering a challenge where their SSH key, along with all their other installs, are lost after restarting their AWS SageMaker notebook. They have to repeat the process of setting up the SSH key and installing their library every time they restart the EC2 instance. The user is looking for a solution to save their SSH key in a place where it won't get lost but can still be found when they pip install.",
        "Question_gpt_summary":"user encount challeng ssh kei instal lost restart notebook repeat process set ssh kei instal librari time restart instanc user look solut save ssh kei place won lost pip instal",
        "Answer_original_content":"home user locat persist switch notebook instanc try save thing persist thing save lost switch instanc privat git integr us git notebook integr us secret manag safe handl credenti perform step automat notebook start lifecycl configur us exampl standardis automatis copi data environ custom",
        "Answer_preprocessed_content":"locat persist switch notebook instanc try save thing persist thing save lost switch instanc privat git integr us git notebook integr us secret manag safe handl credenti perform step automat notebook start lifecycl configur us exampl standardis automatis copi data environ custom",
        "Answer_gpt_summary_original":"Solution:\n- Save the SSH key in the <code>\/home\/ec2-user\/SageMaker<\/code> location to persist it even when the notebook instance is switched off.\n- Use the SageMaker git Notebook integration, which uses Secrets Manager to safely handle private git integration.\n- Use a lifecycle configuration to perform steps automatically when the notebook starts, such as standardizing and automatizing copying of data and environment customization.",
        "Answer_gpt_summary":"solut save ssh kei home user locat persist notebook instanc switch us git notebook integr us secret manag safe handl privat git integr us lifecycl configur perform step automat notebook start standard automat copi data environ custom"
    },
    {
        "Question_title":"How to catalog datasets & models by S3 URI, but keep a local copy?",
        "Question_body":"<p>I'm trying to figure out how to store intermediate Kedro pipeline objects both locally AND on S3. In particular, say I have a dataset on S3:<\/p>\n<pre><code>my_big_dataset.hdf5:\n  type: kedro.extras.datasets.pandas.HDFDataSet\n  filepath: &quot;s3:\/\/my_bucket\/data\/04_feature\/my_big_dataset.hdf5&quot;\n<\/code><\/pre>\n<p>I want to refer to these objects in the catalog by their S3 URI so that my team can use them. HOWEVER, I want to avoid re-downloading the datasets, model weights, etc. every time I run a pipeline by keeping a local copy in addition to the S3 copy. How do I mirror files with Kedro?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1597008515007,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597058318400,
        "Question_score":2.0,
        "Question_view_count":595.0,
        "Answer_body":"<p>This is a good question, Kedro has <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> for caching datasets within the same run, which handles caching the dataset in memory when it's used\/loaded multiple times in the same run. There isn't really the same thing that persists across runs, in general Kedro doesn't do much persistent stuff.<\/p>\n<p>That said, off the top of my head, I can think of two options that (mostly) replicates or gives this functionality:<\/p>\n<ol>\n<li>Use the same <code>catalog<\/code> in the same config environment but with the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/04_kedro_project_setup\/02_configuration.html?#templating-configuration\" rel=\"nofollow noreferrer\"><code>TemplatedConfigLoader<\/code><\/a> where your catalog datasets have their filepaths looking something like:<\/li>\n<\/ol>\n<pre><code>my_dataset:\n  filepath: ${base_data}\/01_raw\/blah.csv\n<\/code><\/pre>\n<p>and you set <code>base_data<\/code> to <code>s3:\/\/bucket\/blah<\/code> when running in &quot;production&quot; mode and with <code>local_filepath\/data<\/code> locally. You can decide how exactly you do this in your overriden <code>context<\/code> method (whether it's using <code>local\/globals.yml<\/code> (see the linked documentation above) or environment variables or what not.<\/p>\n<ol start=\"2\">\n<li>Use separate environments, likely <code>local<\/code> (it's kind of what it was made for!) where you keep a separate copy of your catalog where the filepaths are replaced with local ones.<\/li>\n<\/ol>\n<p>Otherwise, your next best bet is to write a <code>PersistentCachedDataSet<\/code> similar to <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.CachedDataSet.html\" rel=\"nofollow noreferrer\"><code>CachedDataSet<\/code><\/a> which intercepts the loading\/saving for the wrapped dataset and makes a local copy when loading for the first time in a deterministic location that you look up on subsequent loads.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63331505",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1597010598252,
        "Question_original_content":"catalog dataset model uri local copi try figur store intermedi pipelin object local particular dataset big dataset hdf type extra dataset panda hdfdataset filepath bucket data featur big dataset hdf want refer object catalog uri team us want avoid download dataset model weight time run pipelin keep local copi addit copi mirror file",
        "Question_preprocessed_content":"catalog dataset model uri local copi try figur store intermedi pipelin object local particular dataset want refer object catalog uri team us want avoid dataset model weight time run pipelin keep local copi addit copi mirror file",
        "Question_gpt_summary_original":"The user is facing a challenge in storing intermediate Kedro pipeline objects both locally and on S3. They want to refer to the objects in the catalog by their S3 URI to enable their team to use them, but also want to avoid re-downloading the datasets and model weights every time they run a pipeline by keeping a local copy in addition to the S3 copy. The user is seeking guidance on how to mirror files with Kedro.",
        "Question_gpt_summary":"user face challeng store intermedi pipelin object local want refer object catalog uri enabl team us want avoid download dataset model weight time run pipelin keep local copi addit copi user seek guidanc mirror file",
        "Answer_original_content":"good question cacheddataset cach dataset run handl cach dataset memori load multipl time run isn thing persist run gener persist stuff said head think option replic give function us catalog config environ templatedconfigload catalog dataset filepath look like dataset filepath base data raw blah csv set base data bucket blah run product mode local filepath data local decid exactli overriden context method local global yml link document environ variabl us separ environ like local kind separ copi catalog filepath replac local on best bet write persistentcacheddataset similar cacheddataset intercept load save wrap dataset make local copi load time determinist locat look subsequ load",
        "Answer_preprocessed_content":"good question cach dataset run handl cach dataset memori multipl time run isn thing persist run gener persist stuff said head think option replic give function us config environ catalog dataset filepath look like set run product mode local decid exactli overriden method environ variabl us separ environ like separ copi catalog filepath replac local on best bet write similar intercept wrap dataset make local copi load time determinist locat look subsequ load",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Use the same catalog in the same config environment but with the TemplatedConfigLoader where the catalog datasets have their filepaths looking something like: `my_dataset: filepath: ${base_data}\/01_raw\/blah.csv` and set `base_data` to `s3:\/\/bucket\/blah` when running in \"production\" mode and with `local_filepath\/data` locally.\n2. Use separate environments, likely `local`, where you keep a separate copy of your catalog where the filepaths are replaced with local ones.\n3. Write a `PersistentCachedDataSet` similar to `CachedDataSet` which intercepts the loading\/saving for the wrapped dataset and makes a local copy when loading for the first",
        "Answer_gpt_summary":"possibl solut mention discuss us catalog config environ templatedconfigload catalog dataset filepath look like dataset filepath base data raw blah csv set base data bucket blah run product mode local filepath data local us separ environ like local separ copi catalog filepath replac local on write persistentcacheddataset similar cacheddataset intercept load save wrap dataset make local copi load"
    },
    {
        "Question_title":"How to specify HTTP response status code in AML R Web Service",
        "Question_body":"Is there any way to return a custom HTTP status code from R Web Service in Azure ML?\n\nAll the examples of entry scripts in documentation return the response body from the scoring function. In Python Web Service, it is possible to return a HTTP response object with a custom status code. However, R's httr library does not seem to have any function to create response objects directly (only via HTTP method objects such as POST, which call a given URL).\n\nI would like to implement a custom exception handling scheme in R Web Service. Is there any way to return a custom HTTP code from the entry script?\n\nEDIT: Found this idea on the feedback forum, which suggests that the option is not available in Python Web Service either:\nhttps:\/\/feedback.azure.com\/forums\/257792-machine-learning\/suggestions\/40122838-make-http-status-codes-controllable-from-your-scor",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1612354860267,
        "Question_favorite_count":7.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello Lauri,\n\nThanks for the feedback. Yes, we have this product idea in our backlog. I will help to bump up this idea to product group again. ^^\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/257156\/how-to-specify-http-response-status-code-in-aml-r.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1612394092687,
        "Question_original_content":"specifi http respons statu code aml web servic wai return custom http statu code web servic exampl entri script document return respons bodi score function python web servic possibl return http respons object custom statu code httr librari function creat respons object directli http method object post given url like implement custom except handl scheme web servic wai return custom http code entri script edit idea feedback forum suggest option avail python web servic http feedback azur com forum machin learn suggest http statu code control scor",
        "Question_preprocessed_content":"specifi http respons statu code aml web servic wai return custom http statu code web servic exampl entri script document return respons bodi score function python web servic possibl return http respons object custom statu code httr librari function creat respons object directli like implement custom except handl scheme web servic wai return custom http code entri script edit idea feedback forum suggest option avail python web servic",
        "Question_gpt_summary_original":"The user is facing a challenge in specifying a custom HTTP response status code in an R Web Service in Azure ML. While it is possible to return a custom HTTP response object with a status code in Python Web Service, the httr library in R does not have a direct function to create response objects. The user is looking for a way to implement a custom exception handling scheme in R Web Service and is seeking advice on how to return a custom HTTP code from the entry script.",
        "Question_gpt_summary":"user face challeng specifi custom http respons statu code web servic possibl return custom http respons object statu code python web servic httr librari direct function creat respons object user look wai implement custom except handl scheme web servic seek advic return custom http code entri script",
        "Answer_original_content":"hello lauri thank feedback ye product idea backlog help bump idea product group regard yutong",
        "Answer_preprocessed_content":"hello lauri thank feedback ye product idea backlog help bump idea product group regard yutong",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Notebook files have disaperred",
        "Question_body":"Hello,\n\nIn my MLStudio my notebook files window has disappeared so I can not access any of my data (as seen on the image) and I do not know what to do.\n\nPlease your help to solve this as soon as poosible.\n\nThank you.",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1652875560357,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello,\n\nThanks for reaching out to us. Could you please check the access of Storage? https:\/\/docs.microsoft.com\/en-us\/azure\/storage\/blobs\/assign-azure-role-data-access?tabs=portal#assign-an-azure-role\n\nTo access these storage services, you must have at least Storage Blob Data Reader access to the storage account. Only storage account owners can change your access level via the Azure portal.\n\nOr, your admin put the data storage behind V-Net and you can not get access to it- https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-identity-based-data-access#work-with-virtual-networks\nIn this situation, you need to ask permission from your admin.\n\nCould you please share which situation you are in?\n\n\n\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/854288\/notebook-files-have-disaperred.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1652928385600,
        "Question_original_content":"notebook file disap hello mlstudio notebook file window disappear access data seen imag know help solv soon poosibl thank",
        "Question_preprocessed_content":"notebook file disap hello mlstudio notebook file window disappear access data know help solv soon poosibl thank",
        "Question_gpt_summary_original":"The user is facing a challenge where the notebook files window has disappeared in their MLStudio, making it impossible to access any data. They are seeking help to resolve the issue as soon as possible.",
        "Question_gpt_summary":"user face challeng notebook file window disappear mlstudio make imposs access data seek help resolv issu soon possibl",
        "Answer_original_content":"hello thank reach check access storag http doc microsoft com azur storag blob assign azur role data access tab portal assign azur role access storag servic storag blob data reader access storag account storag account owner chang access level azur portal admin data storag net access http doc microsoft com azur machin learn ident base data access work virtual network situat need ask permiss admin share situat regard yutong",
        "Answer_preprocessed_content":"hello thank reach check access storag access storag servic storag blob data reader access storag account storag account owner chang access level azur portal admin data storag access situat need ask permiss admin share situat regard yutong",
        "Answer_gpt_summary_original":"Solutions provided:\n- Check the access of Storage and ensure that the user has at least Storage Blob Data Reader access to the storage account.\n- If the data storage is behind V-Net, the user needs to ask permission from their admin.",
        "Answer_gpt_summary":"solut provid check access storag ensur user storag blob data reader access storag account data storag net user need ask permiss admin"
    },
    {
        "Question_title":"Machine Learning (NLP) on AWS. Cloud9? SageMaker? EC2-AMI?",
        "Question_body":"<p>I have finally arrived in the cloud to put my NLP work to the next level, but I am a bit overwhelmed with all the possibilities I have. So I am coming to you for advice.<\/p>\n\n<p>Currently I see three possibilities:<\/p>\n\n<ul>\n<li><strong>SageMaker<\/strong>\n\n<ul>\n<li>Jupyter Notebooks are great<\/li>\n<li>It's quick and simple<\/li>\n<li>saves a lot of time spent on managing everything, you can very easily get the model into production<\/li>\n<li>costs more<\/li>\n<li>no version control<\/li>\n<\/ul><\/li>\n<li><strong>Cloud9<\/strong><\/li>\n<li><strong>EC2(-AMI)<\/strong><\/li>\n<\/ul>\n\n<p>Well, that's where I am for now. I really like SageMaker, although I don't like the lack of version control (at least I haven't found anything for now).<\/p>\n\n<p>Cloud9 seems just to be an IDE to an EC2 instance.. I haven't found any comparisons of Cloud9 vs SageMaker for Machine Learning. Maybe because Cloud9 is not advertised as an ML solution. But it seems to be an option.<\/p>\n\n<p>What is your take on that question? What have I missed? What would you advise me to go for? What is your workflow and why? <\/p>",
        "Question_answer_count":2,
        "Question_comment_count":3,
        "Question_created_time":1536853194713,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1540233603016,
        "Question_score":1.0,
        "Question_view_count":1442.0,
        "Answer_body":"<blockquote>\n  <p>I am looking for an easy work environment where I can quickly test my models, exactly. And it won't be only me working on it, it's a team effort. <\/p>\n<\/blockquote>\n\n<p>Since you are working as a team I would recommend to use sagemaker with custom docker images. That way you have complete freedom over your algorithm. The docker images are stored in ecr. Here you can upload many versions of the same image and tag them to keep control of the different versions(which you build from a git repo).<\/p>\n\n<p>Sagemaker also gives the execution role to inside the docker image. So you still have full access to other aws resources (if the execution role has the right permissions)<\/p>\n\n<p><a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/scikit_bring_your_own\/scikit_bring_your_own.ipynb<\/a>\nIn my opinion this is a good example to start because it shows how sagemaker is interacting with your image.<\/p>\n\n<p><strong>Some notes on other solutions:<\/strong><\/p>\n\n<p>The problem of every other solution you posted is you want to build and execute on the same machine. Sure you can do this but keep in mind, that gpu instances are expensive and therefore you might only switch to the cloud when the code is ready to run.<\/p>\n\n<p><strong>Some other notes<\/strong><\/p>\n\n<ul>\n<li><p>Jupyter Notebooks in general are not made for collaborative programming. I think they want to change this with jupyter lab but this is still in development and sagemaker only use the notebook at the moment.<\/p><\/li>\n<li><p>EC2 is cheaper as sagemaker but you have  to do more work. Especially if you want to run your model as docker images. Also with sagemaker you can easily  build an endpoint for model inference which would be even more complex to realize with ec2.<\/p><\/li>\n<li><p>Cloud 9 I never used this service and but on first glance it seems good to develop on, but the question remains if you want to do this on a gpu machine. Because you're using ec2 as instance you have the same advantage\/disadvantage.<\/p><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52317237",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1536933417932,
        "Question_original_content":"machin learn nlp aw cloud ami final arriv cloud nlp work level bit overwhelm possibl come advic current possibl jupyt notebook great quick simpl save lot time spent manag easili model product cost version control cloud ami like like lack version control haven cloud id instanc haven comparison cloud machin learn mayb cloud advertis solut option question miss advis workflow",
        "Question_preprocessed_content":"machin learn aw cloud final arriv cloud nlp work level bit overwhelm possibl come advic current possibl jupyt notebook great quick simpl save lot time spent manag easili model product cost version control cloud like like lack version control cloud id haven comparison cloud machin learn mayb cloud advertis solut option question miss advis workflow",
        "Question_gpt_summary_original":"The user is overwhelmed with the possibilities of using machine learning (NLP) on AWS and is seeking advice on which option to choose among SageMaker, Cloud9, and EC2-AMI. They like SageMaker but are concerned about the lack of version control. They are unsure about Cloud9's capabilities for machine learning and are seeking advice on which option to choose.",
        "Question_gpt_summary":"user overwhelm possibl machin learn nlp aw seek advic option choos cloud ami like concern lack version control unsur cloud capabl machin learn seek advic option choos",
        "Answer_original_content":"look easi work environ quickli test model exactli won work team effort work team recommend us custom docker imag wai complet freedom algorithm docker imag store ecr upload version imag tag control differ version build git repo give execut role insid docker imag access aw resourc execut role right permiss http github com awslab amazon exampl blob master advanc function scikit bring scikit bring ipynb opinion good exampl start show interact imag note solut problem solut post want build execut machin sure mind gpu instanc expens switch cloud code readi run note jupyt notebook gener collabor program think want chang jupyt lab develop us notebook moment cheaper work especi want run model docker imag easili build endpoint model infer complex realiz cloud servic glanc good develop question remain want gpu machin instanc advantag disadvantag",
        "Answer_preprocessed_content":"look easi work environ quickli test model exactli won work team effort work team recommend us custom docker imag wai complet freedom algorithm docker imag store ecr upload version imag tag control differ version give execut role insid docker imag access aw resourc opinion good exampl start show interact imag note solut problem solut post want build execut machin sure mind gpu instanc expens switch cloud code readi run note jupyt notebook gener collabor program think want chang jupyt lab develop us notebook moment cheaper work especi want run model docker imag easili build endpoint model infer complex realiz cloud servic glanc good develop question remain want gpu machin instanc",
        "Answer_gpt_summary_original":"Solution: The recommendation is to use SageMaker with custom docker images for complete freedom over the algorithm and version control. Docker images can be stored in ECR, and many versions of the same image can be uploaded and tagged to keep control of different versions. SageMaker also provides the execution role inside the docker image, allowing full access to other AWS resources if the execution role has the right permissions. EC2 is cheaper than SageMaker but requires more work, especially if running the model as docker images. Cloud9 is good for development but may not be suitable for GPU machines. No solution is provided for the lack of version control in SageMaker.",
        "Answer_gpt_summary":"solut recommend us custom docker imag complet freedom algorithm version control docker imag store ecr version imag upload tag control differ version provid execut role insid docker imag allow access aw resourc execut role right permiss cheaper requir work especi run model docker imag cloud good develop suitabl gpu machin solut provid lack version control"
    },
    {
        "Question_title":"Getting `AttributeError : 'NoneType' object has no attribute '_log' `when trying to run test set",
        "Question_body":"<p><strong>Framework: Pytorch<\/strong><br>\n<strong>wandb version : 0.13.3<\/strong><br>\n<strong>workspace: Google colab<\/strong><\/p>\n<pre><code class=\"lang-python\">config = dict(\n    dropout = 0.4,\n    train_batch = 3,\n    val_batch = 1,\n    test_batch = 1,\n    learning_rate = 0.001,\n    epochs = 5,\n    architecture = \"CNN\",\n    model_name = \"efficientnet-b0\",\n    infra = \"Colab\",\n    dataset=\"dysphagia_dataset2\"\n    )\n\n<\/code><\/pre>\n<p>My test function<\/p>\n<pre><code class=\"lang-auto\">def test_model():\n    running_correct = 0.0\n    running_total = 0.0\n    true_labels = []\n    pred_labels = []\n    with torch.no_grad():\n        for data in dataloaders[TEST]:\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            true_labels.append(labels.item())\n            outputs = model_ft(inputs)\n            _, preds = torch.max(outputs.data, 1)\n            pred_labels.append(preds.item())\n            running_total += labels.size(0)\n            running_correct += (preds == labels).sum().item()\n        acc = running_correct\/running_total\n    return (true_labels, pred_labels, running_correct, running_total, acc)\n\n\ntrue_labels, pred_labels, running_correct, running_total, acc = test_model()\n\n<\/code><\/pre>\n<p><strong>Error<\/strong><\/p>\n<pre><code class=\"lang-bash\">AttributeError                            Traceback (most recent call last)\n\n&lt;ipython-input-26-b7dbeaddcbbb&gt; in &lt;module&gt;\n----&gt; 1 true_labels, pred_labels, running_correct, running_total, acc = test_model()\n      2 \n\n4 frames\n\n\/usr\/local\/lib\/python3.7\/dist-packages\/wandb\/wandb_torch.py in log_tensor_stats(self, tensor, name)\n    254             bins = torch.Tensor(bins_np)\n    255 \n--&gt; 256         wandb.run._log(\n    257             {name: wandb.Histogram(np_histogram=(tensor.tolist(), bins.tolist()))},\n    258             commit=False,\n\nAttributeError: 'NoneType' object has no attribute '_log'\n<\/code><\/pre>\n<p>This is how i initialize training:<\/p>\n<pre><code class=\"lang-python\">model_ft = train_model(model_ft, \n                       criterion, \n                       optimizer_ft,\n                       config\n                       )\n<\/code><\/pre>\n<p>my wandb init:<\/p>\n<pre><code class=\"lang-python\">wandb.init(config=config,\n           name='efficientnet0+albumentions',\n           group='pytorch-efficientnet-baseline', \n           project='dysphagia_image_classification',\n           job_type='train')\nconfig = wandb.config\n\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1662745940278,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":753.0,
        "Answer_body":"<p>Finally caught my mistake <img src=\"https:\/\/emoji.discourse-cdn.com\/twitter\/slightly_smiling_face.png?v=12\" title=\":slightly_smiling_face:\" class=\"emoji\" alt=\":slightly_smiling_face:\" loading=\"lazy\" width=\"20\" height=\"20\"><\/p>\n<pre><code class=\"lang-auto\">model_ft._fc = nn.Sequential(\n    nn.BatchNorm1d(num_features=num_ftrs),    \n    nn.Linear(num_ftrs, 512),\n    nn.ReLU(),\n    nn.BatchNorm1d(512),\n    nn.Linear(512, 128),\n    nn.ReLU(),\n    nn.BatchNorm1d(num_features=128),\n    nn.Dropout(p=config.dropout), # Error due to this\n    nn.Linear(128, 2),\n    )\n\nmodel_ft = model_ft.to(device)\n\n<\/code><\/pre>\n<p>I was calling my test function outside of  wandb(only used wandb for training)and wandb must have call <code>.finish<\/code> so, it must have set the my config dict:-&gt; None  as I was passing it to wandb.config.<\/p>\n<p>Now , my model class use one of the config (dropout) but I passed my config file into wandb config so, it set it to None after my model finish training. So, when my def test function use my model, the dropout hyparameter value is None now!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/getting-attributeerror-nonetype-object-has-no-attribute-log-when-trying-to-run-test-set\/3090",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1662752690228,
        "Question_original_content":"get attributeerror nonetyp object attribut log try run test set framework pytorch version workspac googl colab config dict dropout train batch val batch test batch learn rate epoch architectur cnn model efficientnet infra colab dataset dysphagia dataset test function def test model run correct run total true label pred label torch grad data dataload test input label data input input devic label label devic true label append label item output model input pred torch max output data pred label append pred item run total label size run correct pred label sum item acc run correct run total return true label pred label run correct run total acc true label pred label run correct run total acc test model error attributeerror traceback recent true label pred label run correct run total acc test model frame usr local lib python dist packag torch log tensor stat self tensor bin torch tensor bin run log histogram histogram tensor tolist bin tolist commit fals attributeerror nonetyp object attribut log initi train model train model model criterion optim config init init config config efficientnet albument group pytorch efficientnet baselin project dysphagia imag classif job type train config config",
        "Question_preprocessed_content":"get try run test set framework pytorch version workspac googl colab test function error initi train init",
        "Question_gpt_summary_original":"The user is encountering an AttributeError with the message \"'NoneType' object has no attribute '_log'\" when trying to run a test set using Pytorch and wandb version 0.13.3 in a Google Colab workspace. The error occurs when the user tries to log tensor stats using wandb. The user has provided the code for the test function, the initialization of training, and the wandb init.",
        "Question_gpt_summary":"user encount attributeerror messag nonetyp object attribut log try run test set pytorch version googl colab workspac error occur user tri log tensor stat user provid code test function initi train init",
        "Answer_original_content":"final caught mistak model sequenti batchnormd num featur num ftr linear num ftr relu batchnormd linear relu batchnormd num featur dropout config dropout error linear model model devic call test function outsid train finish set config dict pass config model class us config dropout pass config file config set model finish train def test function us model dropout hyparamet valu",
        "Answer_preprocessed_content":"final caught mistak call test function outsid set config pass config model class us config pass config file config set model finish train def test function us model dropout hyparamet valu",
        "Answer_gpt_summary_original":"Solutions provided: The user found the mistake in their code and realized that they were calling the test function outside of wandb, which caused the config dict to be set to None. This caused an error in the model class that used one of the config parameters (dropout). The solution was to call the test function within wandb or to pass the config file directly to the test function.",
        "Answer_gpt_summary":"solut provid user mistak code realiz call test function outsid caus config dict set caus error model class config paramet dropout solut test function pass config file directli test function"
    },
    {
        "Question_title":"What should I do when H2O AutoML returns \"H2OFrame is empty\"?",
        "Question_body":"<p>While training a job on a SageMaker instance using H2o AutoML a message \"This H2OFrame is empty\" has come up after running the code, what should I do to fix the problem?<\/p>\n\n<pre><code>\/opt\/ml\/input\/config\/hyperparameters.json\nAll Parameters:\n{'nfolds': '5', 'training': \"{'classification': 'true', 'target': 'y'}\", 'max_runtime_secs': '3600'}\n\/opt\/ml\/input\/config\/resourceconfig.json\nAll Resources:\n{'current_host': 'algo-1', 'hosts': ['algo-1'], 'network_interface_name': 'eth0'}\nWaiting until DNS resolves: 1\n10.0.182.83\nStarting up H2O-3\nCreating Connection to H2O-3\nAttempt 0: H2O-3 not running yet...\nConnecting to H2O server at http:\/\/127.0.0.1:54321... successful.\n-------------------------- ----------------------------------------\n\n-------------------------- ----------------------------------------\nBeginning Model Training\nParse progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100%\nClassification - If you want to do a regression instead, set \"classification\":\"false\" in \"training\" params, inhyperparamters.json\nConverting specified columns to categorical values:\n[]\nAutoML progress: |\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 100%\nThis H2OFrame is empty.\nException during training: Argument `model` should be a ModelBase, got NoneType None\nTraceback (most recent call last):\nFile \"\/opt\/program\/train\", line 138, in _train_model\nh2o.save_model(aml.leader, path=model_path)\nFile \"\/root\/.local\/lib\/python3.7\/site-packages\/h2o\/h2o.py\", line 969, in save_model\nassert_is_type(model, ModelBase)\nFile \"\/root\/.local\/lib\/python3.7\/site-packages\/h2o\/utils\/typechecks.py\", line 457, in assert_is_type\nskip_frames=skip_frames)\nh2o.exceptions.H2OTypeError: Argument `model` should be a ModelBase, got NoneType None\nH2O session _sid_8aba closed.\n<\/code><\/pre>\n\n<p>I'm wondering if it's a problem because of the max_runtime_secs, my data has around 500 rows and 250000 columns.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_created_time":1568737272183,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1568738644663,
        "Question_score":0.0,
        "Question_view_count":517.0,
        "Answer_body":"<p>thanks @Marcel Mendes Reis for following up on your solution in the comments. I will repost here for others to easily find:<\/p>\n\n<p><em>I realized the issue was due to the max_runtime. When I trained the model with more time I didn't have the problem.<\/em> <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57978333",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1568906188310,
        "Question_original_content":"automl return hofram train job instanc automl messag hofram come run code fix problem opt input config hyperparamet json paramet nfold train classif true target max runtim sec opt input config resourceconfig json resourc current host algo host algo network interfac eth wait dn resolv start creat connect attempt run connect server http success begin model train pars progress classif want regress instead set classif fals train param inhyperparamt json convert specifi column categor valu automl progress hofram except train argument model modelbas got nonetyp traceback recent file opt program train line train model save model aml leader path model path file root local lib python site packag line save model assert type model modelbas file root local lib python site packag util typecheck line assert type skip frame skip frame except hotypeerror argument model modelbas got nonetyp session sid aba close wonder problem max runtim sec data row column",
        "Question_preprocessed_content":"automl return ofram train job instanc automl messag ofram come run code fix problem wonder problem data row column",
        "Question_gpt_summary_original":"The user encountered an error message \"This H2OFrame is empty\" while training a job on a SageMaker instance using H2o AutoML. The error occurred during the model training process and resulted in an exception. The user is unsure if the issue is related to the max_runtime_secs parameter or the size of their data.",
        "Question_gpt_summary":"user encount error messag hofram train job instanc automl error occur model train process result except user unsur issu relat max runtim sec paramet size data",
        "Answer_original_content":"thank marcel mend rei follow solut comment repost easili realiz issu max runtim train model time problem",
        "Answer_preprocessed_content":"thank mend rei follow solut comment repost easili realiz issu train model time problem",
        "Answer_gpt_summary_original":"Solution: The user who encountered the error message \"This H2OFrame is empty\" while training a job on a SageMaker instance using H2o AutoML found that the issue was due to the max_runtime_secs parameter. Increasing the max_runtime_secs parameter resolved the issue.",
        "Answer_gpt_summary":"solut user encount error messag hofram train job instanc automl issu max runtim sec paramet increas max runtim sec paramet resolv issu"
    },
    {
        "Question_title":"How to cancel a running job from the UI?",
        "Question_body":"<p>Am I missing something but how can I cancel a run in my workspace from <a href=\"https:\/\/ms.portal.azure.com\/\" rel=\"nofollow noreferrer\">https:\/\/ms.portal.azure.com\/<\/a> ? The cancel button is always greyed out.<\/p>\n\n<p>I know I can use use the sdk to cancel a run using:<\/p>\n\n<pre><code>run = [ r for r in Experiment(ws, 'myExp').get_runs() if r.id == '899b8314-26b6-458f-9f5c-539ffbf01b91'].pop()\nrun.cancel()\n<\/code><\/pre>\n\n<p>But it would be more convenient to be able to do it from the UI<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1568993659347,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":204.0,
        "Answer_body":"<p>What kind of run is this? Canceling is not currently enabled for pipeline runs in the UI, but is supported for other run types.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1569508172863,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58031370",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1569333175230,
        "Question_original_content":"cancel run job miss cancel run workspac http portal azur com cancel button grei know us us sdk cancel run run experi myexp run ffbfb pop run cancel conveni abl",
        "Question_preprocessed_content":"cancel run job miss cancel run workspac cancel button grei know us us sdk cancel run conveni abl",
        "Question_gpt_summary_original":"The user is facing a challenge in cancelling a running job from the UI in their workspace on https:\/\/ms.portal.azure.com\/. The cancel button is always greyed out, and they have to use the SDK to cancel a run, which is inconvenient.",
        "Question_gpt_summary":"user face challeng cancel run job workspac http portal azur com cancel button grei us sdk cancel run inconveni",
        "Answer_original_content":"kind run cancel current enabl pipelin run support run type",
        "Answer_preprocessed_content":"kind run cancel current enabl pipelin run support run type",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that cancelling pipeline runs from the UI is not currently supported, but cancelling other run types is possible. No other solutions are mentioned.",
        "Answer_gpt_summary":"solut discuss suggest cancel pipelin run current support cancel run type possibl solut mention"
    },
    {
        "Question_title":"How can I print the Canonical String which aws-requests-auth sends?",
        "Question_body":"<p>I want to have a lambda calling a Sagemaker instance in another region. If both are in the same region, everything works fine. If they are not, I get the following error:<\/p>\n\n<pre><code>The request signature we calculated does not match the signature you provided. Check your AWS Secret Access Key and signing method. Consult the service documentation for details.\n\nThe Canonical String for this request should have been\n'POST\n\/endpoints\/foo-endpoint\/invocations\n\nhost:runtime.sagemaker.us-east-1.amazonaws.com\nx-amz-date:20180406T082536Z\n\nhost;x-amz-date\n1234567890foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo'\n\nThe String-to-Sign should have been\n'AWS4-HMAC-SHA256\n20180406T082536Z\n20180406\/us-east-1\/sagemaker\/aws4_request\n987654321abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf'\n<\/code><\/pre>\n\n<p>I use <a href=\"https:\/\/github.com\/DavidMuller\/aws-requests-auth\" rel=\"nofollow noreferrer\"><code>aws-requests-auth<\/code><\/a> (0.4.1) with boto3 (1.5.15 - updating to 1.7.1 didn't change anything, <a href=\"https:\/\/github.com\/boto\/boto3\/blob\/develop\/CHANGELOG.rst\" rel=\"nofollow noreferrer\">changelog<\/a>) like this:<\/p>\n\n<pre><code>import requests\nfrom aws_requests_auth.aws_auth import AWSRequestsAuth\nauth = AWSRequestsAuth(aws_access_key=config['AWS']['ACCESS_KEY'],\n                       aws_secret_access_key=(\n                           config['AWS']['SECRET_ACCESS_KEY']),\n                       aws_host=config['AWS']['HOST'],\n                       aws_region=config['AWS']['REGION'],\n                       aws_service=config['AWS']['SERVICE'])\n\npayload = {'foo': 'bar'}\nresponse = requests.post(post_url,\n                         data=json.dumps(payload),\n                         headers={'content-type': 'application\/json'},\n                         auth=auth)\n<\/code><\/pre>\n\n<p>printing <code>auth<\/code> only gives <code>&lt;aws_requests_auth.aws_auth.AWSRequestsAuth object at 0x7f9d00c98390&gt;<\/code>.<\/p>\n\n<p>Is there a way to print the \"Canonical String\" mentioned in the error message?<\/p>\n\n<p>(Any other ideas how to fix this are appreciated as well)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1523005580303,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1531211963023,
        "Question_score":3.0,
        "Question_view_count":827.0,
        "Answer_body":"<p>A work-around for the asked question:<\/p>\n\n<pre><code>req = requests.request('POST', 'http:\/\/httpbin.org\/get')\nreq.body = b''\nreq.method = ''\nprint(auth.get_aws_request_headers(req,\n                                   aws_access_key=auth.aws_access_key,\n                                   aws_secret_access_key=auth.aws_secret_access_key,\n                                   aws_token=auth.aws_token))\n<\/code><\/pre>\n\n<p>The problem is not solved, though. And now I wonder what the first argument of <code>auth.get_aws_request_headers<\/code> is.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49689216",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1523012141480,
        "Question_original_content":"print canon string aw request auth send want lambda call instanc region region work fine follow error request signatur calcul match signatur provid check aw secret access kei sign method consult servic document detail canon string request post endpoint foo endpoint invoc host runtim east amazonaw com amz date host amz date foobarfoobarfoobarboofoobarfoobarfoobarfoobarfoobarfoo string sign aw hmac sha east aw request abcdeffoobarfoobarfoobarfoobarfoobarfoobarfoobarfoobarf us aw request auth boto updat chang changelog like import request aw request auth aw auth import awsrequestsauth auth awsrequestsauth aw access kei config aw access kei aw secret access kei config aw secret access kei aw host config aw host aw region config aw region aw servic config aw servic payload foo bar respons request post post url data json dump payload header content type applic json auth auth print auth give wai print canon string mention error messag idea fix appreci",
        "Question_preprocessed_content":"print canon string send want lambda call instanc region region work fine follow error us boto like print give wai print canon string mention error messag idea fix appreci",
        "Question_gpt_summary_original":"The user is encountering an error when trying to call a Sagemaker instance in another region using a lambda. The error message indicates that the request signature does not match the signature provided and provides a Canonical String and String-to-Sign that should have been used. The user is using aws-requests-auth and boto3 to make the request and is looking for a way to print the Canonical String mentioned in the error message.",
        "Question_gpt_summary":"user encount error try instanc region lambda error messag indic request signatur match signatur provid provid canon string string sign user aw request auth boto request look wai print canon string mention error messag",
        "Answer_original_content":"work ask question req request request post http httpbin org req bodi req method print auth aw request header req aw access kei auth aw access kei aw secret access kei auth aw secret access kei aw token auth aw token problem solv wonder argument auth aw request header",
        "Answer_preprocessed_content":"ask question problem solv wonder argument",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion.",
        "Answer_gpt_summary":"solut solut provid discuss"
    },
    {
        "Question_title":"Grant Azure Machine Learning access to SQL Server on Virtual Machine with ACL",
        "Question_body":"<p>We are in the process of setting up Azure Machine Learning within our Azure instance. Our SQL Server sits on a virtual machine and access is restricted using ACL's  <\/p>\n\n<p>We have looked extensively for a virtual IP or an IP within Machine Learning to add to the ACL but we cannot find it. <\/p>\n\n<p>We have tested access by entering 0.0.0.0\/0 to our ACL which allows access to ML  but obviously this isnt secure and not something that we wish to continue with. <\/p>\n\n<p>Thanks in advance. <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1427387261240,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":229.0,
        "Answer_body":"<p>Azure public IP address are published and refreshed at regular intervals it can be found here: <a href=\"http:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=41653\" rel=\"nofollow\">http:\/\/www.microsoft.com\/en-us\/download\/details.aspx?id=41653<\/a><\/p>\n\n<p>You can use these to specify the restricted IP range for access<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/29283841",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1427753688880,
        "Question_original_content":"grant access sql server virtual machin acl process set azur instanc sql server sit virtual machin access restrict acl look extens virtual machin learn add acl test access enter acl allow access obvious isnt secur wish continu thank advanc",
        "Question_preprocessed_content":"grant access sql server virtual machin acl process set azur instanc sql server sit virtual machin access restrict acl look extens virtual machin learn add acl test access enter acl allow access obvious isnt secur wish continu thank advanc",
        "Question_gpt_summary_original":"The user is facing challenges in granting Azure Machine Learning access to their SQL Server on a virtual machine due to restricted access using ACLs. They have been unable to find a virtual IP or an IP within Machine Learning to add to the ACL and have resorted to using 0.0.0.0\/0, which is not secure and not a preferred solution.",
        "Question_gpt_summary":"user face challeng grant access sql server virtual machin restrict access acl unabl virtual machin learn add acl resort secur prefer solut",
        "Answer_original_content":"azur public address publish refresh regular interv http microsoft com download detail aspx us specifi restrict rang access",
        "Answer_preprocessed_content":"azur public address publish refresh regular interv us specifi restrict rang access",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using Azure public IP addresses, which are published and refreshed at regular intervals, to specify the restricted IP range for access to the SQL Server on a virtual machine. This solution can help avoid using 0.0.0.0\/0, which is not secure and not a preferred solution.",
        "Answer_gpt_summary":"solut discuss suggest azur public address publish refresh regular interv specifi restrict rang access sql server virtual machin solut help avoid secur prefer solut"
    },
    {
        "Question_title":"VBA And Azure Machine Learning Excel Add In",
        "Question_body":"Hi! I wanted to see if VBA and Azure Machine Learning Excel Add In can be connected to each other. Are there any way to code VBA (use VBA) for controlling or altering Azure Machine Learning Excel Add In? I have used Azure Machine Learning to rate candidate feedback as negative or positive, but it has like a 75 -80% success rate - there are still a good chunk of comments that are rated wrong. However, it is still an amazing tool that I want to use v- I was just wondering if I can increase the accuracy of it somehow by creating a VBA code that connects it to Azure Machine Learning where I can add words related to negative responses or vice versa for positive response to increase the accuracy.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1610149745593,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, we currently don't support VBA and Azure ML Excel add-in integration. You'll need to apply ML techniques for improving your model and re-deploy your model.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/224491\/vba-and-azure-machine-learning-excel-add-in.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1610161769647,
        "Question_original_content":"vba excel add want vba excel add connect wai code vba us vba control alter excel add rate candid feedback neg posit like success rate good chunk comment rate wrong amaz tool want us wonder increas accuraci creat vba code connect add word relat neg respons vice versa posit respons increas accuraci",
        "Question_preprocessed_content":"vba excel add want vba excel add connect wai code vba control alter excel add rate candid feedback neg posit like success rate good chunk comment rate wrong amaz tool want us wonder increas accuraci creat vba code connect add word relat neg respons vice versa posit respons increas accuraci",
        "Question_gpt_summary_original":"The user is facing challenges with the accuracy of Azure Machine Learning Excel Add In in rating candidate feedback as negative or positive, with a success rate of 75-80%. They are wondering if VBA can be used to control or alter the Add In to increase its accuracy by adding words related to negative or positive responses.",
        "Question_gpt_summary":"user face challeng accuraci excel add rate candid feedback neg posit success rate wonder vba control alter add increas accuraci ad word relat neg posit respons",
        "Answer_original_content":"current support vba excel add integr need appli techniqu improv model deploi model",
        "Answer_preprocessed_content":"current support vba excel integr need appli techniqu improv model model",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Vertex AI Managed Notebook, get subnet\/IP",
        "Question_body":"<p>How can I find IP for vertex AI managed notebook instance? The service is differing from user managed notebooks in certain sense. The creation of an instance doesn't create a compute instance, so it's all managed by itself.<\/p>\n<p>My purpose is to whitelist the set of IPs in Mongo atlas. Set of IPs being of all the notebooks in that region. I'm using google-managed networks in this case.<\/p>\n<p>I've a few doubts here:<\/p>\n<ul>\n<li>Since within managed nb, I can change CPU consumption, will this reinstantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs?<\/li>\n<li>Is it possible to add a custom init script?<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1635836251457,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1636016613528,
        "Question_score":1.0,
        "Question_view_count":665.0,
        "Answer_body":"<p>If you want to connect to a database service on GCP, create a network (or use the default) and instantiate the notebook using this network (<code>Advanced options<\/code>) and create the white list for this entire network . It's required because the managed notebook creates a peering network on the network you will use, you can check you in <code>VPC Network<\/code> \u279e <code>VPC Network Peering<\/code>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Pd1ui.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>If you want an external IP, it will not work. Google managed notebooks <strong>does not use external ips<\/strong>, they basically access the internet via NAT gateways (does not matter if you use google or own managed networks) so you will not be able to do what you want. Move for user managed notebooks (where you can assign a fixed external ip) or white list any IP on your Mongo db service if you are not in a production environment.<\/p>\n<p>About yous doubts:<\/p>\n<blockquote>\n<p>Since within managed nb, I can change CPU consumption, will this instantiate a new cluster, with entirely new IP, or it will be 1 from among a group of IPs<\/p>\n<\/blockquote>\n<p>For the internal network it may change when you restart or recreate the notebook instance. For an external network, it does not exists and explained.<\/p>\n<blockquote>\n<p>Is it possible to add a custom init script?<\/p>\n<\/blockquote>\n<p>Basically not. But you can provide custom docker images for the notebook.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1635872119870,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69806432",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1635870534303,
        "Question_original_content":"manag notebook subnet manag notebook instanc servic differ user manag notebook certain sens creation instanc creat comput instanc manag purpos whitelist set ip mongo atla set ip notebook region googl manag network case doubt manag chang cpu consumpt reinstanti new cluster entir new group ip possibl add custom init script",
        "Question_preprocessed_content":"manag notebook manag notebook instanc servic differ user manag notebook certain sens creation instanc creat comput instanc manag purpos whitelist set ip mongo atla set ip notebook region network case doubt manag chang cpu consumpt reinstanti new cluster entir new group ip possibl add custom init script",
        "Question_gpt_summary_original":"The user is facing challenges in finding the IP for their Vertex AI managed notebook instance, which is different from user-managed notebooks. They need to whitelist a set of IPs in Mongo Atlas for all the notebooks in that region, but they have doubts about whether changing CPU consumption will reinstantiate a new cluster with a new IP or if it will be one from a group of IPs. They also want to know if it's possible to add a custom init script.",
        "Question_gpt_summary":"user face challeng find manag notebook instanc differ user manag notebook need whitelist set ip mongo atla notebook region doubt chang cpu consumpt reinstanti new cluster new group ip want know possibl add custom init script",
        "Answer_original_content":"want connect databas servic gcp creat network us default instanti notebook network advanc option creat white list entir network requir manag notebook creat peer network network us check vpc network vpc network peer want extern work googl manag notebook us extern ip basic access internet nat gatewai matter us googl manag network abl want user manag notebook assign fix extern white list mongo servic product environ you doubt manag chang cpu consumpt instanti new cluster entir new group ip intern network chang restart recreat notebook instanc extern network exist explain possibl add custom init script basic provid custom docker imag notebook",
        "Answer_preprocessed_content":"want connect databas servic gcp creat network instanti notebook network creat white list entir network requir manag notebook creat peer network network us check want extern work googl manag notebook us extern ip basic access internet nat gatewai abl want user manag notebook white list mongo servic product environ you doubt manag chang cpu consumpt instanti new cluster entir new group ip intern network chang restart recreat notebook instanc extern network exist explain possibl add custom init script basic provid custom docker imag notebook",
        "Answer_gpt_summary_original":"Solutions provided:\n- Create a network (or use the default) and instantiate the notebook using this network to create a white list for the entire network.\n- For an external network, it is not possible to assign a fixed external IP, so the user can either move to user-managed notebooks or whitelist any IP on their Mongo db service if they are not in a production environment.\n- Changing CPU consumption may change the internal network IP when restarting or recreating the notebook instance, but for an external network, it does not exist.\n- It is not possible to add a custom init script, but the user can provide custom docker images for the notebook.",
        "Answer_gpt_summary":"solut provid creat network us default instanti notebook network creat white list entir network extern network possibl assign fix extern user user manag notebook whitelist mongo servic product environ chang cpu consumpt chang intern network restart recreat notebook instanc extern network exist possibl add custom init script user provid custom docker imag notebook"
    },
    {
        "Question_title":"AWS Sagemaker does not update the package",
        "Question_body":"<p>AWS Sagemaker's notebook comes with Scikit-Learn version 0.19.1<\/p>\n\n<p>I would like to use version 0.20.2. To avoid updating it every time in the notebook code, I tried using the lifecycle configurations. I created one with the following code :<\/p>\n\n<pre><code>#!\/bin\/bash\nset -e\n\/home\/ec2-user\/anaconda3\/bin\/conda install scikit-learn -y\n<\/code><\/pre>\n\n<p>When I run the attached notebook instance and go to the terminal, the version of scikit-learn found with <code>conda list<\/code> is correct (0.20.2). But when I run a notebook and import sklearn, the version is still 0.19.2.<\/p>\n\n<pre><code>import sklearn\nprint(sklearn.__version__)\n<\/code><\/pre>\n\n<p>Is there any virtual environment on the SageMaker instances where I should install the package ? How can I fix my notebook lifecycle configuration ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1547478776530,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":1546.0,
        "Answer_body":"<p>Your conda update does not refer to a specific virtualenv, while your notebook probably does. Therefore you dont see an update on the notebook virtualenv.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54184145",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1547708377156,
        "Question_original_content":"updat packag notebook come scikit learn version like us version avoid updat time notebook code tri lifecycl configur creat follow code bin bash set home user anaconda bin conda instal scikit learn run attach notebook instanc termin version scikit learn conda list correct run notebook import sklearn version import sklearn print sklearn version virtual environ instanc instal packag fix notebook lifecycl configur",
        "Question_preprocessed_content":"updat packag notebook come version like us version avoid updat time notebook code tri lifecycl configur creat follow code run attach notebook instanc termin version correct run notebook import sklearn version virtual environ instanc instal packag fix notebook lifecycl configur",
        "Question_gpt_summary_original":"The user is facing a challenge with AWS Sagemaker's notebook as it does not update the package. The user tried to use lifecycle configurations to update Scikit-Learn version but it did not work. Although the version of Scikit-Learn found with conda list is correct, the version imported in the notebook is still the old one. The user is seeking help to fix the notebook lifecycle configuration and wondering if there is any virtual environment on the SageMaker instances where the package should be installed.",
        "Question_gpt_summary":"user face challeng notebook updat packag user tri us lifecycl configur updat scikit learn version work version scikit learn conda list correct version import notebook old user seek help fix notebook lifecycl configur wonder virtual environ instanc packag instal",
        "Answer_original_content":"conda updat refer specif virtualenv notebook probabl dont updat notebook virtualenv",
        "Answer_preprocessed_content":"conda updat refer specif virtualenv notebook probabl dont updat notebook virtualenv",
        "Answer_gpt_summary_original":"Solution: The user should specify the virtual environment in the conda update command to ensure that the package is updated in the correct environment.",
        "Answer_gpt_summary":"solut user specifi virtual environ conda updat command ensur packag updat correct environ"
    },
    {
        "Question_title":"upload data to S3 with sagemaker",
        "Question_body":"<p>I have a problem with SageMaker when I try to upload Data into S3 bucket . I get this error : <\/p>\n\n<blockquote>\n  <hr>\n\n<pre><code>NameError                                 Traceback (most recent call last)\n&lt;ipython-input-26-d21b1cb0fcab&gt; in &lt;module&gt;()\n     19 download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\n     20 \n---&gt; 21 upload_to_s3('train', 'caltech-256-60-train.rec')\n\n&lt;ipython-input-26-d21b1cb0fcab&gt; in upload_to_s3(channel, file)\n     13     data = open(file, \"rb\")\n     14     key = channel + '\/' + file\n---&gt; 15     s3.Bucket(bucket).put_object(Key=key, Body=data)\n     16 \n     17 \n\nNameError: name 'bucket' is not defined\n<\/code><\/pre>\n<\/blockquote>\n\n<p>Here is the script:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import os\nimport urllib.request\nimport boto3\n\ndef download(url):\n    filename = url.split(\"\/\")[-1]\n    if not os.path.exists(filename):\n        urllib.request.urlretrieve(url, filename)\n\n\ndef upload_to_s3(channel, file):\n    s3 = boto3.resource('s3')\n    data = open(file, \"rb\")\n    key = channel + '\/' + file\n    s3.Bucket(bucket).put_object(Key=key, Body=data)\n\n\n# caltech-256 download('http:\/\/data.mxnet.io\/data\/caltech-256\/caltech-256-60-train.rec')\n\nupload_to_s3('train', 'caltech-256-60-train.rec')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1524475888687,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1544136807192,
        "Question_score":5.0,
        "Question_view_count":9575.0,
        "Answer_body":"<p>It is exactly as the error say, the variable <code>bucket<\/code> is not defined. \nyou might want to do something like <\/p>\n\n<pre><code>bucket = &lt;name of already created bucket in s3&gt;\n<\/code><\/pre>\n\n<p>before you call <\/p>\n\n<pre><code>s3.Bucket(bucket).put_object(Key=key, Body=data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49977679",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1524500722232,
        "Question_original_content":"upload data problem try upload data bucket error nameerror traceback recent download http data mxnet data caltech caltech train rec upload train caltech train rec upload channel file data open file kei channel file bucket bucket object kei kei bodi data nameerror bucket defin script import import urllib request import boto def download url filenam url split path exist filenam urllib request urlretriev url filenam def upload channel file boto resourc data open file kei channel file bucket bucket object kei kei bodi data caltech download http data mxnet data caltech caltech train rec upload train caltech train rec",
        "Question_preprocessed_content":"upload data problem try upload data bucket error script",
        "Question_gpt_summary_original":"The user is encountering an error when trying to upload data into an S3 bucket using SageMaker. The error message indicates that the name 'bucket' is not defined. The user has provided a script that includes functions for downloading and uploading data, but it seems that the bucket name is missing or not properly defined.",
        "Question_gpt_summary":"user encount error try upload data bucket error messag indic bucket defin user provid script includ function download upload data bucket miss properli defin",
        "Answer_original_content":"exactli error variabl bucket defin want like bucket bucket bucket object kei kei bodi data",
        "Answer_preprocessed_content":"exactli error variabl defin want like",
        "Answer_gpt_summary_original":"Solution: The user needs to define the variable 'bucket' with the name of the already created S3 bucket before calling the function to upload data.",
        "Answer_gpt_summary":"solut user need defin variabl bucket creat bucket call function upload data"
    },
    {
        "Question_title":"AWS Sagemaker AttributeError: can't set attribute error",
        "Question_body":"<p>I am new to python programming. Following the AWS learning path:<\/p>\n<p><a href=\"https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult\" rel=\"nofollow noreferrer\">https:\/\/aws.amazon.com\/getting-started\/hands-on\/build-train-deploy-machine-learning-model-sagemaker\/?trk=el_a134p000003yWILAA2&amp;trkCampaign=DS_SageMaker_Tutorial&amp;sc_channel=el&amp;sc_campaign=Data_Scientist_Hands-on_Tutorial&amp;sc_outcome=Product_Marketing&amp;sc_geo=mult<\/a><\/p>\n<p>I am getting an error when excuting the following block (in conda_python3):<\/p>\n<pre><code>test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\nxgb_predictor.content_type = 'text\/csv' # set the data type for an inference\nxgb_predictor.serializer = csv_serializer # set the serializer type\npredictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\npredictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an \narray\nprint(predictions_array.shape)\n<\/code><\/pre>\n<blockquote>\n<p>AttributeError                            Traceback (most recent call last)\n in \n1 test_data_array = test_data.drop(['y_no', 'y_yes'], axis=1).values #load the data into an array\n----&gt; 2 xgb_predictor.content_type = 'text\/csv' # set the data type for an inference\n3 xgb_predictor.serializer = csv_serializer # set the serializer type\n4 predictions = xgb_predictor.predict(test_data_array).decode('utf-8') # predict!\n5 predictions_array = np.fromstring(predictions[1:], sep=',') # and turn the prediction into an array<\/p>\n<\/blockquote>\n<blockquote>\n<p>AttributeError: can't set attribute<\/p>\n<\/blockquote>\n<p>I have looked at several prior questions but couldn't find much information related to this error when it comes to creating data types.<\/p>\n<p>Thanks in advance for any help.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1609067497070,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":2479.0,
        "Answer_body":"<p>If you just remove it then the prediction will work. Therefore, recommend removing this code line.\nxgb_predictor.content_type = 'text\/csv'<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":8.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65465114",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1609399068572,
        "Question_original_content":"attributeerror set attribut error new python program follow aw learn path http aw amazon com get start hand build train deploi machin learn model trk apywilaa trkcampaign tutori channel campaign data scientist hand tutori outcom product market geo mult get error excut follow block conda python test data arrai test data drop ye axi valu load data arrai xgb predictor content type text csv set data type infer xgb predictor serial csv serial set serial type predict xgb predictor predict test data arrai decod utf predict predict arrai fromstr predict sep turn predict arrai print predict arrai shape attributeerror traceback recent test data arrai test data drop ye axi valu load data arrai xgb predictor content type text csv set data type infer xgb predictor serial csv serial set serial type predict xgb predictor predict test data arrai decod utf predict predict arrai fromstr predict sep turn predict arrai attributeerror set attribut look prior question couldn inform relat error come creat data type thank advanc help",
        "Question_preprocessed_content":"attributeerror set attribut error new python program follow aw learn path get error excut follow block attributeerror traceback load data arrai set data type infer set serial type predict predict sep turn predict arrai attributeerror set attribut look prior question couldn inform relat error come creat data type thank advanc help",
        "Question_gpt_summary_original":"The user encountered an AttributeError while executing a block of code in AWS Sagemaker. The error occurred when trying to set the data type for an inference, and the user has not been able to find much information related to this error when creating data types.",
        "Question_gpt_summary":"user encount attributeerror execut block code error occur try set data type infer user abl inform relat error creat data type",
        "Answer_original_content":"remov predict work recommend remov code line xgb predictor content type text csv",
        "Answer_preprocessed_content":"remov predict work recommend remov code line",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to remove the code line that sets the data type for the inference, as it is causing the AttributeError.",
        "Answer_gpt_summary":"solut solut suggest discuss remov code line set data type infer caus attributeerror"
    },
    {
        "Question_title":"How to verify all the dependencies are installed in sagemaker?",
        "Question_body":"<p>I am creating an sagemaker endpoint and loading a pretrained model from an s3 bucket. the model -&gt; model.tar.gz file has directory structure as documented here, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure<\/a><\/p>\n<pre><code>model.tar.gz\/\n|- model.pth\n|- code\/\n  |- inference.py\n  |- requirements.txt  # only for versions 1.3.1 and higher\n<\/code><\/pre>\n<p>I have put few dependencies in requirements.txt, is there a way to verify that all the dependencies were installed correctly?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1645671696107,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":107.0,
        "Answer_body":"<p>It is not possible to get access or SSH into the machine that's running your deployment. So, one way is to assert the versions of your dependencies in the <code>model_fn<\/code> inside &quot;inference.py&quot; something like below.<\/p>\n<p>if your requirements.txt looks like this:<\/p>\n<pre><code>numpy==1.20.3\npandas==1.3.4\n<\/code><\/pre>\n<p>get the versions and assert them in `model_fn like below:<\/p>\n<pre><code>import os\n\n### your other code ###\n\ndef model_fn(model_dir):\n    # assuming you have numpy and pandas\n    assert os.popen(&quot;python3 -m pip freeze | grep -E 'numpy|pandas'&quot;).read() == 'numpy==1.20.3\\npandas==1.3.4\\n'\n    ### your other code ###\n    return xxxx\n\n### your other code ###\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1645695474836,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71246559",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1645694718683,
        "Question_original_content":"verifi depend instal creat endpoint load pretrain model bucket model model tar file directori structur document http readthedoc stabl framework pytorch pytorch html model directori structur model tar model pth code infer requir txt version higher depend requir txt wai verifi depend instal correctli",
        "Question_preprocessed_content":"verifi depend instal creat endpoint load pretrain model bucket model file directori structur document depend wai verifi depend instal correctli",
        "Question_gpt_summary_original":"The user is facing a challenge in verifying whether all the dependencies in the requirements.txt file were installed correctly while creating an sagemaker endpoint and loading a pretrained model from an s3 bucket.",
        "Question_gpt_summary":"user face challeng verifi depend requir txt file instal correctli creat endpoint load pretrain model bucket",
        "Answer_original_content":"possibl access ssh machin run deploy wai assert version depend model insid infer like requir txt look like numpi panda version assert model like import code def model model dir assum numpi panda assert popen python pip freez grep numpi panda read numpi npanda code return code",
        "Answer_preprocessed_content":"possibl access ssh machin run deploy wai assert version depend insid like look like version assert like",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to assert the versions of the dependencies in the `model_fn` inside \"inference.py\" by getting the versions from the requirements.txt file and comparing them with the installed versions using the `os.popen()` command.",
        "Answer_gpt_summary":"solut possibl solut mention discuss assert version depend model insid infer get version requir txt file compar instal version popen command"
    },
    {
        "Question_title":"Azure ML Enpoint deployment failed EAST US region",
        "Question_body":"I have an Azure ML Real-time inference endpoint deployed ran for a month till yesterday. Today it is in the state of \"Failed\".\n\nI did create a new compute and did a new deployment in the same region EAST US and it failed again.\n\nWhat's going? Is this just a problem for me or a general issue?\n\nThanks\n-Dali",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1594945530060,
        "Question_favorite_count":44.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. I successfully deployed in the east us region. Please review the following troubleshooting guidelines. Also check for any service\/resource health issues that could be impacting your service. Let me know if you're still experiencing issues afterwards and please share the logs so we can investigate further. Thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/48609\/azure-ml-enpoint-deployment-failed-east-us-region.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1594953029752,
        "Question_original_content":"enpoint deploy fail east region real time infer endpoint deploi ran month till yesterdai todai state fail creat new comput new deploy region east fail go problem gener issu thank dali",
        "Question_preprocessed_content":"enpoint deploy fail east region infer endpoint deploi ran month till yesterdai todai state fail creat new comput new deploy region east fail go problem gener issu thank dali",
        "Question_gpt_summary_original":"The user's Azure ML Real-time inference endpoint deployment in the EAST US region has failed after running successfully for a month. The user attempted to create a new compute and deploy again in the same region, but it failed again. The user is unsure if this is a general issue or specific to their account.",
        "Question_gpt_summary":"user real time infer endpoint deploy east region fail run successfulli month user attempt creat new comput deploi region fail user unsur gener issu specif account",
        "Answer_original_content":"thank reach successfulli deploi east region review follow troubleshoot guidelin check servic resourc health issu impact servic let know experienc issu share log investig thank",
        "Answer_preprocessed_content":"thank reach successfulli deploi east region review follow troubleshoot guidelin check health issu impact servic let know experienc issu share log investig thank",
        "Answer_gpt_summary_original":"Solutions provided:\n- Review troubleshooting guidelines\n- Check for any service\/resource health issues that could be impacting the service\n- Share logs for further investigation\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid review troubleshoot guidelin check servic resourc health issu impact servic share log investig person opinion bias includ respons"
    },
    {
        "Question_title":"How to register model from the Azure ML Pipeline Script step",
        "Question_body":"<p>I am running the <code>pipeline.submit()<\/code> in AzureML, which has a <code>PythonScriptStep<\/code>.\nInside this step, I download a model from tensorflow-hub, retrain it and save it as a <code>.zip<\/code>, and finally, I would like to register it in the Azure ML.\nBut as inside the script I do not have a workspace, <code>Model.register()<\/code> is not the case.\nSo I am trying to use <code>Run.register_model()<\/code> method as below:<\/p>\n\n<pre><code>os.replace(os.path.join('.', archive_name + '.zip'), \n           os.path.join('.', 'outputs', archive_name + '.zip'))\n\nprint(os.listdir('.\/outputs'))\nprint('========================')\n\nrun_context = Run.get_context()\nfinetuning_model = run_context.register_model(model_name='finetuning_similarity_model',\n                                              model_path=os.path.join(archive_name+'.zip'),\n                                              tags={},\n                                              description=\"Finetuning Similarity model\")\n<\/code><\/pre>\n\n<p>But then I have got an error:<\/p>\n\n<blockquote>\n  <p>ErrorResponse \n  {\n      \"error\": {\n          \"message\": \"Could not locate the provided model_path retrained.zip in the set of files uploaded to the run:<\/p>\n<\/blockquote>\n\n<p>despite I have the retrained <code>.zip<\/code> in the <code>.\/outputs<\/code> dir as we can see from the log:<\/p>\n\n<pre><code>['retrained.zip']\n========================\n<\/code><\/pre>\n\n<p>I guess that I am doing something wrong?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1574164584153,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":1578744224352,
        "Question_score":7.0,
        "Question_view_count":3429.0,
        "Answer_body":"<p>I was able to fix the same issue (<a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.exceptions.modelpathnotfoundexception?view=azure-ml-py\" rel=\"noreferrer\"><code>ModelPathNotFoundException<\/code><\/a>) by explicitly uploading the model into the run history record before trying to register the model:<\/p>\n\n<pre><code>run.upload_file(\"outputs\/my_model.pickle\", \"outputs\/my_model.pickle\")\n<\/code><\/pre>\n\n<p>Which I found surprising because this wasn't mentioned in many of the official examples and according to the <code>upload_file()<\/code> <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.run.run?view=azure-ml-py#upload-file-name--path-or-stream-\" rel=\"noreferrer\">documentation<\/a>:<\/p>\n\n<blockquote>\n  <p>Runs automatically capture file in the specified output directory, which defaults to \".\/outputs\" for most run types. Use upload_file only when additional files need to be uploaded or an output directory is not specified.<\/p>\n<\/blockquote>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1578746319987,
        "Answer_score":14.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58933565",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1578745983587,
        "Question_original_content":"regist model pipelin script step run pipelin submit pythonscriptstep insid step download model tensorflow hub retrain save zip final like regist insid script workspac model regist case try us run regist model method replac path join archiv zip path join output archiv zip print listdir output print run context run context finetun model run context regist model model finetun similar model model path path join archiv zip tag descript finetun similar model got error errorrespons error messag locat provid model path retrain zip set file upload run despit retrain zip output dir log retrain zip guess wrong",
        "Question_preprocessed_content":"regist model pipelin script step run insid step download model retrain save final like regist insid script workspac case try us method got error errorrespons error messag locat provid set file upload run despit retrain dir log guess wrong",
        "Question_gpt_summary_original":"The user is encountering a challenge in registering a model in Azure ML. They are using the Run.register_model() method as they do not have a workspace inside the script. However, they are getting an error message stating that the provided model_path cannot be located despite having the retrained .zip file in the .\/outputs directory. The user is unsure of what they are doing wrong.",
        "Question_gpt_summary":"user encount challeng regist model run regist model method workspac insid script get error messag state provid model path locat despit have retrain zip file output directori user unsur wrong",
        "Answer_original_content":"abl fix issu modelpathnotfoundexcept explicitli upload model run histori record try regist model run upload file output model pickl output model pickl surpris wasn mention offici exampl accord upload file document run automat captur file specifi output directori default output run type us upload file addit file need upload output directori specifi",
        "Answer_preprocessed_content":"abl fix issu explicitli upload model run histori record try regist model surpris wasn mention offici exampl accord document run automat captur file specifi output directori default run type us addit file need upload output directori specifi",
        "Answer_gpt_summary_original":"Solution: The user can explicitly upload the model into the run history record before trying to register the model using the `run.upload_file()` method. This solution was suggested by another user who faced the same issue. Although this solution was not mentioned in many official examples, it worked for the user who suggested it.",
        "Answer_gpt_summary":"solut user explicitli upload model run histori record try regist model run upload file method solut suggest user face issu solut mention offici exampl work user suggest"
    },
    {
        "Question_title":"Vertex Ai issue when deploying a model using Java",
        "Question_body":"<p>This is what I use to deploy an Auto-ML model:<\/p>\n<pre><code>            MachineSpec machineSpec = MachineSpec.newBuilder().setMachineType(&quot;n1-standard-2&quot;).build();\n            DedicatedResources dedicatedResources =\n                    DedicatedResources.newBuilder().setMinReplicaCount(1).setMachineSpec(machineSpec).build();            \n            String model = ModelName.of(project, location, modelId).toString();\n            DeployedModel deployedModel =\n                    DeployedModel.newBuilder()\n                            .setModel(model)\n                            .setDisplayName(deployedModelDisplayName)\n                            .setDedicatedResources(dedicatedResources)\n                            .build();\n            Map&lt;String, Integer&gt; trafficSplit = new HashMap&lt;&gt;();\n            trafficSplit.put(&quot;0&quot;, 100);\n            EndpointName endpoint = EndpointName.of(project, location, endpointId);\n            OperationFuture&lt;DeployModelResponse, DeployModelOperationMetadata&gt; response =\n                    client.deployModelAsync(endpoint, deployedModel, trafficSplit);\n            response.getInitialFuture().get().getName());\n<\/code><\/pre>\n<p>The error appears when I hit this line <code>response.getInitialFuture().get().getName());<\/code><\/p>\n<p>Here is the error:\n<code>INVALID_ARGUMENT: 'dedicated_resources' is not supported for Model projects\/***\/locations\/us-central1\/models\/***<\/code><\/p>\n<p>I can deploy the model using cloud console but not programmatically using java 8. It is a new model and the endpoint is also new without any assigned model to it.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":8,
        "Question_created_time":1640406502527,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":244.0,
        "Answer_body":"<p>I am sorry everyone, I was implementing the wrong section of the documentation. I had to follow AutoML image, not Custom-trained one.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/a4xPR.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70477987",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1640820603630,
        "Question_original_content":"issu deploi model java us deploi auto model machinespec machinespec machinespec newbuild setmachinetyp standard build dedicatedresourc dedicatedresourc dedicatedresourc newbuild setminreplicacount setmachinespec machinespec build string model modelnam project locat modelid tostr deployedmodel deployedmodel deployedmodel newbuild setmodel model setdisplaynam deployedmodeldisplaynam setdedicatedresourc dedicatedresourc build map trafficsplit new hashmap trafficsplit endpointnam endpoint endpointnam project locat endpointid operationfutur respons client deploymodelasync endpoint deployedmodel trafficsplit respons getinitialfutur getnam error appear hit line respons getinitialfutur getnam error invalid argument dedic resourc support model project locat central model deploi model cloud consol programmat java new model endpoint new assign model",
        "Question_preprocessed_content":"issu deploi model java us deploi model error appear hit line error deploi model cloud consol programmat java new model endpoint new assign model",
        "Question_gpt_summary_original":"The user encountered an error when deploying an Auto-ML model using Java. The error message states that \"dedicated_resources\" is not supported for the model project and location. The user is able to deploy the model using the cloud console but not programmatically using Java 8. The model and endpoint are both new without any assigned model to it.",
        "Question_gpt_summary":"user encount error deploi auto model java error messag state dedic resourc support model project locat user abl deploi model cloud consol programmat java model endpoint new assign model",
        "Answer_original_content":"sorri implement wrong section document follow automl imag custom train",
        "Answer_preprocessed_content":"sorri implement wrong section document follow automl imag",
        "Answer_gpt_summary_original":"Solution: The user realized that they were implementing the wrong section of the documentation and had to follow AutoML image instead of Custom-trained one. No other solutions were mentioned.",
        "Answer_gpt_summary":"solut user realiz implement wrong section document follow automl imag instead custom train solut mention"
    },
    {
        "Question_title":"ClearML how to change clearml.conf file in AWS Sagemaker",
        "Question_body":"<p>I am working in AWS Sagemaker Jupyter notebook.\nI have installed clearml package in AWS Sagemaker in Jupyter.\nClearML server was installed on AWS EC2.\nI need to store artifacts and models in AWS S3 bucket, so I want to specify credentials to S3 in clearml.conf file.\nHow can I change clearml.conf file in AWS Sagemaker instance? looks like permission denied to all folders on it.\nOr maybe somebody can suggest a better approach.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1613428648627,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":276.0,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p>To set credentials (and <code>clearml-server<\/code> hosts) you can use <code>Task.set_credentials<\/code>.\nTo specify the S3 bucket as output for all artifacts (and debug images for that matter) you can just set it as the <code>files_server<\/code>.<\/p>\n<p>For example:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from clearml import Task\n\nTask.set_credentials(api_host='http:\/\/clearml-server:8008', web_host='http:\/\/clearml-server:8080', files_host='s3:\/\/my_bucket\/folder\/',\nkey='add_clearml_key_here', secret='add_clearml_key_secret_here')\n<\/code><\/pre>\n<p>To pass your S3 credentials, just add a cell at the top of your jupyter notebook, and set the standard AWS S3 environment variables:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nos.environ['AWS_ACCESS_KEY_ID'] = 's3_bucket_key_here'\nos.environ['AWS_SECRET_ACCESS_KEY'] = 's3_bucket_secret_here'\n# optional\nos.environ['AWS_DEFAULT_REGION'] = 's3_bucket_region'\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66216294",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1613774515323,
        "Question_original_content":"chang conf file work jupyt notebook instal packag jupyt server instal aw need store artifact model aw bucket want specifi credenti conf file chang conf file instanc look like permiss deni folder mayb somebodi suggest better approach",
        "Question_preprocessed_content":"chang conf file work jupyt notebook instal packag jupyt server instal aw need store artifact model aw bucket want specifi credenti conf file chang conf file instanc look like permiss deni folder mayb somebodi suggest better approach",
        "Question_gpt_summary_original":"The user is facing challenges in changing the clearml.conf file in AWS Sagemaker Jupyter notebook to specify credentials to S3 for storing artifacts and models. The user is encountering permission denied errors while attempting to change the file and is seeking suggestions for a better approach.",
        "Question_gpt_summary":"user face challeng chang conf file jupyt notebook specifi credenti store artifact model user encount permiss deni error attempt chang file seek suggest better approach",
        "Answer_original_content":"disclaim train team set credenti server host us task set credenti specifi bucket output artifact debug imag matter set file server exampl import task task set credenti api host http server web host http server file host bucket folder kei add kei secret add kei secret pass credenti add cell jupyt notebook set standard aw environ variabl import environ aw access kei bucket kei environ aw secret access kei bucket secret option environ aw default region bucket region",
        "Answer_preprocessed_content":"disclaim team set credenti us specifi bucket output artifact set exampl pass credenti add cell jupyt notebook set standard aw environ variabl",
        "Answer_gpt_summary_original":"Solution:\n- Use `Task.set_credentials` to set credentials and `files_server` to specify the S3 bucket as output for all artifacts.\n- Add a cell at the top of the Jupyter notebook and set the standard AWS S3 environment variables to pass S3 credentials.",
        "Answer_gpt_summary":"solut us task set credenti set credenti file server specifi bucket output artifact add cell jupyt notebook set standard aw environ variabl pass credenti"
    },
    {
        "Question_title":"my script stops running without any message explaining the reason",
        "Question_body":"Please see the screenshots below. Once it said terminated but without reason:\n\nThe other time there was nothing just stopped:",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1634306015143,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello,\n\nHope you have solved this issue and we are sorry not seeing your response. Since this issue happened without any error details, support ticket would be the best way to debug that. Please let me know if you still need that. Thanks.\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/592153\/my-script-stops-running-without-any-message-explai.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1635818701492,
        "Question_original_content":"script stop run messag explain reason screenshot said termin reason time stop",
        "Question_preprocessed_content":"script stop run messag explain reason screenshot said termin reason time stop",
        "Question_gpt_summary_original":"The user's script has been stopping without any explanation or error message, as shown in the provided screenshots. This has happened multiple times, with one instance showing a \"terminated\" message without any further explanation.",
        "Question_gpt_summary":"user script stop explan error messag shown provid screenshot happen multipl time instanc show termin messag explan",
        "Answer_original_content":"hello hope solv issu sorri see respons issu happen error detail support ticket best wai debug let know need thank regard yutong",
        "Answer_preprocessed_content":"hello hope solv issu sorri see respons issu happen error detail support ticket best wai debug let know need thank regard yutong",
        "Answer_gpt_summary_original":"Solution: No solution was provided in the discussion. The user was advised to submit a support ticket to debug the issue.",
        "Answer_gpt_summary":"solut solut provid discuss user advis submit support ticket debug issu"
    },
    {
        "Question_title":"Random forests on Azure GPU VM using the SDK",
        "Question_body":"Can you please share any code examples for training random forests with GPU on Azure using libraries.\nI want to run on the multiple nodes.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1595050125193,
        "Question_favorite_count":34.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@vautoml-0887 Thanks for the question. You can run LightGBM with boosting=random_forest, Please follow the below documentation:\nhttps:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#boosting\n\n\n\n\nHere is a general tutorial on how to run LightGBM on GPU, You can run it on any Azure GPU VM:\nhttps:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst\n\n\n\n\nIf you need to run it on multiple nodes, there is also a distributed spark implementation available at https:\/\/github.com\/Azure\/mmlspark.\n\n\n\n\nRandom Forests for the GPU using PyCUDA: https:\/\/pypi.org\/project\/cudatree\/",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/49008\/random-forests-on-azure-gpu-vm-using-the-sdk.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1595231268032,
        "Question_original_content":"random forest azur gpu sdk share code exampl train random forest gpu azur librari want run multipl node",
        "Question_preprocessed_content":"random forest azur gpu sdk share code exampl train random forest gpu azur librari want run multipl node",
        "Question_gpt_summary_original":"The user is seeking code examples for training random forests with GPU on Azure using libraries and wants to run it on multiple nodes.",
        "Question_gpt_summary":"user seek code exampl train random forest gpu azur librari want run multipl node",
        "Answer_original_content":"vautoml thank question run lightgbm boost random forest follow document http github com microsoft lightgbm blob master doc paramet rst boost gener tutori run lightgbm gpu run azur gpu http github com microsoft lightgbm blob master doc gpu tutori rst need run multipl node distribut spark implement avail http github com azur mmlspark random forest gpu pycuda http pypi org project cudatre",
        "Answer_preprocessed_content":"thank question run lightgbm follow document gener tutori run lightgbm gpu run azur gpu need run multipl node distribut spark implement avail random forest gpu pycuda",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- LightGBM with boosting=random_forest can be used to train random forests with GPU on Azure. Documentation for this is available at https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/Parameters.rst#boosting.\n- A general tutorial on how to run LightGBM on GPU is available at https:\/\/github.com\/microsoft\/LightGBM\/blob\/master\/docs\/GPU-Tutorial.rst.\n- If the user needs to run it on multiple nodes, a distributed spark implementation is available at https:\/\/github.com\/Azure\/mmlspark.\n- Another option is to use PyCUDA to train random forests for the GPU. The package cudatree can be used for this purpose, and",
        "Answer_gpt_summary":"possibl solut mention discuss lightgbm boost random forest train random forest gpu azur document avail http github com microsoft lightgbm blob master doc paramet rst boost gener tutori run lightgbm gpu avail http github com microsoft lightgbm blob master doc gpu tutori rst user need run multipl node distribut spark implement avail http github com azur mmlspark option us pycuda train random forest gpu packag cudatre purpos"
    },
    {
        "Question_title":"Is it possible to use HyperDriveStep with time-series cross-validation?",
        "Question_body":"<p>I want to deploy a stacked model to Azure Machine Learning Service. The architecture of the solution consists of three models and one meta-model.\nData is a time-series data. <\/p>\n\n<p>I'd like the model to automatically re-train based on some schedule. I'd also like to re-tune hyperparameters during each re-training. <\/p>\n\n<p>AML Service offers <code>HyperDriveStep<\/code> class that can be used in the pipeline for automatic hyperparameter optimization. <\/p>\n\n<p>Is it possible - and if so, how to do it - to use <code>HyperDriveStep<\/code> with time-series CV?<\/p>\n\n<p>I checked the documentation, but haven't found a satisfying answer.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1565358021917,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":189.0,
        "Answer_body":"<p>AzureML HyperDrive is a black box optimizer, meaning that it will just run your code with different parameter combinations based on the configuration you chose. At the same time, it supports Random and Bayesian sampling and has different policies for early stopping (see here for relevant <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-tune-hyperparameters\" rel=\"nofollow noreferrer\">docs<\/a> and here for an <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-tensorflow\/train-hyperparameter-tune-deploy-with-tensorflow.ipynb\" rel=\"nofollow noreferrer\">example<\/a> -- HyperDrive is towards the end of the notebook).<\/p>\n\n<p>The only thing that your model\/script\/training needs to adhere to is to be launched from a script that takes <code>--param<\/code> style parameters. As long as that holds you could optimize the parameters for each of your models individually and then tune the meta-model, or you could tune them all in one run. It will mainly depend on the size of the parameter space and the amount of compute you want to use (or pay for).<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57431340",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1565728003107,
        "Question_original_content":"possibl us hyperdrivestep time seri cross valid want deploi stack model servic architectur solut consist model meta model data time seri data like model automat train base schedul like tune hyperparamet train aml servic offer hyperdrivestep class pipelin automat hyperparamet optim possibl us hyperdrivestep time seri check document haven satisfi answer",
        "Question_preprocessed_content":"possibl us hyperdrivestep want deploi stack model servic architectur solut consist model data data like model automat base schedul like hyperparamet aml servic offer class pipelin automat hyperparamet optim possibl us check document haven satisfi answer",
        "Question_gpt_summary_original":"The user wants to deploy a stacked model to Azure Machine Learning Service consisting of three models and one meta-model. The data is time-series and the user wants the model to automatically re-train and re-tune hyperparameters during each re-training. The user is unsure if it is possible to use the HyperDriveStep class with time-series cross-validation and has not found a satisfying answer in the documentation.",
        "Question_gpt_summary":"user want deploi stack model servic consist model meta model data time seri user want model automat train tune hyperparamet train user unsur possibl us hyperdrivestep class time seri cross valid satisfi answer document",
        "Answer_original_content":"hyperdr black box optim mean run code differ paramet combin base configur chose time support random bayesian sampl differ polici earli stop relev doc exampl hyperdr end notebook thing model script train need adher launch script take param style paramet long hold optim paramet model individu tune meta model tune run mainli depend size paramet space comput want us pai",
        "Answer_preprocessed_content":"hyperdr black box optim mean run code differ paramet combin base configur chose time support random bayesian sampl differ polici earli stop thing need adher launch script take style paramet long hold optim paramet model individu tune tune run mainli depend size paramet space comput want us",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion include using AzureML HyperDrive, which supports Random and Bayesian sampling and has different policies for early stopping, to optimize the parameters for each of the models individually and then tune the meta-model, or to tune all the models in one run. The user can launch the model\/script\/training from a script that takes \"--param\" style parameters. However, it is not clear if HyperDriveStep class can be used with time-series cross-validation.",
        "Answer_gpt_summary":"possibl solut mention discuss includ hyperdr support random bayesian sampl differ polici earli stop optim paramet model individu tune meta model tune model run user launch model script train script take param style paramet clear hyperdrivestep class time seri cross valid"
    },
    {
        "Question_title":"How \"Azure ML export data to SQL database by insert in a row of database.\"",
        "Question_body":"<p>I can only export data from AzureML by write instead to database that created previously. I need to know How to insert and fetch the data continuously the database because I need to use old data as well as the new data that get as the AzureML output to plot graph.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1479709864747,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1480314865163,
        "Question_score":0.0,
        "Question_view_count":193.0,
        "Answer_body":"<ol>\n<li>Create a web service from the AzureML experiment. <\/li>\n<li>Access the web service using a program you written from C# or any language.<\/li>\n<li>You can get the output of the web service as a JSON.<\/li>\n<li>Use typical SQL ADD\/UPDATE queries to update the table<\/li>\n<li>When giving an input for the web service, fetch the data from the DB and pass as the JSON for it. <\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/40714064",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1480308703072,
        "Question_original_content":"export data sql databas insert row databas export data write instead databas creat previous need know insert fetch data continu databas need us old data new data output plot graph",
        "Question_preprocessed_content":"export data sql databas insert row export data write instead databas creat previous need know insert fetch data continu databas need us old data new data output plot graph",
        "Question_gpt_summary_original":"The user is facing challenges in exporting data from AzureML to a SQL database. They are unable to insert and fetch data continuously, which is necessary for plotting graphs using both old and new data.",
        "Question_gpt_summary":"user face challeng export data sql databas unabl insert fetch data continu necessari plot graph old new data",
        "Answer_original_content":"creat web servic experi access web servic program written languag output web servic json us typic sql add updat queri updat tabl give input web servic fetch data pass json",
        "Answer_preprocessed_content":"creat web servic experi access web servic program written languag output web servic json us typic sql queri updat tabl give input web servic fetch data pass json",
        "Answer_gpt_summary_original":"Solution:\n- Create a web service from the AzureML experiment.\n- Access the web service using a program written from C# or any language.\n- Get the output of the web service as a JSON.\n- Use typical SQL ADD\/UPDATE queries to update the table.\n- When giving an input for the web service, fetch the data from the DB and pass it as the JSON for it.",
        "Answer_gpt_summary":"solut creat web servic experi access web servic program written languag output web servic json us typic sql add updat queri updat tabl give input web servic fetch data pass json"
    },
    {
        "Question_title":"Docker-compose wouldn't start on Sagemaker's Notebook instance",
        "Question_body":"<p>Docker-compose seems to have stopped working on Sagemaker Notebook instances. When running <code>docker-compose up<\/code> I encounter the following error:<\/p>\n<pre><code>During handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/bin\/docker-compose&quot;, line 8, in &lt;module&gt;\n    sys.exit(main())\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/main.py&quot;, line 81, in main\n    command_func()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/main.py&quot;, line 200, in perform_command\n    project = project_from_options('.', options)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/command.py&quot;, line 70, in project_from_options\n    enabled_profiles=get_profiles_from_options(options, environment)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/command.py&quot;, line 153, in get_project\n    verbose=verbose, version=api_version, context=context, environment=environment\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/docker_client.py&quot;, line 43, in get_client\n    environment=environment, tls_version=get_tls_version(environment)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/compose\/cli\/docker_client.py&quot;, line 170, in docker_client\n    client = APIClient(use_ssh_client=not use_paramiko_ssh, **kwargs)\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/docker\/api\/client.py&quot;, line 197, in __init__\n    self._version = self._retrieve_server_version()\n  File &quot;\/home\/ec2-user\/anaconda3\/envs\/JupyterSystemEnv\/lib\/python3.6\/site-packages\/docker\/api\/client.py&quot;, line 222, in _retrieve_server_version\n    'Error while fetching server API version: {0}'.format(e)\ndocker.errors.DockerException: Error while fetching server API version: Timeout value connect was Timeout(connect=60, read=60, total=None), but it must be an int, float or None\n<\/code><\/pre>\n<p>I can start Docker containers as usual.<\/p>\n<pre><code>sh-4.2$ docker version\nClient:\n Version:           20.10.7\n API version:       1.41\n Go version:        go1.15.14\n Git commit:        f0df350\n Built:             Tue Sep 28 19:55:40 2021\n OS\/Arch:           linux\/amd64\n Context:           default\n Experimental:      true\n\nServer:\n Engine:\n  Version:          20.10.7\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.15.14\n  Git commit:       b0f5bc3\n  Built:            Tue Sep 28 19:57:35 2021\n  OS\/Arch:          linux\/amd64\n  Experimental:     false\n containerd:\n  Version:          1.4.6\n  GitCommit:        d71fcd7d8303cbf684402823e425e9dd2e99285d\n runc:\n  Version:          1.0.0\n  GitCommit:        %runc_commit\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0\n<\/code><\/pre>\n<p>But <code>docker-compose<\/code> wouldn't work...<\/p>\n<pre><code>sh-4.2$ docker-compose version\ndocker-compose version 1.29.2, build unknown\ndocker-py version: 5.0.0\nCPython version: 3.6.13\nOpenSSL version: OpenSSL 1.1.1l  24 Aug 2021\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1636114943820,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1636115811803,
        "Question_score":0.0,
        "Question_view_count":312.0,
        "Answer_body":"<p>For those of you who (might) have encountered the same issue, here's the fix:<\/p>\n<p>1). Install the newest version of docker-compose:<\/p>\n<pre><code>sh-4.2$ sudo curl -L &quot;https:\/\/github.com\/docker\/compose\/releases\/download\/1.29.2\/docker-compose-$(uname -s)-$(uname -m)&quot; -o \/usr\/local\/bin\/docker-compose\nsh-4.2$ sudo chmod +x \/usr\/local\/bin\/docker-compose\n<\/code><\/pre>\n<p>2). Change your <code>PATH<\/code> accordingly (since docker-compose is installed using <code>conda<\/code> and is picked up first) or use <code>\/usr\/local\/bin\/docker-compose<\/code> onwards:<\/p>\n<pre><code>sh-4.2$ PATH=\/usr\/local\/bin:$PATH\nsh-4.2$ docker-compose version\ndocker-compose version 1.29.2, build 5becea4c\ndocker-py version: 5.0.0\nCPython version: 3.7.10\nOpenSSL version: OpenSSL 1.1.0l  10 Sep 2019\n<\/code><\/pre>\n<p>Perhaps, the issue is related to this:<\/p>\n<blockquote>\n<p>On August 9, 2021 the Jupyter Notebook and Jupyter Lab open source software projects announced 2 security concerns that could impact Amazon Sagemaker Notebook Instance customers.<\/p>\n<p>Sagemaker has deployed updates to address these concerns, and we recommend customers with existing notebook sessions to stop and restart their notebook instance(s) to benefit from these updates. Notebook instances launched after August 10, 2021, when updates were deployed, are not impacted by this issue and do not need to be restarted.<\/p>\n<\/blockquote>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1636183275063,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69853177",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1636114943820,
        "Question_original_content":"docker compos wouldn start notebook instanc docker compos stop work notebook instanc run docker compos encount follow error handl except except occur traceback recent file home user anaconda env jupytersystemenv bin docker compos line sy exit main file home user anaconda env jupytersystemenv lib python site packag compos cli main line main command func file home user anaconda env jupytersystemenv lib python site packag compos cli main line perform command project project option option file home user anaconda env jupytersystemenv lib python site packag compos cli command line project option enabl profil profil option option environ file home user anaconda env jupytersystemenv lib python site packag compos cli command line project verbos verbos version api version context context environ environ file home user anaconda env jupytersystemenv lib python site packag compos cli docker client line client environ environ tl version tl version environ file home user anaconda env jupytersystemenv lib python site packag compos cli docker client line docker client client apicli us ssh client us paramiko ssh kwarg file home user anaconda env jupytersystemenv lib python site packag docker api client line init self version self retriev server version file home user anaconda env jupytersystemenv lib python site packag docker api client line retriev server version error fetch server api version format docker error dockerexcept error fetch server api version timeout valu connect timeout connect read total int float start docker contain usual docker version client version api version version git commit fdf built tue sep arch linux amd context default experiment true server engin version api version minimum version version git commit bfbc built tue sep arch linux amd experiment fals containerd version gitcommit dfcddcbfeed runc version gitcommit runc commit docker init version gitcommit dead docker compos wouldn work docker compos version docker compos version build unknown docker version cpython version openssl version openssl aug",
        "Question_preprocessed_content":"wouldn start notebook instanc stop work notebook instanc run encount follow error start docker contain usual wouldn",
        "Question_gpt_summary_original":"The user is encountering an error when trying to run docker-compose on Sagemaker Notebook instances. The error message indicates that there is a problem with the server API version and a timeout value. However, the user is able to start Docker containers as usual.",
        "Question_gpt_summary":"user encount error try run docker compos notebook instanc error messag indic problem server api version timeout valu user abl start docker contain usual",
        "Answer_original_content":"encount issu fix instal newest version docker compos sudo curl http github com docker compos releas download docker compos unam unam usr local bin docker compos sudo chmod usr local bin docker compos chang path accordingli docker compos instal conda pick us usr local bin docker compos onward path usr local bin path docker compos version docker compos version build beceac docker version cpython version openssl version openssl sep issu relat august jupyt notebook jupyt lab open sourc softwar project announc secur concern impact notebook instanc custom deploi updat address concern recommend custom exist notebook session stop restart notebook instanc benefit updat notebook instanc launch august updat deploi impact issu need restart",
        "Answer_preprocessed_content":"encount issu fix instal newest version chang accordingli us onward issu relat august jupyt notebook jupyt lab open sourc softwar project announc secur concern impact notebook instanc custom deploi updat address concern recommend custom exist notebook session stop restart notebook instanc benefit updat notebook instanc launch august updat deploi impact issu need restart",
        "Answer_gpt_summary_original":"Solution:\n- Install the newest version of docker-compose and change the PATH accordingly. \n- Restart the notebook instance to benefit from the updates deployed by Sagemaker to address the security concerns announced by Jupyter Notebook and Jupyter Lab open source software projects.",
        "Answer_gpt_summary":"solut instal newest version docker compos chang path accordingli restart notebook instanc benefit updat deploi address secur concern announc jupyt notebook jupyt lab open sourc softwar project"
    },
    {
        "Question_title":"Google Colab freezes my browser and pc when trying to reconnect to a notebook",
        "Question_body":"<p>I am training a Machine learning model in google colab, to be more specific I am training a GAN with PyTorch-lightning. The problem occurs is when I get disconnected from my current runtime due to inactivity. When I try to reconnect my Browser(tried on firefox and chrome) becomes first laggy and than freezes, my pc starts to lag so that I am not able to close my browser and it doesn't go away. I am forced to press the power button of my PC in order to restart the PC.\nI have no clue why this happens.\nI tried various batch sizes(also the size 1) but it still happens. It can't be that my dataset is too big either(since i tried it on a dataset with 10images for testing puposes).\nI hope someone can help me.<\/p>\n\n<p>Here is my code (For using the code you will need comet.nl and enter the comet.ml api key):<\/p>\n\n<pre><code>import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision  \nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST\n\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nimport pytorch_lightning as pl\nfrom pytorch_lightning import loggers\n\nimport numpy as np\nfrom numpy.random import choice\n\nfrom PIL import Image\n\nimport os\nfrom pathlib import Path\nimport shutil\n\nfrom collections import OrderedDict\n\n# custom weights initialization called on netG and netD\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\n# randomly flip some labels\ndef noisy_labels(y, p_flip=0.05):  # # flip labels with 5% probability\n    # determine the number of labels to flip\n    n_select = int(p_flip * y.shape[0])\n    # choose labels to flip\n    flip_ix = choice([i for i in range(y.shape[0])], size=n_select)\n    # invert the labels in place\n    y[flip_ix] = 1 - y[flip_ix]\n    return y\n\nclass AddGaussianNoise(object):\n    def __init__(self, mean=0.0, std=0.1):\n        self.std = std\n        self.mean = mean\n\n    def __call__(self, tensor):\n        return tensor + torch.randn(tensor.size()) * self.std + self.mean\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(mean={0}, std={1})'.format(self.mean, self.std)\n\ndef get_valid_labels(img):\n  return (0.8 - 1.1) * torch.rand(img.shape[0], 1, 1, 1) + 1.1  # soft labels\n\ndef get_unvalid_labels(img):\n  return noisy_labels((0.0 - 0.3) * torch.rand(img.shape[0], 1, 1, 1) + 0.3)  # soft labels\n\nclass Generator(nn.Module):\n    def __init__(self, ngf, nc, latent_dim):\n        super(Generator, self).__init__()\n        self.ngf = ngf\n        self.latent_dim = latent_dim\n        self.nc = nc\n\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n             nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*8) x 4 x 4\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*4) x 8 x 8\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf*2) x 16 x 16\n            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ngf) x 32 x 32\n            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass Discriminator(nn.Module):\n    def __init__(self, ndf, nc):\n        super(Discriminator, self).__init__()\n        self.nc = nc\n        self.ndf = ndf\n\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)\n\nclass DCGAN(pl.LightningModule):\n\n    def __init__(self, hparams, logger, checkpoint_folder, experiment_name):\n        super().__init__()\n        self.hparams = hparams\n        self.logger = logger  # only compatible with comet_logger at the moment\n        self.checkpoint_folder = checkpoint_folder\n        self.experiment_name = experiment_name\n\n        # networks\n        self.generator = Generator(ngf=hparams.ngf, nc=hparams.nc, latent_dim=hparams.latent_dim)\n        self.discriminator = Discriminator(ndf=hparams.ndf, nc=hparams.nc)\n        self.generator.apply(weights_init)\n        self.discriminator.apply(weights_init)\n\n        # cache for generated images\n        self.generated_imgs = None\n        self.last_imgs = None\n\n        # For experience replay\n        self.exp_replay_dis = torch.tensor([])\n\n        # creating checkpoint folder\n        dirpath = Path(self.checkpoint_folder)\n        if not dirpath.exists():\n          os.makedirs(dirpath, 0o755)\n\n    def forward(self, z):\n        return self.generator(z)\n\n    def adversarial_loss(self, y_hat, y):\n        return F.binary_cross_entropy(y_hat, y)\n\n    def training_step(self, batch, batch_nb, optimizer_idx):\n        # For adding Instance noise for more visit: https:\/\/www.inference.vc\/instance-noise-a-trick-for-stabilising-gan-training\/\n        std_gaussian = max(0, self.hparams.level_of_noise - ((self.hparams.level_of_noise * 1.5) * (self.current_epoch \/ self.hparams.epochs)))\n        AddGaussianNoiseInst = AddGaussianNoise(std=std_gaussian) # the noise decays over time\n\n        imgs, _ = batch\n        imgs = AddGaussianNoiseInst(imgs) # Adding instance noise to real images\n        self.last_imgs = imgs\n\n        # train generator\n        if optimizer_idx == 0:\n            # sample noise\n            z = torch.randn(imgs.shape[0], self.hparams.latent_dim, 1, 1)\n\n            # generate images\n            self.generated_imgs = self(z)\n            self.generated_imgs = AddGaussianNoiseInst(self.generated_imgs) # Adding instance noise to fake images\n\n            # Experience replay\n            # for discriminator\n            perm = torch.randperm(self.generated_imgs.size(0))  # Shuffeling\n            r_idx = perm[:max(1, self.hparams.experience_save_per_batch)]  # Getting the index\n            self.exp_replay_dis = torch.cat((self.exp_replay_dis, self.generated_imgs[r_idx]), 0).detach()  # Add our new example to the replay buffer\n\n            # ground truth result (ie: all fake)\n            g_loss = self.adversarial_loss(self.discriminator(self.generated_imgs), get_valid_labels(self.generated_imgs)) # adversarial loss is binary cross-entropy\n\n            tqdm_dict = {'g_loss': g_loss}\n            log = {'g_loss': g_loss, \"std_gaussian\": std_gaussian}\n            output = OrderedDict({\n                'loss': g_loss,\n                'progress_bar': tqdm_dict,\n                'log': log\n            })\n            return output\n\n        # train discriminator\n        if optimizer_idx == 1:\n            # Measure discriminator's ability to classify real from generated samples\n            # how well can it label as real?\n            real_loss = self.adversarial_loss(self.discriminator(imgs), get_valid_labels(imgs))\n\n            # Experience replay\n            if self.exp_replay_dis.size(0) &gt;= self.hparams.experience_batch_size:\n              fake_loss = self.adversarial_loss(self.discriminator(self.exp_replay_dis.detach()), get_unvalid_labels(self.exp_replay_dis))  # train on already seen images\n\n              self.exp_replay_dis = torch.tensor([]) # Reset experience replay\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"d_exp_loss\": fake_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n            else:\n              fake_loss = self.adversarial_loss(self.discriminator(self.generated_imgs.detach()), get_unvalid_labels(self.generated_imgs))  # how well can it label as fake?\n\n              # discriminator loss is the average of these\n              d_loss = (real_loss + fake_loss) \/ 2\n\n              tqdm_dict = {'d_loss': d_loss}\n              log = {'d_loss': d_loss, \"std_gaussian\": std_gaussian}\n              output = OrderedDict({\n                  'loss': d_loss,\n                  'progress_bar': tqdm_dict,\n                  'log': log\n              })\n              return output\n\n    def configure_optimizers(self):\n        lr = self.hparams.lr\n        b1 = self.hparams.b1\n        b2 = self.hparams.b2\n\n        opt_g = torch.optim.Adam(self.generator.parameters(), lr=lr, betas=(b1, b2))\n        opt_d = torch.optim.Adam(self.discriminator.parameters(), lr=lr, betas=(b1, b2))\n        return [opt_g, opt_d], []\n\n    def train_dataloader(self):\n        transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n                                        transforms.ToTensor(),\n                                        transforms.Normalize([0.5], [0.5])])\n        dataset = MNIST(os.getcwd(), train=True, download=True, transform=transform)\n        return DataLoader(dataset, batch_size=self.hparams.batch_size)\n        # transform = transforms.Compose([transforms.Resize((self.hparams.image_size, self.hparams.image_size)),\n        #                                 transforms.ToTensor(),\n        #                                 transforms.Normalize([0.5], [0.5])\n        #                                 ])\n\n        # train_dataset = torchvision.datasets.ImageFolder(\n        #     root=\".\/drive\/My Drive\/datasets\/ghibli_dataset_small_overfit\/\",\n        #     transform=transform\n        # )\n        # return DataLoader(train_dataset, num_workers=self.hparams.num_workers, shuffle=True, batch_size=self.hparams.batch_size)\n\n    def on_epoch_end(self):\n        z = torch.randn(4, self.hparams.latent_dim, 1, 1)\n        # match gpu device (or keep as cpu)\n        if self.on_gpu:\n            z = z.cuda(self.last_imgs.device.index)\n\n        # log sampled images\n        sample_imgs = self.generator(z)\n        sample_imgs = sample_imgs.view(-1, self.hparams.nc, self.hparams.image_size, self.hparams.image_size)\n        grid = torchvision.utils.make_grid(sample_imgs, nrow=2)\n        self.logger.experiment.log_image(grid.permute(1, 2, 0), f'generated_images_epoch{self.current_epoch}', step=self.current_epoch)\n\n        # save model\n        if self.current_epoch % self.hparams.save_model_every_epoch == 0:\n          trainer.save_checkpoint(self.checkpoint_folder + \"\/\" + self.experiment_name + \"_epoch_\" + str(self.current_epoch) + \".ckpt\")\n          comet_logger.experiment.log_asset_folder(self.checkpoint_folder, step=self.current_epoch)\n\n          # Deleting the folder where we saved the model so that we dont upload a thing twice\n          dirpath = Path(self.checkpoint_folder)\n          if dirpath.exists() and dirpath.is_dir():\n                shutil.rmtree(dirpath)\n\n          # creating checkpoint folder\n          access_rights = 0o755\n          os.makedirs(dirpath, access_rights)\n\nfrom argparse import Namespace\n\nargs = {\n    'batch_size': 48,\n    'lr': 0.0002,\n    'b1': 0.5,\n    'b2': 0.999,\n    'latent_dim': 128, # tested value which worked(in V4_1): 100\n    'nc': 1,\n    'ndf': 32,\n    'ngf': 32,\n    'epochs': 10,\n    'save_model_every_epoch': 5,\n    'image_size': 64,\n    'num_workers': 2,\n    'level_of_noise': 0.15,\n    'experience_save_per_batch': 1, # this value should be very low; tested value which works: 1\n    'experience_batch_size': 50 # this value shouldnt be too high; tested value which works: 50\n}\nhparams = Namespace(**args)\n\n# Parameters\nexperiment_name = \"DCGAN_V4_2_MNIST\"\ndataset_name = \"MNIST\"\ncheckpoint_folder = \"DCGAN\/\"\ntags = [\"DCGAN\", \"MNIST\", \"OVERFIT\", \"64x64\"]\ndirpath = Path(checkpoint_folder)\n\n# init logger\ncomet_logger = loggers.CometLogger(\n    api_key=\"\",\n    rest_api_key=\"\",\n    project_name=\"gan\",\n    experiment_name=experiment_name,\n    #experiment_key=\"f23d00c0fe3448ee884bfbe3fc3923fd\"  # used for resuming trained id can be found in comet.ml\n)\n\n#defining net\nnet = DCGAN(hparams, comet_logger, checkpoint_folder, experiment_name)\n\n#logging\ncomet_logger.experiment.set_model_graph(str(net))\ncomet_logger.experiment.add_tags(tags=tags)\ncomet_logger.experiment.log_dataset_info(dataset_name)\n\ntrainer = pl.Trainer(#resume_from_checkpoint=\"GHIBLI_DCGAN_OVERFIT_64px_epoch_6000.ckpt\",\n                     logger=comet_logger,\n                     max_epochs=args[\"epochs\"]\n                     )\ntrainer.fit(net)\ncomet_logger.experiment.end()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1586987562523,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1587130797008,
        "Question_score":0.0,
        "Question_view_count":1013.0,
        "Answer_body":"<p>I fixed it with importing this:<\/p>\n\n<pre><code>from IPython.display import clear_output \n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61239274",
        "Tool":"Comet",
        "Platform":"Stack Overflow",
        "Question_closed_time":1587034953183,
        "Question_original_content":"googl colab freez browser try reconnect notebook train machin learn model googl colab specif train gan pytorch lightn problem occur disconnect current runtim inact try reconnect browser tri firefox chrome laggi freez start lag abl close browser awai forc press power button order restart clue happen tri batch size size happen dataset big tri dataset imag test pupos hope help code code need enter api kei import torch import torch import torch function import torchvis import torchvis transform transform torch util data import dataload torchvis dataset import mnist pytorch lightn callback import modelcheckpoint import pytorch lightn pytorch lightn import logger import numpi numpi random import choic pil import imag import pathlib import path import shutil collect import ordereddict custom weight initi call netg netd def weight init classnam class classnam conv init normal weight data elif classnam batchnorm init normal weight data init constant bia data randomli flip label def noisi label flip flip label probabl determin number label flip select int flip shape choos label flip flip choic rang shape size select invert label place flip flip return class addgaussiannois object def init self mean std self std std self mean mean def self tensor return tensor torch randn tensor size self std self mean def repr self return self class mean std format self mean self std def valid label img return torch rand img shape soft label def unvalid label img return noisi label torch rand img shape soft label class gener modul def init self ngf latent dim super gener self init self ngf ngf self latent dim latent dim self self main sequenti input go convolut convtranspos latent dim ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf ngf bia fals batchnormd ngf leakyrelu inplac true state size ngf convtranspos ngf bia fals tanh state size def forward self input return self main input class discrimin modul def init self ndf super discrimin self init self self ndf ndf self main sequenti input convd ndf bia fals leakyrelu inplac true state size ndf convd ndf ndf bia fals batchnormd ndf leakyrelu inplac true state size ndf convd ndf ndf bia fals batchnormd ndf leakyrelu inplac true state size ndf convd ndf ndf bia fals batchnormd ndf leakyrelu inplac true state size ndf convd ndf bia fals sigmoid def forward self input return self main input class dcgan lightningmodul def init self hparam logger checkpoint folder experi super init self hparam hparam self logger logger compat logger moment self checkpoint folder checkpoint folder self experi experi network self gener gener ngf hparam ngf hparam latent dim hparam latent dim self discrimin discrimin ndf hparam ndf hparam self gener appli weight init self discrimin appli weight init cach gener imag self gener img self img experi replai self exp replai di torch tensor creat checkpoint folder dirpath path self checkpoint folder dirpath exist makedir dirpath def forward self return self gener def adversari loss self hat return binari cross entropi hat def train step self batch batch optim idx ad instanc nois visit http infer instanc nois trick stabilis gan train std gaussian max self hparam level nois self hparam level nois self current epoch self hparam epoch addgaussiannoiseinst addgaussiannois std std gaussian nois decai time img batch img addgaussiannoiseinst img ad instanc nois real imag self img img train gener optim idx sampl nois torch randn img shape self hparam latent dim gener imag self gener img self self gener img addgaussiannoiseinst self gener img ad instanc nois fake imag experi replai discrimin perm torch randperm self gener img size shuffel idx perm max self hparam experi save batch get index self exp replai di torch cat self exp replai di self gener img idx detach add new exampl replai buffer ground truth result fake loss self adversari loss self discrimin self gener img valid label self gener img adversari loss binari cross entropi tqdm dict loss loss log loss loss std gaussian std gaussian output ordereddict loss loss progress bar tqdm dict log log return output train discrimin optim idx measur discrimin abil classifi real gener sampl label real real loss self adversari loss self discrimin img valid label img experi replai self exp replai di size self hparam experi batch size fake loss self adversari loss self discrimin self exp replai di detach unvalid label self exp replai di train seen imag self exp replai di torch tensor reset experi replai discrimin loss averag loss real loss fake loss tqdm dict loss loss log loss loss exp loss fake loss std gaussian std gaussian output ordereddict loss loss progress bar tqdm dict log log return output fake loss self adversari loss self discrimin self gener img detach unvalid label self gener img label fake discrimin loss averag loss real loss fake loss tqdm dict loss loss log loss loss std gaussian std gaussian output ordereddict loss loss progress bar tqdm dict log log return output def configur optim self self hparam self hparam self hparam opt torch optim adam self gener paramet beta opt torch optim adam self discrimin paramet beta return opt opt def train dataload self transform transform compos transform resiz self hparam imag size self hparam imag size transform totensor transform normal dataset mnist getcwd train true download true transform transform return dataload dataset batch size self hparam batch size transform transform compos transform resiz self hparam imag size self hparam imag size transform totensor transform normal train dataset torchvis dataset imagefold root drive drive dataset ghibli dataset small overfit transform transform return dataload train dataset num worker self hparam num worker shuffl true batch size self hparam batch size def epoch end self torch randn self hparam latent dim match gpu devic cpu self gpu cuda self img devic index log sampl imag sampl img self gener sampl img sampl img view self hparam self hparam imag size self hparam imag size grid torchvis util grid sampl img nrow self logger experi log imag grid permut gener imag epoch self current epoch step self current epoch save model self current epoch self hparam save model epoch trainer save checkpoint self checkpoint folder self experi epoch str self current epoch ckpt logger experi log asset folder self checkpoint folder step self current epoch delet folder save model dont upload thing twice dirpath path self checkpoint folder dirpath exist dirpath dir shutil rmtree dirpath creat checkpoint folder access right makedir dirpath access right argpars import namespac arg batch size latent dim test valu work ndf ngf epoch save model epoch imag size num worker level nois experi save batch valu low test valu work experi batch size valu shouldnt high test valu work hparam namespac arg paramet experi dcgan mnist dataset mnist checkpoint folder dcgan tag dcgan mnist overfit dirpath path checkpoint folder init logger logger logger logger api kei rest api kei project gan experi experi experi kei fdcfeeebfbefcfd resum train defin net net dcgan hparam logger checkpoint folder experi log logger experi set model graph str net logger experi add tag tag tag logger experi log dataset info dataset trainer trainer resum checkpoint ghibli dcgan overfit epoch ckpt logger logger max epoch arg epoch trainer fit net logger experi end",
        "Question_preprocessed_content":"googl colab freez browser try reconnect notebook train machin learn model googl colab specif train gan problem occur disconnect current runtim inact try reconnect browser laggi freez start lag abl close browser awai forc press power button order restart clue happen tri batch size happen dataset big hope help code",
        "Question_gpt_summary_original":"The user is encountering issues with Google Colab freezing their browser and PC when trying to reconnect to a notebook while training a machine learning model, specifically a GAN with PyTorch-lightning. The issue occurs when the user gets disconnected from their current runtime due to inactivity. They have tried various batch sizes and datasets, but the problem persists. The user has provided their code for reference.",
        "Question_gpt_summary":"user encount issu googl colab freez browser try reconnect notebook train machin learn model specif gan pytorch lightn issu occur user get disconnect current runtim inact tri batch size dataset problem persist user provid code refer",
        "Answer_original_content":"fix import ipython displai import clear output",
        "Answer_preprocessed_content":"fix import",
        "Answer_gpt_summary_original":"Solution: The user found a solution to their problem by importing \"clear_output\" from IPython.display.",
        "Answer_gpt_summary":"solut user solut problem import clear output ipython displai"
    },
    {
        "Question_title":"Voice\/language options during conversion of long text files to speech",
        "Question_body":"The voice\/language options during conversion of long text files to speech. Can anyone help with the doc\/sample for the same.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1627934100000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":375.0,
        "Answer_body":"Howdy Ram56.\u00a0 Could you perhaps elaborate on what you are looking for?\u00a0 We'll be delighted to try and assist.\u00a0\u00a0\n\nHere is the home page for the GCP Text To Speech materials with links to docs:\n\nhttps:\/\/cloud.google.com\/text-to-speech\n\nI fully realize that is a fluffy response ... so if you can add a little more detail to the voice\/language query in your question, we'll get back to you ASAP.\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/Voice-language-options-during-conversion-of-long-text-files-to\/td-p\/165947\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1628005980000,
        "Question_original_content":"voic languag option convers long text file speech voic languag option convers long text file speech help doc sampl",
        "Question_preprocessed_content":"option convers long text file speech option convers long text file speech help",
        "Question_gpt_summary_original":"The user is seeking assistance with voice and language options for converting long text files to speech and is requesting a sample or documentation to help with this task.",
        "Question_gpt_summary":"user seek assist voic languag option convert long text file speech request sampl document help task",
        "Answer_original_content":"howdi ram elabor look delight try assist home page gcp text speech materi link doc http cloud googl com text speech fulli realiz fluffi respons add littl voic languag queri question asap view solut origin post",
        "Answer_preprocessed_content":"howdi ram elabor look delight try assist home page gcp text speech materi link doc fulli realiz fluffi respons add littl queri question asap view solut origin post",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to the home page for the GCP Text To Speech materials with links to documentation. No other solutions are mentioned.",
        "Answer_gpt_summary":"solut discuss provid link home page gcp text speech materi link document solut mention"
    },
    {
        "Question_title":"Azure Machine Learning Prediction - Input and Outputs",
        "Question_body":"<p>I am attempting to follow this <a href=\"http:\/\/www.toptal.com\/machine-learning\/predicting-gas-prices-using-azure-machine-learning-studio\" rel=\"nofollow\">tutorial<\/a> however I was attempting to predict MPG for a set of cars rather than oil prices and have the following set up:<\/p>\n\n<ol>\n<li>MPG Sample dataset<\/li>\n<li>Remove missing values, project everything (weight, displacement, cylinders, etc) except model name<\/li>\n<li>Split 75 to train model, 25 to score model<\/li>\n<li>Train model on MPG column with neural network<\/li>\n<li>Score model which is fed by Train Model and Split<\/li>\n<li>Score model is fed to Evaluate model<\/li>\n<\/ol>\n\n<p>This all seems to run fine and without issue, so I create a scoring experiment and then publish it as a web service, however when I attempt to input values it is asking for an MPG input. My understanding is that this would be the predicted value, so it seems somewhat opposite to have to enter this as a value, or am I just understanding a basic tenet of machine learning? <\/p>\n\n<p>In short: Ideally I would like to be able to enter everything but the MPG and get a prediction on what the MPG is for a given set of value.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1432296670560,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1527.0,
        "Answer_body":"<p>You could also add project columns to exclude label as part of scoring experiment and connect web service output port to the output of project columns<\/p>",
        "Answer_comment_count":8.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/30396392",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1432778316968,
        "Question_original_content":"predict input output attempt follow tutori attempt predict mpg set car oil price follow set mpg sampl dataset remov miss valu project weight displac cylind model split train model score model train model mpg column neural network score model fed train model split score model fed evalu model run fine issu creat score experi publish web servic attempt input valu ask mpg input understand predict valu somewhat opposit enter valu understand basic tenet machin learn short ideal like abl enter mpg predict mpg given set valu",
        "Question_preprocessed_content":"predict input output attempt follow tutori attempt predict mpg set car oil price follow set mpg sampl dataset remov miss valu project model split train model score model train model mpg column neural network score model fed train model split score model fed evalu model run fine issu creat score experi publish web servic attempt input valu ask mpg input understand predict valu somewhat opposit enter valu understand basic tenet machin learn short ideal like abl enter mpg predict mpg given set valu",
        "Question_gpt_summary_original":"The user is encountering a challenge in Azure Machine Learning Prediction where they are attempting to predict MPG for a set of cars, but the system is asking for an MPG input. The user is unsure if they are misunderstanding a basic tenet of machine learning and would like to be able to enter everything but the MPG and get a prediction on what the MPG is for a given set of values.",
        "Question_gpt_summary":"user encount challeng predict attempt predict mpg set car ask mpg input user unsur misunderstand basic tenet machin learn like abl enter mpg predict mpg given set valu",
        "Answer_original_content":"add project column exclud label score experi connect web servic output port output project column",
        "Answer_preprocessed_content":"add project column exclud label score experi connect web servic output port output project column",
        "Answer_gpt_summary_original":"Solution: The discussion suggests adding project columns to exclude the label as part of the scoring experiment and connecting the web service output port to the output of project columns. This solution can help the user enter everything but the MPG and get a prediction on what the MPG is for a given set of values.",
        "Answer_gpt_summary":"solut discuss suggest ad project column exclud label score experi connect web servic output port output project column solut help user enter mpg predict mpg given set valu"
    },
    {
        "Question_title":"connect vertex ai endpoint through .Net",
        "Question_body":"<p>Is there any way to connected google cloud platform service vertex ai endpoint through .Net code ? I am new to gcp vertex. any help is really apricated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1647981155950,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":128.0,
        "Answer_body":"<p>You could refer to <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1 documentation<\/a> for reference for dot net. You can start by:<\/p>\n<ol>\n<li>Installing the package from <a href=\"https:\/\/www.nuget.org\/packages\/Google.Cloud.AIPlatform.V1\/\" rel=\"nofollow noreferrer\">Google AI Platform nuget<\/a><\/li>\n<li>If you don't have an endpoint yet you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.EndpointServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.EndpointServiceClient<\/a>. You can use this class to manage endpoints like create endpoint, delete endpoint, deploy endpoint, etc.\n<ul>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/IndexEndpointServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">EndpointServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<li>If you have an endpoint and you want to run predictions using it, you can check out <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient\" rel=\"nofollow noreferrer\">Google.Cloud.AIPlatform.V1.PredictionServiceClient<\/a>. You can use this class to perform prediction using your endpoint.\n<ul>\n<li>Specifically <a href=\"https:\/\/cloud.google.com\/dotnet\/docs\/reference\/Google.Cloud.AIPlatform.V1\/latest\/Google.Cloud.AIPlatform.V1.PredictionServiceClient#Google_Cloud_AIPlatform_V1_PredictionServiceClient_Predict_Google_Cloud_AIPlatform_V1_EndpointName_System_Collections_Generic_IEnumerable_Google_Protobuf_WellKnownTypes_Value__Google_Protobuf_WellKnownTypes_Value_Google_Api_Gax_Grpc_CallSettings_\" rel=\"nofollow noreferrer\">Predict(EndpointName, IEnumerable, Value, CallSettings)<\/a> method where it accepts an endpoint as parameter.<\/li>\n<li>Check this <a href=\"https:\/\/github.com\/googleapis\/google-cloud-dotnet\/blob\/main\/apis\/Google.Cloud.AIPlatform.V1\/Google.Cloud.AIPlatform.V1.Snippets\/PredictionServiceClientSnippets.g.cs\" rel=\"nofollow noreferrer\">PredictionServiceClient code sample<\/a> for usage.<\/li>\n<\/ul>\n<\/li>\n<\/ol>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71578582",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1648004091847,
        "Question_original_content":"connect endpoint net wai connect googl cloud platform servic endpoint net code new gcp vertex help apric",
        "Question_preprocessed_content":"connect endpoint net wai connect googl cloud platform servic endpoint net code new gcp vertex help apric",
        "Question_gpt_summary_original":"The user is seeking assistance in connecting a Google Cloud Platform service, Vertex AI endpoint, through .Net code. They are new to GCP Vertex and are requesting help.",
        "Question_gpt_summary":"user seek assist connect googl cloud platform servic endpoint net code new gcp vertex request help",
        "Answer_original_content":"refer googl cloud aiplatform document refer dot net start instal packag googl platform nuget endpoint check googl cloud aiplatform endpointservicecli us class manag endpoint like creat endpoint delet endpoint deploi endpoint check endpointservicecli code sampl usag endpoint want run predict check googl cloud aiplatform predictionservicecli us class perform predict endpoint specif predict endpointnam ienumer valu callset method accept endpoint paramet check predictionservicecli code sampl usag",
        "Answer_preprocessed_content":"refer document refer dot net start instal packag googl platform nuget endpoint check us class manag endpoint like creat endpoint delet endpoint deploi endpoint check endpointservicecli code sampl usag endpoint want run predict check us class perform predict endpoint specif predict method accept endpoint paramet check predictionservicecli code sampl usag",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Refer to Google.Cloud.AIPlatform.V1 documentation for reference for dot net.\n- Install the package from Google AI Platform nuget.\n- Use Google.Cloud.AIPlatform.V1.EndpointServiceClient to manage endpoints like create endpoint, delete endpoint, deploy endpoint, etc.\n- Use Google.Cloud.AIPlatform.V1.PredictionServiceClient to perform prediction using your endpoint.",
        "Answer_gpt_summary":"possibl solut mention discuss refer googl cloud aiplatform document refer dot net instal packag googl platform nuget us googl cloud aiplatform endpointservicecli manag endpoint like creat endpoint delet endpoint deploi endpoint us googl cloud aiplatform predictionservicecli perform predict endpoint"
    },
    {
        "Question_title":"DLIB library installation error through pip",
        "Question_body":"<p>I am running a computer vision application which connects with Azure cognitive services and runs facial detection algorithm in backend in loop. It connects with a camera in the car and also alarm. My error is with installation of <code>dlib<\/code> library which handles the facial parameters of the person.<\/p>\n<p>I tried to run the following command:<\/p>\n<pre><code>pip install dlib\n<\/code><\/pre>\n<p>But I am getting the following error<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/uZt3w.png\" rel=\"nofollow noreferrer\">dlib installation error<\/a><\/p>\n<p>I'm using <em>Anaconda Navigator Individual Edition, Python 3.9<\/em> as the programming language.<\/p>\n<p>Any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1651120191987,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1651158458512,
        "Question_score":0.0,
        "Question_view_count":373.0,
        "Answer_body":"<p>As you are using Anaconda Navigator, the IDLE will be either Jupyter Notebook or Spyder. DLIB is a regular upgrading library for facial recognition. It cannot be installed on the base environment. @Avinash mentioned regarding two more libraries <em><strong>cmake<\/strong><\/em> and <em><strong>face_recognition<\/strong><\/em> the major issue for is DLIB.<\/p>\n<p>Follow the below steps to install DLIB using Anaconda Navigator<\/p>\n<ol>\n<li><p>Open <strong>Anaconda Navigator<\/strong> or <strong>Open Anaconda Prompt<\/strong><\/p>\n<\/li>\n<li><p>Click on <strong>Environments<\/strong> (For anaconda navigator UI)<\/p>\n<\/li>\n<li><p>Click on <strong>&quot;Create&quot;<\/strong> and to create a new environment (For anaconda navigator UI)<\/p>\n<\/li>\n<li><p>Select the <strong>python version<\/strong>. (For anaconda navigator UI)<\/p>\n<\/li>\n<li><p>Click on create. (For anaconda navigator UI)<\/p>\n<\/li>\n<li><p>Open anaconda navigator<\/p>\n<\/li>\n<li><p>Enter the following command to navigate from base environment to virtual environment just created<\/p>\n<\/li>\n<li><p>Enter the following command to install DLIB<\/p>\n<\/li>\n<\/ol>\n<p>If you are using normal <em><strong>Python IDLE<\/strong><\/em>, use the following procedure.<\/p>\n<ol>\n<li><strong>Virtual Environment creation using command prompt<\/strong><\/li>\n<\/ol>\n<p>Syntax: <code>python3 -m venv [Virtual Environment Name]<\/code><\/p>\n<p>Code: <code>python3 -m venv dlib<\/code><\/p>\n<ol start=\"2\">\n<li><strong>Activate Virtual Environment<\/strong><\/li>\n<\/ol>\n<p>Syntax: <code>.\\[Virtual Environment Folder Name]\\Scripts\\activate<\/code><\/p>\n<p>Code: <code>.\\dlib\\Scripts\\activate<\/code><\/p>\n<p>or you can directly mention as<\/p>\n<pre><code>activate dlib\n<\/code><\/pre>\n<ol start=\"3\">\n<li><p><strong>Finally you will be shifter from base environment to virtual environment<\/strong><\/p>\n<\/li>\n<li><p><strong>Deactivating virtual environment<\/strong><\/p>\n<p><code>deactivate<\/code><\/p>\n<\/li>\n<\/ol>\n<p>If you are not having proper UI access, then directly go to <strong>Anaconda Navigator<\/strong> and use the below command for virtual environment creation.<\/p>\n<ol>\n<li><p><strong>Open Anaconda Prompt<\/strong><\/p>\n<\/li>\n<li><p><strong>Check for conda installed in path<\/strong><\/p>\n<p><code>conda -V<\/code><\/p>\n<\/li>\n<li><p><strong>Check if conda is updated or not<\/strong><\/p>\n<p><code>conda update conda<\/code><\/p>\n<\/li>\n<li><p><strong>Create a virtual environment<\/strong><\/p>\n<\/li>\n<\/ol>\n<p>syntax: <code>conda create --name [Virtual Environment Name] python=[Version you want to install]<\/code><\/p>\n<p>code: <code>conda create --name dlib python=3.8<\/code><\/p>\n<p>Installing DLIB in virtual environment.<\/p>\n<pre><code>pip install dlib\n<\/code><\/pre>\n<p>remaining activating and deactivating are same.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72038060",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1651126783803,
        "Question_original_content":"dlib librari instal error pip run vision applic connect azur cognit servic run facial detect algorithm backend loop connect camera car alarm error instal dlib librari handl facial paramet person tri run follow command pip instal dlib get follow error dlib instal error anaconda navig individu edit python program languag help appreci",
        "Question_preprocessed_content":"dlib librari instal error pip run vision applic connect azur cognit servic run facial detect algorithm backend loop connect camera car alarm error instal librari handl facial paramet person tri run follow command get follow error dlib instal error anaconda navig individu edit python program languag help appreci",
        "Question_gpt_summary_original":"The user is facing challenges with installing the dlib library through pip for a computer vision application that runs facial detection algorithm in the backend. The user is receiving an installation error and is using Anaconda Navigator Individual Edition with Python 3.9.",
        "Question_gpt_summary":"user face challeng instal dlib librari pip vision applic run facial detect algorithm backend user receiv instal error anaconda navig individu edit python",
        "Answer_original_content":"anaconda navig idl jupyt notebook spyder dlib regular upgrad librari facial recognit instal base environ avinash mention librari cmake face recognit major issu dlib follow step instal dlib anaconda navig open anaconda navig open anaconda prompt click environ anaconda navig click creat creat new environ anaconda navig select python version anaconda navig click creat anaconda navig open anaconda navig enter follow command navig base environ virtual environ creat enter follow command instal dlib normal python idl us follow procedur virtual environ creation command prompt syntax python venv virtual environ code python venv dlib activ virtual environ syntax virtual environ folder script activ code dlib script activ directli mention activ dlib final shifter base environ virtual environ deactiv virtual environ deactiv have proper access directli anaconda navig us command virtual environ creation open anaconda prompt check conda instal path conda check conda updat conda updat conda creat virtual environ syntax conda creat virtual environ python version want instal code conda creat dlib python instal dlib virtual environ pip instal dlib remain activ deactiv",
        "Answer_preprocessed_content":"anaconda navig idl jupyt notebook spyder dlib regular upgrad librari facial recognit instal base environ mention librari cmake major issu dlib follow step instal dlib anaconda navig open anaconda navig open anaconda prompt click environ click creat creat new environ select python version click creat open anaconda navig enter follow command navig base environ virtual environ creat enter follow command instal dlib normal python idl us follow procedur virtual environ creation command prompt syntax code activ virtual environ syntax code directli mention final shifter base environ virtual environ deactiv virtual environ have proper access directli anaconda navig us command virtual environ creation open anaconda prompt check conda instal path check conda updat creat virtual environ syntax code instal dlib virtual environ remain activ deactiv",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Create a new virtual environment using Anaconda Navigator and install DLIB in it.\n2. Create a virtual environment using command prompt and install DLIB in it.\n3. Use Anaconda Prompt to create a virtual environment and install DLIB in it.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss creat new virtual environ anaconda navig instal dlib creat virtual environ command prompt instal dlib us anaconda prompt creat virtual environ instal dlib person opinion bias includ summari"
    },
    {
        "Question_title":"How can I register in Azure ML Service a machine learning model trained locally?",
        "Question_body":"<p>I am trying out <a href=\"https:\/\/azure.microsoft.com\/en-us\/services\/machine-learning-service\/\" rel=\"nofollow noreferrer\">Azure Machine Learning Service<\/a> for ML deployment.<\/p>\n\n<p>I have already trained a model on a compute VM and saved it as pickle, and now would like to deploy it (I am using Python on Azure notebooks for the purpose as of now).<\/p>\n\n<p>From the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#register-model\" rel=\"nofollow noreferrer\">guide<\/a>, it looks like I need to I need a <code>run<\/code> object to be existing in my session to execute the \"model registration\" step:<\/p>\n\n<pre><code># register model \nmodel = run.register_model(model_name='my_model', model_path='outputs\/my_model.pkl')\nprint(model.name, model.id, model.version, sep = '\\t')\n<\/code><\/pre>\n\n<p>However, I haven't created any <code>run<\/code> object as I haven't <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-train-models-with-aml#submit-the-job-to-the-cluster\" rel=\"nofollow noreferrer\">executed<\/a> any experiment for training, I am just starting off with my pickled model.<\/p>\n\n<p>I also tried to register a model by uploading it via the Azure Portal (see screenshot below), but (as the model file is quite large, I assume) it fails with a <code>ajax error 413.<\/code> as in <a href=\"https:\/\/stackoverflow.com\/questions\/55064123\/unable-to-register-an-onnx-model-in-azure-machine-learning-service-workspace\">Unable to register an ONNX model in azure machine learning service workspace<\/a>.  <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/tVKcV.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tVKcV.png\" alt=\"model registering\"><\/a> <\/p>\n\n<p>Is there any way to register and then deploy a pretrained pickled mode (without the need of submitting a <code>run<\/code>, if that makes sense)?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1553160837153,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1554642485923,
        "Question_score":0.0,
        "Question_view_count":1466.0,
        "Answer_body":"<p>Model registration can be done with <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.model.model?view=azure-ml-py#register-workspace--model-path--model-name--tags-none--properties-none--description-none-\" rel=\"nofollow noreferrer\">Model.register<\/a>, without the need of using a <code>run<\/code> object<\/p>\n\n<pre><code>model = Model.register(model_name='my_model', model_path='my_model.pkl', workspace = ws)\n<\/code><\/pre>\n\n<p>for the deployment one can follow steps as outlined in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/tutorial-deploy-models-with-aml#deploy-as-a-web-service\" rel=\"nofollow noreferrer\">Azure ML service doc<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55277334",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1553175443707,
        "Question_original_content":"regist servic machin learn model train local try servic deploy train model comput save pickl like deploi python azur notebook purpos guid look like need need run object exist session execut model registr step regist model model run regist model model model model path output model pkl print model model model version sep haven creat run object haven execut experi train start pickl model tri regist model upload azur portal screenshot model file larg assum fail ajax error unabl regist onnx model servic workspac wai regist deploi pretrain pickl mode need submit run make sens",
        "Question_preprocessed_content":"regist servic machin learn model train local try servic deploy train model comput save pickl like deploi guid look like need need object exist session execut model registr step haven creat object haven execut experi train start pickl model tri regist model upload azur portal fail unabl regist onnx model servic workspac wai regist deploi pretrain pickl mode",
        "Question_gpt_summary_original":"The user is facing challenges in registering a machine learning model trained locally in Azure ML Service. They are unable to execute the \"model registration\" step as they haven't created any run object and haven't submitted any experiment for training. They also tried to register the model by uploading it via the Azure Portal, but it failed due to the large size of the model file. The user is looking for a way to register and deploy a pretrained pickled model without submitting a run.",
        "Question_gpt_summary":"user face challeng regist machin learn model train local servic unabl execut model registr step haven creat run object haven submit experi train tri regist model upload azur portal fail larg size model file user look wai regist deploi pretrain pickl model submit run",
        "Answer_original_content":"model registr model regist need run object model model regist model model model path model pkl workspac deploy follow step outlin servic doc",
        "Answer_preprocessed_content":"model registr need object deploy follow step outlin servic doc",
        "Answer_gpt_summary_original":"Solution: The user can use the Model.register method to register the model without the need for a run object. They can use the following code to register the model:\n\n```\nmodel = Model.register(model_name='my_model', model_path='my_model.pkl', workspace = ws)\n```\n\nFor deployment, the user can follow the steps outlined in the Azure ML service documentation.",
        "Answer_gpt_summary":"solut user us model regist method regist model need run object us follow code regist model model model regist model model model path model pkl workspac deploy user follow step outlin servic document"
    },
    {
        "Question_title":"AWS Sagemaker output how to read file with multiple json objects spread out over multiple lines",
        "Question_body":"<p>I have a bunch of json files that look like this<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774, ], &quot;word&quot;: &quot;blah blah blah&quot;}\n<\/code><\/pre>\n<p>Which I can read in with<\/p>\n<pre><code>f = open(file_name)\ndata = []\nfor line in f:\n   data.append(json.dumps(line))\n<\/code><\/pre>\n<p>But I have another file with output like this<\/p>\n<pre><code>{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n<\/code><\/pre>\n<p>I.e. the json is formatted over several lines, so I can't simply read the json in line for line. Is there an easy way to parse this? Or do I have to write something that stitches together each json object line by line and the does json.loads?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1635629350997,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1635655049276,
        "Question_score":1.0,
        "Question_view_count":248.0,
        "Answer_body":"<p>Hmm,  as far as I know there's unfortunately no way to load a <a href=\"https:\/\/jsonlines.org\/\" rel=\"nofollow noreferrer\">JSONL<\/a> format data using <code>json.loads<\/code>. One option though, is to come up with a helper function that can convert it to a valid JSON string, as below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import json\n\nstring = &quot;&quot;&quot;\n{\n    &quot;predictions&quot;: [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]\n    ]\n}\n{\n    &quot;predictions&quot;: [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]\n    ]\n}\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n    # replace the first occurrence of '{'\n    s = s.replace('{', '[{', 1)\n\n    # replace the last occurrence of '}\n    s = s.rsplit('}', 1)[0] + '}]'\n\n    # now go in and replace all occurrences of '}' immediately followed\n    # by newline with a '},'\n    s = s.replace('}\\n', '},\\n')\n\n    return s\n\n\nprint(json.loads(json_lines_to_json(string)))\n<\/code><\/pre>\n<p>Prints:<\/p>\n<pre><code>[{'predictions': [[0.875780046, 0.124219939], [0.892282844, 0.107717164], [0.887681246, 0.112318777]]}, {'predictions': [[0.0, 1.0], [0.0, 1.0], [0.0, 1.0]]}, {'predictions': [[0.391415, 0.608585], [0.992118478, 0.00788147748], [0.0, 1.0]]}]\n<\/code><\/pre>\n<p><strong>Note:<\/strong> your first example actually doesn't seem like valid JSON (or at least JSON lines from my understanding). In particular, this part appears to be invalid due to a trailing comma after the last array element:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664, ], ...}\n<\/code><\/pre>\n<p>To ensure it's valid after calling the helper function, you'd also need to remove the trailing commas, so each line is in the below format:<\/p>\n<pre><code>{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], ...},\n<\/code><\/pre>\n<hr \/>\n<p>There also appears to be a <a href=\"https:\/\/stackoverflow.com\/questions\/50475635\/loading-jsonl-file-as-json-objects\/50475669\">similar question<\/a> where they suggest splitting on newlines and calling <code>json.loads<\/code> on each line; actually it should be (slightly) less performant to call <code>json.loads<\/code> multiple times on each object, rather than once on the list, as I show below.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from timeit import timeit\nimport json\n\n\nstring = &quot;&quot;&quot;\\\n{&quot;vector&quot;: [0.017906909808516502, 0.052080217748880386, -0.1460590809583664 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.01027186680585146, 0.04181386157870293, -0.07363887131214142 ], &quot;word&quot;: &quot;blah blah blah&quot;}\n{&quot;vector&quot;: [0.011699287220835686, 0.04741542786359787, -0.07899319380521774 ], &quot;word&quot;: &quot;blah blah blah&quot;}\\\n&quot;&quot;&quot;\n\n\ndef json_lines_to_json(s: str) -&gt; str:\n\n    # Strip newlines from end, then replace all occurrences of '}' followed\n    # by a newline, by a '},' followed by a newline.\n    s = s.rstrip('\\n').replace('}\\n', '},\\n')\n\n    # return string value wrapped in brackets (list)\n    return f'[{s}]'\n\n\nn = 10_000\n\nprint('string replace:        ', timeit(r'json.loads(json_lines_to_json(string))', number=n, globals=globals()))\nprint('json.loads each line:  ', timeit(r'[json.loads(line) for line in string.split(&quot;\\n&quot;)]', number=n, globals=globals()))\n<\/code><\/pre>\n<p>Result:<\/p>\n<pre><code>string replace:         0.07599360000000001\njson.loads each line:   0.1078384\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1635655742576,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69782294",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1635633656723,
        "Question_original_content":"output read file multipl json object spread multipl line bunch json file look like vector word blah blah blah vector word blah blah blah vector word blah blah blah read open file data line data append json dump line file output like predict predict predict json format line simpli read json line line easi wai pars write stitch json object line line json load",
        "Question_preprocessed_content":"output read file multipl json object spread multipl line bunch json file look like read file output like json format line simpli read json line line easi wai pars write stitch json object line line",
        "Question_gpt_summary_original":"The user is facing a challenge in reading a file with multiple JSON objects spread out over multiple lines. The user is able to read in JSON files with single-line JSON objects, but is unsure how to parse the multi-line JSON objects. The user is seeking advice on whether there is an easy way to parse the multi-line JSON objects or if they need to write a code to stitch together each JSON object line by line and then use json.loads.",
        "Question_gpt_summary":"user face challeng read file multipl json object spread multipl line user abl read json file singl line json object unsur pars multi line json object user seek advic easi wai pars multi line json object need write code stitch json object line line us json load",
        "Answer_original_content":"hmm far know unfortun wai load jsonl format data json load option come helper function convert valid json string import json string predict predict predict def json line json str str replac occurr replac replac occurr rsplit replac occurr immedi follow newlin replac return print json load json line json string print predict predict predict note exampl actual like valid json json line understand particular appear invalid trail comma arrai element vector ensur valid call helper function need remov trail comma line format vector appear similar question suggest split newlin call json load line actual slightli perform json load multipl time object list timeit import timeit import json string vector word blah blah blah vector word blah blah blah vector word blah blah blah def json line json str str strip newlin end replac occurr follow newlin follow newlin rstrip replac return string valu wrap bracket list return print string replac timeit json load json line json string number global global print json load line timeit json load line line string split number global global result string replac json load line",
        "Answer_preprocessed_content":"hmm far know unfortun wai load jsonl format data option come helper function convert valid json string print note exampl actual like valid json particular appear invalid trail comma arrai element ensur valid call helper function need remov trail comma line format appear similar question suggest split newlin call line actual perform multipl time object list result",
        "Answer_gpt_summary_original":"Two solutions were provided in the discussion. The first solution is to create a helper function that converts the multi-line JSON objects to a valid JSON string by replacing the first occurrence of '{', replacing the last occurrence of '}', and replacing all occurrences of '}' immediately followed by a newline with a '},'. The second solution is to split the multi-line JSON objects on newlines and call json.loads on each line. However, it was noted that calling json.loads multiple times on each object may be less performant than calling it once on the list.",
        "Answer_gpt_summary":"solut provid discuss solut creat helper function convert multi line json object valid json string replac occurr replac occurr replac occurr immedi follow newlin second solut split multi line json object newlin json load line note call json load multipl time object perform call list"
    },
    {
        "Question_title":"Unable to see VIEWS in SageMaker UI",
        "Question_body":"<p>We have a connection from SageMaker to Snowflake, and need to see <strong>views<\/strong> (as opposed to tables) listed when using Data Wrangler. Is there a reason that views are not shown in the data listing of Data Wrangler?<\/p>\n<p>We have checked security settings and access. These are not materialized views.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1648568385610,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":57.0,
        "Answer_body":"<p>Currently Data Wrangler does not support browsing Views in the UI but you can still query them with SELECT.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71665007",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1649289755440,
        "Question_original_content":"unabl view connect snowflak need view oppos tabl list data wrangler reason view shown data list data wrangler check secur set access materi view",
        "Question_preprocessed_content":"unabl view connect snowflak need view list data wrangler reason view shown data list data wrangler check secur set access materi view",
        "Question_gpt_summary_original":"The user is unable to see views in the data listing of Data Wrangler in SageMaker UI despite having a connection from SageMaker to Snowflake. The user has checked security settings and access and confirmed that these are not materialized views.",
        "Question_gpt_summary":"user unabl view data list data wrangler despit have connect snowflak user check secur set access confirm materi view",
        "Answer_original_content":"current data wrangler support brows view queri select",
        "Answer_preprocessed_content":"current data wrangler support brows view queri select",
        "Answer_gpt_summary_original":"Solution: The discussion mentions that currently Data Wrangler does not support browsing views in the UI, but the user can still query them with SELECT.",
        "Answer_gpt_summary":"solut discuss mention current data wrangler support brows view user queri select"
    },
    {
        "Question_title":"ML Pickle file size Azure Machine Learning Service",
        "Question_body":"Is there any restriction on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file?\n\nDoes it cause latency in realtime data processing and getting the prediction results from the pickle file if we have a model that let's say it 5MB and the other one is 500MB (The bigger file has better performance in terms of accuracy)?\nThanks,\n\nJohn",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1599612419390,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@JA-4570 Thanks, For ACI we recommend not using a model over 1GB in size.\nFor AKS you are limited by the memory resources that you request for your service, minus about 500mb for the running python process in the pod.\n\nThere will be no difference in prediction speed once the model is successfully deployed.\nRegistering will take longer as we have to upload the model, and deploying will take longer as the service must download the model.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/89630\/ml-pickle-file-size-azure-machine-learning-service.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1599800075416,
        "Question_original_content":"pickl file size servic restrict regist pickl model servic term size pickl file caus latenc realtim data process get predict result pickl file model let bigger file better perform term accuraci thank john",
        "Question_preprocessed_content":"pickl file size servic restrict regist pickl model servic term size pickl file caus latenc realtim data process get predict result pickl file model let thank john",
        "Question_gpt_summary_original":"The user is inquiring about any restrictions on registering an ML pickle model into Azure Machine Learning Service in terms of the size of the pickle file. They are also concerned about the potential latency in real-time data processing and getting prediction results from a larger pickle file, even if it has better performance in terms of accuracy.",
        "Question_gpt_summary":"user inquir restrict regist pickl model servic term size pickl file concern potenti latenc real time data process get predict result larger pickl file better perform term accuraci",
        "Answer_original_content":"thank aci recommend model size ak limit memori resourc request servic minu run python process pod differ predict speed model successfulli deploi regist longer upload model deploi longer servic download model",
        "Answer_preprocessed_content":"thank aci recommend model size ak limit memori resourc request servic minu run python process pod differ predict speed model successfulli deploi regist longer upload model deploi longer servic download model",
        "Answer_gpt_summary_original":"Solutions provided in the discussion are:\n\n- For ACI, it is recommended not to use a model over 1GB in size.\n- For AKS, the user is limited by the memory resources that they request for their service, minus about 500mb for the running python process in the pod.\n- There will be no difference in prediction speed once the model is successfully deployed.\n- Registering will take longer as the model needs to be uploaded, and deploying will take longer as the service must download the model.",
        "Answer_gpt_summary":"solut provid discuss aci recommend us model size ak user limit memori resourc request servic minu run python process pod differ predict speed model successfulli deploi regist longer model need upload deploi longer servic download model"
    },
    {
        "Question_title":"sage maker using visual studio code locally instead of notebooks - SDK",
        "Question_body":"<p>I prefer to use my local laptop for cost reasons + having an IDE like Visual Studio Code. As it stands my IT department is also not able\/reluctant to setup database connectivity for could 9 etc. (so I would have to move data into S3 from local laptop). I also investigated <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/randomcutforest.html\" rel=\"nofollow noreferrer\">this<\/a> to SSH into an EC2. However, again IT is unwilling to open up network connectivity ...<\/p>\n<p>The issue I have is, that my laptop only has 8GB (windows). So from time to time I would like\/have to use the cloud (e.g. to do hyperparameter optimisation etc.) and ultimately deploy models (e.g. as docker images R + plumber - Python + FASt API etc.). Coming across code like this (run in sagemaker notebooks):<\/p>\n<pre><code>estimator = sagemaker.estimator.Estimator(\n    role=role_arn,\n    instance_count=1,\n    instance_type='ml.m5.xlarge',\n    image_uri=container,\n    debugger_hook_config=debugger_hook_config,\n    rules=rules,\n    sagemaker_session=session)\n<\/code><\/pre>\n<p>which is <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html\" rel=\"nofollow noreferrer\">SageMaker Python SDK<\/a> code. Can I also execute everything locally via Visual Studio Code? I understand that SDKs like this are a constant moving target but can I use this and are there any books\/tutorials (Google did not return much). Thanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1642342574523,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":879.0,
        "Answer_body":"<p>You can. Use <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/use-the-amazon-sagemaker-local-mode-to-train-on-your-notebook-instance\/\" rel=\"nofollow noreferrer\">SageMaker Local<\/a> to execute training and inference locally in docker containers on your laptop.<br \/>\nIn the Estimator, you'll specify: <code>instance_type='local'<\/code> or <code>instance_type='local_gpu'<\/code><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70730897",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1642413829196,
        "Question_original_content":"sage maker visual studio code local instead notebook sdk prefer us local laptop cost reason have id like visual studio code stand depart abl reluct setup databas connect data local laptop investig ssh unwil open network connect issu laptop window time time like us cloud hyperparamet optimis ultim deploi model docker imag plumber python fast api come code like run notebook estim estim estim role role arn instanc count instanc type xlarg imag uri contain debugg hook config debugg hook config rule rule session session python sdk code execut local visual studio code understand sdk like constant move target us book tutori googl return thank",
        "Question_preprocessed_content":"sage maker visual studio code local instead notebook sdk prefer us local laptop cost reason have id like visual studio code stand depart setup databas connect investig ssh unwil open network connect issu laptop time time us cloud ultim deploi model come code like python sdk code execut local visual studio code understand sdk like constant move target us thank",
        "Question_gpt_summary_original":"The user prefers to use their local laptop for cost and IDE reasons, but their IT department is unwilling to set up database connectivity for cloud services like SageMaker. The user's laptop only has 8GB of memory, so they need to use the cloud for tasks like hyperparameter optimization and model deployment. The user is wondering if they can execute SageMaker Python SDK code locally using Visual Studio Code and is looking for resources to help them do so.",
        "Question_gpt_summary":"user prefer us local laptop cost id reason depart unwil set databas connect cloud servic like user laptop memori need us cloud task like hyperparamet optim model deploy user wonder execut python sdk code local visual studio code look resourc help",
        "Answer_original_content":"us local execut train infer local docker contain laptop estim specifi instanc type local instanc type local gpu",
        "Answer_preprocessed_content":"us local execut train infer local docker contain laptop estim specifi",
        "Answer_gpt_summary_original":"Solution: The user can use SageMaker Local to execute training and inference locally in docker containers on their laptop. They can specify \"instance_type='local'\" or \"instance_type='local_gpu'\" in the Estimator.",
        "Answer_gpt_summary":"solut user us local execut train infer local docker contain laptop specifi instanc type local instanc type local gpu estim"
    },
    {
        "Question_title":"how to deploy a xgboost model on amazon sagemaker?",
        "Question_body":"<p>Is there a way to deploy a xgboost model trained locally using amazon sagemaker? I only saw tutorial talking about both training and deploying model with amazon sagemaker.\nThanks.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1531756686157,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":1641.0,
        "Answer_body":"<p>This <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/d5681a07611ae29567355b60b2f22500b561218b\/advanced_functionality\/xgboost_bring_your_own_model\/xgboost_bring_your_own_model.ipynb\" rel=\"nofollow noreferrer\">example notebook<\/a> is good starting point showing how to use a pre-existing scikit-learn xgboost model with the Amazon SageMaker to create a hosted endpoint for that model.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51365850",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1531778753543,
        "Question_original_content":"deploi xgboost model wai deploi xgboost model train local saw tutori talk train deploi model thank",
        "Question_preprocessed_content":"deploi xgboost model wai deploi xgboost model train local saw tutori talk train deploi model thank",
        "Question_gpt_summary_original":"The user is facing a challenge in deploying a locally trained xgboost model on Amazon Sagemaker and is seeking guidance on how to do so. They have only found tutorials that cover both training and deployment on Sagemaker.",
        "Question_gpt_summary":"user face challeng deploi local train xgboost model seek guidanc tutori cover train deploy",
        "Answer_original_content":"exampl notebook good start point show us pre exist scikit learn xgboost model creat host endpoint model",
        "Answer_preprocessed_content":"exampl notebook good start point show us xgboost model creat host endpoint model",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to an example notebook that demonstrates how to use a pre-existing scikit-learn xgboost model with Amazon SageMaker to create a hosted endpoint for that model.",
        "Answer_gpt_summary":"solut discuss provid link exampl notebook demonstr us pre exist scikit learn xgboost model creat host endpoint model"
    },
    {
        "Question_title":"Can I use AWS Sagemaker without S3",
        "Question_body":"<p>If I am not using the notebook on AWS but instead just the Sagemaker CLI and want to train a model, can I specify a local path to read from and write to?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1530312595250,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":826.0,
        "Answer_body":"<p>If you use local mode with the SageMaker Python SDK, you can train using local data:<\/p>\n\n<pre><code>from sagemaker.mxnet import MXNet\n\nmxnet_estimator = MXNet('train.py',\n                        train_instance_type='local',\n                        train_instance_count=1)\n\nmxnet_estimator.fit('file:\/\/\/tmp\/my_training_data')\n<\/code><\/pre>\n\n<p>However, this only works if you are training a model locally, not on SageMaker. If you want to train on SageMaker, then yes, you do need to use S3.<\/p>\n\n<p>For more about local mode: <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode\" rel=\"noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk#local-mode<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51110274",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1530570724340,
        "Question_original_content":"us notebook aw instead cli want train model specifi local path read write",
        "Question_preprocessed_content":"us notebook aw instead cli want train model specifi local path read write",
        "Question_gpt_summary_original":"The user is facing a challenge of whether they can use AWS Sagemaker without S3 by specifying a local path to read from and write to while training a model using the Sagemaker CLI instead of the notebook on AWS.",
        "Question_gpt_summary":"user face challeng us specifi local path read write train model cli instead notebook aw",
        "Answer_original_content":"us local mode python sdk train local data mxnet import mxnet mxnet estim mxnet train train instanc type local train instanc count mxnet estim fit file tmp train data work train model local want train ye need us local mode http github com aw python sdk local mode",
        "Answer_preprocessed_content":"us local mode python sdk train local data work train model local want train ye need us local mode",
        "Answer_gpt_summary_original":"Solution: The user can use local mode with the SageMaker Python SDK to train using local data by specifying a local path to read from and write to. However, this only works if the user is training a model locally, not on SageMaker. If the user wants to train on SageMaker, then they need to use S3.",
        "Answer_gpt_summary":"solut user us local mode python sdk train local data specifi local path read write work user train model local user want train need us"
    },
    {
        "Question_title":"How do I deploy the run after retraining a published endpoint and consume it?",
        "Question_body":"I have created an ML model and created a real time endpoint with the model and also published the pipeline. I retrained it with a different parameter and ran the experiment. Also I have published the endpoint. Now how do I deploy it or replace it with the already created endpoint?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1612959835657,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, this document provides information on how to update a web service that was deployed with Azure Machine Learning. Hope this helps.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/267341\/how-do-i-deploy-the-run-after-retraining-a-publish.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1612984587887,
        "Question_original_content":"deploi run retrain publish endpoint consum creat model creat real time endpoint model publish pipelin retrain differ paramet ran experi publish endpoint deploi replac creat endpoint",
        "Question_preprocessed_content":"deploi run retrain publish endpoint consum creat model creat real time endpoint model publish pipelin retrain differ paramet ran experi publish endpoint deploi replac creat endpoint",
        "Question_gpt_summary_original":"The user has encountered challenges in deploying and consuming a retrained ML model. They have created a real-time endpoint and published a pipeline, but are unsure how to deploy the retrained model or replace the existing endpoint with the updated one.",
        "Question_gpt_summary":"user encount challeng deploi consum retrain model creat real time endpoint publish pipelin unsur deploi retrain model replac exist endpoint updat",
        "Answer_original_content":"document provid inform updat web servic deploi hope help",
        "Answer_preprocessed_content":"document provid inform updat web servic deploi hope help",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to a document that explains how to update a web service that was deployed with Azure Machine Learning.",
        "Answer_gpt_summary":"solut discuss provid link document explain updat web servic deploi"
    },
    {
        "Question_title":"Save_period Not Working",
        "Question_body":"<p>I\u2019m trying to train a model, but I keep receiving an error that tells me \u201ctrain.py: error: unrecognized arguments: --save_period 1.\u201d<br>\n<div class=\"lightbox-wrapper\"><a class=\"lightbox\" href=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755.png\" data-download-href=\"\/uploads\/short-url\/8oz1v5B3P0W95o6FWMDht4yfVLD.png?dl=1\" title=\"image\" rel=\"noopener nofollow ugc\"><img src=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_690x337.png\" alt=\"image\" data-base62-sha1=\"8oz1v5B3P0W95o6FWMDht4yfVLD\" width=\"690\" height=\"337\" srcset=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_690x337.png, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_1035x505.png 1.5x, https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/original\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755.png 2x\" data-small-upload=\"https:\/\/global.discourse-cdn.com\/business7\/uploads\/wandb\/optimized\/1X\/3ad842158d3fe6c02426800cceb90f785a60b755_2_10x10.png\"><div class=\"meta\">\n<svg class=\"fa d-icon d-icon-far-image svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#far-image\"><\/use><\/svg><span class=\"filename\">image<\/span><span class=\"informations\">1326\u00d7648 66 KB<\/span><svg class=\"fa d-icon d-icon-discourse-expand svg-icon\" aria-hidden=\"true\"><use xlink:href=\"#discourse-expand\"><\/use><\/svg>\n<\/div><\/a><\/div><br>\nWhat issue do I have here?  Thanks in advance.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1636684136179,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":217.0,
        "Answer_body":"<p>I think you have  a typo. According to the usage info, the argument name is <code>--save-period<\/code>  , not <code>--save_period<\/code><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/save-period-not-working\/1264",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1636785650863,
        "Question_original_content":"save period work try train model receiv error tell train error unrecogn argument save period imag issu thank advanc",
        "Question_preprocessed_content":"work try train model receiv error tell error unrecogn argument imag issu thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error while trying to train a model, specifically with the \"--save_period 1\" argument. They are seeking assistance in resolving the issue.",
        "Question_gpt_summary":"user encount error try train model specif save period argument seek assist resolv issu",
        "Answer_original_content":"think typo accord usag info argument save period save period",
        "Answer_preprocessed_content":"think typo accord usag info argument",
        "Answer_gpt_summary_original":"Solution: The user may have a typo in their argument name. The correct argument name is \"--save-period\" instead of \"--save_period\".",
        "Answer_gpt_summary":"solut user typo argument correct argument save period instead save period"
    },
    {
        "Question_title":"Ambiguous error in Azure Machine Learning Designer 'Evaluate Model' Module",
        "Question_body":"I am getting the following error from the Evaluate Model module in Azure Machine Learning Designer:\n\n\nWhen I open the Assigned Data to Clusters module everything seems fine. I downloaded the output for Assigned Data to Clusters and played with cluster number 31 and there doesn't seem to be any issue. Additionally, I am using Azure Modules, so I am confused as to why this is failing. Please provide some clarity into this issue. This is a part of my pipeline:\n\nAdditionally, it seems unless I successfully run the Evaluate Model module, I cannot create an inference pipeline. If this is untrue, please help me out here as well. There is no option for me to 'Create an Inference Pipeline' which shown in this tutorial; step 1.\n\nPlease let me know if you need any other information.\n\nThanks in advance.",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1622567822803,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Can you please check if the Assignment cluster 31 has NaN value? The Assign Data to Clusters leverages SKlearn, and from the error message, seems the Assignment column had NaN value which resulted in an error. If that's the case, let us know, so we can enable Evaluate Module module to deal with NaN values, and in the meantime, here's a short-term workaround:\n\nConnect Clean Missing Data module to Assign Data to Cluster module, to clean the missing values.\n\n\n\nUse Edit Metadata module to convert Assignment to Integer and categorical type, this is because if Assignment column has NaN value before and its column type was double, we need to convert it to integer.\n\n\n\nConnect Edit Metadata to Evaluate Model module.\n\n\n\n\nHope this help!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/418016\/ambiguous-error-in-azure-machine-learning-designer.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1623450957476,
        "Question_original_content":"ambigu error design evalu model modul get follow error evalu model modul design open assign data cluster modul fine download output assign data cluster plai cluster number issu addition azur modul confus fail provid clariti issu pipelin addition successfulli run evalu model modul creat infer pipelin untru help option creat infer pipelin shown tutori step let know need inform thank advanc",
        "Question_preprocessed_content":"ambigu error design evalu model modul get follow error evalu model modul design open assign data cluster modul fine download output assign data cluster plai cluster number issu addition azur modul confus fail provid clariti issu pipelin addition successfulli run evalu model modul creat infer pipelin untru help option creat infer pipelin shown tutori step let know need inform thank advanc",
        "Question_gpt_summary_original":"The user is encountering an ambiguous error in the 'Evaluate Model' module of Azure Machine Learning Designer. They have checked the output of the 'Assigned Data to Clusters' module and found no issues. The user is confused as to why this error is occurring since they are using Azure Modules. Additionally, they are unable to create an inference pipeline unless the 'Evaluate Model' module runs successfully. The user is seeking clarity on these issues.",
        "Question_gpt_summary":"user encount ambigu error evalu model modul design check output assign data cluster modul issu user confus error occur azur modul addition unabl creat infer pipelin evalu model modul run successfulli user seek clariti issu",
        "Answer_original_content":"check assign cluster nan valu assign data cluster leverag sklearn error messag assign column nan valu result error case let know enabl evalu modul modul deal nan valu meantim short term workaround connect clean miss data modul assign data cluster modul clean miss valu us edit metadata modul convert assign integ categor type assign column nan valu column type doubl need convert integ connect edit metadata evalu model modul hope help",
        "Answer_preprocessed_content":"check assign cluster nan valu assign data cluster leverag sklearn error messag assign column nan valu result error case let know enabl evalu modul modul deal nan valu meantim workaround connect clean miss data modul assign data cluster modul clean miss valu us edit metadata modul convert assign integ categor type assign column nan valu column type doubl need convert integ connect edit metadata evalu model modul hope help",
        "Answer_gpt_summary_original":"Solution:\n\n- Check if the Assignment cluster 31 has NaN value. If yes, enable the Evaluate Module module to deal with NaN values.\n- Use a short-term workaround by connecting the Clean Missing Data module to Assign Data to Cluster module to clean the missing values.\n- Use the Edit Metadata module to convert Assignment to Integer and categorical type. \n- Connect Edit Metadata to Evaluate Model module.",
        "Answer_gpt_summary":"solut check assign cluster nan valu ye enabl evalu modul modul deal nan valu us short term workaround connect clean miss data modul assign data cluster modul clean miss valu us edit metadata modul convert assign integ categor type connect edit metadata evalu model modul"
    },
    {
        "Question_title":"Sagemaker batch transform 415 error",
        "Question_body":"Hi, I need to run XGBoost inferences on 15MM samples (3.9Gb when stored as csv). Since Batch transform does not seem to work on such large batches (max payload 100MB) I split my input file into 646 files, each around 6Mb, stored in S3. I am running the code below:\n\ntransformer = XGB.transformer(\n    instance_count=2, instance_type='ml.c5.9xlarge',\n    output_path='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/xgbtransform\/',\n    max_payload=100)\n\ntransformer.transform(\n    data='s3:\/\/xxxxxxxxxxxxx\/sagemaker\/recsys\/testchunks\/',\n    split_type='Line')\n\n\nBut the job fails - Sagemaker tells \"ClientError: Too many objects failed. See logs for more information\" and cloudwatch logs show:\n\nBad HTTP status returned from invoke: 415\n'NoneType' object has no attribute 'lower'\n\n\nDid I forget something in my batch transform settings?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1532625720000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":424.0,
        "Answer_body":"This indicates that the algorithm thinks it has been passed bad data. Perhaps a problem with your splitting?\n\nI would suggest two things:\n\nTry running the algorithm on the original data using the \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\" arguments and see if you have better luck.\nLook in the cloudwatch logs for your run and see if there's any helpful information about what the algorithm didn't like. You can find these in the log group \"\/aws\/sagemaker\/TransformJobs\" in the log stream that begins with your job name.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUr4Vq95ScROqSguzxNQYDOg\/sagemaker-batch-transform-415-error",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1532634125000,
        "Question_original_content":"batch transform error need run xgboost infer sampl store csv batch transform work larg batch max payload split input file file store run code transform xgb transform instanc count instanc type xlarg output path recsi xgbtransform max payload transform transform data recsi testchunk split type line job fail tell clienterror object fail log inform cloudwatch log bad http statu return invok nonetyp object attribut lower forget batch transform set",
        "Question_preprocessed_content":"batch transform error need run xgboost infer sampl batch transform work larg batch split input file file store run code transform job fail tell clienterror object fail log inform cloudwatch log bad http statu return invok nonetyp object attribut lower forget batch transform set",
        "Question_gpt_summary_original":"The user is facing challenges while running XGBoost inferences on 15MM samples using Sagemaker batch transform. The user split the input file into 646 files, each around 6Mb, stored in S3. However, the job fails with a \"ClientError: Too many objects failed. See logs for more information\" and cloudwatch logs show \"Bad HTTP status returned from invoke: 415 'NoneType' object has no attribute 'lower'\". The user is seeking help to identify if there is any issue with the batch transform settings.",
        "Question_gpt_summary":"user face challeng run xgboost infer sampl batch transform user split input file file store job fail clienterror object fail log inform cloudwatch log bad http statu return invok nonetyp object attribut lower user seek help identifi issu batch transform set",
        "Answer_original_content":"indic algorithm think pass bad data problem split suggest thing try run algorithm origin data splittyp line batchstrategi multirecord argument better luck look cloudwatch log run help inform algorithm like log group aw transformjob log stream begin job",
        "Answer_preprocessed_content":"indic algorithm think pass bad data problem split suggest thing try run algorithm origin data splittyp line batchstrategi multirecord argument better luck look cloudwatch log run help inform algorithm like log group log stream begin job",
        "Answer_gpt_summary_original":"Solutions provided:\n- Try running the algorithm on the original data using the \"SplitType\": \"Line\" and \"BatchStrategy\": \"MultiRecord\" arguments.\n- Look in the cloudwatch logs for the run and see if there's any helpful information about what the algorithm didn't like.",
        "Answer_gpt_summary":"solut provid try run algorithm origin data splittyp line batchstrategi multirecord argument look cloudwatch log run help inform algorithm like"
    },
    {
        "Question_title":"Erro InvalidInputDatatype: Input of type 'Unknown' is not supported in azure (azureml.train.automl)",
        "Question_body":"<p>I have a pandas's DataFrame created by:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>TB_HISTORICO_MODELO = pd.read_sql(&quot;&quot;&quot;select DAT_INICIO_SEMANA_PLAN\n,COD_NEGOCIO\n,VENDA\n,LUCRO\n,MODULADO\n,RUPTURA\n,QTD_ESTOQUE_MEDIO\n,PECAS from TB&quot;&quot;&quot;, cursor)\n\nTB_HISTORICO_MODELO[&quot;DAT_INICIO_SEMANA_PLAN&quot;] = pd.to_datetime(TB_HISTORICO_MODELO[&quot;DAT_INICIO_SEMANA_PLAN&quot;])\n\ndataset = TB_HISTORICO_MODELO[TB_HISTORICO_MODELO['COD_NEGOCIO']=='A101'].drop(columns=['COD_NEGOCIO']) .reset_index(drop=True)\n<\/code><\/pre>\n<p>Everything look like right.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; dataset.dtypes\nDAT_INICIO_SEMANA_PLAN    datetime64[ns]\nVENDA                            float64\nLUCRO                            float64\nMODULADO                           int64\nRUPTURA                            int64\nQTD_ESTOQUE_MEDIO                  int64\nPECAS                            float64\ndtype: object\n<\/code><\/pre>\n<p>But when I rum this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>#%% Create the AutoML Config file and run the experiment on Azure\n\nfrom azureml.train.automl import AutoMLConfig\n\ntime_series_settings = {\n   'time_column_name': 'DAT_INICIO_SEMANA_PLAN',\n   'max_horizon': 14,\n   'country_or_region': 'BR',\n   'target_lags': 'auto'\n}\n\nautoml_config = AutoMLConfig(task='forecasting',\n                            primary_metric='normalized_root_mean_squared_error',\n                            blocked_models=['ExtremeRandomTrees'],\n                            experiment_timeout_minutes=30,\n                            training_data=dataset,\n                            label_column_name='VENDA',\n                            compute_target = compute_cluster,\n                            enable_early_stopping=True,\n                            n_cross_validations=3,\n                            # max_concurrent_iterations=4,\n                            # max_cores_per_iteration=-1,\n                            verbosity=logging.INFO,\n                            **time_series_settings)\n\nremote_run = Experimento.submit(automl_config, show_output=True)\n<\/code><\/pre>\n<p>I get the message<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>&gt;&gt;&gt; remote_run = Experimento.submit(automl_config, show_output=True)\nTraceback (most recent call last):\n  File &quot;&lt;stdin&gt;&quot;, line 1, in &lt;module&gt;\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/core\/experiment.py&quot;, line 219, in submit\n    run = submit_func(config, self.workspace, self.name, **kwargs)\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py&quot;, line 92, in _automl_static_submit\n    automl_config_object._validate_config_settings(workspace)\n  File &quot;\/home\/fnord\/venv\/lib64\/python3.6\/site-packages\/azureml\/train\/automl\/automlconfig.py&quot;, line 1775, in _validate_config_settings\n    supported_types=&quot;, &quot;.join(SupportedInputDatatypes.REMOTE_RUN_SCENARIO)\nazureml.train.automl.exceptions.ConfigException: ConfigException:\n        Message: Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]\n        InnerException: None\n        ErrorResponse \n{\n    &quot;error&quot;: {\n        &quot;code&quot;: &quot;UserError&quot;,\n        &quot;message&quot;: &quot;Input of type 'Unknown' is not supported. Supported types: [azureml.data.tabular_dataset.TabularDataset, azureml.pipeline.core.pipeline_output_dataset.PipelineOutputTabularDataset]&quot;,\n        &quot;details_uri&quot;: &quot;https:\/\/aka.ms\/AutoMLConfig&quot;,\n        &quot;target&quot;: &quot;training_data&quot;,\n        &quot;inner_error&quot;: {\n            &quot;code&quot;: &quot;BadArgument&quot;,\n            &quot;inner_error&quot;: {\n                &quot;code&quot;: &quot;ArgumentInvalid&quot;,\n                &quot;inner_error&quot;: {\n                    &quot;code&quot;: &quot;InvalidInputDatatype&quot;\n                }\n            }\n        }\n    }\n}\n\n<\/code><\/pre>\n<p>Where is wrong?<\/p>\n<p>documentation:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/pt-br\/python\/api\/azureml-train-automl-client\/azureml.train.automl.automlconfig.automlconfig<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1602873387197,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":382.0,
        "Answer_body":"<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-configure-auto-train#data-source-and-format?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">Configure AutoML Doc<\/a> says:<\/p>\n<blockquote>\n<p>For remote experiments, training data must be accessible from the remote compute. AutoML only accepts Azure Machine Learning TabularDatasets when working on a remote compute.<\/p>\n<\/blockquote>\n<p>It looks as if your <code>dataset<\/code> object is a Pandas DataFrame, when it should really be an Azure ML <code>Dataset<\/code>. Check out <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-register-datasets?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">this doc<\/a> on creating Datasets.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1603005259150,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64394661",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1603004866532,
        "Question_original_content":"erro invalidinputdatatyp input type unknown support azur train automl panda datafram creat historico modelo read sql select dat inicio semana plan cod negocio venda lucro modulado ruptura qtd estoqu medio peca cursor historico modelo dat inicio semana plan datetim historico modelo dat inicio semana plan dataset historico modelo historico modelo cod negocio drop column cod negocio reset index drop true look like right dataset dtype dat inicio semana plan datetim venda float lucro float modulado int ruptura int qtd estoqu medio int peca float dtype object rum creat automl config file run experi azur train automl import automlconfig time seri set time column dat inicio semana plan max horizon countri region target lag auto automl config automlconfig task forecast primari metric normal root mean squar error block model extremerandomtre experi timeout minut train data dataset label column venda comput target comput cluster enabl earli stop true cross valid max concurr iter max core iter verbos log info time seri set remot run experimento submit automl config output true messag remot run experimento submit automl config output true traceback recent file line file home fnord venv lib python site packag core experi line submit run submit func config self workspac self kwarg file home fnord venv lib python site packag train automl automlconfig line automl static submit automl config object valid config set workspac file home fnord venv lib python site packag train automl automlconfig line valid config set support type join supportedinputdatatyp remot run scenario train automl except configexcept configexcept messag input type unknown support support type data tabular dataset tabulardataset pipelin core pipelin output dataset pipelineoutputtabulardataset innerexcept errorrespons error code usererror messag input type unknown support support type data tabular dataset tabulardataset pipelin core pipelin output dataset pipelineoutputtabulardataset detail uri http aka automlconfig target train data inner error code badargu inner error code argumentinvalid inner error code invalidinputdatatyp wrong document http doc microsoft com azur machin learn configur auto train http doc microsoft com python api train automl client train automl automlconfig automlconfig",
        "Question_preprocessed_content":"erro invalidinputdatatyp input type unknown support azur panda datafram creat look like right rum messag wrong document",
        "Question_gpt_summary_original":"The user is encountering an error message when trying to run an experiment on Azure using AutoMLConfig. The error message states that the input type 'Unknown' is not supported and provides a list of supported input types. The user is unsure of what is causing the error and is seeking assistance.",
        "Question_gpt_summary":"user encount error messag try run experi azur automlconfig error messag state input type unknown support provid list support input type user unsur caus error seek assist",
        "Answer_original_content":"configur automl doc sai remot experi train data access remot comput automl accept tabulardataset work remot comput look dataset object panda datafram dataset check doc creat dataset",
        "Answer_preprocessed_content":"configur automl doc sai remot experi train data access remot comput automl accept tabulardataset work remot comput look object panda datafram check doc creat dataset",
        "Answer_gpt_summary_original":"Solution: The error message is caused by using a Pandas DataFrame as the dataset object instead of an Azure ML Dataset. The solution is to create an Azure ML Dataset and use it as the input for the experiment. The user can refer to the provided documentation on creating and registering datasets for guidance.",
        "Answer_gpt_summary":"solut error messag caus panda datafram dataset object instead dataset solut creat dataset us input experi user refer provid document creat regist dataset guidanc"
    },
    {
        "Question_title":"Azure Machine Learning Studio: cannot create Datastore from Azure SQL Database",
        "Question_body":"<p>I am trying to connect to an Azure SQL Database from inside Azure Machine Learning Studio. Based on <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py<\/a>, it seems that the recommended pattern is to create a Datastore using the Datastore.register_azure_sql_database method as follows:<\/p>\n<pre><code>import os\nfrom azureml.core import Workspace, Datastore\n\nws = Workspace.from_config() # asks for interactive authentication the first time\n\nsql_datastore_name  = &quot;datastore_test_01&quot; # any name should be fine\nserver_name         = os.getenv(&quot;SQL_SERVERNAME&quot;    , &quot;{SQL_SERVERNAME}&quot;) # Name of the Azure SQL server\ndatabase_name       = os.getenv(&quot;SQL_DATABASENAME&quot;  , &quot;{SQL_DATABASENAME}&quot;) # Name of the Azure SQL database\nusername            = os.getenv(&quot;SQL_USER_NAME&quot;     , &quot;{SQL_USER_NAME}&quot;) # The username of the database user.\npassword            = os.getenv(&quot;SQL_USER_PASSWORD&quot; , &quot;{SQL_USER_PASSWORD}&quot;) # The password of the database user.\n\nsql_datastore = Datastore.register_azure_sql_database(workspace      = ws,\n                                                      datastore_name = sql_datastore_name,\n                                                      server_name    = server_name,\n                                                      database_name  = database_name,\n                                                      username       = username,\n                                                      password       = password)\n<\/code><\/pre>\n<p>I am pretty sure I have set all parameters right, having copied them from the ADO.NET connection string at my SQL Database resource --&gt; Settings --&gt; Connection strings:<\/p>\n<pre><code>Server=tcp:{SQL_SERVERNAME}.database.windows.net,1433;Initial Catalog={SQL_DATABASENAME};Persist Security Info=False;User ID={SQL_USER_NAME};Password={SQL_USER_PASSWORD};MultipleActiveResultSets=False;Encrypt=True;TrustServerCertificate=False;Connection Timeout=30;\n<\/code><\/pre>\n<p>However, I get the following error:<\/p>\n<pre><code>Registering datastore failed with a 400 error code and error message 'Azure SQL Database Error -2146232060: Please check the correctness of the datastore information.'\n<\/code><\/pre>\n<p>Am I missing something? E.g., a firewall rule? I have also tried adding the Azure ML compute resource's public IP address to the list of allowed IP addresses in my SQL Database resource, but still no success.<\/p>\n<hr \/>\n<p><strong>UPDATE<\/strong>: adding <code>skip_validation = True<\/code> to <code>Datastore.register_azure_sql_database<\/code> solves the issue. I can then query the data with<\/p>\n<pre><code>from azureml.core import Dataset\nfrom azureml.data.datapath import DataPath\n\nquery   = DataPath(sql_datastore, 'SELECT * FROM my_table')\ntabular = Dataset.Tabular.from_sql_query(query, query_timeout = 10)\ndf = tabular.to_pandas_dataframe()\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1598976793243,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1599032818923,
        "Question_score":1.0,
        "Question_view_count":793.0,
        "Answer_body":"<p>is the datastore behind vnet? where are you running the registration code above? On a compute instance behind the same vnet?\nhere is the doc that describe what you need to do to connect to data behind vnet:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-enable-virtual-network#use-datastores-and-datasets<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63691515",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1598979186260,
        "Question_original_content":"studio creat datastor azur sql databas try connect azur sql databas insid studio base http doc microsoft com python api core core datastor datastor view azur recommend pattern creat datastor datastor regist azur sql databas method follow import core import workspac datastor workspac config ask interact authent time sql datastor datastor test fine server getenv sql servernam sql servernam azur sql server databas getenv sql databasenam sql databasenam azur sql databas usernam getenv sql user sql user usernam databas user password getenv sql user password sql user password password databas user sql datastor datastor regist azur sql databas workspac datastor sql datastor server server databas databas usernam usernam password password pretti sure set paramet right have copi ado net connect string sql databas resourc set connect string server tcp sql servernam databas window net initi catalog sql databasenam persist secur info fals user sql user password sql user password multipleactiveresultset fals encrypt true trustservercertif fals connect timeout follow error regist datastor fail error code error messag azur sql databas error check correct datastor inform miss firewal rule tri ad comput resourc public address list allow address sql databas resourc success updat ad skip valid true datastor regist azur sql databas solv issu queri data core import dataset data datapath import datapath queri datapath sql datastor select tabl tabular dataset tabular sql queri queri queri timeout tabular panda datafram",
        "Question_preprocessed_content":"studio creat datastor azur sql databas try connect azur sql databas insid studio base recommend pattern creat datastor method follow pretti sure set paramet right have copi connect string sql databas resourc set connect string follow error miss firewal rule tri ad comput resourc public address list allow address sql databas resourc success updat ad solv issu queri data",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to connect to an Azure SQL Database from inside Azure Machine Learning Studio. The user is following the recommended pattern to create a Datastore using the Datastore.register_azure_sql_database method, but is receiving a 400 error code and error message 'Azure SQL Database Error -2146232060: Please check the correctness of the datastore information.' The user has tried adding the Azure ML compute resource's public IP address to the list of allowed IP addresses in the SQL Database resource, but still no success. The user has found a solution by adding skip_validation = True to Datastore.register_azure_sql_database.",
        "Question_gpt_summary":"user encount challeng try connect azur sql databas insid studio user follow recommend pattern creat datastor datastor regist azur sql databas method receiv error code error messag azur sql databas error check correct datastor inform user tri ad comput resourc public address list allow address sql databas resourc success user solut ad skip valid true datastor regist azur sql databas",
        "Answer_original_content":"datastor vnet run registr code comput instanc vnet doc need connect data vnet http doc microsoft com azur machin learn enabl virtual network us datastor dataset",
        "Answer_preprocessed_content":"datastor vnet run registr code comput instanc vnet doc need connect data vnet",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to check if the datastore is behind a virtual network and if the registration code is being run on a compute instance behind the same virtual network. If the datastore is behind a virtual network, the user can follow the instructions provided in the documentation to enable virtual network and use datastores and datasets. Another solution mentioned is to add skip_validation = True to Datastore.register_azure_sql_database.",
        "Answer_gpt_summary":"solut possibl solut mention discuss check datastor virtual network registr code run comput instanc virtual network datastor virtual network user follow instruct provid document enabl virtual network us datastor dataset solut mention add skip valid true datastor regist azur sql databas"
    },
    {
        "Question_title":"Unable to use GPU to train a NN model in azure machine learning service using P100-NC6s-V2 compute. Fails wth CUDA error",
        "Question_body":"<p>I\u2019ve recently started working with azure for ML and am trying to use machine learning service workspace.\nI\u2019ve set up a workspace with the compute set to NC6s-V2 machines since I need train a NN using images on GPU. <\/p>\n\n<p>The issue is that the training still happens on the CPU \u2013 the logs say it\u2019s not able to find CUDA. Here\u2019s the warning log when running my script.\nAny clues how to solve this issue?<\/p>\n\n<p>I\u2019ve also mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. <\/p>\n\n<p>Here's my code for the estimator,<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>script_params = {\n         '--input_data_folder': ds.path('dataset').as_mount(),\n         '--zip_file_name': 'train.zip',\n         '--run_mode': 'train'\n    }\n\n\nest = Estimator(source_directory='.\/scripts',\n                     script_params=script_params,\n                     compute_target=compute_target,\n                     entry_script='main.py',\n                     conda_packages=['scikit-image', 'keras', 'tqdm', 'pillow', 'matplotlib', 'scipy', 'tensorflow-gpu']\n                     )\n\nrun = exp.submit(config=est)\n\nrun.wait_for_completion(show_output=True)\n<\/code><\/pre>\n\n<p>The compute target was made as per the sample code on github:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>compute_name = \"P100-NC6s-V2\"\ncompute_min_nodes = 0\ncompute_max_nodes = 4\n\nvm_size = \"STANDARD_NC6S_V2\"\n\nif compute_name in ws.compute_targets:\n    compute_target = ws.compute_targets[compute_name]\n    if compute_target and type(compute_target) is AmlCompute:\n        print('found compute target. just use it. ' + compute_name)\nelse:\n    print('creating a new compute target...')\n    provisioning_config = AmlCompute.provisioning_configuration(vm_size=vm_size,\n                                                                min_nodes=compute_min_nodes,\n                                                                max_nodes=compute_max_nodes)\n\n    # create the cluster\n    compute_target = ComputeTarget.create(\n        ws, compute_name, provisioning_config)\n\n    # can poll for a minimum number of nodes and for a specific timeout.\n    # if no min node count is provided it will use the scale settings for the cluster\n    compute_target.wait_for_completion(\n        show_output=True, min_node_count=None, timeout_in_minutes=20)\n\n    # For a more detailed view of current AmlCompute status, use get_status()\n    print(compute_target.get_status().serialize())\n\n<\/code><\/pre>\n\n<p>This is the warning with which it fails to use the GPU:<\/p>\n\n<pre><code>2019-08-12 14:50:16.961247: I tensorflow\/compiler\/xla\/service\/service.cc:168] XLA service 0x55a7ce570830 executing computations on platform Host. Devices:\n2019-08-12 14:50:16.961278: I tensorflow\/compiler\/xla\/service\/service.cc:175]   StreamExecutor device (0): &lt;undefined&gt;, &lt;undefined&gt;\n2019-08-12 14:50:16.971025: I tensorflow\/stream_executor\/platform\/default\/dso_loader.cc:53] Could not dlopen library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: \/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/intel64\/lib:\/opt\/intel\/compilers_and_libraries_2018.3.222\/linux\/mpi\/mic\/lib:\/azureml-envs\/azureml_5fdf05c5671519f307e0f43128b8610e\/lib:\n2019-08-12 14:50:16.971054: E tensorflow\/stream_executor\/cuda\/cuda_driver.cc:318] failed call to cuInit: UNKNOWN ERROR (303)\n2019-08-12 14:50:16.971081: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971089: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:176] hostname: 4bd815dfb0e74e3da901861a4746184f000000\n2019-08-12 14:50:16.971164: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:200] libcuda reported version is: Not found: was unable to find libcuda.so DSO loaded into this program\n2019-08-12 14:50:16.971202: I tensorflow\/stream_executor\/cuda\/cuda_diagnostics.cc:204] kernel reported version is: 418.40.4\nDevice mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n2019-08-12 14:50:16.973301: I tensorflow\/core\/common_runtime\/direct_session.cc:296] Device mapping:\n\/job:localhost\/replica:0\/task:0\/device:XLA_CPU:0 -&gt; device: XLA_CPU device\n\n<\/code><\/pre>\n\n<p>It's currently using the CPU as per the logs. Any clues how to resolve the issue here?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1565670597500,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1565670701316,
        "Question_score":5.0,
        "Question_view_count":1402.0,
        "Answer_body":"<p>Instead of base Estimator, you can use the Tensorflow Estimator with Keras and other libraries layered on top. That way you don't have to worry about setting up and configuring the GPU libraries, as the Tensorflow Estimator uses a Docker image with GPU libraries pre-configured. <\/p>\n\n<p>See here for documentation:<\/p>\n\n<p><a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-train-core\/azureml.train.dnn.tensorflow?view=azure-ml-py\" rel=\"nofollow noreferrer\">API Reference<\/a> You can use <code>conda_packages<\/code> argument to specify additional libraries. Also set argument <code>use_gpu = True<\/code>.<\/p>\n\n<p><a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/training-with-deep-learning\/train-hyperparameter-tune-deploy-with-keras\/train-hyperparameter-tune-deploy-with-keras.ipynb\" rel=\"nofollow noreferrer\">Example Notebook<\/a><\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57471129",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1565705412680,
        "Question_original_content":"unabl us gpu train model servic nc comput fail wth cuda error iv recent start work azur try us machin learn servic workspac iv set workspac comput set nc machin need train imag gpu issu train happen cpu log abl cuda here warn log run script clue solv issu iv mention explicitli tensorflow gpu packag conda packag option estim code estim script param input data folder path dataset mount zip file train zip run mode train est estim sourc directori script script param script param comput target comput target entri script main conda packag scikit imag kera tqdm pillow matplotlib scipi tensorflow gpu run exp submit config est run wait complet output true comput target sampl code github comput nc comput min node comput max node size standard nc comput comput target comput target comput target comput comput target type comput target amlcomput print comput target us comput print creat new comput target provis config amlcomput provis configur size size min node comput min node max node comput max node creat cluster comput target computetarget creat comput provis config poll minimum number node specif timeout min node count provid us scale set cluster comput target wait complet output true min node count timeout minut detail view current amlcomput statu us statu print comput target statu serial warn fail us gpu tensorflow compil xla servic servic xla servic xace execut comput platform host devic tensorflow compil xla servic servic streamexecutor devic tensorflow stream executor platform default dso loader dlopen librari libcuda dlerror libcuda open share object file file directori librari path opt intel compil librari linux mpi intel lib opt intel compil librari linux mpi mic lib opt intel compil librari linux mpi intel lib opt intel compil librari linux mpi mic lib env fdfcfefb lib tensorflow stream executor cuda cuda driver fail cuinit unknown error tensorflow stream executor cuda cuda diagnost retriev cuda diagnost inform host bddfbeedaaf tensorflow stream executor cuda cuda diagnost hostnam bddfbeedaaf tensorflow stream executor cuda cuda diagnost libcuda report version unabl libcuda dso load program tensorflow stream executor cuda cuda diagnost kernel report version devic map job localhost replica task devic xla cpu devic xla cpu devic tensorflow core common runtim direct session devic map job localhost replica task devic xla cpu devic xla cpu devic current cpu log clue resolv issu",
        "Question_preprocessed_content":"unabl us gpu train model servic comput fail wth cuda error iv recent start work azur try us machin learn servic workspac iv set workspac comput set machin need train imag gpu issu train happen cpu log abl cuda here warn log run script clue solv issu iv mention explicitli packag conda packag option estim code estim comput target sampl code github warn fail us gpu current cpu log clue resolv issu",
        "Question_gpt_summary_original":"The user is facing challenges in using GPU to train a neural network model in Azure Machine Learning Service using P100-NC6s-V2 compute. The training is happening on the CPU and the logs show that it's not able to find CUDA. The user has mentioned explicitly tensorflow-gpu package in the conda packages option of the estimator. The warning log shows that it fails to use the GPU and is currently using the CPU.",
        "Question_gpt_summary":"user face challeng gpu train neural network model servic nc comput train happen cpu log abl cuda user mention explicitli tensorflow gpu packag conda packag option estim warn log show fail us gpu current cpu",
        "Answer_original_content":"instead base estim us tensorflow estim kera librari layer wai worri set configur gpu librari tensorflow estim us docker imag gpu librari pre configur document api refer us conda packag argument specifi addit librari set argument us gpu true exampl notebook",
        "Answer_preprocessed_content":"instead base estim us tensorflow estim kera librari layer wai worri set configur gpu librari tensorflow estim us docker imag gpu librari document api refer us argument specifi addit librari set argument exampl notebook",
        "Answer_gpt_summary_original":"Solution: The user can use the Tensorflow Estimator with pre-configured GPU libraries instead of the base Estimator. They can specify additional libraries using the `conda_packages` argument and set `use_gpu=True`. Documentation and an example notebook are provided for reference.",
        "Answer_gpt_summary":"solut user us tensorflow estim pre configur gpu librari instead base estim specifi addit librari conda packag argument set us gpu true document exampl notebook provid refer"
    },
    {
        "Question_title":"PowerBI and MLflow integration (through AzureML)",
        "Question_body":"<p>I'm currently trying to integrate an ML model currently deployed as a webservice on AzureML with PowerBI.<\/p>\n<p>I see that it can be <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#invoking-the-azure-ml-model-in-power-bi\" rel=\"nofollow noreferrer\">integrated<\/a> but the model requires the addition of a schema file when it is <a href=\"https:\/\/docs.microsoft.com\/en-us\/power-bi\/transform-model\/service-machine-learning-integration#schema-discovery-for-machine-learning-models\" rel=\"nofollow noreferrer\">being deployed as a webservice<\/a>. Without this, the model can't be viewed in PowerBI.<\/p>\n<p>The problem that I have come up against is that I use MLflow to log ML model performances and subsequently to deploy a selected model onto AzureML as a webservice using MLflow's AzureML integration - mlflow.azureml.deploy(). This unfortunately doesn't have the option to define a schema file before the model is deployed, thus resulting in no model being available in PowerBI as it lacks the required schema file.<\/p>\n<p>My options seem to be:<\/p>\n<ol>\n<li>Find a workaround, possibly using the working <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/databricks\/applications\/mlflow\/model-serving\" rel=\"nofollow noreferrer\">REST api of the model in a power query<\/a>.<\/li>\n<li>Rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.<\/li>\n<\/ol>\n<p>I thought I would ask to see if I am maybe missing something as I can't find a workaround using my current code to define a schema file in MLflow when deploying with mlflow.azureml.deploy().<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1600261190477,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1600855880503,
        "Question_score":0.0,
        "Question_view_count":405.0,
        "Answer_body":"<p>Point number 2 is the way we solved this issue. Instead of using MLflow to deploy to a scoring service on Azure, we wrote a custom code which load MLflow model when container is initialised.<\/p>\n<p>Scoring code is something like this:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nfrom mlflow.pyfunc import load_model\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\n\ndef init():\n    global model\n    model = load_model(os.path.join(os.environ.get(&quot;AZUREML_MODEL_DIR&quot;), &quot;awesome_model&quot;))\n\n@input_schema('data', NumpyParameterType(input_sample))\n@output_schema(NumpyParameterType(output_sample))\n\ndef run(data):\n    return model.predict(data)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1600855957376,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63920599",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1600604920243,
        "Question_original_content":"powerbi integr current try integr model current deploi webservic powerbi integr model requir addit schema file deploi webservic model view powerbi problem come us log model perform subsequ deploi select model webservic integr deploi unfortun option defin schema file model deploi result model avail powerbi lack requir schema file option workaround possibl work rest api model power queri rewrit deploy code handl webservic deploy step azur instead thought ask mayb miss workaround current code defin schema file deploi deploi",
        "Question_preprocessed_content":"powerbi integr current try integr model current deploi webservic powerbi integr model requir addit schema file deploi webservic model view powerbi problem come us log model perform subsequ deploi select model webservic integr unfortun option defin schema file model deploi result model avail powerbi lack requir schema file option workaround possibl work rest api model power queri rewrit deploy code handl webservic deploy step azur instead thought ask mayb miss workaround current code defin schema file deploi",
        "Question_gpt_summary_original":"The user is facing challenges in integrating an ML model deployed on AzureML with PowerBI due to the requirement of a schema file. The user uses MLflow to log ML model performances and deploy a selected model onto AzureML as a webservice using mlflow.azureml.deploy(), which does not have the option to define a schema file before deployment, resulting in no model being available in PowerBI. The user's options are to find a workaround using the REST API of the model in a power query or rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow.",
        "Question_gpt_summary":"user face challeng integr model deploi powerbi requir schema file user us log model perform deploi select model webservic deploi option defin schema file deploy result model avail powerbi user option workaround rest api model power queri rewrit deploy code handl webservic deploy step azur instead",
        "Answer_original_content":"point number wai solv issu instead deploi score servic azur wrote custom code load model contain initialis score code like import import json pyfunc import load model infer schema schema decor import input schema output schema infer schema paramet type numpi paramet type import numpyparametertyp def init global model model load model path join environ model dir awesom model input schema data numpyparametertyp input sampl output schema numpyparametertyp output sampl def run data return model predict data",
        "Answer_preprocessed_content":"point number wai solv issu instead deploi score servic azur wrote custom code load model contain initialis score code like",
        "Answer_gpt_summary_original":"Solution: The user can rewrite the deployment code and handle the webservice deployment steps in Azure instead of MLflow. This can be done by writing a custom code that loads the MLflow model when the container is initialized. The scoring code can be written to accept input and output schema using the REST API of the model in a power query.",
        "Answer_gpt_summary":"solut user rewrit deploy code handl webservic deploy step azur instead write custom code load model contain initi score code written accept input output schema rest api model power queri"
    },
    {
        "Question_title":"Mixing shell variables and python variables in IPython '!command'",
        "Question_body":"<p>Trying to figure out whether this behaviour on IPython (v7.12.0, on Amazon SageMaker) is a bug or I'm missing some proper way \/ documented constraint...<\/p>\n<p>Say I have some Python variables like:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>NODE_VER = &quot;v16.14.2&quot;\nNODE_DISTRO = &quot;linux-x64&quot;\n<\/code><\/pre>\n<p>These commands both work fine in a notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo $PATH\n# Shows **contents of system path**\n!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:\n# Shows \/usr\/local\/lib\/nodejs\/node-v16.14.2-linux-x64\/bin\n<\/code><\/pre>\n<p>...But this does not:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n# Shows:\n# \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:**contents of system path**\n<\/code><\/pre>\n<p>I've tried a couple of combinations of e.g. using <code>$NODE_VER<\/code> syntax instead (which produces <code>node--\/<\/code> instead of <code>node-{NODE_VER}-{NODE_DISTRO}\/<\/code>, but seems like any combination using both shell variables (PATH) and Python variables (NODE_VER\/NODE_DISTRO) fails.<\/p>\n<p>Can anybody help me understand why and how to work around it?<\/p>\n<p>My end goal, as you might have guessed already, is to actually add this folder to the PATH rather than just echoing it - something like:<\/p>\n<pre><code>!export PATH=\/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$PATH\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1648174324543,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":160.0,
        "Answer_body":"<p><a href=\"https:\/\/stackoverflow.com\/questions\/69194172\/how-to-reference-both-a-python-and-environment-variable-in-jupyter-bash-magic\">How to reference both a python and environment variable in jupyter bash magic?<\/a><\/p>\n<p>Try<\/p>\n<pre><code>!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$$PATH\n<\/code><\/pre>\n<p><code>$$PATH<\/code> forces it to use the system variable rather than try to find a Python\/local one.<\/p>\n<p>Various examples:<\/p>\n<pre><code>In [130]: foo = 'foo*.txt'\nIn [131]: HOME = 'myvar'\nIn [132]: !echo $foo\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt\nIn [133]: !echo $foo $HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt myvar\nIn [134]: !echo $foo $$HOME\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\nIn [135]: !echo $foo $PWD\n\/home\/paul\/mypy\nIn [136]: !echo $foo $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\nIn [137]: !echo {foo} $PWD\n{foo} \/home\/paul\/mypy\nIn [138]: !echo {foo} $$PWD\nfoo1.txt foobar0.txt foobar2.txt foobar3.txt foo.txt \/home\/paul\/mypy\n<\/code><\/pre>\n<p>Any variable not locally defined forces the behavior you see:<\/p>\n<pre><code>In [139]: !echo $abc\n\nIn [140]: !echo {foo} $abc\n{foo}\n<\/code><\/pre>\n<p>It may put the substitution in a <code>try\/except<\/code> block, and &quot;give up&quot; if there's any <code>NameError<\/code>.<\/p>\n<p>This substitution can occur in most of the magics, not just <code>!<\/code>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1648192108500,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71611419",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1648184137556,
        "Question_original_content":"mix shell variabl python variabl ipython command try figur behaviour ipython bug miss proper wai document constraint python variabl like node ver node distro linux command work fine notebook echo path show content path echo usr local lib nodej node node ver node distro bin show usr local lib nodej node linux bin echo usr local lib nodej node node ver node distro bin path show usr local lib nodej node node ver node distro bin content path tri coupl combin node ver syntax instead produc node instead node node ver node distro like combin shell variabl path python variabl node ver node distro fail anybodi help understand work end goal guess actual add folder path echo like export path usr local lib nodej node node ver node distro bin path",
        "Question_preprocessed_content":"mix shell variabl python variabl ipython command try figur behaviour ipython bug miss proper wai document python variabl like command work fine notebook tri coupl combin syntax instead python variabl fail anybodi help understand work end goal guess actual add folder path echo like",
        "Question_gpt_summary_original":"The user is encountering challenges when trying to mix shell variables and Python variables in IPython. They have tried different combinations of using shell variables and Python variables, but none of them have worked. The user's end goal is to add a folder to the PATH, but they are unable to do so.",
        "Question_gpt_summary":"user encount challeng try mix shell variabl python variabl ipython tri differ combin shell variabl python variabl work user end goal add folder path unabl",
        "Answer_original_content":"refer python environ variabl jupyt bash magic try echo usr local lib nodej node node ver node distro bin path path forc us variabl try python local exampl foo foo txt home myvar echo foo foo txt foobar txt foobar txt foobar txt foo txt echo foo home foo txt foobar txt foobar txt foobar txt foo txt myvar echo foo home foo txt foobar txt foobar txt foobar txt foo txt home paul echo foo pwd home paul mypi echo foo pwd foo txt foobar txt foobar txt foobar txt foo txt home paul mypi echo foo pwd foo home paul mypi echo foo pwd foo txt foobar txt foobar txt foobar txt foo txt home paul mypi variabl local defin forc behavior echo abc echo foo abc foo substitut try block nameerror substitut occur magic",
        "Answer_preprocessed_content":"refer python environ variabl jupyt bash magic try forc us variabl try exampl variabl local defin forc behavior substitut block substitut occur magic",
        "Answer_gpt_summary_original":"Solution:\n- Use the command `!echo \/usr\/local\/lib\/nodejs\/node-{NODE_VER}-{NODE_DISTRO}\/bin:$$PATH` to add a folder to the PATH.\n- Use `$$PATH` to force the system variable to be used instead of trying to find a Python\/local one.\n- Any variable not locally defined forces the behavior of using the system variable.\n- The substitution can occur in most of the magics, not just `!`.",
        "Answer_gpt_summary":"solut us command echo usr local lib nodej node node ver node distro bin path add folder path us path forc variabl instead try python local variabl local defin forc behavior variabl substitut occur magic"
    },
    {
        "Question_title":"Executing pipeline in AML from ADF suddenly stopped working",
        "Question_body":"I have a pipeline defined in Azure Machine Learning. It was launched every day with Azure Data Factory with Machine Learning Execute Pipeline activity. This solution worked without any issues for a few weeks, but since 12\/09\/2021 all pipeline runs have failed with error: User starting the run is not an owner or assigned user to the Compute Instance.\nI did not change anything in ADF or AML.\n\nShould I assign compute to ADF? How to do this?",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1639398869747,
        "Question_favorite_count":16.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"I ran into this same issue in a slightly different context. I didn't manage to figure out the root cause but managed to resolve it in practice by standing up a Compute Cluster instead of a Compute Instance (see https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-create-attach-compute-cluster?tabs=python)",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/661588\/how-to-run-a-pipeline-in-aml-from-adf.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1639690537336,
        "Question_original_content":"execut pipelin aml adf suddenli stop work pipelin defin launch dai azur data factori machin learn execut pipelin activ solut work issu week pipelin run fail error user start run owner assign user comput instanc chang adf aml assign comput adf",
        "Question_preprocessed_content":"execut pipelin aml adf suddenli stop work pipelin defin launch dai azur data factori machin learn execut pipelin activ solut work issu week pipelin run fail error user start run owner assign user comput instanc chang adf aml assign comput adf",
        "Question_gpt_summary_original":"The user is facing challenges with executing a pipeline in Azure Machine Learning from Azure Data Factory, which suddenly stopped working. The pipeline was launched every day without any issues for a few weeks, but since 12\/09\/2021, all pipeline runs have failed with an error message indicating that the user starting the run is not an owner or assigned user to the Compute Instance. The user did not make any changes to Azure Data Factory or Azure Machine Learning. The user is seeking advice on whether to assign compute to Azure Data Factory and how to do it.",
        "Question_gpt_summary":"user face challeng execut pipelin azur data factori suddenli stop work pipelin launch dai issu week pipelin run fail error messag indic user start run owner assign user comput instanc user chang azur data factori user seek advic assign comput azur data factori",
        "Answer_original_content":"ran issu slightli differ context manag figur root caus manag resolv practic stand comput cluster instead comput instanc http doc microsoft com azur machin learn creat attach comput cluster tab python",
        "Answer_preprocessed_content":"ran issu slightli differ context manag figur root caus manag resolv practic stand comput cluster instead comput instanc",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to create and attach a Compute Cluster instead of a Compute Instance. The user who faced a similar issue resolved it by using a Compute Cluster. However, the root cause of the issue is unknown. No solution was mentioned regarding assigning compute to Azure Data Factory.",
        "Answer_gpt_summary":"solut possibl solut mention discuss creat attach comput cluster instead comput instanc user face similar issu resolv comput cluster root caus issu unknown solut mention assign comput azur data factori"
    },
    {
        "Question_title":"Automated streaming integration and multiple requests for SageMaker endpoint",
        "Question_body":"A data scientist is looking to host a Tensorflow model in SageMaker and process low volume streaming event data (~2-3 per second) to collect inferences about each event. Data scientist is looking at having the SageMaker inference model plugged in as a Kinesis Data Analytics Application but Kinesis Data Analytics currently only supports SQL or Flink.\n\nOne option to set up an ECS or Lambda service to consume data from Kinesis or SNS and invoke the SageMaker inference endpoint per message, but if there is a more automated and optimal solution available for these kind of workflows.\n\nIt is not possible to pass multiple requests currently to a SageMaker endpoint, yet Tensorflow models tend to perform much better on batches of data rather than multiple single invocations so some windowing would be beneficial. Ideally the client would want to react to an inference within 10-15 seconds of the event being processed so an S3 based batch approach is probably too slow.\n\nIs there anything you can recommend for handling this sort of workload?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1599656695000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":88.0,
        "Answer_body":"To build integration between SageMaker endpoints and Kinesis Data Application use this blog - https:\/\/aws.amazon.com\/blogs\/architecture\/realtime-in-stream-inference-kinesis-sagemaker-flink\/. It help to setup serverless service to invoke the SageMaker inference endpoint.\n\nTo use batching. The Tensorflow documentation mentions the following:\n\nThis link mentions that you can include multiple instances in your predict request (or multiple examples in classify\/regress requests) to get multiple prediction results in one request to your Endpoint.\nThis link mentions that you can configure SageMaker TensorFlow Serving Container to batch multiple records together before performing an inference\n\nYou would still have to handle the logic internally in ECS\/Lambda to control how many records you consume from your stream in one batch, but at least you will be able to infer on the whole batch on the SageMaker endpoint end based on the above.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU6E1eYARES123bRYAe2B0Ag\/automated-streaming-integration-and-multiple-requests-for-sage-maker-endpoint",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1600149573000,
        "Question_original_content":"autom stream integr multipl request endpoint data scientist look host tensorflow model process low volum stream event data second collect infer event data scientist look have infer model plug kinesi data analyt applic kinesi data analyt current support sql flink option set ec lambda servic consum data kinesi sn invok infer endpoint messag autom optim solut avail kind workflow possibl pass multipl request current endpoint tensorflow model tend perform better batch data multipl singl invoc window benefici ideal client want react infer second event process base batch approach probabl slow recommend handl sort workload",
        "Question_preprocessed_content":"autom stream integr multipl request endpoint data scientist look host tensorflow model process low volum stream event data collect infer event data scientist look have infer model plug kinesi data analyt applic kinesi data analyt current support sql flink option set ec lambda servic consum data kinesi sn invok infer endpoint messag autom optim solut avail kind workflow possibl pass multipl request current endpoint tensorflow model tend perform better batch data multipl singl invoc window benefici ideal client want react infer second event process base batch approach probabl slow recommend handl sort workload",
        "Question_gpt_summary_original":"The user, a data scientist, is facing challenges in hosting a Tensorflow model in SageMaker and processing low volume streaming event data to collect inferences about each event. Kinesis Data Analytics, which the user is considering, currently only supports SQL or Flink. The user is exploring options to set up an ECS or Lambda service to consume data from Kinesis or SNS and invoke the SageMaker inference endpoint per message. However, passing multiple requests to a SageMaker endpoint is not currently possible, and Tensorflow models perform better on batches of data rather than multiple single invocations. The user is looking for an automated and optimal solution for these workflows that can react to an inference within 10-15 seconds of the event being processed.",
        "Question_gpt_summary":"user data scientist face challeng host tensorflow model process low volum stream event data collect infer event kinesi data analyt user consid current support sql flink user explor option set ec lambda servic consum data kinesi sn invok infer endpoint messag pass multipl request endpoint current possibl tensorflow model perform better batch data multipl singl invoc user look autom optim solut workflow react infer second event process",
        "Answer_original_content":"build integr endpoint kinesi data applic us blog http aw amazon com blog architectur realtim stream infer kinesi flink help setup serverless servic invok infer endpoint us batch tensorflow document mention follow link mention includ multipl instanc predict request multipl exampl classifi regress request multipl predict result request endpoint link mention configur tensorflow serv contain batch multipl record perform infer handl logic intern ec lambda control record consum stream batch abl infer batch endpoint end base",
        "Answer_preprocessed_content":"build integr endpoint kinesi data applic us blog help setup serverless servic invok infer endpoint us batch tensorflow document mention follow link mention includ multipl instanc predict request multipl predict result request endpoint link mention configur tensorflow serv contain batch multipl record perform infer handl logic intern control record consum stream batch abl infer batch endpoint end base",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Build integration between SageMaker endpoints and Kinesis Data Application using the blog mentioned in the discussion to set up a serverless service to invoke the SageMaker inference endpoint.\n2. Use batching by including multiple instances in the predict request or configuring the SageMaker TensorFlow Serving Container to batch multiple records together before performing an inference. However, the user would still have to handle the logic internally in ECS\/Lambda to control how many records to consume from the stream in one batch.",
        "Answer_gpt_summary":"possibl solut mention discuss build integr endpoint kinesi data applic blog mention discuss set serverless servic invok infer endpoint us batch includ multipl instanc predict request configur tensorflow serv contain batch multipl record perform infer user handl logic intern ec lambda control record consum stream batch"
    },
    {
        "Question_title":"will experiments disappear if I don\u2019t migrate to Designer?",
        "Question_body":"do I still have the access to it?",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1654743684097,
        "Question_favorite_count":12.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @Alexandre-2525\n\nI hope Rohit's reponse is helpful, please let us know if you have more question. All the data of studio will be avaiable till August 2024, you still have time to decide if you want to keep them, but Designer will provide the same experience and supporting the same function, you may want to try.\n\nPlease kindly accept the answer if you feel helpful to support the community, thanks a lot.\n\n\n\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/882424\/will-experiments-disappear-if-i-dont-migrate-to-de.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1655315539550,
        "Question_original_content":"experi disappear dont migrat design access",
        "Question_preprocessed_content":"experi disappear dont migrat design access",
        "Question_gpt_summary_original":"The user is concerned about losing access to their experiments if they do not migrate to Designer. They are seeking clarification on whether they will still have access to their experiments if they do not make the migration.",
        "Question_gpt_summary":"user concern lose access experi migrat design seek clarif access experi migrat",
        "Answer_original_content":"hello alexandr hope rohit repons help let know question data studio avaiabl till august time decid want design provid experi support function want try kindli accept answer feel help support commun thank lot regard yutong",
        "Answer_preprocessed_content":"hello hope rohit repons help let know question data studio avaiabl till august time decid want design provid experi support function want try kindli accept answer feel help support commun thank lot regard yutong",
        "Answer_gpt_summary_original":"No solutions were provided in this discussion. The user was seeking clarification on whether they will still have access to their experiments if they do not migrate to Designer.",
        "Answer_gpt_summary":"solut provid discuss user seek clarif access experi migrat design"
    },
    {
        "Question_title":"Migrate models from Azure Machine Learning Studio (classic) v1 to Azure Machine Learning Studio v2",
        "Question_body":"Is it possible to migrate models created in Azure ML studio (classic) to Azure ML studio v2?\n\nCan I move my models from v1 to v2 in an easy way?\n\nThanks in advance!",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1601619807160,
        "Question_favorite_count":6.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello Helena,\n\nThanks for your waiting. There is not a tool for auto-migration from V1 to V2 for now and future since the architecture of studio(classic) and machine learning studio is totally different. So I don't think it's easy to migrate. But we will have plan for migration in next few month, there should be a way to migrate from studio(classic) to machine learning studio with effort.\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/114944\/migrate-models-from-azure-machine-learning-studio.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1602438917847,
        "Question_original_content":"migrat model studio classic studio possibl migrat model creat studio classic studio model easi wai thank advanc",
        "Question_preprocessed_content":"migrat model studio studio possibl migrat model creat studio studio model easi wai thank advanc",
        "Question_gpt_summary_original":"The user is facing the challenge of migrating models created in Azure Machine Learning Studio (classic) v1 to Azure Machine Learning Studio v2 and is seeking information on whether it is possible to do so easily.",
        "Question_gpt_summary":"user face challeng migrat model creat studio classic studio seek inform possibl easili",
        "Answer_original_content":"hello helena thank wait tool auto migrat futur architectur studio classic machin learn studio total differ think easi migrat plan migrat month wai migrat studio classic machin learn studio effort regard yutong",
        "Answer_preprocessed_content":"hello helena thank wait tool futur architectur studio machin learn studio total differ think easi migrat plan migrat month wai migrat studio machin learn studio effort regard yutong",
        "Answer_gpt_summary_original":"Solution: The discussion mentions that there is currently no tool for auto-migration from Azure Machine Learning Studio (classic) v1 to Azure Machine Learning Studio v2. However, the team has plans to create a migration tool in the next few months, which should make the migration process easier.",
        "Answer_gpt_summary":"solut discuss mention current tool auto migrat studio classic studio team plan creat migrat tool month migrat process easier"
    },
    {
        "Question_title":"Using tidyverse to read data from s3 bucket",
        "Question_body":"<p>I'm trying to read a <code>.csv<\/code> file stored in an s3 bucket, and I'm getting errors. I'm following the instructions <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/advanced_functionality\/r_kernel\/using_r_with_amazon_sagemaker.ipynb\" rel=\"nofollow noreferrer\">here<\/a>, but either it does not work or I am making a mistake and I'm not getting what I'm doing wrong.<\/p>\n\n<p>Here's what I'm trying to do:<\/p>\n\n<pre><code># I'm working on a SageMaker notebook instance\nlibrary(reticulate)\nlibrary(tidyverse)\n\nsagemaker &lt;- import('sagemaker')\nsagemaker.session &lt;- sagemaker$Session()\n\nregion &lt;- sagemaker.session$boto_region_name\nbucket &lt;- \"my-bucket\"\nprefix &lt;- \"data\/staging\"\nbucket.path &lt;- sprintf(\"https:\/\/s3-%s.amazonaws.com\/%s\", region, bucket)\nrole &lt;- sagemaker$get_execution_role()\n\nclient &lt;- sagemaker.session$boto_session$client('s3')\nkey &lt;- sprintf(\"%s\/%s\", prefix, 'my_file.csv')\n\nmy.obj &lt;- client$get_object(Bucket=bucket, Key=key)\n\nmy.df &lt;- read_csv(my.obj$Body) # This is where it all breaks down:\n## \n## Error: `file` must be a string, raw vector or a connection.\n## Traceback:\n## \n## 1. read_csv(my.obj$Body)\n## 2. read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, \n##  .     locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, n_max = n_max, guess_max = guess_max, \n##  .     progress = progress)\n## 3. col_spec_standardise(data, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment, guess_max = guess_max, col_names = col_names, \n##  .     col_types = col_types, tokenizer = tokenizer, locale = locale)\n## 4. datasource(file, skip = skip, skip_empty_rows = skip_empty_rows, \n##  .     comment = comment)\n## 5. stop(\"`file` must be a string, raw vector or a connection.\", \n##  .     call. = FALSE)\n<\/code><\/pre>\n\n<p>When working with Python, I can read a CSV file using someting like this:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>import pandas as pd\n# ... Lots of boilerplate code\nmy_data = pd.read_csv(client.get_object(Bucket=bucket, Key=key)['Body'])\n<\/code><\/pre>\n\n<p>This is very similar to what I'm trying to do in R, and it works with Python... so why does it not work on R?<\/p>\n\n<p>Can you point me in the right path?<\/p>\n\n<p><strong>Note:<\/strong> Although I could use a Python kernel for this, I'd like to stick to R, because I'm more fluent with it than with Python, at least when it comes to dataframe crunching.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1586299117960,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1735.0,
        "Answer_body":"<p>I'd recommend trying the <code>aws.s3<\/code> package instead:<\/p>\n\n<p><a href=\"https:\/\/github.com\/cloudyr\/aws.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/aws.s3<\/a><\/p>\n\n<p>Pretty simple - set your env variables:<\/p>\n\n<pre><code>Sys.setenv(\"AWS_ACCESS_KEY_ID\" = \"mykey\",\n           \"AWS_SECRET_ACCESS_KEY\" = \"mysecretkey\",\n           \"AWS_DEFAULT_REGION\" = \"us-east-1\",\n           \"AWS_SESSION_TOKEN\" = \"mytoken\")\n<\/code><\/pre>\n\n<p>and then once that is out of the way:<\/p>\n\n<p><code>aws.s3::s3read_using(read.csv, object = \"s3:\/\/bucket\/folder\/data.csv\")<\/code><\/p>\n\n<p>Update: I see you're also already familiar with boto and trying to use reticulate so leaving this easy wrapper for that here:\n<a href=\"https:\/\/github.com\/cloudyr\/roto.s3\" rel=\"nofollow noreferrer\">https:\/\/github.com\/cloudyr\/roto.s3<\/a><\/p>\n\n<p>Looks like it has a great api for example the variable layout you're aiming to use:<\/p>\n\n<pre><code>download_file(\n  bucket = \"is.rud.test\", \n  key = \"mtcars.csv\", \n  filename = \"\/tmp\/mtcars-again.csv\", \n  profile_name = \"personal\"\n)\n\nread_csv(\"\/tmp\/mtcars-again.csv\")\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1586740595876,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/61090530",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1586562277483,
        "Question_original_content":"tidyvers read data bucket try read csv file store bucket get error follow instruct work make mistak get wrong try work notebook instanc librari reticul librari tidyvers import session session region session boto region bucket bucket prefix data stage bucket path sprintf http amazonaw com region bucket role execut role client session boto session client kei sprintf prefix file csv obj client object bucket bucket kei kei read csv obj bodi break error file string raw vector connect traceback read csv obj bodi read delimit file token col name col name col type col type local local skip skip skip row skip row comment comment max max guess max guess max progress progress col spec standardis data skip skip skip row skip row comment comment guess max guess max col name col name col type col type token token local local datasourc file skip skip skip row skip row comment comment stop file string raw vector connect fals work python read csv file somet like import panda lot boilerpl code data read csv client object bucket bucket kei kei bodi similar try work python work point right path note us python kernel like stick fluent python come datafram crunch",
        "Question_preprocessed_content":"tidyvers read data bucket try read file store bucket get error follow instruct work make mistak get wrong try work python read csv file somet like similar try work work point right path note us python kernel like stick fluent python come datafram crunch",
        "Question_gpt_summary_original":"The user is encountering errors while trying to read a .csv file stored in an s3 bucket using R and tidyverse. They are following instructions provided in a GitHub repository but are still unable to read the file. The user is seeking guidance to resolve the issue and is hesitant to switch to a Python kernel.",
        "Question_gpt_summary":"user encount error try read csv file store bucket tidyvers follow instruct provid github repositori unabl read file user seek guidanc resolv issu hesit switch python kernel",
        "Answer_original_content":"recommend try aw packag instead http github com cloudyr aw pretti simpl set env variabl sy setenv aw access kei mykei aw secret access kei mysecretkei aw default region east aw session token mytoken wai aw sread read csv object bucket folder data csv updat familiar boto try us reticul leav easi wrapper http github com cloudyr roto look like great api exampl variabl layout aim us download file bucket rud test kei mtcar csv filenam tmp mtcar csv profil person read csv tmp mtcar csv",
        "Answer_preprocessed_content":"recommend try packag instead pretti simpl set env variabl wai updat familiar boto try us reticul leav easi wrapper look like great api exampl variabl layout aim us",
        "Answer_gpt_summary_original":"Solution: The user can try using the `aws.s3` package to read the .csv file stored in the s3 bucket. They need to set their environment variables and then use the `s3read_using` function to read the file. Another solution is to use the `roto.s3` package, which has an API for the variable layout the user is trying to use. The `download_file` function can be used to download the file and then `read_csv` can be used to read the file.",
        "Answer_gpt_summary":"solut user try aw packag read csv file store bucket need set environ variabl us sread function read file solut us roto packag api variabl layout user try us download file function download file read csv read file"
    },
    {
        "Question_title":"Dependency missing when running AzureML Estimator in docker environment",
        "Question_body":"<h3>Scenario description<\/h3>\n\n<p>I'm trying to submit a training script to AzureML (want to use AmlCompute, but I'm starting\/testing locally first, for debugging purposes).<\/p>\n\n<p>The <code>train.py<\/code> script I have uses a custom package (<code>arcus.ml<\/code>) and I believe I have specified the right settings and dependencies, but still I get the error: <\/p>\n\n<p><code>User program failed with ModuleNotFoundError: No module named 'arcus.ml'<\/code><\/p>\n\n<h3>Code and reproduction<\/h3>\n\n<p>This the python code I have:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>name='test'\nscript_params = {\n    '--test-par': 0.2\n}\n\nest = Estimator(source_directory='.\/' + name,\n                   script_params=script_params,\n                   compute_target='local',\n                   entry_script='train.py',\n                   pip_requirements_file='requirements.txt',\n                   conda_packages=['scikit-learn','tensorflow', 'keras'])\n\nrun = exp.submit(est)\nprint(run.get_portal_url())\n<\/code><\/pre>\n\n<p>This is the (fully simplified) train.py script in the <code>test<\/code>directory:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from arcus.ml import dataframes as adf\nfrom azureml.core import Workspace, Dataset, Datastore, Experiment, Run\n\n# get hold of the current run\nrun = Run.get_context()\nws = run.get_environment()\n\nprint('training finished')\n<\/code><\/pre>\n\n<p>And this is my requirements.txt file<\/p>\n\n<pre><code>arcus-azureml\narcus-ml\nnumpy\npandas\nazureml-core\ntqdm\njoblib\nscikit-learn\nmatplotlib\ntensorflow\nkeras\n<\/code><\/pre>\n\n<h3>Logs<\/h3>\n\n<p>In the logs file of the run, I can see this section, sot it seems the external module is being installed anyhow.<\/p>\n\n<pre><code>Collecting arcus-azureml\n  Downloading arcus_azureml-1.0.3-py3-none-any.whl (3.1 kB)\nCollecting arcus-ml\n  Downloading arcus_ml-1.0.6-py3-none-any.whl (2.1 kB)\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1591043054577,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":208.0,
        "Answer_body":"<p>I think this error isn't necessarily about Azure ML. I think the error has to do w\/ the difference b\/w using a hyphen and a period in your package name. But I'm a python packaging newb. \nIn a new conda environment on my laptop, I ran the following<\/p>\n\n<pre><code>&gt; conda create -n arcus python=3.6 -y\n&gt; conda activate arcus\n&gt; pip install arcus-ml\n&gt; python\n&gt;&gt;&gt; from arcus.ml import dataframes as adf\nModuleNotFoundError: No module named 'arcus'\n<\/code><\/pre>\n\n<p>When I look in the env's site packages folder, I didn't see the <code>arcus\/ml<\/code> folder structure I was expecting. There's no arcus code there at all, only the <code>.dist-info<\/code> file<\/p>\n\n<h3><code>~\/opt\/anaconda3\/envs\/arcus\/lib\/python3.6\/site-packages<\/code><\/h3>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/caExn.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/caExn.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1591044471110,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62140446",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1591043858447,
        "Question_original_content":"depend miss run estim docker environ scenario descript try submit train script want us amlcomput start test local debug purpos train script us custom packag arcu believ specifi right set depend error user program fail modulenotfounderror modul name arcu code reproduct python code test script param test par est estim sourc directori script param script param comput target local entri script train pip requir file requir txt conda packag scikit learn tensorflow kera run exp submit est print run portal url fulli simplifi train script testdirectori arcu import datafram adf core import workspac dataset datastor experi run hold current run run run context run environ print train finish requir txt file arcu arcu numpi panda core tqdm joblib scikit learn matplotlib tensorflow kera log log file run section sot extern modul instal collect arcu download arcu whl collect arcu download arcu whl",
        "Question_preprocessed_content":"depend miss run estim docker environ scenario descript try submit train script script us custom packag believ specifi right set depend error code reproduct python code script directori file log log file run section sot extern modul instal",
        "Question_gpt_summary_original":"The user is encountering a challenge when running an AzureML estimator in a docker environment. The user is trying to submit a training script to AzureML, but is receiving an error message indicating that a custom package called \"arcus.ml\" is missing. The user has specified the correct settings and dependencies, but the error persists. The logs show that the external module is being installed, but the error still occurs.",
        "Question_gpt_summary":"user encount challeng run estim docker environ user try submit train script receiv error messag indic custom packag call arcu miss user specifi correct set depend error persist log extern modul instal error occur",
        "Answer_original_content":"think error isn necessarili think error differ hyphen period packag python packag newb new conda environ laptop ran follow conda creat arcu python conda activ arcu pip instal arcu python arcu import datafram adf modulenotfounderror modul name arcu look env site packag folder arcu folder structur expect arcu code dist info file opt anaconda env arcu lib python site packag",
        "Answer_preprocessed_content":"think error isn necessarili think error differ hyphen period packag python packag newb new conda environ laptop ran follow look env site packag folder folder structur expect arcu code file",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Azure ML and r scripts",
        "Question_body":"<p>I have dataframe with columns <\/p>\n\n<pre><code>date    open    high    low     close   adjclose    volume\n<\/code><\/pre>\n\n<p>I want to add one more column named \"result\"(1 if close > open, 0 if close &lt; open)<\/p>\n\n<p>I do<\/p>\n\n<pre><code># Map 1-based optional input ports to variables\ndata &lt;- maml.mapInputPort(1) # class: data.frame\n\n\n\n# calculate pass\/fail\ndata$result &lt;- as.factor(sapply(data$close,function(res) \n    if (res - data$open &gt;= 0) '1' else '0'))\n\n# Select data.frame to be sent to the output Dataset port\nmaml.mapOutputPort(\"data\");\n<\/code><\/pre>\n\n<p>But I have only 1 in result. Where is the problem?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1554571406233,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":77.0,
        "Answer_body":"<p>The <code>if\/else<\/code> can return only a single TRUE\/FALSE and is not vectorized for length > 1.  It may be suitable to use <code>ifelse<\/code> (but that is also not required and would be less efficient compared to direct coersion of logical vector to binary (<code>as.integer<\/code>).   In the OP's code, the 'close' column elements are looped  (<code>sapply<\/code>) and subtracted from the whole 'open' column.  The intention might be to do elementwise subtraction.  In that case, <code>-<\/code> between the columns is much cleaner and efficient (as these operations are vectorized)<\/p>\n\n<pre><code>data$result &lt;- with(data, factor(as.integer((close - open) &gt;= 0)))\n<\/code><\/pre>\n\n<p>In the above, we get the difference between the columns ('close', 'open'), check if it is greater than or equal to 0 (returns logical vector), convert it to binary (<code>as.integer<\/code> - TRUE -> 1, FALSE -> 0) and then change it to <code>factor<\/code> type (if needed)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/55551617",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1554572339496,
        "Question_original_content":"script datafram column date open high low close adjclos volum want add column name result close open close select data frame sent output dataset port maml mapoutputport data result problem",
        "Question_preprocessed_content":"script datafram column want add column name result close open close open result problem",
        "Question_gpt_summary_original":"The user is facing a challenge in adding a new column named \"result\" to a dataframe using Azure ML and R scripts. The column should have a value of 1 if the \"close\" value is greater than the \"open\" value, and 0 if the \"close\" value is less than the \"open\" value. However, the user is only getting a value of 1 in the \"result\" column and is seeking help to identify the problem.",
        "Question_gpt_summary":"user face challeng ad new column name result datafram script column valu close valu greater open valu close valu open valu user get valu result column seek help identifi problem",
        "Answer_original_content":"return singl true fals vector length suitabl us ifels requir effici compar direct coersion logic vector binari integ code close column element loop sappli subtract open column intent elementwis subtract case column cleaner effici oper vector data result differ column close open check greater equal return logic vector convert binari integ true fals chang factor type need",
        "Answer_preprocessed_content":"return singl vector length suitabl us code close column element loop subtract open column intent elementwis subtract case column cleaner effici differ column check greater equal convert binari chang type",
        "Answer_gpt_summary_original":"Solution:\nThe user can use the following code to add a new column named \"result\" to a dataframe using Azure ML and R scripts. The column should have a value of 1 if the \"close\" value is greater than the \"open\" value, and 0 if the \"close\" value is less than the \"open\" value.\n\n<pre><code>data$result &lt;- with(data, factor(as.integer((close - open) &gt;= 0)))\n<\/code><\/pre>\n\nThe above code gets the difference between the columns ('close', 'open'), checks if it is greater than or equal to 0 (returns logical vector), converts it to binary (<code>as.integer<\/code> - TRUE -> 1",
        "Answer_gpt_summary":"solut user us follow code add new column name result datafram script column valu close valu greater open valu close valu open valu data result code get differ column close open check greater equal return logic vector convert binari integ true"
    },
    {
        "Question_title":"How to create Sagemaker studio project using aws cdk",
        "Question_body":"<p>I am trying to create sagemaker studio project using aws cdk following below steps:<\/p>\n<p>create domain (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate user (<a href=\"https:\/\/github.com\/aws-samples\/aws-cdk-sagemaker-studio.git\" rel=\"nofollow noreferrer\">using this example<\/a>)\ncreate jupyter app\ncreate project<\/p>\n<p>Code for creating jupyter app:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             app_name: str,\n             app_type: str,\n             domain_id: str,\n             user_profile_name: str,\n             depends_on=None, **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_jupyter_app = sg.CfnApp(self, construct_id,\n                                      app_name=app_name,\n                                      app_type=app_type,\n                                      domain_id=domain_id,\n                                      user_profile_name=user_profile_name\n                                      )\n    sagemaker_jupyter_app.add_depends_on(depends_on_user_creation)\n<\/code><\/pre>\n<p>Code for creating project:<\/p>\n<pre><code>\ndef __init__(self, scope: Construct,\n             construct_id: str, *,\n             project_name: str,\n             project_description: str,\n             product_id: str,\n             depends_on=None,\n             **kwargs) -&gt; None:\n    super().__init__(scope, construct_id)\n\n    sagemaker_studio_project = sg.CfnProject(self, construct_id,\n                                             project_name=project_name,\n                                             service_catalog_provisioning_details={\n                                                 &quot;ProductId&quot;: &quot;prod-7tjedn5dz4jrw&quot;\n                                             },\n                                             project_description=project_description\n                                             )\n<\/code><\/pre>\n<p>Domain, user, jupyter app all gets created successfully. The problem comes in with project.\nBelow is the error :<\/p>\n<blockquote>\n<p>Resource handler returned message: &quot;Product prod-7tjedn5dz4jrw does\nnot exist or access was denied (Service: SageMaker, Status Code: 400,\nRequest ID: 768116aa-e77b-4691-a972-38b83093fdc4)&quot; (RequestToken:\n45ca2a0c-3f03-e3e0-f29d-d9443ff4dfc1, HandlerErrorCode:\nGeneralServiceException)<\/p>\n<\/blockquote>\n<p>I am running this code from an ec2 instance that has SagemakerFullAccess\nI also tried attaching SagemakerFullAccess execution role with project...but got the same error.\nI have also attached below policy to my domain:<\/p>\n<ul>\n<li>AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy<\/li>\n<\/ul>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1650541371883,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":491.0,
        "Answer_body":"<p>Basically this was an issue related to IAM.\nRunning cdk program requires bootstrapping it using the command <code>cdk bootstrap<\/code>\nAfter running this command cdk was creating a bunch of roles out of which one role will be related to cloudformation's execution role. Something like<\/p>\n<blockquote>\n<p>cdk-serialnumber-cfn-exec-role-Id-region<\/p>\n<\/blockquote>\n<p>Now this role was used by cloudformation to run the stack.<\/p>\n<p>Using sagemaker from console automatically adds the role associated with domain\/user at<\/p>\n<blockquote>\n<p>ServiceCatalog -&gt; Portfolios -&gt; Imported -&gt; Amazon SageMaker Solutions and ML Ops products -&gt; Groups, roles, and users<\/p>\n<\/blockquote>\n<p>Thats was the reason why product id was accessible from console.<\/p>\n<p>After adding the role created by cdk bootsrap to the above path I was able to run my stack.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71953876",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1650740870208,
        "Question_original_content":"creat studio project aw cdk try creat studio project aw cdk follow step creat domain exampl creat user exampl creat jupyt app creat project code creat jupyt app def init self scope construct construct str app str app type str domain str user profil str depend kwarg super init scope construct jupyt app cfnapp self construct app app app type app type domain domain user profil user profil jupyt app add depend depend user creation code creat project def init self scope construct construct str project str project descript str product str depend kwarg super init scope construct studio project cfnproject self construct project project servic catalog provis detail productid prod tjedndzjrw project descript project descript domain user jupyt app get creat successfulli problem come project error resourc handler return messag product prod tjedndzjrw exist access deni servic statu code request bfdc requesttoken caac dffdfc handlererrorcod generalserviceexcept run code instanc fullaccess tri attach fullaccess execut role project got error attach polici domain amazonadmin servicecatalogproductsservicerolepolici",
        "Question_preprocessed_content":"creat studio project aw cdk try creat studio project aw cdk follow step creat domain creat user creat jupyt app creat project code creat jupyt app code creat project domain user jupyt app get creat successfulli problem come project error resourc handler return messag product exist access deni run code instanc fullaccess tri attach fullaccess execut role got error attach polici domain",
        "Question_gpt_summary_original":"The user is encountering an error while trying to create a Sagemaker studio project using AWS CDK. The domain, user, and Jupyter app are created successfully, but the project creation fails with an error message stating that the product does not exist or access was denied. The user has tried running the code from an EC2 instance with SagemakerFullAccess and attaching the SagemakerFullAccess execution role to the project, but the error persists. The user has also attached the AmazonSageMakerAdmin-ServiceCatalogProductsServiceRolePolicy to the domain.",
        "Question_gpt_summary":"user encount error try creat studio project aw cdk domain user jupyt app creat successfulli project creation fail error messag state product exist access deni user tri run code instanc fullaccess attach fullaccess execut role project error persist user attach amazonadmin servicecatalogproductsservicerolepolici domain",
        "Answer_original_content":"basic issu relat iam run cdk program requir bootstrap command cdk bootstrap run command cdk creat bunch role role relat cloudform execut role like cdk serialnumb cfn exec role region role cloudform run stack consol automat add role associ domain user servicecatalog portfolio import solut op product group role user that reason product access consol ad role creat cdk bootsrap path abl run stack",
        "Answer_preprocessed_content":"basic issu relat iam run cdk program requir bootstrap command run command cdk creat bunch role role relat cloudform execut role like role cloudform run stack consol automat add role associ servicecatalog portfolio import solut op product group role user that reason product access consol ad role creat cdk bootsrap path abl run stack",
        "Answer_gpt_summary_original":"Solution: The issue was related to IAM. Running the CDK program requires bootstrapping it using the command \"cdk bootstrap\". After running this command, CDK creates a bunch of roles, out of which one role will be related to CloudFormation's execution role. This role is used by CloudFormation to run the stack. Using SageMaker from the console automatically adds the role associated with the domain\/user at \"ServiceCatalog -> Portfolios -> Imported -> Amazon SageMaker Solutions and ML Ops products -> Groups, roles, and users\". After adding the role created by CDK bootstrap to the above path, the user was able to run the stack.",
        "Answer_gpt_summary":"solut issu relat iam run cdk program requir bootstrap command cdk bootstrap run command cdk creat bunch role role relat cloudform execut role role cloudform run stack consol automat add role associ domain user servicecatalog portfolio import solut op product group role user ad role creat cdk bootstrap path user abl run stack"
    },
    {
        "Question_title":"How to use scikit learn model from inside sagemaker 'model.tar.gz' file?",
        "Question_body":"<p>New to Sagemaker..<\/p>\n<p>Trained a &quot;linear-learner&quot; classification model using the Sagemaker API, and it saved a &quot;model.tar.gz&quot; file in my s3 path. From what I understand SM just used an image of a scikit logreg model.<\/p>\n<p>Finally, I'd like to gain access to the model object itself, so I unpacked the &quot;model.tar.gz&quot; file only to find another file called &quot;model_algo-1&quot; with no extension.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/DPUMO.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DPUMO.png\" alt=\"contents of unknown file\" \/><\/a><\/p>\n<p>Can anyone tell me how I can find the &quot;real&quot; modeling object without using the inference\/Endpoint delpoy API provided by Sagemaker? There are some things I want to look at manually.<\/p>\n<p>Thanks,\nCraig<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658258900593,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":42.0,
        "Answer_body":"<p>Linear-Learner is a built in algorithm written using MX-net and the binary is also MXNET compatible. You can't use this model outside of SageMaker as there is no open source implementation for this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73042521",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1658447826900,
        "Question_original_content":"us scikit learn model insid model tar file new train linear learner classif model api save model tar file path understand imag scikit logreg model final like gain access model object unpack model tar file file call model algo extens tell real model object infer endpoint delpoi api provid thing want look manual thank craig",
        "Question_preprocessed_content":"us scikit learn model insid file new train classif model api save file path understand imag scikit logreg model final like gain access model object unpack file file call extens tell real model object delpoi api provid thing want look manual thank craig",
        "Question_gpt_summary_original":"The user trained a linear-learner classification model using Sagemaker API and saved a \"model.tar.gz\" file in their S3 path. However, upon unpacking the file, they found another file called \"model_algo-1\" with no extension and are unable to access the actual modeling object without using the inference\/Endpoint deploy API provided by Sagemaker. The user is seeking assistance in finding the \"real\" modeling object to manually examine it.",
        "Question_gpt_summary":"user train linear learner classif model api save model tar file path unpack file file call model algo extens unabl access actual model object infer endpoint deploi api provid user seek assist find real model object manual examin",
        "Answer_original_content":"linear learner built algorithm written net binari mxnet compat us model outsid open sourc implement",
        "Answer_preprocessed_content":"built algorithm written binari mxnet compat us model outsid open sourc implement",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Clear Feature with Auto ML",
        "Question_body":"Hello, I am trying to add a user Id column to my dataset but I don't want the user Id to impact the results of the ML.\n\nI am using Auto ML on my dataset to generate a model and then deployed the model to an endpoint.\n\nCurrently I am calling the endpoint like:\n\n {\"data\":[\n        {\n           \"TEMP\":\"X\",\n         }\n     ]\n }\n\n\n\nand I would like to call it like:\n\n {\"data\":[\n     {\n       \"TEMP\":\"X\",\n       \"userID\": 5434643\n      }\n   ]}\n\n\n\nI'm wondering if there is a way I can do this? I've seen about using Clear Feature in Edit Metadata for the Designer but I'm wondering if something similar can be done for automated ML?\n\nThanks so much!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1627927876257,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, thanks for reaching out. You can customize featurization in automl to only include features relevant for prediction. Here's the documentation. Hope it helps!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/498759\/clear-feature-with-auto-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1627949805196,
        "Question_original_content":"clear featur auto hello try add user column dataset want user impact result auto dataset gener model deploi model endpoint current call endpoint like data temp like like data temp userid wonder wai seen clear featur edit metadata design wonder similar autom thank",
        "Question_preprocessed_content":"clear featur auto hello try add user column dataset want user impact result auto dataset gener model deploi model endpoint current call endpoint like like like wonder wai seen clear featur edit metadata design wonder similar autom thank",
        "Question_gpt_summary_original":"The user wants to add a user Id column to their dataset without it impacting the results of the ML. They are using Auto ML to generate a model and have deployed it to an endpoint. They are looking for a way to add the user Id column to the endpoint data without affecting the ML results and are wondering if Clear Feature in Edit Metadata for the Designer can be used for automated ML.",
        "Question_gpt_summary":"user want add user column dataset impact result auto gener model deploi endpoint look wai add user column endpoint data affect result wonder clear featur edit metadata design autom",
        "Answer_original_content":"thank reach custom featur automl includ featur relev predict document hope help",
        "Answer_preprocessed_content":"thank reach custom featur automl includ featur relev predict document hope help",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to customize featurization in automl to only include features relevant for prediction. The user can refer to the documentation provided for more information.",
        "Answer_gpt_summary":"solut solut suggest discuss custom featur automl includ featur relev predict user refer document provid inform"
    },
    {
        "Question_title":"Azure Machine Learning Designer Error: JobConfigurationMaxSizeExceeded",
        "Question_body":"<p>I have an Azure Machine Learning Designer pipeline that I've run successfully many dozens of times.  Suddenly, today, The pipeline is getting down to the 'Train Model' node and failing with the following error:<\/p>\n<p><code>JobConfigurationMaxSizeExceeded: The specified job configuration exceeds the max allowed size of 32768 characters. Please reduce the size of the job's command line arguments and environment settings<\/code><\/p>\n<p>How do I address this error in designer-built pipelines?<\/p>\n<p>I have even gone back to previously successful runs of this pipeline and resubmitted one of these runs which also failed with the exact same error.  A resubmitted run should have the exact same pipeline architecture and input data (afaik), so it seems like a problem outside my control.<\/p>\n<p>Pipeline with error:\n<a href=\"https:\/\/i.stack.imgur.com\/uLoIe.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uLoIe.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Pipeline run overview:\n<a href=\"https:\/\/i.stack.imgur.com\/eTzTA.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eTzTA.png\" alt=\"enter image description here\" \/><\/a>\nAny ideas?<\/p>\n<p>EDIT:  I'm able to repro this with a really simple pipeline.  Simply trying to exclude columns in a <code>Select Columns<\/code> node from a dataset gives me this error:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/qZKj1.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/qZKj1.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1638827883783,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1638837173276,
        "Question_score":3.0,
        "Question_view_count":358.0,
        "Answer_body":"<p>This appears to be a bug introduced by Microsoft's rollout of their new Compute Common Runtime.<\/p>\n<p>If I go into any nodes failing with the <code>JobConfigurationMaxSizeExceeded<\/code> exception and manually set <code>AZUREML_COMPUTE_USE_COMMON_RUNTIME:false<\/code> in their  <code>Environment JSON<\/code> field, then they work correctly.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70252478",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1638852273883,
        "Question_original_content":"design error jobconfigurationmaxsizeexceed design pipelin run successfulli dozen time suddenli todai pipelin get train model node fail follow error jobconfigurationmaxsizeexceed specifi job configur exce max allow size charact reduc size job command line argument environ set address error design built pipelin gone previous success run pipelin resubmit run fail exact error resubmit run exact pipelin architectur input data afaik like problem outsid control pipelin error pipelin run overview idea edit abl repro simpl pipelin simpli try exclud column select column node dataset give error",
        "Question_preprocessed_content":"design error jobconfigurationmaxsizeexceed design pipelin run successfulli dozen time suddenli todai pipelin get train model node fail follow error address error pipelin gone previous success run pipelin resubmit run fail exact error resubmit run exact pipelin architectur input data like problem outsid control pipelin error pipelin run overview idea edit abl repro simpl pipelin simpli try exclud column node dataset give error",
        "Question_gpt_summary_original":"The user is encountering an error in Azure Machine Learning Designer pipeline while running the 'Train Model' node. The error message indicates that the job configuration exceeds the maximum allowed size of 32768 characters. The user has tried resubmitting previously successful runs of the pipeline, but the error persists. The user is seeking advice on how to address this error in designer-built pipelines.",
        "Question_gpt_summary":"user encount error design pipelin run train model node error messag indic job configur exce maximum allow size charact user tri resubmit previous success run pipelin error persist user seek advic address error design built pipelin",
        "Answer_original_content":"appear bug introduc microsoft rollout new comput common runtim node fail jobconfigurationmaxsizeexceed except manual set comput us common runtim fals environ json field work correctli",
        "Answer_preprocessed_content":"appear bug introduc microsoft rollout new comput common runtim node fail except manual set field work correctli",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to manually set \"AZUREML_COMPUTE_USE_COMMON_RUNTIME:false\" in the Environment JSON field of the nodes failing with the JobConfigurationMaxSizeExceeded exception. This has been reported to work correctly.",
        "Answer_gpt_summary":"solut possibl solut mention discuss manual set comput us common runtim fals environ json field node fail jobconfigurationmaxsizeexceed except report work correctli"
    },
    {
        "Question_title":"azure ml experiment return different results than webservice",
        "Question_body":"<p>same input is used in two cases, but different result is returned from python module<\/p>\n\n<p>here is the python script that return the result to the webservice:<\/p>\n\n<pre><code>import pandas as pd\nimport sys\n\n\n  def get_segments(dataframe):\n     dataframe['segment']=dataframe['segment'].astype('str')\n     segments = dataframe.loc[~dataframe['segment'].duplicated()]['segment']\n     return segments\n\n\n  def azureml_main(dataframe1 = None, dataframe2 = None):\n\n   df = dataframe1\n   segments = get_segments(df)\n   segmentCount =segments.size\n\n   if (segmentCount &gt; 0) :\n      res = pd.DataFrame(columns=['segmentId','recommendation'],index=[range(segmentCount)])\n    i=0    \n    for seg in segments:\n        d= df.query('segment ==[\"{}\"]'.format(seg)).sort(['count'],ascending=[0])\n\n        res['segmentId'][i]=seg\n        recommendation='['\n        for index, x in d.iterrows():\n            item=str(x['ItemId'])\n            recommendation = recommendation + item + ','\n        recommendation = recommendation[:-1] + ']'\n        res['recommendation'][i]= recommendation\n        i=i+1\n   else:\n\n      res = pd.DataFrame(columns=[seg,pdver],index=[range(segmentCount)])\n\nreturn res,\n<\/code><\/pre>\n\n<p>when in experiment it returnd the actual itemIds, when in webservice it returns some numbers<\/p>\n\n<p>the purpose of this code is to pivot some table by segment column for recommendation<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1467651436410,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1468140771380,
        "Question_score":3.0,
        "Question_view_count":264.0,
        "Answer_body":"<p>After discussion with the product team from Microsoft. the issue was resolved.\nthe product team rolled out an update to the web service first, and only later to the ML-Studio, which fixed an issue with categorical attributes in \"Execute python script\".\nthe issue was in a earlier stage of the flow and has nothing to do with the python code above.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38189399",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1468139423743,
        "Question_original_content":"experi return differ result webservic input case differ result return python modul python script return result webservic import panda import sy def segment datafram datafram segment datafram segment astyp str segment datafram loc datafram segment duplic segment return segment def main datafram datafram datafram segment segment segmentcount segment size segmentcount re datafram column segmentid recommend index rang segmentcount seg segment queri segment format seg sort count ascend re segmentid seg recommend index iterrow item str itemid recommend recommend item recommend recommend re recommend recommend re datafram column seg pdver index rang segmentcount return re experi returnd actual itemid webservic return number purpos code pivot tabl segment column recommend",
        "Question_preprocessed_content":"experi return differ result webservic input case differ result return python modul python script return result webservic experi returnd actual itemid webservic return number purpos code pivot tabl segment column recommend",
        "Question_gpt_summary_original":"The user is facing a challenge where the same input is used in two cases, but different results are returned from the Python module. The Python script is designed to pivot some table by segment column for recommendation, but when used in the experiment, it returns the actual itemIds, while in the webservice, it returns some numbers.",
        "Question_gpt_summary":"user face challeng input case differ result return python modul python script design pivot tabl segment column recommend experi return actual itemid webservic return number",
        "Answer_original_content":"discuss product team microsoft issu resolv product team roll updat web servic later studio fix issu categor attribut execut python script issu earlier stage flow python code",
        "Answer_preprocessed_content":"discuss product team microsoft issu resolv product team roll updat web servic later fix issu categor attribut execut python script issu earlier stage flow python code",
        "Answer_gpt_summary_original":"Solution: The product team from Microsoft rolled out an update to the web service and ML-Studio, which fixed an issue with categorical attributes in \"Execute python script\". The issue was in an earlier stage of the flow and had nothing to do with the Python code.",
        "Answer_gpt_summary":"solut product team microsoft roll updat web servic studio fix issu categor attribut execut python script issu earlier stage flow python code"
    },
    {
        "Question_title":"How to connect AMLS to ADLS Gen 2?",
        "Question_body":"<p>I would like to register a dataset from ADLS Gen2 in my Azure Machine Learning workspace (<code>azureml-core==1.12.0<\/code>). Given that service principal information is not required in the Python SDK <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.datastore.datastore?view=azure-ml-py#register-azure-data-lake-gen2-workspace--datastore-name--filesystem--account-name--tenant-id-none--client-id-none--client-secret-none--resource-url-none--authority-url-none--protocol-none--endpoint-none--overwrite-false-\" rel=\"noreferrer\">documentation<\/a> for <code>.register_azure_data_lake_gen2()<\/code>, I successfully used the following code to register ADLS gen2 as a datastore:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Datastore\n\nadlsgen2_datastore_name = os.environ['adlsgen2_datastore_name']\naccount_name=os.environ['account_name'] # ADLS Gen2 account name\nfile_system=os.environ['filesystem']\n\nadlsgen2_datastore = Datastore.register_azure_data_lake_gen2(\n    workspace=ws,\n    datastore_name=adlsgen2_datastore_name,\n    account_name=account_name, \n    filesystem=file_system\n)\n<\/code><\/pre>\n<p>However, when I try to register a dataset, using<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Dataset\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndata = Dataset.Tabular.from_delimited_files((adls_ds, 'folder\/data.csv'))\n<\/code><\/pre>\n<p>I get an error<\/p>\n<blockquote>\n<p>Cannot load any data from the specified path. Make sure the path is accessible and contains data.\n<code>ScriptExecutionException<\/code> was caused by <code>StreamAccessException<\/code>.\nStreamAccessException was caused by AuthenticationException.\n<code>'AdlsGen2-ReadHeaders'<\/code> for '[REDACTED]' on storage failed with status code 'Forbidden' (This request is not authorized to perform this operation using this permission.), client request ID &lt;CLIENT_REQUEST_ID&gt;, request ID &lt;REQUEST_ID&gt;. Error message: [REDACTED]\n| session_id=&lt;SESSION_ID&gt;<\/p>\n<\/blockquote>\n<p>Do I need the to enable the service principal to get this to work? Using the ML Studio UI, it appears that the service principal is required even to register the datastore.<\/p>\n<p>Another issue I noticed is that AMLS is trying to access the dataset here:\n<code>https:\/\/adls_gen2_account_name.**dfs**.core.windows.net\/container\/folder\/data.csv<\/code> whereas the actual URI in ADLS Gen2 is: <code>https:\/\/adls_gen2_account_name.**blob**.core.windows.net\/container\/folder\/data.csv<\/code><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1600115991930,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1600160631356,
        "Question_score":7.0,
        "Question_view_count":3331.0,
        "Answer_body":"<p>According to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-data#azure-data-lake-storage-generation-2\" rel=\"noreferrer\">documentation<\/a>,you need to enable the service principal.<\/p>\n<p>1.you need to register your application and grant the service principal with <strong>Storage Blob Data Reader access<\/strong>.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FZl8O.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>2.try this code:<\/p>\n<pre><code>adlsgen2_datastore = Datastore.register_azure_data_lake_gen2(workspace=ws,\n                                                             datastore_name=adlsgen2_datastore_name,\n                                                             account_name=account_name,\n                                                             filesystem=file_system,\n                                                             tenant_id=tenant_id,\n                                                             client_id=client_id,\n                                                             client_secret=client_secret\n                                                             )\n\nadls_ds = Datastore.get(ws, datastore_name=adlsgen2_datastore_name)\ndataset = Dataset.Tabular.from_delimited_files((adls_ds,'sample.csv'))\nprint(dataset.to_pandas_dataframe())\n<\/code><\/pre>\n<p><strong>Result:<\/strong><\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/50mit.png\" rel=\"noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/50mit.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1600166834147,
        "Answer_score":9.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63891547",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1600155716360,
        "Question_original_content":"connect aml adl gen like regist dataset adl gen workspac core given servic princip inform requir python sdk document regist azur data lake gen successfulli follow code regist adl gen datastor core import datastor adlsgen datastor environ adlsgen datastor account environ account adl gen account file environ filesystem adlsgen datastor datastor regist azur data lake gen workspac datastor adlsgen datastor account account filesystem file try regist dataset core import dataset adl datastor datastor adlsgen datastor data dataset tabular delimit file adl folder data csv error load data specifi path sure path access contain data scriptexecutionexcept caus streamaccessexcept streamaccessexcept caus authenticationexcept adlsgen readhead redact storag fail statu code forbidden request author perform oper permiss client request request error messag redact session need enabl servic princip work studio appear servic princip requir regist datastor issu notic aml try access dataset http adl gen account df core window net contain folder data csv actual uri adl gen http adl gen account blob core window net contain folder data csv",
        "Question_preprocessed_content":"connect aml adl gen like regist dataset adl gen workspac given servic princip inform requir python sdk document successfulli follow code regist adl gen datastor try regist dataset error load data specifi path sure path access contain data caus streamaccessexcept caus authenticationexcept storag fail statu code forbidden client request request error messag need enabl servic princip work studio appear servic princip requir regist datastor issu notic aml try access dataset actual uri adl gen",
        "Question_gpt_summary_original":"The user is facing challenges while trying to register a dataset from ADLS Gen2 in their Azure Machine Learning workspace. Although they were able to register ADLS Gen2 as a datastore without using service principal information, they encountered an error while trying to register a dataset. The error message suggests that the path is inaccessible and contains no data. The user is unsure if they need to enable the service principal to make it work. Additionally, they noticed that AMLS is trying to access the dataset using the wrong URI.",
        "Question_gpt_summary":"user face challeng try regist dataset adl gen workspac abl regist adl gen datastor servic princip inform encount error try regist dataset error messag suggest path inaccess contain data user unsur need enabl servic princip work addition notic aml try access dataset wrong uri",
        "Answer_original_content":"accord document need enabl servic princip need regist applic grant servic princip storag blob data reader access try code adlsgen datastor datastor regist azur data lake gen workspac datastor adlsgen datastor account account filesystem file tenant tenant client client client secret client secret adl datastor datastor adlsgen datastor dataset dataset tabular delimit file adl sampl csv print dataset panda datafram result",
        "Answer_preprocessed_content":"accord document need enabl servic princip need regist applic grant servic princip storag blob data reader access try code result",
        "Answer_gpt_summary_original":"Solution:\n- Enable the service principal and grant it with Storage Blob Data Reader access.\n- Use the provided code to register the ADLS Gen2 datastore and access the dataset.",
        "Answer_gpt_summary":"solut enabl servic princip grant storag blob data reader access us provid code regist adl gen datastor access dataset"
    },
    {
        "Question_title":"Azure ML: How to save and process CSV files with semicolon as delimiter?",
        "Question_body":"<p>Azure ML support says to me that delimiter must be comma, this would cause too much hassle with data having semicolon as separator and with a lot of commas in the cell values. <\/p>\n\n<p>So how to process semicolon separated CSV files in Azure ML? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1485357971913,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":5.0,
        "Question_view_count":2908.0,
        "Answer_body":"<p>Azure ML only accepts the comma <code>,<\/code> separated CSV. Do a little work around.\nOpen your data file using a text editor. (Notepad will do the trick). Find and replace all semicolons with 'tab' (Make it a TSV) and the commas in data values may not occur a problem then. Make sure to define that the input is a TSV; not a CSV. <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41855344",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1485838771543,
        "Question_original_content":"save process csv file semicolon delimit support sai delimit comma caus hassl data have semicolon separ lot comma cell valu process semicolon separ csv file",
        "Question_preprocessed_content":"save process csv file semicolon delimit support sai delimit comma caus hassl data have semicolon separ lot comma cell valu process semicolon separ csv file",
        "Question_gpt_summary_original":"The user is facing a challenge in processing semicolon separated CSV files in Azure ML as the platform only supports comma as a delimiter. The user has reached out to Azure ML support for a solution, but they have suggested that using semicolon as a delimiter would cause complications with data having semicolon as a separator and with a lot of commas in the cell values.",
        "Question_gpt_summary":"user face challeng process semicolon separ csv file platform support comma delimit user reach support solut suggest semicolon delimit caus complic data have semicolon separ lot comma cell valu",
        "Answer_original_content":"accept comma separ csv littl work open data file text editor notepad trick replac semicolon tab tsv comma data valu occur problem sure defin input tsv csv",
        "Answer_preprocessed_content":"accept comma separ csv littl work open data file text editor replac semicolon tab comma data valu occur problem sure defin input tsv csv",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to open the semicolon separated CSV file in a text editor and replace all semicolons with tabs to create a tab-separated file (TSV). This workaround can be used as Azure ML supports TSV files. It is important to define the input as a TSV and not a CSV. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut solut suggest discuss open semicolon separ csv file text editor replac semicolon tab creat tab separ file tsv workaround support tsv file import defin input tsv csv solut mention discuss"
    },
    {
        "Question_title":"Terraform - Optional Nested Variable",
        "Question_body":"<p>I'm trying to create a module for Sagemaker endpoints. There's an optional object variable called <code>async_inference_config<\/code>. If you omit it, the endpoint being deployed is synchronous, but if you include it, the endpoint deployed is asynchronous. To satisfy both of these usecases, the <code>async_inference_config<\/code> needs to be an optional block.<\/p>\n<p>I am unsure of how to make this block optional though.<br \/>\nAny guidance would be greatly appreciated. See example below of structure of the optional parameter.<\/p>\n<p><strong>Example:<\/strong><\/p>\n<pre><code>resource &quot;aws_sagemaker_endpoint_configuration&quot; &quot;sagemaker_endpoint_configuration&quot; {\n  count = var.create ? 1 : 0\n\n  name = var.endpoint_configuration_name\n  production_variants {\n    instance_type          = var.instance_type\n    initial_instance_count = var.instance_count\n    model_name             = var.model_name\n    variant_name           = var.variant_name\n  }\n  async_inference_config {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n  lifecycle {\n    create_before_destroy = true\n    ignore_changes        = [&quot;name&quot;]\n  }\n\n  tags = var.tags\n\n  depends_on = [aws_sagemaker_model.sagemaker_model]\n}\n<\/code><\/pre>\n<p><strong>Update:<\/strong> What I tried based on the below suggestion, which seemed to work<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.async_inference_config == null ? [] : [true]\n    content {\n      output_config {\n        s3_output_path = lookup(var.async_inference_config, &quot;s3_output_path&quot;, null)\n      }\n      client_config {\n        max_concurrent_invocations_per_instance = lookup(var.async_inference_config, &quot;max_concurrent_invocations_per_instance&quot;, null)\n      }\n    }\n  }\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658386596203,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1658442017223,
        "Question_score":0.0,
        "Question_view_count":55.0,
        "Answer_body":"<p>You could use a <code>dynamic<\/code> block [1] in combination with <code>for_each<\/code> meta-argument [2]. It would look something like:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.s3_output_path != null &amp;&amp; var.max_concurrent_invocations_per_instance != null ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<p>Of course, you could come up with a different variable, say <code>enable_async_inference_config<\/code> (probalby of type <code>bool<\/code>) and base the <code>for_each<\/code> on that, e.g.:<\/p>\n<pre><code>dynamic &quot;async_inference_config&quot; {\n    for_each = var.enable_async_inference_config ? [1] : []\n    content {\n    output_config {\n      s3_output_path = var.s3_output_path\n    }\n    client_config {\n      max_concurrent_invocations_per_instance = var.max_concurrent_invocations_per_instance\n    }\n  }\n}\n<\/code><\/pre>\n<hr \/>\n<p>[1] <a href=\"https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/expressions\/dynamic-blocks<\/a><\/p>\n<p>[2] <a href=\"https:\/\/www.terraform.io\/language\/meta-arguments\/for_each\" rel=\"nofollow noreferrer\">https:\/\/www.terraform.io\/language\/meta-arguments\/for_each<\/a><\/p>",
        "Answer_comment_count":13.0,
        "Answer_last_edit_time":1658923699476,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73061907",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1658387166296,
        "Question_original_content":"terraform option nest variabl try creat modul endpoint option object variabl call async infer config omit endpoint deploi synchron includ endpoint deploi asynchron satisfi usecas async infer config need option block unsur block option guidanc greatli appreci exampl structur option paramet exampl resourc aw endpoint configur endpoint configur count var creat var endpoint configur product variant instanc type var instanc type initi instanc count var instanc count model var model variant var variant async infer config output config output path var output path client config max concurr invoc instanc var max concurr invoc instanc lifecycl creat destroi true ignor chang tag var tag depend aw model model updat tri base suggest work dynam async infer config var async infer config null true content output config output path lookup var async infer config output path null client config max concurr invoc instanc lookup var async infer config max concurr invoc instanc null",
        "Question_preprocessed_content":"terraform option nest variabl try creat modul endpoint option object variabl call omit endpoint deploi synchron includ endpoint deploi asynchron satisfi usecas need option block unsur block option guidanc greatli appreci exampl structur option paramet exampl updat tri base suggest work",
        "Question_gpt_summary_original":"The user is trying to create a module for Sagemaker endpoints, and is facing a challenge with an optional object variable called \"async_inference_config\". If the variable is omitted, the endpoint being deployed is synchronous, but if it is included, the endpoint deployed is asynchronous. The user needs to make this block optional, but is unsure of how to do so. The user has provided an example of the structure of the optional parameter and has also shared an update on what they have tried based on a suggestion.",
        "Question_gpt_summary":"user try creat modul endpoint face challeng option object variabl call async infer config variabl omit endpoint deploi synchron includ endpoint deploi asynchron user need block option unsur user provid exampl structur option paramet share updat tri base suggest",
        "Answer_original_content":"us dynam block combin meta argument look like dynam async infer config var output path null var max concurr invoc instanc null content output config output path var output path client config max concurr invoc instanc var max concurr invoc instanc cours come differ variabl enabl async infer config probalbi type bool base dynam async infer config var enabl async infer config content output config output path var output path client config max concurr invoc instanc var max concurr invoc instanc http terraform languag express dynam block http terraform languag meta argument",
        "Answer_preprocessed_content":"us block combin look like cours come differ variabl base",
        "Answer_gpt_summary_original":"The solution suggested in the discussion is to use a dynamic block in combination with the for_each meta-argument. The user can create a dynamic block for the optional object variable and use the for_each meta-argument to make it optional. The user can also create a different variable and base the for_each on that.",
        "Answer_gpt_summary":"solut suggest discuss us dynam block combin meta argument user creat dynam block option object variabl us meta argument option user creat differ variabl base"
    },
    {
        "Question_title":"\"Session has expired\" message with Machine Learning Studio",
        "Question_body":"<p>I am getting consistent error \"Your session has expired\" (screenshot below), after logging in to machine learning studio. <\/p>\n\n<p>I have tried chrome incognito and guest windows, but no difference. <\/p>\n\n<p>I am using a new account and have signed up for Free workspace. Any suggestion to get past this or delete workspace, to start again?<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/fsqtw.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/fsqtw.png\" alt=\"Error screenshot\"><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1557237613340,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1557240710416,
        "Question_score":0.0,
        "Question_view_count":482.0,
        "Answer_body":"<p>I can reproduce your issue, I sign out and log in <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a> again, it solved my problem. Or you can try to clear the browsing data or change a browser. Anyway, the issue should be caused by the browser, not azure. Even if your account is not the owner of the workspace, when you click <code>Sign In<\/code> in <a href=\"https:\/\/studio.azureml.net\/\" rel=\"nofollow noreferrer\">https:\/\/studio.azureml.net\/<\/a> , it will create a free workspace(with a different workspace id) for you automatically.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/MXDJC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/MXDJC.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>If you want to delete the workspace, you need to let the owner of the workspace delete it, navigate to the <code>SETTINGS<\/code> on the left of the studio -> <code>NAME<\/code> -> <code>DELETE WORKSPACE<\/code>. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/E6aUl.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/E6aUl.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1557300456467,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56024354",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1557300039456,
        "Question_original_content":"session expir messag machin learn studio get consist error session expir screenshot log machin learn studio tri chrome incognito guest window differ new account sign free workspac suggest past delet workspac start",
        "Question_preprocessed_content":"session expir messag machin learn studio get consist error session expir log machin learn studio tri chrome incognito guest window differ new account sign free workspac suggest past delet workspac start",
        "Question_gpt_summary_original":"The user is encountering a consistent \"Session has expired\" error message when logging into Machine Learning Studio, despite trying different browsers. They are using a new account and have signed up for a Free workspace. The user is seeking suggestions on how to resolve the issue or delete the workspace to start again.",
        "Question_gpt_summary":"user encount consist session expir error messag log machin learn studio despit try differ browser new account sign free workspac user seek suggest resolv issu delet workspac start",
        "Answer_original_content":"reproduc issu sign log http studio net solv problem try clear brows data chang browser issu caus browser azur account owner workspac click sign http studio net creat free workspac differ workspac automat want delet workspac need let owner workspac delet navig set left studio delet workspac",
        "Answer_preprocessed_content":"reproduc issu sign log solv problem try clear brows data chang browser issu caus browser azur account owner workspac click creat free workspac automat want delet workspac need let owner workspac delet navig left studio",
        "Answer_gpt_summary_original":"Solutions provided:\n- Sign out and log in again to solve the problem.\n- Clear browsing data or change the browser.\n- The issue is caused by the browser, not Azure.\n- If the user wants to delete the workspace, they need to let the owner of the workspace delete it by navigating to SETTINGS -> NAME -> DELETE WORKSPACE.",
        "Answer_gpt_summary":"solut provid sign log solv problem clear brows data chang browser issu caus browser azur user want delet workspac need let owner workspac delet navig set delet workspac"
    },
    {
        "Question_title":"No Data being exported from 'Export Data' module in Azure ML",
        "Question_body":"Hi,\n\nI am trying to export data from Azure ML to an Azure SQL Database using the 'Export Data' module but the log file contains the following messages and no data is exported to the database.\n\n\"Not exporting to run RunHistory as the exporter is either stopped or there is no data\"\n\n\"Process exiting with code: 0\n\nThere is definitely data flowing to the 'Export Data' module from an 'Execute R Script' module as I have checked the Result dataset.\n\nWould appreciate some assistance.\n\nThank you.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1629008927050,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi,\n\nI have resolved this issue. I had set the export table to be dbo.TestTable rather than just TestTable. As the table dbo.TestTable did not exist the 'Export module' created it in the dbo schema so the table name effectively became dbo.dbo.TestTable.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/514067\/no-data-being-exported-from-39export-data39-module.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1629106747876,
        "Question_original_content":"data export export data modul try export data azur sql databas export data modul log file contain follow messag data export databas export run runhistori export stop data process exit code definit data flow export data modul execut script modul check result dataset appreci assist thank",
        "Question_preprocessed_content":"data export export data modul try export data azur sql databas export data modul log file contain follow messag data export databas export run runhistori export stop data process exit code definit data flow export data modul execut script modul check result dataset appreci assist thank",
        "Question_gpt_summary_original":"The user is facing challenges in exporting data from Azure ML to an Azure SQL Database using the 'Export Data' module. The log file shows messages indicating that no data is being exported due to the exporter being stopped or no data being available. The user has confirmed that data is flowing to the 'Export Data' module from an 'Execute R Script' module.",
        "Question_gpt_summary":"user face challeng export data azur sql databas export data modul log file show messag indic data export export stop data avail user confirm data flow export data modul execut script modul",
        "Answer_original_content":"resolv issu set export tabl dbo testtabl testtabl tabl dbo testtabl exist export modul creat dbo schema tabl effect dbo dbo testtabl",
        "Answer_preprocessed_content":"resolv issu set export tabl testtabl tabl exist export modul creat dbo schema tabl effect",
        "Answer_gpt_summary_original":"Solution: The user resolved the issue by setting the export table to just TestTable instead of dbo.TestTable, as the table dbo.TestTable did not exist and the 'Export module' created it in the dbo schema, effectively making the table name dbo.dbo.TestTable.",
        "Answer_gpt_summary":"solut user resolv issu set export tabl testtabl instead dbo testtabl tabl dbo testtabl exist export modul creat dbo schema effect make tabl dbo dbo testtabl"
    },
    {
        "Question_title":"pip install azureml-sdk with latest patches to underlying libraries",
        "Question_body":"<p>How do I upgrade <code>azureml-sdk<\/code> such that the newest release of <code>azureml-core<\/code>, <code>1.1.5.5<\/code>, is installed? \nIf <code>azureml-sdk<\/code> is not installed, <code>pip install --upgrade azureml-sdk<\/code> will install <code>azureml-core==1.1.5.5<\/code>. If it is already installed, then it won't.<\/p>\n\n<pre><code>$ pip list --format=freeze | grep 'azureml-core'`\n&gt; azureml-core==1.1.5.1\n$ pip install --upgrade azureml-sdk[interpret,notebooks]\n$ pip list --format=freeze | grep 'azureml-core'`\n&gt; azureml-core==1.1.5.1\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1584723850063,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1584724828660,
        "Question_score":0.0,
        "Question_view_count":1142.0,
        "Answer_body":"<p>You can use the eager strategy to force an upgrade of requirements:<\/p>\n\n<pre><code>pip install -U --upgrade-strategy eager azureml-sdk\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60778546",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1584735835707,
        "Question_original_content":"pip instal sdk latest patch underli librari upgrad sdk newest releas core instal sdk instal pip instal upgrad sdk instal core instal won pip list format freez grep core core pip instal upgrad sdk interpret notebook pip list format freez grep core core",
        "Question_preprocessed_content":"pip instal sdk latest patch underli librari upgrad newest releas instal instal instal instal won",
        "Question_gpt_summary_original":"The user is facing a challenge in upgrading the azureml-sdk to the latest release of azureml-core, version 1.1.5.5. The user has tried upgrading using pip install --upgrade azureml-sdk[interpret,notebooks], but the upgrade did not install the latest version of azureml-core.",
        "Question_gpt_summary":"user face challeng upgrad sdk latest releas core version user tri upgrad pip instal upgrad sdk interpret notebook upgrad instal latest version core",
        "Answer_original_content":"us eager strategi forc upgrad requir pip instal upgrad strategi eager sdk",
        "Answer_preprocessed_content":"us eager strategi forc upgrad requir",
        "Answer_gpt_summary_original":"Solution: The user can use the eager strategy to force an upgrade of requirements by running the command \"pip install -U --upgrade-strategy eager azureml-sdk\".",
        "Answer_gpt_summary":"solut user us eager strategi forc upgrad requir run command pip instal upgrad strategi eager sdk"
    },
    {
        "Question_title":"How to get Model ID of the Latest Version registered in Azure Machine Learning Service Model Registry using az ml cli?",
        "Question_body":"<p>Using Azure Machine Learning CLI extension, how do we get the Model ID for the latest version of a Model (with known model name)?<\/p>\n<p>To get the entire list of Model Details with a given name the command is<\/p>\n<pre><code>az ml model list --model-name [Model_Name] --resource-group [RGP_NAME] --subscription-id [SUB_ID] --workspace-name [WS_NAME]\n<\/code><\/pre>\n<p>Running this will give a list of all the models:<\/p>\n<pre><code>[\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T07:02:03.814172+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:2&quot;\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 3\n  },\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T06:46:34.301054+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:2&quot;,\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 2\n  },\n  {\n    &quot;createdTime&quot;: &quot;2021-03-19T06:38:56.558385+00:00&quot;,\n    &quot;framework&quot;: &quot;Custom&quot;,\n    &quot;frameworkVersion&quot;: null,\n    &quot;id&quot;: &quot;model:1&quot;,\n    &quot;name&quot;: &quot;model&quot;,\n    &quot;version&quot;: 1\n  }\n]\n<\/code><\/pre>\n<p>The <a href=\"https:\/\/docs.microsoft.com\/en-us\/cli\/azure\/ext\/azure-cli-ml\/ml\/model?view=azure-cli-latest#ext_azure_cli_ml_az_ml_model_list\" rel=\"nofollow noreferrer\">Microsoft Documentation<\/a> mentions, we can use a <code>-l<\/code> parameter to get the latest version details:<\/p>\n<pre><code>az ml model list --model-name [Model_Name] --resource-group [RGP_NAME] --subscription-id [SUB_ID] --workspace-name [WS_NAME] -l\n<\/code><\/pre>\n<p>However, running this gives the following error:<\/p>\n<pre><code>ERROR: UnrecognizedArgumentError: unrecognized arguments: -l\n<\/code><\/pre>\n<p>What is the syntax to use this <code>-l<\/code> flag?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_created_time":1616140269540,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":545.0,
        "Answer_body":"<p>If we wish to obtain the model-id for the latest model, instead of using <code>az ml model<\/code> list with <code>-l<\/code> flag, using <code>az model show<\/code> will return the details for the latest model. The syntax to get a string for model-id will be:<\/p>\n<pre><code>az ml model show --model-id $(TRN_MODEL_ID) --resource-group $(AML_TRN_RG) --subscription-id $(AML_TRN_SUB_ID) --workspace-name $(AML_TRN_WS) --query name -o tsv\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66704314",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1616423749203,
        "Question_original_content":"model latest version regist servic model registri cli cli extens model latest version model known model entir list model detail given command model list model model resourc group rgp subscript sub workspac run list model createdtim framework custom frameworkvers null model model version createdtim framework custom frameworkvers null model model version createdtim framework custom frameworkvers null model model version microsoft document mention us paramet latest version detail model list model model resourc group rgp subscript sub workspac run give follow error error unrecognizedargumenterror unrecogn argument syntax us flag",
        "Question_preprocessed_content":"model latest version regist servic model registri cli cli extens model latest version model entir list model detail given command run list model microsoft document mention us paramet latest version detail run give follow error syntax us flag",
        "Question_gpt_summary_original":"The user is trying to get the Model ID for the latest version of a Model using Azure Machine Learning CLI extension. They have tried using the command \"az ml model list\" with the \"-l\" parameter to get the latest version details, but it resulted in an error. The user is seeking help to understand the correct syntax to use the \"-l\" flag.",
        "Question_gpt_summary":"user try model latest version model cli extens tri command model list paramet latest version detail result error user seek help understand correct syntax us flag",
        "Answer_original_content":"wish obtain model latest model instead model list flag model return detail latest model syntax string model model model trn model resourc group aml trn subscript aml trn sub workspac aml trn queri tsv",
        "Answer_preprocessed_content":"wish obtain latest model instead list flag return detail latest model syntax string",
        "Answer_gpt_summary_original":"Solution: Instead of using \"az ml model list\" with the \"-l\" flag, the user can use \"az model show\" to obtain the model ID for the latest model. The syntax to get a string for model ID will be: \"az ml model show --model-id $(TRN_MODEL_ID) --resource-group $(AML_TRN_RG) --subscription-id $(AML_TRN_SUB_ID) --workspace-name $(AML_TRN_WS) --query name -o tsv\".",
        "Answer_gpt_summary":"solut instead model list flag user us model obtain model latest model syntax string model model model trn model resourc group aml trn subscript aml trn sub workspac aml trn queri tsv"
    },
    {
        "Question_title":"Can I use AWS CLI to add tags to all processing jobs matching a certain regex",
        "Question_body":"<p>I have close to 100 processing jobs to which I want to add certain tags. I've found commands that you can use to tag one resource with a list of tags. Is there any way I can do this for multiple jobs? Through CLI or through python+boto?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1657515736330,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1657607287156,
        "Question_score":0.0,
        "Question_view_count":38.0,
        "Answer_body":"<p>You can <code>ResourceGroupsTaggingAPI<\/code>'s method <code>tag_resources()<\/code>.<br \/>\nThis is used to apply one or more tags to the specified list of resources.<\/p>\n<p>References:<\/p>\n<ol>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.tag_resources\" rel=\"nofollow noreferrer\">Tag Resources using boto3<\/a><\/li>\n<li><a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/resourcegroupstaggingapi.html#ResourceGroupsTaggingAPI.Client.untag_resources\" rel=\"nofollow noreferrer\">UnTag Resources using boto3<\/a><\/li>\n<\/ol>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72933908",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1657516175783,
        "Question_original_content":"us aw cli add tag process job match certain regex close process job want add certain tag command us tag resourc list tag wai multipl job cli python boto",
        "Question_preprocessed_content":"us aw cli add tag process job match certain regex close process job want add certain tag command us tag resourc list tag wai multipl job cli python boto",
        "Question_gpt_summary_original":"The user is facing a challenge of adding tags to close to 100 processing jobs. They are looking for a way to add tags to multiple jobs using AWS CLI or python+boto instead of tagging one resource at a time.",
        "Question_gpt_summary":"user face challeng ad tag close process job look wai add tag multipl job aw cli python boto instead tag resourc time",
        "Answer_original_content":"resourcegroupstaggingapi method tag resourc appli tag specifi list resourc refer tag resourc boto untag resourc boto",
        "Answer_preprocessed_content":"method appli tag specifi list resourc refer tag resourc boto untag resourc boto",
        "Answer_gpt_summary_original":"Solution: The user can use the `tag_resources()` method of `ResourceGroupsTaggingAPI` using boto3 to apply one or more tags to the specified list of resources. References to the documentation of `tag_resources()` and `untag_resources()` methods are provided.",
        "Answer_gpt_summary":"solut user us tag resourc method resourcegroupstaggingapi boto appli tag specifi list resourc refer document tag resourc untag resourc method provid"
    },
    {
        "Question_title":"Can't associate sweeps with project",
        "Question_body":"<p>Hi,<\/p>\n<p>I\u2019m using wandb (great product!!!) and have been able to set up projects, do runs and am now working with sweeps (FANTASTIC!). However I can\u2019t figure out how to associate my sweeps with a project.<\/p>\n<p>I have:<\/p>\n<pre><code class=\"lang-auto\">import wandb\n\nsweep_config = {\n  \"project\" : \"HDBSCAN_Clustering\",\n  \"method\" : \"random\",\n  \"parameters\" : {\n    \"min_cluster_size\" :{\n      \"values\": [*range(20,500)]\n    },\n    \"min_sample_pct\" :{\n      \"values\": [.25, .5, .75, 1.0]\n    }\n  }\n}\n<\/code><\/pre>\n<p>Then when I:<\/p>\n<p>sweep_id = wandb.sweep(sweep_config)<\/p>\n<p>I get<\/p>\n<p><code>Sweep URL: https:\/\/wandb.ai\/teamberkeley\/uncategorized\/sweeps\/jk9c1l8q<\/code><\/p>\n<p>Note:  teamberkeley\/<em>uncategorized<\/em>\/sweeps<\/p>\n<p>They are of course uncategorized in the projects interface as well.<\/p>\n<p>No luck with running wandb.init beforehand either thusly:<\/p>\n<p>wandb.init(project=\u2018HDBSCAN_Clustering\u2019)<\/p>\n<p>Same result (despite the fact that at this point if I do \u2018runs\u2019 with wandb they are attached to the correct project after this init). Please let me know what I\u2019m doing wrong!<\/p>",
        "Question_answer_count":9,
        "Question_comment_count":0,
        "Question_created_time":1655665054170,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":565.0,
        "Answer_body":"<p>Ahhh fixed.  The entity is \u2018drob707\u2019, not \u2018drob\u2019.  Thanks!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/cant-associate-sweeps-with-project\/2636",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1656556961635,
        "Question_original_content":"associ sweep project great product abl set project run work sweep fantast figur associ sweep project import sweep config project hdbscan cluster method random paramet min cluster size valu rang min sampl pct valu sweep sweep sweep config sweep url http teamberkelei uncategor sweep jkclq note teamberkelei uncategor sweep cours uncategor project interfac luck run init thusli init project hdbscan cluster result despit fact point run attach correct project init let know wrong",
        "Question_preprocessed_content":"associ sweep project abl set project run work sweep figur associ sweep project note cours uncategor project interfac luck run init thusli result let know wrong",
        "Question_gpt_summary_original":"The user is having trouble associating their sweeps with a project in wandb. They have tried setting the project name in the sweep configuration and using wandb.init with the project name, but the sweeps are still showing up as uncategorized in the projects interface. The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user have troubl associ sweep project tri set project sweep configur init project sweep show uncategor project interfac user seek assist resolv issu",
        "Answer_original_content":"fix entiti drob drob thank",
        "Answer_preprocessed_content":"fix entiti drob drob thank",
        "Answer_gpt_summary_original":"Solution: The user realized that they were using the wrong entity name and corrected it to 'drob707'. This resolved the issue of sweeps showing up as uncategorized in the projects interface.",
        "Answer_gpt_summary":"solut user realiz wrong entiti correct drob resolv issu sweep show uncategor project interfac"
    },
    {
        "Question_title":"how to configure ideal value for MaxConcurrentTransforms in setting up a sagemaker batch transform ?",
        "Question_body":"based on the documentation , https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/batch-transform.html, it states that \" The ideal value for MaxConcurrentTransforms is equal to the number of compute workers in the batch transform job.\" how to figure out what the number of compute workers is , i assume this depends on the instance type. also what about the instance count parameter we can set , do we have to take that into account as well?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1649205572690,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":152.0,
        "Answer_body":"The ideal value for MaxConcurrentTransforms varies based on instance type as well as based on your specific model.\n\nit could make sense to increase MaxConcurrentTransforms up to the core count of the instance you are using (for cpu based transform), however, you should also take into account the memory utilisation by your model.\n\nThe ultimate answer is it \"depends\" and I would recommend that you experiment with increasing this number gradually from 1 up to instance core count, while monitoring RAM\/cpu utilisation to find the optimal.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKcUBF0wPQSyerTP2IK53hQ\/how-to-configure-ideal-value-for-max-concurrent-transforms-in-setting-up-a-sagemaker-batch-transform",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1649246917948,
        "Question_original_content":"configur ideal valu maxconcurrenttransform set batch transform base document http doc aw amazon com latest batch transform html state ideal valu maxconcurrenttransform equal number comput worker batch transform job figur number comput worker assum depend instanc type instanc count paramet set account",
        "Question_preprocessed_content":"configur ideal valu maxconcurrenttransform set batch transform base document state ideal valu maxconcurrenttransform equal number comput worker batch transform figur number comput worker assum depend instanc type instanc count paramet set account",
        "Question_gpt_summary_original":"The user is facing challenges in configuring the ideal value for MaxConcurrentTransforms while setting up a Sagemaker batch transform. The documentation suggests that the ideal value for MaxConcurrentTransforms is equal to the number of compute workers in the batch transform job, but the user is unsure how to determine the number of compute workers, which may depend on the instance type. The user is also unsure if the instance count parameter needs to be taken into account.",
        "Question_gpt_summary":"user face challeng configur ideal valu maxconcurrenttransform set batch transform document suggest ideal valu maxconcurrenttransform equal number comput worker batch transform job user unsur determin number comput worker depend instanc type user unsur instanc count paramet need taken account",
        "Answer_original_content":"ideal valu maxconcurrenttransform vari base instanc type base specif model sens increas maxconcurrenttransform core count instanc cpu base transform account memori utilis model ultim answer depend recommend experi increas number gradual instanc core count monitor ram cpu utilis optim",
        "Answer_preprocessed_content":"ideal valu maxconcurrenttransform vari base instanc type base specif model sens increas maxconcurrenttransform core count instanc account memori utilis model ultim answer depend recommend experi increas number gradual instanc core count monitor utilis optim",
        "Answer_gpt_summary_original":"Solution: The ideal value for MaxConcurrentTransforms varies based on the instance type and the specific model being used. It is recommended to increase MaxConcurrentTransforms up to the core count of the instance for CPU-based transforms, while also taking into account the memory utilization by the model. It is suggested to experiment with increasing the number gradually from 1 up to the instance core count while monitoring RAM\/CPU utilization to find the optimal value.",
        "Answer_gpt_summary":"solut ideal valu maxconcurrenttransform vari base instanc type specif model recommend increas maxconcurrenttransform core count instanc cpu base transform take account memori util model suggest experi increas number gradual instanc core count monitor ram cpu util optim valu"
    },
    {
        "Question_title":"What are valid Azure ML Workspace connection argument options?",
        "Question_body":"<p>I want to build an Azure ML environment with two python packages that I have in Azure Devops.\nFor this I need a workspace connection to Azure Devops. One package is published to an artifact feed and I can access it using the python SDK using a personal access token:<\/p>\n<pre><code>ws.set_connection(name=&quot;ConnectionName&quot;, \n                  category= &quot;PythonFeed&quot;, \n                  target = &quot;https:\/\/pkgs.dev.azure.com\/&quot;, \n                  authType = &quot;PAT&quot;, \n                  value = PAT_TOKEN)\n<\/code><\/pre>\n<p>However, for the other I need to get the package from the git repository in Azure Devops. The documentation of the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.workspace.workspace?view=azure-ml-py#azureml-core-workspace-workspace-set-connection\" rel=\"nofollow noreferrer\">Python SDK<\/a> and the underlying <a href=\"https:\/\/docs.microsoft.com\/en-us\/rest\/api\/azureml\/workspace-connections\/create\" rel=\"nofollow noreferrer\">REST API<\/a> don't give the options for the arguments, only that they need to be strings (see links).<\/p>\n<p>My question: what are the options for the following arguments:<\/p>\n<ul>\n<li>authType<\/li>\n<li>category<\/li>\n<li>valueFormat<\/li>\n<\/ul>\n<p>And what do I need to set for target argument, so that I can connect to the Azure DevOps repository with potentially different authentication?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1646219844250,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":93.0,
        "Answer_body":"<p>To get the package from a Azure DevOps git repository you can change the target to the repository URL:<\/p>\n<pre><code>ws.set_connection(\n    name=&quot;ConnectionName&quot;, \n    category = &quot;PythonFeed&quot;,\n    target = &quot;https:\/\/dev.azure.com\/&lt;MY-ORG&gt;\/&lt;MY-PROJECT&gt;\/_git\/&lt;MY-REPO&gt;&quot;, \n    authType = &quot;PAT&quot;, \n    value = &lt;PAT-TOKEN&gt;)\n<\/code><\/pre>\n<p>Note here that there is no user specified in the URL (the standard &quot;clone&quot; URL in Azure DevOps also contains &quot;DevOps-Vx@&quot;).<\/p>\n<p>As for any other options for &quot;authType&quot;, &quot;category&quot; and &quot;valueFormat&quot;, I don't know.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71321757",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659435013616,
        "Question_original_content":"valid workspac connect argument option want build environ python packag azur devop need workspac connect azur devop packag publish artifact feed access python sdk person access token set connect connectionnam categori pythonfe target http pkg dev azur com authtyp pat valu pat token need packag git repositori azur devop document python sdk underli rest api option argument need string link question option follow argument authtyp categori valueformat need set target argument connect azur devop repositori potenti differ authent",
        "Question_preprocessed_content":"valid workspac connect argument option want build environ python packag azur devop need workspac connect azur devop packag publish artifact feed access python sdk person access token need packag git repositori azur devop document python sdk underli rest api option argument need string question option follow argument authtyp categori valueformat need set target argument connect azur devop repositori potenti differ authent",
        "Question_gpt_summary_original":"The user is trying to build an Azure ML environment with two python packages from Azure Devops, but is facing challenges in setting up a workspace connection to access the packages. While they were able to access one package using a personal access token, they are unsure about the options for the authType, category, and valueFormat arguments for the other package. Additionally, they are unsure about what to set for the target argument to connect to the Azure DevOps repository with potentially different authentication.",
        "Question_gpt_summary":"user try build environ python packag azur devop face challeng set workspac connect access packag abl access packag person access token unsur option authtyp categori valueformat argument packag addition unsur set target argument connect azur devop repositori potenti differ authent",
        "Answer_original_content":"packag azur devop git repositori chang target repositori url set connect connectionnam categori pythonfe target http dev azur com git authtyp pat valu note user specifi url standard clone url azur devop contain devop option authtyp categori valueformat know",
        "Answer_preprocessed_content":"packag azur devop git repositori chang target repositori url note user specifi url option authtyp categori valueformat know",
        "Answer_gpt_summary_original":"Solution: The user can change the target to the repository URL to get the package from an Azure DevOps git repository. They can set the connection with the name, category, target, authType, and value arguments. However, there were no solutions provided for the authType, category, and valueFormat arguments for the other package.",
        "Answer_gpt_summary":"solut user chang target repositori url packag azur devop git repositori set connect categori target authtyp valu argument solut provid authtyp categori valueformat argument packag"
    },
    {
        "Question_title":"Sagemaker Built-in Algorithms",
        "Question_body":"I am exploring the Sagemaker Built-in algorithms, and I am curious to learn more about the details of the algorithms. However, I am surprised that it is hard to find any references for the research background and implementation details in the numerous documents and tutorials for particular algorithms. If such information exists somewhere, I would highly appreciate a pointer. Thanks a lot in advance!",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1652686627960,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":"thanks for your interest in the built-in algorithms! You can find research papers in the documentation of many of them. And documentation page has a section \"how it works\" explaining the science of every algorithm. For example:\n\nBlazingText: BlazingText: Scaling and Accelerating Word2Vec using Multiple GPUs, Gupta et Khare\nDeepAR DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks, Salinas et al.\nFactorization Machines\nIP Insights\nKMeans\nKNN\nLDA\nLinear Learner\nNTM\nObject2Vec\nObject Detection (it's an SSD model)\nPCA\nRandom Cut Forest: Robust Random Cut Forest Based Anomaly Detection On Streams, Guha et al\nSemantic Segmentation\nSeq2seq\nXGBoost",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDkYruiibS9S05bzFSkLaxg\/sagemaker-built-in-algorithms",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1652688680111,
        "Question_original_content":"built algorithm explor built algorithm curiou learn detail algorithm surpris hard refer research background implement detail numer document tutori particular algorithm inform exist highli appreci pointer thank lot advanc",
        "Question_preprocessed_content":"algorithm explor algorithm curiou learn detail algorithm surpris hard refer research background implement detail numer document tutori particular algorithm inform exist highli appreci pointer thank lot advanc",
        "Question_gpt_summary_original":"The user is facing challenges in finding research background and implementation details of Sagemaker Built-in algorithms despite going through numerous documents and tutorials. They are seeking help in finding relevant information.",
        "Question_gpt_summary":"user face challeng find research background implement detail built algorithm despit go numer document tutori seek help find relev inform",
        "Answer_original_content":"thank built algorithm research paper document document page section work explain scienc algorithm exampl blazingtext blazingtext scale acceler wordvec multipl gpu gupta khare deepar deepar probabilist forecast autoregress recurr network salina factor machin insight kmean knn lda linear learner ntm objectvec object detect ssd model pca random cut forest robust random cut forest base anomali detect stream guha semant segment seqseq xgboost",
        "Answer_preprocessed_content":"thank algorithm research paper document document page section work explain scienc algorithm exampl blazingtext blazingtext scale acceler word vec multipl gpu gupta khare deepar deepar probabilist forecast autoregress recurr network salina factor machin insight kmean knn lda linear learner ntm object vec object detect pca random cut forest robust random cut forest base anomali detect stream guha semant segment seq seq xgboost",
        "Answer_gpt_summary_original":"Solution: The discussion provides a solution to the challenge by suggesting that the user can find research papers in the documentation of many of the Sagemaker Built-in algorithms. The documentation page also has a section \"how it works\" explaining the science of every algorithm. The discussion further provides a list of research papers for some of the algorithms, including BlazingText, DeepAR, Factorization Machines, IP Insights, KMeans, KNN, LDA, Linear Learner, NTM, Object2Vec, Object Detection, PCA, Random Cut Forest, Semantic Segmentation, Seq2seq, and XGBoost.",
        "Answer_gpt_summary":"solut discuss provid solut challeng suggest user research paper document built algorithm document page section work explain scienc algorithm discuss provid list research paper algorithm includ blazingtext deepar factor machin insight kmean knn lda linear learner ntm objectvec object detect pca random cut forest semant segment seqseq xgboost"
    },
    {
        "Question_title":"How to install TensorFlow in jupyter notebook on Azure Machine Learning Studio",
        "Question_body":"<p>I'm trying to test Azure Machine Learning Studio. <\/p>\n\n<p>I want to use TensorFlow, but it is not installed on Jupyter notebook.<\/p>\n\n<p>How can I use some machine learning libraries like TensorFlow, Theano, Keras,... on the notebook?<\/p>\n\n<p>I tried this:<\/p>\n\n<pre><code>!pip install tensorflow \n<\/code><\/pre>\n\n<p>But, I got error as below:<\/p>\n\n<pre><code>Collecting tensorflow\n  Downloading tensorflow-0.12.0rc0-cp34-cp34m-manylinux1_x86_64.whl (43.1MB)\n    100% |################################| 43.1MB 27kB\/s \nCollecting protobuf==3.1.0 (from tensorflow)\n  Downloading protobuf-3.1.0-py2.py3-none-any.whl (339kB)\n    100% |################################| 348kB 3.7MB\/s \nCollecting six&gt;=1.10.0 (from tensorflow)\n  Downloading six-1.10.0-py2.py3-none-any.whl\nRequirement already satisfied: numpy&gt;=1.11.0 in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages (from tensorflow)\nRequirement already satisfied: wheel&gt;=0.26 in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages (from tensorflow)\nRequirement already satisfied: setuptools in \/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/setuptools-27.2.0-py3.4.egg (from protobuf==3.1.0-&gt;tensorflow)\nInstalling collected packages: six, protobuf, tensorflow\n  Found existing installation: six 1.9.0\n    DEPRECATION: Uninstalling a distutils installed project (six) has been deprecated and will be removed in a future version. This is due to the fact that uninstalling a distutils project will only partially uninstall the project.\n    Uninstalling six-1.9.0:\n      Successfully uninstalled six-1.9.0\n  Rolling back uninstall of six\nException:\nTraceback (most recent call last):\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/basecommand.py\", line 215, in main\n    status = self.run(options, args)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/commands\/install.py\", line 342, in run\n    prefix=options.prefix_path,\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_set.py\", line 784, in install\n    **kwargs\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_install.py\", line 851, in install\n    self.move_wheel_files(self.source_dir, root=root, prefix=prefix)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/req\/req_install.py\", line 1064, in move_wheel_files\n    isolated=self.isolated,\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/wheel.py\", line 345, in move_wheel_files\n    clobber(source, lib_dir, True)\n  File \"\/home\/nbcommon\/anaconda3_23\/lib\/python3.4\/site-packages\/pip\/wheel.py\", line 329, in clobber\n    os.utime(destfile, (st.st_atime, st.st_mtime))\nPermissionError: [Errno 1] Operation not permitted\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1481173386563,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":7.0,
        "Question_view_count":2886.0,
        "Answer_body":"<p>As you noticed, the active user doesn't have permissions to write to the <code>site-packages<\/code> directory in Azure Machine Learning Studio notebooks. You could try installing the package to another directory where you do have write permissions (like the default working directory) and importing from there, but I recommend the following lower-hassle option.<\/p>\n\n<p><a href=\"https:\/\/notebooks.azure.com\" rel=\"nofollow noreferrer\">Azure Notebooks<\/a> is a separate Jupyter Notebook service that will allow you to install tensorflow, theano, and keras. Like the notebooks in AML Studio, these notebooks will persist in your account. The primary downside is that if you want to access your workspace through e.g. the Python <code>azureml<\/code> package, you'll need to <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\" rel=\"nofollow noreferrer\">provide your workspace id\/authorization token<\/a> to set up the connection. (In Azure ML Studio, those values are loaded automatically from the current workspace.) Otherwise I believe Azure Notebooks can do everything you are used to doing inside AML Studio only.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41032108",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1485956468903,
        "Question_original_content":"instal tensorflow jupyt notebook studio try test studio want us tensorflow instal jupyt notebook us machin learn librari like tensorflow theano kera notebook tri pip instal tensorflow got error collect tensorflow download tensorflow cpm manylinux whl collect protobuf tensorflow download protobuf whl collect tensorflow download whl requir satisfi numpi home nbcommon anaconda lib python site packag tensorflow requir satisfi wheel home nbcommon anaconda lib python site packag tensorflow requir satisfi setuptool home nbcommon anaconda lib python site packag setuptool egg protobuf tensorflow instal collect packag protobuf tensorflow exist instal deprec uninstal distutil instal project deprec remov futur version fact uninstal distutil project partial uninstal project uninstal successfulli uninstal roll uninstal except traceback recent file home nbcommon anaconda lib python site packag pip basecommand line main statu self run option arg file home nbcommon anaconda lib python site packag pip command instal line run prefix option prefix path file home nbcommon anaconda lib python site packag pip req req set line instal kwarg file home nbcommon anaconda lib python site packag pip req req instal line instal self wheel file self sourc dir root root prefix prefix file home nbcommon anaconda lib python site packag pip req req instal line wheel file isol self isol file home nbcommon anaconda lib python site packag pip wheel line wheel file clobber sourc lib dir true file home nbcommon anaconda lib python site packag pip wheel line clobber utim destfil atim mtime permissionerror errno oper permit",
        "Question_preprocessed_content":"instal tensorflow jupyt notebook studio try test studio want us tensorflow instal jupyt notebook us machin learn librari like tensorflow theano notebook tri got error",
        "Question_gpt_summary_original":"The user is encountering challenges in installing TensorFlow on Jupyter notebook in Azure Machine Learning Studio. The user attempted to install TensorFlow using the command \"!pip install tensorflow\" but encountered an error related to permission.",
        "Question_gpt_summary":"user encount challeng instal tensorflow jupyt notebook studio user attempt instal tensorflow command pip instal tensorflow encount error relat permiss",
        "Answer_original_content":"notic activ user permiss write site packag directori studio notebook try instal packag directori write permiss like default work directori import recommend follow lower hassl option azur notebook separ jupyt notebook servic allow instal tensorflow theano kera like notebook aml studio notebook persist account primari downsid want access workspac python packag need provid workspac author token set connect studio valu load automat current workspac believ azur notebook insid aml studio",
        "Answer_preprocessed_content":"notic activ user permiss write directori studio notebook try instal packag directori write permiss import recommend follow option azur notebook separ jupyt notebook servic allow instal tensorflow theano kera like notebook aml studio notebook persist account primari downsid want access workspac python packag need provid workspac token set connect believ azur notebook insid aml studio",
        "Answer_gpt_summary_original":"Solution: The user can try installing TensorFlow to another directory where they have write permissions or use Azure Notebooks, a separate Jupyter Notebook service that allows the installation of TensorFlow, theano, and keras. The downside of using Azure Notebooks is that the user will need to provide their workspace id\/authorization token to set up the connection if they want to access their workspace through the Python azureml package.",
        "Answer_gpt_summary":"solut user try instal tensorflow directori write permiss us azur notebook separ jupyt notebook servic allow instal tensorflow theano kera downsid azur notebook user need provid workspac author token set connect want access workspac python packag"
    },
    {
        "Question_title":"Deployment from Designer fails in every possible way",
        "Question_body":"I trained a model with Designer, created a real-time inference pipeline which was succesfully submitted. When deploying to either ACI or AKS it fails and I get the error \"ModuleNotFoundError: No module named 'azureml.api'\". I've had no problems deploying this model many times in the past and haven't changed anything. Even if I use one of the sample pipelines (automobiles basic), I get the same error when deploying to real-time.",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_created_time":1632862175993,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"It's an known issue caused by unexpected module version upgrade. It's been resolved by applying hotfix to all regions. For users, please rerun training pipeline by check on \"Regenerate Output\", and run corresponding inference pipeline and try deployment again.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/569925\/deployment-from-designer-fails-in-every-possible-w.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1632968850120,
        "Question_original_content":"deploy design fail possibl wai train model design creat real time infer pipelin succesfulli submit deploi aci ak fail error modulenotfounderror modul name api problem deploi model time past haven chang us sampl pipelin automobil basic error deploi real time",
        "Question_preprocessed_content":"deploy design fail possibl wai train model design creat infer pipelin succesfulli submit deploi aci ak fail error modulenotfounderror modul name problem deploi model time past haven chang us sampl pipelin error deploi",
        "Question_gpt_summary_original":"The user is encountering challenges with deploying a model from Designer to ACI or AKS. The error message \"ModuleNotFoundError: No module named 'azureml.api'\" is preventing successful deployment, even when using sample pipelines. The user has not made any changes to the model and has successfully deployed it in the past.",
        "Question_gpt_summary":"user encount challeng deploi model design aci ak error messag modulenotfounderror modul name api prevent success deploy sampl pipelin user chang model successfulli deploi past",
        "Answer_original_content":"known issu caus unexpect modul version upgrad resolv appli hotfix region user rerun train pipelin check regener output run correspond infer pipelin try deploy",
        "Answer_preprocessed_content":"known issu caus unexpect modul version upgrad resolv appli hotfix region user rerun train pipelin check regener output run correspond infer pipelin try deploy",
        "Answer_gpt_summary_original":"Solution: The issue causing the error message has been identified as an unexpected module version upgrade. The solution is to apply a hotfix to all regions. Users should rerun the training pipeline by checking on \"Regenerate Output\" and then run the corresponding inference pipeline before attempting deployment again.",
        "Answer_gpt_summary":"solut issu caus error messag identifi unexpect modul version upgrad solut appli hotfix region user rerun train pipelin check regener output run correspond infer pipelin attempt deploy"
    },
    {
        "Question_title":"Sagemaker Training Job Not Uploading\/Saving Training Model to S3 Output Path",
        "Question_body":"<p>Ok I've been dealing with this issue in Sagemaker for almost a week and I'm ready to pull my hair out. I've got a custom training script paired with a data processing script in a BYO algorithm Docker deployment type scenario. It's a Pytorch model built with Python 3.x, and the BYO Docker file was originally built for Python 2, but I can't see an issue with the problem that I am having.....which is that after a successful training run Sagemaker doesn't save the model to the target S3 bucket.<\/p>\n<p>I've searched far and wide and can't seem to find an applicable answer anywhere. This is all done inside a Notebook instance. Note: I am using this as a contractor and don't have full permissions to the rest of AWS, including downloading the Docker image.<\/p>\n<p>Dockerfile:<\/p>\n<pre><code>FROM ubuntu:18.04\n\nMAINTAINER Amazon AI &lt;sage-learner@amazon.com&gt;\n\nRUN apt-get -y update &amp;&amp; apt-get install -y --no-install-recommends \\\n         wget \\\n         python-pip \\\n         python3-pip3\n         nginx \\\n         ca-certificates \\\n    &amp;&amp; rm -rf \/var\/lib\/apt\/lists\/*\n\nRUN wget https:\/\/bootstrap.pypa.io\/get-pip.py &amp;&amp; python3 get-pip.py &amp;&amp; \\\n    pip3 install future numpy torch scipy scikit-learn pandas flask gevent gunicorn &amp;&amp; \\\n        rm -rf \/root\/.cache\n\nENV PYTHONUNBUFFERED=TRUE\nENV PYTHONDONTWRITEBYTECODE=TRUE\nENV PATH=&quot;\/opt\/program:${PATH}&quot;\n\nCOPY decision_trees \/opt\/program\nWORKDIR \/opt\/program\n<\/code><\/pre>\n<p>Docker Image Build:<\/p>\n<pre><code>%%sh\n\nalgorithm_name=&quot;name-this-algo&quot;\n\ncd container\n\nchmod +x decision_trees\/train\nchmod +x decision_trees\/serve\n\naccount=$(aws sts get-caller-identity --query Account --output text)\n\nregion=$(aws configure get region)\nregion=${region:-us-east-2}\n\nfullname=&quot;${account}.dkr.ecr.${region}.amazonaws.com\/${algorithm_name}:latest&quot;\n\naws ecr describe-repositories --repository-names &quot;${algorithm_name}&quot; &gt; \/dev\/null 2&gt;&amp;1\n\nif [ $? -ne 0 ]\nthen\n    aws ecr create-repository --repository-name &quot;${algorithm_name}&quot; &gt; \/dev\/null\nfi\n\n# Get the login command from ECR and execute it directly\n$(aws ecr get-login --region ${region} --no-include-email)\n\n# Build the docker image locally with the image name and then push it to ECR\n# with the full name.\n\ndocker build  -t ${algorithm_name} .\ndocker tag ${algorithm_name} ${fullname}\n\ndocker push ${fullname}\n<\/code><\/pre>\n<p>Env setup and session start:<\/p>\n<pre><code>common_prefix = &quot;pytorch-lstm&quot;\ntraining_input_prefix = common_prefix + &quot;\/training-input-data&quot;\nbatch_inference_input_prefix = common_prefix + &quot;\/batch-inference-input-data&quot;\n\nimport os\nfrom sagemaker import get_execution_role\nimport sagemaker as sage\n\nsess = sage.Session()\n\nrole = get_execution_role()\nprint(role)\n<\/code><\/pre>\n<p>Training Directory, Image, and Estimator Setup, then a <code>fit<\/code> call:<\/p>\n<pre><code>TRAINING_WORKDIR = &quot;a\/local\/directory&quot;\n\ntraining_input = sess.upload_data(TRAINING_WORKDIR, key_prefix=training_input_prefix)\nprint (&quot;Training Data Location &quot; + training_input)\n\naccount = sess.boto_session.client('sts').get_caller_identity()['Account']\nregion = sess.boto_session.region_name\nimage = '{}.dkr.ecr.{}.amazonaws.com\/image-that-works:working'.format(account, region)\n\ntree = sage.estimator.Estimator(image,\n                       role, 1, 'ml.p2.xlarge',\n                       output_path=&quot;s3:\/\/sagemaker-directory-that-definitely\/exists&quot;,\n                       sagemaker_session=sess)\n\ntree.fit(training_input)\n<\/code><\/pre>\n<p>The above script is working, for sure. I have print statements in my script and they are printing the expected results to the console. This runs as it's supposed to, finishes up, and says that it's deploying model artifacts when IT DEFINITELY DOES NOT.<\/p>\n<p>Model Deployment:<\/p>\n<pre><code>model = tree.create_model()\npredictor = tree.deploy(1, 'ml.m4.xlarge')\n<\/code><\/pre>\n<p>This throws an error that the model can't be found. A call to <code>aws sagemaker describe-training-job<\/code> shows that the training was completed but I found that the time it took to upload the model was super fast, so obviously there's an error somewhere and it's not telling me. Thankfully it's not just uploading it to the aether.<\/p>\n<pre><code>{\n            &quot;Status&quot;: &quot;Uploading&quot;,\n            &quot;StartTime&quot;: 1595982984.068,\n            &quot;EndTime&quot;: 1595982989.994,\n            &quot;StatusMessage&quot;: &quot;Uploading generated training model&quot;\n        },\n<\/code><\/pre>\n<p>Here's what I've tried so far:<\/p>\n<ol>\n<li>I've tried uploading it to a different bucket. I figured my permissions were the problem so I pointed it to one that I new allowed me to upload as I had done it before to that bucket. No dice.<\/li>\n<li>I tried backporting the script to Python 2.x, but that caused more problems than it probably would have solved, and I don't really see how that would be the problem anyways.<\/li>\n<li>I made sure the Notebook's IAM role has sufficient permissions, and it does have a SagemakerFullAccess policy<\/li>\n<\/ol>\n<p>What bothers me is that there's no error log I can see. If I could be directed to that I would be happy too, but if there's some hidden Sagemaker kungfu that I don't know about I would be forever grateful.<\/p>\n<hr \/>\n<p>EDIT<\/p>\n<p>The training job runs and prints to both the Jupyter cell and CloudWatch as expected. I've since lost the cell output in the notebook but below is the last few lines in CloudWatch. The first number is the epoch and the rest are various custom model metrics.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/y3I9L.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/y3I9L.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1595988324557,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1596039837756,
        "Question_score":2.0,
        "Question_view_count":2438.0,
        "Answer_body":"<p>Can you verify from the training job logs that your training script is running? It doesn't look like your Docker image would respond to the command <code>train<\/code>, which is what SageMaker requires, and so I suspect that your model isn't actually getting trained\/saved to <code>\/opt\/ml\/model<\/code>.<\/p>\n<p>AWS documentation about how SageMaker runs the Docker container: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-training-algo-dockerfile.html<\/a><\/p>\n<p>edit: summarizing from the comments below - the training script must also save the model to <code>\/opt\/ml\/model<\/code> (the model isn't saved automatically).<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1619730108172,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63145277",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1596039270563,
        "Question_original_content":"train job upload save train model output path deal issu week readi pull hair got custom train script pair data process script byo algorithm docker deploy type scenario pytorch model built python byo docker file origin built python issu problem have success train run save model target bucket search far wide applic answer insid notebook instanc note contractor permiss rest aw includ download docker imag dockerfil ubuntu maintain amazon run apt updat apt instal instal recommend wget python pip python pip nginx certif var lib apt list run wget http bootstrap pypa pip python pip pip instal futur numpi torch scipi scikit learn panda flask gevent gunicorn root cach env pythonunbuff true env pythondontwritebytecod true env path opt program path copi decis tree opt program workdir opt program docker imag build algorithm algo contain chmod decis tree train chmod decis tree serv account aw st caller ident queri account output text region aw configur region region region east fullnam account dkr ecr region amazonaw com algorithm latest aw ecr repositori repositori name algorithm dev null aw ecr creat repositori repositori algorithm dev null login command ecr execut directli aw ecr login region region includ email build docker imag local imag push ecr docker build algorithm docker tag algorithm fullnam docker push fullnam env setup session start common prefix pytorch lstm train input prefix common prefix train input data batch infer input prefix common prefix batch infer input data import import execut role import sage sess sage session role execut role print role train directori imag estim setup fit train workdir local directori train input sess upload data train workdir kei prefix train input prefix print train data locat train input account sess boto session client st caller ident account region sess boto session region imag dkr ecr amazonaw com imag work work format account region tree sage estim estim imag role xlarg output path directori definit exist session sess tree fit train input script work sure print statement script print expect result consol run suppos finish sai deploi model artifact definit model deploy model tree creat model predictor tree deploi xlarg throw error model train job show train complet time took upload model super fast obvious error tell thankfulli upload aether statu upload starttim endtim statusmessag upload gener train model tri far tri upload differ bucket figur permiss problem point new allow upload bucket dice tri backport script python caus problem probabl solv problem anywai sure notebook iam role suffici permiss fullaccess polici bother error log direct happi hidden kungfu know forev grate edit train job run print jupyt cell cloudwatch expect lost cell output notebook line cloudwatch number epoch rest custom model metric",
        "Question_preprocessed_content":"train job train model output path deal issu week readi pull hair got custom train script pair data process script byo algorithm docker deploy type scenario pytorch model built python byo docker file origin built python issu problem success train run save model target bucket search far wide applic answer insid notebook instanc note contractor permiss rest aw includ download docker imag dockerfil docker imag build env setup session start train directori imag estim setup script work sure print statement script print expect result consol run suppos finish sai deploi model artifact definit model deploy throw error model show train complet time took upload model super fast obvious error tell thankfulli upload aether tri far tri upload differ bucket figur permiss problem point new allow upload bucket dice tri backport script python caus problem probabl solv problem anywai sure notebook iam role suffici permiss fullaccess polici bother error log direct happi hidden kungfu know forev grate edit train job run print jupyt cell cloudwatch expect lost cell output notebook line cloudwatch number epoch rest custom model metric",
        "Question_gpt_summary_original":"The user is facing challenges with Sagemaker not saving the model to the target S3 bucket after a successful training run. The user has tried uploading it to a different bucket, backporting the script to Python 2.x, and ensuring that the Notebook's IAM role has sufficient permissions. The user is also unable to find any error logs and is seeking help to resolve the issue.",
        "Question_gpt_summary":"user face challeng save model target bucket success train run user tri upload differ bucket backport script python ensur notebook iam role suffici permiss user unabl error log seek help resolv issu",
        "Answer_original_content":"verifi train job log train script run look like docker imag respond command train requir suspect model isn actual get train save opt model aw document run docker contain http doc aw amazon com latest algorithm train algo dockerfil html edit summar comment train script save model opt model model isn save automat",
        "Answer_preprocessed_content":"verifi train job log train script run look like docker imag respond command requir suspect model isn actual get aw document run docker contain edit summar comment train script save model",
        "Answer_gpt_summary_original":"Solution: The user is advised to verify from the training job logs that the training script is running. It is suspected that the Docker image is not responding to the command \"train\", which is required by SageMaker, and hence the model is not getting trained\/saved to \"\/opt\/ml\/model\". The training script must also save the model to \"\/opt\/ml\/model\" as the model is not saved automatically. AWS documentation about how SageMaker runs the Docker container is also provided as a reference. No personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"solut user advis verifi train job log train script run suspect docker imag respond command train requir model get train save opt model train script save model opt model model save automat aw document run docker contain provid refer person opinion bias includ summari"
    },
    {
        "Question_title":"Azure Machine Learning Exit Code 143",
        "Question_body":"Hi There,\nI am running a very simple pipeline that contains a dataset and a SQL transformation task. When i run the two tasks i get an error : 2021\/09\/07 17:49:47 Wrapper cmd failed with err: exit status 143 which i can't seem to find anywhere. I am running a compute VM DS1.\nany direction?\nThanks,",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1631037752020,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Incase anyone is wondering, you must increase the compute with more memory to avoid this...",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/543071\/azure-machine-learning-exit-code-143.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1631040059280,
        "Question_original_content":"exit code run simpl pipelin contain dataset sql transform task run task error wrapper cmd fail err exit statu run comput direct thank",
        "Question_preprocessed_content":"exit code run simpl pipelin contain dataset sql transform task run task error wrapper cmd fail err exit statu run comput direct thank",
        "Question_gpt_summary_original":"The user is encountering an error with exit code 143 while running a simple pipeline containing a dataset and a SQL transformation task on Azure Machine Learning. The error message is not familiar to the user, and they are seeking guidance to resolve the issue. The user is running a compute VM DS1.",
        "Question_gpt_summary":"user encount error exit code run simpl pipelin contain dataset sql transform task error messag familiar user seek guidanc resolv issu user run comput",
        "Answer_original_content":"incas wonder increas comput memori avoid",
        "Answer_preprocessed_content":"incas wonder increas comput memori avoid",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to increase the compute with more memory to avoid the error with exit code 143 while running a simple pipeline containing a dataset and a SQL transformation task on Azure Machine Learning.",
        "Answer_gpt_summary":"solut solut mention discuss increas comput memori avoid error exit code run simpl pipelin contain dataset sql transform task"
    },
    {
        "Question_title":"What does \"local\" mean in compute target?",
        "Question_body":"Hi guys, I'm new to Azure ML. Following the URL below, I tried to run my python script on local machine. By local, I meant exactly Windows on my local physical machine in my house. But it seems python script 'transform_titanic.py' was executed on Azure.\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-set-up-training-targets#local-compute-target\n\nI executed the script below on my local computer, and expected it runs 'transform_titanic.py' on my local computer.\n\nfrom azureml.core import Environment, Experiment, ScriptRunConfig, Workspace\nfrom dotenv import load_dotenv\nload_dotenv()\nws = Workspace(\n    os.environ['SUBSCRIPTION_ID']\n    os.environ['RESOURCE_GROUP']\n    os.environ['WORKSPACE_NAME']\n)\nexp = Experiment(workspace=ws, name='experiment')\nenv = Environment('user-managed-env')\nenv.python.user_managed_dependencies = True\nscript_run_config = ScriptRunConfig(\n    source_directory='src\/transform',\n    script='transform_titanic.py',\n    arguments=['--input_dataset_name1', 'titanic'],\n)\nscript_run_config.run_config.target = 'local'\nscript_run_config.run_config.environment = env\nrun = exp.submit(config=script_run_config)\nprint(run.get_portal_url())\nrun.wait_for_completion()",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1600495202147,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Sorry, I found it was run on my local computer. Some artifact created in the script was in C:\\Users{username}\\AppData\\Local\\Temp\\azureml_runs\\local_experiment_XXXXXXXXXX",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/99901\/what-does-34local34-mean-in-compute-target.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1600495830720,
        "Question_original_content":"local mean comput target gui new follow url tri run python script local machin local meant exactli window local physic machin hous python script transform titan execut azur http doc microsoft com azur machin learn set train target local comput target execut script local expect run transform titan local core import environ experi scriptrunconfig workspac dotenv import load dotenv load dotenv workspac environ subscript environ resourc group environ workspac exp experi workspac experi env environ user manag env env python user manag depend true script run config scriptrunconfig sourc directori src transform script transform titan argument input dataset titan script run config run config target local script run config run config environ env run exp submit config script run config print run portal url run wait complet",
        "Question_preprocessed_content":"local mean comput target gui new follow url tri run python script local machin local meant exactli window local physic machin hous python script execut azur execut script local expect run local core import environ experi scriptrunconfig workspac dotenv import workspac exp experi env true scriptrunconfig local env run",
        "Question_gpt_summary_original":"The user encountered a challenge in running a Python script on their local machine using Azure ML. Despite specifying \"local\" as the compute target, the script was executed on Azure instead of their Windows machine.",
        "Question_gpt_summary":"user encount challeng run python script local machin despit specifi local comput target script execut azur instead window machin",
        "Answer_original_content":"sorri run local artifact creat script user usernam appdata local temp run local experi",
        "Answer_preprocessed_content":"sorri run local artifact creat script",
        "Answer_gpt_summary_original":"Solution: The user found that the script was actually executed on their local machine, and that some artifact was created in a specific directory. No further solutions were mentioned.",
        "Answer_gpt_summary":"solut user script actual execut local machin artifact creat specif directori solut mention"
    },
    {
        "Question_title":"What does this python function signature means in Kedro Tutorial?",
        "Question_body":"<p>I am looking at Kedro Library as my team are looking into using it for our data pipeline.<\/p>\n<p>While going to the offical tutorial - Spaceflight.<\/p>\n<p>I came across this function:<\/p>\n<pre><code>def preprocess_companies(companies: pd.DataFrame) -&gt; pd.DataFrame:\n&quot;&quot;&quot;Preprocess the data for companies.\n\n    Args:\n        companies: Source data.\n    Returns:\n        Preprocessed data.\n\n&quot;&quot;&quot;\n\ncompanies[&quot;iata_approved&quot;] = companies[&quot;iata_approved&quot;].apply(_is_true)\n\ncompanies[&quot;company_rating&quot;] = companies[&quot;company_rating&quot;].apply(_parse_percentage)\n\nreturn companies\n<\/code><\/pre>\n<ul>\n<li>companies is the name of the csv file containing the data<\/li>\n<\/ul>\n<p>Looking at the function, my assumption is that <code>(companies: pd.Dafarame)<\/code> is the shorthand to read the &quot;companies&quot; dataset as a dataframe. If so, I do not understand what does <code>-&gt; pd.Dataframe<\/code> at the end means<\/p>\n<p>I tried looking at python documentation regarding such style of code but I did not managed to find any<\/p>\n<p>Much help is appreciated to assist me in understanding this.<\/p>\n<p>Thank you<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1613060367020,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":135.0,
        "Answer_body":"<p>The <code>-&gt;<\/code> notation is <a href=\"https:\/\/docs.python.org\/3\/library\/typing.html\" rel=\"nofollow noreferrer\">type hinting<\/a>, as is the <code>:<\/code> part in the <code>companies: pd.DataFrame<\/code> function definition. This is not essential to do in Python but many people like to include it. The function definition would work exactly the same if it didn't contain this but instead read:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def preprocess_companies(companies):\n<\/code><\/pre>\n<p>This is a general Python thing rather than anything kedro-specific.<\/p>\n<p>The way that kedro registers <code>companies<\/code> as a kedro dataset is completely separate from this function definition and is done through the catalog.yml file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>companies:\n  type: pandas.CSVDataSet\n  filepath: data\/01_raw\/companies.csv\n<\/code><\/pre>\n<p>There will then a <em>node<\/em> defined (in pipeline.py) to specify that the <code>preprocess_companies<\/code> function should take as input the kedro dataset <code>companies<\/code>:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>node(\n    func=preprocess_companies,\n    inputs=&quot;companies&quot;,  # THIS LINE REFERS TO THE DATASET NAME\n    outputs=&quot;preprocessed_companies&quot;,\n    name=&quot;preprocessing_companies&quot;,\n),\n<\/code><\/pre>\n<p>In theory the name of the parameter in the function itself could be completely different, e.g.<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>def preprocess_companies(anything_you_want):\n<\/code><\/pre>\n<p>... although it is very common to give it the same name as the dataset.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1613067465070,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66158536",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1613062868172,
        "Question_original_content":"python function signatur mean tutori look librari team look data pipelin go offic tutori spaceflight came function def preprocess compani compani datafram datafram preprocess data compani arg compani sourc data return preprocess data compani iata approv compani iata approv appli true compani compani rate compani compani rate appli pars percentag return compani compani csv file contain data look function assumpt compani dafaram shorthand read compani dataset datafram understand datafram end mean tri look python document style code manag help appreci assist understand thank",
        "Question_preprocessed_content":"python function signatur mean tutori look librari team look data pipelin go offic tutori spaceflight came function compani csv file contain data look function assumpt shorthand read compani dataset datafram understand end mean tri look python document style code manag help appreci assist understand thank",
        "Question_gpt_summary_original":"The user is encountering challenges in understanding the meaning of the function signature in Kedro Tutorial. Specifically, they are unsure about the purpose of the \"-> pd.DataFrame\" at the end of the function signature. They have attempted to find information on this style of code in Python documentation but have been unsuccessful.",
        "Question_gpt_summary":"user encount challeng understand mean function signatur tutori specif unsur purpos datafram end function signatur attempt inform style code python document unsuccess",
        "Answer_original_content":"notat type hint compani datafram function definit essenti python peopl like includ function definit work exactli contain instead read def preprocess compani compani gener python thing specif wai regist compani dataset complet separ function definit catalog yml file compani type panda csvdataset filepath data raw compani csv node defin pipelin specifi preprocess compani function input dataset compani node func preprocess compani input compani line refer dataset output preprocess compani preprocess compani theori paramet function complet differ def preprocess compani want common dataset",
        "Answer_preprocessed_content":"notat type hint function definit essenti python peopl like includ function definit work exactli contain instead read gener python thing specif wai regist dataset complet separ function definit file node defin specifi function input dataset theori paramet function complet differ common dataset",
        "Answer_gpt_summary_original":"Solution:\n- The \"-> pd.DataFrame\" at the end of the function signature is a type hinting notation, which is not essential but many people like to include it.\n- The function definition would work exactly the same if it didn't contain this notation.\n- The way that Kedro registers datasets is completely separate from this function definition and is done through the catalog.yml file.\n- A node is defined in pipeline.py to specify that the function should take as input the Kedro dataset. \n- In theory, the name of the parameter in the function itself could be completely different, although it is very common to give it the same name as the dataset.",
        "Answer_gpt_summary":"solut datafram end function signatur type hint notat essenti peopl like includ function definit work exactli contain notat wai regist dataset complet separ function definit catalog yml file node defin pipelin specifi function input dataset theori paramet function complet differ common dataset"
    },
    {
        "Question_title":"The amlignore file doesn't reduce the size of snapshot",
        "Question_body":"<p>To overcome <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-save-write-experiment-files#storage-limits-of-experiment-snapshots\" rel=\"nofollow noreferrer\">300MB snapshot size limit<\/a> I created an .amlignore file in the root of my repository:<\/p>\n\n<pre><code>\/*\n!\/root\n<\/code><\/pre>\n\n<p>The intention is to exclude everything except <code>\/root<\/code> directory where all python code is. The size of the <code>root<\/code> directory is less than 1MB, still I get an error of exceeding snapshot limit size of 300MB. What am I doing wrong?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1570814619870,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":522.0,
        "Answer_body":"<p>This is fixed in version <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/azure-machine-learning-release-notes#azure-machine-learning-sdk-for-python-v1074\" rel=\"nofollow noreferrer\">1.0.74 of azureml-sdk<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58345935",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1573932699567,
        "Question_original_content":"amlignor file reduc size snapshot overcom snapshot size limit creat amlignor file root repositori root intent exclud root directori python code size root directori error exceed snapshot limit size wrong",
        "Question_preprocessed_content":"amlignor file reduc size snapshot overcom snapshot size limit creat amlignor file root repositori intent exclud directori python code size directori error exceed snapshot limit size wrong",
        "Question_gpt_summary_original":"The user created an .amlignore file to exclude everything except the \/root directory where all the Python code is located in order to overcome the 300MB snapshot size limit. However, even though the size of the \/root directory is less than 1MB, the user is still getting an error of exceeding the snapshot limit size of 300MB.",
        "Question_gpt_summary":"user creat amlignor file exclud root directori python code locat order overcom snapshot size limit size root directori user get error exceed snapshot limit size",
        "Answer_original_content":"fix version sdk",
        "Answer_preprocessed_content":"fix version sdk",
        "Answer_gpt_summary_original":"Solution: The issue of exceeding the snapshot limit size of 300MB has been fixed in version 1.0.74 of azureml-sdk.",
        "Answer_gpt_summary":"solut issu exceed snapshot limit size fix version sdk"
    },
    {
        "Question_title":"How to pass image to AWS SageMaker endpoint",
        "Question_body":"<p>I need to use the <a href=\"https:\/\/aws.amazon.com\/marketplace\/pp\/prodview-7y6xdiukxucr2\" rel=\"nofollow noreferrer\">WireframeToCode<\/a> model from the AWS Marketplace, I used Nodejs to read and send the file data to the model like this:<\/p>\n\n<pre><code>var sageMakerRuntime = new AWS.SageMakerRuntime();\n\nvar bitmap = fs.readFileSync(\"sample.jpeg\", \"utf8\");\nvar buffer = new Buffer.from(bitmap, \"base64\");\n\nvar params = {\n  Body: buffer.toJSON(),\n  EndpointName: \"wireframe-to-code\",\n  Accept: \"image\/jpeg\",\n  ContentType: \"application\/json\"\n};\n\nsageMakerRuntime.invokeEndpoint(params, function(err, data) {\n  if (err) console.log(err, err.stack);\n  else console.log(data);\n});\n<\/code><\/pre>\n\n<p>but i get this error:<\/p>\n\n<blockquote>\n  <p>message: 'Expected params.Body to be a string, Buffer, Stream, Blob,\n  or typed array object',   code: 'InvalidParameterType',   time:\n  2020-03-30T11:06:27.535Z<\/p>\n<\/blockquote>\n\n<p>From the documentation, the supported content type for input is  <code>image\/jpeg<\/code> output is <code>application\/json<\/code>.<\/p>\n\n<p>when I try to convert the Body to a string like this: <code>JSON.stringify(buffer.toJSON())<\/code> I get this error:<\/p>\n\n<blockquote>\n  <p>Received client error (415) from model with message \"This predictor\n  only supports JSON formatted data\"<\/p>\n<\/blockquote>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1585568125227,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":2170.0,
        "Answer_body":"<p>I had to pass in bitmap and change <code>ContentType<\/code> to <code>\"image\/jpeg\"<\/code><\/p>\n\n<pre><code>const AWS = require(\"aws-sdk\");\nconst fs = require(\"fs\");\n\nconst sageMakerRuntime = new AWS.SageMakerRuntime({\n  region: \"us-east-1\",\n  accessKeyId: \"XXXXXXXXXXXX\",\n  secretAccessKey: \"XXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n});\n\nconst bitmap = fs.readFileSync(\"sample.jpeg\");\n\nvar params = {\n  Body: bitmap,\n  EndpointName: \"wireframe-to-code\",\n  ContentType: \"image\/jpeg\"\n};\n\nsageMakerRuntime.invokeEndpoint(params, function(err, data) {\n  if (err) {\n    console.log(err, err.stack);\n  } else {\n    responseData = JSON.parse(Buffer.from(data.Body).toString());\n    console.log(responseData);\n  }\n});\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/60929678",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1586158824067,
        "Question_original_content":"pass imag endpoint need us wireframetocod model aw marketplac nodej read send file data model like var runtim new aw runtim var bitmap readfilesync sampl jpeg utf var buffer new buffer bitmap base var param bodi buffer tojson endpointnam wirefram code accept imag jpeg contenttyp applic json runtim invokeendpoint param function err data err consol log err err stack consol log data error messag expect param bodi string buffer stream blob type arrai object code invalidparametertyp time document support content type input imag jpeg output applic json try convert bodi string like json stringifi buffer tojson error receiv client error model messag predictor support json format data",
        "Question_preprocessed_content":"pass imag endpoint need us wireframetocod model aw marketplac nodej read send file data model like error messag expect string buffer stream blob type arrai object code invalidparametertyp time document support content type input output try convert bodi string like error receiv client error model messag predictor support json format data",
        "Question_gpt_summary_original":"The user is encountering challenges in passing an image to an AWS SageMaker endpoint using Nodejs. The user is receiving an error message stating that the expected parameter type is invalid and is unable to convert the Body to a string due to the predictor only supporting JSON formatted data. The supported content type for input is image\/jpeg and output is application\/json.",
        "Question_gpt_summary":"user encount challeng pass imag endpoint nodej user receiv error messag state expect paramet type invalid unabl convert bodi string predictor support json format data support content type input imag jpeg output applic json",
        "Answer_original_content":"pass bitmap chang contenttyp imag jpeg const aw requir aw sdk const requir const runtim new aw runtim region east accesskeyid secretaccesskei const bitmap readfilesync sampl jpeg var param bodi bitmap endpointnam wirefram code contenttyp imag jpeg runtim invokeendpoint param function err data err consol log err err stack responsedata json pars buffer data bodi tostr consol log responsedata",
        "Answer_preprocessed_content":"pass bitmap chang",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to pass in the image as a bitmap and change the ContentType to \"image\/jpeg\". The code snippet provided in the discussion shows how to implement this solution using Nodejs and AWS SageMakerRuntime.",
        "Answer_gpt_summary":"solut solut provid discuss pass imag bitmap chang contenttyp imag jpeg code snippet provid discuss show implement solut nodej runtim"
    },
    {
        "Question_title":"AutoML : TensorFlowDNN and TensorFlowLinearRegressor are blacklisted by default",
        "Question_body":"Hello,\n\nI am running an AutoML experiment for a regression task, and looking at the YAML file which is generated it seems that TensorFlowLinearRegressor and TensorFlowDNN models are listed as both 'supported_models' and 'blacklist_algos'.\n\nI tried to deactivate the automatic blacklisting of models by specifying the parameter 'auto_blacklist' to False, and 'blacklist_models' and 'blacklist_algos' parameters to Null, but it doesn't change anything.\n\n automl_settings = {\n     \"primary_metric\": 'normalized_mean_absolute_error',\n     \"featurization\": 'auto',\n     \"verbosity\": logging.INFO,\n     \"n_cross_validations\": 5,\n     \"auto_blacklist\": False,\n     \"blacklist_models\": None,\n     \"blacklist_algos\": None\n }\n run = experiment.submit(automl_config, show_output=True)\n\n\n\nThe generated YAML file (excerpt):\n\n \"whitelist_models\":null,\n \"blacklist_algos\":[\"TensorFlowDNN\",\"TensorFlowLinearRegressor\"],\n \"supported_models\":[\"ElasticNet\",\"GradientBoosting\",\"LightGBM\",\"TensorFlowLinearRegressor\",\"TensorFlowDNN\",\"LassoLars\",\"DecisionTree\",\"RandomForest\",\"FastLinearRegressor\",\"OnlineGradientDescentRegressor\",\"ExtremeRandomTrees\",\"TabnetRegressor\",\"XGBoostRegressor\",\"KNN\",\"SGD\"],\n \"private_models\":[],\n \"auto_blacklist\":false\n\n\n\nMaybe the problem comes from the fact that Deep learning is set to 'Disabled' in the configuration settings, as shown on the following picture:\n\nAre deep learning models not supported anymore by AutoML?",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1653464529937,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"@ThierryL-3166 Thanks for the question.\n\nAs mentioned in the below document The following support models in AutoML TensorFlowDNN, TensorFlowLinearRegressor are deprecated.\nhttps:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-automl-core\/azureml.automl.core.shared.constants.supportedmodels.regression?view=azure-ml-py",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/863297\/automl-tensorflowdnn-and-tensorflowlinearregressor.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1654236226443,
        "Question_original_content":"automl tensorflowdnn tensorflowlinearregressor blacklist default hello run automl experi regress task look yaml file gener tensorflowlinearregressor tensorflowdnn model list support model blacklist algo tri deactiv automat blacklist model specifi paramet auto blacklist fals blacklist model blacklist algo paramet null chang automl set primari metric normal mean absolut error featur auto verbos log info cross valid auto blacklist fals blacklist model blacklist algo run experi submit automl config output true gener yaml file excerpt whitelist model null blacklist algo tensorflowdnn tensorflowlinearregressor support model elasticnet gradientboost lightgbm tensorflowlinearregressor tensorflowdnn lassolar decisiontre randomforest fastlinearregressor onlinegradientdescentregressor extremerandomtre tabnetregressor xgboostregressor knn sgd privat model auto blacklist fals mayb problem come fact deep learn set disabl configur set shown follow pictur deep learn model support anymor automl",
        "Question_preprocessed_content":"automl tensorflowdnn tensorflowlinearregressor blacklist default hello run automl experi regress task look yaml file gener tensorflowlinearregressor tensorflowdnn model list tri deactiv automat blacklist model specifi paramet fals paramet null chang run gener yaml file mayb problem come fact deep learn set disabl configur set shown follow pictur deep learn model support anymor automl",
        "Question_gpt_summary_original":"The user is encountering challenges while running an AutoML experiment for a regression task as TensorFlowLinearRegressor and TensorFlowDNN models are listed as both 'supported_models' and 'blacklist_algos'. The user tried to deactivate the automatic blacklisting of models by specifying the parameter 'auto_blacklist' to False, and 'blacklist_models' and 'blacklist_algos' parameters to Null, but it doesn't change anything. The generated YAML file shows that deep learning is set to 'Disabled' in the configuration settings, which may be the reason for the issue. The user is unsure if deep learning models are still supported by AutoML.",
        "Question_gpt_summary":"user encount challeng run automl experi regress task tensorflowlinearregressor tensorflowdnn model list support model blacklist algo user tri deactiv automat blacklist model specifi paramet auto blacklist fals blacklist model blacklist algo paramet null chang gener yaml file show deep learn set disabl configur set reason issu user unsur deep learn model support automl",
        "Answer_original_content":"thierryl thank question mention document follow support model automl tensorflowdnn tensorflowlinearregressor deprec http doc microsoft com python api automl core automl core share constant supportedmodel regress view azur",
        "Answer_preprocessed_content":"thank question mention document follow support model automl tensorflowdnn tensorflowlinearregressor deprec",
        "Answer_gpt_summary_original":"Solution: The discussion mentions that the TensorFlowDNN and TensorFlowLinearRegressor models are deprecated in AutoML, which means they are no longer supported. Therefore, the user should consider using other supported models for their regression task. No other solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut discuss mention tensorflowdnn tensorflowlinearregressor model deprec automl mean longer support user consid support model regress task solut provid discuss"
    },
    {
        "Question_title":"upload image dataset to S3 sagemaker",
        "Question_body":"<p>my dataset is 3 folders (train, validation and test) of images. each folder has two subfolders (cat1 and cat2). I am using AWS sage maker to preprocess my data and train my model. we all know that we have to upload the training data to S3 bucket before starting the &quot;.fit&quot; process.\nI want to know how to upload my data set to S3<\/p>\n<pre><code># general prefix\nprefix='chest-xray'\n#unique train\/test prefixes\ntrain_prefix   = '{}\/{}'.format(prefix, 'train')\nval_prefix   = '{}\/{}'.format(prefix, 'validation')\ntest_prefix    = '{}\/{}'.format(prefix, 'test')\n\n# uploading data to S3, and saving locations\ntrain_path  = sagemaker_session.upload_data(train_data, bucket=bucket, key_prefix=train_prefix)\n<\/code><\/pre>\n<p>what the train_data parameters should look like<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1611594075077,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":444.0,
        "Answer_body":"<p>According to the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/utility\/session.html#sagemaker.session.Session.upload_data\" rel=\"nofollow noreferrer\">documentation<\/a> <code>train_data<\/code> is the local path of the file to upload to S3, so you need this file locally where you are launching the training job. If you are using a notebook this is not the way to do. You have instead to manually upload your dataset in a S3 bucket. I suggest to preprocess your dataset in a single file (tfrecord for example if you are using TF) and upload that file to S3. You can do it using the AWS web console or using the AWS-CLI with the <code>aws s3 cp yourfile s3:\/\/your-bucket <\/code>command.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65889143",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1611678605710,
        "Question_original_content":"upload imag dataset dataset folder train valid test imag folder subfold cat cat aw sage maker preprocess data train model know upload train data bucket start fit process want know upload data set gener prefix prefix chest xrai uniqu train test prefix train prefix format prefix train val prefix format prefix valid test prefix format prefix test upload data save locat train path session upload data train data bucket bucket kei prefix train prefix train data paramet look like",
        "Question_preprocessed_content":"upload imag dataset dataset folder imag folder subfold aw sage maker preprocess data train model know upload train data bucket start process want know upload data set paramet look like",
        "Question_gpt_summary_original":"The user is facing a challenge of uploading their image dataset, which consists of three folders (train, validation, and test) with two subfolders each (cat1 and cat2), to S3 bucket using AWS SageMaker. They are unsure about the format of the train_data parameters required for the upload process.",
        "Question_gpt_summary":"user face challeng upload imag dataset consist folder train valid test subfold cat cat bucket unsur format train data paramet requir upload process",
        "Answer_original_content":"accord document train data local path file upload need file local launch train job notebook wai instead manual upload dataset bucket suggest preprocess dataset singl file tfrecord exampl upload file aw web consol aw cli aw yourfil bucket command",
        "Answer_preprocessed_content":"accord document local path file upload need file local launch train job notebook wai instead manual upload dataset bucket suggest preprocess dataset singl file upload file aw web consol command",
        "Answer_gpt_summary_original":"Solution: The user can preprocess their dataset into a single file (such as tfrecord) and upload it to an S3 bucket using the AWS web console or AWS-CLI with the command \"aws s3 cp yourfile s3:\/\/your-bucket\". The \"train_data\" parameter requires the local path of the file to upload to S3, so the user needs to have the file locally where they are launching the training job.",
        "Answer_gpt_summary":"solut user preprocess dataset singl file tfrecord upload bucket aw web consol aw cli command aw yourfil bucket train data paramet requir local path file upload user need file local launch train job"
    },
    {
        "Question_title":"How can I match my local azure automl python sdk version to the remote version?",
        "Question_body":"<p>I'm using the azure automl python sdk to download and save a model then reload it. I get the following error:<\/p>\n<pre><code>anaconda3\\envs\\automl_21\\lib\\site-packages\\sklearn\\base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. This might lead to breaking code or invalid results. Use at your own risk.\n  UserWarning)\n<\/code><\/pre>\n<p>How can I ensure that the versions match?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1617943623520,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":62.0,
        "Answer_body":"<p>My Microsoft contact says -<\/p>\n<p>&quot;For this, their best  bet is probably to see what the training env was pinned to and install those same pins. They can get that env by running child_run.get_environment() and then pip install all the pkgs listed in there with the pins listed there.&quot;<\/p>\n<p>A useful code snippet.<\/p>\n<pre><code>for run in experiment.get_runs():\n    tags_dictionary = run.get_tags()\n    best_run = AutoMLRun(experiment, tags_dictionary['automl_best_child_run_id'])\n    env = best_run.get_environment()\n    print(env.python.conda_dependencies.serialize_to_string())\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1618178182616,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67015185",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1618176409272,
        "Question_original_content":"match local azur automl python sdk version remot version azur automl python sdk download save model reload follow error anaconda env automl lib site packag sklearn base userwarn try unpickl estim pipelin version version post lead break code invalid result us risk userwarn ensur version match",
        "Question_preprocessed_content":"match local azur automl python sdk version remot version azur automl python sdk download save model reload follow error ensur version match",
        "Question_gpt_summary_original":"The user is encountering an error while using the Azure AutoML Python SDK to download and save a model, which warns about trying to unpickle an estimator Pipeline from version 0.22.1 when using version 0.22.2.post1. The user is seeking guidance on how to match the local and remote versions to avoid breaking code or invalid results.",
        "Question_gpt_summary":"user encount error azur automl python sdk download save model warn try unpickl estim pipelin version version post user seek guidanc match local remot version avoid break code invalid result",
        "Answer_original_content":"microsoft contact sai best bet probabl train env pin instal pin env run child run environ pip instal pkg list pin list us code snippet run experi run tag dictionari run tag best run automlrun experi tag dictionari automl best child run env best run environ print env python conda depend serial string",
        "Answer_preprocessed_content":"microsoft contact sai best bet probabl train env pin instal pin env run pip instal pkg list pin list us code snippet",
        "Answer_gpt_summary_original":"Solution: The Microsoft contact suggests that the user should check the training environment and install the same pinned packages. The user can get the environment by running child_run.get_environment() and then pip install all the packages listed in there with the pins listed there. A code snippet is also provided to get the environment.",
        "Answer_gpt_summary":"solut microsoft contact suggest user check train environ instal pin packag user environ run child run environ pip instal packag list pin list code snippet provid environ"
    },
    {
        "Question_title":"Change in Machine Learning Designer",
        "Question_body":"I can\u2019t find some of the basic modules from this week. Any significant change about Designer?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660839023247,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @67603284\n\nThanks for using Microsoft Q&A platform, there is no surprising change in Azure Machine Learning Designer.\n\nBased on my experience, you may use the filter so you can not see some of the modules as below screenshot.\n\n\nIf this is not your case, could you please share which module you have lost? Thanks.\n\nI hope this helps.\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful to support the community, thanks a lot.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/972775\/change-in-machine-learning-designer.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1660841903856,
        "Question_original_content":"chang machin learn design basic modul week signific chang design",
        "Question_preprocessed_content":"chang machin learn design basic modul week signific chang design",
        "Question_gpt_summary_original":"The user is facing challenges in finding some basic modules in the Machine Learning Designer and is questioning if there have been any significant changes made to the Designer.",
        "Question_gpt_summary":"user face challeng find basic modul machin learn design question signific chang design",
        "Answer_original_content":"hello thank microsoft platform surpris chang design base experi us filter modul screenshot case share modul lost thank hope help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_preprocessed_content":"hello thank microsoft platform surpris chang design base experi us filter modul screenshot case share modul lost thank hope help regard yutong kindli accept answer feel help support commun thank lot",
        "Answer_gpt_summary_original":"Solution: The respondent suggests using the filter to locate the missing modules in the Azure Machine Learning Designer. However, no specific solution is provided as the user did not mention which module they were unable to find.",
        "Answer_gpt_summary":"solut respond suggest filter locat miss modul design specif solut provid user mention modul unabl"
    },
    {
        "Question_title":"Passing data from Azure Data Factory into Azure ML",
        "Question_body":"In Azure ML the input data has to be defined as a Dataset (to create a pipeline). In my code I am passing datasets with the following syntax: input_data = Dataset.File.from_files(datapath)\n\nI would like to change this datapath as an input parameter from Data Factory (for example via PipelineParamater), so I can apply the same Data Factory pipeline for different datasets. However, in Data Factory you can only pass string as a parameter, not a DataPath.\n\nWhat is the solution around this?",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1626444050490,
        "Question_favorite_count":13.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi @IlzeAmanda-9677 ,\n\nThank you for posting your query on Microsoft Q&A Portal and sharing clarifications on ask.\n\nUnfortunately, we cannot create user defined types in Azure data factory at this moment.\n\nBut, I will encourage you to log your feedback using below link. Product team will actively monitor feedback there and consider them for future releases. Thank you.\nhttps:\/\/feedback.azure.com\/forums\/270578-data-factory\n\nHope this will help.\n\nPlease accept an answer if correct. Original posters help the community find answers faster by identifying the correct answer. Here is how.\n\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/479034\/passing-data-from-azure-data-factory-into-azure-ml.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1626693295843,
        "Question_original_content":"pass data azur data factori input data defin dataset creat pipelin code pass dataset follow syntax input data dataset file file datapath like chang datapath input paramet data factori exampl pipelineparamat appli data factori pipelin differ dataset data factori pass string paramet datapath solut",
        "Question_preprocessed_content":"pass data azur data factori input data defin dataset code pass dataset follow syntax like chang datapath input paramet data factori appli data factori pipelin differ dataset data factori pass string paramet datapath solut",
        "Question_gpt_summary_original":"The user is facing a challenge in passing data from Azure Data Factory into Azure ML. They want to change the datapath as an input parameter from Data Factory, but Data Factory only allows passing strings as parameters, not datapaths. The user is seeking a solution to this problem.",
        "Question_gpt_summary":"user face challeng pass data azur data factori want chang datapath input paramet data factori data factori allow pass string paramet datapath user seek solut problem",
        "Answer_original_content":"ilzeamanda thank post queri microsoft portal share clarif ask unfortun creat user defin type azur data factori moment encourag log feedback link product team activ monitor feedback consid futur releas thank http feedback azur com forum data factori hope help accept answer correct origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif",
        "Answer_preprocessed_content":"thank post queri microsoft portal share clarif ask unfortun creat user defin type azur data factori moment encourag log feedback link product team activ monitor feedback consid futur releas thank hope help accept answer correct origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The user is encouraged to log their feedback on the Azure Data Factory feedback forum for consideration in future releases.",
        "Answer_gpt_summary":"solut solut provid discuss user encourag log feedback azur data factori feedback forum consider futur releas"
    },
    {
        "Question_title":"Is there a way to stop Azure ML throwing an error when exporting zero lines of data?",
        "Question_body":"<p>I am currently developing an Azure ML pipeline that as one of its outputs is maintaining a SQL table holding all of the unique items that are fed into it. There is no way to know in advance if the data fed into the pipeline is new unique items or repeats of previous items, so before updating the table that it maintains it pulls the data already in that table and drops any of the new items that already appear.<\/p>\n<p>However, due to this there are cases where this self-reference results in zero new items being found, and as such there is nothing to export to the SQL table. When this happens Azure ML throws an error, as it is considered an error for there to be zero lines of data to export. In my case, however, this is expected behaviour, and as such absolutely fine.<\/p>\n<p>Is there any way for me to suppress this error, so that when it has zero lines of data to export it just skips the export module and moves on?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1622071490553,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":94.0,
        "Answer_body":"<p>This issue has been resolved by an update to Azure Machine Learning; You can now run pipelines with a flag set to &quot;Continue on Failure Step&quot;, which means that steps following the failed data export will continue to run.<\/p>\n<p>This does mean you will need to design your pipeline to be able to handles upstream failures in its downstream modules; this must be done very carefully.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67713876",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1630895031176,
        "Question_original_content":"wai stop throw error export zero line data current develop pipelin output maintain sql tabl hold uniqu item fed wai know advanc data fed pipelin new uniqu item repeat previou item updat tabl maintain pull data tabl drop new item appear case self refer result zero new item export sql tabl happen throw error consid error zero line data export case expect behaviour absolut fine wai suppress error zero line data export skip export modul move",
        "Question_preprocessed_content":"wai stop throw error export zero line data current develop pipelin output maintain sql tabl hold uniqu item fed wai know advanc data fed pipelin new uniqu item repeat previou item updat tabl maintain pull data tabl drop new item appear case result zero new item export sql tabl happen throw error consid error zero line data export case expect behaviour absolut fine wai suppress error zero line data export skip export modul move",
        "Question_gpt_summary_original":"The user is facing a challenge with Azure ML throwing an error when there are zero lines of data to export to a SQL table. The pipeline drops any new items that already appear, resulting in cases where there are no new items to export. The user wants to know if there is a way to suppress the error and skip the export module when there are zero lines of data to export.",
        "Question_gpt_summary":"user face challeng throw error zero line data export sql tabl pipelin drop new item appear result case new item export user want know wai suppress error skip export modul zero line data export",
        "Answer_original_content":"issu resolv updat run pipelin flag set continu failur step mean step follow fail data export continu run mean need design pipelin abl handl upstream failur downstream modul carefulli",
        "Answer_preprocessed_content":"issu resolv updat run pipelin flag set continu failur step mean step follow fail data export continu run mean need design pipelin abl handl upstream failur downstream modul carefulli",
        "Answer_gpt_summary_original":"Solution: The issue has been resolved by an update to Azure Machine Learning. You can now run pipelines with a flag set to \"Continue on Failure Step\", which means that steps following the failed data export will continue to run. However, you will need to design your pipeline to be able to handle upstream failures in its downstream modules carefully.",
        "Answer_gpt_summary":"solut issu resolv updat run pipelin flag set continu failur step mean step follow fail data export continu run need design pipelin abl handl upstream failur downstream modul carefulli"
    },
    {
        "Question_title":"Sagemaker Notebook Instance Type Recommendation",
        "Question_body":"<p>I will be running ml models on a pretty large dataset. It is about 15 gb, with 200 columns and 4.3 million rows. I'm wondering what the best Notebook instance type is for this kind of dataset in AWS Sagemaker.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1573155038577,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":2326.0,
        "Answer_body":"<p><strong>For choosing a SageMaker hosted notebook type:<\/strong><\/p>\n\n<p>Do you plan to do all of your preprocessing of your data in-memory on the notebook, or do you plan to orchestrate ETL with external services? <\/p>\n\n<p>If you're planning to load the dataset into memory on the notebook instance for exploration\/preprocessing, the primary bottleneck here would be ensuring the instance has enough memory for your dataset. This would require at least the 16gb types (<em>.xlarge<\/em>) (full list of ML instance types <a href=\"https:\/\/aws.amazon.com\/sagemaker\/pricing\/instance-types\/\" rel=\"noreferrer\">available here<\/a>). Further, depending on how compute intensive your pre-processing is, and your desired pre-processing completion time, you can opt for a compute optimized instance (<em>c4, c5<\/em>) to speed this up.<\/p>\n\n<hr>\n\n<p><strong>For the training job, specifically:<\/strong><\/p>\n\n<p>Using the Amazon SageMaker SDK, your training data will be loaded and distributed to the training cluster, allowing your training job to be completely separate from the instance your hosted notebook is running on.<\/p>\n\n<p>Figuring out the ideal instance type for training will depend on whether your algorithm of choice\/training job is memory, CPU, or IO bound. Since your dataset will likely be loaded onto your training cluster from S3, the instance you choose for your hosted notebook will have no bearing on the speed of your training job.<\/p>\n\n<hr>\n\n<p><strong>Broadly:<\/strong>\nWhen it comes to SageMaker notebooks, the best practice is to use your notebook as a \"puppeteer\" or orchestrator, that calls out to external services (AWS Glue or Amazon EMR for preprocessing, SageMaker for training, S3 for storage, etc). It is best to treat them as ephemeral forms of compute\/storage for building and kicking off your experiment pipeline.<\/p>\n\n<p>This will allow you to more closely pair compute, storage, and hosting resources\/services with the demands for your workload, ultimately resulting in the best bang for your buck by not having you pay for latent or unused resources.<\/p>\n\n<hr>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":6.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58755708",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1573157762368,
        "Question_original_content":"notebook instanc type recommend run model pretti larg dataset column million row wonder best notebook instanc type kind dataset",
        "Question_preprocessed_content":"notebook instanc type recommend run model pretti larg dataset column million row wonder best notebook instanc type kind dataset",
        "Question_gpt_summary_original":"The user is seeking a recommendation for the best AWS Sagemaker Notebook instance type to use for running machine learning models on a large dataset of 15 GB, 200 columns, and 4.3 million rows.",
        "Question_gpt_summary":"user seek recommend best notebook instanc type us run machin learn model larg dataset column million row",
        "Answer_original_content":"choos host notebook type plan preprocess data memori notebook plan orchestr etl extern servic plan load dataset memori notebook instanc explor preprocess primari bottleneck ensur instanc memori dataset requir type xlarg list instanc type avail depend comput intens pre process desir pre process complet time opt comput optim instanc speed train job specif sdk train data load distribut train cluster allow train job complet separ instanc host notebook run figur ideal instanc type train depend algorithm choic train job memori cpu bound dataset like load train cluster instanc choos host notebook bear speed train job broadli come notebook best practic us notebook puppet orchestr call extern servic aw glue amazon emr preprocess train storag best treat ephemer form comput storag build kick experi pipelin allow close pair comput storag host resourc servic demand workload ultim result best bang buck have pai latent unus resourc",
        "Answer_preprocessed_content":"choos host notebook type plan preprocess data notebook plan orchestr etl extern servic plan load dataset memori notebook instanc primari bottleneck ensur instanc memori dataset requir type depend comput intens desir complet time opt comput optim instanc speed train job specif sdk train data load distribut train cluster allow train job complet separ instanc host notebook run figur ideal instanc type train depend algorithm job memori cpu bound dataset like load train cluster instanc choos host notebook bear speed train job broadli come notebook best practic us notebook puppet orchestr call extern servic best treat ephemer form build kick experi pipelin allow close pair comput storag host demand workload ultim result best bang buck have pai latent unus resourc",
        "Answer_gpt_summary_original":"Solution:\n- For preprocessing of data in-memory on the notebook, the instance should have at least 16gb memory, and a compute optimized instance can be used to speed up the process.\n- For the training job, the instance type will depend on whether the algorithm is memory, CPU, or IO bound, and the instance used for the hosted notebook will have no bearing on the speed of the training job.\n- It is best to use the notebook as an orchestrator that calls out to external services for preprocessing, training, storage, etc.",
        "Answer_gpt_summary":"solut preprocess data memori notebook instanc memori comput optim instanc speed process train job instanc type depend algorithm memori cpu bound instanc host notebook bear speed train job best us notebook orchestr call extern servic preprocess train storag"
    },
    {
        "Question_title":"Sagemaker Java client generate IOrecord",
        "Question_body":"<p>I am trying to build a training set for Sagemaker using the Linear Learner algorithm. This algorithm supports recordIO wrapped protobuf and csv as format for the training data. As the training data is generated using spark I am having issues to generate a csv file from a dataframe (this seem broken for now), so I am trying to use protobuf. <\/p>\n\n<p>I managed to create a binary file for the training dataset using Protostuff which is a library that allows to generate protobuf messages from POJO objects. The problem is when triggering the training job I receive that message from SageMaker:\nClientError: No training data processed. Either the training channel is empty or the mini-batch size is too high. Verify that training data contains non-empty files and the mini-batch size is less than the number of records per training host.<\/p>\n\n<p>The training file is certainly not null. I suspect the way I generate the training data to be incorrect as I am able to train models using the libsvm format. Is there a way to generate IOrecord using the Sagemaker java client ?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1525984750483,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":222.0,
        "Answer_body":"<p>Answering my own question. It was an issue in the algorithm configuration. I reduced mini batch size and it worked fine.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50281188",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1526653073368,
        "Question_original_content":"java client gener iorecord try build train set linear learner algorithm algorithm support recordio wrap protobuf csv format train data train data gener spark have issu gener csv file datafram broken try us protobuf manag creat binari file train dataset protostuff librari allow gener protobuf messag pojo object problem trigger train job receiv messag clienterror train data process train channel mini batch size high verifi train data contain non file mini batch size number record train host train file certainli null suspect wai gener train data incorrect abl train model libsvm format wai gener iorecord java client",
        "Question_preprocessed_content":"java client gener iorecord try build train set linear learner algorithm algorithm support recordio wrap protobuf csv format train data train data gener spark have issu gener csv file datafram try us protobuf manag creat binari file train dataset protostuff librari allow gener protobuf messag pojo object problem trigger train job receiv messag clienterror train data process train channel size high verifi train data contain file size number record train host train file certainli null suspect wai gener train data incorrect abl train model libsvm format wai gener iorecord java client",
        "Question_gpt_summary_original":"The user is facing challenges in building a training set for Sagemaker using the Linear Learner algorithm. They are having issues generating a CSV file from a dataframe and are trying to use protobuf instead. Although they managed to create a binary file for the training dataset using Protostuff, they are receiving an error message from SageMaker stating that no training data has been processed. The user suspects that the way they generate the training data is incorrect and is seeking a way to generate IOrecord using the Sagemaker java client.",
        "Question_gpt_summary":"user face challeng build train set linear learner algorithm have issu gener csv file datafram try us protobuf instead manag creat binari file train dataset protostuff receiv error messag state train data process user suspect wai gener train data incorrect seek wai gener iorecord java client",
        "Answer_original_content":"answer question issu algorithm configur reduc mini batch size work fine",
        "Answer_preprocessed_content":"answer question issu algorithm configur reduc mini batch size work fine",
        "Answer_gpt_summary_original":"Solution: The user found a solution to their challenge by reducing the mini batch size in the algorithm configuration. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user solut challeng reduc mini batch size algorithm configur solut mention discuss"
    },
    {
        "Question_title":"Customer Error: Additional hyperparameters are not allowed - Image classification training- Sagemaker",
        "Question_body":"<p>I'm learning image classification with Amazon SageMaker. I was trying to follow their  learning demo <strong>Image classification transfer learning demo<\/strong> (<code>Image-classification-transfer-learning-highlevel.ipynb<\/code>)<\/p>\n\n<p>I got up to Start the training. Executed below.<\/p>\n\n<pre><code>ic.fit(inputs=data_channels, logs=True)\n<\/code><\/pre>\n\n<p>Set the hyper parameters as given in the demo<\/p>\n\n<pre><code>ic.set_hyperparameters(num_layers=18,\n                             use_pretrained_model=1,\n                             image_shape = \"3,224,224\",\n                             num_classes=257,\n                             num_training_samples=15420,\n                             mini_batch_size=128,\n                             epochs=1,\n                             learning_rate=0.01,\n                             precission_dtype='float32')\n<\/code><\/pre>\n\n<p>Got the client error<\/p>\n\n<pre><code>ERROR 140291262150464] Customer Error: Additional hyperparameters are not allowed (u'precission_dtype' was unexpected) (caused by ValidationError)\n\nCaused by: Additional properties are not allowed (u'precission_dtype' was unexpected)\n<\/code><\/pre>\n\n<p>Does anyone know how to overcome this? I'm also reporting this to aws support. Posting here for sharing and get a fix. Thanks !<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1539753916957,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":146.0,
        "Answer_body":"<p>Assuming you are referring to <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-transfer-learning-highlevel.ipynb<\/a> - you have a typo, it's <code>precision_dtype<\/code>, not <code>precission_dtype<\/code>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52847777",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1539831993640,
        "Question_original_content":"custom error addit hyperparamet allow imag classif train learn imag classif try follow learn demo imag classif transfer learn demo imag classif transfer learn highlevel ipynb got start train execut fit input data channel log true set hyper paramet given demo set hyperparamet num layer us pretrain model imag shape num class num train sampl mini batch size epoch learn rate preciss dtype float got client error error custom error addit hyperparamet allow preciss dtype unexpect caus validationerror caus addit properti allow preciss dtype unexpect know overcom report aw support post share fix thank",
        "Question_preprocessed_content":"custom error addit hyperparamet allow imag classif train learn imag classif try follow learn demo imag classif transfer learn demo got start train execut set hyper paramet given demo got client error know overcom report aw support post share fix thank",
        "Question_gpt_summary_original":"The user encountered an error while trying to set hyperparameters for image classification training using Amazon SageMaker. The error message stated that additional hyperparameters are not allowed, specifically the \"precission_dtype\" parameter. The user is seeking help to overcome this issue and has reported it to AWS support.",
        "Question_gpt_summary":"user encount error try set hyperparamet imag classif train error messag state addit hyperparamet allow specif preciss dtype paramet user seek help overcom issu report aw support",
        "Answer_original_content":"assum refer http github com awslab amazon exampl blob master introduct amazon algorithm imageclassif caltech imag classif transfer learn highlevel ipynb typo precis dtype preciss dtype",
        "Answer_preprocessed_content":"assum refer typo",
        "Answer_gpt_summary_original":"Solution: The error message was caused by a typo in the hyperparameter name. The correct parameter name is \"precision_dtype\" instead of \"precission_dtype\".",
        "Answer_gpt_summary":"solut error messag caus typo hyperparamet correct paramet precis dtype instead preciss dtype"
    },
    {
        "Question_title":"machine learning algorithms questions",
        "Question_body":"Hi :\n\nI am planing to use k-means to form algorithm to do project. However, I am aware that there are certain shortcomings to find the optimal groups using k-means.\n\nCould you please tell the limitation and provide me with a detailed example?\n\nThanks",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1647856847267,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":null,
        "Answer_body":"Hello @hideonbush again,\n\nGenerally to think about k-means, please refer to below cons and pros. If you can provide more details and how you want to develop your project, I can share more:\n\nPros:\n\nK-means is very simple, highly flexible, and efficient.\n\n\nEasy to adjust and interpret the clustering results. Easy to explain the results in contrast to Neural Networks.\n\n\nThe efficiency of k-means implies that the algorithm is good at segmenting a dataset.\n\n\nAn instance can change cluster (move to another cluster) when the centroids are recomputed\n\nCons\n\nIt does not allow to develop the most optimal set of clusters and the number of clusters must be decided before the analysis. How many clusters to include is left at the discretion of the researcher. This involves a combination of common sense, domain knowledge, and statistical tools. Too many clusters tell you nothing because of the groups becoming very small and there are too many of them.\n\n\nWhen doing the analysis, the k-means algorithm will randomly select several different places from which to develop clusters. This can be good or bad depending on where the algorithm chooses to begin at. From there, the center of the clusters is recalculated until an adequate \"center'' is found for the number of clusters requested.\n\n\nThe order of the data input has an impact on the final results.\n\nHope this helps!\n\nRegards,\nYutong\n\n-Please kindly accept the answer if you feel helpful, thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/780362\/machine-learning-algorithms-questions.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1647898847903,
        "Question_original_content":"machin learn algorithm question plane us mean form algorithm project awar certain shortcom optim group mean tell limit provid detail exampl thank",
        "Question_preprocessed_content":"machin learn algorithm question plane us form algorithm project awar certain shortcom optim group tell limit provid detail exampl thank",
        "Question_gpt_summary_original":"The user is planning to use k-means algorithm for a project but is aware of its limitations in finding optimal groups. They are seeking information on the limitations and a detailed example.",
        "Question_gpt_summary":"user plan us mean algorithm project awar limit find optim group seek inform limit detail exampl",
        "Answer_original_content":"hello hideonbush gener think mean refer con pro provid detail want develop project share pro mean simpl highli flexibl effici easi adjust interpret cluster result easi explain result contrast neural network effici mean impli algorithm good segment dataset instanc chang cluster cluster centroid recomput con allow develop optim set cluster number cluster decid analysi cluster includ left discret research involv combin common sens domain knowledg statist tool cluster tell group small analysi mean algorithm randomli select differ place develop cluster good bad depend algorithm choos begin center cluster recalcul adequ center number cluster request order data input impact final result hope help regard yutong kindli accept answer feel help thank",
        "Answer_preprocessed_content":"hello gener think refer con pro provid detail want develop project share pro simpl highli flexibl effici easi adjust interpret cluster result easi explain result contrast neural network effici impli algorithm good segment dataset instanc chang cluster centroid recomput con allow develop optim set cluster number cluster decid analysi cluster includ left discret research involv combin common sens domain knowledg statist tool cluster tell group small analysi algorithm randomli select differ place develop cluster good bad depend algorithm choos begin center cluster recalcul adequ center number cluster request order data input impact final result hope help regard yutong kindli accept answer feel help thank",
        "Answer_gpt_summary_original":"Solutions provided: \n- The user can adjust and interpret the clustering results easily. \n- The algorithm is good at segmenting a dataset. \n- An instance can change cluster when the centroids are recomputed. \n\nLimitations mentioned: \n- K-means does not allow to develop the most optimal set of clusters and the number of clusters must be decided before the analysis. \n- The order of the data input has an impact on the final results.",
        "Answer_gpt_summary":"solut provid user adjust interpret cluster result easili algorithm good segment dataset instanc chang cluster centroid recomput limit mention mean allow develop optim set cluster number cluster decid analysi order data input impact final result"
    },
    {
        "Question_title":"Ask AWS SageMaker",
        "Question_body":"Can we make a new code through the sagemaker studio?\nIn my computer, GPU is GTX2080ti model, so if I use AWS sagemaker for paid service, can I get better performance?\nHow much GPU performance can you improve compared to before?\nI want to proceed with object segmentation through AWS sagemaker, can I use the code I used through sagemaker studio?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660876307100,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":35.0,
        "Answer_body":"My apologies, I am not fully sure on all the questions. But let me still make an attempt to respond to see if it helps.\n\nYes, you can write your own custom code through SageMaker studio.\n\nThis may not be an apple to apple comparison. The main advantage in this context, is your able to scale out your training to multiple nodes and cores (if your underlying model supports that). Likewise you can scale out the deployment as well. Typically the studio notebook is backed by a lightweight EC2 instance, but there are a large range of EC2 instances for training on SageMaker. Please refer to the following links for further assistance. 1. https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/notebooks-available-instance-types.html 2. https:\/\/aws.amazon.com\/ec2\/instance-types\/\n\nPlease refer to the response above for question # 2.\n\nDid you mean semantic segmentation? If yes, the answer is yes too.\n\nHope that helps!\n\nRegards, Punya",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUCKKplP7ES22DuZf8QJ38JA\/ask-aws-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1661427331608,
        "Question_original_content":"ask new code studio gpu gtxti model us paid servic better perform gpu perform improv compar want proce object segment us code studio",
        "Question_preprocessed_content":"ask new code studio gpu gtx model us paid servic better perform gpu perform improv compar want proce object segment us code studio",
        "Question_gpt_summary_original":"The user is facing challenges related to using AWS SageMaker for better GPU performance and object segmentation. They are unsure if they can create new code through SageMaker Studio and how much GPU performance improvement they can expect. They also want to know if they can use their existing code for object segmentation through SageMaker.",
        "Question_gpt_summary":"user face challeng relat better gpu perform object segment unsur creat new code studio gpu perform improv expect want know us exist code object segment",
        "Answer_original_content":"apolog fulli sure question let attempt respond help ye write custom code studio appl appl comparison main advantag context abl scale train multipl node core underli model support likewis scale deploy typic studio notebook back lightweight instanc larg rang instanc train refer follow link assist http doc aw amazon com latest notebook avail instanc type html http aw amazon com instanc type refer respons question mean semant segment ye answer ye hope help regard punya",
        "Answer_preprocessed_content":"apolog fulli sure question let attempt respond help ye write custom code studio appl appl comparison main advantag context abl scale train multipl node core likewis scale deploy typic studio notebook back lightweight instanc larg rang instanc train refer follow link assist refer respons question mean semant segment ye answer ye hope help regard punya",
        "Answer_gpt_summary_original":"Solutions provided:\n- Custom code can be written through SageMaker Studio.\n- Training and deployment can be scaled out to multiple nodes and cores.\n- There are a range of EC2 instances available for training on SageMaker.\n- Existing code can be used for object segmentation through SageMaker.\n- Semantic segmentation is also possible through SageMaker.",
        "Answer_gpt_summary":"solut provid custom code written studio train deploy scale multipl node core rang instanc avail train exist code object segment semant segment possibl"
    },
    {
        "Question_title":"How do parallel trials in GCP Vertex AI work?",
        "Question_body":"<p>When you make a hyperparameter tuning job, you can specify the number of trials to run in parallel. After that, you also select the type and count of the workers. What I don't understand is when I make two or more trials run in parallel, yet only one worker, each task is said to occupy 100% of the CPU. However, if one task occupies all of the CPU's resources, how can 2 of them run in parallel? Does GCP provision more than 1 machine?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1641920680143,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":168.0,
        "Answer_body":"<p><strong>Parallel trials<\/strong> allows you to run the trials concurrently depending on your input on the maximum number of trials.<\/p>\n<p>You are correct with your statement &quot;<em>one worker, each task is said to occupy 100% of the CPU<\/em>&quot; and for GCP to run other tasks in parallel,<\/p>\n<blockquote>\n<p>the hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer). The work pool spec that you set for your job is used for each individual training cluster.<\/p>\n<\/blockquote>\n<p>Please see <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning#parallel-trials\" rel=\"nofollow noreferrer\">Parallel Trials Documentation<\/a> for more details.<\/p>\n<p>And for more details about Hyperparameter Tuning, you may refer to <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/training\/using-hyperparameter-tuning\" rel=\"nofollow noreferrer\">Hyperparameter Tuning Documentation<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70670669",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1641974198663,
        "Question_original_content":"parallel trial gcp work hyperparamet tune job specifi number trial run parallel select type count worker understand trial run parallel worker task said occupi cpu task occupi cpu resourc run parallel gcp provis machin",
        "Question_preprocessed_content":"parallel trial gcp work hyperparamet tune job specifi number trial run parallel select type count worker understand trial run parallel worker task said occupi cpu task occupi cpu resourc run parallel gcp provis machin",
        "Question_gpt_summary_original":"The user is facing challenges in understanding how parallel trials work in GCP Vertex AI. They are confused about how multiple trials can run in parallel with only one worker and how each task can occupy 100% of the CPU's resources. The user is unsure if GCP provisions more than one machine to handle parallel trials.",
        "Question_gpt_summary":"user face challeng understand parallel trial work gcp confus multipl trial run parallel worker task occupi cpu resourc user unsur gcp provis machin handl parallel trial",
        "Answer_original_content":"parallel trial allow run trial concurr depend input maximum number trial correct statement worker task said occupi cpu gcp run task parallel hyperparamet tune servic provis multipl train process cluster multipl individu machin case singl process trainer work pool spec set job individu train cluster parallel trial document detail detail hyperparamet tune refer hyperparamet tune document",
        "Answer_preprocessed_content":"parallel trial allow run trial concurr depend input maximum number trial correct statement worker task said occupi cpu gcp run task parallel hyperparamet tune servic provis multipl train process cluster work pool spec set job individu train cluster parallel trial document detail detail hyperparamet tune refer hyperparamet tune document",
        "Answer_gpt_summary_original":"Solution: The hyperparameter tuning service provisions multiple training processing clusters (or multiple individual machines in the case of a single-process trainer) to run parallel trials. The work pool spec that is set for the job is used for each individual training cluster. The user can refer to the Parallel Trials Documentation and Hyperparameter Tuning Documentation for more details.",
        "Answer_gpt_summary":"solut hyperparamet tune servic provis multipl train process cluster multipl individu machin case singl process trainer run parallel trial work pool spec set job individu train cluster user refer parallel trial document hyperparamet tune document detail"
    },
    {
        "Question_title":"Azure: importing not already existing packages in 'src'",
        "Question_body":"<p>I have an experiment in which a module R script uses functions defined in a zip source (Data Exploration). <a href=\"https:\/\/blogs.msdn.microsoft.com\/benjguin\/2014\/09\/24\/how-to-upload-an-r-package-to-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> it's described how to do about the packages not already existing in the Azure environment.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/jKLlP.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/jKLlP.png\" alt=\"enter image description here\"><\/a> <\/p>\n\n<p>The DataExploration module has been imported from a file Azure.zip containing all the packages and functions I need (as shown in the next picture).<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/WlrVE.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/WlrVE.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I run the experiment nothing goes wrong. At the contrary, watching the log it seems clear that Azure is able to manage the source.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/AuJLD.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/AuJLD.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>The problem is that, when I deploy the web service (classic), if I run the experiment I get the following error:<\/p>\n\n<blockquote>\n  <p>FailedToEvaluateRScript: The following error occurred during\n  evaluation of R script: R_tryEval: return error: Error in\n  .zip.unpack(pkg, tmpDir) : zip file 'src\/scales_0.4.0.zip' not found ,\n  Error code: LibraryExecutionError, Http status code: 400, Timestamp:\n  Thu, 21 Jul 2016 09:05:25 GMT<\/p>\n<\/blockquote>\n\n<p>It's like he cannot see the scales_0.4.0.zip into the 'src' folder.<\/p>\n\n<p>The strange fact is that all used to work until some days ago. Then I have copied the experiment on a second workspace and it gives me the above error. <\/p>\n\n<p>I have also tried to upload again the DataExploration module on the new workspace, but it's the same.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1469093505773,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":144.0,
        "Answer_body":"<p>I have \"solved\" thanks to the help of the AzureML support: it is a bug they are trying to solve right now.<\/p>\n\n<p>The bug shows up when you have <strong>more R script modules<\/strong>, and the <strong>first has no a zip<\/strong> input module while the following have. <\/p>\n\n<p><em>Workaround<\/em>: connect the zip input module to the first R script module too.\n<a href=\"https:\/\/i.stack.imgur.com\/C4fV9.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/C4fV9.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/38500359",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1469606718630,
        "Question_original_content":"azur import exist packag src experi modul script us function defin zip sourc data explor describ packag exist azur environ dataexplor modul import file azur zip contain packag function need shown pictur run experi goe wrong contrari watch log clear azur abl manag sourc problem deploi web servic classic run experi follow error failedtoevaluaterscript follow error occur evalu script tryeval return error error zip unpack pkg tmpdir zip file src scale zip error code libraryexecutionerror http statu code timestamp thu jul gmt like scale zip src folder strang fact work dai ago copi experi second workspac give error tri upload dataexplor modul new workspac",
        "Question_preprocessed_content":"azur import exist packag src experi modul script us function defin zip sourc describ packag exist azur environ dataexplor modul import file contain packag function need run experi goe wrong contrari watch log clear azur abl manag sourc problem deploi web servic run experi follow error failedtoevaluaterscript follow error occur evalu script return error error tmpdir zip file error code libraryexecutionerror http statu code timestamp thu jul gmt like src folder strang fact work dai ago copi experi second workspac give error tri upload dataexplor modul new workspac",
        "Question_gpt_summary_original":"The user is facing challenges in importing packages that do not already exist in the Azure environment. The user has imported a module from a zip file containing all the required packages and functions, and the experiment runs without any issues. However, when the user deploys the web service, an error occurs, indicating that the zip file 'src\/scales_0.4.0.zip' is not found. The user has tried uploading the module again on a new workspace, but the issue persists.",
        "Question_gpt_summary":"user face challeng import packag exist azur environ user import modul zip file contain requir packag function experi run issu user deploi web servic error occur indic zip file src scale zip user tri upload modul new workspac issu persist",
        "Answer_original_content":"solv thank help support bug try solv right bug show script modul zip input modul follow workaround connect zip input modul script modul",
        "Answer_preprocessed_content":"solv thank help support bug try solv right bug show script modul zip input modul follow workaround connect zip input modul script modul",
        "Answer_gpt_summary_original":"Solution: The user was facing a bug in AzureML that occurs when the first R script module has no zip input module while the following ones have. The AzureML support team is working on solving the issue. As a workaround, the user can connect the zip input module to the first R script module as well.",
        "Answer_gpt_summary":"solut user face bug occur script modul zip input modul follow on support team work solv issu workaround user connect zip input modul script modul"
    },
    {
        "Question_title":"Python click incorrectly parses arguments when called in Vertex AI Pipeline",
        "Question_body":"<p>I'm trying to run a simple Ada-boosted Decision Tree regressor on GCP Vertex AI. To parse hyperparams and other arguments I use Click for Python, a very simple CLI library. Here's the setup for my task function:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>@click.command()\n@click.argument(&quot;input_path&quot;, type=str)\n@click.option(&quot;--output-path&quot;, type=str, envvar='AIP_MODEL_DIR')\n@click.option('--gcloud', is_flag=True, help='Run as if in Google Cloud Vertex AI Pipeline')\n@click.option('--grid', is_flag=True, help='Perform a grid search instead of a single run. Ignored with --gcloud')\n@click.option(&quot;--max_depth&quot;, type=int, default=4, help='Max depth of decision tree', show_default=True)\n@click.option(&quot;--n_estimators&quot;, type=int, default=50, help='Number of AdaBoost boosts', show_default=True)\ndef click_main(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators)\n\n\ndef train_model(input_path, output_path, gcloud, grid, max_depth, n_estimators):\n    print(input_path, output_path, gcloud)\n    logger = logging.getLogger(__name__)\n    logger.info(&quot;training models from processed data&quot;)\n    ...\n<\/code><\/pre>\n<p>When I run it locally like below, Click correctly grabs the params both from console and environment and proceeds with model training (<code>AIP_MODEL_DIR<\/code> is <code>gs:\/\/(BUCKET_NAME)\/models<\/code>)<\/p>\n<pre><code>\u276f python3 -m src.models.train_model gs:\/\/(BUCKET_NAME)\/data\/processed --gcloud\n\ngs:\/\/(BUCKET_NAME)\/data\/processed gs:\/\/(BUCKET_NAME)\/models True\n\n<\/code><\/pre>\n<p>However, when I put this code on the Vertex AI Pipeline, it throws an error, namely<\/p>\n<pre><code>FileNotFoundError: b\/(BUCKET_NAME)\/o\/data%2Fprocessed%20%20--gcloud%2Fprocessed_features.csv\n<\/code><\/pre>\n<p>As it is clearly seen, Click grabs both the parameter and the <code>--gcloud<\/code> option and assigns it to <code>input_path<\/code>. The print statement before that confirms it, both by having one too many spaces and <code>--gcloud<\/code> being parsed as false.<\/p>\n<pre><code>gs:\/\/(BUCKET_NAME)\/data\/processed  --gcloud gs:\/\/(BUCKET_NAME)\/models\/1\/model\/ False\n<\/code><\/pre>\n<p>Has anyone here encountered this issue or have any idea how to solve it?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1641797901177,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1641896853596,
        "Question_score":1.0,
        "Question_view_count":129.0,
        "Answer_body":"<p>I think is due the nature of <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/arguments\/?highlight=arguments\" rel=\"nofollow noreferrer\">arguments<\/a> and <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/options\/?highlight=options\" rel=\"nofollow noreferrer\">options<\/a>, you are mixing arguments and options although is not implicit stated in the documentation but argument will eat up the options that follow. If nargs is not allocated it will default to 1 considering everything after it follows as string which it looks like this is the case.<\/p>\n<blockquote>\n<p>nargs \u2013 the number of arguments to match. If not 1 the return value is a tuple instead of single value. The default for nargs is 1 (except if the type is a tuple, then it\u2019s the arity of the tuple).<\/p>\n<\/blockquote>\n<p>I think you should first use options followed by the argument as display on the <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/documentation\/?highlight=arguments\" rel=\"nofollow noreferrer\">documentation page<\/a>. Other approach is to group it under a command as show on this <a href=\"https:\/\/click.palletsprojects.com\/en\/7.x\/commands\/\" rel=\"nofollow noreferrer\">link<\/a>.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70648776",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1641810988040,
        "Question_original_content":"python click incorrectli pars argument call pipelin try run simpl ada boost decis tree regressor gcp pars hyperparam argument us click python simpl cli librari setup task function click command click argument input path type str click option output path type str envvar aip model dir click option gcloud flag true help run googl cloud pipelin click option grid flag true help perform grid search instead singl run ignor gcloud click option max depth type int default help max depth decis tree default true click option estim type int default help number adaboost boost default true def click main input path output path gcloud grid max depth estim train model input path output path gcloud grid max depth estim def train model input path output path gcloud grid max depth estim print input path output path gcloud logger log getlogg logger info train model process data run local like click correctli grab param consol environ proce model train aip model dir bucket model python src model train model bucket data process gcloud bucket data process bucket model true code pipelin throw error filenotfounderror bucket data fprocess gcloud fprocess featur csv clearli seen click grab paramet gcloud option assign input path print statement confirm have space gcloud pars fals bucket data process gcloud bucket model model fals encount issu idea solv",
        "Question_preprocessed_content":"python click incorrectli pars argument call pipelin try run simpl decis tree regressor gcp pars hyperparam argument us click python simpl cli librari setup task function run local like click correctli grab param consol environ proce model train code pipelin throw error clearli seen click grab paramet option assign print statement confirm have space pars fals encount issu idea solv",
        "Question_gpt_summary_original":"The user is encountering an issue with Python Click library incorrectly parsing arguments when called in Vertex AI Pipeline. Click is grabbing both the parameter and the --gcloud option and assigning it to input_path, causing an error. The issue is not present when running the code locally. The user is seeking help to solve this issue.",
        "Question_gpt_summary":"user encount issu python click librari incorrectli pars argument call pipelin click grab paramet gcloud option assign input path caus error issu present run code local user seek help solv issu",
        "Answer_original_content":"think natur argument option mix argument option implicit state document argument eat option follow narg alloc default consid follow string look like case narg number argument match return valu tupl instead singl valu default narg type tupl ariti tupl think us option follow argument displai document page approach group command link",
        "Answer_preprocessed_content":"think natur argument option mix argument option implicit state document argument eat option follow narg alloc default consid follow string look like case narg number argument match return valu tupl instead singl valu default narg think us option follow argument displai document page approach group command link",
        "Answer_gpt_summary_original":"Solutions provided:\n- Use options followed by the argument as displayed on the documentation page.\n- Group the options and arguments under a command.",
        "Answer_gpt_summary":"solut provid us option follow argument displai document page group option argument command"
    },
    {
        "Question_title":"Add custom packages to Azure Machine Learing Studio",
        "Question_body":"<p>I need to use the function tsCV on azure machine learning studio to evaluate models of forecast, but i got the error <\/p>\n\n<pre><code>could not find function \"tsCV\n<\/code><\/pre>\n\n<p>I'm trying to update the forecast package, but no package are loaded.\nI followed this tutorial\n<a href=\"http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html\" rel=\"noreferrer\">http:\/\/blog.revolutionanalytics.com\/2015\/10\/using-minicran-in-azure-ml.html<\/a>\nand \n<a href=\"https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/\" rel=\"noreferrer\">https:\/\/blog.tallan.com\/2016\/12\/27\/adding-r-packages-in-azure-ml\/<\/a>\nbut i dont get the same result.\nNo packages are load.<\/p>\n\n<p>I need an example of a package with R code that works o Azure ML or an update of forecast package to use tsCV function.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1536677172073,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":404.0,
        "Answer_body":"<p>I have installed the latest version of the forecast package and here are the steps I followed during the installation. <\/p>\n\n<ol>\n<li>Download latest version of CRAN<\/li>\n<li>Be sure that tsCV is working locally<\/li>\n<li>Zip all the dependencies + forecast package<\/li>\n<li>Zip all the generated zips together and upload it to the AMLStudio<\/li>\n<li>Run the following code:<\/li>\n<\/ol>\n\n<blockquote>\n<pre><code>install.packages(\"src\/glue.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/assertthat.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/fansi.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/utf8.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/stringr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/labeling.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/munsell.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/R6.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RColorBrewer.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/cli.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/crayon.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/pillar.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/xts.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/TTR.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/curl.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/digest.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/gtable.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lazyeval.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/plyr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/reshape2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/rlang.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/scales.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tibble.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/viridisLite.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/withr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/quadprog.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/quantmod.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/colorspace.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/fracdiff.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/ggplot2.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/lmtest.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/magrittr.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/Rcpp.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/timeDate.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/tseries.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/urca.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/uroot.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/zoo.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/RcppArmadillo.zip\", lib = \".\", repos = NULL, verbose = TRUE)\ninstall.packages(\"src\/forecast.zip\", lib = \".\", repos = NULL, verbose = TRUE)\n\nlibrary(forecast, lib.loc=\".\", verbose=TRUE)\nfar2 &lt;- function(x, h){forecast(Arima(x, order=c(2,0,0)), h=h)}\ne &lt;- tsCV(lynx, far2, h=1)\n<\/code><\/pre>\n<\/blockquote>\n\n<p><a href=\"https:\/\/drive.google.com\/open?id=10Bj0RGCmRFrRECLQrVc26nbx3T-bNSL6\" rel=\"nofollow noreferrer\">Here is the zip I have generated:<\/a><\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/bbowH.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/bbowH.png\" alt=\"My experiment\"><\/a><\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/52278613",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1537192338792,
        "Question_original_content":"add custom packag azur machin lear studio need us function tscv studio evalu model forecast got error function tscv try updat forecast packag packag load follow tutori http blog revolutionanalyt com minicran azur html http blog tallan com ad packag azur dont result packag load need exampl packag code work updat forecast packag us tscv function",
        "Question_preprocessed_content":"add custom packag azur machin lear studio need us function tscv studio evalu model forecast got error try updat forecast packag packag load follow tutori dont result packag load need exampl packag code work updat forecast packag us tscv function",
        "Question_gpt_summary_original":"The user is facing challenges in adding custom packages to Azure Machine Learning Studio. They are trying to use the function tsCV to evaluate forecast models but are encountering an error. The user has attempted to update the forecast package and followed tutorials but is unable to load any packages. They are seeking an example of a package with R code that works on Azure ML or an update of the forecast package to use the tsCV function.",
        "Question_gpt_summary":"user face challeng ad custom packag studio try us function tscv evalu forecast model encount error user attempt updat forecast packag follow tutori unabl load packag seek exampl packag code work updat forecast packag us tscv function",
        "Answer_original_content":"instal latest version forecast packag step follow instal download latest version cran sure tscv work local zip depend forecast packag zip gener zip upload amlstudio run follow code instal packag src glue zip lib repo null verbos true instal packag src stringi zip lib repo null verbos true instal packag src assertthat zip lib repo null verbos true instal packag src fansi zip lib repo null verbos true instal packag src utf zip lib repo null verbos true instal packag src stringr zip lib repo null verbos true instal packag src label zip lib repo null verbos true instal packag src munsel zip lib repo null verbos true instal packag src zip lib repo null verbos true instal packag src rcolorbrew zip lib repo null verbos true instal packag src cli zip lib repo null verbos true instal packag src crayon zip lib repo null verbos true instal packag src pillar zip lib repo null verbos true instal packag src xt zip lib repo null verbos true instal packag src ttr zip lib repo null verbos true instal packag src curl zip lib repo null verbos true instal packag src digest zip lib repo null verbos true instal packag src gtabl zip lib repo null verbos true instal packag src lazyev zip lib repo null verbos true instal packag src plyr zip lib repo null verbos true instal packag src reshap zip lib repo null verbos true instal packag src rlang zip lib repo null verbos true instal packag src scale zip lib repo null verbos true instal packag src tibbl zip lib repo null verbos true instal packag src viridislit zip lib repo null verbos true instal packag src withr zip lib repo null verbos true instal packag src quadprog zip lib repo null verbos true instal packag src quantmod zip lib repo null verbos true instal packag src colorspac zip lib repo null verbos true instal packag src fracdiff zip lib repo null verbos true instal packag src ggplot zip lib repo null verbos true instal packag src lmtest zip lib repo null verbos true instal packag src magrittr zip lib repo null verbos true instal packag src rcpp zip lib repo null verbos true instal packag src timed zip lib repo null verbos true instal packag src tseri zip lib repo null verbos true instal packag src urca zip lib repo null verbos true instal packag src uroot zip lib repo null verbos true instal packag src zoo zip lib repo null verbos true instal packag src rcpparmadillo zip lib repo null verbos true instal packag src forecast zip lib repo null verbos true librari forecast lib loc verbos true far function forecast arima order tscv lynx far zip gener",
        "Answer_preprocessed_content":"instal latest version forecast packag step follow instal download latest version cran sure tscv work local zip depend forecast packag zip gener zip upload amlstudio run follow code zip gener",
        "Answer_gpt_summary_original":"The discussion provides a solution to the challenge. The solution involves downloading the latest version of CRAN, zipping all the dependencies and the forecast package, and uploading it to AMLStudio. Then, the user needs to run the provided code to install and load all the packages. Finally, the user can use the tsCV function to evaluate forecast models. The user also provides a link to the zip file they generated.",
        "Answer_gpt_summary":"discuss provid solut challeng solut involv download latest version cran zip depend forecast packag upload amlstudio user need run provid code instal load packag final user us tscv function evalu forecast model user provid link zip file gener"
    },
    {
        "Question_title":"How do I add a directory of .wav files to the Kedro data catalogue?",
        "Question_body":"<p>This is my first time trying to use the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/index.html\" rel=\"nofollow noreferrer\">Kedro<\/a> package.<\/p>\n<p>I have a list of .wav files in an s3 bucket, and I'm keen to know how I can have them available within the Kedro data catalog.<\/p>\n<p>Any thoughts?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1611660152193,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":294.0,
        "Answer_body":"<p>I don't believe there's currently a dataset format that handles <code>.wav<\/code> files. You'll need to build a <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/07_extend_kedro\/03_custom_datasets.html\" rel=\"nofollow noreferrer\">custom dataset<\/a> that uses something like <a href=\"https:\/\/docs.python.org\/3\/library\/wave.html\" rel=\"nofollow noreferrer\">Wave<\/a> - not as much work as it sounds!<\/p>\n<p>This will enable you to do something like this in your catalog:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>dataset:\n  type: my_custom_path.WaveDataSet\n  filepath: path\/to\/individual\/wav_file.wav # this can be a s3:\/\/url\n<\/code><\/pre>\n<p>and you can then access your WAV data natively within your Kedro pipeline. You can do this for each <code>.wav<\/code> file you have.<\/p>\n<p>If you wanted to be able to access a whole folders worth of wav files, you might want to explore the notion of a &quot;wrapper&quot; dataset like the <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/kedro.io.PartitionedDataSet.html\" rel=\"nofollow noreferrer\">PartitionedDataSet<\/a> whose <a href=\"https:\/\/kedro.readthedocs.io\/en\/stable\/05_data\/02_kedro_io.html#partitioned-dataset-definition\" rel=\"nofollow noreferrer\">usage guide<\/a> can be found in the documentation.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1611661825368,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65900415",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1611661436392,
        "Question_original_content":"add directori wav file data catalogu time try us packag list wav file bucket keen know avail data catalog thought",
        "Question_preprocessed_content":"add directori wav file data catalogu time try us packag list wav file bucket keen know avail data catalog thought",
        "Question_gpt_summary_original":"The user is facing a challenge in adding a directory of .wav files from an s3 bucket to the Kedro data catalogue. They are seeking advice on how to make these files available within the Kedro data catalog.",
        "Question_gpt_summary":"user face challeng ad directori wav file bucket data catalogu seek advic file avail data catalog",
        "Answer_original_content":"believ current dataset format handl wav file need build custom dataset us like wave work sound enabl like catalog dataset type custom path wavedataset filepath path individu wav file wav url access wav data nativ pipelin wav file want abl access folder worth wav file want explor notion wrapper dataset like partitioneddataset usag guid document",
        "Answer_preprocessed_content":"believ current dataset format handl file need build custom dataset us like wave work sound enabl like catalog access wav data nativ pipelin file want abl access folder worth wav file want explor notion wrapper dataset like partitioneddataset usag guid document",
        "Answer_gpt_summary_original":"Solution: The user needs to build a custom dataset that uses something like Wave to handle .wav files. This will enable them to access their WAV data natively within their Kedro pipeline. They can do this for each .wav file they have. If they want to access a whole folder's worth of .wav files, they can explore the notion of a \"wrapper\" dataset like the PartitionedDataSet.",
        "Answer_gpt_summary":"solut user need build custom dataset us like wave handl wav file enabl access wav data nativ pipelin wav file want access folder worth wav file explor notion wrapper dataset like partitioneddataset"
    },
    {
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_body":"I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs & returns the desired results. However, no files are written to the specified S3 location.\n\nEndpoint Configuration\n\nThe endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a ml.m4.xlarge instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried s3:\/\/<bucket-name> as well as s3:\/\/<bucket-name>\/<some-other-path>. With the \"Capture content type\" I tried leaving everything blank, setting text\/csv in \"CSV\/Text\" and application\/json in \"JSON\".\n\nEndpoint Invokation\n\nThe endpoint is invoked in a Lambda function with a client. Here's the call:\n\nsagemaker_body_source = {\n            \"segments\": segments,\n            \"language\": language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[\"predictions\"]\n\n\nInternally, the endpoint uses a Flask API with an \/invocation path that returns the result.\n\nLogs\n\nThe endpoint itself works fine and the Flask API is logging input and output:\n\nINFO:api:body: {'segments': [<strings...>], 'language': 'de'}\n\nINFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660135320930,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":52.0,
        "Answer_body":"So the issue seemed to be related to the IAM role. The default role (ModelEndpoint-Role) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUKWPP4eXTTZe5qIUDJAXnsQ\/sagemaker-data-capture-does-not-write-files",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1660656368966,
        "Question_original_content":"data captur write file want enabl data captur specif endpoint far consol endpoint work fine log return desir result file written specifi locat endpoint configur endpoint base train job scikit learn classifi variant xlarg instanc type data captur enabl sampl percentag data captur storag locat tri captur content type tri leav blank set text csv csv text applic json json endpoint invok endpoint invok lambda function client bodi sourc segment segment languag languag payload json dump bodi sourc encod respons self client invok endpoint endpointnam endpoint bodi payload contenttyp applic json accept applic json result json load respons bodi read decod return result predict intern endpoint us flask api invoc path return result log endpoint work fine flask api log input output info api bodi segment languag info api output predict text text label label",
        "Question_preprocessed_content":"data captur write file want enabl data captur specif endpoint endpoint work fine log return desir result file written specifi locat endpoint configur endpoint base train job scikit learn classifi variant instanc type data captur enabl sampl percentag data captur storag locat tri captur content type tri leav blank set json endpoint invok endpoint invok lambda function client payload respons bodi payload result return result intern endpoint us flask api invoc path return result log endpoint work fine flask api log input output info api bodi info api output",
        "Question_gpt_summary_original":"The user is facing a challenge with Sagemaker Data Capture as no files are being written to the specified S3 location despite enabling data capture for a specific endpoint. The endpoint is based on a training job with a scikit learn classifier and has only one variant which is a ml.m4.xlarge instance type. The endpoint is invoked in a Lambda function with a client and internally uses a Flask API with an \/invocation path that returns the result. The endpoint works fine and logs input and output, but no files are being written to the specified S3 location.",
        "Question_gpt_summary":"user face challeng data captur file written specifi locat despit enabl data captur specif endpoint endpoint base train job scikit learn classifi variant xlarg instanc type endpoint invok lambda function client intern us flask api invoc path return result endpoint work fine log input output file written specifi locat",
        "Answer_original_content":"issu relat iam role default role modelendpoint role access write file work sdk us role studio receiv error messag",
        "Answer_preprocessed_content":"issu relat iam role default role access write file work sdk us role studio receiv error messag",
        "Answer_gpt_summary_original":"Solution: The issue was related to the IAM role. The default role (ModelEndpoint-Role) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. The solution is to create a new IAM role with the necessary permissions to write to the specified S3 location and assign it to the endpoint.",
        "Answer_gpt_summary":"solut issu relat iam role default role modelendpoint role access write file work sdk us role studio solut creat new iam role necessari permiss write specifi locat assign endpoint"
    },
    {
        "Question_title":"How to restrict model predicted value within range?",
        "Question_body":"<p>I want to do linear regression with aws sagemaker. Where i have trained my model with some values and it's predicting values as per inputs. but sometimes it predicts value out of range as in i am predicting percentage which can't go less than 0 and more than 100. how can i restrict it here:<\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nlinear = \nsagemaker.estimator.Estimator(containers[boto3.Session().region_name],\nrole, \ntrain_instance_count=1, \ntrain_instance_type='ml.c4.xlarge',\noutput_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n sagemaker_session=sess)\nlinear.set_hyperparameters(feature_dim=5,\nmini_batch_size=100,\npredictor_type='regressor',\nepochs=10,\nnum_models=32,\nloss='absolute_loss')\n\nlinear.fit({'train': s3_train_data, 'validation': s3_validation_data})\n<\/code><\/pre>\n\n<p>how can i make my model not to predict values out of range : [0,100].<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1522933178110,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":817.0,
        "Answer_body":"<p>Yes you can. You can implement the output_fn to \"brick wall\" your output. SageMaker would call the output_fn after the model returns the value to do any post-processing of the result. \nThis can be done by creating a separate python file, specify the output_fn method there. \nProvide this python file when instantiating your Estimator. \nsomething like <\/p>\n\n<pre><code>sess = sagemaker.Session()\n\nlinear = \nsagemaker.estimator.Estimator(containers[boto3.Session().region_name],\nrole, \ntrain_instance_count=1, \ntrain_instance_type='ml.c4.xlarge',\noutput_path='s3:\/\/{}\/{}\/output'.format(bucket, prefix),\n sagemaker_session=sess)\nlinear.set_hyperparameters(feature_dim=5,\nmini_batch_size=100,\npredictor_type='regressor',\nepochs=10,\nnum_models=32,\nloss='absolute_loss', \n<\/code><\/pre>\n\n<blockquote>\n  <p>entry_point = 'entry.py'<\/p>\n<\/blockquote>\n\n<p>)<\/p>\n\n<pre><code>linear.fit({'train': s3_train_data, 'validation': s3_validation_data})\n<\/code><\/pre>\n\n<p>Your entry.py could look something like <\/p>\n\n<pre><code>def output_fn(data, accepts):\n    \"\"\"\n    Args:\n        data: A result from TensorFlow Serving\n        accepts: The Amazon SageMaker InvokeEndpoint Accept value. The content type the response object should be\n            serialized to.\n    Returns:\n        object: The serialized object that will be send to back to the client.\n\n    \"\"\"    \n<\/code><\/pre>\n\n<blockquote>\n  <p>Implement the logic to \"brick wall\" here.<\/p>\n<\/blockquote>\n\n<pre><code>    return data.outputs['outputs'].string_val\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49673042",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1522946824052,
        "Question_original_content":"restrict model predict valu rang want linear regress train model valu predict valu input predict valu rang predict percentag restrict sess session linear estim estim contain boto session region role train instanc count train instanc type xlarg output path output format bucket prefix session sess linear set hyperparamet featur dim mini batch size predictor type regressor epoch num model loss absolut loss linear fit train train data valid valid data model predict valu rang",
        "Question_preprocessed_content":"restrict model predict valu rang want linear regress train model valu predict valu input predict valu rang predict percentag restrict model predict valu rang",
        "Question_gpt_summary_original":"The user is facing a challenge with their linear regression model in AWS Sagemaker, where the predicted values sometimes fall outside the range of 0 to 100, which is not acceptable for the percentage values they are predicting. The user is seeking a solution to restrict the predicted values within the desired range.",
        "Question_gpt_summary":"user face challeng linear regress model predict valu fall outsid rang accept percentag valu predict user seek solut restrict predict valu desir rang",
        "Answer_original_content":"ye implement output brick wall output output model return valu post process result creat separ python file specifi output method provid python file instanti estim like sess session linear estim estim contain boto session region role train instanc count train instanc type xlarg output path output format bucket prefix session sess linear set hyperparamet featur dim mini batch size predictor type regressor epoch num model loss absolut loss entri point entri linear fit train train data valid valid data entri look like def output data accept arg data result tensorflow serv accept invokeendpoint accept valu content type respons object serial return object serial object send client implement logic brick wall return data output output string val",
        "Answer_preprocessed_content":"ye implement brick wall output model return valu result creat separ python file specifi method provid python file instanti estim like look like implement logic brick wall",
        "Answer_gpt_summary_original":"Solution: The user can implement the output_fn to \"brick wall\" the output and restrict the predicted values within the desired range of 0 to 100. This can be done by creating a separate python file, specifying the output_fn method there, and providing this python file when instantiating the Estimator. The entry.py file should contain the output_fn method, where the logic to \"brick wall\" the output can be implemented.",
        "Answer_gpt_summary":"solut user implement output brick wall output restrict predict valu desir rang creat separ python file specifi output method provid python file instanti estim entri file contain output method logic brick wall output implement"
    },
    {
        "Question_title":"Is there a tool for my scenario?",
        "Question_body":"I have a scenario where we need to compare the content of two pages. Page2 will have the same text as page1 + some minor changes (extra image, extra space, etc). The changes are repetitive and very easy to catch with human eye. We need compare the 2 pages and confirm that indeed Page2 corresponds to Page1 and the changes have been applied correctly.\nWe have currently 10 000 of such page pairs, and expect half a million. We have people doing this manually.\n\nI would like them to stop.\n\nIs there an AI tool we could leverage for this scenario? I could feed it 1000 pairs and expect it to learn the pattern. I could provide either images of those pages or html. We have quite a big error margin (5% totally wrong, 15% partially wrong). I am totally green in the area, so any suggestions are welcome.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1601450792517,
        "Question_favorite_count":4.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@Ana-7414 I think your use case is a perfect scenario for the Azure computer vision READ API. This API can read printed and handwritten text and provide the output of the document\/image as a JSON response which can be compared or processed by your application to determine if the content is similar with in the margin of error specified. This service can currently process PDF documents for upto the first two pages for free, this will help you to evaluate the API without any cost. If the results are satisfactory you can switch to paid tier to process multiple documents without the limits of the free tier. You can try this API with any of the language that is suitable with this quickstart.\n\nWe hope this helps you to get started. If this response is helpful, please accept the same as answer. Thanks!!",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/112364\/is-there-a-tool-for-my-scenario.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1601488203636,
        "Question_original_content":"tool scenario scenario need compar content page page text page minor chang extra imag extra space chang repetit easi catch human ey need compar page confirm page correspond page chang appli correctli current page pair expect half million peopl manual like stop tool leverag scenario feed pair expect learn pattern provid imag page html big error margin total wrong partial wrong total green area suggest welcom",
        "Question_preprocessed_content":"tool scenario scenario need compar content page page text page minor chang chang repetit easi catch human ey need compar page confirm page correspond page chang appli correctli current page pair expect half million peopl manual like stop tool leverag scenario feed pair expect learn pattern provid imag page html big error margin total green area suggest welcom",
        "Question_gpt_summary_original":"The user needs to compare the content of two pages, with one page having minor changes from the other. They have 10,000 page pairs to compare manually and expect half a million more. The user is looking for an AI tool that can learn the pattern from 1000 pairs and can work with either images or HTML. The user has a 5% error margin for totally wrong and 15% for partially wrong comparisons.",
        "Question_gpt_summary":"user need compar content page page have minor chang page pair compar manual expect half million user look tool learn pattern pair work imag html user error margin total wrong partial wrong comparison",
        "Answer_original_content":"ana think us case perfect scenario azur vision read api api read print handwritten text provid output document imag json respons compar process applic determin content similar margin error specifi servic current process pdf document upto page free help evalu api cost result satisfactori switch paid tier process multipl document limit free tier try api languag suitabl quickstart hope help start respons help accept answer thank",
        "Answer_preprocessed_content":"think us case perfect scenario azur vision read api api read print handwritten text provid output json respons compar process applic determin content similar margin error specifi servic current process pdf document upto page free help evalu api cost result satisfactori switch paid tier process multipl document limit free tier try api languag suitabl quickstart hope help start respons help accept answer thank",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use the Azure computer vision READ API. This API can read printed and handwritten text and provide the output of the document\/image as a JSON response which can be compared or processed by the user's application to determine if the content is similar within the margin of error specified. The user can try this API with any suitable language using the quickstart. The API can process PDF documents for up to the first two pages for free, and if the results are satisfactory, the user can switch to the paid tier to process multiple documents without the limits of the free tier.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us azur vision read api api read print handwritten text provid output document imag json respons compar process user applic determin content similar margin error specifi user try api suitabl languag quickstart api process pdf document page free result satisfactori user switch paid tier process multipl document limit free tier"
    },
    {
        "Question_title":"GCP cannot create Managed Notebook on Vertex AI",
        "Question_body":"<p>Using a GCP account that started as free, but does have billing enabled, I can't create a managed notebook and get the following popup error:<\/p>\n<p>Quota exceeded for quota metric 'Create Runtime API requests' and limit 'Create Runtime API requests per minute' of service 'notebooks.googleapis.com' for consumer 'project_number:....'<\/p>\n<p>Navigating to Quotas --&gt; Notebook API --&gt; Create Runtime API requests per minute<\/p>\n<p>Edit Quota: Create Runtime API requests per minute\nCurrent limit: 0\nEnter a new quota limit between 0 and 0.<\/p>\n<p>0 doesn't work..<\/p>\n<p>Is there something that I can do, or should have done already to increase this quota?<\/p>\n<p>TIA for any help.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1638037173043,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":285.0,
        "Answer_body":"<p>Managed notebooks is still pre-GA and is currently unavailable to the projects with insufficient service usage history.<\/p>\n<p>You can wait for the GA of the service or use a project with more service usage.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70137519",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1638931046247,
        "Question_original_content":"gcp creat manag notebook gcp account start free bill enabl creat manag notebook follow popup error quota exceed quota metric creat runtim api request limit creat runtim api request minut servic notebook googleapi com consum project number navig quota notebook api creat runtim api request minut edit quota creat runtim api request minut current limit enter new quota limit work increas quota tia help",
        "Question_preprocessed_content":"gcp creat manag notebook gcp account start free bill enabl creat manag notebook follow popup error quota exceed quota metric creat runtim api request limit creat runtim api request minut servic consum navig quota notebook api creat runtim api request minut edit quota creat runtim api request minut current limit enter new quota limit increas quota tia help",
        "Question_gpt_summary_original":"The user is unable to create a managed notebook on Vertex AI due to a quota limit issue. The error message states that the quota for 'Create Runtime API requests per minute' has been exceeded. The user has tried to edit the quota limit but is unable to do so. The user is seeking help to increase the quota limit.",
        "Question_gpt_summary":"user unabl creat manag notebook quota limit issu error messag state quota creat runtim api request minut exceed user tri edit quota limit unabl user seek help increas quota limit",
        "Answer_original_content":"manag notebook pre current unavail project insuffici servic usag histori wait servic us project servic usag",
        "Answer_preprocessed_content":"manag notebook current unavail project insuffici servic usag histori wait servic us project servic usag",
        "Answer_gpt_summary_original":"Solution: The discussion does not provide any solution to increase the quota limit for 'Create Runtime API requests per minute'. However, it suggests waiting for the GA of the service or using a project with more service usage.",
        "Answer_gpt_summary":"solut discuss provid solut increas quota limit creat runtim api request minut suggest wait servic project servic usag"
    },
    {
        "Question_title":"How to return a float with 'text\/csv' as \"Content-Type\" from SageMaker endpoint that uses custom inference code?",
        "Question_body":"<p>I am trying to return output(or predictions) from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as &quot;Content-Type&quot;. I have tried multiple ways, but the sagemaker always returns with 'text\/html; charset=utf-8' as the &quot;Content-Type&quot;, and I would like SageMaker to return 'text\/csv' or 'text\/csv; charset=utf-8'.<\/p>\n<p>Here's the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#process-output\" rel=\"nofollow noreferrer\"><code>output_fn<\/code><\/a> from my inference-code:<\/p>\n<pre><code>** my other code **\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return output_float\n<\/code><\/pre>\n<p>above function returns number with float data-type and I got error(in cloudwatch logs) that  this function should only be returning string, tuple, dict or Respoonse instance.<\/p>\n<p>So, here are all the different ways I have tried to have SageMaker return my number with 'text\/csv' but only receives 'text\/html; charset=utf-8'<\/p>\n<ol>\n<li><code>return json.dumps(output_float)<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float}\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li><code>return f&quot;{output_float},\\n&quot;<\/code>. this sent 'text\/html; charset=utf-8'.<\/li>\n<li>by using <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/serializers.html\" rel=\"nofollow noreferrer\"><code>sagemaker.serializers.CSVSerializer<\/code><\/a> like this:\n<pre><code>from sagemaker.serializers import CSVSerializer\ncsv_serialiser = CSVSerializer(content_type='text\/csv')\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return csv_serialiser.serialize(output_float)\n<\/code><\/pre>\nI got <code>'NoneType' object has no attribute 'startswith'<\/code> error with this.<\/li>\n<li>as a tuple: <code>return (output_float,)<\/code>. I haven't noted down what this did, but it sure didn't return the number with 'text\/csv' as &quot;Content-Type&quot;.<\/li>\n<li>made changes in my model object to return a float on calling <code>.predict_proba<\/code> on my model-object and deployed it from sagemaker studio without using any custom inference code and deployed this from SageMaker studio. but got this error after sending a request to the endpoint: <code>'NoneType' object has no attribute 'startswith'<\/code>, but at my side, when I pass proper inputs to the unpicked model and call .predict_proba, i get float as expected.<\/li>\n<li>by returning <a href=\"https:\/\/tedboy.github.io\/flask\/generated\/generated\/flask.Response.html#flask.Response\" rel=\"nofollow noreferrer\"><code>flask.Response<\/code><\/a> like this:\n<pre><code>from flask import Response\n\ndef output_fn(prediction, content_type='text\/csv'):\n    ** my other code **\n    return Response(response=output_float, status=200, headers={'Content-Type':'text\/csv; charset=utf-8'})\n<\/code><\/pre>\nbut, I got some IndexError with this(I didn't notedown the traceback.)<\/li>\n<\/ol>\n<p>Some other info:<\/p>\n<ol>\n<li>Model I am using is completely outside of SageMaker, not from a training job or anything like that.<\/li>\n<li>All of the aforementioned endpoints have been deployed entirely from aws-cli with relevant .json files, except the one in point-8 above.<\/li>\n<\/ol>\n<p>How do I return number with &quot;text\/csv&quot; as content-type from sagemaker? I need my output in &quot;text\/csv&quot; content-type specifically for Model-Quality-Monitor. How do I do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1646135687137,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1646652069630,
        "Question_score":0.0,
        "Question_view_count":461.0,
        "Answer_body":"<p>From the <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/scikit_bring_your_own\/container\/decision_trees\/predictor.py#L88\" rel=\"nofollow noreferrer\">scikit_bring_your_own<\/a> example, I suggest testing by setting the Response as follows:<\/p>\n<pre><code>return flask.Response(response= output_float, status=200, mimetype=&quot;text\/csv&quot;)\n<\/code><\/pre>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71308112",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1646685903080,
        "Question_original_content":"return float text csv content type endpoint us custom infer code try return output predict endpoint text csv text csv charset utf content type tri multipl wai return text html charset utf content type like return text csv text csv charset utf output infer code code def output predict content type text csv code return output float function return number float data type got error cloudwatch log function return string tupl dict respoons instanc differ wai tri return number text csv receiv text html charset utf return json dump output float sent text html charset utf return output float sent text html charset utf return output float sent text html charset utf return output float sent text html charset utf return output float sent text html charset utf serial csvserial like serial import csvserial csv serialis csvserial content type text csv def output predict content type text csv code return csv serialis serial output float got nonetyp object attribut startswith error tupl return output float haven note sure return number text csv content type chang model object return float call predict proba model object deploi studio custom infer code deploi studio got error send request endpoint nonetyp object attribut startswith pass proper input unpick model predict proba float expect return flask respons like flask import respons def output predict content type text csv code return respons respons output float statu header content type text csv charset utf got indexerror notedown traceback info model complet outsid train job like aforement endpoint deploi entir aw cli relev json file point return number text csv content type need output text csv content type specif model qualiti monitor",
        "Question_preprocessed_content":"return float endpoint us custom infer code try return output endpoint tri multipl wai return like return function return number float got error function return string tupl dict respoons instanc differ wai tri return number receiv sent sent sent sent sent like got error tupl haven note sure return number chang model object return float call deploi studio custom infer code deploi studio got error send request endpoint pass proper input unpick model float expect return like got indexerror info model complet outsid train job like aforement endpoint deploi entir relev json file return number need output specif",
        "Question_gpt_summary_original":"The user is trying to return output from a SageMaker endpoint with 'text\/csv' or 'text\/csv; charset=utf-8' as \"Content-Type\", but SageMaker always returns with 'text\/html; charset=utf-8' as the \"Content-Type\". The user has tried multiple ways to return the number with 'text\/csv', but none of them worked. The user has also encountered errors such as \"NoneType\" object has no attribute 'startswith' and IndexError. The user needs the output in 'text\/csv' content-type specifically for Model-Quality-Monitor.",
        "Question_gpt_summary":"user try return output endpoint text csv text csv charset utf content type return text html charset utf content type user tri multipl wai return number text csv work user encount error nonetyp object attribut startswith indexerror user need output text csv content type specif model qualiti monitor",
        "Answer_original_content":"scikit bring exampl suggest test set respons follow return flask respons respons output float statu mimetyp text csv",
        "Answer_preprocessed_content":"exampl suggest test set respons follow",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to set the Response as follows: \n\nreturn flask.Response(response= output_float, status=200, mimetype=\"text\/csv\")\n\nThis solution is suggested based on the scikit_bring_your_own example.",
        "Answer_gpt_summary":"solut possibl solut mention discuss set respons follow return flask respons respons output float statu mimetyp text csv solut suggest base scikit bring exampl"
    },
    {
        "Question_title":"Caret package in Microsoft Azure ML",
        "Question_body":"<p>I want to load caret library in Azure ML. This works when R version is set to be CRAN R 3.1.0, but on Microsoft R Open 3.2.2 won't work. I must use R Open version because of the other packages that I'm using in my project, which are not supported in that earlier version 3.1.0.\nTherefore, the question is how to load this library on ML Azure using Microsoft R Open 3.2.2?<\/p>\n\n<p>Thanks!<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1484227623947,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1543765030772,
        "Question_score":2.0,
        "Question_view_count":961.0,
        "Answer_body":"<p>From one of your comments above, it sounds like the version of the <code>caret<\/code> package you've used requires an R version >3.1.2. I recommend using an older version of the package: the <code>caret<\/code> binary from <a href=\"http:\/\/cran.cnr.berkeley.edu\/bin\/windows\/contrib\/3.1\/\" rel=\"nofollow noreferrer\">this 3.1 archive<\/a> (6.0-68) worked for me. I used these statements to load the package:<\/p>\n\n<pre><code>install.packages(\"src\/caret_6.0-68.zip\", lib=\".\", repos= NULL, verbose=TRUE)\nlibrary(\"caret\", lib.loc=\".\", verbose=TRUE)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41614428",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1485310605283,
        "Question_original_content":"caret packag want load caret librari work version set cran microsoft open won work us open version packag project support earlier version question load librari azur microsoft open thank",
        "Question_preprocessed_content":"caret packag want load caret librari work version set cran microsoft open won work us open version packag project support earlier version question load librari azur microsoft open thank",
        "Question_gpt_summary_original":"The user is facing a challenge in loading the caret library in Microsoft Azure ML. The library works with CRAN R 3.1.0 but not with Microsoft R Open 3.2.2, which the user needs to use due to other packages in their project. The user is seeking a solution to load the library using Microsoft R Open 3.2.2 in Azure ML.",
        "Question_gpt_summary":"user face challeng load caret librari librari work cran microsoft open user need us packag project user seek solut load librari microsoft open",
        "Answer_original_content":"comment sound like version caret packag requir version recommend older version packag caret binari archiv work statement load packag instal packag src caret zip lib repo null verbos true librari caret lib loc verbos true",
        "Answer_preprocessed_content":"comment sound like version packag requir version recommend older version packag binari archiv work statement load packag",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to use an older version of the caret package (6.0-68) that is compatible with Microsoft R Open 3.2.2. The user can download the package from the 3.1 archive and install it using the provided statements.",
        "Answer_gpt_summary":"solut possibl solut mention discuss us older version caret packag compat microsoft open user download packag archiv instal provid statement"
    },
    {
        "Question_title":"What are SageMaker pipelines actually?",
        "Question_body":"<p>Sagemaker pipelines are rather unclear to me, I'm not experienced in the field of ML but I'm working on figuring out the pipeline definitions.<\/p>\n<p>I have a few questions:<\/p>\n<ul>\n<li><p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/li>\n<li><p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/li>\n<li><p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/li>\n<\/ul>\n<p>I can't seem to find any examples besides the Python SDK usage, how come?<\/p>\n<p>The docs and workshops seem only to properly describe the Python SDK usage,it would be really helpful if someone could clear this up for me!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1638395443060,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":716.0,
        "Answer_body":"<p>SageMaker has two things called Pipelines: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/pipelines.html\" rel=\"nofollow noreferrer\">Model Building Pipelines<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/inference-pipelines.html\" rel=\"nofollow noreferrer\">Serial Inference Pipelines<\/a>. I believe you're referring to the former<\/p>\n<p>A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints<\/p>\n<p>A serial inference pipeline is two or more SageMaker models run one after the other<\/p>\n<p>A model building pipeline is defined in JSON, and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker<\/p>\n<blockquote>\n<p>Is sagemaker pipelines a stand-alone service\/feature? Because I don't see any option to create them through the console, though I do see CloudFormation and CDK resources.<\/p>\n<\/blockquote>\n<p>You can create\/modify them using the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePipeline.html\" rel=\"nofollow noreferrer\">API<\/a>, which can also be called via the <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/create-pipeline.html\" rel=\"nofollow noreferrer\">CLI<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/pipelines\/sagemaker.workflow.pipelines.html#sagemaker.workflow.pipeline.Pipeline.create\" rel=\"nofollow noreferrer\">Python SDK<\/a>, or <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-pipeline.html\" rel=\"nofollow noreferrer\">CloudFormation<\/a>. These all use the AWS API under the hood<\/p>\n<p>You can start\/stop\/view them in SageMaker Studio:<\/p>\n<pre><code>Left-side Navigation bar &gt; SageMaker resources &gt; Drop-down menu &gt; Pipelines\n<\/code><\/pre>\n<blockquote>\n<p>Is a sagemaker pipeline essentially codepipeline? How do these integrate, how do these differ?<\/p>\n<\/blockquote>\n<p>Unlikely. CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration as far as I can tell, other than that you can start a SM pipeline with CP<\/p>\n<blockquote>\n<p>There's also a Python SDK, how does this differ from the CDK and CloudFormation?<\/p>\n<\/blockquote>\n<p>The Python SDK is a stand-alone library to interact with SageMaker in a developer-friendly fashion. It's more dynamic than CloudFormation. Let's you build pipelines using code. Whereas CloudFormation takes a static JSON string<\/p>\n<p>A very simple example of Python SageMaker SDK usage:<\/p>\n\n<pre class=\"lang-python prettyprint-override\"><code>processor = SKLearnProcessor(\n    framework_version=&quot;0.23-1&quot;,\n    instance_count=1,\n    instance_type=&quot;ml.m5.large&quot;,\n    role=&quot;role-arn&quot;,\n)\n\nprocessing_step = ProcessingStep(\n    name=&quot;processing&quot;,\n    processor=processor,\n    code=&quot;preprocessor.py&quot;\n)\n\npipeline = Pipeline(name=&quot;foo&quot;, steps=[processing_step])\npipeline.upsert(role_arn = ...)\npipeline.start()\n<\/code><\/pre>\n<p><code>pipeline.definition()<\/code> produces rather verbose JSON like this:<\/p>\n\n<pre class=\"lang-json prettyprint-override\"><code>{\n&quot;Version&quot;: &quot;2020-12-01&quot;,\n&quot;Metadata&quot;: {},\n&quot;Parameters&quot;: [],\n&quot;PipelineExperimentConfig&quot;: {\n    &quot;ExperimentName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineName&quot;\n    },\n    &quot;TrialName&quot;: {\n        &quot;Get&quot;: &quot;Execution.PipelineExecutionId&quot;\n    }\n},\n&quot;Steps&quot;: [\n    {\n        &quot;Name&quot;: &quot;processing&quot;,\n        &quot;Type&quot;: &quot;Processing&quot;,\n        &quot;Arguments&quot;: {\n            &quot;ProcessingResources&quot;: {\n                &quot;ClusterConfig&quot;: {\n                    &quot;InstanceType&quot;: &quot;ml.m5.large&quot;,\n                    &quot;InstanceCount&quot;: 1,\n                    &quot;VolumeSizeInGB&quot;: 30\n                }\n            },\n            &quot;AppSpecification&quot;: {\n                &quot;ImageUri&quot;: &quot;246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-scikit-learn:0.23-1-cpu-py3&quot;,\n                &quot;ContainerEntrypoint&quot;: [\n                    &quot;python3&quot;,\n                    &quot;\/opt\/ml\/processing\/input\/code\/preprocessor.py&quot;\n                ]\n            },\n            &quot;RoleArn&quot;: &quot;arn:aws:iam::123456789012:role\/foo&quot;,\n            &quot;ProcessingInputs&quot;: [\n                {\n                    &quot;InputName&quot;: &quot;code&quot;,\n                    &quot;AppManaged&quot;: false,\n                    &quot;S3Input&quot;: {\n                        &quot;S3Uri&quot;: &quot;s3:\/\/bucket\/preprocessor.py&quot;,\n                        &quot;LocalPath&quot;: &quot;\/opt\/ml\/processing\/input\/code&quot;,\n                        &quot;S3DataType&quot;: &quot;S3Prefix&quot;,\n                        &quot;S3InputMode&quot;: &quot;File&quot;,\n                        &quot;S3DataDistributionType&quot;: &quot;FullyReplicated&quot;,\n                        &quot;S3CompressionType&quot;: &quot;None&quot;\n                    }\n                }\n            ]\n        }\n    }\n  ]\n}\n<\/code><\/pre>\n<p>You could <em>use<\/em> the above JSON with CloudFormation\/CDK, but you <em>build<\/em> the JSON with the SageMaker SDK<\/p>\n<p>You can also define model building workflows using Step Function State Machines, using the <a href=\"https:\/\/aws-step-functions-data-science-sdk.readthedocs.io\/en\/stable\/\" rel=\"nofollow noreferrer\">Data Science SDK<\/a>, or <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/workflows\/airflow\/index.html\" rel=\"nofollow noreferrer\">Airflow<\/a><\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1654197850167,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70191668",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1638396070903,
        "Question_original_content":"pipelin actual pipelin unclear experienc field work figur pipelin definit question pipelin stand servic featur option creat consol cloudform cdk resourc pipelin essenti codepipelin integr differ python sdk differ cdk cloudform exampl python sdk usag come doc workshop properli python sdk usag help clear",
        "Question_preprocessed_content":"pipelin actual pipelin unclear experienc field work figur pipelin definit question pipelin option creat consol cloudform cdk resourc pipelin essenti codepipelin integr differ python sdk differ cdk cloudform exampl python sdk usag come doc workshop properli python sdk usag help clear",
        "Question_gpt_summary_original":"The user is facing challenges in understanding SageMaker pipelines and has several questions regarding its features and integration with other services such as CloudFormation and CDK. They are also confused about the differences between SageMaker pipelines and codepipeline, as well as the Python SDK and CloudFormation. The user is having difficulty finding examples of SageMaker pipelines and is seeking clarification on its usage.",
        "Question_gpt_summary":"user face challeng understand pipelin question featur integr servic cloudform cdk confus differ pipelin codepipelin python sdk cloudform user have difficulti find exampl pipelin seek clarif usag",
        "Answer_original_content":"thing call pipelin model build pipelin serial infer pipelin believ refer model build pipelin defin step machin learn workflow pre process hyperparamet tune batch transform set endpoint serial infer pipelin model run model build pipelin defin json host run sort proprietari serverless fashion pipelin stand servic featur option creat consol cloudform cdk resourc creat modifi api call cli python sdk cloudform us aw api hood start stop view studio left navig bar resourc drop menu pipelin pipelin essenti codepipelin integr differ unlik codepipelin build deploi code specif direct integr far tell start pipelin python sdk differ cdk cloudform python sdk stand librari interact develop friendli fashion dynam cloudform let build pipelin code cloudform take static json string simpl exampl python sdk usag processor sklearnprocessor framework version instanc count instanc type larg role role arn process step processingstep process processor processor code preprocessor pipelin pipelin foo step process step pipelin upsert role arn pipelin start pipelin definit produc verbos json like version metadata paramet pipelineexperimentconfig experimentnam execut pipelinenam trialnam execut pipelineexecutionid step process type process argument processingresourc clusterconfig instancetyp larg instancecount volumesizeingb appspecif imageuri dkr ecr west amazonaw com scikit learn cpu containerentrypoint python opt process input code preprocessor rolearn arn aw iam role foo processinginput inputnam code appmanag fals sinput suri bucket preprocessor localpath opt process input code sdatatyp sprefix sinputmod file sdatadistributiontyp fullyrepl scompressiontyp us json cloudform cdk build json sdk defin model build workflow step function state machin data scienc sdk airflow",
        "Answer_preprocessed_content":"thing call pipelin model build pipelin serial infer pipelin believ refer model build pipelin defin step machin learn workflow hyperparamet tune batch transform set endpoint serial infer pipelin model run model build pipelin defin json sort proprietari serverless fashion pipelin option creat consol cloudform cdk resourc api call cli python sdk cloudform us aw api hood studio pipelin essenti codepipelin integr differ unlik codepipelin build deploi code specif direct integr far tell start pipelin python sdk differ cdk cloudform python sdk librari interact fashion dynam cloudform let build pipelin code cloudform take static json string simpl exampl python sdk usag produc verbos json like us json build json sdk defin model build workflow step function state machin data scienc sdk airflow",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- SageMaker has two types of pipelines: Model Building Pipelines and Serial Inference Pipelines.\n- A model building pipeline defines steps in a machine learning workflow, such as pre-processing, hyperparameter tuning, batch transformations, and setting up endpoints.\n- A model building pipeline is defined in JSON and is hosted\/run in some sort of proprietary, serverless fashion by SageMaker.\n- You can create\/modify SageMaker pipelines using the API, which can also be called via the CLI, Python SDK, or CloudFormation.\n- You can start\/stop\/view SageMaker pipelines in SageMaker Studio.\n- CodePipeline is more for building and deploying code, not specific to SageMaker. There is no direct integration",
        "Answer_gpt_summary":"possibl solut mention discuss type pipelin model build pipelin serial infer pipelin model build pipelin defin step machin learn workflow pre process hyperparamet tune batch transform set endpoint model build pipelin defin json host run sort proprietari serverless fashion creat modifi pipelin api call cli python sdk cloudform start stop view pipelin studio codepipelin build deploi code specif direct integr"
    },
    {
        "Question_title":"Multiple Inputs\/Outputs from Execute R Script",
        "Question_body":"<p>Assume I have an Execute R Script that calculates multiple variables, say X and Y.\nIs it possible to save X as a dataset ds_X and Y as a dataset ds_Y?<\/p>\n\n<p>The problem is that there is only 1 output port available that needs to be mapped to a data.frame. Am I missing an option to add more output ports?\nSame problem for input ports. I may connect 2 of the \"Enter Data Manually\" modules to it, but what if I need 3? The current workaround is to put CSV files in a ZIP file and connect that. Are there easier solution?<\/p>\n\n<p><strong>Example of what i tried:<\/strong><\/p>\n\n<p>I tried adding ds_X and ds_Y to a list. The idea is to pass this list to multiple \"Execute R Script\" modules and use the required list elements there.\nMapping a list to an output port does not seem to work though:<\/p>\n\n<pre><code># Calculate lots of stuff - results are ds_X and ds_Y\nds_X &lt;- mtcars\nds_Y &lt;- cars\nout &lt;- list(ds_X, ds_Y)\n\nmaml.mapOutputPort(\"out\")\n<\/code><\/pre>\n\n<p>results in an error:<\/p>\n\n<pre><code>Error: Mapped variable must be of class type data.frame at this time.\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_created_time":1505987948480,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1505991363608,
        "Question_score":0.0,
        "Question_view_count":754.0,
        "Answer_body":"<p>You can author custom R Modules. <\/p>\n\n<p>Here is some documentation: \n<a href=\"https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/04\/23\/build-your-own-r-modules-in-azure-ml\/\" rel=\"nofollow noreferrer\">https:\/\/blogs.technet.microsoft.com\/machinelearning\/2015\/04\/23\/build-your-own-r-modules-in-azure-ml\/<\/a>\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-custom-r-modules\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-custom-r-modules<\/a><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/46340959",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1506004266756,
        "Question_original_content":"multipl input output execut script assum execut script calcul multipl variabl possibl save dataset dataset problem output port avail need map data frame miss option add output port problem input port connect enter data manual modul need current workaround csv file zip file connect easier solut exampl tri tri ad list idea pass list multipl execut script modul us requir list element map list output port work calcul lot stuff result mtcar car list maml mapoutputport result error error map variabl class type data frame time",
        "Question_preprocessed_content":"multipl execut script assum execut script calcul multipl variabl possibl save dataset dataset problem output port avail need map miss option add output port problem input port connect enter data manual modul need current workaround csv file zip file connect easier solut exampl tri tri ad list idea pass list multipl execut script modul us requir list element map list output port work result error",
        "Question_gpt_summary_original":"The user is facing challenges with the Execute R Script in Azure Machine Learning Studio, specifically with regards to multiple inputs and outputs. They are unable to save X and Y as separate datasets due to the availability of only one output port. Similarly, they are unable to connect more than two \"Enter Data Manually\" modules to the input port. The user has attempted to add ds_X and ds_Y to a list, but mapping a list to an output port does not seem to work.",
        "Question_gpt_summary":"user face challeng execut script studio specif regard multipl input output unabl save separ dataset avail output port similarli unabl connect enter data manual modul input port user attempt add list map list output port work",
        "Answer_original_content":"author custom modul document http blog technet microsoft com machinelearn build modul azur http doc microsoft com azur machin learn machin learn custom modul",
        "Answer_preprocessed_content":"author custom modul document",
        "Answer_gpt_summary_original":"Solution: The user can author custom R Modules. Microsoft provides documentation on how to build custom R modules in Azure ML Studio. The user can refer to the provided links for more information.",
        "Answer_gpt_summary":"solut user author custom modul microsoft provid document build custom modul studio user refer provid link inform"
    },
    {
        "Question_title":"SageMaker notebook instance in VPC failed to connect to local database",
        "Question_body":"hi there,\n\nI am setting up a jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. But it shows the following error messages.\n\": com.microsoft.sqlserver.jdbc.SQLServerException: The TCP\/IP connection to the host xxx.xxx.xxx.xxx, port 21000 has failed. Error: \"connect timed out. Verify the connection properties. Make sure that an instance of SQL Server is running on the host and accepting TCP\/IP connections at the port. Make sure that TCP connections to the port are not blocked by a firewall.\".\"\n\nI used the exactly the same VPC, subnet, security group as what i used in a glue job to extract the data from the local db. While the glue job works but the SageMaker notebook failed. I am sure the firewalls are opened.\nCould anyone tell me how to solve it?\nI also came across the following articles, but i am not sure if it is the root cause.\nhttps:\/\/aws.amazon.com\/blogs\/machine-learning\/understanding-amazon-sagemaker-notebook-instance-networking-configurations-and-advanced-routing-options\/",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1557130614000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":260.0,
        "Answer_body":"Hi,\nThe principle here is that there much be network connectivity between the Notebook Instance and the DB Instance, and the security groups on the DB Instance should allow in-bound traffic from the Notebook Instance\n\nOne example of such as setup is\n\nRDS DB Instance is VPC vpc-a and Subnet subnet-b.\n\nSageMaker Notebook is launched in VPC vpc-a, Subnet subnet-b, with Security Group sg-c with DirectIntenetAccess \"Disabled\"\n\nIn the RDS DB Instance's Security Group rules, you can add an Inbound Rule to allow inbound traffic from the SageMaker Notebook security group \"sg-c\"\n-- Type - Protocol - Port Range - Source\n-- MYSQL\/Aurora - TCP - 3306 - sg-c\n\nSample Code:\n\n! pip install mysql-connector\r\n\r\nimport mysql.connector\r\nmydb = mysql.connector.connect(\r\nhost=\"$RDS_ENDPOINT\",\r\nuser=\"$RDS_USERNAME\",\r\npasswd=\"$RDS_PASSWORD\"\r\n)\r\ncursor = mydb.cursor()\r\ncursor.execute(\"SHOW DATABASES\") \n\n\nThanks for using Amazon SageMaker and let us know if there's anything else we can help with!\n\nEdited by: JaipreetS-AWS on May 6, 2019 3:04 PM",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUUUm8LxKZTzixOCI_IovK1A\/sage-maker-notebook-instance-in-vpc-failed-to-connect-to-local-database",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1557180251000,
        "Question_original_content":"notebook instanc vpc fail connect local databas set jupyt notebook vpc jdbc jar connect local databas show follow error messag com microsoft sqlserver jdbc sqlserverexcept tcp connect host port fail error connect time verifi connect properti sure instanc sql server run host accept tcp connect port sure tcp connect port block firewal exactli vpc subnet secur group glue job extract data local glue job work notebook fail sure firewal open tell solv came follow articl sure root caus http aw amazon com blog machin learn understand amazon notebook instanc network configur advanc rout option",
        "Question_preprocessed_content":"notebook instanc vpc fail connect local databas set jupyt notebook vpc jdbc jar connect local databas show follow error messag connect host port fail error connect time verifi connect properti sure instanc sql server run host accept connect port sure tcp connect port block exactli vpc subnet secur group glue job extract data local glue job work notebook fail sure firewal open tell solv came follow articl sure root caus",
        "Question_gpt_summary_original":"The user is facing challenges in setting up a Jupyter notebook in SageMaker within the VPC and using the jdbc jars to connect to the local database. The notebook is showing an error message related to TCP\/IP connection failure to the host and port. The user has used the same VPC, subnet, and security group as used in a glue job, which works fine. The user is unsure if the issue is related to the networking configurations and advanced routing options.",
        "Question_gpt_summary":"user face challeng set jupyt notebook vpc jdbc jar connect local databas notebook show error messag relat tcp connect failur host port user vpc subnet secur group glue job work fine user unsur issu relat network configur advanc rout option",
        "Answer_original_content":"principl network connect notebook instanc instanc secur group instanc allow bound traffic notebook instanc exampl setup rd instanc vpc vpc subnet subnet notebook launch vpc vpc subnet subnet secur group directintenetaccess disabl rd instanc secur group rule add inbound rule allow inbound traffic notebook secur group type protocol port rang sourc mysql aurora tcp sampl code pip instal mysql connector import mysql connector mydb mysql connector connect host rd endpoint user rd usernam passwd rd password cursor mydb cursor cursor execut databas thank let know help edit jaipreet aw",
        "Answer_preprocessed_content":"principl network connect notebook instanc instanc secur group instanc allow traffic notebook instanc exampl setup rd instanc vpc subnet notebook launch vpc subnet secur group directintenetaccess disabl rd instanc secur group rule add inbound rule allow inbound traffic notebook secur group type protocol port rang sourc tcp sampl code pip instal import mydb cursor databas thank let know help edit",
        "Answer_gpt_summary_original":"Solution: The user can check if there is network connectivity between the Notebook Instance and the DB Instance, and if the security groups on the DB Instance allow in-bound traffic from the Notebook Instance. One example of such a setup is to launch the SageMaker Notebook in the same VPC and subnet as the RDS DB Instance, with a security group that has DirectInternetAccess disabled. Then, in the RDS DB Instance's Security Group rules, add an Inbound Rule to allow inbound traffic from the SageMaker Notebook security group. The user can also use the provided sample code to connect to the database.",
        "Answer_gpt_summary":"solut user check network connect notebook instanc instanc secur group instanc allow bound traffic notebook instanc exampl setup launch notebook vpc subnet rd instanc secur group directinternetaccess disabl rd instanc secur group rule add inbound rule allow inbound traffic notebook secur group user us provid sampl code connect databas"
    },
    {
        "Question_title":"Moving resources across the regions",
        "Question_body":"Hi,\nWe have resource group(RG1) created in Germany region. Resource created are\n1) Vnet\n2) VPN gateway\n3) Disks - HDD\n4) VM (database installed)\n\nWe wanted to use Azure machine learning service to read data from database(present in resource group RG1 in German region). We later found that azure machine learning service is not available in Germany region and in order to create end points for Azure ML service both the networks(vnets of the DB and Azure ML ) should be in the same region.\n\nSo we tried migrating the resource and resource group (RG1) from German region to west Europe by using wizard on the portal. But we got prompted that disk , vnet and vpn gateway cannot be moved to different region ) .\n\nIs there anyway we could move them ? or any alternate solutions ? .Else, we would end up recreating every thing in westeurope which i would like to avoid.\n\nRegards,\nSuman",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1606983526427,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Maybe this is helpful:\n\nhttps:\/\/docs.microsoft.com\/en-us\/azure\/site-recovery\/azure-to-azure-tutorial-migrate\n\n(If the reply was helpful please don't forget to upvote and\/or accept as answer, thank you)\n\nRegards\nAndreas Baumgarten",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/184565\/moving-resources-across-the-regions.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1606984258243,
        "Question_original_content":"move resourc region resourc group creat germani region resourc creat vnet vpn gatewai disk hdd databas instal want us servic read data databas present resourc group german region later servic avail germani region order creat end point servic network vnet region tri migrat resourc resourc group german region west europ wizard portal got prompt disk vnet vpn gatewai move differ region altern solut end recreat thing westeurop like avoid regard suman",
        "Question_preprocessed_content":"move resourc region resourc group creat germani region resourc creat vnet vpn gatewai disk hdd want us servic read data databas later servic avail germani region order creat end point servic network region tri migrat resourc resourc group german region west europ wizard portal got prompt disk vnet vpn gatewai move differ region altern solut end recreat thing westeurop like avoid regard suman",
        "Question_gpt_summary_original":"The user is facing challenges in moving their resource group from the German region to West Europe in order to use Azure machine learning service to read data from a database. They discovered that Azure machine learning service is not available in the German region and in order to create endpoints for Azure ML service, both networks should be in the same region. However, they were unable to move the disk, Vnet, and VPN gateway to a different region using the wizard on the portal. The user is seeking alternative solutions to avoid recreating everything in West Europe.",
        "Question_gpt_summary":"user face challeng move resourc group german region west europ order us servic read data databas discov servic avail german region order creat endpoint servic network region unabl disk vnet vpn gatewai differ region wizard portal user seek altern solut avoid recreat west europ",
        "Answer_original_content":"mayb help http doc microsoft com azur site recoveri azur azur tutori migrat repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_preprocessed_content":"mayb help repli help forget upvot accept answer thank regard andrea baumgarten",
        "Answer_gpt_summary_original":"Solution: The discussion provides a link to a Microsoft tutorial on Azure-to-Azure migration using Azure Site Recovery. It is suggested as a possible solution to the challenge of moving the resource group from the German region to West Europe without having to recreate everything.",
        "Answer_gpt_summary":"solut discuss provid link microsoft tutori azur azur migrat azur site recoveri suggest possibl solut challeng move resourc group german region west europ have recreat"
    },
    {
        "Question_title":"Trouble connecting to AMLS web service on AKS using Python requests",
        "Question_body":"<p>I am having trouble contacting an AMLS web service hosted on AKS in a vnet. I am able to successfully provision AKS and deploy the models, but I am not able to access the web service using the Python requests module:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>headers = {'Content-Type':'application\/json',\n           'Authorization': 'Bearer ' + &lt;AKS_KEY&gt;}\nresp = requests.post(&lt;AKS_URI&gt;, json={&quot;data&quot;:{&quot;x&quot;: &quot;1&quot;}}, headers=headers)\nprint(resp.text)\n<\/code><\/pre>\n<p>I get the following error:<\/p>\n<blockquote>\n<p>Error: HTTPConnectionPool(host='', port=80): Max retries exceeded with url: &lt;AKS_URL&gt; (Caused by NewConnectionError('&lt;urllib3.connection.HTTPConnection object at 0x7f33f6035a10&gt;: Failed to establish a new connection: [Errno 110] Connection timed out'))<\/p>\n<\/blockquote>\n<p>However, I am able to successfully connect to the web service using Postman:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>curl --location --request POST &lt;AKS_URI&gt; \\\n--header 'Authorization: Bearer &lt;AKS_KEY&gt;' \\\n--header 'Content-Type: application\/json' \\\n--data-raw '{&quot;data&quot;: {&quot;x&quot;: &quot;1&quot;}}'\n<\/code><\/pre>\n<p>If I load the AKS service in my AMLS workspace <code>aks_service.run()<\/code> also gives me the same error message. I don't have these problems when I deploy without vnet integration.<\/p>\n<p>What could be causing this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_created_time":1600383486567,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1600397592800,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":"<p>I fixed this by adding an inbound security rule enabled for the scoring endpoint in the NSG group that controls the virtual network.<\/p>\n<p>This should be done so that the scoring endpoint can be called from outside the virtual network (see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-secure-inferencing-vnet\" rel=\"nofollow noreferrer\">documentation<\/a>), but apparently Postman can figure out how to access the endpoint without this security rule!<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63947132",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1600397953580,
        "Question_original_content":"troubl connect aml web servic ak python request have troubl contact aml web servic host ak vnet abl successfulli provis ak deploi model abl access web servic python request modul header content type applic json author bearer resp request post json data header header print resp text follow error error httpconnectionpool host port max retri exceed url caus newconnectionerror fail establish new connect errno connect time abl successfulli connect web servic postman curl locat request post header author bearer header content type applic json data raw data load ak servic aml workspac ak servic run give error messag problem deploi vnet integr caus",
        "Question_preprocessed_content":"troubl connect aml web servic ak python request have troubl contact aml web servic host ak vnet abl successfulli provis ak deploi model abl access web servic python request modul follow error error httpconnectionpool max retri exceed url abl successfulli connect web servic postman load ak servic aml workspac give error messag problem deploi vnet integr caus",
        "Question_gpt_summary_original":"The user is having trouble accessing an AMLS web service hosted on AKS in a vnet using the Python requests module. They are able to provision AKS and deploy the models successfully, but encounter a \"Connection timed out\" error when attempting to access the web service. However, they are able to connect to the web service using Postman. The same error message is received when loading the AKS service in the AMLS workspace. The user does not encounter these problems when deploying without vnet integration.",
        "Question_gpt_summary":"user have troubl access aml web servic host ak vnet python request modul abl provis ak deploi model successfulli encount connect time error attempt access web servic abl connect web servic postman error messag receiv load ak servic aml workspac user encount problem deploi vnet integr",
        "Answer_original_content":"fix ad inbound secur rule enabl score endpoint nsg group control virtual network score endpoint call outsid virtual network document appar postman figur access endpoint secur rule",
        "Answer_preprocessed_content":"fix ad inbound secur rule enabl score endpoint nsg group control virtual network score endpoint call outsid virtual network appar postman figur access endpoint secur rule",
        "Answer_gpt_summary_original":"Solution: The user fixed the issue by adding an inbound security rule enabled for the scoring endpoint in the NSG group that controls the virtual network. This should be done so that the scoring endpoint can be called from outside the virtual network, as mentioned in the documentation. However, it was noted that Postman was able to access the endpoint without this security rule.",
        "Answer_gpt_summary":"solut user fix issu ad inbound secur rule enabl score endpoint nsg group control virtual network score endpoint call outsid virtual network mention document note postman abl access endpoint secur rule"
    },
    {
        "Question_title":"Is it possible to log confidence intervals?",
        "Question_body":"<p>I log model scores by steps and at every step I have metric value, confidence interval lower bound, confidence interval upper bound. Is it possible to log confidence intervals (on one graph) and show the confidence interval using different color?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1673459967645,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":133.0,
        "Answer_body":"<p>Thank you so much for the example! This helps a whole lot. Currently this isn\u2019t a feature we have in our product, but I\u2019ll create a feature request for this and our team will reach out to you once there are any updates on this ticket.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/is-it-possible-to-log-confidence-intervals\/3684",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1674027872136,
        "Question_original_content":"possibl log confid interv log model score step step metric valu confid interv lower bound confid interv upper bound possibl log confid interv graph confid interv differ color",
        "Question_preprocessed_content":"possibl log confid interv log model score step step metric valu confid interv lower bound confid interv upper bound possibl log confid interv confid interv differ color",
        "Question_gpt_summary_original":"The user wants to log confidence intervals along with metric values while modeling. They are looking for a way to display the confidence intervals on a graph using different colors.",
        "Question_gpt_summary":"user want log confid interv metric valu model look wai displai confid interv graph differ color",
        "Answer_original_content":"thank exampl help lot current isnt featur product ill creat featur request team reach updat ticket",
        "Answer_preprocessed_content":"thank exampl help lot current isnt featur product ill creat featur request team reach updat ticket",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"Coordinate wandb local across two laptops",
        "Question_body":"<p>Hi Wandb Community!<\/p>\n<p>I have a laptop in my office that I am able to run <code>wandb local<\/code> on and sync my ML experiments to my account email address. My company gave me another laptop to work from on the road and I would like to set up <code>wandb local<\/code> on that laptop to streamline ML experiments I do in office and on the road.<\/p>\n<p>On my laptop, I have <code>wandb<\/code> and <code>docker<\/code> installed successfully. I can also run <code>wandb local<\/code> successfully. However, I\u2019m not sure if I need to copy the same api key and license over for the single account to work on both machines. Is there a smart way to do this?<\/p>\n<p>Thanks in advance!<\/p>\n<p>wand: 0.12.18<br>\nOS: Ubuntu 22.04<br>\ndocker: 20.10.17<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1655327268687,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":141.0,
        "Answer_body":"<p><a class=\"mention\" href=\"\/u\/aclifton314\">@aclifton314<\/a> ,<\/p>\n<p>Thank you for writing in with your question. Local is explicitly an \u201con-device\u201d service.If you want to share data across devices,  you would want to host you instance on a server to be able to reach it from anywhere, otherwise your two laptops wont share data. You can however still use the individual machine to sync data back\/forth to W&amp;B cloud,  pull experiments\/runs\/metrics\/ ect. to the individual machines. If this is your intended approach then you can copy the same API and Local License key to both machines.  <a href=\"https:\/\/docs.wandb.ai\/guides\/self-hosted\/local#login\">Here<\/a> is a quick reference on how to switch between a private instance and the wandb cloud when you need to sync the data. Please let us know if you have additional questions.<\/p>\n<p>Regards,<\/p>\n<p>Mohammad<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/coordinate-wandb-local-across-two-laptops\/2620",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1655499106272,
        "Question_original_content":"coordin local laptop commun laptop offic abl run local sync experi account email address compani gave laptop work road like set local laptop streamlin experi offic road laptop docker instal successfulli run local successfulli sure need copi api kei licens singl account work machin smart wai thank advanc wand ubuntu docker",
        "Question_preprocessed_content":"coordin local laptop commun laptop offic abl run sync experi account email address compani gave laptop work road like set laptop streamlin experi offic road laptop instal successfulli run successfulli sure need copi api kei licens singl account work machin smart wai thank advanc wand ubuntu docker",
        "Question_gpt_summary_original":"The user is facing a challenge of setting up wandb local on a second laptop provided by their company to streamline their ML experiments. They have successfully installed wandb and docker on their laptop and can run wandb local, but they are unsure if they need to copy the same API key and license over to the second laptop for the single account to work on both machines.",
        "Question_gpt_summary":"user face challeng set local second laptop provid compani streamlin experi successfulli instal docker laptop run local unsur need copi api kei licens second laptop singl account work machin",
        "Answer_original_content":"aclifton thank write question local explicitli devic servic want share data devic want host instanc server abl reach laptop wont share data us individu machin sync data forth cloud pull experi run metric ect individu machin intend approach copi api local licens kei machin quick refer switch privat instanc cloud need sync data let know addit question regard mohammad",
        "Answer_preprocessed_content":"thank write question local explicitli want share data devic want host instanc server abl reach laptop wont share data us individu machin sync data cloud pull ect individu machin intend approach copi api local licens kei machin quick refer switch privat instanc cloud need sync data let know addit question regard mohammad",
        "Answer_gpt_summary_original":"Solution: The user can copy the same API and Local License key to both laptops if they intend to use each machine to sync data back and forth to W&B cloud. However, if they want to share data across devices, they would need to host their instance on a server to be able to reach it from anywhere. The user can refer to the provided link for a quick reference on how to switch between a private instance and the wandb cloud when they need to sync the data.",
        "Answer_gpt_summary":"solut user copi api local licens kei laptop intend us machin sync data forth cloud want share data devic need host instanc server abl reach user refer provid link quick refer switch privat instanc cloud need sync data"
    },
    {
        "Question_title":"Why does the Optuna CSV file only display 1 item per parameter when I have multiple?",
        "Question_body":"<p>I have created the following piece of code in Python in order to optimize my network using Optuna.<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n\nmodel = Sequential([\n            layers.Conv2D(filters=dict_params['num_filters_1'],\n                          kernel_size=dict_params['kernel_size_1'],\n                          activation=dict_params['activations_1'],\n                          strides=dict_params['stride_num_1'],\n                          input_shape=self.input_shape),\n            layers.BatchNormalization(),\n            layers.MaxPooling2D(2, 2),\n\n            layers.Conv2D(filters=dict_params['num_filters_2'],\n                          kernel_size=dict_params['kernel_size_2'],\n                          activation=dict_params['activations_2'],\n                          strides=dict_params['stride_num_2']),\n<\/code><\/pre>\n<p>As you can see, I made multiple activation trials instead of one because I wanted to see if the model produced better results when each layer had a different activation function. I did the same with other parameters as you can see. My confusion begins when I return the study.bestparams object:<\/p>\n<pre><code>{&quot;num_filters&quot;: 32, &quot;kernel_size&quot;: 4, &quot;strides&quot;: 1, &quot;activation&quot;: &quot;selu&quot;, &quot;num_dense_nodes&quot;: 64, &quot;batch_size&quot;: 64}\n<\/code><\/pre>\n<p>The best parameters from the trials produced only one parameter. It does not tell me where the parameter was used and also doesn't display the other 3 activation functions I used (or the other parameters for that matter). Is there a way to precisely display the best settings my model used and at which layers? (I am aware of saving the best model and model summary but this does not help me too much)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1630949196040,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1631055883776,
        "Question_score":0.0,
        "Question_view_count":82.0,
        "Answer_body":"<p>The problem  is you used the same parameter name  for all activations. Instead of :<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>\n<p>Try:<\/p>\n<pre><code>    activations_1 = trial.suggest_categorical('activation1', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_2 = trial.suggest_categorical('activation2', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_3 = trial.suggest_categorical('activation3', ['relu', 'sigmoid', 'tanh', 'selu']) \n    activations_4 = trial.suggest_categorical('activation4', ['relu', 'sigmoid', 'tanh', 'selu'])\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69078338",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Question_closed_time":1631093676416,
        "Question_original_content":"csv file displai item paramet multipl creat follow piec code python order optim network activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu model sequenti layer convd filter dict param num filter kernel size dict param kernel size activ dict param activ stride dict param stride num input shape self input shape layer batchnorm layer maxpoolingd layer convd filter dict param num filter kernel size dict param kernel size activ dict param activ stride dict param stride num multipl activ trial instead want model produc better result layer differ activ function paramet confus begin return studi bestparam object num filter kernel size stride activ selu num dens node batch size best paramet trial produc paramet tell paramet displai activ function paramet matter wai precis displai best set model layer awar save best model model summari help",
        "Question_preprocessed_content":"csv file displai item paramet multipl creat follow piec code python order optim network multipl activ trial instead want model produc better result layer differ activ function paramet confus begin return object best paramet trial produc paramet tell paramet displai activ function wai precis displai best set model layer",
        "Question_gpt_summary_original":"The user encountered a challenge with the Optuna CSV file as it only displayed one item per parameter despite having multiple activation trials. The best parameters from the trials produced only one parameter, which did not tell the user where the parameter was used and also did not display the other activation functions used or the other parameters. The user is seeking a way to precisely display the best settings the model used and at which layers.",
        "Question_gpt_summary":"user encount challeng csv file displai item paramet despit have multipl activ trial best paramet trial produc paramet tell user paramet displai activ function paramet user seek wai precis displai best set model layer",
        "Answer_original_content":"problem paramet activ instead activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu try activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu activ trial suggest categor activ relu sigmoid tanh selu",
        "Answer_preprocessed_content":"problem paramet activ instead try",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to use different parameter names for each activation function instead of using the same parameter name for all activations. By doing so, the user can precisely display the best settings the model used and at which layers.",
        "Answer_gpt_summary":"solut solut suggest discuss us differ paramet name activ function instead paramet activ user precis displai best set model layer"
    },
    {
        "Question_title":"Run !docker build from Managed Notebook cell in GCP Vertex AI Workbench",
        "Question_body":"<p>I am trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook.<\/p>\n<p>After having prepared the correct Dockerfile and the URI where to push the image that contains my train.py script, I try to push the image directly in a notebook cell.<\/p>\n<p>The exact command I try to execute is: <code>!docker build .\/ -t $IMAGE_URI<\/code>, where IMAGE_URI is the environmental variable previously defined. However I try to run this command I get the error: <code>\/bin\/bash: docker: command not found<\/code>. I also tried to execute it with the magic cell %%bash, importing the subprocess library and also execute the command stored in a .sh file.<\/p>\n<p>Unfortunately none of the above solutions work, they all return the same <strong>command not found<\/strong> error with code 127.<\/p>\n<p>If instead I run the command from a bash present in the Jupyterlab it works fine as expected.<\/p>\n<p>Is there any workaround to make the push execute inside the jupyter notebook? I was trying to keep the whole custom training process inside the same notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1651142988007,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1651151719750,
        "Question_score":2.0,
        "Question_view_count":248.0,
        "Answer_body":"<p>If you follow this <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/workbench\/user-managed\/create-user-managed-notebooks-instance-console-quickstart\" rel=\"nofollow noreferrer\">guide<\/a> to create a user-managed notebook from Vertex AI workbench and select Python 3, then it comes with Docker available.<\/p>\n<p>So you will be able to use Docker commands such as <code>! docker build .<\/code> inside the user-managed notebook.<\/p>\n<p>Example:\n<a href=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DtlQp.png\" alt=\"Vertex AI Managed Notebook\" \/><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1661351120347,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72042363",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1661350818332,
        "Question_original_content":"run docker build manag notebook cell gcp workbench try push docker imag googl cloud platform contain registri defin custom train job directli insid notebook have prepar correct dockerfil uri push imag contain train script try push imag directli notebook cell exact command try execut docker build imag uri imag uri environment variabl previous defin try run command error bin bash docker command tri execut magic cell bash import subprocess librari execut command store file unfortun solut work return command error code instead run command bash present jupyterlab work fine expect workaround push execut insid jupyt notebook try custom train process insid notebook",
        "Question_preprocessed_content":"run docker build manag notebook cell gcp workbench try push docker imag googl cloud platform contain registri defin custom train job directli insid notebook have prepar correct dockerfil uri push imag contain script try push imag directli notebook cell exact command try execut environment variabl previous defin try run command error tri execut magic cell bash import subprocess librari execut command store file unfortun solut work return command error code instead run command bash present jupyterlab work fine expect workaround push execut insid jupyt notebook try custom train process insid notebook",
        "Question_gpt_summary_original":"The user is facing challenges while trying to push a docker image on Google Cloud Platform container registry to define a custom training job directly inside a notebook. The user is encountering a \"command not found\" error with code 127 while trying to execute the command \"!docker build .\/ -t $IMAGE_URI\" in a notebook cell. The user has tried various solutions, including using the magic cell %%bash and importing the subprocess library, but none of them have worked. The command works fine when executed from a bash present in the Jupyterlab. The user is looking for a workaround to execute the push inside the jupyter notebook to keep the whole custom training process inside the same notebook.",
        "Question_gpt_summary":"user face challeng try push docker imag googl cloud platform contain registri defin custom train job directli insid notebook user encount command error code try execut command docker build imag uri notebook cell user tri solut includ magic cell bash import subprocess librari work command work fine execut bash present jupyterlab user look workaround execut push insid jupyt notebook custom train process insid notebook",
        "Answer_original_content":"follow guid creat user manag notebook workbench select python come docker avail abl us docker command docker build insid user manag notebook exampl",
        "Answer_preprocessed_content":"follow guid creat notebook workbench select python come docker avail abl us docker command insid notebook exampl",
        "Answer_gpt_summary_original":"Solution: One possible solution is to create a user-managed notebook from Vertex AI workbench and select Python 3, which comes with Docker available. This will allow the user to use Docker commands such as \"!docker build .\" inside the user-managed notebook.",
        "Answer_gpt_summary":"solut possibl solut creat user manag notebook workbench select python come docker avail allow user us docker command docker build insid user manag notebook"
    },
    {
        "Question_title":"Creating a model for use in a pipeline from a hyperparameter tuning job",
        "Question_body":"<p>I'm trying to implement the best estimator from a hyperparameter tuning job into a pipeline object to deploy an endpoint.<\/p>\n\n<p>I've read the docs in a best effort to include the results from the tuning job in the pipeline, but I'm having trouble creating the Model() class object.<\/p>\n\n<pre><code># This is the hyperparameter tuning job\ntuner.fit({'train': s3_train, 'validation': s3_val}, \ninclude_cls_metadata=False)\n\n\n#With a standard Model (Not from the tuner) the process was as follows:\nscikit_learn_inferencee_model_name = sklearn_preprocessor.create_model()\nxgb_model_name = Model(model_data=xgb_model.model_data, image=xgb_image)\n\n\nmodel_name = 'xgb-inference-pipeline-' + timestamp_prefix\nendpoint_name = 'xgb-inference-pipeline-ep-' + timestamp_prefix\nsm_model = PipelineModel(\n    name=model_name, \n    role=role, \n    models=[\n        scikit_learn_inferencee_model_name, \n        xgb_model_name])\n\nsm_model.deploy(initial_instance_count=1, instance_type='ml.c4.xlarge', \nendpoint_name=endpoint_name)\n<\/code><\/pre>\n\n<p>I would like to be able to cleanly instantiate a model object using my results from the tuning job and pass it into the PipelineModel object. Any guidance is appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1558813444520,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":455.0,
        "Answer_body":"<p>I think you are on the right track. Do you get any error? Refer this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/e9c295c8538d29cc9fea2f73a29649126628064a\/advanced_functionality\/inference_pipeline_sparkml_xgboost_abalone\/inference_pipeline_sparkml_xgboost_abalone.ipynb\" rel=\"nofollow noreferrer\">notebook<\/a>  for instantiating the model from the tuner and use in inference pipeline.<\/p>\n\n<p>Editing previous response based on the comment. To create model from the best training job of the hyperparameter tuning job, you can use below snippet<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from sagemaker.tuner import HyperparameterTuner\nfrom sagemaker.estimator import Estimator\nfrom sagemaker.model import Model\n\n# Attach to an existing hyperparameter tuning job.\nxgb_tuning_job_name = 'my_xgb_hpo_tuning_job_name'\nxgb_tuner = HyperparameterTuner.attach(xgb_tuning_job_name)\n\n# Get the best XGBoost training job name from the HPO job\nxgb_best_training_job = xgb_tuner.best_training_job()\nprint(xgb_best_training_job)\n\n# Attach estimator to the best training job name\nxgb_best_estimator = Estimator.attach(xgb_best_training_job)\n\n# Create model to be passed to the inference pipeline\nxgb_model = Model(model_data=xgb_best_estimator.model_data,\n                  role=sagemaker.get_execution_role(),\n                  image=xgb_best_estimator.image_name)\n<\/code><\/pre>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":1559176084156,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56308169",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1558879357103,
        "Question_original_content":"creat model us pipelin hyperparamet tune job try implement best estim hyperparamet tune job pipelin object deploi endpoint read doc best effort includ result tune job pipelin have troubl creat model class object hyperparamet tune job tuner fit train train valid val includ cl metadata fals standard model tuner process follow scikit learn inference model sklearn preprocessor creat model xgb model model model data xgb model model data imag xgb imag model xgb infer pipelin timestamp prefix endpoint xgb infer pipelin timestamp prefix model pipelinemodel model role role model scikit learn inference model xgb model model deploi initi instanc count instanc type xlarg endpoint endpoint like abl cleanli instanti model object result tune job pass pipelinemodel object guidanc appreci",
        "Question_preprocessed_content":"creat model us pipelin hyperparamet tune job try implement best estim hyperparamet tune job pipelin object deploi endpoint read doc best effort includ result tune job pipelin have troubl creat model class object like abl cleanli instanti model object result tune job pass pipelinemodel object guidanc appreci",
        "Question_gpt_summary_original":"The user is facing challenges in creating a Model() class object to deploy an endpoint using the best estimator from a hyperparameter tuning job. They have read the documentation but are having trouble including the results from the tuning job in the pipeline. The user is seeking guidance on how to instantiate a model object using the tuning job results and pass it into the PipelineModel object.",
        "Question_gpt_summary":"user face challeng creat model class object deploi endpoint best estim hyperparamet tune job read document have troubl includ result tune job pipelin user seek guidanc instanti model object tune job result pass pipelinemodel object",
        "Answer_original_content":"think right track error refer notebook instanti model tuner us infer pipelin edit previou respons base comment creat model best train job hyperparamet tune job us snippet tuner import hyperparametertun estim import estim model import model attach exist hyperparamet tune job xgb tune job xgb hpo tune job xgb tuner hyperparametertun attach xgb tune job best xgboost train job hpo job xgb best train job xgb tuner best train job print xgb best train job attach estim best train job xgb best estim estim attach xgb best train job creat model pass infer pipelin xgb model model model data xgb best estim model data role execut role imag xgb best estim imag",
        "Answer_preprocessed_content":"think right track error refer notebook instanti model tuner us infer pipelin edit previou respons base comment creat model best train job hyperparamet tune job us snippet",
        "Answer_gpt_summary_original":"Solution: The discussion provides a code snippet to create a model object using the best training job from a hyperparameter tuning job. The code uses the `HyperparameterTuner` and `Estimator` classes to attach to the best training job and create an estimator object. Then, the `Model` class is used to create a model object that can be passed into the inference pipeline.",
        "Answer_gpt_summary":"solut discuss provid code snippet creat model object best train job hyperparamet tune job code us hyperparametertun estim class attach best train job creat estim object model class creat model object pass infer pipelin"
    },
    {
        "Question_title":"How to prevent a NoCredentialsError when calling the fit method in SageMaker?",
        "Question_body":"<p>I am a newbie when it comes to Python SageMaker (my background is C#). Currently, I have a problem because the last method call (I mean the fit method) results in a \"NoCredentialsError\". I do not understand that. The AWS credentials have been set and I do use them to communicate with AWS, for example to communicate with S3. How can I prevent this error? <\/p>\n\n<pre><code>import io\nimport os\nimport gzip\nimport pickle\nimport urllib.request\nimport boto3\nimport sagemaker\nimport sagemaker.amazon.common as smac\n\nDOWNLOADED_FILENAME = 'C:\/Users\/Daan\/PycharmProjects\/downloads\/mnist.pkl.gz'\nif not os.path.exists(DOWNLOADED_FILENAME):\n    urllib.request.urlretrieve(\"http:\/\/deeplearning.net\/data\/mnist\/mnist.pkl.gz\", DOWNLOADED_FILENAME)\n\nwith gzip.open(DOWNLOADED_FILENAME, 'rb') as f:\n    train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\nvectors = train_set[0].T\nbuf = io.BytesIO()\nsmac.write_numpy_to_dense_tensor(buf, vectors)\nbuf.seek(0)\nkey = 'recordio-pb-data'\nbucket_name = 'SOMEKINDOFBUCKETNAME'\nprefix = 'sagemaker\/pca'\npath = os.path.join(prefix, 'train', key)\nprint(path)\n\nsession = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\nregion='eu-west-1'\nsagemakerSession= sagemaker.Session(sagemaker_client=client,boto_session=session)\ns3_resource=session.resource('s3')\nbucket = s3_resource.Bucket(bucket_name)\ncurrent_bucket = bucket.Object(path)\n\ntrain_data = 's3:\/\/{}\/{}\/train\/{}'.format(bucket_name, prefix, key)\nprint('uploading training data location: {}'.format(train_data))\ncurrent_bucket.upload_fileobj(buf)\n\noutput_location = 's3:\/\/{}\/{}\/output'.format('SOMEBUCKETNAME', prefix)\nprint('training artifacts will be uploaded to: {}'.format(output_location))\n\nregion='eu-west-1'\n\ncontainers = {'us-west-2': 'SOMELOCATION',\n              'us-east-1': 'SOMELOCATION',\n              'us-east-2': 'SOMELOCATION',\n              'eu-west-1': 'SOMELOCATION'}\ncontainer = containers[region]\n\nrole='AmazonSageMaker-ExecutionRole-SOMEVALUE'\npca = sagemaker.estimator.Estimator(container,\n                                    role,\n                                    train_instance_count=1,\n                                    train_instance_type='ml.c4.xlarge',\n                                    output_path=output_location,\n                                    sagemaker_session=sagemakerSession)\n\n\npca.set_hyperparameters(feature_dim=50000,\n                        num_components=10,\n                        subtract_mean=True,\n                        algorithm_mode='randomized',\n                        mini_batch_size=200)\n\npca.fit(inputs=train_data)\n\nprint('END')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1526393414957,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":447.0,
        "Answer_body":"<p>I am not sure if you have masked the actual access id and key or this is what you are running.<\/p>\n<pre><code>session = boto3.session.Session(aws_access_key_id='SECRET',aws_secret_access_key='SECRET',region_name='eu-west-1')\nclient = boto3.client('sagemaker',region_name='eu-west-1',aws_access_key_id='SECRET',aws_secret_access_key='SECRET')\n<\/code><\/pre>\n<p>I am hoping you are providing the actual aws_access_key_id and aws_secret_access_key in the above lines of code.<\/p>\n<p>Another way of specifying the same and not hardcoding in the code is to create a credentials file in your profile directory i.e.<\/p>\n<p>in Mac    ~\/.aws\/<\/p>\n<p>and in Windows <code>&quot;%UserProfile%\\.aws&quot;<\/code><\/p>\n<p>the file is a plain text file with a name &quot;credentials&quot; (without the quotes).\nfile contains<\/p>\n<pre><code>[default]\naws_access_key_id=XXXXXXXXXXXXXX\naws_secret_access_key=YYYYYYYYYYYYYYYYYYYYYYYYYYY\n<\/code><\/pre>\n<p>AWS CLI would pick it up from the above location and use it. You can also use non-default profiles and pass on the profile with<\/p>\n<pre><code>os.environ[&quot;AWS_PROFILE&quot;] = &quot;profile-name&quot;\n<\/code><\/pre>\n<p>Hope this helps.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1636970707960,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50352412",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1526405500043,
        "Question_original_content":"prevent nocredentialserror call fit method newbi come python background current problem method mean fit method result nocredentialserror understand aw credenti set us commun aw exampl commun prevent error import import import gzip import pickl import urllib request import boto import import amazon common smac download filenam user daan pycharmproject download mnist pkl path exist download filenam urllib request urlretriev http deeplearn net data mnist mnist pkl download filenam gzip open download filenam train set valid set test set pickl load encod latin vector train set buf bytesio smac write numpi dens tensor buf vector buf seek kei recordio data bucket somekindofbucketnam prefix pca path path join prefix train kei print path session boto session session aw access kei secret aw secret access kei secret region west client boto client region west aw access kei secret aw secret access kei secret region west session session client client boto session session resourc session resourc bucket resourc bucket bucket current bucket bucket object path train data train format bucket prefix kei print upload train data locat format train data current bucket upload fileobj buf output locat output format somebucketnam prefix print train artifact upload format output locat region west contain west someloc east someloc east someloc west someloc contain contain region role amazon executionrol somevalu pca estim estim contain role train instanc count train instanc type xlarg output path output locat session session pca set hyperparamet featur dim num compon subtract mean true algorithm mode random mini batch size pca fit input train data print end",
        "Question_preprocessed_content":"prevent nocredentialserror call fit method newbi come python current problem method result nocredentialserror understand aw credenti set us commun aw exampl commun prevent error",
        "Question_gpt_summary_original":"The user is encountering a \"NoCredentialsError\" when calling the fit method in SageMaker, despite having set the AWS credentials and using them to communicate with AWS. The user is seeking advice on how to prevent this error.",
        "Question_gpt_summary":"user encount nocredentialserror call fit method despit have set aw credenti commun aw user seek advic prevent error",
        "Answer_original_content":"sure mask actual access kei run session boto session session aw access kei secret aw secret access kei secret region west client boto client region west aw access kei secret aw secret access kei secret hope provid actual aw access kei aw secret access kei line code wai specifi hardcod code creat credenti file profil directori mac aw window userprofil aw file plain text file credenti quot file contain default aw access kei aw secret access kei aw cli pick locat us us non default profil pass profil environ aw profil profil hope help",
        "Answer_preprocessed_content":"sure mask actual access kei run hope provid actual line code wai specifi hardcod code creat credenti file profil directori mac window file plain text file credenti file contain aw cli pick locat us us profil pass profil hope help",
        "Answer_gpt_summary_original":"Solution:\n- The user can create a credentials file in their profile directory and specify the AWS access key and secret access key in it. The AWS CLI would pick it up from the specified location and use it. The file is a plain text file with a name \"credentials\" (without the quotes) and contains the access key and secret access key. The user can also use non-default profiles and pass on the profile with \"os.environ[\"AWS_PROFILE\"] = \"profile-name\"\".",
        "Answer_gpt_summary":"solut user creat credenti file profil directori specifi aw access kei secret access kei aw cli pick specifi locat us file plain text file credenti quot contain access kei secret access kei user us non default profil pass profil environ aw profil profil"
    },
    {
        "Question_title":"How long does it take for AWS tech support team to respond to a \"system impaired\" issue?",
        "Question_body":"Hi all,\n\nI have raised a ticket for multiple issues we've been having with SageMaker lately, the ticket was created more than 36 hours ago, and I have not had any response, in fact the ticket hasn't even been assigned yet.\n\nThe case ID is 10300240931.\n\nI thought AWS guarantee a response under 12 hours for \"system impaired\" issues, does anyone know what I can do to accelerate this?\n\nthank you! Ruoy",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1656580942341,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":"Hi Ruoy! My advice here is to scale this issue via your account team, they will have the mechanisms to scale this concern. If you are on basic or developer support, you could look into upgrading to business support for a day and open a live chat with support! Hope this helps",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUFgnjt9J3T0iXhE0axG10vQ\/how-long-does-it-take-for-aws-tech-support-team-to-respond-to-a-system-impaired-issue",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1656585271688,
        "Question_original_content":"long aw tech support team respond impair issu rais ticket multipl issu have late ticket creat hour ago respons fact ticket hasn assign case thought aw guarante respons hour impair issu know acceler thank ruoi",
        "Question_preprocessed_content":"long aw tech support team respond impair issu rais ticket multipl issu have late ticket creat hour ago respons fact ticket hasn assign case thought aw guarante respons hour impair issu know acceler thank ruoi",
        "Question_gpt_summary_original":"The user has raised a ticket for multiple issues with SageMaker on AWS, including a \"system impaired\" issue. The ticket was created more than 36 hours ago, but the user has not received any response yet, and the ticket has not been assigned. The user is concerned about the delay and is seeking advice on how to accelerate the response time.",
        "Question_gpt_summary":"user rais ticket multipl issu aw includ impair issu ticket creat hour ago user receiv respons ticket assign user concern delai seek advic acceler respons time",
        "Answer_original_content":"ruoi advic scale issu account team mechan scale concern basic develop support look upgrad busi support dai open live chat support hope help",
        "Answer_preprocessed_content":"ruoi advic scale issu account team mechan scale concern basic develop support look upgrad busi support dai open live chat support hope help",
        "Answer_gpt_summary_original":"Solutions provided:\n- Scale the issue via the account team\n- Upgrade to business support for a day and open a live chat with support\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid scale issu account team upgrad busi support dai open live chat support person opinion bias includ respons"
    },
    {
        "Question_title":"Can't change virtual environment within Azure ML notebook",
        "Question_body":"<p>For some reason the jupyter notebooks on my VM are in the wrong environment (ie stuck in <code>(base)<\/code>). Furthermore, I can change the environment in the terminal but not in the notebook. Here is what happens when I attempt <code>!conda activate desired_env<\/code> in the notebook:<\/p>\n<pre class=\"lang-sh prettyprint-override\"><code>CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\nTo initialize your shell, run\n\n    $ conda init &lt;SHELL_NAME&gt;\n\nCurrently supported shells are:\n  - bash\n  - fish\n  - tcsh\n  - xonsh\n  - zsh\n  - powershell\n\nSee 'conda init --help' for more information and options.\n\nIMPORTANT: You may need to close and restart your shell after running 'conda init'.\n\n\n# conda environments:\n#\nbase                  *  \/anaconda\nazureml_py36             \/anaconda\/envs\/azureml_py36\nazureml_py38             \/anaconda\/envs\/azureml_py38\nazureml_py38_pytorch     \/anaconda\/envs\/azureml_py38_pytorch\nazureml_py38_tensorflow     \/anaconda\/envs\/azureml_py38_tensorflow\n<\/code><\/pre>\n<p>I tried the answers <a href=\"https:\/\/stackoverflow.com\/questions\/61915607\/commandnotfounderror-your-shell-has-not-been-properly-configured-to-use-conda\">here<\/a> (e.g., first running <code>!source \/anaconda\/etc\/profile.d\/conda.sh<\/code>).<\/p>\n<p>I also tried activating the environment using <code>source<\/code> rather than 'conda activate': <code>!source \/anaconda\/envs\/desired_env\/bin\/activate<\/code>. This runs but doesn't actually do anything when I see the current environment in <code>conda env list<\/code><\/p>\n<p>Edit: also adding that if I install a package in the <code>(base)<\/code> environment in the terminal, I still don't have access to it in jupyter notebook.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1636646694700,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1636647028003,
        "Question_score":1.0,
        "Question_view_count":672.0,
        "Answer_body":"<p>I'm the PM that released AzureML Notebooks, you can't activate a Conda env from a cell, you have to create a new kernel will the Conda Env. Here are the instructions: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-access-terminal#add-new-kernels<\/a><\/p>",
        "Answer_comment_count":8.0,
        "Answer_last_edit_time":null,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69931411",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1636652640790,
        "Question_original_content":"chang virtual environ notebook reason jupyt notebook wrong environ stuck base furthermor chang environ termin notebook happen attempt conda activ desir env notebook commandnotfounderror shell properli configur us conda activ initi shell run conda init current support shell bash fish tcsh xonsh zsh powershel conda init help inform option import need close restart shell run conda init conda environ base anaconda anaconda env anaconda env pytorch anaconda env pytorch tensorflow anaconda env tensorflow tri answer run sourc anaconda profil conda tri activ environ sourc conda activ sourc anaconda env desir env bin activ run actual current environ conda env list edit ad instal packag base environ termin access jupyt notebook",
        "Question_preprocessed_content":"chang virtual environ notebook reason jupyt notebook wrong environ furthermor chang environ termin notebook happen attempt notebook tri answer tri activ environ conda activ run actual current environ edit ad instal packag environ termin access jupyt notebook",
        "Question_gpt_summary_original":"The user is facing challenges in changing the virtual environment within Azure ML notebook. The user is unable to change the environment in the notebook and is receiving an error message when attempting to do so. The user has tried various solutions, including activating the environment using 'source' and installing packages in the base environment, but none have been successful.",
        "Question_gpt_summary":"user face challeng chang virtual environ notebook user unabl chang environ notebook receiv error messag attempt user tri solut includ activ environ sourc instal packag base environ success",
        "Answer_original_content":"releas notebook activ conda env cell creat new kernel conda env instruct http doc microsoft com azur machin learn access termin add new kernel",
        "Answer_preprocessed_content":"releas notebook activ conda env cell creat new kernel conda env instruct",
        "Answer_gpt_summary_original":"Solution: The PM who released AzureML Notebooks suggested creating a new kernel with the Conda environment instead of trying to activate it from a cell. The instructions for adding new kernels can be found in the provided link.",
        "Answer_gpt_summary":"solut releas notebook suggest creat new kernel conda environ instead try activ cell instruct ad new kernel provid link"
    },
    {
        "Question_title":"Kedro using wrong conda environment",
        "Question_body":"<p>I have created a conda environment called <code>Foo<\/code>. After activating this environment I installed Kedro with <code>pip<\/code>, since <code>conda<\/code> was giving me a conflict. Even though I'm inside the <code>Foo<\/code> environment, when I run:<\/p>\n<pre><code>kedro jupyter lab\n<\/code><\/pre>\n<p>It picks up the modules from my <code>base<\/code> environment, not the <code>Foo<\/code> environment. Any idea, why this is happening, and how I can change what modules my notebook detect?<\/p>\n<p><strong>Edit<\/strong><\/p>\n<p>By mangling with my code I found out that on the <code>\\AppData\\Roaming\\jupyter\\kernels\\kedro_project\\kernel.json<\/code> it was calling the python from the base environment, not the <code>Foo<\/code> environment. I changed it manually, but is there a mode automatic way of setting the <code>\\AppData\\Roaming\\jupyter\\kernels\\kedro_project\\kernel.json<\/code> to use the current environment I'm on?<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":1,
        "Question_created_time":1652795556263,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1652825440368,
        "Question_score":0.0,
        "Question_view_count":115.0,
        "Answer_body":"<p>The custom Kedro kernel spec is a feature that I recently added to Kedro. When you run <code>kedro jupyter lab\/notebook<\/code> it should automatically pick up on the conda environment without you needing to manually edit the kernel.json file. I tested this myself to check that it worked so I'm very interested in understanding what's going on here!<\/p>\n<p>The function <a href=\"https:\/\/github.com\/kedro-org\/kedro\/blob\/58c57c384f5257b998edebb99d94bff46574ae1e\/kedro\/framework\/cli\/jupyter.py#L99\" rel=\"nofollow noreferrer\"><code>_create_kernel<\/code><\/a> is what makes the the Kedro kernel spec. The docstring for that explains what's going on, but in short we delegate to <a href=\"https:\/\/github.com\/ipython\/ipykernel\/blob\/d02f4371348187c3e5e87a46388bbef92615c110\/ipykernel\/kernelspec.py#L92\" rel=\"nofollow noreferrer\"><code>ipykernel.kernelspec.install<\/code><\/a>. This generates a kernelspec that points towards the Python path given by <code>sys.executable<\/code> (see <a href=\"https:\/\/github.com\/ipython\/ipykernel\/blob\/d02f4371348187c3e5e87a46388bbef92615c110\/ipykernel\/kernelspec.py#L27\" rel=\"nofollow noreferrer\"><code>make_ipkernel_cmd<\/code><\/a>). In theory this should already point towards the correct Python path, which takes account of the conda environment.<\/p>\n<p>It's worth checking <code>which kedro<\/code> to see which conda environment that points to, and if we need to debug further then please do raise an issue on our <a href=\"https:\/\/github.com\/kedro-org\/kedro\" rel=\"nofollow noreferrer\">Github repo<\/a>. I'd definitely like to get to the bottom of this and understand where the problem is.<\/p>\n<p>P.S. you can also do a plain <code>jupyter lab\/notebook<\/code> to launch a kernel with the right conda environment and then run <code>%load_ext kedro.extras.extensions.ipython<\/code> in the first cell. This is basically equivalent to using the Kedro kernelspec, which loads the Kedro IPython extension automatically.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72275283",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1652879216830,
        "Question_original_content":"wrong conda environ creat conda environ call foo activ environ instal pip conda give conflict insid foo environ run jupyt lab pick modul base environ foo environ idea happen chang modul notebook detect edit mangl code appdata roam jupyt kernel project kernel json call python base environ foo environ chang manual mode automat wai set appdata roam jupyt kernel project kernel json us current environ",
        "Question_preprocessed_content":"wrong conda environ creat conda environ call activ environ instal give conflict insid environ run pick modul environ environ idea happen chang modul notebook detect edit mangl code call python base environ environ chang manual mode automat wai set us current environ",
        "Question_gpt_summary_original":"The user created a conda environment called \"Foo\" and installed Kedro with pip while inside that environment. However, when running \"kedro jupyter lab\", the modules from the \"base\" environment are picked up instead of \"Foo\". The user discovered that the kernel.json file was calling the python from the base environment instead of \"Foo\" and manually changed it. The user is seeking a more automatic way of setting the kernel.json file to use the current environment.",
        "Question_gpt_summary":"user creat conda environ call foo instal pip insid environ run jupyt lab modul base environ pick instead foo user discov kernel json file call python base environ instead foo manual chang user seek automat wai set kernel json file us current environ",
        "Answer_original_content":"custom kernel spec featur recent ad run jupyt lab notebook automat pick conda environ need manual edit kernel json file test check work interest understand go function creat kernel make kernel spec docstr explain go short deleg ipykernel kernelspec instal gener kernelspec point python path given sy execut ipkernel cmd theori point correct python path take account conda environ worth check conda environ point need debug rais issu github repo definit like understand problem plain jupyt lab notebook launch kernel right conda environ run load ext extra extens ipython cell basic equival kernelspec load ipython extens automat",
        "Answer_preprocessed_content":"custom kernel spec featur recent ad run automat pick conda environ need manual edit file test check work interest understand go function make kernel spec docstr explain go short deleg gener kernelspec point python path given theori point correct python path take account conda environ worth check conda environ point need debug rais issu github repo definit like understand problem plain launch kernel right conda environ run cell basic equival kernelspec load ipython extens automat",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion:\n- The custom Kedro kernel spec should automatically pick up on the conda environment without the need to manually edit the kernel.json file.\n- The function \"_create_kernel\" is what makes the Kedro kernel spec and delegates to \"ipykernel.kernelspec.install\" which generates a kernelspec that points towards the Python path given by \"sys.executable\". In theory, this should already point towards the correct Python path, which takes account of the conda environment.\n- Check \"which kedro\" to see which conda environment that points to.\n- If further debugging is needed, raise an issue on the Kedro Github repo.\n- An alternative solution is to do a plain \"jupyter lab",
        "Answer_gpt_summary":"possibl solut mention discuss custom kernel spec automat pick conda environ need manual edit kernel json file function creat kernel make kernel spec deleg ipykernel kernelspec instal gener kernelspec point python path given sy execut theori point correct python path take account conda environ check conda environ point debug need rais issu github repo altern solut plain jupyt lab"
    },
    {
        "Question_title":"Can H2o AutoML benefit from a GPU instance on Sagemaker platform?",
        "Question_body":"<p>I'm running some projects with H2o AutoML using Sagemaker notebook instances, and I would like to know if H2o AutoML can benefit from a GPU Sagemaker instance, if so, how should I configure the notebook? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":4,
        "Question_created_time":1567711533230,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":462.0,
        "Answer_body":"<p><a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html\" rel=\"nofollow noreferrer\">H2O AutoML<\/a> contains a handful of algorithms and one of them is <a href=\"http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science\/xgboost.html\" rel=\"nofollow noreferrer\">XGBoost<\/a>, which has been part of H2O AutoML since H2O version 3.22.0.1.  XGBoost is the only GPU-capable algorithm inside of H2O AutoML, however, a lot of the models that are trained in AutoML are XGBoost models, so it still can be useful to utilize a GPU. Keep in mind that you must use H2O 3.22 or above to use this feature.<\/p>\n\n<p>My suggestion is to test it on a GPU-enabled instance and compare the results to a non-GPU instance and see if it's worth the extra cost.  <\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":1567747843127,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57811873",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1567727347088,
        "Question_original_content":"automl benefit gpu instanc platform run project automl notebook instanc like know automl benefit gpu instanc configur notebook",
        "Question_preprocessed_content":"automl benefit gpu instanc platform run project automl notebook instanc like know automl benefit gpu instanc configur notebook",
        "Question_gpt_summary_original":"The user is seeking information on whether H2o AutoML can benefit from a GPU Sagemaker instance and how to configure the notebook if so.",
        "Question_gpt_summary":"user seek inform automl benefit gpu instanc configur notebook",
        "Answer_original_content":"automl contain hand algorithm xgboost automl version xgboost gpu capabl algorithm insid automl lot model train automl xgboost model us util gpu mind us us featur suggest test gpu enabl instanc compar result non gpu instanc worth extra cost",
        "Answer_preprocessed_content":"automl contain hand algorithm xgboost automl version xgboost algorithm insid automl lot model train automl xgboost model us util gpu mind us us featur suggest test instanc compar result instanc worth extra cost",
        "Answer_gpt_summary_original":"Solution: The XGBoost algorithm in H2O AutoML is GPU-capable, so it can benefit from a GPU Sagemaker instance. It is recommended to test the performance on a GPU-enabled instance and compare it to a non-GPU instance to determine if the extra cost is worth it. H2O version 3.22 or above is required to use this feature.",
        "Answer_gpt_summary":"solut xgboost algorithm automl gpu capabl benefit gpu instanc recommend test perform gpu enabl instanc compar non gpu instanc determin extra cost worth version requir us featur"
    },
    {
        "Question_title":"What does \"save_graph\" keyword in WandbCallback mean?",
        "Question_body":"<p>I'm using Weights and Biases to track my deep learning models. To monitor everything I use the <code>WandbCallback<\/code> in <code>.fit<\/code>.\nIn the <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/integrations\/keras\/wandbcallback\" rel=\"nofollow noreferrer\">WandbCallback documentation<\/a> there is the keyword <code>save_graph<\/code> which defaults to True. The description is very brief and I wondered what that saved graph is and what it's for? Is saving a graph a costly operation? For what is it needed? (like does it complement something else, like saving the best model?)<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1636577514790,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":60.0,
        "Answer_body":"<p>That is used to create log a <a href=\"https:\/\/docs.wandb.ai\/ref\/python\/data-types\/graph\" rel=\"nofollow noreferrer\">wandb.Graph<\/a> of the model. This class is typically used for saving and diplaying neural net models. It represents the graph as an array of nodes and edges. The nodes can have labels that can be visualized by wandb. Here's an example of the graph that it produces: <a href=\"https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model\" rel=\"nofollow noreferrer\">https:\/\/wandb.ai\/l2k2\/keras-examples\/runs\/ieqy2e9h\/model<\/a><\/p>\n<p>Here's the code that does that within the callback. <a href=\"https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552\" rel=\"nofollow noreferrer\">https:\/\/github.com\/wandb\/client\/blob\/1609f82c84e2244ed8fe62c746099d2094bd746a\/wandb\/integration\/keras\/keras.py#L552<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69920078",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_closed_time":1636642289972,
        "Question_original_content":"save graph keyword callback mean track deep learn model monitor us callback fit callback document keyword save graph default true descript brief wonder save graph save graph costli oper need like complement like save best model",
        "Question_preprocessed_content":"keyword callback mean track deep learn model monitor us callback document keyword default true descript brief wonder save graph save graph costli oper need",
        "Question_gpt_summary_original":"The user is encountering challenges in understanding the purpose and cost of the \"save_graph\" keyword in the WandbCallback used for monitoring deep learning models with Weights and Biases. They are seeking clarification on what the saved graph is for and whether it is a costly operation, as well as whether it complements other model-saving functions.",
        "Question_gpt_summary":"user encount challeng understand purpos cost save graph keyword callback monitor deep learn model seek clarif save graph costli oper complement model save function",
        "Answer_original_content":"creat log graph model class typic save diplai neural net model repres graph arrai node edg node label visual exampl graph produc http kera exampl run ieqyeh model code callback http github com client blob fceedfecdbda integr kera kera",
        "Answer_preprocessed_content":"creat log graph model class typic save diplai neural net model repres graph arrai node edg node label visual exampl graph produc code callback",
        "Answer_gpt_summary_original":"Solution:\n- The \"save_graph\" keyword in the WandbCallback is used to create a log of the model as a wandb.Graph, which is typically used for saving and displaying neural net models.\n- The saved graph represents the graph as an array of nodes and edges, with nodes that can have labels that can be visualized by wandb.\n- The code for creating the graph is available in the WandbCallback. \n- No information is provided on whether it is a costly operation or whether it complements other model-saving functions.",
        "Answer_gpt_summary":"solut save graph keyword callback creat log model graph typic save displai neural net model save graph repres graph arrai node edg node label visual code creat graph avail callback inform provid costli oper complement model save function"
    },
    {
        "Question_title":"How to start Spark session on Vertex AI workbench Jupyterlab notebook?",
        "Question_body":"<p>Can you kindly show me how do we start the Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook?\n<br> This is working fine in Google Colaboratory by the way.\n<br> What is missing here?<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\nimport sys\n\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\n\nfrom sparknlp.base import *\nfrom sparknlp.common import *\nfrom sparknlp.annotator import *\n\nfrom pyspark.ml import Pipeline\nfrom pyspark.sql import SparkSession\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><a href=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/JQI5Z.png\" alt=\"Error\" \/><\/a><\/p>\n<p><br><br> <strong>UPDATE_2022-07-21:<\/strong>\n<br> Hi @Sayan. I am still not able to start Spark session on Vertex AI workbench Jupyterlab notebook after running the commands =(\n<a href=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/4bxzJ.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\nimport os\n# Included else &quot;JAVA_HOME is not set&quot;\n# https:\/\/github.com\/jupyter\/jupyter\/issues\/248\nos.environ[&quot;JAVA_HOME&quot;] = &quot;C:\/Program Files\/Java\/jdk-18.0.1.1&quot;\nos.environ[&quot;PATH&quot;] = os.environ[&quot;JAVA_HOME&quot;] + &quot;\/bin:&quot; + os.environ[&quot;PATH&quot;]\n\nimport sparknlp\nspark = sparknlp.start()\n\nprint(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\nprint(&quot;Apache Spark version: {}&quot;.format(spark.version))\n<\/code><\/pre>\n<p>The error:<\/p>\n<pre><code>\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 71: C:\/Program Files\/Java\/jdk-18.0.1.1\/bin\/java: No such file or directory\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/bin\/spark-class: line 96: CMD: bad array subscript\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n\/tmp\/ipykernel_5831\/489505405.py in &lt;module&gt;\n      6 \n      7 import sparknlp\n----&gt; 8 spark = sparknlp.start()\n      9 \n     10 print(&quot;Spark NLP version: {}&quot;.format(sparknlp.version()))\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start(gpu, m1, memory, cache_folder, log_folder, cluster_tmp_dir, real_time_output, output_level)\n    242         return SparkRealTimeOutput()\n    243     else:\n--&gt; 244         spark_session = start_without_realtime_output()\n    245         return spark_session\n    246 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sparknlp\/__init__.py in start_without_realtime_output()\n    152             builder.config(&quot;spark.jsl.settings.storage.cluster_tmp_dir&quot;, cluster_tmp_dir)\n    153 \n--&gt; 154         return builder.getOrCreate()\n    155 \n    156     def start_with_realtime_output():\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/sql\/session.py in getOrCreate(self)\n    267                         sparkConf.set(key, value)\n    268                     # This SparkContext may be an existing one.\n--&gt; 269                     sc = SparkContext.getOrCreate(sparkConf)\n    270                     # Do not update `SparkConf` for existing `SparkContext`, as it's shared\n    271                     # by all sessions.\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in getOrCreate(cls, conf)\n    481         with SparkContext._lock:\n    482             if SparkContext._active_spark_context is None:\n--&gt; 483                 SparkContext(conf=conf or SparkConf())\n    484             assert SparkContext._active_spark_context is not None\n    485             return SparkContext._active_spark_context\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in __init__(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls)\n    193             )\n    194 \n--&gt; 195         SparkContext._ensure_initialized(self, gateway=gateway, conf=conf)\n    196         try:\n    197             self._do_init(\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/context.py in _ensure_initialized(cls, instance, gateway, conf)\n    415         with SparkContext._lock:\n    416             if not SparkContext._gateway:\n--&gt; 417                 SparkContext._gateway = gateway or launch_gateway(conf)\n    418                 SparkContext._jvm = SparkContext._gateway.jvm\n    419 \n\n\/opt\/conda\/lib\/python3.7\/site-packages\/pyspark\/java_gateway.py in launch_gateway(conf, popen_kwargs)\n    104 \n    105             if not os.path.isfile(conn_info_file):\n--&gt; 106                 raise RuntimeError(&quot;Java gateway process exited before sending its port number&quot;)\n    107 \n    108             with open(conn_info_file, &quot;rb&quot;) as info:\n\nRuntimeError: Java gateway process exited before sending its port number\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1658299714453,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1658406889243,
        "Question_score":1.0,
        "Question_view_count":238.0,
        "Answer_body":"<p>One possible reason is that <code>Java<\/code> is not installed. When you create a <strong>Python-3 Vertex AI Workbench<\/strong> you can have either <code>Debian<\/code> or <code>Ubuntu<\/code> as an OS and it does not come with Java pre-installed. You need to install it manually.\nTo install you can use<\/p>\n<pre><code>sudo apt-get update\nsudo apt-get install default-jdk\n<\/code><\/pre>\n<p>You can follow this <a href=\"https:\/\/www.digitalocean.com\/community\/tutorials\/how-to-install-java-with-apt-get-on-ubuntu-16-04\" rel=\"nofollow noreferrer\">tutorial<\/a> to install Open JDK.<\/p>\n<p>All your problems lie with installing JDK and setting its path in the environment. Once you do this properly you don't need to set path in python also.\nYour code should look something like this<\/p>\n<pre><code># Install Spark NLP from PyPI\n!pip install -q spark-nlp==4.0.1 pyspark==3.3.0\n\n#no need to set the environment path\n\nimport sparknlp\n#all other imports\n\nimport pandas as pd\n\nspark=sparknlp.start() \n\nprint(&quot;Spark NLP version: &quot;, sparknlp.version())\nprint(&quot;Apache Spark version: &quot;, spark.version)\n\nspark\n<\/code><\/pre>\n<p><strong>EDIT:<\/strong>\nI have tried your code and had the same error.All I did was Open the terminal inside JupyterLab of the workbench and installed java there.<\/p>\n<p>Opened the JupyterLab from Workbench\n<a href=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/lpE1Q.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Notebook instance.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vIZYB.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Opening the terminal from <em><strong><code>File-&gt;New-&gt;Terminal<\/code><\/strong><\/em>\n<a href=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/CHCzs.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>From here I downloaded and installed the Java.<\/p>\n<p>You can check whether it has been installed and added to your path by running <code>java --version<\/code> it will return the current version.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1658411352227,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73047089",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1658384407350,
        "Question_original_content":"start spark session workbench jupyterlab notebook kindli start spark session googl cloud workbench jupyterlab notebook work fine googl colaboratori wai miss instal spark nlp pypi pip instal spark nlp pyspark import import sy http github com jupyt jupyt issu environ java home program file java jdk environ path environ java home bin environ path import sparknlp sparknlp base import sparknlp common import sparknlp annot import pyspark import pipelin pyspark sql import sparksess import panda spark sparknlp start print spark nlp version sparknlp version print apach spark version spark version spark updat sayan abl start spark session workbench jupyterlab notebook run command instal spark nlp pypi pip instal spark nlp pyspark import includ java home set http github com jupyt jupyt issu environ java home program file java jdk environ path environ java home bin environ path import sparknlp spark sparknlp start print spark nlp version format sparknlp version print apach spark version format spark version error opt conda lib python site packag pyspark bin spark class line program file java jdk bin java file directori opt conda lib python site packag pyspark bin spark class line cmd bad arrai subscript runtimeerror traceback recent tmp ipykernel import sparknlp spark sparknlp start print spark nlp version format sparknlp version opt conda lib python site packag sparknlp init start gpu memori cach folder log folder cluster tmp dir real time output output level return sparkrealtimeoutput spark session start realtim output return spark session opt conda lib python site packag sparknlp init start realtim output builder config spark jsl set storag cluster tmp dir cluster tmp dir return builder getorcr def start realtim output opt conda lib python site packag pyspark sql session getorcr self sparkconf set kei valu sparkcontext exist sparkcontext getorcr sparkconf updat sparkconf exist sparkcontext share session opt conda lib python site packag pyspark context getorcr cl conf sparkcontext lock sparkcontext activ spark context sparkcontext conf conf sparkconf assert sparkcontext activ spark context return sparkcontext activ spark context opt conda lib python site packag pyspark context init self master appnam sparkhom pyfil environ batchsiz serial conf gatewai jsc profil cl udf profil cl sparkcontext ensur initi self gatewai gatewai conf conf try self init opt conda lib python site packag pyspark context ensur initi cl instanc gatewai conf sparkcontext lock sparkcontext gatewai sparkcontext gatewai gatewai launch gatewai conf sparkcontext jvm sparkcontext gatewai jvm opt conda lib python site packag pyspark java gatewai launch gatewai conf popen kwarg path isfil conn info file rais runtimeerror java gatewai process exit send port number open conn info file info runtimeerror java gatewai process exit send port number",
        "Question_preprocessed_content":"start spark session workbench jupyterlab notebook kindli start spark session googl cloud workbench jupyterlab notebook work fine googl colaboratori wai miss abl start spark session workbench jupyterlab notebook run command error",
        "Question_gpt_summary_original":"The user is facing challenges in starting a Spark session on Google Cloud Vertex AI workbench Jupyterlab notebook. They have tried installing Spark NLP and PySpark, setting the JAVA_HOME environment variable, and importing the necessary libraries, but they are still encountering errors. The user has also provided an update with a new error message that includes a bad array subscript and a runtime error related to the Java gateway process.",
        "Question_gpt_summary":"user face challeng start spark session googl cloud workbench jupyterlab notebook tri instal spark nlp pyspark set java home environ variabl import necessari librari encount error user provid updat new error messag includ bad arrai subscript runtim error relat java gatewai process",
        "Answer_original_content":"possibl reason java instal creat python workbench debian ubuntu come java pre instal need instal manual instal us sudo apt updat sudo apt instal default jdk follow tutori instal open jdk problem lie instal jdk set path environ properli need set path python code look like instal spark nlp pypi pip instal spark nlp pyspark need set environ path import sparknlp import import panda spark sparknlp start print spark nlp version sparknlp version print apach spark version spark version spark edit tri code error open termin insid jupyterlab workbench instal java open jupyterlab workbench notebook instanc open termin file new termin download instal java check instal ad path run java version return current version",
        "Answer_preprocessed_content":"possibl reason instal creat workbench come java need instal manual instal us follow tutori instal open jdk problem lie instal jdk set path environ properli need set path python code look like edit tri code open termin insid jupyterlab workbench instal java open jupyterlab workbench notebook instanc open termin download instal java check instal ad path run return current version",
        "Answer_gpt_summary_original":"Solution:\n- Install Java manually using the command \"sudo apt-get update\" and \"sudo apt-get install default-jdk\".\n- Set the path of Java in the environment.\n- Open the terminal inside JupyterLab of the workbench and install Java there.\n- Check whether Java has been installed and added to the path by running \"java --version\".",
        "Answer_gpt_summary":"solut instal java manual command sudo apt updat sudo apt instal default jdk set path java environ open termin insid jupyterlab workbench instal java check java instal ad path run java version"
    },
    {
        "Question_title":"How do you clear the persistent storage for a notebook instance on AWS SageMaker?",
        "Question_body":"<p>So I'm running into the following error on AWS SageMaker when trying to save:<\/p>\n<blockquote>\n<p>Unexpected error while saving file: untitled.ipynb [Errno 28] No space left on device<\/p>\n<\/blockquote>\n<p>If I remove my notebook, create a new identical one and run it, everything works fine. However, I'm suspecting the Jupyter checkpoint takes up too much space if I save the notebook while it's running and therefore I'm running out of space. Sadly, getting more storage is not an option for me, so I'm wondering if there's any command I can use to clear the storage before running my notebook?<\/p>\n<p>More specifically, clearing the persistent storage in the beginning and at the end of the training process.<\/p>\n<p>I have googled like a maniac but there is no suggestion aside from &quot;just increase the amount of storage bro&quot; and that's why I'm asking the question here.<\/p>\n<p>Thanks in advance!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1617191466313,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":973.0,
        "Answer_body":"<p>If you don't want your data to be persistent across multiple notebook runs, just store them in <code>\/tmp<\/code> which is not persistent. You have at least 10GB. More details <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/howitworks-create-ws.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66887340",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1617198319367,
        "Question_original_content":"clear persist storag notebook instanc run follow error try save unexpect error save file untitl ipynb errno space left devic remov notebook creat new ident run work fine suspect jupyt checkpoint take space save notebook run run space sadli get storag option wonder command us clear storag run notebook specif clear persist storag begin end train process googl like maniac suggest asid increas storag bro ask question thank advanc",
        "Question_preprocessed_content":"clear persist storag notebook instanc run follow error try save unexpect error save file space left devic remov notebook creat new ident run work fine suspect jupyt checkpoint take space save notebook run run space sadli get storag option wonder command us clear storag run notebook specif clear persist storag begin end train process googl like maniac suggest asid increas storag bro ask question thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error on AWS SageMaker when trying to save their notebook due to insufficient storage space. They suspect that the Jupyter checkpoint is taking up too much space and are looking for a command to clear the persistent storage before and after the training process. The user has searched for solutions but has not found any aside from increasing the storage capacity.",
        "Question_gpt_summary":"user encount error try save notebook insuffici storag space suspect jupyt checkpoint take space look command clear persist storag train process user search solut asid increas storag capac",
        "Answer_original_content":"want data persist multipl notebook run store tmp persist detail",
        "Answer_preprocessed_content":"want data persist multipl notebook run store persist detail",
        "Answer_gpt_summary_original":"Solution: The user can store their data in \/tmp which is not persistent and has at least 10GB of storage space. This will prevent the Jupyter checkpoint from taking up too much space and avoid the error encountered when trying to save the notebook.",
        "Answer_gpt_summary":"solut user store data tmp persist storag space prevent jupyt checkpoint take space avoid error encount try save notebook"
    },
    {
        "Question_title":"Why is there no specific directory that SageMaker was supposed to create automatically?",
        "Question_body":"<p>I'm trying to deploy my model (container) on AWS SageMaker. I've pushed the container to AWS ECR.\nThen I use an AWS Lambda that basically runs <code>create_training_job()<\/code> via the <code>boto3<\/code> SageMaker client. It runs the container in <strong>train<\/strong> mode and puts the generated artifact to the S3. Like that:<\/p>\n<pre><code>sm = boto3.client('sagemaker')\n\nsm.create_training_job(\n        TrainingJobName=full_job_name,\n        HyperParameters={\n            'general': json.dumps(\n                {\n                    'environment': ENVIRONMENT,\n                    'region': REGION,\n                    'version': date_suffix,\n                    'hyperparameter_tuning': training_params.get('hyperparameter_tuning', False),\n                    'basket_analysis': training_params.get('basket_analysis', True),\n                    'init_inventory_cache': training_params.get('init_inventory_cache', True),\n                }\n            ),\n            'aws_profile': '***-dev',\n            'db_config': json.dumps(database_mapping),\n            'model_server_params': json.dumps(training_params.get('model_server_params', {}))\n        },\n\n        AlgorithmSpecification={\n            'TrainingImage': training_image,\n            'TrainingInputMode': 'File',\n        },\n        RoleArn=ROLE_ARN,\n        OutputDataConfig={\n            'S3OutputPath': S3_OUTPUT_PATH\n        },\n        ResourceConfig={\n            'InstanceType': INSTANCE_TYPE,\n            'InstanceCount': 1,\n            'VolumeSizeInGB': 20,\n        },\n        # VpcConfig={\n        #     'SecurityGroupIds': SECURITY_GROUPS.split(','),\n        #     'Subnets': SUBNETS.split(',')\n        # },\n        StoppingCondition={\n            'MaxRuntimeInSeconds': int(MAX_RUNTIME_SEC),\n            #        'MaxWaitTimeInSeconds': 1800\n        },\n        Tags=[ ],\n        EnableNetworkIsolation=False,\n        EnableInterContainerTrafficEncryption=False,\n        EnableManagedSpotTraining=False,\n    )\n<\/code><\/pre>\n<p>I have a logger inside the container that says that <code>opt\/ml\/input\/config\/hyperparameters.json<\/code> now exists. It has been added by SageMaker. Fine.<\/p>\n<p>But then, when I try to run the same container in <code>serve<\/code> mode (so basically to deploy it) I encounter that <code>opt\/ml\/input\/config\/hyperparameters.json<\/code> doesn't exist anymore. I deploy it this way:<\/p>\n<pre><code>     sm.create_model(\n        ModelName=model_name,\n        PrimaryContainer={\n            'Image': training_image,\n            'ModelDataUrl': model_artifact,\n            'Environment': {\n                'version': version\n            }\n        },\n        ExecutionRoleArn=role_arn,\n        Tags=[ ],\n        # VpcConfig = {\n        #     'SecurityGroupIds': os.environ['security_groups'].split(','),\n        #     'Subnets': os.environ['subnets'].split(',')\n        # }\n    )\n\n    sm.create_endpoint_config(\n        EndpointConfigName=config_name,\n        ProductionVariants=[\n            {\n                'VariantName': variant_name,\n                'ModelName': model_name,\n                'InitialInstanceCount': instance_count,\n                'InstanceType': instance_type,\n                'InitialVariantWeight': 1\n            },\n        ],\n        Tags=[ ],\n    )\n\n    existing_endpoints = sm.list_endpoints(NameContains=endpoint_name)\n\n    scaling_resource_id = f'endpoint\/{endpoint_name}\/variant\/{variant_name}'\n\n    if not existing_endpoints['Endpoints']:\n        sm.create_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=config_name\n        )\n    else:\n        if aas.describe_scalable_targets(\n                ServiceNamespace='sagemaker',\n                ResourceIds=[scaling_resource_id],\n                ScalableDimension='sagemaker:variant:DesiredInstanceCount')['ScalableTargets']:\n            aas.deregister_scalable_target(\n                ServiceNamespace='sagemaker',\n                ResourceId=scaling_resource_id,\n                ScalableDimension='sagemaker:variant:DesiredInstanceCount'\n            )\n\n        sm.update_endpoint(\n            EndpointName=endpoint_name,\n            EndpointConfigName=config_name\n        )\n<\/code><\/pre>\n<p>It is important since it seemed to be a convenient way to pass some parameters inside the container from outside (like management console).<\/p>\n<p>I thought that this file\/directory will still exist after the <strong>train<\/strong>. Any ideas?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1629250878187,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1629669900607,
        "Question_score":0.0,
        "Question_view_count":129.0,
        "Answer_body":"<p>tl;dr: two options:<\/p>\n<ul>\n<li>Copy the <code>hyperparameters.json<\/code> file to <code>\/opt\/ml\/model<\/code> in the training logic and it will be packed with the model artifacts;<\/li>\n<li>Pass whatever parameters you want through the <code>PrimaryContainer<\/code> parameter's <code>Environment<\/code> property.<\/li>\n<\/ul>\n<p>Long version:<\/p>\n<p>That file, <code>opt\/ml\/input\/config\/hyperparameters.json<\/code>, (in fact the whole <code>\/opt\/ml\/input<\/code> folder) is mounted on the <strong>training<\/strong> container when it is created. It is provided by SageMaker, based on information you provide, <em>only<\/em> for training purposes. SageMaker does not change your container in any way, and it doesn't preserve this or any configuration file it passes to the training job once training is done. If you want to pass parameters to the inference endpoint, that is not the way.<\/p>\n<p>You <em>could<\/em> copy the <code>hyperparameters.json<\/code> file to the <code>\/opt\/ml\/model<\/code> folder, and it'd be packed with the model in the <code>model.tar.gz<\/code> tarball. Your infrence code could then use that - but that's not the prescribed way to pass parameters to an endpoint, and it cause problems with your framework.<\/p>\n<p>The generally prescribed way to pass parameters to SageMaker endpoints is through the <strong>environment<\/strong>. If you check the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/sagemaker.html#SageMaker.Client.create_model\" rel=\"nofollow noreferrer\">boto3 docs for create_model<\/a>, you'll see that there's an <code>Environment<\/code> key within the <code>PrimaryContainer<\/code> parameter (also for each of the <code>Containers<\/code> parameter). In fact, your code above already uses that to pass a <code>version<\/code> parameter. You should use that to pass any parameters to your model and, from there, to the endpoint based on it.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68825648",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1629579807236,
        "Question_original_content":"specif directori suppos creat automat try deploi model contain push contain aw ecr us aw lambda basic run creat train job boto client run contain train mode put gener artifact like boto client creat train job trainingjobnam job hyperparamet gener json dump environ environ region region version date suffix hyperparamet tune train param hyperparamet tune fals basket analysi train param basket analysi true init inventori cach train param init inventori cach true aw profil dev config json dump databas map model server param json dump train param model server param algorithmspecif trainingimag train imag traininginputmod file rolearn role arn outputdataconfig soutputpath output path resourceconfig instancetyp instanc type instancecount volumesizeingb vpcconfig securitygroupid secur group split subnet subnet split stoppingcondit maxruntimeinsecond int max runtim sec maxwaittimeinsecond tag enablenetworkisol fals enableintercontainertrafficencrypt fals enablemanagedspottrain fals logger insid contain sai opt input config hyperparamet json exist ad fine try run contain serv mode basic deploi encount opt input config hyperparamet json exist anymor deploi wai creat model modelnam model primarycontain imag train imag modeldataurl model artifact environ version version executionrolearn role arn tag vpcconfig securitygroupid environ secur group split subnet environ subnet split creat endpoint config endpointconfignam config productionvari variantnam variant modelnam model initialinstancecount instanc count instancetyp instanc type initialvariantweight tag exist endpoint list endpoint namecontain endpoint scale resourc endpoint endpoint variant variant exist endpoint endpoint creat endpoint endpointnam endpoint endpointconfignam config aa scalabl target servicenamespac resourceid scale resourc scalabledimens variant desiredinstancecount scalabletarget aa deregist scalabl target servicenamespac resourceid scale resourc scalabledimens variant desiredinstancecount updat endpoint endpointnam endpoint endpointconfignam config import conveni wai pass paramet insid contain outsid like manag consol thought file directori exist train idea",
        "Question_preprocessed_content":"specif directori suppos creat automat try deploi model push contain aw ecr us aw lambda basic run client run contain train mode put gener artifact like logger insid contain sai exist ad fine try run contain mode encount exist anymor deploi wai import conveni wai pass paramet insid contain outsid thought exist train idea",
        "Question_gpt_summary_original":"The user is encountering a challenge with AWS SageMaker where the directory that SageMaker was supposed to create automatically, <code>opt\/ml\/input\/config\/hyperparameters.json<\/code>, disappears after running the container in <code>train<\/code> mode and trying to deploy it in <code>serve<\/code> mode. The user is unsure why this is happening and is seeking advice on how to resolve the issue.",
        "Question_gpt_summary":"user encount challeng directori suppos creat automat opt input config hyperparamet json disappear run contain train mode try deploi serv mode user unsur happen seek advic resolv issu",
        "Answer_original_content":"option copi hyperparamet json file opt model train logic pack model artifact pass paramet want primarycontain paramet environ properti long version file opt input config hyperparamet json fact opt input folder mount train contain creat provid base inform provid train purpos chang contain wai preserv configur file pass train job train want pass paramet infer endpoint wai copi hyperparamet json file opt model folder pack model model tar tarbal infrenc code us prescrib wai pass paramet endpoint caus problem framework gener prescrib wai pass paramet endpoint environ check boto doc creat model environ kei primarycontain paramet contain paramet fact code us pass version paramet us pass paramet model endpoint base",
        "Answer_preprocessed_content":"option copi file train logic pack model artifact pass paramet want paramet properti long version file mount train contain creat provid base inform provid train purpos chang contain wai preserv configur file pass train job train want pass paramet infer endpoint wai copi file folder pack model tarbal infrenc code us prescrib wai pass paramet endpoint caus problem framework gener prescrib wai pass paramet endpoint environ check boto doc kei paramet fact code us pass paramet us pass paramet model endpoint base",
        "Answer_gpt_summary_original":"Two possible solutions were discussed in the conversation. The first solution is to copy the \"hyperparameters.json\" file to the \"\/opt\/ml\/model\" folder, which will be packed with the model in the \"model.tar.gz\" tarball. The second solution is to pass the required parameters through the \"Environment\" property of the \"PrimaryContainer\" parameter. It was mentioned that passing parameters through the \"hyperparameters.json\" file is not the prescribed way to pass parameters to an endpoint and could cause problems with the framework.",
        "Answer_gpt_summary":"possibl solut discuss convers solut copi hyperparamet json file opt model folder pack model model tar tarbal second solut pass requir paramet environ properti primarycontain paramet mention pass paramet hyperparamet json file prescrib wai pass paramet endpoint caus problem framework"
    },
    {
        "Question_title":"Making predictions with Azure Machine learning with new data that contains headers (like pd.Dataframe)",
        "Question_body":"<p>My question is somehow related to <a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html<\/a> - however, the provided solution does not seem to work.<\/p>\n<p>I am constructing a simple model with heart-disease dataset but I wrap it into Pipeline as I use some featurization steps (scaling, encoding etc.) The full script below:<\/p>\n<pre><code>import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.linear_model import LogisticRegression\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport joblib\nimport pickle\n\n# data input\ndf = pd.read_csv('heart.csv')\n\n# numerical variables\nnum_cols = ['age',\n            'trestbps',\n            'chol',\n            'thalach',\n            'oldpeak'\n]\n\n# categorical variables\ncat_cols = ['sex',\n            'cp',\n            'fbs',\n            'restecg',\n            'exang',\n            'slope',\n            'ca',\n            'thal']\n\n# changing format of the categorical variables\ndf[cat_cols] = df[cat_cols].apply(lambda x: x.astype('object'))\n\n# target variable\ny = df['target']\n\n# features\nX = df.drop(['target'], axis=1)\n\n# data split:\n\n# random seed\nnp.random.seed(42)\n\n# splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y,\n                                                    test_size=0.2,\n                                                    stratify=y)\n\n# double check\nX_train.shape, X_test.shape, y_train.shape, y_test.shape\n\n# pipeline for numerical data\nnum_preprocessing = Pipeline([('num_imputer', SimpleImputer(strategy='mean')), # imputing with mean\n                                                   ('minmaxscaler', MinMaxScaler())]) # scaling\n\n# pipeline for categorical data\ncat_preprocessing = Pipeline([('cat_imputer', SimpleImputer(strategy='constant', fill_value='missing')), # filling missing values\n                                                ('onehot', OneHotEncoder(drop='first', handle_unknown='error'))]) # One Hot Encoding\n\n# preprocessor - combining pipelines\npreprocessor = ColumnTransformer([\n                                  ('categorical', cat_preprocessing, cat_cols),\n                                  ('numerical', num_preprocessing, num_cols)\n                                                           ])\n\n# initial model parameters\nlog_ini_params = {'penalty': 'l2', \n                  'tol': 0.0073559740277086005, \n                  'C': 1.1592424247511928, \n                  'fit_intercept': True, \n                  'solver': 'liblinear'}\n\n# model - Pipeline\nlog_clf = Pipeline([('preprocessor', preprocessor),\n                  ('clf', LogisticRegression(**log_ini_params))])\n\nlog_clf.fit(X_train, y_train)\n\n# dumping the model\nf = 'model\/log.pkl'\nwith open(f, 'wb') as file:\n    pickle.dump(log_clf, file)\n\n# loading it\nloaded_model = joblib.load(f)\n\n# double check on a single datapoint\nnew_data = pd.DataFrame({'age': 71,\n                         'sex': 0,\n                         'cp': 0,\n                         'trestbps': 112,\n                         'chol': 203,\n                         'fbs': 0,\n                         'restecg': 1,\n                         'thalach': 185,\n                         'exang': 0,\n                         'oldpeak': 0.1,\n                         'slope': 2,\n                         'ca': 0,\n                          'thal': 2}, index=[0])\n\nloaded_model.predict(new_data)\n\n<\/code><\/pre>\n<p>...and it works just fine.  Then I deploy the model to the Azure Web Service using these steps:<\/p>\n<ol>\n<li>I create the score.py file<\/li>\n<\/ol>\n<pre><code>import joblib\nfrom azureml.core.model import Model\nimport json\n\ndef init():\n    global model\n    model_path = Model.get_model_path('log') # logistic\n    print('Model Path is  ', model_path)\n    model = joblib.load(model_path)\n\n\ndef run(data):\n    try:\n        data = json.loads(data)\n        result = model.predict(data['data'])\n        # any data type, as long as it is JSON serializable.\n        return {'data' : result.tolist() , 'message' : 'Successfully classified heart diseases'}\n    except Exception as e:\n        error = str(e)\n        return {'data' : error , 'message' : 'Failed to classify heart diseases'}\n<\/code><\/pre>\n<ol>\n<li>I deploy the model:<\/li>\n<\/ol>\n<pre><code>from azureml.core import Workspace\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.webservice import Webservice\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.environment import Environment\nfrom azureml.core import Workspace\nfrom azureml.core.model import Model\nfrom azureml.core.conda_dependencies import CondaDependencies\n\nws = Workspace.from_config()\n\nmodel = Model.register(workspace = ws,\n              model_path ='model\/log.pkl',\n              model_name = 'log',\n              tags = {'version': '1'},\n              description = 'Heart disease classification',\n              )\n\n# to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0'], conda_packages = ['scikit-learn==0.23.2'])\nenv.python.conda_dependencies = cd\n\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Registered Environment')\n\nmyenv = Environment.get(workspace=ws, name='env')\n\nmyenv.save_to_directory('.\/environ', overwrite=True)\n\naciconfig = AciWebservice.deploy_configuration(\n            cpu_cores=1,\n            memory_gb=1,\n            tags={'data':'heart disease classifier'},\n            description='Classification of heart diseases',\n            )\n\ninference_config = InferenceConfig(entry_script='score.py', environment=myenv)\n\nservice = Model.deploy(workspace=ws,\n                name='hd-model-log',\n                models=[model],\n                inference_config=inference_config,\n                deployment_config=aciconfig, \n                overwrite = True)\n\nservice.wait_for_deployment(show_output=True)\nurl = service.scoring_uri\nprint(url)\n<\/code><\/pre>\n<p>The deployment is fine:<\/p>\n<blockquote>\n<p>Succeeded\nACI service creation operation finished, operation &quot;Succeeded&quot;<\/p>\n<\/blockquote>\n<p>But I can not make any predictions with the new data. I try to use:<\/p>\n<pre><code>import pandas as pd\n\nnew_data = pd.DataFrame([[71, 0, 0, 112, 203, 0, 1, 185, 0, 0.1, 2, 0, 2],\n                         [80, 0, 0, 115, 203, 0, 1, 185, 0, 0.1, 2, 0, 0]],\n                         columns=['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal'])\n<\/code><\/pre>\n<p>Following the answer from this topic (<a href=\"https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/answers\/questions\/217305\/data-input-format-call-the-service-for-azure-ml-ti.html<\/a>) I transform the data:<\/p>\n<pre><code>test_sample = json.dumps({'data': new_data.to_dict(orient='records')})\n<\/code><\/pre>\n<p>And try to make some predictions:<\/p>\n<pre><code>import json\nimport requests\ndata = test_sample\nheaders = {'Content-Type':'application\/json'}\nr = requests.post(url, data=data, headers = headers)\nprint(r.status_code)\nprint(r.json())\n<\/code><\/pre>\n<p>However, I encounter an error:<\/p>\n<blockquote>\n<p>200\n{'data': &quot;Expected 2D array, got 1D array instead:\\narray=[{'age': 71, 'sex': 0, 'cp': 0, 'trestbps': 112, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': &gt; 2}\\n {'age': 80, 'sex': 0, 'cp': 0, 'trestbps': 115, 'chol': 203, 'fbs': 0, 'restecg': 1, 'thalach': 185, 'exang': 0, 'oldpeak': 0.1, 'slope': 2, 'ca': 0, 'thal': 0}].\\nReshape your data either using array.reshape(-1, &gt; 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.&quot;, 'message': 'Failed to classify heart diseases'}<\/p>\n<\/blockquote>\n<p>How is it possible to adjust the input data to this form of predictions and add other output like predict_proba so I could store them in a separate output dataset?<\/p>\n<p>I know this error is somehow related either with the &quot;run&quot; part of the score.py file or the last code cell that calls the webservice, but I'm unable to find it.<\/p>\n<p>Would really appreciate some help.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1653476427160,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1653505948283,
        "Question_score":1.0,
        "Question_view_count":280.0,
        "Answer_body":"<p>I believe I managed to solve the problem - even though I encountered some serious issues. :)<\/p>\n<ol>\n<li>As described here <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-advanced-entry-script\" rel=\"nofollow noreferrer\">here<\/a> - I edited the <code>score.py<\/code> script:<\/li>\n<\/ol>\n<pre><code>import joblib\nfrom azureml.core.model import Model\nimport numpy as np\nimport json\nimport pandas as pd\nimport numpy as np\n\nfrom inference_schema.schema_decorators import input_schema, output_schema\nfrom inference_schema.parameter_types.numpy_parameter_type import NumpyParameterType\nfrom inference_schema.parameter_types.pandas_parameter_type import PandasParameterType\nfrom inference_schema.parameter_types.standard_py_parameter_type import StandardPythonParameterType\n    \ndata_sample = PandasParameterType(pd.DataFrame({'age': pd.Series([0], dtype='int64'),\n                                                'sex': pd.Series(['example_value'], dtype='object'),\n                                                'cp': pd.Series(['example_value'], dtype='object'),\n                                                'trestbps': pd.Series([0], dtype='int64'),\n                                                'chol': pd.Series([0], dtype='int64'),\n                                                'fbs': pd.Series(['example_value'], dtype='object'),\n                                                'restecg': pd.Series(['example_value'], dtype='object'),\n                                                'thalach': pd.Series([0], dtype='int64'),\n                                                'exang': pd.Series(['example_value'], dtype='object'),\n                                                'oldpeak': pd.Series([0.0], dtype='float64'),\n                                                'slope': pd.Series(['example_value'], dtype='object'),\n                                                'ca': pd.Series(['example_value'], dtype='object'),\n                                                'thal': pd.Series(['example_value'], dtype='object')}))\n\ninput_sample = StandardPythonParameterType({'data': data_sample})\nresult_sample = NumpyParameterType(np.array([0]))\noutput_sample = StandardPythonParameterType({'Results':result_sample})\n\ndef init():\n    global model\n    # Example when the model is a file\n    model_path = Model.get_model_path('log') # logistic\n    print('Model Path is  ', model_path)\n    model = joblib.load(model_path)\n\n@input_schema('Inputs', input_sample)\n@output_schema(output_sample)\ndef run(Inputs):\n    try:\n        data = Inputs['data']\n        result = model.predict_proba(data)\n        return result.tolist()\n    except Exception as e:\n        error = str(e)\n        return error\n<\/code><\/pre>\n<ol start=\"2\">\n<li>In the deployment step I adjusted the <code>CondaDependencies<\/code>:<\/li>\n<\/ol>\n<pre><code># to install required packages\nenv = Environment('env')\ncd = CondaDependencies.create(pip_packages=['pandas==1.1.5', 'azureml-defaults','joblib==0.17.0', 'inference-schema==1.3.0'], conda_packages = ['scikit-learn==0.22.2.post1'])\nenv.python.conda_dependencies = cd\n# Register environment to re-use later\nenv.register(workspace = ws)\nprint('Registered Environment')\n<\/code><\/pre>\n<p>as<\/p>\n<p>a) It is necessary to include <code>inference-schema<\/code> in the <code>Dependencies<\/code> file\nb) I downgraded <code>scikit-learn<\/code> to <code>scikit-learn==0.22.2.post1<\/code> version because of <a href=\"https:\/\/github.com\/hyperopt\/hyperopt\/issues\/668\" rel=\"nofollow noreferrer\">this issue<\/a><\/p>\n<p>Now, when I feed the model with new data:<\/p>\n<pre><code>new_data = {\n  &quot;Inputs&quot;: {\n    &quot;data&quot;: [\n      {\n        &quot;age&quot;: 71,\n        &quot;sex&quot;: &quot;0&quot;,\n        &quot;cp&quot;: &quot;0&quot;,\n        &quot;trestbps&quot;: 112,\n        &quot;chol&quot;: 203,\n        &quot;fbs&quot;: &quot;0&quot;,\n        &quot;restecg&quot;: &quot;1&quot;,\n        &quot;thalach&quot;: 185,\n        &quot;exang&quot;: &quot;0&quot;,\n        &quot;oldpeak&quot;: 0.1,\n        &quot;slope&quot;: &quot;2&quot;,\n        &quot;ca&quot;: &quot;0&quot;,\n        &quot;thal&quot;: &quot;2&quot;\n      }\n    ]\n  }\n}\n<\/code><\/pre>\n<p>And use it for prediction:<\/p>\n<pre><code>import json\nimport requests\ndata = new_data\nheaders = {'Content-Type':'application\/json'}\nr = requests.post(url, str.encode(json.dumps(data)), headers = headers)\nprint(r.status_code)\nprint(r.json())\n<\/code><\/pre>\n<p>I get:<\/p>\n<p><code>200 [[0.02325369841858338, 0.9767463015814166]]<\/code><\/p>\n<p>Uff! Maybe someone will benefit from my painful learning path! :)<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1653559436590,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72376401",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1653558488392,
        "Question_original_content":"make predict new data contain header like datafram question relat http doc microsoft com answer question data input format servic azur html provid solut work construct simpl model heart diseas dataset wrap pipelin us featur step scale encod script import panda import numpi sklearn preprocess import minmaxscal sklearn compos import columntransform sklearn preprocess import onehotencod sklearn pipelin import pipelin sklearn model select import train test split sklearn imput import simpleimput sklearn linear model import logisticregress matplotlib inlin import matplotlib pyplot plt import seaborn sn import joblib import pickl data input read csv heart csv numer variabl num col ag trestbp chol thalach oldpeak categor variabl cat col sex fb restecg exang slope thal chang format categor variabl cat col cat col appli lambda astyp object target variabl target featur drop target axi data split random seed random seed split data train test train test train test split test size stratifi doubl check train shape test shape train shape test shape pipelin numer data num preprocess pipelin num imput simpleimput strategi mean imput mean minmaxscal minmaxscal scale pipelin categor data cat preprocess pipelin cat imput simpleimput strategi constant valu miss fill miss valu onehot onehotencod drop handl unknown error hot encod preprocessor combin pipelin preprocessor columntransform categor cat preprocess cat col numer num preprocess num col initi model paramet log ini param penalti tol fit intercept true solver liblinear model pipelin log clf pipelin preprocessor preprocessor clf logisticregress log ini param log clf fit train train dump model model log pkl open file pickl dump log clf file load load model joblib load doubl check singl datapoint new data datafram ag sex trestbp chol fb restecg thalach exang oldpeak slope thal index load model predict new data work fine deploi model azur web servic step creat score file import joblib core model import model import json def init global model model path model model path log logist print model path model path model joblib load model path def run data try data json load data result model predict data data data type long json serializ return data result tolist messag successfulli classifi heart diseas except error str return data error messag fail classifi heart diseas deploi model core import workspac core webservic import aciwebservic core webservic import webservic core model import inferenceconfig core environ import environ core import workspac core model import model core conda depend import condadepend workspac config model model regist workspac model path model log pkl model log tag version descript heart diseas classif instal requir packag env environ env condadepend creat pip packag panda default joblib conda packag scikit learn env python conda depend regist environ us later env regist workspac print regist environ myenv environ workspac env myenv save directori environ overwrit true aciconfig aciwebservic deploi configur cpu core memori tag data heart diseas classifi descript classif heart diseas infer config inferenceconfig entri script score environ myenv servic model deploi workspac model log model model infer config infer config deploy config aciconfig overwrit true servic wait deploy output true url servic score uri print url deploy fine succeed aci servic creation oper finish oper succeed predict new data try us import panda new data datafram column ag sex trestbp chol fb restecg thalach exang oldpeak slope thal follow answer topic http doc microsoft com answer question data input format servic azur html transform data test sampl json dump data new data dict orient record try predict import json import request data test sampl header content type applic json request post url data data header header print statu code print json encount error data expect arrai got arrai instead narrai ag sex trestbp chol fb restecg thalach exang oldpeak slope thal ag sex trestbp chol fb restecg thalach exang oldpeak slope thal nreshap data arrai reshap data singl featur arrai reshap contain singl sampl messag fail classifi heart diseas possibl adjust input data form predict add output like predict proba store separ output dataset know error relat run score file code cell call webservic unabl appreci help",
        "Question_preprocessed_content":"make predict new data contain header question relat provid solut work construct simpl model dataset wrap pipelin us featur step script work fine deploi model azur web servic step creat file deploi model deploy fine succeed aci servic creation oper finish oper succeed predict new data try us follow answer topic transform data try predict encount error data expect arrai got arrai sex trestbp chol fb restecg thalach exang oldpeak slope thal data data singl featur contain singl messag fail classifi heart diseas possibl adjust input data form predict add output like store separ output dataset know error relat run file code cell call webservic unabl appreci help",
        "Question_gpt_summary_original":"The user is facing challenges in making predictions with Azure Machine Learning using new data that contains headers. The provided solution does not seem to work, and the user encounters an error when trying to make predictions with the new data. The error message suggests that the input data needs to be adjusted to a specific form for predictions and to add other output like predict_proba to store them in a separate output dataset. The user is seeking help to resolve these issues.",
        "Question_gpt_summary":"user face challeng make predict new data contain header provid solut work user encount error try predict new data error messag suggest input data need adjust specif form predict add output like predict proba store separ output dataset user seek help resolv issu",
        "Answer_original_content":"believ manag solv problem encount issu describ edit score script import joblib core model import model import numpi import json import panda import numpi infer schema schema decor import input schema output schema infer schema paramet type numpi paramet type import numpyparametertyp infer schema paramet type panda paramet type import pandasparametertyp infer schema paramet type standard paramet type import standardpythonparametertyp data sampl pandasparametertyp datafram ag seri dtype int sex seri exampl valu dtype object seri exampl valu dtype object trestbp seri dtype int chol seri dtype int fb seri exampl valu dtype object restecg seri exampl valu dtype object thalach seri dtype int exang seri exampl valu dtype object oldpeak seri dtype float slope seri exampl valu dtype object seri exampl valu dtype object thal seri exampl valu dtype object input sampl standardpythonparametertyp data data sampl result sampl numpyparametertyp arrai output sampl standardpythonparametertyp result result sampl def init global model exampl model file model path model model path log logist print model path model path model joblib load model path input schema input input sampl output schema output sampl def run input try data input data result model predict proba data return result tolist except error str return error deploy step adjust condadepend instal requir packag env environ env condadepend creat pip packag panda default joblib infer schema conda packag scikit learn post env python conda depend regist environ us later env regist workspac print regist environ necessari includ infer schema depend file downgrad scikit learn scikit learn post version issu feed model new data new data input data ag sex trestbp chol fb restecg thalach exang oldpeak slope thal us predict import json import request data new data header content type applic json request post url str encod json dump data header header print statu code print json uff mayb benefit pain learn path",
        "Answer_preprocessed_content":"believ manag solv problem encount issu describ edit script deploy step adjust necessari includ file downgrad version issu feed model new data us predict uff mayb benefit pain learn path",
        "Answer_gpt_summary_original":"Solution:\n1. Edit the score.py script as described in the documentation to adjust the input data to a specific form for predictions and to add other output like predict_proba to store them in a separate output dataset.\n2. Adjust the CondaDependencies in the deployment step to include inference-schema in the Dependencies file and downgrade scikit-learn to scikit-learn==0.22.2.post1 version.\nAfter making these changes, the user was able to make predictions with new data containing headers without encountering any errors.",
        "Answer_gpt_summary":"solut edit score script describ document adjust input data specif form predict add output like predict proba store separ output dataset adjust condadepend deploy step includ infer schema depend file downgrad scikit learn scikit learn post version make chang user abl predict new data contain header encount error"
    },
    {
        "Question_title":"azuremlsdk for R error Could not retrieve user token. Please run 'az login'",
        "Question_body":"I'm trying to create a workspace in azure machine learning and receiving this error after 2 browser Windows open and I click log in.\n\nlibrary(azuremlsdk)\nnew_ws <- create_workspace(name = 'muffin',\n\n\n+ subscription_id = 'XXXXXXXXXXXX',\n+ resource_group = 'white',\n+ location = 'eastus2',\n+ create_resource_group = T)\nNote, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\nYou have logged in. Now let us find all the subscriptions to which you have access...\nNote, we have launched a browser for you to login. For old experience with device code, use \"az login --use-device-code\"\nYou have logged in. Now let us find all the subscriptions to which you have access...\nError in py_call_impl(callable, dots$args, dots$keywords) :\nAuthenticationException: AuthenticationException:\nMessage: Could not retrieve user token. Please run 'az login'\nInnerException It is required that you pass in a value for the \"algorithms\" argument when calling decode().\nErrorResponse\n{\n\"error\": {\n\"code\": \"UserError\",\n\"inner_error\": {\n\"code\": \"Authentication\"\n},\n\"message\": \"Could not retrieve user token. Please run 'az login'\"\n}\n}\n\n\nhow do I get passed this error?",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1621290302507,
        "Question_favorite_count":10.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"You have to use this command to make it install the correct version of miniconda reticulate::py_install(\"PyJWT==1.7.1\"). If you don't do that it seems to install the wrong version. I also had to manually delete the r-miniconda folder in \\appdata\\local\\r-miniconda which got installed previously to get it to install the correct version. It's pretty outrageous they leave that out of the tutorial when it ain't going to work otherwise.\n\nIf you try to do the accident.R tutorial for azuremlsdk-r next make sure you add the line\n\ninteractive_auth <- interactive_login_authentication(tenant_id=\"<tenant id>\")\n\nto your code otherwise you'll get a permissions error and it won't work.\n\nThen to the create_workspace or get_workspace function you have to add auth = interactive_auth after a comma.\n\nIt should look like this\n\nnew_ws <- get_workspace(name = \"<workspace name>\",\nsubscription_id = \"<subscription id>\",\nresource_group = \"<resource name>\",\nauth = interactive_auth)\n\nTo find the tenant ID I had to download the azure CLI and run the command az login. Not sure if there is another way to find a tenant ID or not.\n\n\n\n\nTo leave out critical steps from a tutorial is gross incompetence on the part of Azure. How anyone who isn't a comp sci phd uses this service is a mystery to me.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/398420\/azuremlsdk-for-r-error-could-not-retrieve-user-tok.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1621844855832,
        "Question_original_content":"sdk error retriev user token run login try creat workspac receiv error browser window open click log librari sdk new creat workspac muffin subscript resourc group white locat eastu creat resourc group note launch browser login old experi devic code us login us devic code log let subscript access note launch browser login old experi devic code us login us devic code log let subscript access error impl callabl dot arg dot keyword authenticationexcept authenticationexcept messag retriev user token run login innerexcept requir pass valu algorithm argument call decod errorrespons error code usererror inner error code authent messag retriev user token run login pass error",
        "Question_preprocessed_content":"sdk error retriev user token run login try creat workspac receiv error browser window open click log librari muffin white locat eastu note launch browser login old experi devic code us login log let subscript note launch browser login old experi devic code us login log let subscript error dot arg dot keyword authenticationexcept authenticationexcept messag retriev user token run login innerexcept requir pass valu algorithm argument call decod errorrespons error messag retriev user token run login pass error",
        "Question_gpt_summary_original":"The user is encountering an error while trying to create a workspace in Azure Machine Learning using the azuremlsdk for R. The error message states that the user token could not be retrieved and prompts the user to run 'az login'. The user has tried logging in through two browser windows but is still unable to create the workspace. The user is seeking assistance to resolve this issue.",
        "Question_gpt_summary":"user encount error try creat workspac sdk error messag state user token retriev prompt user run login user tri log browser window unabl creat workspac user seek assist resolv issu",
        "Answer_original_content":"us command instal correct version miniconda reticul instal pyjwt instal wrong version manual delet miniconda folder appdata local miniconda got instal previous instal correct version pretti outrag leav tutori ain go work try accid tutori sdk sure add line interact auth interact login authent tenant code permiss error won work creat workspac workspac function add auth interact auth comma look like new workspac subscript resourc group auth interact auth tenant download azur cli run command login sure wai tenant leav critic step tutori gross incompet azur isn comp sci phd us servic mysteri",
        "Answer_preprocessed_content":"us command instal correct version miniconda instal wrong version manual delet folder got instal previous instal correct version pretti outrag leav tutori ain go work try tutori sure add line code permiss error won work function add auth comma look like auth tenant download azur cli run command login sure wai tenant leav critic step tutori gross incompet azur isn comp sci phd us servic mysteri",
        "Answer_gpt_summary_original":"Solutions provided:\n- Use the command 'reticulate::py_install(\"PyJWT==1.7.1\")' to install the correct version of miniconda.\n- Manually delete the r-miniconda folder in \\appdata\\local\\r-miniconda which got installed previously to get it to install the correct version.\n- Add the line 'interactive_auth <- interactive_login_authentication(tenant_id=\"<tenant id>\")' to the code.\n- Add 'auth = interactive_auth' after a comma to the create_workspace or get_workspace function.\n- Download the azure CLI and run the command 'az login' to find the tenant ID.\n\nNo solution provided for the issue of encountering an error while trying to create a workspace in Azure Machine",
        "Answer_gpt_summary":"solut provid us command reticul instal pyjwt instal correct version miniconda manual delet miniconda folder appdata local miniconda got instal previous instal correct version add line interact auth interact login authent tenant code add auth interact auth comma creat workspac workspac function download azur cli run command login tenant solut provid issu encount error try creat workspac azur machin"
    },
    {
        "Question_title":"Size of the input \/ output parameters in the pipeline step in Azure",
        "Question_body":"<p>while running pipeline creation python script facing the following error.\n&quot;AzureMLCompute job failed. JobConfigurationMaxSizeExceeded: The specified job configuration exceeds the max allowed size of 32768 characters. Please reduce the size of the job's command line arguments and environment settings&quot;<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1632798980813,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":4.0,
        "Question_view_count":208.0,
        "Answer_body":"<p>When we tried to pass a quite lengthy content as argument value to a Pipeline. You can try to upload file to blob, optionally create a dataset, then pass on dataset name or file path to AML pipeline as parameter. The pipeline step will read content of the file from the blob.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69355385",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1632803094283,
        "Question_original_content":"size input output paramet pipelin step azur run pipelin creation python script face follow error comput job fail jobconfigurationmaxsizeexceed specifi job configur exce max allow size charact reduc size job command line argument environ set",
        "Question_preprocessed_content":"size input output paramet pipelin step azur run pipelin creation python script face follow error comput job fail jobconfigurationmaxsizeexceed specifi job configur exce max allow size charact reduc size job command line argument environ set",
        "Question_gpt_summary_original":"The user encountered an error while running a pipeline creation python script in Azure, which stated that the job configuration exceeded the maximum allowed size of 32768 characters. The user was advised to reduce the size of the job's command line arguments and environment settings.",
        "Question_gpt_summary":"user encount error run pipelin creation python script azur state job configur exceed maximum allow size charact user advis reduc size job command line argument environ set",
        "Answer_original_content":"tri pass lengthi content argument valu pipelin try upload file blob option creat dataset pass dataset file path aml pipelin paramet pipelin step read content file blob",
        "Answer_preprocessed_content":"tri pass lengthi content argument valu pipelin try upload file blob option creat dataset pass dataset file path aml pipelin paramet pipelin step read content file blob",
        "Answer_gpt_summary_original":"Solution: The user was advised to reduce the size of the job's command line arguments and environment settings. Another solution suggested was to upload the file to blob, create a dataset, and then pass on the dataset name or file path to AML pipeline as a parameter. The pipeline step will read the content of the file from the blob.",
        "Answer_gpt_summary":"solut user advis reduc size job command line argument environ set solut suggest upload file blob creat dataset pass dataset file path aml pipelin paramet pipelin step read content file blob"
    },
    {
        "Question_title":"Issues accessing a FileDataset created from HTTP URIs in a PythonScriptStep",
        "Question_body":"<p>I\u2019m having some issues trying to access a FileDataset created from two http URIs in an Azure ML Pipeline PythonScriptStep.<\/p>\n<p>In the step, I\u2019m only getting a single file named <code>['https%3A\u2019]<\/code> when doing an <code>os.listdir()<\/code> on my mount point. I would have expected two files, with their actual names instead. This happens both when sending the dataset <code>as_upload<\/code> and <code>as_mount<\/code>. Even happens when I send the dataset reference to the pipeline step and mount it directly from the step.<\/p>\n<p>The dataset is registered in a notebook, the same notebook that creates and invokes the pipeline, as seen below:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>tempFileData = Dataset.File.from_files(\n        ['https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg',\n        'https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg'])\ntempFileData.register(ws, name='FileData', create_new_version=True)\n\n#...\n\nread_datasets_step = PythonScriptStep(\n    name='The Dataset Reader',\n    script_name='read-datasets.py',\n    inputs=[fileData.as_named_input('Files'), fileData.as_named_input('Files_mount').as_mount(), fileData.as_named_input('Files_download').as_download()],\n    compute_target=compute_target,\n    source_directory='.\/dataset-reader',\n    allow_reuse=False,\n)\n\n<\/code><\/pre>\n<p>The <code>FileDataset<\/code> seems to be registered properly, if I examine it within the notebook I get the following result:<\/p>\n<pre class=\"lang-json prettyprint-override\"><code>{\n  &quot;source&quot;: [\n    &quot;https:\/\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot;,\n    &quot;https:\/\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;\n  ],\n  &quot;definition&quot;: [\n    &quot;GetFiles&quot;\n  ],\n  &quot;registration&quot;: {\n    &quot;id&quot;: &quot;...&quot;,\n    &quot;name&quot;: &quot;FileData&quot;,\n    &quot;version&quot;: 4,\n    &quot;workspace&quot;: &quot;Workspace.create(...)&quot;\n  }\n}\n<\/code><\/pre>\n<p>For reference, the machine running the notebook is using AML SDK v1.24, whereas the node running the pipeline steps is running v1.25.<\/p>\n<p>Has anybody encountered anything like this? Is there a way to make it work?<\/p>\n<p>Note that I'm specifically looking at file datasets created from web uris, and not necessarily interested in getting a <code>FileDataset<\/code> to work with blob storage or similar.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1618832324140,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1618849094430,
        "Question_score":1.0,
        "Question_view_count":91.0,
        "Answer_body":"<p>The files should've been mounted at path &quot;https%3A\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg&quot; and &quot;https%3A\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg&quot;.<\/p>\n<p>We retain the directory structure following the url structure to avoid potential conflicts.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67161293",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1618855169223,
        "Question_original_content":"issu access filedataset creat http uri pythonscriptstep have issu try access filedataset creat http uri pipelin pythonscriptstep step get singl file name http listdir mount point expect file actual name instead happen send dataset upload mount happen send dataset refer pipelin step mount directli step dataset regist notebook notebook creat invok pipelin seen tempfiledata dataset file file http vladiliescu net imag deploi model azur pipelin jpg http vladiliescu net imag revers engin autom jpg tempfiledata regist filedata creat new version true read dataset step pythonscriptstep dataset reader script read dataset input filedata name input file filedata name input file mount mount filedata name input file download download comput target comput target sourc directori dataset reader allow reus fals filedataset regist properli examin notebook follow result sourc http vladiliescu net imag deploi model azur pipelin jpg http vladiliescu net imag revers engin autom jpg definit getfil registr filedata version workspac workspac creat refer machin run notebook aml sdk node run pipelin step run anybodi encount like wai work note specif look file dataset creat web uri necessarili interest get filedataset work blob storag similar",
        "Question_preprocessed_content":"issu access filedataset creat http uri pythonscriptstep have issu try access filedataset creat http uri pipelin pythonscriptstep step get singl file name mount point expect file actual name instead happen send dataset happen send dataset refer pipelin step mount directli step dataset regist notebook notebook creat invok pipelin seen regist properli examin notebook follow result refer machin run notebook aml sdk node run pipelin step run anybodi encount like wai work note specif look file dataset creat web uri necessarili interest get work blob storag similar",
        "Question_gpt_summary_original":"The user is facing issues accessing a FileDataset created from two http URIs in an Azure ML Pipeline PythonScriptStep. When doing an os.listdir() on the mount point, the user is only getting a single file named ['https%3A'] instead of two files with their actual names. This happens both when sending the dataset as_upload and as_mount. The dataset is registered properly, but the issue persists. The machine running the notebook is using AML SDK v1.24, whereas the node running the pipeline steps is running v1.25.",
        "Question_gpt_summary":"user face issu access filedataset creat http uri pipelin pythonscriptstep listdir mount point user get singl file name http instead file actual name happen send dataset upload mount dataset regist properli issu persist machin run notebook aml sdk node run pipelin step run",
        "Answer_original_content":"file mount path http vladiliescu net imag deploi model azur pipelin jpg http vladiliescu net imag revers engin autom jpg retain directori structur follow url structur avoid potenti conflict",
        "Answer_preprocessed_content":"file mount path retain directori structur follow url structur avoid potenti conflict",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to retain the directory structure following the URL structure to avoid potential conflicts. The files should have been mounted at the path \"https%3A\/vladiliescu.net\/images\/deploying-models-with-azure-ml-pipelines.jpg\" and \"https%3A\/vladiliescu.net\/images\/reverse-engineering-automated-ml.jpg\".",
        "Answer_gpt_summary":"solut solut mention discuss retain directori structur follow url structur avoid potenti conflict file mount path http vladiliescu net imag deploi model azur pipelin jpg http vladiliescu net imag revers engin autom jpg"
    },
    {
        "Question_title":"memory outage while running module",
        "Question_body":"I am encountering an issue of error 0138, while training the data, at the end it shows memory has been exhausted exception\n\nI do not think my data has exceed the limit of azure ML studio, is there any way to solve this?",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1653902834057,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @darya-9510\n\nThanks for reaching out to us. This issue seldoms happen. Could you please share your structure to us and how is your dataset size? Based on the error info, too many steps in your experiment may cause that.\n\nI would suggest you try to remove some unnecessary one to try and see. If you believe your structure is reasonable, please share it to us. But it should be fine if you have not put too much.\n\nRegards,\nYutong\n\n-Please kindly accept the answer to help the community if you feel helpful, thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/869594\/memory-outage-while-running-module.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1653942581987,
        "Question_original_content":"memori outag run modul encount issu error train data end show memori exhaust except think data exce limit studio wai solv",
        "Question_preprocessed_content":"memori outag run modul encount issu error train data end show memori exhaust except think data exce limit studio wai solv",
        "Question_gpt_summary_original":"The user is facing an error 0138 while training data, which indicates a memory outage. They are unsure if their data has exceeded the limit of Azure ML studio and are seeking a solution to the problem.",
        "Question_gpt_summary":"user face error train data indic memori outag unsur data exceed limit studio seek solut problem",
        "Answer_original_content":"hello darya thank reach issu seldom happen share structur dataset size base error info step experi caus suggest try remov unnecessari try believ structur reason share fine regard yutong kindli accept answer help commun feel help thank",
        "Answer_preprocessed_content":"hello thank reach issu seldom happen share structur dataset size base error info step experi caus suggest try remov unnecessari try believ structur reason share fine regard yutong kindli accept answer help commun feel help thank",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to remove unnecessary steps from the experiment to reduce the dataset size and avoid memory outage. However, no specific solution is provided for checking if the data has exceeded the limit of Azure ML studio.",
        "Answer_gpt_summary":"solut solut suggest discuss remov unnecessari step experi reduc dataset size avoid memori outag specif solut provid check data exceed limit studio"
    },
    {
        "Question_title":"Why is env different in an Azure ML notbook and an Azure ML terminal?",
        "Question_body":"<p>I'm using MS Azure ML and have found that when I start a Notebook (from the Azure ML Studio) it is executing in a a different environment than if I create a Python script and run it from the studio. I want to be able to create a specific environment and have the Notebook use that. The environment that the Notebook seems to run does not contain the packages I need and I want to preserve different environments.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1641247442447,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":225.0,
        "Answer_body":"<p>First open a terminal, using the same compute target as you want to use with your Notebook afterwards, and to use and <strong>existing environment<\/strong> you can do:<\/p>\n<pre><code>conda activate existing_env\nconda install ipykernel\npython -m ipykernel install --user --name existing_env --display-name &quot;Python 3.8 - Existing Environment&quot;   \n<\/code><\/pre>\n<p>However, to create a <strong>new environment<\/strong> and use it in you AzureML Notebook, you have to do the following commands:<\/p>\n<pre><code>conda create --name new_env python=3.8\nconda activate new_env\nconda install pip\nconda install ipykernel\npython -m ipykernel install --user --name new_env --display-name &quot;Python 3.8 - New Environment&quot;\n<\/code><\/pre>\n<p>And then last, but not least, you have to edit the Jupyter Kernel display names:<\/p>\n<p><strong>IMPORTANT<\/strong> Please ensure you are comfortable running all these steps:<\/p>\n<pre><code>jupyter kernelspec list\ncd &lt;folder-that-matches-the-kernel-of-your-environment&gt;\nsudo nano kernel.json\n<\/code><\/pre>\n<p>Then edit the name to match what you want and save the file.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1641841183276,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70571948",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1641553768432,
        "Question_original_content":"env differ notbook termin start notebook studio execut differ environ creat python script run studio want abl creat specif environ notebook us environ notebook run contain packag need want preserv differ environ",
        "Question_preprocessed_content":"env differ notbook termin start notebook execut differ environ creat python script run studio want abl creat specif environ notebook us environ notebook run contain packag need want preserv differ environ",
        "Question_gpt_summary_original":"The user is facing challenges with the environment in Azure ML, as the Notebook and Python script seem to be executing in different environments. The Notebook environment does not contain the necessary packages, and the user wants to create a specific environment to be used in the Notebook.",
        "Question_gpt_summary":"user face challeng environ notebook python script execut differ environ notebook environ contain necessari packag user want creat specif environ notebook",
        "Answer_original_content":"open termin comput target want us notebook us exist environ conda activ exist env conda instal ipykernel python ipykernel instal user exist env displai python exist environ creat new environ us notebook follow command conda creat new env python conda activ new env conda instal pip conda instal ipykernel python ipykernel instal user new env displai python new environ edit jupyt kernel displai name import ensur comfort run step jupyt kernelspec list sudo nano kernel json edit match want save file",
        "Answer_preprocessed_content":"open termin comput target want us notebook us exist environ creat new environ us notebook follow command edit jupyt kernel displai name import ensur comfort run step edit match want save file",
        "Answer_gpt_summary_original":"Solution:\n- To use an existing environment in AzureML Notebook, open a terminal, activate the environment, install ipykernel, and install the kernel with a display name.\n- To create a new environment, create a new environment, activate it, install pip and ipykernel, and install the kernel with a display name.\n- Edit the Jupyter Kernel display names to match the desired name.",
        "Answer_gpt_summary":"solut us exist environ notebook open termin activ environ instal ipykernel instal kernel displai creat new environ creat new environ activ instal pip ipykernel instal kernel displai edit jupyt kernel displai name match desir"
    },
    {
        "Question_title":"Using Azure Storage libraries in AzureML - Custom python library",
        "Question_body":"<p>I am sort of new to Python, so I probably don't understand fully how to exactly import the libraries correctly into Azure ML.<\/p>\n\n<p>I have a bunch of data stored in Table storage which I have local Python code to successfully join all of them as a preparation for the ML experiment. I learned that AzureML environment does not have the Azure-Storage libraries installed, and therefore procceded the steps according <a href=\"https:\/\/msdn.microsoft.com\/en-us\/library\/azure\/dn955437.aspx\" rel=\"nofollow noreferrer\">this<\/a> to upload a ZIP file containing the Azure-storage libraries that I found under anaconda3\\lib\\site-packages. I took all of the azure directories and shoved them under one single zip file and followed the bottom of the document in the link to upload the zip file as a DataSet and attach the dataset to an Execute Python script node in ML.<\/p>\n\n<p>I am getting errors like this when I try to run the node:<\/p>\n\n<pre><code>requestId = 825883c7ccb74f7e869e68e60d3cd919 errorComponent=Module. taskStatusCode=400. e \"C:\\pyhome\\lib\\ssl.py\", line 856, in send return self._sslobj.write(data) File \"C:\\pyhome\\lib\\ssl.py\", line 581, in write return self._sslobj.write(data)socket.timeout: The write operation timed outDuring handling of the above exception, another exception occurred:Traceback (most recent call last): File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 376, in send timeout=timeout File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 609, in urlopen _stacktrace=sys.exc_info()[2]) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\util\\retry.py\", line 247, in increment raise six.reraise(type(error), error, _stacktrace) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\packages\\six.py\", line 309, in reraise raise value.with_traceback(tb) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 559, in urlopen body=body, headers=headers) File \"C:\\pyhome\\lib\\site-packages\\requests\\packages\\urllib3\\connectionpool.py\", line 353, in _make_request conn.request(method, url, **httplib_request_kw) File \"C:\\pyhome\\lib\\http\\client.py\", line 1083, in request self._send_request(method, url, body, headers) File \"C:\\pyhome\\lib\\http\\client.py\", line 1128, in _send_request self.endheaders(body) File \"C:\\pyhome\\lib\\http\\client.py\", line 1079, in endheaders self._send_output(message_body) File \"C:\\pyhome\\lib\\http\\client.py\", line 911, in _send_output self.send(msg) File \"C:\\pyhome\\lib\\http\\client.py\", line 885, in send self.sock.sendall(data) File \"C:\\pyhome\\lib\\ssl.py\", line 886, in sendall v = self.send(data[count:]) File \"C:\\pyhome\\lib\\ssl.py\", line 856, in send return self._sslobj.write(data) File \"C:\\pyhome\\lib\\ssl.py\", line 581, in write return self._sslobj.write(data)requests.packages.urllib3.exceptions.ProtocolError: ('Connection aborted.', timeout('The write operation timed out',))During handling of the above exception, another exception occurred:Traceback (most recent call last): File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 221, in _perform_request response = self._httpclient.perform_request(request) File \"c:\\temp\\script bundle\\azure\\storage\\_http\\httpclient.py\", line 114, in perform_request proxies=self.proxies) File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 468, in request resp = self.send(prep, **send_kwargs) File \"C:\\pyhome\\lib\\site-packages\\requests\\sessions.py\", line 576, in send r = adapter.send(request, **kwargs) File \"C:\\pyhome\\lib\\site-packages\\requests\\adapters.py\", line 426, in send raise ConnectionError(err, request=request)requests.exceptions.ConnectionError: ('Connection aborted.', timeout('The write operation timed out',))During handling of the above exception, another exception occurred:Traceback (most recent call last): File \"C:\\server\\invokepy.py\", line 199, in batch odfs = mod.azureml_main(*idfs) File \"C:\\temp\\fa22884a19884f658d411dc0bdf05715.py\", line 33, in azureml_main data = table_service.query_entities(table_name) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 728, in query_entities resp = self._query_entities(*args, **kwargs) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 795, in _query_entities operation_context=_context) File \"c:\\temp\\script bundle\\azure\\storage\\table\\tableservice.py\", line 1093, in _perform_request return super(TableService, self)._perform_request(request, parser, parser_args, operation_context) File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 279, in _perform_request raise ex File \"c:\\temp\\script bundle\\azure\\storage\\storageclient.py\", line 251, in _perform_request raise AzureException(ex.args[0])azure.common.AzureException: ('Connection aborted.', timeout('The write operation timed out',))Process returned with non-zero exit code \n<\/code><\/pre>\n\n<p>I am not sure what I am doing wrong<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1511629802230,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":150.0,
        "Answer_body":"<p>In Azure ML Studio, Python scripts (and R scripts for that matter) run in a sandbox so cannot access resources over the network. See <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio\/execute-python-scripts?#limitations\" rel=\"nofollow noreferrer\">limitations<\/a>:<\/p>\n<blockquote>\n<p>The Execute Python Script currently has the following limitations:<\/p>\n<ol>\n<li>Sandboxed execution. The Python runtime is currently sandboxed and, as\na result, does not allow access to the network...<\/li>\n<\/ol>\n<\/blockquote>\n<p>So if you want to read from a blob use the separate Import Data module and if you want to write to a blob use the separate Export Data module.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1645949683923,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/47488544",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1511661620620,
        "Question_original_content":"azur storag librari custom python librari sort new python probabl understand fulli exactli import librari correctli bunch data store tabl storag local python code successfulli join prepar experi learn environ azur storag librari instal procced step accord upload zip file contain azur storag librari anaconda lib site packag took azur directori shove singl zip file follow document link upload zip file dataset attach dataset execut python script node get error like try run node requestid cccbfeeedcd errorcompon modul taskstatuscod pyhom lib ssl line send return self sslobj write data file pyhom lib ssl line write return self sslobj write data socket timeout write oper time outdur handl except except occur traceback recent file pyhom lib site packag request adapt line send timeout timeout file pyhom lib site packag request packag urllib connectionpool line urlopen stacktrac sy exc info file pyhom lib site packag request packag urllib util retri line increment rais rerais type error error stacktrac file pyhom lib site packag request packag urllib packag line rerais rais valu traceback file pyhom lib site packag request packag urllib connectionpool line urlopen bodi bodi header header file pyhom lib site packag request packag urllib connectionpool line request conn request method url httplib request file pyhom lib http client line request self send request method url bodi header file pyhom lib http client line send request self endhead bodi file pyhom lib http client line endhead self send output messag bodi file pyhom lib http client line send output self send msg file pyhom lib http client line send self sock sendal data file pyhom lib ssl line sendal self send data count file pyhom lib ssl line send return self sslobj write data file pyhom lib ssl line write return self sslobj write data request packag urllib except protocolerror connect abort timeout write oper time handl except except occur traceback recent file temp script bundl azur storag storagecli line perform request respons self httpclient perform request request file temp script bundl azur storag http httpclient line perform request proxi self proxi file pyhom lib site packag request session line request resp self send prep send kwarg file pyhom lib site packag request session line send adapt send request kwarg file pyhom lib site packag request adapt line send rais connectionerror err request request request except connectionerror connect abort timeout write oper time handl except except occur traceback recent file server invokepi line batch odf mod main idf file temp faafddcbdf line main data tabl servic queri entiti tabl file temp script bundl azur storag tabl tableservic line queri entiti resp self queri entiti arg kwarg file temp script bundl azur storag tabl tableservic line queri entiti oper context context file temp script bundl azur storag tabl tableservic line perform request return super tableservic self perform request request parser parser arg oper context file temp script bundl azur storag storagecli line perform request rais file temp script bundl azur storag storagecli line perform request rais azureexcept arg azur common azureexcept connect abort timeout write oper time process return non zero exit code sure wrong",
        "Question_preprocessed_content":"azur storag librari custom python librari sort new python probabl understand fulli exactli import librari correctli bunch data store tabl storag local python code successfulli join prepar experi learn environ librari instal procced step accord upload zip file contain librari took azur directori shove singl zip file follow document link upload zip file dataset attach dataset execut python script node get error like try run node sure wrong",
        "Question_gpt_summary_original":"The user is facing challenges in importing Azure-Storage libraries correctly into Azure ML. They have uploaded a ZIP file containing the libraries as a DataSet and attached it to an Execute Python script node in ML. However, they are encountering errors related to connection timeouts when trying to run the node. The user is unsure of what they are doing wrong.",
        "Question_gpt_summary":"user face challeng import azur storag librari correctli upload zip file contain librari dataset attach execut python script node encount error relat connect timeout try run node user unsur wrong",
        "Answer_original_content":"studio python script script matter run sandbox access resourc network limit execut python script current follow limit sandbox execut python runtim current sandbox result allow access network want read blob us separ import data modul want write blob us separ export data modul",
        "Answer_preprocessed_content":"studio python script run sandbox access resourc network limit execut python script current follow limit sandbox execut python runtim current sandbox result allow access want read blob us separ import data modul want write blob us separ export data modul",
        "Answer_gpt_summary_original":"Solution: The user is encountering errors related to connection timeouts when trying to run the Execute Python script node in Azure ML. This is because Python scripts run in a sandbox in Azure ML Studio and cannot access resources over the network. To read from a blob, the user should use the Import Data module, and to write to a blob, they should use the Export Data module.",
        "Answer_gpt_summary":"solut user encount error relat connect timeout try run execut python script node python script run sandbox studio access resourc network read blob user us import data modul write blob us export data modul"
    },
    {
        "Question_title":"How to plot history of training metrics in Sagemaker .py training",
        "Question_body":"<p>I am running a notebook in Sagemaker and I use a .py file for training:<\/p>\n<pre><code>tf_estimator = TensorFlow(entry_point='train_cnn.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='local',  #We use the local instance\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1} #One epoch just to check everything is ok\n                         )\n\n#We fit the model with the data\ntf_estimator.fit({'training': training_input_path, 'validation': validation_input_path})\n<\/code><\/pre>\n<p>In the train_cnn file I use a standard CNN. However, the last part of the file indicates to plot the history of the training like this:<\/p>\n<pre><code>model.compile(loss=tensorflow.keras.losses.binary_crossentropy,\n            optimizer=Adam(lr=lr),\n            metrics=['accuracy'])\n\nsnn=model.fit(train_images, train_labels, batch_size=batch_size,\n            validation_data=(test_images, test_labels),\n            epochs=epochs,\n            verbose=2)\n\nscore = model.evaluate(test_images, test_labels, verbose=0)\nprint('Validation loss    :', score[0])\nprint('Validation accuracy:', score[1])\n   \nplt.figure(0)\nplt.plot(snn.history['acc'], 'r')\nplt.plot(snn.history['val_acc'], 'g')\nplt.xticks(np.arange(0, 11, 2.0))  \nplt.rcParams['figure.figsize'] = (8, 6)  \nplt.xlabel(&quot;Num of Epochs&quot;)  \nplt.ylabel(&quot;Accuracy&quot;)  \nplt.title(&quot;Training Accuracy&quot;)  \nplt.legend(['train', 'validation'])\nplt.figure(1)  \nplt.plot(snn.history['loss'],'r')  \nplt.plot(snn.history['val_loss'],'g')  \nplt.xticks(np.arange(0, 11, 2.0))  \nplt.rcParams['figure.figsize'] = (8, 6)  \nplt.xlabel(&quot;Num of Epochs&quot;)  \nplt.ylabel(&quot;Loss&quot;)  \nplt.title(&quot;Training Loss vs Validation Loss&quot;)  \nplt.legend(['train','validation'])\nplt.show()  \n<\/code><\/pre>\n<p>However, there is nothing displayed and the training shows a succes. Maybe because is performed in another instance. Here the displayed information:<\/p>\n<pre><code>Epoch 1\/1\nalgo-1-tn2vd_1  |  - 2s - loss: 0.8858 - acc: 0.4615 - val_loss: 3.0894 - val_acc: 0.5000\nalgo-1-tn2vd_1  | Validation loss    : 3.0894343852996826\nalgo-1-tn2vd_1  | Validation accuracy: 0.5\nalgo-1-tn2vd_1  | WARNING:tensorflow:From \/usr\/local\/lib\/python3.6\/dist-packages\/tensorflow\/python\/saved_model\/simple_save.py:85: calling SavedModelBuilder.add_meta_graph_and_variables (from tensorflow.python.saved_model.builder_impl) with legacy_init_op is deprecated and will be removed in a future version.\nalgo-1-tn2vd_1  | Instructions for updating:\nalgo-1-tn2vd_1  | Pass your op to the equivalent parameter main_op instead.\nalgo-1-tn2vd_1  | 2020-07-12 00:42:23,538 sagemaker-containers INFO     Reporting training SUCCESS\ntmpuzys_qpc_algo-1-tn2vd_1 exited with code 0\nAborting on container exit...\n===== Job Complete =====\n<\/code><\/pre>\n<p>Is there a way to <strong>plot the history of the training job?<\/strong> For instances, like the one in the next Figure <a href=\"https:\/\/i.stack.imgur.com\/LUDe7.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/LUDe7.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1594515152440,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":877.0,
        "Answer_body":"<p>A SageMaker training job in &quot;local&quot; is actually executing <em>inside of a Docker container<\/em> that is isolated from the Python kernel that is executing your notebook. Therefore, the <code>plt.show()<\/code> in the <code>train_cnn.py<\/code> script doesn't actually get routed to the notebook UI in the same way that executing that command directly from a notebook would.<\/p>\n<p>Instead of using <code>plt.show()<\/code>, consider using <a href=\"https:\/\/matplotlib.org\/3.1.1\/api\/_as_gen\/matplotlib.pyplot.savefig.html\" rel=\"nofollow noreferrer\">plt.savefig()<\/a> to output the plot to an image:<\/p>\n<pre><code>plt.savefig(&quot;training_results.png&quot;)  \n<\/code><\/pre>\n<p>Upon termination of the training container, SageMaker will zip up all the output artifacts (including the plot) and ship them to S3 in your training script. Alternatively, you could upload the plot straight to S3 -- see <a href=\"https:\/\/stackoverflow.com\/questions\/31485660\/python-uploading-a-plot-from-memory-to-s3-using-matplotlib-and-boto\">python - uploading a plot from memory to s3 using matplotlib and boto<\/a> for an example of this.<\/p>\n<p>As a side note: have you considered using TensorBoard? It can offer a better experience for browsing results of a training script, and SageMaker should have a first class integration to make it easy to enable. Take a look at the <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html#sagemaker.tensorflow.estimator.TensorFlow.fit\" rel=\"nofollow noreferrer\"><code>run_tensorboard_locally<\/code> argument<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/62856049",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1594657730596,
        "Question_original_content":"plot histori train metric train run notebook us file train estim tensorflow entri point train cnn role role train instanc count train instanc type local us local instanc framework version version script mode true hyperparamet epoch epoch check fit model data estim fit train train input path valid valid input path train cnn file us standard cnn file indic plot histori train like model compil loss tensorflow kera loss binari crossentropi optim adam metric accuraci snn model fit train imag train label batch size batch size valid data test imag test label epoch epoch verbos score model evalu test imag test label verbos print valid loss score print valid accuraci score plt figur plt plot snn histori acc plt plot snn histori val acc plt xtick arang plt rcparam figur figsiz plt xlabel num epoch plt ylabel accuraci plt titl train accuraci plt legend train valid plt figur plt plot snn histori loss plt plot snn histori val loss plt xtick arang plt rcparam figur figsiz plt xlabel num epoch plt ylabel loss plt titl train loss valid loss plt legend train valid plt displai train show succ mayb perform instanc displai inform epoch algo tnvd loss acc val loss val acc algo tnvd valid loss algo tnvd valid accuraci algo tnvd warn tensorflow usr local lib python dist packag tensorflow python save model simpl save call savedmodelbuild add meta graph variabl tensorflow python save model builder impl legaci init deprec remov futur version algo tnvd instruct updat algo tnvd pass equival paramet main instead algo tnvd contain info report train success tmpuzi qpc algo tnvd exit code abort contain exit job complet wai plot histori train job instanc like figur",
        "Question_preprocessed_content":"plot histori train metric train run notebook us file train file us standard cnn file indic plot histori train like displai train show succ mayb perform instanc displai inform wai plot histori train job instanc like figur",
        "Question_gpt_summary_original":"The user is facing a challenge in plotting the history of a training job in Sagemaker using a .py file for training. The user has implemented a standard CNN in the train_cnn file and has included code to plot the history of the training, but nothing is displayed. The training is successful, but the user suspects that the plot is not displayed because it is performed in another instance. The user is seeking a way to plot the history of the training job.",
        "Question_gpt_summary":"user face challeng plot histori train job file train user implement standard cnn train cnn file includ code plot histori train displai train success user suspect plot displai perform instanc user seek wai plot histori train job",
        "Answer_original_content":"train job local actual execut insid docker contain isol python kernel execut notebook plt train cnn script actual rout notebook wai execut command directli notebook instead plt consid plt savefig output plot imag plt savefig train result png termin train contain zip output artifact includ plot ship train script altern upload plot straight python upload plot memori matplotlib boto exampl note consid tensorboard offer better experi brows result train script class integr easi enabl look run tensorboard local argument",
        "Answer_preprocessed_content":"train job local actual execut insid docker contain isol python kernel execut notebook script actual rout notebook wai execut command directli notebook instead consid output plot imag termin train contain zip output artifact ship train script altern upload plot straight python upload plot memori matplotlib boto exampl note consid tensorboard offer better experi brows result train script class integr easi enabl look argument",
        "Answer_gpt_summary_original":"The solution suggested in the discussion is to use `plt.savefig()` instead of `plt.show()` to output the plot to an image. Upon termination of the training container, SageMaker will zip up all the output artifacts (including the plot) and ship them to S3 in the training script. Alternatively, the user could upload the plot straight to S3. The discussion also suggests using TensorBoard as it can offer a better experience for browsing results of a training script, and SageMaker should have a first-class integration to make it easy to enable.",
        "Answer_gpt_summary":"solut suggest discuss us plt savefig instead plt output plot imag termin train contain zip output artifact includ plot ship train script altern user upload plot straight discuss suggest tensorboard offer better experi brows result train script class integr easi enabl"
    },
    {
        "Question_title":"deploy model and expose model as web service via azure machine learning + azuremlsdk in R",
        "Question_body":"<p>I am trying to follow <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/articles\/train-and-deploy-first-model.html\" rel=\"nofollow noreferrer\">this post<\/a> to deploy a &quot;model&quot; in Azure.<\/p>\n<p>A code snipet is as follows and the model, which is simply a function adding 2 numbers, seems to register fine. I don't even use the model to isolate the problem after 1000s of attempts as this scoring code shows:<\/p>\n<pre><code>library(jsonlite)\n\ninit &lt;- function()\n{\n  message(&quot;hello world&quot;)\n  \n  function(data)\n  {\n    vars &lt;- as.data.frame(fromJSON(data))\n    prediction &lt;- 2\n    toJSON(prediction)\n  }\n}\n<\/code><\/pre>\n<p>Should be fine shouldn't it? Any way I run this code snippet:<\/p>\n<pre><code>r_env &lt;- r_environment(name = &quot;basic_env&quot;)\ninference_config &lt;- inference_config(\n  entry_script = &quot;score.R&quot;,\n  source_directory = &quot;.&quot;,\n  environment = r_env)\n\naci_config &lt;- aci_webservice_deployment_config(cpu_cores = 1, memory_gb = 0.5)\n\naci_service &lt;- deploy_model(ws, \n                            'xxxxx', \n                            list(model), \n                            inference_config, \n                            aci_config)\n\nwait_for_deployment(aci_service, show_output = TRUE)\n<\/code><\/pre>\n<p>Which produces this (after a looooong time):<\/p>\n<pre><code>Running.....................................................................\nFailed\nService deployment polling reached non-successful terminal state, current service state: Failed\nOperation ID: 14c35064-7ff4-46aa-9bfa-ab8a63218a2c\nMore information can be found using '.get_logs()'\nError:\n{\n  &quot;code&quot;: &quot;AciDeploymentFailed&quot;,\n  &quot;statusCode&quot;: 400,\n  &quot;message&quot;: &quot;Aci Deployment failed with exception: Error in entry script, RuntimeError: Error in file(filename, \\&quot;r\\&quot;, encoding = encoding) : , please run print(service.get_logs()) to get details.&quot;,\n  &quot;details&quot;: [\n    {\n      &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n      &quot;message&quot;: &quot;Error in entry script, RuntimeError: Error in file(filename, \\&quot;r\\&quot;, encoding = encoding) : , please run print(service.get_logs()) to get details.&quot;\n    }\n  ]\n}\n<\/code><\/pre>\n<p>It does not tell me much. Not sure how to debug this further? How can I run this:<\/p>\n<pre><code>print(service.get_logs())\n<\/code><\/pre>\n<p>and where please? Guess this is a Python artifact? Any other input very much welcome.<\/p>\n<p>PS:<\/p>\n<p>At this point in time, I have my suspicion that the above R entry file definition is not what is expected these days. Looking at the Python equivalent taken from <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=azcli\" rel=\"nofollow noreferrer\">here<\/a>:<\/p>\n<pre><code>import json\n\ndef init():\n    print(&quot;This is init&quot;)\n\ndef run(data):\n    test = json.loads(data)\n    print(f&quot;received data {test}&quot;)\n    return f&quot;test is {test}&quot;\n<\/code><\/pre>\n<p>Would something like this not be more suitable (tried it without success).<\/p>\n<pre><code>library(jsonlite)\n\ninit &lt;- function()\n{\n    message(&quot;hello world&quot;)\n}\n\ninit &lt;- function()\n{\n    return(42)\n}\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1620998276233,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1621008123540,
        "Question_score":2.0,
        "Question_view_count":282.0,
        "Answer_body":"<p>Great to see people putting the R SDK through it's paces!<\/p>\n<p>The vignette you're using is obviously a great way to get started. It seems you're almost all the way through without a hitch.<\/p>\n<p>Deployment is always tricky, and I'm not expert myself. I'd point you to this <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-troubleshoot-deployment-local?WT.mc_id=AI-MVP-5003930\" rel=\"nofollow noreferrer\">guide on troubleshooting deployment locally<\/a>. Similar functionality exists for the R SDK, namely: <a href=\"https:\/\/azure.github.io\/azureml-sdk-for-r\/reference\/local_webservice_deployment_config.html\" rel=\"nofollow noreferrer\"><code>local_webservice_deployment_config()<\/code><\/a>.<\/p>\n<p>So I think you change your example to this:<\/p>\n<pre class=\"lang-r prettyprint-override\"><code>deployment_config &lt;- local_webservice_deployment_config(port = 8890)\n<\/code><\/pre>\n<p>Once you know the service is working locally, the issue you're having with the ACI webservice becomes a lot easier to narrow down.<\/p>",
        "Answer_comment_count":8.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/67535014",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1621007625123,
        "Question_original_content":"deploi model expos model web servic sdk try follow post deploi model azur code snipet follow model simpli function ad number regist fine us model isol problem attempt score code show librari jsonlit init function messag hello world function data var data frame fromjson data predict tojson predict fine shouldn wai run code snippet env environ basic env infer config infer config entri script score sourc directori environ env aci config aci webservic deploy config cpu core memori aci servic deploi model list model infer config aci config wait deploy aci servic output true produc loong time run fail servic deploy poll reach non success termin state current servic state fail oper bfa abaac inform log error code acideploymentfail statuscod messag aci deploy fail except error entri script runtimeerror error file filenam encod encod run print servic log detail detail code crashloopbackoff messag error entri script runtimeerror error file filenam encod encod run print servic log detail tell sure debug run print servic log guess python artifact input welcom point time suspicion entri file definit expect dai look python equival taken import json def init print init def run data test json load data print receiv data test return test test like suitabl tri success librari jsonlit init function messag hello world init function return",
        "Question_preprocessed_content":"deploi model expos model web servic sdk try follow post deploi model azur code snipet follow model simpli function ad number regist fine us model isol problem attempt score code show fine shouldn wai run code snippet produc tell sure debug run guess python artifact input welcom point time suspicion entri file definit expect dai look python equival taken like suitabl",
        "Question_gpt_summary_original":"The user is encountering challenges while trying to deploy a model as a web service via Azure Machine Learning and AzureMLSDK in R. The user's code snippet seems to register fine, but the deployment fails with a non-successful terminal state error message. The error message does not provide much information, and the user is unsure how to debug it further. The user suspects that the R entry file definition may not be what is expected. The user is seeking advice on how to run the print(service.get_logs()) command and where to run it.",
        "Question_gpt_summary":"user encount challeng try deploi model web servic sdk user code snippet regist fine deploy fail non success termin state error messag error messag provid inform user unsur debug user suspect entri file definit expect user seek advic run print servic log command run",
        "Answer_original_content":"great peopl put sdk pace vignett obvious great wai start wai hitch deploy tricki expert point guid troubleshoot deploy local similar function exist sdk local webservic deploy config think chang exampl deploy config local webservic deploy config port know servic work local issu have aci webservic lot easier narrow",
        "Answer_preprocessed_content":"great peopl put sdk pace vignett obvious great wai start wai hitch deploy tricki expert point guid troubleshoot deploy local similar function exist sdk think chang exampl know servic work local issu have aci webservic lot easier narrow",
        "Answer_gpt_summary_original":"Solution: The user is advised to refer to the troubleshooting guide on deployment locally and use the local_webservice_deployment_config() function in the R SDK to configure the deployment. Once the service is working locally, the issue with the ACI webservice can be narrowed down.",
        "Answer_gpt_summary":"solut user advis refer troubleshoot guid deploy local us local webservic deploy config function sdk configur deploy servic work local issu aci webservic narrow"
    },
    {
        "Question_title":"Unable to use the same HumanWorkflow within textract for more than 1 file\/call",
        "Question_body":"I created the private team from the Amazon SageMaker console for labeling tasks followed by the creation of the human review workflow, which I later integrated with the Amazon Textract for Key-Value pair extraction.\n\nWhile I called the analyze_document (along with HumanLoop configuration) to extract key-value pairs for the first time it worked as expected and I was able to see the Job in the labeling project console. However, when I called it again (irrespective of the same or different file) the HumanLoop started giving the below error.\n\n\"[ERROR] InvalidParameterException: An error occurred (InvalidParameterException) when calling the AnalyzeDocument operation: HumanLoop 'textractworkflow1' already exists and it is associated with a different InputContent. Please use a new HumanLoopName and try your request again.\"\n\nDo we have to create a new Human Review Loop each time we trigger analyze_document with another file?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1639923120530,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":50.0,
        "Answer_body":"I think you are using the same human loop name for multiple tasks and that is causing this issue. You have to make sure that within the HumanLoopConfig configuration, the HumanLoopName should be unique for each task. You can also refer to this video",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUDgM77ZgnTbWjW57_v9rCGw\/unable-to-use-the-same-human-workflow-within-textract-for-more-than-1-file-call",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1640758908043,
        "Question_original_content":"unabl us humanworkflow textract file creat privat team consol label task follow creation human review workflow later integr amazon textract kei valu pair extract call analyz document humanloop configur extract kei valu pair time work expect abl job label project consol call irrespect differ file humanloop start give error error invalidparameterexcept error occur invalidparameterexcept call analyzedocu oper humanloop textractworkflow exist associ differ inputcont us new humanloopnam try request creat new human review loop time trigger analyz document file",
        "Question_preprocessed_content":"unabl us humanworkflow textract creat privat team consol label task follow creation human review workflow later integr amazon textract pair extract call extract pair time work expect abl job label project consol call humanloop start give error error invalidparameterexcept error occur call analyzedocu oper humanloop textractworkflow exist associ differ inputcont us new humanloopnam try request creat new human review loop time trigger file",
        "Question_gpt_summary_original":"The user is facing challenges while using the same HumanWorkflow within Textract for more than one file\/call. The HumanLoop is giving an error message stating that it already exists and is associated with a different InputContent, which is preventing the user from using it for another file. The user is unsure if they need to create a new Human Review Loop each time they trigger analyze_document with another file.",
        "Question_gpt_summary":"user face challeng humanworkflow textract file humanloop give error messag state exist associ differ inputcont prevent user file user unsur need creat new human review loop time trigger analyz document file",
        "Answer_original_content":"think human loop multipl task caus issu sure humanloopconfig configur humanloopnam uniqu task refer video",
        "Answer_preprocessed_content":"think human loop multipl task caus issu sure humanloopconfig configur humanloopnam uniqu task refer video",
        "Answer_gpt_summary_original":"Solution: The user needs to ensure that the HumanLoopName is unique for each task within the HumanLoopConfig configuration. This will prevent the error message from appearing and allow the user to use the HumanWorkflow for multiple files\/calls. A video reference is also provided for further assistance.",
        "Answer_gpt_summary":"solut user need ensur humanloopnam uniqu task humanloopconfig configur prevent error messag appear allow user us humanworkflow multipl file call video refer provid assist"
    },
    {
        "Question_title":"How could I save dataset from ipython notebook in Azure ML Studio?",
        "Question_body":"<p>I use next command to save output results:<\/p>\n\n<pre><code>ws.datasets.add_from_dataframe(data, 'GenericCSV', 'output.csv', 'Uotput results')\n<\/code><\/pre>\n\n<p>where <code>ws<\/code> is <code>azureml.Workspace<\/code> object and <code>data<\/code> is <code>pandas.DataFrame<\/code>.<\/p>\n\n<p>It works fine if my dataset size less than 4 mb. Otherwise I got a error:<\/p>\n\n<pre><code>AzureMLHttpError: Maximum request length exceeded.\n<\/code><\/pre>\n\n<p>As I understood this is the error raised by Azure environment limits and the maximum size of the dataset could not be changed. <\/p>\n\n<p>I could split my dataset to 4 mb parts and download them from Azure ML studio, but it is very inconvinient if size of my output dataset is more than 400 mb.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_created_time":1457889087747,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1457963855452,
        "Question_score":4.0,
        "Question_view_count":3128.0,
        "Answer_body":"<p>I have read the source code in the python package <strong>azureml<\/strong>, and found out that they are using a simple request post when uploading a dataset, which has a limited content length 4194304 bytes.<\/p>\n\n<p>I tried to modify the code inside \"http.py\" within the python package <strong>azureml<\/strong>. I posted the request with a chunked data, and I got the following error:<\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \".\\azuremltest.py\", line 10, in &lt;module&gt;\n    ws.datasets.add_from_dataframe(frame, 'GenericCSV', 'output2.csv', 'Uotput results')\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\__init__.py\", line 507, in add_from_dataframe\n    return self._upload(raw_data, data_type_id, name, description)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\__init__.py\", line 550, in _upload\nraw_data, None)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\http.py\", line 135, in upload_dataset\n    upload_result = self._send_post_req(api_path, raw_data)\n  File \"C:\\Python34\\lib\\site-packages\\azureml\\http.py\", line 197, in _send_post_req\n    raise AzureMLHttpError(response.text, response.status_code)\nazureml.errors.AzureMLHttpError: Chunked transfer encoding is not permitted. Upload size must be indicated in the Content-Length header.\nRequest ID: 7b692d82-845c-4106-b8ec-896a91ecdf2d 2016-03-14 04:32:55Z\n<\/code><\/pre>\n\n<p>The REST API in <strong>azureml<\/strong> package does not support chunked transfer encoding. Hence, I took a look at how the Azure ML studio implements this, and I found out this:<\/p>\n\n<ol>\n<li><p>It post a request with content-length=0 to <code>https:\/\/studioapi.azureml.net\/api\/resourceuploads\/workspaces\/&lt;workspace_id&gt;\/?userStorage=true&amp;dataTypeId=GenericCSV<\/code>, which will return an id in the response body.<\/p><\/li>\n<li><p>Break the .csv file into chunks less than 4194304 bytes, and post them to <code>https:\/\/studioapi.azureml.net\/api\/blobuploads\/workspaces\/&lt;workspace_id&gt;\/?numberOfBlocks=&lt;the number of chunks&gt;&amp;blockId=&lt;index of chunk&gt;&amp;uploadId=&lt;the id you get from previous request&gt;&amp;dataTypeId=GenericCSV<\/code><\/p><\/li>\n<\/ol>\n\n<p>If you really want this functionality, you can implement it with python and the above REST API.<\/p>\n\n<p>If you think it's too complicated, report the issue to <a href=\"https:\/\/github.com\/Azure\/Azure-MachineLearning-ClientLibrary-Python\/issues\" rel=\"nofollow\">this<\/a>. The <strong>azureml<\/strong> python package is still under development, so your suggestion would be very helpful for them.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1457939067983,
        "Answer_score":4.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/35973168",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1457935312407,
        "Question_original_content":"save dataset ipython notebook studio us command save output result dataset add datafram data genericcsv output csv uotput result workspac object data panda datafram work fine dataset size got error httperror maximum request length exceed understood error rais azur environ limit maximum size dataset chang split dataset part download studio inconvini size output dataset",
        "Question_preprocessed_content":"save dataset ipython notebook studio us command save output result object work fine dataset size got error understood error rais azur environ limit maximum size dataset chang split dataset part download studio inconvini size output dataset",
        "Question_gpt_summary_original":"The user is facing a challenge in saving a dataset from ipython notebook in Azure ML Studio. The command used to save the output results works fine for datasets less than 4 mb, but for larger datasets, the user encounters an error message indicating that the maximum request length has been exceeded. The user understands that this is a limitation of the Azure environment and cannot be changed. The user is considering splitting the dataset into 4 mb parts, but this would be inconvenient for datasets larger than 400 mb.",
        "Question_gpt_summary":"user face challeng save dataset ipython notebook studio command save output result work fine dataset larger dataset user encount error messag indic maximum request length exceed user understand limit azur environ chang user consid split dataset part inconveni dataset larger",
        "Answer_original_content":"read sourc code python packag simpl request post upload dataset limit content length byte tri modifi code insid http python packag post request chunk data got follow error traceback recent file test line dataset add datafram frame genericcsv output csv uotput result file python lib site packag init line add datafram return self upload raw data data type descript file python lib site packag init line upload raw data file python lib site packag http line upload dataset upload result self send post req api path raw data file python lib site packag http line send post req rais httperror respons text respons statu code error httperror chunk transfer encod permit upload size indic content length header request bec aecdfd rest api packag support chunk transfer encod took look studio implement post request content length http studioapi net api resourceupload workspac userstorag true datatypeid genericcsv return respons bodi break csv file chunk byte post http studioapi net api blobupload workspac numberofblock blockid uploadid datatypeid genericcsv want function implement python rest api think complic report issu python packag develop suggest help",
        "Answer_preprocessed_content":"read sourc code python packag simpl request post upload dataset limit content length byte tri modifi code insid python packag post request chunk data got follow error rest api packag support chunk transfer encod took look studio implement post request return respons bodi break csv file chunk byte post want function implement python rest api think complic report issu python packag develop suggest help",
        "Answer_gpt_summary_original":"Solution:\n- The REST API in the azureml package does not support chunked transfer encoding. Hence, the user can implement the functionality with python and the REST API provided by Azure ML Studio.\n- Alternatively, the user can report the issue to the Azure-MachineLearning-ClientLibrary-Python repository on GitHub.",
        "Answer_gpt_summary":"solut rest api packag support chunk transfer encod user implement function python rest api provid studio altern user report issu azur machinelearn clientlibrari python repositori github"
    },
    {
        "Question_title":"Pytorch CUDA OutOfMemory Error while training",
        "Question_body":"<p>I'm trying to train a PyTorch FLAIR model in AWS Sagemaker.\nWhile doing so getting the following error:<\/p>\n<pre><code>RuntimeError: CUDA out of memory. Tried to allocate 84.00 MiB (GPU 0; 11.17 GiB total capacity; 9.29 GiB already allocated; 7.31 MiB free; 10.80 GiB reserved in total by PyTorch)\n<\/code><\/pre>\n<p>For training I used sagemaker.pytorch.estimator.PyTorch class.<\/p>\n<p>I tried with different variants of instance types from ml.m5, g4dn to p3(even with a 96GB memory one).\nIn the ml.m5 getting the error with CPUmemoryIssue, in g4dn with GPUMemoryIssue and in the P3 getting GPUMemoryIssue mostly because Pytorch is using only one of the GPU of 12GB out of 8*12GB.<\/p>\n<p>Not getting anywhere to complete this training, even in local tried with a CPU machine and got the following error:<\/p>\n<pre><code>RuntimeError: [enforce fail at ..\\c10\\core\\CPUAllocator.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 67108864 bytes. Buy new RAM!\n<\/code><\/pre>\n<p>The model training script:<\/p>\n<pre><code>    corpus = ClassificationCorpus(data_folder, test_file='..\/data\/exports\/val.csv', train_file='..\/data\/exports\/train.csv')\n                                          \n    print(&quot;finished loading corpus&quot;)\n\n    word_embeddings = [WordEmbeddings('glove'), FlairEmbeddings('news-forward-fast'), FlairEmbeddings('news-backward-fast')]\n\n    document_embeddings = DocumentLSTMEmbeddings(word_embeddings, hidden_size=512, reproject_words=True, reproject_words_dimension=256)\n\n    classifier = TextClassifier(document_embeddings, label_dictionary=corpus.make_label_dictionary(), multi_label=False)\n\n    trainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n\n    trainer.train('..\/model_files', max_epochs=12,learning_rate=0.0001, train_with_dev=False, embeddings_storage_mode=&quot;none&quot;)\n<\/code><\/pre>\n<p>P.S.: I was able to train the same architecture with a smaller dataset in my local GPU machine with a 4GB GTX 1650 DDR5 memory and it was really quick.<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":4,
        "Question_created_time":1597607305160,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597613347630,
        "Question_score":2.0,
        "Question_view_count":2033.0,
        "Answer_body":"<p>Okay, so after 2 days of continuous debugging was able to find out the root cause.\nWhat I understood is Flair does not have any limitation on the sentence length, in the sense the word count, it is taking the highest length sentence as the maximum.\nSo there it was causing issue, as in my case there were few content with 1.5 lakh rows which is too much to load the embedding of into the memory, even a 16GB GPU.\nSo there it was breaking.<\/p>\n<p><strong>To solve this<\/strong>: For content with this much lengthy words, you can take chunk of n words(10K in my case) from these kind of content from any portion(left\/right\/middle anywhere) and trunk the rest, or simply ignore those records for training if it is very minimal in comparative count.<\/p>\n<p>After this I hope you will be able to progress with your training, as it happened in my case.<\/p>\n<p>P.S.: If you are following this thread and face similar issue feel free to comment back so that I can explore and help on your case of the issue.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63441299",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1597771718656,
        "Question_original_content":"pytorch cuda outofmemori error train try train pytorch flair model get follow error runtimeerror cuda memori tri alloc mib gpu gib total capac gib alloc mib free gib reserv total pytorch train pytorch estim pytorch class tri differ variant instanc type gdn memori get error cpumemoryissu gdn gpumemoryissu get gpumemoryissu pytorch gpu get complet train local tri cpu machin got follow error runtimeerror enforc fail core cpualloc cpp data defaultcpualloc memori tri alloc byte bui new ram model train script corpu classificationcorpu data folder test file data export val csv train file data export train csv print finish load corpu word embed wordembed glove flairembed new forward fast flairembed new backward fast document embed documentlstmembed word embed hidden size reproject word true reproject word dimens classifi textclassifi document embed label dictionari corpu label dictionari multi label fals trainer modeltrain classifi corpu optim adam trainer train model file max epoch learn rate train dev fals embed storag mode abl train architectur smaller dataset local gpu machin gtx ddr memori quick",
        "Question_preprocessed_content":"pytorch cuda outofmemori error train try train pytorch flair model get follow error train class tri differ variant instanc type memori get error cpumemoryissu gpumemoryissu get gpumemoryissu pytorch gpu get complet train local tri cpu machin got follow error model train script abl train architectur smaller dataset local gpu machin gtx ddr memori quick",
        "Question_gpt_summary_original":"The user is encountering CUDA out of memory errors while training a PyTorch FLAIR model in AWS Sagemaker. They have tried different instance types but still face memory issues, including CPU memory issues and GPU memory issues. The user also encountered a memory error while training the model on a local CPU machine. The model training script includes the use of word embeddings and document embeddings. The user was able to train the same architecture with a smaller dataset on their local GPU machine with 4GB memory.",
        "Question_gpt_summary":"user encount cuda memori error train pytorch flair model tri differ instanc type face memori issu includ cpu memori issu gpu memori issu user encount memori error train model local cpu machin model train script includ us word embed document embed user abl train architectur smaller dataset local gpu machin memori",
        "Answer_original_content":"okai dai continu debug abl root caus understood flair limit sentenc length sens word count take highest length sentenc maximum caus issu case content lakh row load embed memori gpu break solv content lengthi word chunk word case kind content portion left right middl trunk rest simpli ignor record train minim compar count hope abl progress train happen case follow thread face similar issu feel free comment explor help case issu",
        "Answer_preprocessed_content":"okai dai continu debug abl root caus understood flair limit sentenc length sens word count take highest length sentenc maximum caus issu case content lakh row load embed memori gpu break solv content lengthi word chunk word kind content trunk rest simpli ignor record train minim compar count hope abl progress train happen case follow thread face similar issu feel free comment explor help case issu",
        "Answer_gpt_summary_original":"Solution: The user found that the root cause of the CUDA out of memory errors was due to the high length of some sentences in the dataset, which caused issues when loading the embeddings into memory. To solve this, the user suggests taking a chunk of n words from these lengthy sentences and truncating the rest, or simply ignoring those records for training if they are minimal in count. This solution helped the user to progress with their training.",
        "Answer_gpt_summary":"solut user root caus cuda memori error high length sentenc dataset caus issu load embed memori solv user suggest take chunk word lengthi sentenc truncat rest simpli ignor record train minim count solut help user progress train"
    },
    {
        "Question_title":"Endpoint stuck in 'transitioning' state",
        "Question_body":"I'm trying to use Azure ML to host an image classification model trained in lobe.ai (externally trained model).\n\nI've used the 'no code' model deployment approach described here\n\nI've been able to authenticate my workspace and register my TensorFlow model, but the endpoint is stuck on transitioning for over 2 hours.\n\nAny ideas?\n\n from azureml.core import Model\n    \n model = Model.register(workspace=ws,\n                        model_name='cxr',                            # Name of the registered model in your workspace.\n                        model_path='cxr_test',                       # Local Tensorflow SavedModel folder to upload and register as a model.\n                        model_framework=Model.Framework.TENSORFLOW,  # Framework used to create the model.\n                        model_framework_version='1.15.3',            # Version of Tensorflow used to create the model.\n                        description='Pneumonia-prediction model')\n    \n service_name = 'tensorflow-cxr-service'\n service = Model.deploy(ws, service_name, [model])",
        "Question_answer_count":1,
        "Question_comment_count":5,
        "Question_created_time":1604939373570,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"We have created a support ticket for this issue and we will update the solution later. Thanks.\n\n\n\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/156439\/endpoint-stuck-in-39transitioning39-state.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1605125180896,
        "Question_original_content":"endpoint stuck transit state try us host imag classif model train lobe extern train model code model deploy approach describ abl authent workspac regist tensorflow model endpoint stuck transit hour idea core import model model model regist workspac model cxr regist model workspac model path cxr test local tensorflow savedmodel folder upload regist model model framework model framework tensorflow framework creat model model framework version version tensorflow creat model descript pneumonia predict model servic tensorflow cxr servic servic model deploi servic model",
        "Question_preprocessed_content":"endpoint stuck transit state try us host imag classif model train code model deploy approach describ abl authent workspac regist tensorflow model endpoint stuck transit hour idea core import model model regist model workspac local tensorflow savedmodel folder upload regist model framework creat model version tensorflow creat model model servic",
        "Question_gpt_summary_original":"The user is encountering a challenge with Azure ML where the endpoint for their image classification model is stuck in a 'transitioning' state for over 2 hours. They have registered their TensorFlow model and used a 'no code' deployment approach, but are seeking ideas for resolving the issue.",
        "Question_gpt_summary":"user encount challeng endpoint imag classif model stuck transit state hour regist tensorflow model code deploy approach seek idea resolv issu",
        "Answer_original_content":"creat support ticket issu updat solut later thank regard yutong",
        "Answer_preprocessed_content":"creat support ticket issu updat solut later thank regard yutong",
        "Answer_gpt_summary_original":"Solution: No solution provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"(UserError) Error when parsing request; unable to deserialize request body",
        "Question_body":"Hi, getting this error when i run an azureml experiment with custom_docker_image (basegpu image of mcr) - can anybody help me understand this? Have tested this in local compute and it works, not sure why this does not work on a training cluster vm?\n\n\n\n\n    azureml._restclient.exceptions.ServiceException: ServiceException:\n         Code: 400\n         Message: (UserError) Error when parsing request; unable to deserialize request body\n         Details:\n        \n         Headers: {\n             \"Date\": \"Mon, 08 Jun 2020 11:03:52 GMT\",\n             \"Content-Type\": \"application\/json; charset=utf-8\",\n             \"Transfer-Encoding\": \"chunked\",\n             \"Connection\": \"keep-alive\",\n             \"Request-Context\": \"appId=cid-v1:6a27ce65-5555-41a3-85f7-b7a1ce31fd6b\",\n             \"x-ms-response-type\": \"error\",\n             \"Strict-Transport-Security\": \"max-age=15724800; includeSubDomains; preload\"\n         }\n         InnerException: {\n         \"additional_properties\": {},\n         \"error\": {\n             \"additional_properties\": {},\n             \"code\": \"UserError\",\n             \"message\": \"Error when parsing request; unable to deserialize request body\",\n             \"details_uri\": null,\n             \"target\": null,\n             \"details\": [],\n             \"inner_error\": null,\n             \"debug_info\": null,\n             \"message_format\": null,\n             \"message_parameters\": null,\n             \"reference_code\": null\n         },\n         \"correlation\": {\n             \"operation\": \"e96d6285280f5849a4a5e3f172d65d36\",\n             \"request\": \"1beee8ecb7180147\"\n         },\n         \"environment\": \"westeurope\",\n         \"location\": \"westeurope\",\n         \"time\": {}\n     }",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1591616365497,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"My bad, after giving it some days and looking at the code, I noticed i had forgotten to add the parameters for the estimator configuration. Here is the estimator configuration that works for me:\n\n\n\n estimator = Estimator(source_directory=experiment_folder,\n                       compute_target=compute_target,\n                       script_params=script_params,\n                       entry_script='rps_efn_b0.py',\n                       node_count=1,        \n                       conda_packages=['ipykernel'],\n                       pip_packages = ['azureml-sdk',\n                                       'pyarrow',\n                                       'pyspark',\n                                       'azureml-mlflow',\n                                       'joblib',\n                                       'matplotlib',\n                                       'Pillow',\n                                       'tensorflow==2.2',\n                                       'tensorflow-datasets',\n                                       'tensorflow-hub',\n                                       'azureml-defaults',\n                                       'azureml-dataprep[fuse,pandas]'],\n                       custom_docker_image='mcr.microsoft.com\/azureml\/base-gpu:openmpi3.1.2-cuda10.1-cudnn7-ubuntu18.04')",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/33313\/usererror-error-when-parsing-request-unable-to-des.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1591884305636,
        "Question_original_content":"usererror error pars request unabl deseri request bodi get error run experi custom docker imag basegpu imag mcr anybodi help understand test local comput work sure work train cluster restclient except serviceexcept serviceexcept code messag usererror error pars request unabl deseri request bodi detail header date mon jun gmt content type applic json charset utf transfer encod chunk connect aliv request context appid cid ac bacefdb respons type error strict transport secur max ag includesubdomain preload innerexcept addit properti error addit properti code usererror messag error pars request unabl deseri request bodi detail uri null target null detail inner error null debug info null messag format null messag paramet null refer code null correl oper edfaaefdd request becb environ westeurop locat westeurop time",
        "Question_preprocessed_content":"error pars request unabl deseri request bodi get error run experi anybodi help understand test local comput work sure work train cluster serviceexcept code messag error pars request unabl deseri request bodi detail header innerexcept error code usererror messag error pars request unabl deseri request bodi null target null detail null null null null null correl environ westeurop locat westeurop time",
        "Question_gpt_summary_original":"The user encountered an error when running an AzureML experiment with a custom Docker image on a training cluster VM. The error message indicates that there was an issue parsing the request and deserializing the request body, resulting in a 400 error. The user is seeking help to understand why this error occurred.",
        "Question_gpt_summary":"user encount error run experi custom docker imag train cluster error messag indic issu pars request deseri request bodi result error user seek help understand error occur",
        "Answer_original_content":"bad give dai look code notic forgotten add paramet estim configur estim configur work estim estim sourc directori experi folder comput target comput target script param script param entri script rp efn node count conda packag ipykernel pip packag sdk pyarrow pyspark joblib matplotlib pillow tensorflow tensorflow dataset tensorflow hub default dataprep fuse panda custom docker imag mcr microsoft com base gpu openmpi cuda cudnn ubuntu",
        "Answer_preprocessed_content":"bad give dai look code notic forgotten add paramet estim configur estim configur work estim",
        "Answer_gpt_summary_original":"Solution: The user found a solution to the error by adding the parameters for the estimator configuration. They provided the working estimator configuration that includes the necessary parameters for the custom Docker image to run successfully on the training cluster VM.",
        "Answer_gpt_summary":"solut user solut error ad paramet estim configur provid work estim configur includ necessari paramet custom docker imag run successfulli train cluster"
    },
    {
        "Question_title":"SageMaker Ground Truth with TensorFlow",
        "Question_body":"<p>I've seen examples of labeling data using SageMaker Ground Truth and then using that data to train off-the-shelf SageMaker models. However, am I able to use this same annotation format with TensorFlow Script Mode? <\/p>\n\n<p>More specifically, I have a tensorflow.keras model I'm training using TF Script Mode, and I'd like to take data labeled with Ground Truth and convert my script from File mode to Pipe mode.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1548942784467,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":454.0,
        "Answer_body":"<p>I am from Amazon SageMaker Ground Truth team and happy to assist you in your experiment. Just to be clear our understanding, are you running TF model in SageMaker using TF estimator in your own container (<a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst\" rel=\"nofollow noreferrer\">https:\/\/github.com\/aws\/sagemaker-python-sdk\/blob\/master\/src\/sagemaker\/tensorflow\/README.rst<\/a>)? <\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54462105",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1549915976767,
        "Question_original_content":"ground truth tensorflow seen exampl label data ground truth data train shelf model abl us annot format tensorflow script mode specif tensorflow kera model train script mode like data label ground truth convert script file mode pipe mode",
        "Question_preprocessed_content":"ground truth tensorflow seen exampl label data ground truth data train model abl us annot format tensorflow script mode specif model train script mode like data label ground truth convert script file mode pipe mode",
        "Question_gpt_summary_original":"The user is facing a challenge of whether they can use the annotation format of SageMaker Ground Truth with TensorFlow Script Mode. They have a tensorflow.keras model that they want to train using TF Script Mode and want to convert their script from File mode to Pipe mode using data labeled with Ground Truth.",
        "Question_gpt_summary":"user face challeng us annot format ground truth tensorflow script mode tensorflow kera model want train script mode want convert script file mode pipe mode data label ground truth",
        "Answer_original_content":"ground truth team happi assist experi clear understand run model estim contain http github com aw python sdk blob master src tensorflow readm rst",
        "Answer_preprocessed_content":"ground truth team happi assist experi clear understand run model estim contain",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How to update the existing web service with a new docker image on Azure Machine Learning Services?",
        "Question_body":"<p>I am currently working on machine learning project with Azure Machine Learning Services. But I found the problem that I can't update a new docker image to the existing web service (I want to same url as running we service). <\/p>\n\n<p>I have read the documentation but it doesn't really tell me how to update (documentation link: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where<\/a>). \nThe documentation said that we have to use update() with image = new-image. <\/p>\n\n<pre><code>from azureml.core.webservice import Webservice\n\nservice_name = 'aci-mnist-3\n\n# Retrieve existing service\nservice = Webservice(name = service_name, workspace = ws)\n\n# Update the image used by the service\nservice.update(image = new-image)\n\nprint(service.state)\n<\/code><\/pre>\n\n<p>But the <code>new-image<\/code> isn't described where it comes from. <\/p>\n\n<p>Does anyone know how to figure out this problem?<\/p>\n\n<p>Thank you<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1544435800590,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1554674817316,
        "Question_score":2.0,
        "Question_view_count":281.0,
        "Answer_body":"<p>The documentation could be a little more clear on this part, I agree. The <code>new-image<\/code> is an image object that you should pass into the <code>update()<\/code> function. If you just created the image you might already have the object in a variable, then just pass it. If not, then you can obtain it from your workspace using<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.image.image import Image\nnew_image = Image(ws, image_name)\n<\/code><\/pre>\n\n<p>where <code>ws<\/code> is your workspace object and <code>image_name<\/code> is a string with the name of the image you want to obtain. Then you go on calling <code>update()<\/code> as<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core.webservice import Webservice\n\nservice_name = 'aci-mnist-3'\n\n# Retrieve existing service\nservice = Webservice(name = service_name, workspace = ws)\n\n# Update the image used by the service\nservice.update(image = new_image) # Note that dash isn't supported in variable names\n\nprint(service.state)\n<\/code><\/pre>\n\n<p>You can find more information in the <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/overview\/azure\/ml\/intro?view=azure-ml-py\" rel=\"nofollow noreferrer\">SDK documentation<\/a><\/p>\n\n<p>EDIT:\nBoth the <code>Image<\/code> and the <code>Webservice<\/code> classes above are abstract parent classes.<\/p>\n\n<p>For the <code>Image<\/code> object, you should really use one of these classes, depending on your case:<\/p>\n\n<ul>\n<li><code>ContainerImage<\/code><\/li>\n<li><code>UnknownImage<\/code><\/li>\n<\/ul>\n\n<p>(see <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.image?view=azure-ml-py\" rel=\"nofollow noreferrer\">Image package<\/a> in the documentation).<\/p>\n\n<p>For the <code>Webservice<\/code> object, you should use one of these classes, depending on your case:<\/p>\n\n<ul>\n<li><code>AciWebservice<\/code><\/li>\n<li><code>AksWebservice<\/code><\/li>\n<li><code>UnknownWebservice<\/code><\/li>\n<\/ul>\n\n<p>(see <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.webservice?view=azure-ml-py\" rel=\"nofollow noreferrer\">Webservice package<\/a> in the documentation).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1566684334863,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53703191",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1544528992767,
        "Question_original_content":"updat exist web servic new docker imag servic current work machin learn project servic problem updat new docker imag exist web servic want url run servic read document tell updat document link http doc microsoft com azur machin learn servic deploi document said us updat imag new imag core webservic import webservic servic aci mnist retriev exist servic servic webservic servic workspac updat imag servic servic updat imag new imag print servic state new imag isn describ come know figur problem thank",
        "Question_preprocessed_content":"updat exist web servic new docker imag servic current work machin learn project servic problem updat new docker imag exist web servic read document tell updat document said us updat imag isn describ come know figur problem thank",
        "Question_gpt_summary_original":"The user is facing a challenge in updating a new docker image to an existing web service on Azure Machine Learning Services. The documentation does not provide clear instructions on how to update the image, and the user is unsure where the \"new-image\" comes from in the code provided. The user is seeking assistance in resolving this issue.",
        "Question_gpt_summary":"user face challeng updat new docker imag exist web servic servic document provid clear instruct updat imag user unsur new imag come code provid user seek assist resolv issu",
        "Answer_original_content":"document littl clear agre new imag imag object pass updat function creat imag object variabl pass obtain workspac core imag imag import imag new imag imag imag workspac object imag string imag want obtain call updat core webservic import webservic servic aci mnist retriev exist servic servic webservic servic workspac updat imag servic servic updat imag new imag note dash isn support variabl name print servic state inform sdk document edit imag webservic class abstract parent class imag object us class depend case containerimag unknownimag imag packag document webservic object us class depend case aciwebservic akswebservic unknownwebservic webservic packag document",
        "Answer_preprocessed_content":"document littl clear agre imag object pass function creat imag object variabl pass obtain workspac workspac object string imag want obtain call inform sdk document edit class abstract parent class object us class depend case imag packag document object us class depend case webservic packag document",
        "Answer_gpt_summary_original":"Solution:\nThe user can obtain the image object from the workspace using the Image class and then pass it into the update() function of the Webservice class. The Image class can be instantiated using the ContainerImage or UnknownImage class depending on the case, and the Webservice class can be instantiated using the AciWebservice, AksWebservice, or UnknownWebservice class depending on the case. The SDK documentation provides more information on this.",
        "Answer_gpt_summary":"solut user obtain imag object workspac imag class pass updat function webservic class imag class instanti containerimag unknownimag class depend case webservic class instanti aciwebservic akswebservic unknownwebservic class depend case sdk document provid inform"
    },
    {
        "Question_title":"matplotlib does not upgrade beyound 3.2.1",
        "Question_body":"<p>I am using Notebook on Azure Machine Learning Studio and I am trying to install latest version of matplotlib, but it continues using version 3.2.1 I tried conda, pip, and also tried to uninstall and install again, nothing works.\nsample commands tried:<\/p>\n<pre><code>!pip install matplotlib\n!conda install matplotlib\n<\/code><\/pre>\n<p>Am I missing sth? thanks<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":6,
        "Question_created_time":1658878043067,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":68.0,
        "Answer_body":"<p>The version of <strong>Matplotlib<\/strong> will depend on the version of python. There is no support for the latest version of matplotlib library for <strong>python version 3.8<\/strong>. The latest version of matplotlib can be available only up to <strong>3.7 version<\/strong> of python. Even though we tried to upgrade it with <strong>pip, conda<\/strong> , by default <strong>version of python program<\/strong>, it will take the supporting version of matplotlib.<\/p>\n<p>For the current version of matplotlib in the current case of the situation, it is <strong>3.2<\/strong>. It is suggestable to check the version of python, whether it is 3.6 or not. <strong>Python version 3.6 supports 3.2 and 3.1 versions of matplotlib<\/strong>.<\/p>\n<p>If still the python version is showing the latest version of python, try to <strong>degrade<\/strong> the version and check.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73130723",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659334721720,
        "Question_original_content":"matplotlib upgrad beyound notebook studio try instal latest version matplotlib continu version tri conda pip tri uninstal instal work sampl command tri pip instal matplotlib conda instal matplotlib miss sth thank",
        "Question_preprocessed_content":"matplotlib upgrad beyound notebook studio try instal latest version matplotlib continu version tri conda pip tri uninstal instal work sampl command tri miss sth thank",
        "Question_gpt_summary_original":"The user is facing challenges upgrading matplotlib beyond version 3.2.1 on Azure Machine Learning Studio Notebook. The user has tried various methods such as conda, pip, and uninstalling and reinstalling, but none of them have worked.",
        "Question_gpt_summary":"user face challeng upgrad matplotlib version studio notebook user tri method conda pip uninstal reinstal work",
        "Answer_original_content":"version matplotlib depend version python support latest version matplotlib librari python version latest version matplotlib avail version python tri upgrad pip conda default version python program support version matplotlib current version matplotlib current case situat suggest check version python python version support version matplotlib python version show latest version python try degrad version check",
        "Answer_preprocessed_content":"version matplotlib depend version python support latest version matplotlib librari python version latest version matplotlib avail version python tri upgrad pip conda default version python program support version matplotlib current version matplotlib current case situat suggest check version python python version support version matplotlib python version show latest version python try degrad version check",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n- The latest version of Matplotlib library is only available up to version 3.7 of Python, so upgrading beyond version 3.2.1 may not be possible.\n- Matplotlib version 3.2 is currently available in the situation, and it is suggested to check if the Python version is 3.6, which supports versions 3.2 and 3.1 of Matplotlib.\n- If the Python version is still showing the latest version, try downgrading the version and check.",
        "Answer_gpt_summary":"possibl solut mention discuss latest version matplotlib librari avail version python upgrad version possibl matplotlib version current avail situat suggest check python version support version matplotlib python version show latest version try downgrad version check"
    },
    {
        "Question_title":"Is there a way for Optuna `suggest_categorical`to return multiple choices from list?",
        "Question_body":"<p>I am using Optuna for hyperparametrization of my model. And i have a field where I want to test multiple combinations from a list. For example: I have <code>[&quot;lisa&quot;,&quot;adam&quot;,&quot;test&quot;]<\/code> and i want <code>suggest_categorical<\/code> to return not just one, but a random combination: maybe <code>[&quot;lisa&quot;, &quot;adam&quot;]<\/code>, maybe <code>[&quot;adam&quot;]<\/code>, maybe <code>[&quot;lisa&quot;, &quot;adam&quot;, &quot;test&quot;]<\/code>. Is there a way to get this with built in Optuna function?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1660737627797,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":56.0,
        "Answer_body":"<p>You could use <code>itertools.combinations<\/code> to generate all possible combinations of list items and then pass them to optuna's <code>suggest_categorical<\/code> as choices:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>import optuna\nimport itertools\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# generate the combinations\niterable = ['lisa', 'adam', 'test']\ncombinations = []\nfor r in range(1, len(iterable) + 1):\n    combinations.extend([list(x) for x in itertools.combinations(iterable=iterable, r=r)])\nprint(combinations)\n# [['lisa'], ['adam'], ['test'], ['lisa', 'adam'], ['lisa', 'test'], ['adam', 'test'], ['lisa', 'adam', 'test']]\n\n# sample the combinations\ndef objective(trial):\n    combination = trial.suggest_categorical(name='combination', choices=combinations)\n    return round(random.random(), 2)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=3)\n# [I 2022-08-18 08:03:51,658] A new study created in memory with name: no-name-3874ce95-2394-4526-bb19-0d9822d7e45c\n# [I 2022-08-18 08:03:51,659] Trial 0 finished with value: 0.94 and parameters: {'combination': ['adam']}. Best is trial 0 with value: 0.94.\n# [I 2022-08-18 08:03:51,660] Trial 1 finished with value: 0.87 and parameters: {'combination': ['lisa', 'test']}. Best is trial 1 with value: 0.87.\n# [I 2022-08-18 08:03:51,660] Trial 2 finished with value: 0.29 and parameters: {'combination': ['lisa', 'adam']}. Best is trial 2 with value: 0.29.\n<\/code><\/pre>\n<p>Using lists as choices in optuna's <code>suggest_categorical<\/code> throws a warning message, but apparently this is mostly inconsequential (see <a href=\"https:\/\/github.com\/optuna\/optuna\/issues\/2341\" rel=\"nofollow noreferrer\">this issue<\/a> in optuna's GitHub repository).<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1660803027380,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73388133",
        "Tool":"Optuna",
        "Platform":"Stack Overflow",
        "Question_closed_time":1660802705056,
        "Question_original_content":"wai suggest categor return multipl choic list hyperparametr model field want test multipl combin list exampl lisa adam test want suggest categor return random combin mayb lisa adam mayb adam mayb lisa adam test wai built function",
        "Question_preprocessed_content":"wai return multipl choic list hyperparametr model field want test multipl combin list exampl want return random combin mayb mayb mayb wai built function",
        "Question_gpt_summary_original":"The user is facing a challenge with using Optuna for hyperparameter optimization of their model. They want to test multiple combinations from a list using the `suggest_categorical` function, but the function only returns one choice at a time. The user is looking for a way to get a random combination of choices from the list using the built-in Optuna function.",
        "Question_gpt_summary":"user face challeng hyperparamet optim model want test multipl combin list suggest categor function function return choic time user look wai random combin choic list built function",
        "Answer_original_content":"us itertool combin gener possibl combin list item pass suggest categor choic import import itertool import random import warn warn filterwarn ignor gener combin iter lisa adam test combin rang len iter combin extend list itertool combin iter iter print combin lisa adam test lisa adam lisa test adam test lisa adam test sampl combin def object trial combin trial suggest categor combin choic combin return round random random studi creat studi studi optim object trial new studi creat memori ddec trial finish valu paramet combin adam best trial valu trial finish valu paramet combin lisa test best trial valu trial finish valu paramet combin lisa adam best trial valu list choic suggest categor throw warn messag appar inconsequenti issu github repositori",
        "Answer_preprocessed_content":"us gener possibl combin list item pass choic list choic throw warn messag appar inconsequenti",
        "Answer_gpt_summary_original":"Solution: One possible solution is to use `itertools.combinations` to generate all possible combinations of list items and then pass them to Optuna's `suggest_categorical` as choices. The combinations can be sampled using the `objective` function and the `create_study` function from Optuna. However, using lists as choices in Optuna's `suggest_categorical` throws a warning message, but this is mostly inconsequential.",
        "Answer_gpt_summary":"solut possibl solut us itertool combin gener possibl combin list item pass suggest categor choic combin sampl object function creat studi function list choic suggest categor throw warn messag inconsequenti"
    },
    {
        "Question_title":"Reloading from checkpoing during AWS Sagemaker Training",
        "Question_body":"<p>Sagemaker is a great tool to train your models, and we save some money by using AWS spot instances. However, training jobs sometimes get stopped in the middle. We are using some mechanisms to continue from the latest checkpoint after a restart. See also the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/model-checkpoints.html\" rel=\"nofollow noreferrer\">docs<\/a>.<\/p>\n<p>Still, how do you efficiently test such a mechanism? Can you trigger it yourself? Otherwise you have to wait until the spot instance actually \u00eds restarted.<\/p>\n<p>Also, are you expected to use the linked <code>checkpoint_s3_uri<\/code> argument or the <code>model_dir<\/code> for this? E.g. the <code>TensorFlow<\/code> estimator <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/tensorflow\/sagemaker.tensorflow.html#tensorflow-estimator\" rel=\"nofollow noreferrer\">docs<\/a> seem to suggest something <code>model_dir<\/code>for checkpoints.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1616592352363,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1616592807080,
        "Question_score":0.0,
        "Question_view_count":286.0,
        "Answer_body":"<p>Since you can't manually terminate a sagemaker instance, run an Amazon SageMaker Managed Spot training for a small number of epochs, Amazon SageMaker would have backed up your checkpoint files to S3. Check that checkpoints are there. Now run a second training run, but this time provide the first jobs\u2019 checkpoint location to <code>checkpoint_s3_uri<\/code>. Reference is <a href=\"https:\/\/towardsdatascience.com\/a-quick-guide-to-using-spot-instances-with-amazon-sagemaker-b9cfb3a44a68\" rel=\"nofollow noreferrer\">here<\/a>, this also answer your second question.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66782040",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1616662041463,
        "Question_original_content":"reload checkpo train great tool train model save monei aw spot instanc train job stop middl mechan continu latest checkpoint restart doc effici test mechan trigger wait spot instanc actual restart expect us link checkpoint uri argument model dir tensorflow estim doc suggest model dirfor checkpoint",
        "Question_preprocessed_content":"reload checkpo train great tool train model save monei aw spot instanc train job stop middl mechan continu latest checkpoint restart doc effici test mechan trigger wait spot instanc actual restart expect us link argument estim doc suggest checkpoint",
        "Question_gpt_summary_original":"The user is facing challenges with training jobs getting stopped in the middle while using AWS Sagemaker and spot instances. They are using mechanisms to continue from the latest checkpoint after a restart, but are unsure how to efficiently test this mechanism and whether to use the checkpoint_s3_uri or model_dir argument.",
        "Question_gpt_summary":"user face challeng train job get stop middl spot instanc mechan continu latest checkpoint restart unsur effici test mechan us checkpoint uri model dir argument",
        "Answer_original_content":"manual termin instanc run manag spot train small number epoch back checkpoint file check checkpoint run second train run time provid job checkpoint locat checkpoint uri refer answer second question",
        "Answer_preprocessed_content":"manual termin instanc run manag spot train small number epoch back checkpoint file check checkpoint run second train run time provid job checkpoint locat refer answer second question",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to run an Amazon SageMaker Managed Spot training for a small number of epochs and check if the checkpoint files are backed up to S3. Then, run a second training run and provide the first job's checkpoint location to the checkpoint_s3_uri argument. This solution can help test the mechanism of continuing from the latest checkpoint after a restart and also answers the question of whether to use the checkpoint_s3_uri or model_dir argument.",
        "Answer_gpt_summary":"solut possibl solut mention discuss run manag spot train small number epoch check checkpoint file back run second train run provid job checkpoint locat checkpoint uri argument solut help test mechan continu latest checkpoint restart answer question us checkpoint uri model dir argument"
    },
    {
        "Question_title":"In Azure ML Studio, score model doesn't return predicted values from an R model",
        "Question_body":"<p>I built a multiclass SVM model in R and used Create R model module from azure to train and predict my testing dataset. Here are the trainer and the score R scripts.<\/p>\n\n<p><strong>Trainer R script:<\/strong> <\/p>\n\n<pre><code>library(e1071)\nfeatures &lt;- get.feature.columns(dataset)\nlabels   &lt;- as.factor(get.label.column(dataset))\ntrain.data &lt;- data.frame(features, labels)\nfeature.names &lt;- get.feature.column.names(dataset)\nnames(train.data) &lt;- c(feature.names, \"Class\")\nmodel &lt;- svm(Class ~ . , train.data)\n<\/code><\/pre>\n\n<p><strong>Scores R script:<\/strong><\/p>\n\n<pre><code>library(e1071)    \nclasses &lt;- predict(model, dataset)\nclasses &lt;- as.factor(classes)\nres &lt;- data.frame(classes, probabilities = 0.5)\nprint(str(res))\nprint(res)\nscores &lt;- res\n<\/code><\/pre>\n\n<p>Note in my code, I hardcoded the probability values to simplify the code.<\/p>\n\n<p>Here is my component design in Azure: <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/hjMC4.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/hjMC4.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>When I run the experiment, all the components work fine. However, in the score model, the scored dataset port does not show the predicted values. It only shows feature values from the testing dataset. I checked the output log of <em>Score model<\/em> and I could see the model has nicely predicted the testing data (note I added print commands in the Scores R script). But this is not enough and I need the prediction returned from the score model so I can pass it via API.<\/p>\n\n<p>Has anyone faced this issue before?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1533539693247,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1533560847647,
        "Question_score":0.0,
        "Question_view_count":657.0,
        "Answer_body":"<p>I found an answer for this. In fact, I cannot see the result in the outcome of the scoring model but when I linked it to a <em>select column in the dataset<\/em> module, I see the predicted columns there.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/51702359",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1534294797823,
        "Question_original_content":"studio score model return predict valu model built multiclass svm model creat model modul azur train predict test dataset trainer score script trainer script librari featur featur column dataset label factor label column dataset train data data frame featur label featur name featur column name dataset name train data featur name class model svm class train data score script librari class predict model dataset class factor class re data frame class probabl print str re print re score re note code hardcod probabl valu simplifi code compon design azur run experi compon work fine score model score dataset port predict valu show featur valu test dataset check output log score model model nice predict test data note ad print command score script need predict return score model pass api face issu",
        "Question_preprocessed_content":"studio score model return predict valu model built multiclass svm model creat model modul azur train predict test dataset trainer score script trainer script score script note code hardcod probabl valu simplifi code compon design azur run experi compon work fine score model score dataset port predict valu show featur valu test dataset check output log score model model nice predict test data need predict return score model pass api face issu",
        "Question_gpt_summary_original":"The user built a multiclass SVM model in R and used Azure to train and predict their testing dataset. While the experiment runs fine, the score model does not show the predicted values, only the feature values from the testing dataset. The user checked the output log and saw that the model predicted the testing data, but needs the prediction returned from the score model to pass via API.",
        "Question_gpt_summary":"user built multiclass svm model azur train predict test dataset experi run fine score model predict valu featur valu test dataset user check output log saw model predict test data need predict return score model pass api",
        "Answer_original_content":"answer fact result outcom score model link select column dataset modul predict column",
        "Answer_preprocessed_content":"answer fact result outcom score model link select column dataset modul predict column",
        "Answer_gpt_summary_original":"Solution: The user found a solution to their problem by linking the score model to a \"select column in the dataset\" module, which allowed them to see the predicted columns.",
        "Answer_gpt_summary":"solut user solut problem link score model select column dataset modul allow predict column"
    },
    {
        "Question_title":"Query a table\/database in Athena from a Notebook instance",
        "Question_body":"<p>I have developed different Athena Workgroups for different teams so that I can separate their queries and their query results. The users would like to query the tables available to them from their notebook instances (JupyterLab). I am having difficulty finding code which successfully covers the requirement of querying a table from the user's specific workgroup. I have only found code that will query the table from the primary workgroup. <\/p>\n\n<p>The code I have currently used is added below. <\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from pyathena import connect\nimport pandas as pd\nconn = connect(s3_staging_dir='&lt;ATHENA QUERY RESULTS LOCATION&gt;',\nregion_name='&lt;YOUR REGION, for example, us-west-2&gt;')\n\n\ndf = pd.read_sql(\"SELECT * FROM &lt;DATABASE-NAME&gt;.&lt;YOUR TABLE NAME&gt; limit 8;\", conn)\ndf\n<\/code><\/pre>\n\n<p>This code does not work as the users only have access to perform queries from their specific workgroups hence get errors when this code is run. It also does not cover the requirement of separating the user's queries in user specific workgroups. <\/p>\n\n<p>Any suggestions on how I can add alter the code so that I can run the queries within a specific workgroup from the notebook instance?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1565704743610,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":5367.0,
        "Answer_body":"<p>Documentation of <code>pyathena<\/code> is not super extensive, but after looking into source code we can see that <code>connect<\/code> simply creates instance of <code>Connection<\/code> class.<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def connect(*args, **kwargs):\n    from pyathena.connection import Connection\n    return Connection(*args, **kwargs)\n<\/code><\/pre>\n\n<p>Now, after looking into signature of <code>Connection.__init__<\/code> on <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/blob\/master\/pyathena\/connection.py\" rel=\"nofollow noreferrer\">GitHub<\/a> we can see parameter <code>work_group=None<\/code> which name in the same way as one of the parameters for <code>start_query_execution<\/code> from the <a href=\"https:\/\/boto3.amazonaws.com\/v1\/documentation\/api\/latest\/reference\/services\/athena.html#Athena.Client.start_query_execution\" rel=\"nofollow noreferrer\">official<\/a> AWS Python API <code>boto3<\/code>. Here is what their documentation say about it:<\/p>\n\n<blockquote>\n  <p>WorkGroup (string) -- The name of the workgroup in which the query is being started.<\/p>\n<\/blockquote>\n\n<p>After following through usages and imports in <code>Connection<\/code> we endup with <a href=\"https:\/\/github.com\/laughingman7743\/PyAthena\/blob\/master\/pyathena\/common.py\" rel=\"nofollow noreferrer\">BaseCursor<\/a> class that under the hood makes a call to <code>start_query_execution<\/code> while unpacking a dictionary with parameters assembled by <code>BaseCursor._build_start_query_execution_request<\/code> method. That is excatly where we can see familar syntax for submitting queries to AWS Athena, in particular the following part:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>if self._work_group or work_group:\n    request.update({\n        'WorkGroup': work_group if work_group else self._work_group\n    })\n<\/code><\/pre>\n\n<p>So this should do a trick for your case:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nfrom pyathena import connect\n\n\nconn = connect(\n    s3_staging_dir='&lt;ATHENA QUERY RESULTS LOCATION&gt;',\n    region_name='&lt;YOUR REGION, for example, us-west-2&gt;',\n    work_group='&lt;USER SPECIFIC WORKGROUP&gt;'\n)\n\ndf = pd.read_sql(\"SELECT * FROM &lt;DATABASE-NAME&gt;.&lt;YOUR TABLE NAME&gt; limit 8;\", conn)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1565966644207,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57479389",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1565963773252,
        "Question_original_content":"queri tabl databas athena notebook instanc develop differ athena workgroup differ team separ queri queri result user like queri tabl avail notebook instanc jupyterlab have difficulti find code successfulli cover requir queri tabl user specif workgroup code queri tabl primari workgroup code current ad pyathena import connect import panda conn connect stage dir region read sql select limit conn code work user access perform queri specif workgroup error code run cover requir separ user queri user specif workgroup suggest add alter code run queri specif workgroup notebook instanc",
        "Question_preprocessed_content":"queri athena notebook instanc develop differ athena workgroup differ team separ queri queri result user like queri tabl avail notebook instanc have difficulti find code successfulli cover requir queri tabl user specif workgroup code queri tabl primari workgroup code current ad code work user access perform queri specif workgroup error code run cover requir separ user queri user specif workgroup suggest add alter code run queri specif workgroup notebook instanc",
        "Question_gpt_summary_original":"The user is facing challenges in querying tables from their specific workgroup in Athena using JupyterLab. The current code they have only queries tables from the primary workgroup, which results in errors for users who only have access to their specific workgroup. The user is seeking suggestions on how to modify the code to query tables within a specific workgroup from the notebook instance.",
        "Question_gpt_summary":"user face challeng queri tabl specif workgroup athena jupyterlab current code queri tabl primari workgroup result error user access specif workgroup user seek suggest modifi code queri tabl specif workgroup notebook instanc",
        "Answer_original_content":"document pyathena super extens look sourc code connect simpli creat instanc connect class def connect arg kwarg pyathena connect import connect return connect arg kwarg look signatur connect init github paramet work group wai paramet start queri execut offici aw python api boto document workgroup string workgroup queri start follow usag import connect endup basecursor class hood make start queri execut unpack dictionari paramet assembl basecursor build start queri execut request method excatli familar syntax submit queri aw athena particular follow self work group work group request updat workgroup work group work group self work group trick case import panda pyathena import connect conn connect stage dir region work group read sql select limit conn",
        "Answer_preprocessed_content":"document super extens look sourc code simpli creat instanc class look signatur github paramet wai paramet offici aw python api document workgroup workgroup queri start follow usag import endup basecursor class hood make unpack dictionari paramet assembl method excatli familar syntax submit queri aw athena particular follow trick case",
        "Answer_gpt_summary_original":"The solution suggested in the discussion is to modify the code by adding the parameter \"work_group\" to the \"connect\" function and specifying the user-specific workgroup. This can be done by importing pandas and pyathena, creating a connection object with the specified workgroup, and then using the connection object to query tables from the specific workgroup.",
        "Answer_gpt_summary":"solut suggest discuss modifi code ad paramet work group connect function specifi user specif workgroup import panda pyathena creat connect object specifi workgroup connect object queri tabl specif workgroup"
    },
    {
        "Question_title":"Customer Error: imread read blank (None) image for file- Sagemaker AWS",
        "Question_body":"<p>I am following this <a href=\"https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_mscoco_multi_label\/Image-classification-multilabel-lst.ipynb\" rel=\"nofollow noreferrer\">tutorial<\/a> with my custom data and my custom S3 buckets where train and validation data are. I am getting the following error:<\/p>\n<pre><code>Customer Error: imread read blank (None) image for file: \/opt\/ml\/input\/data\/train\/s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n<\/code><\/pre>\n<p>I have all my training data are in one folder named '<code>train<\/code>' I have set up my <code>lst<\/code> file like this suggested by <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/image-classification.html\" rel=\"nofollow noreferrer\">doc<\/a>,<\/p>\n<pre><code>22  1   s3:\/\/image-classification\/image_classification_model_data\/train\/img-001.png\n86  0   s3:\/\/image-classification\/image_classification_model_data\/train\/img-002.png\n...\n<\/code><\/pre>\n<p>My other configurations:<\/p>\n<pre><code>s3_bucket = 'image-classification'\nprefix =  'image_classification_model_data'\n\n\ns3train = 's3:\/\/{}\/{}\/train\/'.format(s3_bucket, prefix)\ns3validation = 's3:\/\/{}\/{}\/validation\/'.format(s3_bucket, prefix)\n\ns3train_lst = 's3:\/\/{}\/{}\/train_lst\/'.format(s3_bucket, prefix)\ns3validation_lst = 's3:\/\/{}\/{}\/validation_lst\/'.format(s3_bucket, prefix)\n\n\n\ntrain_data = sagemaker.inputs.TrainingInput(s3train, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data = sagemaker.inputs.TrainingInput(s3validation, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\ntrain_data_lst = sagemaker.inputs.TrainingInput(s3train_lst, distribution='FullyReplicated', \n                        content_type='application\/x-image', s3_data_type='S3Prefix')\n\nvalidation_data_lst = sagemaker.inputs.TrainingInput(s3validation_lst, distribution='FullyReplicated', \n                             content_type='application\/x-image', s3_data_type='S3Prefix')\n\n\ndata_channels = {'train': train_data, 'validation': validation_data, 'train_lst': train_data_lst, \n                 'validation_lst': validation_data_lst}\n<\/code><\/pre>\n<p>I checked the images downloaded and checked physically, I see the image. Now sure what this error gets thrown out as <code>blank<\/code>. Any suggestion would be great.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1616651745117,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1616652513867,
        "Question_score":0.0,
        "Question_view_count":164.0,
        "Answer_body":"<p>Sagemaker copies the input data you specify in <code>s3train<\/code> into the instance in <code>\/opt\/ml\/input\/data\/train\/<\/code> and that's why you have an error, because as you can see from the error message is trying to concatenate the filename in the <code>lst<\/code> file with the path where it expect the image to be. So just put only the filenames in your <code>lst<\/code>and should be fine (remove the s3 path).<\/p>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66793845",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1616684272012,
        "Question_original_content":"custom error imread read blank imag file aw follow tutori custom data custom bucket train valid data get follow error custom error imread read blank imag file opt input data train imag classif imag classif model data train img png train data folder name train set lst file like suggest doc imag classif imag classif model data train img png imag classif imag classif model data train img png configur bucket imag classif prefix imag classif model data strain train format bucket prefix svalid valid format bucket prefix strain lst train lst format bucket prefix svalid lst valid lst format bucket prefix train data input traininginput strain distribut fullyrepl content type applic imag data type sprefix valid data input traininginput svalid distribut fullyrepl content type applic imag data type sprefix train data lst input traininginput strain lst distribut fullyrepl content type applic imag data type sprefix valid data lst input traininginput svalid lst distribut fullyrepl content type applic imag data type sprefix data channel train train data valid valid data train lst train data lst valid lst valid data lst check imag download check physic imag sure error get thrown blank suggest great",
        "Question_preprocessed_content":"custom error imread read blank imag file aw follow tutori custom data custom bucket train valid data get follow error train data folder name set file like suggest doc configur check imag download check physic imag sure error get thrown suggest great",
        "Question_gpt_summary_original":"The user is encountering an error while following an image classification tutorial using custom data and S3 buckets. The error message indicates that the image file is blank, even though the user has physically checked and confirmed that the image is present. The user has provided their lst file and other configurations, but is unsure why the error is occurring and is seeking suggestions to resolve the issue.",
        "Question_gpt_summary":"user encount error follow imag classif tutori custom data bucket error messag indic imag file blank user physic check confirm imag present user provid lst file configur unsur error occur seek suggest resolv issu",
        "Answer_original_content":"copi input data specifi strain instanc opt input data train error error messag try concaten filenam lst file path expect imag filenam lstand fine remov path",
        "Answer_preprocessed_content":"copi input data specifi instanc error error messag try concaten filenam file path expect imag filenam fine",
        "Answer_gpt_summary_original":"Solution: The user can resolve the error by removing the S3 path from the filenames in their lst file and only including the filenames. This will allow Sagemaker to copy the input data into the instance and concatenate the filename in the lst file with the path where it expects the image to be.",
        "Answer_gpt_summary":"solut user resolv error remov path filenam lst file includ filenam allow copi input data instanc concaten filenam lst file path expect imag"
    },
    {
        "Question_title":"Building an autoencoder with Keras and Kedro",
        "Question_body":"<p>I'm trying to build an autoencoder, which I'm sure I'm doing something wrong. I tried separating the creation of the model from the actual training but this is not really working out for me and is giving me the following error.<\/p>\n<pre><code>AssertionError: Could not compute output KerasTensor(type_spec=TensorSpec(shape=(None, 310), dtype=tf.float32, name=None), name='dense_7\/Sigmoid:0', description=&quot;created by layer 'dense_7'&quot;)\n<\/code><\/pre>\n<p>I'm doing this all using the Kedro framework. I have a pipeline.py file with the pipeline definition and a nodes.py with the functions that I want to use. So far, this is my project structure:<\/p>\n<p>pipelines.py:<\/p>\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes.autoencoder_nodes import *\n\ndef train_autoencoder_pipeline():\n    return Pipeline([\n        # Build neural network\n        node(\n            build_models, \n            inputs=[\n                &quot;train_x&quot;, \n                &quot;params:autoencoder_n_hidden_layers&quot;,\n                &quot;params:autoencoder_latent_space_size&quot;,\n                &quot;params:autoencoder_regularization_strength&quot;,\n                &quot;params:seed&quot;\n                ],\n            outputs=dict(\n                pre_train_autoencoder=&quot;pre_train_autoencoder&quot;,\n                pre_train_encoder=&quot;pre_train_encoder&quot;,\n                pre_train_decoder=&quot;pre_train_decoder&quot;\n            ), name=&quot;autoencoder-create-models&quot;\n        ),\n        # Scale features\n        node(fit_scaler, inputs=&quot;train_x&quot;, outputs=&quot;autoencoder_scaler&quot;, name=&quot;autoencoder-fit-scaler&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;train_x&quot;], outputs=&quot;autoencoder_scaled_train_x&quot;, name=&quot;autoencoder-scale-train&quot;),\n        node(tranform_scaler, inputs=[&quot;autoencoder_scaler&quot;, &quot;test_x&quot;], outputs=&quot;autoencoder_scaled_test_x&quot;, name=&quot;autoencoder-scale-test&quot;),\n\n        # Train autoencoder\n        node(\n            train_autoencoder, \n            inputs=[\n                &quot;autoencoder_scaled_train_x&quot;,\n                &quot;autoencoder_scaled_test_x&quot;,\n                &quot;pre_train_autoencoder&quot;, \n                &quot;pre_train_encoder&quot;, \n                &quot;pre_train_decoder&quot;,\n                &quot;params:autoencoder_epochs&quot;,\n                &quot;params:autoencoder_batch_size&quot;,\n                &quot;params:seed&quot;\n            ],\n            outputs= dict(\n                autoencoder=&quot;autoencoder&quot;,\n                encoder=&quot;encoder&quot;,\n                decoder=&quot;decoder&quot;,\n                autoencoder_history=&quot;autoencoder_history&quot;,\n            ),\n            name=&quot;autoencoder-train-model&quot;\n        )])\n<\/code><\/pre>\n<p>nodes.py:<\/p>\n<pre><code>from sklearn.preprocessing import MinMaxScaler\nfrom tensorflow import keras\nimport tensorflow as tf\n\nfrom typing import Dict, Any, Tuple\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport logging\n\n\ndef build_models(data: pd.DataFrame, n_hidden_layers: int, latent_space_size: int, retularization_stregth: float, seed: int) -&gt; Tuple[keras.Model, keras.Model, keras.Model]:\n    assert n_hidden_layers &gt;= 1, &quot;There must be at least 1 hidden layer for the autoencoder&quot;\n    \n    n_features = data.shape[1]\n    tf.random.set_seed(seed)\n    input_layer = keras.Input(shape=(n_features,))\n    \n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(input_layer)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n    encoded = keras.layers.Dense(latent_space_size, activation=&quot;sigmoid&quot;)(hidden)\n\n    hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(encoded)\n    hidden = keras.layers.LeakyReLU()(hidden)\n    \n    for _ in range(n_hidden_layers - 1):\n        hidden = keras.layers.Dense(n_features, kernel_regularizer=keras.regularizers.l1(retularization_stregth))(hidden)\n        hidden = keras.layers.LeakyReLU()(hidden)\n    \n\n    decoded = keras.layers.Dense(n_features, activation=&quot;sigmoid&quot;)(hidden)\n\n    # Defines the neural networks\n    autoencoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    encoder = keras.models.Model(inputs=input_layer, outputs=encoded)\n    decoder = keras.models.Model(inputs=input_layer, outputs=decoded)\n    autoencoder.compile(optimizer=&quot;adam&quot;, loss=&quot;mean_absolute_error&quot;)\n\n    return dict(\n        pre_train_autoencoder=autoencoder,\n        pre_train_encoder=encoder,\n        pre_train_decoder=decoder\n    )\n\ndef fit_scaler(data: pd.DataFrame) -&gt; MinMaxScaler:\n    scaler = MinMaxScaler()\n    scaler.fit(data)\n    return scaler\n\ndef tranform_scaler(scaler: MinMaxScaler, data: pd.DataFrame) -&gt; np.array:\n    return scaler.transform(data)\n\ndef train_autoencoder(\n    train_x: pd.DataFrame, test_x: pd.DataFrame, \n    autoencoder: keras.Model, encoder: keras.Model, decoder: keras.Model, \n    epochs: int, batch_size: int, seed: int) -&gt; Dict[str, Any]:\n\n    tf.random.set_seed(seed)\n    callbacks = [\n        keras.callbacks.History(),\n        keras.callbacks.EarlyStopping(patience=3)\n    ]\n    logging.info(train_x.shape)\n    logging.info(test_x.shape)\n\n    history = autoencoder.fit(\n        train_x, train_x,\n        validation_data=(test_x, test_x),\n        callbacks=callbacks, \n        epochs=epochs,\n        batch_size=batch_size\n    )\n\n    return dict(\n        autoencoder=autoencoder,\n        encoder=encoder,\n        decoder=decoder,\n        autoencoder_history=history,\n    )\n<\/code><\/pre>\n<p>catalog.yaml:<\/p>\n<pre><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n\nautoencoder_train_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_train_x.csv\n\nautoencoder_test_x:\n  type: pandas.CSVDataSet\n  filepath: data\/04_feature\/autoencoder_test_x.csv\n<\/code><\/pre>\n<p>And finally parameters.yaml:<\/p>\n<pre><code>seed: 200\n# Autoencoder\nautoencoder_n_hidden_layers: 3\nautoencoder_latent_space_size: 15\nautoencoder_epochs: 100\nautoencoder_batch_size: 32\nautoencoder_regularization_strength: 0.001\n<\/code><\/pre>\n<p>I believe that Keras is not seeing the whole graph since they will be out of the scope for the buld_models function, but I'm not sure whether this is the case, or how to fix it. Any help would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1629078707297,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":245.0,
        "Answer_body":"<p>I was able to set up your project locally and reproduce the error. To fix it, I had to add the <code>pre_train_*<\/code> outputs to the catalog as well. Therefore, it's my <code>catalog.yaml<\/code> file:<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>autoencoder_scaler:\n  type: pickle.PickleDataSet\n  filepath: data\/06_models\/autoencoder_scaler.pkl\n\npre_train_autoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_autoencoder.h5\n\npre_train_encoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_encoder.h5\n\npre_train_decoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/pre_train_decoder.h5\n\nautoencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/autoencoder.h5\n\nencoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/encoder.h5\n\ndecoder:\n  type: kedro.extras.datasets.tensorflow.TensorFlowModelDataset\n  filepath: data\/06_models\/decoder.h5\n<\/code><\/pre>\n<p>Also, I changed the return of <code>train_autoencoder<\/code> node to:<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>return dict(\n    autoencoder=autoencoder,\n    autoencoder_history=history.history,\n)\n<\/code><\/pre>\n<p>Note that I changed the <code>autoencoder_history<\/code> to return <code>history.history<\/code> since <code>MemoryDataset<\/code> can't pickle the object <code>history<\/code> by itself. The <code>history.history<\/code> is a dictionary with losses of train and validation sets.<\/p>\n<p>You can find the complete code <a href=\"https:\/\/gist.github.com\/arnaldog12\/a3f7801fe3910c02f4bcea8be61b910c\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1629217805312,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68796641",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1629132605447,
        "Question_original_content":"build autoencod kera try build autoencod sure wrong tri separ creation model actual train work give follow error assertionerror comput output kerastensor type spec tensorspec shape dtype float dens sigmoid descript creat layer dens framework pipelin file pipelin definit node function want us far project structur pipelin pipelin import pipelin node node autoencod node import def train autoencod pipelin return pipelin build neural network node build model input train param autoencod hidden layer param autoencod latent space size param autoencod regular strength param seed output dict pre train autoencod pre train autoencod pre train encod pre train encod pre train decod pre train decod autoencod creat model scale featur node fit scaler input train output autoencod scaler autoencod fit scaler node tranform scaler input autoencod scaler train output autoencod scale train autoencod scale train node tranform scaler input autoencod scaler test output autoencod scale test autoencod scale test train autoencod node train autoencod input autoencod scale train autoencod scale test pre train autoencod pre train encod pre train decod param autoencod epoch param autoencod batch size param seed output dict autoencod autoencod encod encod decod decod autoencod histori autoencod histori autoencod train model node sklearn preprocess import minmaxscal tensorflow import kera import tensorflow type import dict tupl import matplotlib pyplot plt import panda import numpi import log def build model data datafram hidden layer int latent space size int retular stregth float seed int tupl kera model kera model kera model assert hidden layer hidden layer autoencod featur data shape random set seed seed input layer kera input shape featur hidden kera layer dens featur kernel regular kera regular retular stregth input layer hidden kera layer leakyrelu hidden rang hidden layer hidden kera layer dens featur kernel regular kera regular retular stregth hidden hidden kera layer leakyrelu hidden encod kera layer dens latent space size activ sigmoid hidden hidden kera layer dens featur kernel regular kera regular retular stregth encod hidden kera layer leakyrelu hidden rang hidden layer hidden kera layer dens featur kernel regular kera regular retular stregth hidden hidden kera layer leakyrelu hidden decod kera layer dens featur activ sigmoid hidden defin neural network autoencod kera model model input input layer output decod encod kera model model input input layer output encod decod kera model model input input layer output decod autoencod compil optim adam loss mean absolut error return dict pre train autoencod autoencod pre train encod encod pre train decod decod def fit scaler data datafram minmaxscal scaler minmaxscal scaler fit data return scaler def tranform scaler scaler minmaxscal data datafram arrai return scaler transform data def train autoencod train datafram test datafram autoencod kera model encod kera model decod kera model epoch int batch size int seed int dict str random set seed seed callback kera callback histori kera callback earlystop patienc log info train shape log info test shape histori autoencod fit train train valid data test test callback callback epoch epoch batch size batch size return dict autoencod autoencod encod encod decod decod autoencod histori histori catalog yaml autoencod scaler type pickl pickledataset filepath data model autoencod scaler pkl autoencod type extra dataset tensorflow tensorflowmodeldataset filepath data model autoencod encod type extra dataset tensorflow tensorflowmodeldataset filepath data model encod decod type extra dataset tensorflow tensorflowmodeldataset filepath data model decod autoencod train type panda csvdataset filepath data featur autoencod train csv autoencod test type panda csvdataset filepath data featur autoencod test csv final paramet yaml seed autoencod autoencod hidden layer autoencod latent space size autoencod epoch autoencod batch size autoencod regular strength believ kera see graph scope buld model function sure case fix help appreci",
        "Question_preprocessed_content":"build autoencod kera try build autoencod sure wrong tri separ creation model actual train work give follow error framework file pipelin definit function want us far project structur final believ kera see graph scope function sure case fix help appreci",
        "Question_gpt_summary_original":"The user is encountering an error while building an autoencoder using Keras and Kedro frameworks. They have defined the pipeline and nodes in separate files and are trying to separate the creation of the model from the actual training. However, they are getting an AssertionError and suspect that Keras is not seeing the whole graph. They are unsure of how to fix this issue.",
        "Question_gpt_summary":"user encount error build autoencod kera framework defin pipelin node separ file try separ creation model actual train get assertionerror suspect kera see graph unsur fix issu",
        "Answer_original_content":"abl set project local reproduc error fix add pre train output catalog catalog yaml file autoencod scaler type pickl pickledataset filepath data model autoencod scaler pkl pre train autoencod type extra dataset tensorflow tensorflowmodeldataset filepath data model pre train autoencod pre train encod type extra dataset tensorflow tensorflowmodeldataset filepath data model pre train encod pre train decod type extra dataset tensorflow tensorflowmodeldataset filepath data model pre train decod autoencod type extra dataset tensorflow tensorflowmodeldataset filepath data model autoencod encod type extra dataset tensorflow tensorflowmodeldataset filepath data model encod decod type extra dataset tensorflow tensorflowmodeldataset filepath data model decod chang return train autoencod node return dict autoencod autoencod autoencod histori histori histori note chang autoencod histori return histori histori memorydataset pickl object histori histori histori dictionari loss train valid set complet code",
        "Answer_preprocessed_content":"abl set project local reproduc error fix add output catalog file chang return node note chang return pickl object dictionari loss train valid set complet code",
        "Answer_gpt_summary_original":"Solution: The user can fix the AssertionError by adding the pre_train_* outputs to the catalog.yaml file. They also need to change the return of train_autoencoder node to return a dictionary with autoencoder and autoencoder_history.",
        "Answer_gpt_summary":"solut user fix assertionerror ad pre train output catalog yaml file need chang return train autoencod node return dictionari autoencod autoencod histori"
    },
    {
        "Question_title":"AWS SageMaker - How to load trained sklearn model to serve for inference?",
        "Question_body":"<p>I am trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions. All I want to use sagemaker for, is to deploy and server model I had serialised using <code>joblib<\/code>, nothing more. every blog I have read and sagemaker python documentation showed that sklearn model had to be trained on sagemaker in order to be deployed in sagemaker.<\/p>\n<p>When I was going through the SageMaker documentation I learned that sagemaker does let users <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html#load-a-model\" rel=\"nofollow noreferrer\">load a serialised model<\/a> stored in S3 as shown below:<\/p>\n<pre><code>def model_fn(model_dir):\n    clf = joblib.load(os.path.join(model_dir, &quot;model.joblib&quot;))\n    return clf\n<\/code><\/pre>\n<p>And this is what documentation says about the argument <code>model_dir<\/code>:<\/p>\n<blockquote>\n<p>SageMaker will inject the directory where your model files and\nsub-directories, saved by save, have been mounted. Your model function\nshould return a model object that can be used for model serving.<\/p>\n<\/blockquote>\n<p>This again means that training has to be done on sagemaker.<\/p>\n<p>So, is there a way I can just specify the S3 location of my serialised model and have sagemaker de-serialise(or load) the model from S3 and use it for inference?<\/p>\n<h2>EDIT 1:<\/h2>\n<p>I used code in the answer to my application and I got below error when trying to deploy from notebook of SageMaker studio. I believe SageMaker is screaming that training wasn't done on SageMaker.<\/p>\n<pre><code>---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n&lt;ipython-input-4-6662bbae6010&gt; in &lt;module&gt;\n      1 predictor = model.deploy(\n      2     initial_instance_count=1,\n----&gt; 3     instance_type='ml.m4.xlarge'\n      4 )\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in deploy(self, initial_instance_count, instance_type, serializer, deserializer, accelerator_type, endpoint_name, use_compiled_model, wait, model_name, kms_key, data_capture_config, tags, **kwargs)\n    770         &quot;&quot;&quot;\n    771         removed_kwargs(&quot;update_endpoint&quot;, kwargs)\n--&gt; 772         self._ensure_latest_training_job()\n    773         self._ensure_base_job_name()\n    774         default_name = name_from_base(self.base_job_name)\n\n\/opt\/conda\/lib\/python3.7\/site-packages\/sagemaker\/estimator.py in _ensure_latest_training_job(self, error_message)\n   1128         &quot;&quot;&quot;\n   1129         if self.latest_training_job is None:\n-&gt; 1130             raise ValueError(error_message)\n   1131 \n   1132     delete_endpoint = removed_function(&quot;delete_endpoint&quot;)\n\nValueError: Estimator is not associated with a training job\n<\/code><\/pre>\n<p>My code:<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\n# from sagemaker.pytorch import PyTorchModel\nfrom sagemaker.sklearn import SKLearn\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nsm_role = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\n\nmodel_file = &quot;s3:\/\/sagemaker-manual-bucket\/sm_model_artifacts\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = SKLearn(model_data=model_file,\n                entry_point='inference.py',\n                name='rf_try_1',\n                role=sm_role,\n                source_dir='code',\n                framework_version='0.20.0',\n                instance_count=1,\n                instance_type='ml.m4.xlarge',\n                predictor_cls=AnalysisClass)\npredictor = model.deploy(initial_instance_count=1,\n                         instance_type='ml.m4.xlarge')\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1607263332140,
        "Question_favorite_count":3.0,
        "Question_last_edit_time":1607399645212,
        "Question_score":3.0,
        "Question_view_count":3221.0,
        "Answer_body":"<p>Yes you can. AWS documentation focuses on end-to-end from training to deployment in SageMaker which makes the impression that training has to be done on sagemaker. AWS documentation and examples should have clear separation among Training in Estimator, Saving and loading model, and Deployment model to SageMaker Endpoint.<\/p>\n<h2>SageMaker Model<\/h2>\n<p>You need to create the <a href=\"https:\/\/docs.aws.amazon.com\/AWSCloudFormation\/latest\/UserGuide\/aws-resource-sagemaker-model.html\" rel=\"nofollow noreferrer\">AWS::SageMaker::Model<\/a> resource which refers to the &quot;model&quot; you have trained <strong>and more<\/strong>. AWS::SageMaker::Model is in CloudFormation document but it is only to explain what AWS resource you need.<\/p>\n<p><a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreateModel.html\" rel=\"nofollow noreferrer\">CreateModel<\/a> API creates a SageMaker model resource. The parameters specifie the docker image to use, model location in S3, IAM role to use, etc. See <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/your-algorithms-inference-code.html#your-algorithms-inference-code-load-artifacts\" rel=\"nofollow noreferrer\">How SageMaker Loads Your Model Artifacts<\/a>.<\/p>\n<h3>Docker image<\/h3>\n<p>Obviously you need the framework e.g. ScikitLearn, TensorFlow, PyTorch, etc that you used to train your model to get inferences. You need a docker image that has the framework, and HTTP front end to respond to the prediction calls. See <a href=\"https:\/\/github.com\/aws\/sagemaker-inference-toolkit\" rel=\"nofollow noreferrer\">SageMaker Inference Toolkit<\/a> and <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/amazon-sagemaker-toolkits.html\" rel=\"nofollow noreferrer\">Using the SageMaker Training and Inference Toolkits<\/a>.<\/p>\n<p>To build the image is not easy. Hence AWS provides pre-built images called <a href=\"https:\/\/docs.aws.amazon.com\/deep-learning-containers\/latest\/devguide\/deep-learning-containers-images.html\" rel=\"nofollow noreferrer\">AWS Deep Learning Containers<\/a> and available images are in <a href=\"https:\/\/github.com\/aws\/deep-learning-containers\/blob\/master\/available_images.md\" rel=\"nofollow noreferrer\">Github<\/a>.<\/p>\n<p>If your framework and the version is listed there, you can use it as the image. Otherwise you need to build by yourself. See <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-mlops-workshop\/blob\/master\/lab\/01_CreateAlgorithmContainer\/01_Creating%20a%20Classifier%20Container.ipynb\" rel=\"nofollow noreferrer\">Building a docker container for training\/deploying our classifier<\/a>.<\/p>\n<h2>SageMaker Python SDK for Frameworks<\/h2>\n<p>Create SageMaker Model by yourself using API is hard. Hence AWS SageMaker Python SDK has provided utilities to create the SageMaker models for several frameworks. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/index.html\" rel=\"nofollow noreferrer\">Frameworks<\/a> for available frameworks. If it is not there, you may still be able to use <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html#sagemaker.model.FrameworkModel\" rel=\"nofollow noreferrer\">sagemaker.model.FrameworkModel<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/inference\/model.html\" rel=\"nofollow noreferrer\">Model<\/a> to load your trained model. For your case, see <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/sklearn\/using_sklearn.html\" rel=\"nofollow noreferrer\">Using Scikit-learn with the SageMaker Python SDK<\/a>.<\/p>\n<h3>model.tar.gz<\/h3>\n<p>For instance if you used PyTorch and save the model as model.pth. To load the model and the inference code to get the prediction from the model, you need to create a model.tar.gz file. The structure inside the model.tar.gz is explained in <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#model-directory-structure\" rel=\"nofollow noreferrer\">Model Directory Structure<\/a>. If you use Windows, beware of the CRLF to LF. AWS SageMaker runs in *NIX environment. See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-the-directory-structure-for-your-model-files\" rel=\"nofollow noreferrer\">Create the directory structure for your model files<\/a>.<\/p>\n<pre><code>|- model.pth        # model file is inside \/ directory.\n|- code\/            # Code artefacts must be inside \/code\n  |- inference.py   # Your inference code for the framework\n  |- requirements.txt  # only for versions 1.3.1 and higher. Name must be &quot;requirements.txt&quot;\n<\/code><\/pre>\n<p>Save the tar.gz file in S3. Make sure of the IAM role to access the S3 bucket and objects.<\/p>\n<h3>Loading model and get inference<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#create-a-pytorchmodel-object\" rel=\"nofollow noreferrer\">Create a PyTorchModel object<\/a>. When instantiating the PyTorchModel class, SageMaker automatically selects the AWS Deep Learning Container image for PyTorch for the version specified in <strong>framework_version<\/strong>. If the image for the version does not exist, then it fails. This has not been documented in AWS but need to be aware of. SageMaker then internally calls the CreateModel API with the S3 model file location and the AWS Deep Learning Container image URL.<\/p>\n<pre><code>import sagemaker\nfrom sagemaker import get_execution_role\nfrom sagemaker.pytorch import PyTorchModel\nfrom sagemaker.predictor import RealTimePredictor, json_serializer, json_deserializer\n\nrole = sagemaker.get_execution_role()  # IAM role to run SageMaker, access S3 and ECR\nmodel_file = &quot;s3:\/\/YOUR_BUCKET\/YOUR_FOLDER\/model.tar.gz&quot;   # Must be &quot;.tar.gz&quot; suffix\n\n\nclass AnalysisClass(RealTimePredictor):\n    def __init__(self, endpoint_name, sagemaker_session):\n        super().__init__(\n            endpoint_name,\n            sagemaker_session=sagemaker_session,\n            serializer=json_serializer,\n            deserializer=json_deserializer,   # To be able to use JSON serialization\n            content_type='application\/json'   # To be able to send JSON as HTTP body\n        )\n\nmodel = PyTorchModel(\n    model_data=model_file,\n    name='YOUR_MODEL_NAME_WHATEVER',\n    role=role,\n    entry_point='inference.py',\n    source_dir='code',              # Location of the inference code\n    framework_version='1.5.0',      # Availble AWS Deep Learning PyTorch container version must be specified\n    predictor_cls=AnalysisClass     # To specify the HTTP request body format (application\/json)\n)\n\npredictor = model.deploy(\n    initial_instance_count=1,\n    instance_type='ml.m5.xlarge'\n)\n\ntest_data = {&quot;body&quot;: &quot;YOUR PREDICTION REQUEST&quot;}\nprediction = predictor.predict(test_data)\n<\/code><\/pre>\n<p>By default, SageMaker uses NumPy as the serialization format. To be able to use JSON, need to specify the serializer and content_type. Instead of using RealTimePredictor class, you can specify them to predictor.<\/p>\n<pre><code>predictor.serializer=json_serializer\npredictor.predict(test_data)\n<\/code><\/pre>\n<p>Or<\/p>\n<pre><code>predictor.serializer=None # As the serializer is None, predictor won't serialize the data\nserialized_test_data=json.dumps(test_data) \npredictor.predict(serialized_test_data)\n<\/code><\/pre>\n<h3>Inference code sample<\/h3>\n<p>See <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-input\" rel=\"nofollow noreferrer\">Process Model Input<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#get-predictions-from-a-pytorch-model\" rel=\"nofollow noreferrer\">Get Predictions from a PyTorch Model<\/a> and <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/frameworks\/pytorch\/using_pytorch.html#process-model-output\" rel=\"nofollow noreferrer\">Process Model Output<\/a>. The prediction request is sent as JSON in HTTP request body in this example.<\/p>\n<pre><code>import os\nimport sys\nimport datetime\nimport json\nimport torch\nimport numpy as np\n\nCONTENT_TYPE_JSON = 'application\/json'\n\ndef model_fn(model_dir):\n    # SageMaker automatically load the model.tar.gz from the S3 and \n    # mount the folders inside the docker container. The  'model_dir'\n    # points to the root of the extracted tar.gz file.\n\n    model_path = f'{model_dir}\/'\n    \n    # Load the model\n    # You can load whatever from the Internet, S3, wherever &lt;--- Answer to your Question\n    # NO Need to use the model in tar.gz. You can place a dummy model file.\n    ...\n\n    return model\n\n\ndef predict_fn(input_data, model):\n    # Do your inference\n    ...\n\ndef input_fn(serialized_input_data, content_type=CONTENT_TYPE_JSON):\n    input_data = json.loads(serialized_input_data)\n    return input_data\n\n\ndef output_fn(prediction_output, accept=CONTENT_TYPE_JSON):\n    if accept == CONTENT_TYPE_JSON:\n        return json.dumps(prediction_output), accept\n    raise Exception('Unsupported content type') \n<\/code><\/pre>\n<h2>Related<\/h2>\n<ul>\n<li><a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/overview.html#using-models-trained-outside-of-amazon-sagemaker\" rel=\"nofollow noreferrer\">Using Models Trained Outside of Amazon SageMaker\n<\/a><\/li>\n<\/ul>\n<h2>Note<\/h2>\n<p>SageMaker team keeps changing the implementations and the documentations are frequently obsolete. When you are sure you did follow the documents and it does not work, obsolete documentation is quite likely. In such case, need to clarify with AWS support, or open an issue in the Github.<\/p>",
        "Answer_comment_count":6.0,
        "Answer_last_edit_time":1630389412780,
        "Answer_score":7.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/65168915",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1607293074048,
        "Question_original_content":"load train sklearn model serv infer try deploi model train sklearn endpoint serv api predict want us deploi server model serialis joblib blog read python document show sklearn model train order deploi go document learn let user load serialis model store shown def model model dir clf joblib load path join model dir model joblib return clf document sai argument model dir inject directori model file sub directori save save mount model function return model object model serv mean train wai specifi locat serialis model serialis load model us infer edit code answer applic got error try deploi notebook studio believ scream train wasn valueerror traceback recent predictor model deploi initi instanc count instanc type xlarg opt conda lib python site packag estim deploi self initi instanc count instanc type serial deseri acceler type endpoint us compil model wait model km kei data captur config tag kwarg remov kwarg updat endpoint kwarg self ensur latest train job self ensur base job default base self base job opt conda lib python site packag estim ensur latest train job self error messag self latest train job rais valueerror error messag delet endpoint remov function delet endpoint valueerror estim associ train job code import import execut role pytorch import pytorchmodel sklearn import sklearn predictor import realtimepredictor json serial json deseri role execut role iam role run access ecr model file manual bucket model artifact model tar tar suffix class analysisclass realtimepredictor def init self endpoint session super init endpoint session session serial json serial deseri json deseri abl us json serial content type applic json abl send json http bodi model sklearn model data model file entri point infer try role role sourc dir code framework version instanc count instanc type xlarg predictor cl analysisclass predictor model deploi initi instanc count instanc type xlarg",
        "Question_preprocessed_content":"load train sklearn model serv infer try deploi model train sklearn endpoint serv api predict want us deploi server model serialis blog read python document show sklearn model train order deploi go document learn let user load serialis model store shown document sai argument inject directori model file save save mount model function return model object model serv mean train wai specifi locat serialis model load model us infer edit code answer applic got error try deploi notebook studio believ scream train wasn code",
        "Question_gpt_summary_original":"The user is trying to deploy a model trained with sklearn to an endpoint and serve it as an API for predictions using AWS SageMaker. However, the user has encountered challenges as every blog and SageMaker documentation suggests that the sklearn model has to be trained on SageMaker in order to be deployed in SageMaker. The user is looking for a way to specify the S3 location of the serialized model and have SageMaker de-serialize (or load) the model from S3 and use it for inference. The user has also encountered an error when trying to deploy the model from the notebook of SageMaker studio, which suggests that training was not done on SageMaker.",
        "Question_gpt_summary":"user try deploi model train sklearn endpoint serv api predict user encount challeng blog document suggest sklearn model train order deploi user look wai specifi locat serial model serial load model us infer user encount error try deploi model notebook studio suggest train",
        "Answer_original_content":"ye aw document focus end end train deploy make impress train aw document exampl clear separ train estim save load model deploy model endpoint model need creat aw model resourc refer model train aw model cloudform document explain aw resourc need createmodel api creat model resourc paramet specifi docker imag us model locat iam role us load model artifact docker imag obvious need framework scikitlearn tensorflow pytorch train model infer need docker imag framework http end respond predict call infer toolkit train infer toolkit build imag easi aw provid pre built imag call aw deep learn contain avail imag github framework version list us imag need build build docker contain train deploi classifi python sdk framework creat model api hard python sdk provid util creat model framework framework avail framework abl us model frameworkmodel model load train model case scikit learn python sdk model tar instanc pytorch save model model pth load model infer code predict model need creat model tar file structur insid model tar explain model directori structur us window bewar crlf run nix environ creat directori structur model file model pth model file insid directori code code artefact insid code infer infer code framework requir txt version higher requir txt save tar file sure iam role access bucket object load model infer creat pytorchmodel object instanti pytorchmodel class automat select aw deep learn contain imag pytorch version specifi framework version imag version exist fail document aw need awar intern call createmodel api model file locat aw deep learn contain imag url import import execut role pytorch import pytorchmodel predictor import realtimepredictor json serial json deseri role execut role iam role run access ecr model file bucket folder model tar tar suffix class analysisclass realtimepredictor def init self endpoint session super init endpoint session session serial json serial deseri json deseri abl us json serial content type applic json abl send json http bodi model pytorchmodel model data model file model role role entri point infer sourc dir code locat infer code framework version availbl aw deep learn pytorch contain version specifi predictor cl analysisclass specifi http request bodi format applic json predictor model deploi initi instanc count instanc type xlarg test data bodi predict request predict predictor predict test data default us numpi serial format abl us json need specifi serial content type instead realtimepredictor class specifi predictor predictor serial json serial predictor predict test data predictor serial serial predictor won serial data serial test data json dump test data predictor predict serial test data infer code sampl process model input predict pytorch model process model output predict request sent json http request bodi exampl import import sy import datetim import json import torch import numpi content type json applic json def model model dir automat load model tar mount folder insid docker contain model dir point root extract tar file model path model dir load model load internet answer question need us model tar place dummi model file return model def predict input data model infer def input serial input data content type content type json input data json load serial input data return input data def output predict output accept content type json accept content type json return json dump predict output accept rais except unsupport content type relat model train outsid note team keep chang implement document frequent obsolet sure follow document work obsolet document like case need clarifi aw support open issu github",
        "Answer_preprocessed_content":"ye aw document focus train deploy make impress train aw document exampl clear separ train estim save load model deploy model endpoint model need creat aw model resourc refer model train aw model cloudform document explain aw resourc need createmodel api creat model resourc paramet specifi docker imag us model locat iam role us load model artifact docker imag obvious need framework scikitlearn tensorflow pytorch train model infer need docker imag framework http end respond predict call infer toolkit train infer toolkit build imag easi aw provid imag call aw deep learn contain avail imag github framework version list us imag need build build docker contain classifi python sdk framework creat model api hard python sdk provid util creat model framework framework avail framework abl us model load train model case python sdk instanc pytorch save model load model infer code predict model need creat file structur insid explain model directori structur us window bewar crlf run nix environ creat directori structur model file save file sure iam role access bucket object load model infer creat pytorchmodel object instanti pytorchmodel class automat select aw deep learn contain imag pytorch version specifi imag version exist fail document aw need awar intern call createmodel api model file locat aw deep learn contain imag url default us numpi serial format abl us json need specifi serial instead realtimepredictor class specifi predictor infer code sampl process model input predict pytorch model process model output predict request sent json http request bodi exampl relat model train outsid note team keep chang implement document frequent obsolet sure follow document work obsolet document like case need clarifi aw support open issu github",
        "Answer_gpt_summary_original":"The discussion provides several solutions to the challenge of deploying a model trained with sklearn to an endpoint and serving it as an API for predictions using AWS SageMaker. One solution involves creating an AWS::SageMaker::Model resource and using the CreateModel API to specify the docker image to use, model location in S3, IAM role to use, etc. Another solution involves using the SageMaker Python SDK for Frameworks to create the SageMaker models for several frameworks. The discussion also provides information on creating a model.tar.gz file, loading the model, and getting inference. Finally, the discussion notes that the SageMaker team frequently changes implementations and documentation, so it is important to clarify with AWS support or open an issue in Github if the documentation is obsolete",
        "Answer_gpt_summary":"discuss provid solut challeng deploi model train sklearn endpoint serv api predict solut involv creat aw model resourc createmodel api specifi docker imag us model locat iam role us solut involv python sdk framework creat model framework discuss provid inform creat model tar file load model get infer final discuss note team frequent chang implement document import clarifi aw support open issu github document obsolet"
    },
    {
        "Question_title":"How to Deploy Amazon-SageMaker Locally in Python",
        "Question_body":"<p>I trained my model in Amazon-SageMaker and downloaded it to my local computer. Unfortunately, I don't have any idea how to run the model locally.<\/p>\n\n<p>The Model is in a directory with files like:<\/p>\n\n<pre><code>image-classification-0001.params\nimage-classification-0002.params\nimage-classification-0003.params\nimage-classification-0004.params\nimage-classification-0005.params\nimage-classification-symbol.json\nmodel-shapes.json\n<\/code><\/pre>\n\n<p>Would anyone know how to run this locally with Python, or be able to point me to a resource that could help? I am trying to avoid calling the model using the Amazon API.<\/p>\n\n<p>Edit: The model I used was created with code very similar to this <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/introduction_to_amazon_algorithms\/imageclassification_caltech\/Image-classification-fulltraining.ipynb\" rel=\"nofollow noreferrer\">example<\/a>.<\/p>\n\n<p>Any help is appreciated, I will award the bounty to whoever is most helpful, even if they don't completely solve the question. <\/p>",
        "Question_answer_count":3,
        "Question_comment_count":3,
        "Question_created_time":1520225638010,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1536012091088,
        "Question_score":3.0,
        "Question_view_count":3742.0,
        "Answer_body":"<p>Following SRC's advice, I was able to get it to work by following the instructions in this <a href=\"https:\/\/stackoverflow.com\/questions\/47190614\/how-to-load-a-trained-mxnet-model\">question<\/a> and this <a href=\"http:\/\/mxnet.incubator.apache.org\/tutorials\/python\/predict_image.html\" rel=\"nofollow noreferrer\">doc<\/a> which describe how to load a MXnet model.<\/p>\n\n<p>I loaded the model like so:<\/p>\n\n<pre><code>lenet_model = mx.mod.Module.load('model_directory\/image-classification',5)\nimage_l = 64\nimage_w = 64\nlenet_model.bind(for_training=False, data_shapes=[('data',(1,3,image_l,image_w))],label_shapes=lenet_model._label_shapes)\n<\/code><\/pre>\n\n<p>Then predicted using the slightly modified helper functions in the previously linked documentation:<\/p>\n\n<pre><code>import mxnet as mx\nimport matplotlib.pyplot as plot\nimport cv2\nimport numpy as np\nfrom mxnet.io import DataBatch\n\ndef get_image(url, show=False):\n    # download and show the image\n    fname = mx.test_utils.download(url)\n    img = cv2.cvtColor(cv2.imread(fname), cv2.COLOR_BGR2RGB)\n    if img is None:\n         return None\n    if show:\n         plt.imshow(img)\n         plt.axis('off')\n    # convert into format (batch, RGB, width, height)\n    img = cv2.resize(img, (64, 64))\n    img = np.swapaxes(img, 0, 2)\n    img = np.swapaxes(img, 1, 2)\n    img = img[np.newaxis, :]\n    return img\n\ndef predict(url, labels):\n    img = get_image(url, show=True)\n    # compute the predict probabilities\n    lenet_model.forward(DataBatch([mx.nd.array(img)]))\n    prob = lenet_model.get_outputs()[0].asnumpy()\n\n    # print the top-5\n    prob = np.squeeze(prob)\n    a = np.argsort(prob)[::-1]\n\n    for i in a[0:5]:\n       print('probability=%f, class=%s' %(prob[i], labels[i]))\n<\/code><\/pre>\n\n<p>Finally I called the prediction with this code:<\/p>\n\n<pre><code>labels = ['a','b','c', 'd','e', 'f']\npredict('https:\/\/eximagesite\/img_tst_a.jpg', labels )\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":1582341918227,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/49103679",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1520719061830,
        "Question_original_content":"deploi amazon local python train model amazon download local unfortun idea run model local model directori file like imag classif param imag classif param imag classif param imag classif param imag classif param imag classif symbol json model shape json know run local python abl point resourc help try avoid call model amazon api edit model creat code similar exampl help appreci award bounti help complet solv question",
        "Question_preprocessed_content":"deploi amazon local python train model amazon download local unfortun idea run model local model directori file like know run local python abl point resourc help try avoid call model amazon api edit model creat code similar exampl help appreci award bounti help complet solv question",
        "Question_gpt_summary_original":"The user has downloaded their model trained in Amazon-SageMaker to their local computer but does not know how to run it locally using Python. They are seeking help to avoid calling the model using the Amazon API. The model is in a directory with files such as image-classification-0001.params, image-classification-symbol.json, and model-shapes.json. The user has also provided a link to the code used to create the model.",
        "Question_gpt_summary":"user download model train amazon local know run local python seek help avoid call model amazon api model directori file imag classif param imag classif symbol json model shape json user provid link code creat model",
        "Answer_original_content":"follow src advic abl work follow instruct question doc load mxnet model load model like lenet model mod modul load model directori imag classif imag imag lenet model bind train fals data shape data imag imag label shape lenet model label shape predict slightli modifi helper function previous link document import mxnet import matplotlib pyplot plot import import numpi mxnet import databatch def imag url fals download imag fname test util download url img cvtcolor imread fname color bgrrgb img return plt imshow img plt axi convert format batch rgb width height img resiz img img swapax img img swapax img img img newaxi return img def predict url label img imag url true comput predict probabl lenet model forward databatch arrai img prob lenet model output asnumpi print prob squeez prob argsort prob print probabl class prob label final call predict code label predict http eximagesit img tst jpg label",
        "Answer_preprocessed_content":"follow src advic abl work follow instruct question doc load mxnet model load model like predict slightli modifi helper function previous link document final call predict code",
        "Answer_gpt_summary_original":"The solution provided in the discussion is to load the MXnet model using the instructions provided in the StackOverflow question and the MXnet documentation. The user loaded the model and predicted using the helper functions provided in the documentation. The user modified the helper functions slightly and called the prediction using the provided code.",
        "Answer_gpt_summary":"solut provid discuss load mxnet model instruct provid stackoverflow question mxnet document user load model predict helper function provid document user modifi helper function slightli call predict provid code"
    },
    {
        "Question_title":"How to use Tensorboard within a notebook running on Amazon SageMaker Studio Lab?",
        "Question_body":"<p>I have a Jupyter notebook running within an <code>Amazon SageMaker Studio Lab<\/code> (<a href=\"https:\/\/studiolab.sagemaker.aws\/\" rel=\"nofollow noreferrer\">https:\/\/studiolab.sagemaker.aws\/<\/a>) environment, and I want to use Tensordboard to monitor my model's performance inside the notebook.<\/p>\n<p>I have used the following commands to set up the Tensorboard:<\/p>\n<pre><code>%load_ext tensorboard\n# tb_log_dir variable holds the path to the log directory\n%tensorboard --logdir tb_log_dir\n<\/code><\/pre>\n<p>But nothing shows up in the output of the cell where I execute the commands. See:<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/TwbTk.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>The two buttons shown in the picture are not responding, BTW.<\/p>\n<p>How to solve this problem? Any suggestions would be appreciated.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1656241190023,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1656243378470,
        "Question_score":1.0,
        "Question_view_count":121.0,
        "Answer_body":"<p>I would try the canonical way to use tensorboard in AWS Sagemaker, it should be supported also by Studio Lab, it is described <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tensorboard.html\" rel=\"nofollow noreferrer\">here<\/a>. Basically install tensorboard and using the <code>EFS_PATH_LOG_DIR<\/code> launch tensorboard using the embedded console (you can do the following also from a cell):<\/p>\n<pre><code>pip install tensorboard\ntensorboard --logdir &lt;EFS_PATH_LOG_DIR&gt;\n<\/code><\/pre>\n<p>Be careful with the EFS_PATH_LOG_DIR, be sure this folder is valida path from the location you are, for example by default you are located in <code>studio-lab-user\/sagemaker-studiolab-notebooks\/<\/code> so the proper command would be <code>!tensorboard --logdir logs\/fit<\/code>.<\/p>\n<p>Then open a browser to:<\/p>\n<pre><code>https:\/\/&lt;YOUR URL&gt;\/studiolab\/default\/jupyter\/proxy\/6006\/\n<\/code><\/pre>",
        "Answer_comment_count":5.0,
        "Answer_last_edit_time":1657181121616,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72760982",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1656410088380,
        "Question_original_content":"us tensorboard notebook run studio lab jupyt notebook run studio lab http studiolab aw environ want us tensordboard monitor model perform insid notebook follow command set tensorboard load ext tensorboard log dir variabl hold path log directori tensorboard logdir log dir show output cell execut command button shown pictur respond btw solv problem suggest appreci",
        "Question_preprocessed_content":"us tensorboard notebook run studio lab jupyt notebook run environ want us tensordboard monitor model perform insid notebook follow command set tensorboard show output cell execut command button shown pictur respond btw solv problem suggest appreci",
        "Question_gpt_summary_original":"The user is facing challenges in using Tensorboard within a Jupyter notebook running on Amazon SageMaker Studio Lab. The user has tried setting up Tensorboard using the given commands, but nothing shows up in the output of the cell where the commands are executed. The two buttons shown in the picture are also not responding. The user is seeking suggestions to solve this problem.",
        "Question_gpt_summary":"user face challeng tensorboard jupyt notebook run studio lab user tri set tensorboard given command show output cell command execut button shown pictur respond user seek suggest solv problem",
        "Answer_original_content":"try canon wai us tensorboard support studio lab describ basic instal tensorboard ef path log dir launch tensorboard embed consol follow cell pip instal tensorboard tensorboard logdir care ef path log dir sure folder valida path locat exampl default locat studio lab user studiolab notebook proper command tensorboard logdir log fit open browser http studiolab default jupyt proxi",
        "Answer_preprocessed_content":"try canon wai us tensorboard support studio lab describ basic instal tensorboard launch tensorboard embed consol care sure folder valida path locat exampl default locat proper command open browser",
        "Answer_gpt_summary_original":"Solution:\n- Install Tensorboard using the command \"pip install tensorboard\".\n- Launch Tensorboard using the command \"tensorboard --logdir <EFS_PATH_LOG_DIR>\" and make sure that the EFS_PATH_LOG_DIR is a valid path from the location you are in.\n- Open a browser to \"https:\/\/<YOUR URL>\/studiolab\/default\/jupyter\/proxy\/6006\/\".",
        "Answer_gpt_summary":"solut instal tensorboard command pip instal tensorboard launch tensorboard command tensorboard logdir sure ef path log dir valid path locat open browser http studiolab default jupyt proxi"
    },
    {
        "Question_title":"Artifacts (local) caching - how does it really work?",
        "Question_body":"<p>Hi all,<\/p>\n<p>I\u2019m trying to figure out how does the caching  of artifacts work. Let\u2019s say I want to download a model artifact to run some evaluation on. I don\u2019t need the file on disk to persist rather I just want to load it into memory. What I do right now in my evaluation script is:<\/p>\n<pre><code class=\"lang-auto\">import tempfile\nimport wandb\n\nartifact = wandb.use_artifact(model_weights_uri)\nwith tempfile.TemporaryDirectory() as tmpdirname:\n    artifact.download(tmpdirname)\n    model_weights = load_pickle(os.path.join(tmpdirname, \"model_weights.pickle\"))\n<\/code><\/pre>\n<p>And from that point on I use the <code>model_weights<\/code> as it was loaded into memory.<\/p>\n<p>My first question is: if I run the code twice (on the same machine), <strong>will the model-weights be downloaded again<\/strong> or are they cached somewhere? assuming the logged artifact wasn\u2019t changed of course. And if they are cached, where are they cached?<br>\nI\u2019m also not clear about the <code>artifact<\/code> directory (which is used if I run <code>artifact.download()<\/code> without any argument). Does that directory serve as cache? if so, what does the <code>.cache<\/code> directory used for?<\/p>\n<p>I would appreciate answers to my questions and perhaps a  general explanation of the artifact caching mechanism &amp; best practices.<\/p>\n<p>Thanks!<br>\nRan<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1650203452610,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":680.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/ranshadmi-nexite\">@ranshadmi-nexite<\/a>,<\/p>\n<p>Thank you for your question. You are right, all Artifacts are cached on your system under <code>~\/.cache\/wandb\/artifacts<\/code> and organized by their checksum. So if you try to download a file with checksum <code>x<\/code> and that file has been logged in an Artifact from your machine or downloaded to your machine as part of an artifact before, we just pull it from the cache by checking if there is a cached Artifact file with checksum <code>x<\/code>.<\/p>\n<p>So, if you run the same code twice, assuming the version of the artifact you are trying to download has not changed, the artifact can simply be pickked up from your cache directory.<\/p>\n<p>Also, when calling <code>artifact.download()<\/code> without any arguments, the artifact is saved in the directory in which the code is running. This, however,  is not the directory that serves as a cache, that still remains <code>.cache<\/code> which acts as a central location to look for artifacts before fetching it.<\/p>\n<p>Thanks,<br>\nRamit<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/artifacts-local-caching-how-does-it-really-work\/2255",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1650312955392,
        "Question_original_content":"artifact local cach work try figur cach artifact work let want download model artifact run evalu dont need file disk persist want load memori right evalu script import tempfil import artifact us artifact model weight uri tempfil temporarydirectori tmpdirnam artifact download tmpdirnam model weight load pickl path join tmpdirnam model weight pickl point us model weight load memori question run code twice machin model weight download cach assum log artifact wasnt chang cours cach cach clear artifact directori run artifact download argument directori serv cach cach directori appreci answer question gener explan artifact cach mechan best practic thank ran",
        "Question_preprocessed_content":"artifact cach work try figur cach artifact work let want download model artifact run evalu dont need file disk persist want load memori right evalu script point us load memori question run code twice download cach assum log artifact wasnt chang cours cach cach clear directori directori serv cach directori appreci answer question gener explan artifact cach mechan best practic thank ran",
        "Question_gpt_summary_original":"The user is trying to understand how the caching of artifacts works. They want to know if the model-weights will be downloaded again if they run the code twice on the same machine and where the cached files are stored. The user is also unclear about the artifact directory and the .cache directory and is seeking a general explanation of the artifact caching mechanism and best practices.",
        "Question_gpt_summary":"user try understand cach artifact work want know model weight download run code twice machin cach file store user unclear artifact directori cach directori seek gener explan artifact cach mechan best practic",
        "Answer_original_content":"ranshadmi nexit thank question right artifact cach cach artifact organ checksum try download file checksum file log artifact machin download machin artifact pull cach check cach artifact file checksum run code twice assum version artifact try download chang artifact simpli pick cach directori call artifact download argument artifact save directori code run directori serv cach remain cach act central locat look artifact fetch thank ramit",
        "Answer_preprocessed_content":"thank question right artifact cach organ checksum try download file checksum file log artifact machin download machin artifact pull cach check cach artifact file checksum run code twice assum version artifact try download chang artifact simpli pick cach directori call argument artifact save directori code run directori serv cach remain act central locat look artifact fetch thank ramit",
        "Answer_gpt_summary_original":"Solution:\n- All Artifacts are cached on the user's system under `~\/.cache\/wandb\/artifacts` and organized by their checksum.\n- If the user runs the same code twice and the version of the artifact they are trying to download has not changed, the artifact can be picked up from the cache directory.\n- When calling `artifact.download()` without any arguments, the artifact is saved in the directory in which the code is running.\n- The `.cache` directory acts as a central location to look for artifacts before fetching it.",
        "Answer_gpt_summary":"solut artifact cach user cach artifact organ checksum user run code twice version artifact try download chang artifact pick cach directori call artifact download argument artifact save directori code run cach directori act central locat look artifact fetch"
    },
    {
        "Question_title":"\u201cFailure Exception: OSError: [Errno 30] Read-only file system\u201d when using AzureML in Python Azure Function",
        "Question_body":"Issue\nI am trying prepare and then submit a new experiment to Azure Machine Learning from an Azure Function in Python. I therefore register a new dataset for my Azure ML workspace, which contains the training data for my ML model using dataset.register(.... However, when I try to create this dataset with the following line of code\n\n dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n\nthen I get a Failure Exception: OSError: [Errno 30] Read-only file system ....\n\nIdeas\n1. I know that I shouldn't write to the file system from within an Azure function if possible. But I actually don't want to write anything to the local file system. I only want to create the dataset as a reference to my blob storage under datastore_path and then register this to my Azure Machine Learning workspace. But it seems that the method from_delimited_files is trying to write to the file system anyway (maybe some caching?).\n2. I also know that there is a temp folder in which writing temporary files is permitted. However, I belive I cannot really control where this method is writing data. I already tried changing the current working directory to this temp folder just before the function call using os.chdir(tempfile.gettempdir()), but that didn't help.\n\nAny other ideas? I don't think I am doing something particularly unusually...\n\nDetails\nI am using python 3.7 and azureml-sdk 1.9.0 and I can run the python script locally without problems. I currently deploy from VSCode using the Azure Functions extension version 0.23.0 (and an Azure DevOps pipeline for CI\/CD).\n\nHere is my full stack trace:\n\n Microsoft.Azure.WebJobs.Host.FunctionInvocationException: Exception while executing function: Functions.HttpTrigger_Train\n  ---> Microsoft.Azure.WebJobs.Script.Workers.Rpc.RpcException: Result: Failure\n Exception: OSError: [Errno 30] Read-only file system: '\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/bin\/deps.lock'\n Stack:   File \"\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py\", line 345, in _handle__invocation_request\n     self.__run_sync_func, invocation_id, fi.func, args)\n   File \"\/usr\/local\/lib\/python3.7\/concurrent\/futures\/thread.py\", line 57, in run\n     result = self.fn(*self.args, **self.kwargs)\n   File \"\/azure-functions-host\/workers\/python\/3.7\/LINUX\/X64\/azure_functions_worker\/dispatcher.py\", line 480, in __run_sync_func\n     return func(**params)\n   File \"\/home\/site\/wwwroot\/HttpTrigger_Train\/__init__.py\", line 11, in main\n     train()\n   File \"\/home\/site\/wwwroot\/shared_code\/train.py\", line 70, in train\n     dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/_loggerfactory.py\", line 126, in wrapper\n     return func(*args, **kwargs)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/data\/dataset_factory.py\", line 308, in from_delimited_files\n     quoting=support_multi_line)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/readers.py\", line 100, in read_csv\n     df = Dataflow._path_to_get_files_block(path, archive_options)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/dataflow.py\", line 2387, in _path_to_get_files_block\n     return datastore_to_dataflow(path)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py\", line 41, in datastore_to_dataflow\n     datastore, datastore_value = get_datastore_value(source)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py\", line 83, in get_datastore_value\n     _set_auth_type(workspace)\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/_datastore_helper.py\", line 134, in _set_auth_type\n     get_engine_api().set_aml_auth(SetAmlAuthMessageArgument(AuthType.SERVICEPRINCIPAL, json.dumps(auth)))\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py\", line 18, in get_engine_api\n     _engine_api = EngineAPI()\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/api.py\", line 55, in __init__\n     self._message_channel = launch_engine()\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/azureml\/dataprep\/api\/engineapi\/engine.py\", line 300, in launch_engine\n     dependencies_path = runtime.ensure_dependencies()\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py\", line 141, in ensure_dependencies\n     with _FileLock(deps_lock_path, raise_on_timeout=timeout_exception):\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py\", line 113, in __enter__\n     self.acquire()\n   File \"\/home\/site\/wwwroot\/.python_packages\/lib\/site-packages\/dotnetcore2\/runtime.py\", line 72, in acquire\n     self.lockfile = os.open(self.lockfile_path, os.O_CREAT | os.O_EXCL | os.O_RDWR)\n    \n    at Microsoft.Azure.WebJobs.Script.Description.WorkerFunctionInvoker.InvokeCore(Object[] parameters, FunctionInvocationContext context) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/Workers\/WorkerFunctionInvoker.cs:line 85\n    at Microsoft.Azure.WebJobs.Script.Description.FunctionInvokerBase.Invoke(Object[] parameters) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionInvokerBase.cs:line 85\n    at Microsoft.Azure.WebJobs.Script.Description.FunctionGenerator.Coerce[T](Task`1 src) in \/src\/azure-functions-host\/src\/WebJobs.Script\/Description\/FunctionGenerator.cs:line 225\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionInvoker`2.InvokeAsync(Object instance, Object[] arguments) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionInvoker.cs:line 52\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.InvokeAsync(IFunctionInvoker invoker, ParameterHelper parameterHelper, CancellationTokenSource timeoutTokenSource, CancellationTokenSource functionCancellationTokenSource, Boolean throwOnTimeout, TimeSpan timerInterval, IFunctionInstance instance) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 587\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithWatchersAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 532\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, ParameterHelper parameterHelper, IFunctionOutputDefinition outputDefinition, ILogger logger, CancellationTokenSource functionCancellationTokenSource) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 470\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 278\n    --- End of inner exception stack trace ---\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.ExecuteWithLoggingAsync(IFunctionInstanceEx instance, FunctionStartedMessage message, FunctionInstanceLogEntry instanceLogEntry, ParameterHelper parameterHelper, ILogger logger, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 325\n    at Microsoft.Azure.WebJobs.Host.Executors.FunctionExecutor.TryExecuteAsyncCore(IFunctionInstanceEx functionInstance, CancellationToken cancellationToken) in C:\\projects\\azure-webjobs-sdk-rqm4t\\src\\Microsoft.Azure.WebJobs.Host\\Executors\\FunctionExecutor.cs:line 117",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1597403151470,
        "Question_favorite_count":5.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"The issue was an incompatible OS version in my virtual environment.\n\nA huge thanks goes to PramodValavala-MSFT for his idea to create a docker container! Following his suggestion, I suddenly got the following error message for the dataset = Dataset.Tabular.from_delimited_files(path = datastore_paths) command:\n\nException: NotImplementedError: Unsupported Linux distribution debian 10.\n\nwhich reminded me of the following warning in the azure machine learning documentation:\n\nChoosing the predefined docker image 2.0-python3.7 (running Debian 9) instead of 3.0-python3.7 (running Debian 10) solved the issue (see https:\/\/hub.docker.com\/_\/microsoft-azure-functions-python).\n\nI suspect that the default virtual environment, which I was using originally, also ran on an incompatible OS.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/67126\/failure-exception-oserror-errno-30-read-only-file.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1597616477787,
        "Question_original_content":"failur except oserror errno read file python azur function issu try prepar submit new experi azur function python regist new dataset workspac contain train data model dataset regist try creat dataset follow line code dataset dataset tabular delimit file path datastor path failur except oserror errno read file idea know shouldn write file azur function possibl actual want write local file want creat dataset refer blob storag datastor path regist workspac method delimit file try write file mayb cach know temp folder write temporari file permit beliv control method write data tri chang current work directori temp folder function chdir tempfil gettempdir help idea think particularli unusu detail python sdk run python script local problem current deploi vscode azur function extens version azur devop pipelin stack trace microsoft azur webjob host functioninvocationexcept except execut function function httptrigger train microsoft azur webjob script worker rpc rpcexcept result failur except oserror errno read file home site root python packag lib site packag dotnetcor bin dep lock stack file azur function host worker python linux azur function worker dispatch line handl invoc request self run sync func invoc func arg file usr local lib python concurr futur thread line run result self self arg self kwarg file azur function host worker python linux azur function worker dispatch line run sync func return func param file home site root httptrigger train init line main train file home site root share code train line train dataset dataset tabular delimit file path datastor path file home site root python packag lib site packag data loggerfactori line wrapper return func arg kwarg file home site root python packag lib site packag data dataset factori line delimit file quot support multi line file home site root python packag lib site packag dataprep api reader line read csv dataflow path file block path archiv option file home site root python packag lib site packag dataprep api dataflow line path file block return datastor dataflow path file home site root python packag lib site packag dataprep api datastor helper line datastor dataflow datastor datastor valu datastor valu sourc file home site root python packag lib site packag dataprep api datastor helper line datastor valu set auth type workspac file home site root python packag lib site packag dataprep api datastor helper line set auth type engin api set aml auth setamlauthmessageargu authtyp serviceprincip json dump auth file home site root python packag lib site packag dataprep api engineapi api line engin api engin api engineapi file home site root python packag lib site packag dataprep api engineapi api line init self messag channel launch engin file home site root python packag lib site packag dataprep api engineapi engin line launch engin depend path runtim ensur depend file home site root python packag lib site packag dotnetcor runtim line ensur depend filelock dep lock path rais timeout timeout except file home site root python packag lib site packag dotnetcor runtim line enter self acquir file home site root python packag lib site packag dotnetcor runtim line acquir self lockfil open self lockfil path creat excl rdwr microsoft azur webjob script descript workerfunctioninvok invokecor object paramet functioninvocationcontext context src azur function host src webjob script descript worker workerfunctioninvok line microsoft azur webjob script descript functioninvokerbas invok object paramet src azur function host src webjob script descript functioninvokerbas line microsoft azur webjob script descript functiongener coerc task src src azur function host src webjob script descript functiongener line microsoft azur webjob host executor functioninvok invokeasync object instanc object argument project azur webjob sdk rqmt src microsoft azur webjob host executor functioninvok line microsoft azur webjob host executor functionexecutor invokeasync ifunctioninvok invok parameterhelp parameterhelp cancellationtokensourc timeouttokensourc cancellationtokensourc functioncancellationtokensourc boolean throwontimeout timespan timerinterv ifunctioninst instanc project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor executewithwatchersasync ifunctioninstanceex instanc parameterhelp parameterhelp ilogg logger cancellationtokensourc functioncancellationtokensourc project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor executewithloggingasync ifunctioninstanceex instanc parameterhelp parameterhelp ifunctionoutputdefinit outputdefinit ilogg logger cancellationtokensourc functioncancellationtokensourc project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor executewithloggingasync ifunctioninstanceex instanc functionstartedmessag messag functioninstancelogentri instancelogentri parameterhelp parameterhelp ilogg logger cancellationtoken cancellationtoken project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line end inner except stack trace microsoft azur webjob host executor functionexecutor executewithloggingasync ifunctioninstanceex instanc functionstartedmessag messag functioninstancelogentri instancelogentri parameterhelp parameterhelp ilogg logger cancellationtoken cancellationtoken project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line microsoft azur webjob host executor functionexecutor tryexecuteasynccor ifunctioninstanceex functioninst cancellationtoken cancellationtoken project azur webjob sdk rqmt src microsoft azur webjob host executor functionexecutor line",
        "Question_preprocessed_content":"failur except oserror file python azur function issu try prepar submit new experi azur function python regist new dataset workspac contain train data model try creat dataset follow line code dataset failur except oserror file idea know shouldn write file azur function possibl actual want write local file want creat dataset refer blob storag regist workspac method try write file know temp folder write temporari file permit beliv control method write data tri chang current work directori temp folder function help idea think particularli detail python sdk run python script local problem current deploi vscode azur function extens version stack trace except execut function result failur except oserror file stack file line arg file line run result file line return func file line main train file line train dataset file line wrapper return func file line file line file line return file line datastor file line file line file line engineapi file line file line file line file line file line acquir paramet functioninvocationcontext context paramet src instanc object argument invok parameterhelp parameterhelp cancellationtokensourc timeouttokensourc cancellationtokensourc functioncancellationtokensourc boolean throwontimeout timespan timerinterv ifunctioninst instanc instanc parameterhelp parameterhelp ilogg logger cancellationtokensourc functioncancellationtokensourc instanc parameterhelp parameterhelp ifunctionoutputdefinit outputdefinit ilogg logger cancellationtokensourc functioncancellationtokensourc instanc functionstartedmessag messag functioninstancelogentri instancelogentri parameterhelp parameterhelp ilogg logger cancellationtoken cancellationtoken end inner except stack trace instanc functionstartedmessag messag functioninstancelogentri instancelogentri parameterhelp parameterhelp ilogg logger cancellationtoken cancellationtoken functioninst cancellationtoken cancellationtoken",
        "Question_gpt_summary_original":"The user is encountering a \"Failure Exception: OSError: [Errno 30] Read-only file system\" error when trying to create a dataset reference to their blob storage using Azure Machine Learning from an Azure Function in Python. They have tried changing the current working directory to the temp folder, but it did not help. The user is using python 3.7 and azureml-sdk 1.9.0 and can run the python script locally without problems.",
        "Question_gpt_summary":"user encount failur except oserror errno read file error try creat dataset refer blob storag azur function python tri chang current work directori temp folder help user python sdk run python script local problem",
        "Answer_original_content":"issu incompat version virtual environ huge thank goe pramodvalavala msft idea creat docker contain follow suggest suddenli got follow error messag dataset dataset tabular delimit file path datastor path command except notimplementederror unsupport linux distribut debian remind follow warn document choos predefin docker imag python run debian instead python run debian solv issu http hub docker com microsoft azur function python suspect default virtual environ origin ran incompat",
        "Answer_preprocessed_content":"issu incompat version virtual environ huge thank goe idea creat docker contain follow suggest suddenli got follow error messag dataset command except notimplementederror unsupport linux distribut debian remind follow warn document choos predefin docker imag instead solv issu suspect default virtual environ origin ran incompat",
        "Answer_gpt_summary_original":"Solution: The user was able to solve the issue by creating a docker container and choosing the predefined docker image 2.0-python3.7 (running Debian 9) instead of 3.0-python3.7 (running Debian 10). This solved the issue as the default virtual environment, which the user was using originally, also ran on an incompatible OS.",
        "Answer_gpt_summary":"solut user abl solv issu creat docker contain choos predefin docker imag python run debian instead python run debian solv issu default virtual environ user origin ran incompat"
    },
    {
        "Question_title":"Powershell AzureML Get-AmlWorkspace",
        "Question_body":"<pre><code>Get-AmlWorkspace : One or more errors occurred.\nAt line:1 char:1\n+ Get-AmlWorkspace\n+ ~~~~~~~~~~~~~~~~\n+ CategoryInfo          : NotSpecified: (:) [Get-AmlWorkspace], \nAggregateException\n+ FullyQualifiedErrorId : \nSystem.AggregateException,AzureML.PowerShell.GetWorkspace\n<\/code><\/pre>\n\n<p>I am trying to use Powershell to connect to Azure ML studio as it looks like an easier way to manage a workspace. I've downloaded the dll file from <a href=\"https:\/\/github.com\/hning86\/azuremlps\" rel=\"nofollow noreferrer\">https:\/\/github.com\/hning86\/azuremlps<\/a> and changed my config.json file, but get the error above if I try to run any AzureML commands. I've unblocked the DLL file and imported the AzureMLPS module, and I can see the module and commands I am trying to use have been imported by doing <code>Get-Module<\/code> and <code>Get-Command<\/code><\/p>\n\n<p>For info I've not used Powershell before.<\/p>\n\n<p>Any suggestions much appreciated!<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1499681034153,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":428.0,
        "Answer_body":"<p>Have you installed Azure PowerShell Installer on your local machine?\n<strong><a href=\"https:\/\/github.com\/Azure\/azure-powershell\/releases\" rel=\"nofollow noreferrer\">Click here<\/a><\/strong> for more info.<\/p>\n\n<p>Download the latest <strong>Azure PowerShell Installer (4.3.1)<\/strong>, then install on your local machine. Then retry using Azure PowerShell module and commands.<\/p>\n\n<p>I installed mine last May, using Azure PowerShell 4.0.1, and the command Get-AmlWorkspace is working.<\/p>\n\n<pre><code># Set local folder location\nSet-Location -Path \"C:\\Insert here the location of AzureMLPS.dll\"\n\n# Unblock and import Azure Powershell Module (leverages config.json file)\nUnblock-File .\\AzureMLPS.dll\nImport-Module .\\AzureMLPS.dll\n\n# Get Azure ML Workspace info\nGet-AmlWorkspace\n<\/code><\/pre>\n\n<p>The output on my side looks like this:\n<a href=\"https:\/\/i.stack.imgur.com\/mEGeT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mEGeT.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45009184",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1503389654688,
        "Question_original_content":"powershel amlworkspac amlworkspac error occur line char amlworkspac categoryinfo notspecifi amlworkspac aggregateexcept fullyqualifiederrorid aggregateexcept powershel getworkspac try us powershel connect studio look like easier wai manag workspac download dll file http github com hning chang config json file error try run command unblock dll file import modul modul command try us import modul command info powershel suggest appreci",
        "Question_preprocessed_content":"powershel try us powershel connect studio look like easier wai manag workspac download dll file chang file error try run command unblock dll file import modul modul command try us import info powershel suggest appreci",
        "Question_gpt_summary_original":"The user is encountering errors while trying to use Powershell to connect to Azure ML studio. They have downloaded the necessary dll file and changed the config.json file, but still receive an error when running AzureML commands. The user has imported the AzureMLPS module and can see the imported commands, but is still unable to use them.",
        "Question_gpt_summary":"user encount error try us powershel connect studio download necessari dll file chang config json file receiv error run command user import modul import command unabl us",
        "Answer_original_content":"instal azur powershel instal local machin click info download latest azur powershel instal instal local machin retri azur powershel modul command instal azur powershel command amlworkspac work set local folder locat set locat path insert locat dll unblock import azur powershel modul leverag config json file unblock file dll import modul dll workspac info amlworkspac output look like",
        "Answer_preprocessed_content":"instal azur powershel instal local machin click info download latest azur powershel instal instal local machin retri azur powershel modul command instal azur powershel command work output look like",
        "Answer_gpt_summary_original":"Solution:\n- Install the latest Azure PowerShell Installer (4.3.1) on the local machine.\n- Set the local folder location and unblock and import the Azure PowerShell module using the config.json file.\n- Use the Get-AmlWorkspace command to get Azure ML Workspace info.",
        "Answer_gpt_summary":"solut instal latest azur powershel instal local machin set local folder locat unblock import azur powershel modul config json file us amlworkspac command workspac info"
    },
    {
        "Question_title":"Where does Python modules installed on Azure Machine Learning Studio",
        "Question_body":"<p>I understand the azure machine learning studio (classic) version using Anaconda distribution but my question is where would the python modules like pandas\/tensorflow are installed when using <strong>IPython interface of Azure ML<\/strong>. Is this on AML studio itself or in azure blob (AML studio uses blob as backend store)? <\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1578440246977,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1578440835316,
        "Question_score":0.0,
        "Question_view_count":136.0,
        "Answer_body":"<p>Here is my screenshots for tabs <code>EXPERIMENTS<\/code> and <code>NOTEBOOKS<\/code> in Azure Machine Learning Studio (classic), as the figures below.<\/p>\n\n<p>Fig 1. I created a <code>Excute Python Script<\/code> module with the code to print the <code>sys.path<\/code> and the real path of <code>pandas<\/code> installed.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/KdkJa.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/KdkJa.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 2. The <code>View output log<\/code> page of the code in Fig 1 shows <code>EXPERIMENTS<\/code> is a runtime of Anaconda on Windows. <\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/zo9te.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/zo9te.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 3. I created a notebook named <code>demo<\/code> and run the same code as Fig 1, the result shows <code>NOTEBOOKS<\/code> is a runtime of Anaconda on Linux, even the notenooks url is started with <code>notebooks.azure.com<\/code>.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/uAGRk.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/uAGRk.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Fig 4. I used different commands like <code>lsb_release -a<\/code>, <code>fdisk -l<\/code>, <code>lsdev<\/code>, <code>ls \/dev<\/code>, <code>df -a<\/code> to try to see the Linux version and its disk or partition information, the result shows it's a Ubuntu Linux container.<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/R4wC6.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/R4wC6.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Other infomation what you want to know, you can try to check by yourself.<\/p>",
        "Answer_comment_count":3.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59637596",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1578465543192,
        "Question_original_content":"python modul instal studio understand studio classic version anaconda distribut question python modul like panda tensorflow instal ipython interfac aml studio azur blob aml studio us blob backend store",
        "Question_preprocessed_content":"python modul instal studio understand studio version anaconda distribut question python modul like instal ipython interfac aml studio azur blob",
        "Question_gpt_summary_original":"The user is unsure where Python modules like pandas and tensorflow are installed when using the IPython interface of Azure ML. They are questioning whether the modules are installed on AML studio or in Azure blob, which is used as the backend store for AML studio.",
        "Question_gpt_summary":"user unsur python modul like panda tensorflow instal ipython interfac question modul instal aml studio azur blob backend store aml studio",
        "Answer_original_content":"screenshot tab experi notebook studio classic figur fig creat excut python script modul code print sy path real path panda instal fig view output log page code fig show experi runtim anaconda window fig creat notebook name demo run code fig result show notebook runtim anaconda linux notenook url start notebook azur com fig differ command like lsb releas fdisk lsdev dev try linux version disk partit inform result show ubuntu linux contain infom want know try check",
        "Answer_preprocessed_content":"screenshot tab studio figur fig creat modul code print real path instal fig page code fig show runtim anaconda window fig creat notebook name run code fig result show runtim anaconda linux notenook url start fig differ command like try linux version disk partit inform result show ubuntu linux contain infom want know try check",
        "Answer_gpt_summary_original":"Solution: The user can check the runtime environment of Azure ML Studio by creating an \"Execute Python Script\" module and printing the `sys.path` and the real path of the module they are interested in. They can also check the runtime environment of a notebook by running the same code in the notebook and checking the output. In this case, the user found that the runtime environment for \"EXPERIMENTS\" was Anaconda on Windows, while the runtime environment for \"NOTEBOOKS\" was Anaconda on Linux. Additionally, the user used various commands to determine that the Linux version was Ubuntu.",
        "Answer_gpt_summary":"solut user check runtim environ studio creat execut python script modul print sy path real path modul interest check runtim environ notebook run code notebook check output case user runtim environ experi anaconda window runtim environ notebook anaconda linux addition user command determin linux version ubuntu"
    },
    {
        "Question_title":"Machine Learning\u306b\u3064\u3044\u3066\u306e\u8cea\u554f",
        "Question_body":"\u63b2\u984c\u306e\u4ef6\u306b\u3064\u304d\u307e\u3057\u3066\u3001\u73fe\u5728Machine Learning\u3092\u4f7f\u7528\u3057\u3066\u6a5f\u68b0\u5b66\u7fd2\u3092\u884c\u3063\u3066\u3044\u307e\u3059\u3002\n\u305d\u3053\u3067\u8cea\u554f\u306b\u306a\u308b\u306e\u3067\u3059\u304c\u3001\u30c7\u30b6\u30a4\u30ca\u30fc\u6a5f\u80fd\u3092\u4f7f\u7528\u3057\u3066\u5b66\u7fd2\u7d50\u679c\u3092CSV\u3067\u30a8\u30af\u30b9\u30dd\u30fc\u30c8\u3057\u3088\u3046\u3068\u3057\u3066\u3044\u308b\u306e\u3067\u3059\u304c\u3001\nExport Data\u30e2\u30c7\u30eb\u3067CSV\u5f62\u5f0f\u306b\u8a2d\u5b9a\u3057\u3066\u3044\u3066\u3082CSV\u3067\u306f\u306a\u3044\u5f62\u5f0f\u3067\u5171\u6709\u305b\u308c\u3066\u3057\u307e\u3046\u306e\u3067\u3059\u304c\u3001\u539f\u56e0\u304c\u308f\u304b\u3089\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002\n\u3054\u6559\u793a\u306e\u307b\u3069\u3088\u308d\u3057\u304f\u304a\u9858\u3044\u3044\u305f\u3057\u307e\u3059\u3002",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1631251067517,
        "Question_favorite_count":9.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"@63862379 Are you referring to the export data module of the designer from ml.azure.com?\nI think I understand the issue, Are you seeing that the .csv format of file is not listed on the blob storage?\n\nSince the input is a dataframe directory to export module the output format selected should still be the format you selected, in this case CSV. The file name extension only might be missing. You can still open the csv file in excel and it will recognize the delimiters and headers so you can convert it into excel files.\n\nYou can also avoid this by providing the .csv extension in the path itself in export settings and file will be exported as a csv file directly.\n\n\n\n\nPlease don't forget to click on  or upvote  button whenever the information provided helps you. Original posters help the community find answers faster by identifying the correct answer. Here is how\n\nWant a reminder to come back and check responses? Here is how to subscribe to a notification\n\nIf you are interested in joining the VM program and help shape the future of Q&A: Here is how you can be part of Q&A Volunteer Moderators",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/546760\/machine-learning%E3%81%AB%E3%81%A4%E3%81%84%E3%81%A6%E3%81%AE%E8%B3%AA%E5%95%8F.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1631268910236,
        "Question_original_content":"machin learn machin learn csv export datacsvcsv",
        "Question_preprocessed_content":"machin learn machin learn csv export datacsvcsv",
        "Question_gpt_summary_original":"The user is facing challenges with exporting machine learning results in CSV format using the designer function. Despite setting the Export Data model to CSV format, the shared file is not in CSV format, and the user is unsure of the cause of the issue.",
        "Question_gpt_summary":"user face challeng export machin learn result csv format design function despit set export data model csv format share file csv format user unsur caus issu",
        "Answer_original_content":"refer export data modul design azur com think understand issu see csv format file list blob storag input datafram directori export modul output format select format select case csv file extens miss open csv file excel recogn delimit header convert excel file avoid provid csv extens path export set file export csv file directli forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_preprocessed_content":"refer export data modul design think understand issu see csv format file list blob storag input datafram directori export modul output format select format select case csv file extens miss open csv file excel recogn delimit header convert excel file avoid provid csv extens path export set file export csv file directli forget click upvot button inform provid help origin poster help commun answer faster identifi correct answer want remind come check respons subscrib notif interest join program help shape futur volunt moder",
        "Answer_gpt_summary_original":"Solutions provided:\n- The file may still be in CSV format even if the file extension is missing. The user can try opening the file in Excel and it should recognize the delimiters and headers, allowing the user to convert it into an Excel file.\n- The user can provide the .csv extension in the path itself in export settings to ensure that the file is exported as a CSV file directly.",
        "Answer_gpt_summary":"solut provid file csv format file extens miss user try open file excel recogn delimit header allow user convert excel file user provid csv extens path export set ensur file export csv file directli"
    },
    {
        "Question_title":"sagemaker horizontally scaling tensorflow (keras) model",
        "Question_body":"<p>I am roughly following this script <a href=\"https:\/\/gitlab.com\/juliensimon\/dlnotebooks\/blob\/master\/keras\/05-keras-blog-post\/Fashion%20MNIST-SageMaker.ipynb\" rel=\"nofollow noreferrer\">fashion-MNIST-sagemaker<\/a>.<\/p>\n\n<p>I see that in the notebook <\/p>\n\n<pre><code>from sagemaker.tensorflow import TensorFlow\n\ntf_estimator = TensorFlow(entry_point='mnist_keras_tf.py', \n                          role=role,\n                          train_instance_count=1, \n                          train_instance_type='local',\n                          framework_version='1.12', \n                          py_version='py3',\n                          script_mode=True,\n                          hyperparameters={'epochs': 1}\n                         )\n<\/code><\/pre>\n\n<p>I am wondering to what extent I can and should use the <code>train_instance_count<\/code> parameter. Will it distribute training along some dimension automatically, if yes - what is the dimension?<\/p>\n\n<p>Further, does it generally make sense to distribute training horizontally in a keras (with tensorflow) based setting?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1577037857400,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":295.0,
        "Answer_body":"<p>distributed training is model and framework specific. Not all models are easy to distribute, and from ML framework to ML framework things are not equally easy. <strong>It is rarely automatic, even less so with TensorFlow and Keras.<\/strong><\/p>\n\n<p>Neural nets are conceptually easy to distribute under the data-parallel paradigm, whereby the gradient computation of a given mini-batch is split among workers, which could be multiple devices in the same host (multi-device) or multiple hosts with each multiple devices (multi-device multi-host). The D2L.ai course provides an in-depth view of how neural nets are distributed <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/multiple-gpus.html\" rel=\"nofollow noreferrer\">here<\/a> and <a href=\"http:\/\/d2l.ai\/chapter_computational-performance\/parameterserver.html\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Keras used to be trivial to distribute in multi-device, single host fashion with the <a href=\"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/utils\/multi_gpu_model\" rel=\"nofollow noreferrer\"><code>multi_gpu_model<\/code>, which will sadly get deprecated in 4 months<\/a>. In your case, you seem to refer to multi-host model (more than one machine), and that requires writing ad-hoc synchronization code such as the one seen in <a href=\"https:\/\/www.tensorflow.org\/tutorials\/distribute\/multi_worker_with_keras\" rel=\"nofollow noreferrer\">this official tutorial<\/a>. <\/p>\n\n<p>Now let's look at how does this relate to SageMaker.<\/p>\n\n<p>SageMaker comes with 3 options for algorithm development. Using distributed training may require a varying amount of custom work depending on the option you choose:<\/p>\n\n<ol>\n<li><p>The <strong>built-in algorithms<\/strong> is a library of 18 pre-written algorithms. Many of them are written to be distributed in single-host multi-GPU or multi-GPU multi-host. With that first option, you don't have anything to do apart from setting <code>train_instance_count<\/code> > 1 to distribute over multiple instances<\/p><\/li>\n<li><p>The <strong>Framework containers<\/strong> (the option you are using) are containers developed for popular frameworks (TensorFlow, PyTorch, Sklearn, MXNet) and provide pre-written docker environment in which you can write arbitrary code. In this options, some container will support one-click creation of ephemeral training clusters to do distributed training, <strong>however using <code>train_instance_count<\/code> greater than one is not enough to distribute the training of your model. It will just run your script on multiple machines. In order to distribute your training, you must write appropriate distribution and synchronization code in your <code>mnist_keras_tf.py<\/code> script.<\/strong> For some frameworks such code modification will be very simple, for example for TensorFlow and Keras, SageMaker comes with Horovod pre-installed. Horovod is a peer-to-peer ring-style communication mechanism that requires very little code modification and is highly scalable (<a href=\"https:\/\/eng.uber.com\/horovod\/\" rel=\"nofollow noreferrer\">initial annoucement from Uber<\/a>, <a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/using_tf.html#training-with-horovod\" rel=\"nofollow noreferrer\">SageMaker doc<\/a>, <a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/blob\/master\/sagemaker-python-sdk\/tensorflow_script_mode_horovod\/tensorflow_script_mode_horovod.ipynb\" rel=\"nofollow noreferrer\">SageMaker example<\/a>, <a href=\"https:\/\/aws.amazon.com\/fr\/blogs\/machine-learning\/launching-tensorflow-distributed-training-easily-with-horovod-or-parameter-servers-in-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">SageMaker blog post<\/a>). My recommendation would be to try using Horovod to distribute your code. Similarly, in Apache MXNet you can easily create Parameter Stores to host model parameters in a distributed fashion and sync with them from multiple nodes. MXNet scalability and ease of distribution is one of the reason Amazon loves it.<\/p><\/li>\n<li><p>The <strong>Bring-Your-Own Container<\/strong> requires you to write both docker container and algorithm code. In this situation, you can of course distribute your training over multiple machines but you also have to write machine-to-machine communication code<\/p><\/li>\n<\/ol>\n\n<p>For your specific situation my recommendation would be to scale horizontally first in a single node with multiple GPUs over bigger and bigger machine types, because latency and complexity increase drastically as you switch from single-host to multi-host context. If truly necessary, use multi-node context and things maybe easier if that's done with Horovod.\nIn any case, things are still much easier to do with SageMaker since it manages creation of ephemeral, billed-per-second clusters with built-in, logging and metadata and artifacts persistence and also handles fast training data loading from s3, sharded over training nodes. <\/p>\n\n<p><strong>Note on the relevancy of distributed training<\/strong>: Keep in mind that when you distribute over N devices a model that was running fine on one device, you usually grow the batch size by N so that the per-device batch size stays constant and each device keeps being busy. This will disturb your model convergence, because bigger batches means a less noisy SGD. A common heuristic is to grow the learning rate by N (more info in <a href=\"https:\/\/arxiv.org\/abs\/1706.02677\" rel=\"nofollow noreferrer\">this great paper from Priya Goyal et al<\/a>), but this on the other hand induces instability at the first couple epochs, so it is sometimes associated with a learning rate warmup. Scaling SGD to work well with very large batches is still an active research problem, with new ideas coming up frequently. Reaching good model performance with very large batches sometimes require ad-hoc research and a fair amount of parameter tuning, occasionally to the extent where the extra money spent on finding how to distribute well overcome the benefits of the faster training you eventually manage to run. A situation where distributed training makes sense is when an individual record represent too much compute to form a big enough physical batch on a device, a situation seen on big input sizes (eg vision over HD pictures) or big parameter counts (eg BERT). That being said, for those models requiring very big logical batch you don't necessarily have to distribute things physically: you can run sequentially N batches through your single GPU and wait N per-device batches before doing the gradient averaging and parameter update to simulate having an N times bigger GPU. (a clever hack sometimes called <em>gradient accumulation<\/em>)<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1577101498307,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/59446807",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1577100115740,
        "Question_original_content":"horizont scale tensorflow kera model roughli follow script fashion mnist notebook tensorflow import tensorflow estim tensorflow entri point mnist kera role role train instanc count train instanc type local framework version version script mode true hyperparamet epoch wonder extent us train instanc count paramet distribut train dimens automat ye dimens gener sens distribut train horizont kera tensorflow base set",
        "Question_preprocessed_content":"horizont scale tensorflow model roughli follow script notebook wonder extent us paramet distribut train dimens automat ye dimens gener sens distribut train horizont kera base set",
        "Question_gpt_summary_original":"The user is following a script for horizontally scaling a TensorFlow (Keras) model in SageMaker. They are unsure about the extent to which they should use the \"train_instance_count\" parameter and whether it will distribute training automatically. They are also questioning the effectiveness of horizontally scaling training in a Keras (with TensorFlow) based setting.",
        "Question_gpt_summary":"user follow script horizont scale tensorflow kera model unsur extent us train instanc count paramet distribut train automat question effect horizont scale train kera tensorflow base set",
        "Answer_original_content":"distribut train model framework specif model easi distribut framework framework thing equal easi rare automat tensorflow kera neural net conceptu easi distribut data parallel paradigm gradient comput given mini batch split worker multipl devic host multi devic multipl host multipl devic multi devic multi host cours provid depth view neural net distribut kera trivial distribut multi devic singl host fashion multi gpu model sadli deprec month case refer multi host model machin requir write hoc synchron code seen offici tutori let look relat come option algorithm develop distribut train requir vari custom work depend option choos built algorithm librari pre written algorithm written distribut singl host multi gpu multi gpu multi host option apart set train instanc count distribut multipl instanc framework contain option contain develop popular framework tensorflow pytorch sklearn mxnet provid pre written docker environ write arbitrari code option contain support click creation ephemer train cluster distribut train train instanc count greater distribut train model run script multipl machin order distribut train write appropri distribut synchron code mnist kera script framework code modif simpl exampl tensorflow kera come horovod pre instal horovod peer peer ring style commun mechan requir littl code modif highli scalabl initi annouc uber doc exampl blog post recommend try horovod distribut code similarli apach mxnet easili creat paramet store host model paramet distribut fashion sync multipl node mxnet scalabl eas distribut reason amazon love bring contain requir write docker contain algorithm code situat cours distribut train multipl machin write machin machin commun code specif situat recommend scale horizont singl node multipl gpu bigger bigger machin type latenc complex increas drastic switch singl host multi host context truli necessari us multi node context thing mayb easier horovod case thing easier manag creation ephemer bill second cluster built log metadata artifact persist handl fast train data load shard train node note relev distribut train mind distribut devic model run fine devic usual grow batch size devic batch size stai constant devic keep busi disturb model converg bigger batch mean noisi sgd common heurist grow learn rate info great paper priya goyal hand induc instabl coupl epoch associ learn rate warmup scale sgd work larg batch activ research problem new idea come frequent reach good model perform larg batch requir hoc research fair paramet tune occasion extent extra monei spent find distribut overcom benefit faster train eventu manag run situat distribut train make sens individu record repres comput form big physic batch devic situat seen big input size vision pictur big paramet count bert said model requir big logic batch necessarili distribut thing physic run sequenti batch singl gpu wait devic batch gradient averag paramet updat simul have time bigger gpu clever hack call gradient accumul",
        "Answer_preprocessed_content":"distribut train model framework specif model easi distribut framework framework thing equal easi rare automat tensorflow kera neural net conceptu easi distribut paradigm gradient comput given split worker multipl devic host multipl host multipl devic cours provid view neural net distribut kera trivial distribut singl host fashion sadli deprec month case refer model requir write synchron code seen offici tutori let look relat come option algorithm develop distribut train requir vari custom work depend option choos algorithm librari algorithm written distribut option apart set distribut multipl instanc framework contain contain develop popular framework provid docker environ write arbitrari code option contain support creation ephemer train cluster distribut train greater distribut train model run script multipl machin order distribut train write appropri distribut synchron code script framework code modif simpl exampl tensorflow kera come horovod horovod commun mechan requir littl code modif highli scalabl recommend try horovod distribut code similarli apach mxnet easili creat paramet store host model paramet distribut fashion sync multipl node mxnet scalabl eas distribut reason amazon love contain requir write docker contain algorithm code situat cours distribut train multipl machin write commun code specif situat recommend scale horizont singl node multipl gpu bigger bigger machin type latenc complex increas drastic switch context truli necessari us context thing mayb easier horovod case thing easier manag creation ephemer cluster log metadata artifact persist handl fast train data load shard train node note relev distribut train mind distribut devic model run fine devic usual grow batch size batch size stai constant devic keep busi disturb model converg bigger batch mean noisi sgd common heurist grow learn rate hand induc instabl coupl epoch associ learn rate warmup scale sgd work larg batch activ research problem new idea come frequent reach good model perform larg batch requir research fair paramet tune occasion extent extra monei spent find distribut overcom benefit faster train eventu manag run situat distribut train make sens individu record repres comput form big physic batch devic situat seen big input size big paramet count said model requir big logic batch necessarili distribut thing physic run sequenti batch singl gpu wait batch gradient averag paramet updat simul have time bigger gpu",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. For using SageMaker's built-in algorithms, setting the \"train_instance_count\" parameter to a value greater than 1 is enough to distribute over multiple instances.\n2. For using SageMaker's Framework containers, using \"train_instance_count\" greater than one is not enough to distribute the training of the model. Appropriate distribution and synchronization code must be written in the script. Horovod can be used for TensorFlow and Keras to distribute the code.\n3. For using SageMaker's Bring-Your-Own Container, machine-to-machine communication code must be written.\n4. Scaling horizontally first in a single node with multiple GPUs over bigger and bigger machine types is recommended before switching to multi-host context",
        "Answer_gpt_summary":"possibl solut mention discuss built algorithm set train instanc count paramet valu greater distribut multipl instanc framework contain train instanc count greater distribut train model appropri distribut synchron code written script horovod tensorflow kera distribut code bring contain machin machin commun code written scale horizont singl node multipl gpu bigger bigger machin type recommend switch multi host context"
    },
    {
        "Question_title":"AWS SageMaker is not authorized to perform: ecr:CreateRepository on resource: *",
        "Question_body":"<p>I am creating my own Docker image so that I can use my own models in AWS SageMaker. I sucessfully created a Docker image using command line inside the Jupyter Notebook in SageMaker ml.t2.medium instance using a customized Dockerfile:<\/p>\n\n<pre><code>REPOSITORY            TAG                 IMAGE ID            CREATED             SIZE\nsklearn               latest              01234212345        6 minutes ago       1.23GB\n<\/code><\/pre>\n\n<p>But when I run in Jupyter:<\/p>\n\n<pre><code>! aws ecr create-repository --repository-name sklearn\n<\/code><\/pre>\n\n<p>I get the following error:<\/p>\n\n<pre><code>An error occurred (AccessDeniedException) when calling the CreateRepository operation: User: arn:aws:sts::1234567:assumed-role\/AmazonSageMaker-ExecutionRole-12345\/SageMaker is not authorized to perform: ecr:CreateRepository on resource: *\n<\/code><\/pre>\n\n<p>I already set up SageMaker, EC2, EC2ContainerService permissions and the following policy for EC2Container but I still get the same error.<\/p>\n\n<pre><code>{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Effect\": \"Allow\",\n      \"Action\": [\n        \"sagemaker:*\",\n        \"ec2:*\"\n      ],\n      \"Resource\": \"*\"\n    }\n  ]\n}\n<\/code><\/pre>\n\n<p>Any idea on how I can solve this issue?<\/p>\n\n<p>Thanks in advance.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1528052776997,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1528055205112,
        "Question_score":1.0,
        "Question_view_count":4509.0,
        "Answer_body":"<p>I solved the problem. We must set a permission at SageMaker Execution Role as following:<\/p>\n\n<pre><code>{\n\"Version\": \"2012-10-17\",\n\"Statement\": [\n    {\n        \"Effect\": \"Allow\",\n        \"Action\": [\n            \"ecr:*\"            ],\n        \"Resource\": \"*\"\n    }\n]}\n<\/code><\/pre>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/50669991",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1528126261856,
        "Question_original_content":"author perform ecr createrepositori resourc creat docker imag us model sucessfulli creat docker imag command line insid jupyt notebook medium instanc custom dockerfil repositori tag imag creat size sklearn latest minut ago run jupyt aw ecr creat repositori repositori sklearn follow error error occur accessdeniedexcept call createrepositori oper user arn aw st assum role amazon executionrol author perform ecr createrepositori resourc set eccontainerservic permiss follow polici eccontain error version statement effect allow action resourc idea solv issu thank advanc",
        "Question_preprocessed_content":"author perform ecr createrepositori resourc creat docker imag us model sucessfulli creat docker imag command line insid jupyt notebook instanc custom dockerfil run jupyt follow error set containerservic permiss follow polici contain error idea solv issu thank advanc",
        "Question_gpt_summary_original":"The user is encountering an \"AccessDeniedException\" error when trying to create a repository using AWS ECR in SageMaker. The user has already set up permissions for SageMaker, EC2, and EC2ContainerService, but the error persists. The user is seeking advice on how to resolve the issue.",
        "Question_gpt_summary":"user encount accessdeniedexcept error try creat repositori aw ecr user set permiss eccontainerservic error persist user seek advic resolv issu",
        "Answer_original_content":"solv problem set permiss execut role follow version statement effect allow action ecr resourc",
        "Answer_preprocessed_content":"solv problem set permiss execut role follow",
        "Answer_gpt_summary_original":"Solution: The user can resolve the \"AccessDeniedException\" error by setting a permission at SageMaker Execution Role as shown in the code snippet provided in the discussion.",
        "Answer_gpt_summary":"solut user resolv accessdeniedexcept error set permiss execut role shown code snippet provid discuss"
    },
    {
        "Question_title":"Kedro - Memory management",
        "Question_body":"<p>I am working on a Kedro 0.17.2 project that is running on out-of-memory issues and I'm trying to reduce the memory footprint.<\/p>\n<p>I'm doing the profiling by using <code>mprof<\/code> from the <code>memory-profiler<\/code> library and I noticed that there is always a child process and memory seems to duplicate in the main process after the first computation in the node that is running. Is it possible that Kedro is duplicating the dataframes in memory? And, if so, is there a way to avoid this?<\/p>\n<p>Notes:<\/p>\n<ul>\n<li>I'm using the <code>SequentialRunner<\/code><\/li>\n<li>I'm not using the <code>is_async<\/code> cli option<\/li>\n<li>I'm not using either multithreading or multiprocessing in the node execution<\/li>\n<\/ul>\n<p><a href=\"https:\/\/i.stack.imgur.com\/87hhM.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/87hhM.png\" alt=\"enter image description here\" \/><\/a><\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1634349281137,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":3.0,
        "Question_view_count":308.0,
        "Answer_body":"<p>It turns out this issue is caused by a possible bug in the <code>memory-profiler<\/code> library that is used in the <code>kedro.extras.decorators.memory_profiler.mem_profile<\/code> decorator.<\/p>\n<p>The kedro decorator makes use of the <code>memory_usage<\/code> function in the <code>memory-profiler<\/code> module. It is used to sample the total memory being used by the running function from within the python process.<\/p>\n<p>There is an open issue about this problem but with no solution yet.\n<a href=\"https:\/\/github.com\/pythonprofilers\/memory_profiler\/issues\/332\" rel=\"nofollow noreferrer\">https:\/\/github.com\/pythonprofilers\/memory_profiler\/issues\/332<\/a><\/p>\n<p>For the moment I have just removed the decorator.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69592125",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1634968179750,
        "Question_original_content":"memori manag work project run memori issu try reduc memori footprint profil mprof memori profil librari notic child process memori duplic main process comput node run possibl duplic datafram memori wai avoid note sequentialrunn async cli option multithread multiprocess node execut",
        "Question_preprocessed_content":"memori manag work project run issu try reduc memori footprint profil librari notic child process memori duplic main process comput node run possibl duplic datafram memori wai avoid note cli option multithread multiprocess node execut",
        "Question_gpt_summary_original":"The user is facing out-of-memory issues while working on a Kedro 0.17.2 project. They are using mprof from the memory-profiler library for profiling and have noticed that there is always a child process and memory seems to duplicate in the main process after the first computation in the node that is running. The user is wondering if Kedro is duplicating the dataframes in memory and if there is a way to avoid this. They have provided some notes on their usage of SequentialRunner, is_async cli option, and multithreading\/multiprocessing in node execution.",
        "Question_gpt_summary":"user face memori issu work project mprof memori profil librari profil notic child process memori duplic main process comput node run user wonder duplic datafram memori wai avoid provid note usag sequentialrunn async cli option multithread multiprocess node execut",
        "Answer_original_content":"turn issu caus possibl bug memori profil librari extra decor memori profil mem profil decor decor make us memori usag function memori profil modul sampl total memori run function python process open issu problem solut http github com pythonprofil memori profil issu moment remov decor",
        "Answer_preprocessed_content":"turn issu caus possibl bug librari decor decor make us function modul sampl total memori run function python process open issu problem solut moment remov decor",
        "Answer_gpt_summary_original":"Solution: There is currently no solution to the out-of-memory issue caused by the memory-profiler library used in the kedro.extras.decorators.memory_profiler.mem_profile decorator. The issue has been reported on GitHub, but no solution has been provided yet. The user has removed the decorator for the time being.",
        "Answer_gpt_summary":"solut current solut memori issu caus memori profil librari extra decor memori profil mem profil decor issu report github solut provid user remov decor time"
    },
    {
        "Question_title":"Deploying custom model on Azure ML Studio",
        "Question_body":"<p>In Azure ML Studio, we have the option of choosing a number of inbuilt ML models like Classification, Regression, etc. , which we can drag and drop to our workflow.<\/p>\n\n<p>My question is, can I upload a custom ML model that I have built locally on my system in Python, and add it to the workflow?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1565761282107,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1565767277470,
        "Question_score":1.0,
        "Question_view_count":1187.0,
        "Answer_body":"<ol>\n<li>Take the model.pkl file, zip it, and upload it into Azure Machine Learning Studio. Click the \u201cNew\u201d icon in the bottom left:\n<a href=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/Iwvhi.jpg\" alt=\"\"><\/a><\/li>\n<li>In the pane that comes up, click on dataset, and then \u201cFrom Local File\u201d:\n<a href=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/DvyjO.jpg\" alt=\"\"><\/a><\/li>\n<li>Select the zip file where you stored your serialized model and click the tick. You expirement should look like this:\n<a href=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/0efka.jpg\" alt=\"\"><\/a><\/li>\n<li>Put the following code to run your classification experiment:<\/li>\n<\/ol>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pandas as pd\nimport sys\nimport pickle\n\ndef azureml_main(dataframe1 = None, dataframe2 = None):\n    sys.path.insert(0,\".\\Script Bundle\")\n    model = pickle.load(open(\".\\Script Bundle\\model.pkl\", 'rb'))\n    pred = model.predict(dataframe1)\n    return pd.DataFrame([pred[0]])\n<\/code><\/pre>\n\n<p><strong>Update<\/strong> \nIf you want to declare this experiment as an API you need to add web input and output to the Python script module.\n<a href=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/eqV8W.png\" alt=\"enter image description here\"><\/a><\/p>",
        "Answer_comment_count":7.0,
        "Answer_last_edit_time":1566565696980,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57488706",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1566202471556,
        "Question_original_content":"deploi custom model studio studio option choos number inbuilt model like classif regress drag drop workflow question upload custom model built local python add workflow",
        "Question_preprocessed_content":"deploi custom model studio studio option choos number inbuilt model like classif regress drag drop workflow question upload custom model built local python add workflow",
        "Question_gpt_summary_original":"The user is facing a challenge of deploying a custom machine learning model on Azure ML Studio and wants to know if it is possible to upload a locally built model in Python to the workflow.",
        "Question_gpt_summary":"user face challeng deploi custom machin learn model studio want know possibl upload local built model python workflow",
        "Answer_original_content":"model pkl file zip upload studio click new icon left pane come click dataset local file select zip file store serial model click tick expir look like follow code run classif experi import panda import sy import pickl def main datafram datafram sy path insert script bundl model pickl load open script bundl model pkl pred model predict datafram return datafram pred updat want declar experi api need add web input output python script modul",
        "Answer_preprocessed_content":"file zip upload studio click new icon left pane come click dataset local file select zip file store serial model click tick expir look like follow code run classif experi updat want declar experi api need add web input output python script modul",
        "Answer_gpt_summary_original":"Solution: The user can upload the locally built model in Python to Azure ML Studio by zipping the model.pkl file and uploading it as a dataset in the workflow. Then, the user can run the classification experiment by adding the provided code to the Python script module. If the user wants to declare this experiment as an API, they need to add web input and output to the Python script module.",
        "Answer_gpt_summary":"solut user upload local built model python studio zip model pkl file upload dataset workflow user run classif experi ad provid code python script modul user want declar experi api need add web input output python script modul"
    },
    {
        "Question_title":"An example to create a training model on real data in AzureML",
        "Question_body":"<p>Can you introduce a real sample for azure ML and show how can it be possible to see the result of the training?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1483472453410,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":76.0,
        "Answer_body":"<p><a href=\"http:\/\/blog.learningtree.com\/how-to-build-a-predictive-model-using-azure-machine-learning\/\" rel=\"nofollow noreferrer\">Here<\/a> is a good sample to create your first model.\nI should notice that I can't load data from url, as there is a forbidden error to load from url, and I don't know why! \nAnyhow, you can import data manually by copy the data from <a href=\"http:\/\/blog.learningtree.com\/wp-content\/uploads\/2015\/01\/breast-cancer-wisconsin.data_.arff_.txt\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Also, you can find the created model which is published here: \n<a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1\" rel=\"nofollow noreferrer\">https:\/\/gallery.cortanaintelligence.com\/Experiment\/Cancer-Model-1<\/a><\/p>\n\n<p>About see the result of the training model, you can right click on the tick (highlighted by a red circle in the following picture) of Evaluation Model. Then, in the opened menu, go to \"Evaluation Result -> Visualization\".\n<a href=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/vhJWE.jpg\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>After that you can see a window like the following (which shows ROC curve and some related result such as accuracy of the training model):<\/p>\n\n<p><a href=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/FAuzU.png\" alt=\"enter image description here\"><\/a>\n<a href=\"https:\/\/i.stack.imgur.com\/B39lI.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/B39lI.png\" alt=\"enter image description here\"><\/a><\/p>\n\n<p>Besides, you can see <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/machine-learning-azure-ml-customer-churn-scenario\" rel=\"nofollow noreferrer\">this example<\/a> as an another sample.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1483537837232,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/41451123",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1483472453408,
        "Question_original_content":"exampl creat train model real data introduc real sampl possibl result train",
        "Question_preprocessed_content":"exampl creat train model real data introduc real sampl possibl result train",
        "Question_gpt_summary_original":"The user is requesting a real sample for Azure ML and wants to know how to view the results of the training.",
        "Question_gpt_summary":"user request real sampl want know view result train",
        "Answer_original_content":"good sampl creat model notic load data url forbidden error load url know import data manual copi data creat model publish http galleri cortanaintellig com experi cancer model result train model right click tick highlight red circl follow pictur evalu model open menu evalu result visual window like follow show roc curv relat result accuraci train model exampl sampl",
        "Answer_preprocessed_content":"good sampl creat model notic load data url forbidden error load url know import data manual copi data creat model publish result train model right click tick evalu model open menu evalu result visual window like follow exampl sampl",
        "Answer_gpt_summary_original":"Solution:\n- A sample to create the first model is provided in the link.\n- Data can be imported manually by copying it from the given link.\n- The created model is published and can be found in the provided link.\n- To view the result of the training model, right-click on the tick of Evaluation Model and go to \"Evaluation Result -> Visualization\". A window will appear showing the ROC curve and related results such as accuracy of the training model.\n- Another example is provided as a sample.",
        "Answer_gpt_summary":"solut sampl creat model provid link data import manual copi given link creat model publish provid link view result train model right click tick evalu model evalu result visual window appear show roc curv relat result accuraci train model exampl provid sampl"
    },
    {
        "Question_title":"Get Variable from SageMaker Script Processor",
        "Question_body":"<p>I am using SageMaker for distributed TensorFlow model training and serving.  I am trying to get the shape of the pre-processed datasets from the ScriptProcessor so I can provide it to the TensorFlow Environment.<\/p>\n<pre><code>script_processor = ScriptProcessor(command=['python3'],\n                image_uri=preprocess_img_uri,\n                role=role,\n                instance_count=1,\n                sagemaker_session=sm_session,\n                instance_type=preprocess_instance_type)\n\nscript_processor.run(code=preprocess_script_uri,\n                inputs=[ProcessingInput(\n                        source=source_dir + username + '\/' + dataset_name,\n                        destination='\/opt\/ml\/processing\/input')],\n                outputs=[\n                        ProcessingOutput(output_name=&quot;train_data&quot;, source=&quot;\/opt\/ml\/processing\/train&quot;),\n                        ProcessingOutput(output_name=&quot;test_data&quot;, source=&quot;\/opt\/ml\/processing\/test&quot;)\n                ],\n\n                arguments = ['--filepath', dataset_name, '--labels', 'labels', '--test_size', '0.2', '--shuffle', 'False', '--lookback', '5',])\n\npreprocessing_job_description = script_processor.jobs[-1].describe()\n\noutput_config = preprocessing_job_description[&quot;ProcessingOutputConfig&quot;]\nfor output in output_config[&quot;Outputs&quot;]:\n    if output[&quot;OutputName&quot;] == &quot;train_data&quot;:\n        preprocessed_training_data = output[&quot;S3Output&quot;][&quot;S3Uri&quot;]\n    if output[&quot;OutputName&quot;] == &quot;test_data&quot;:\n        preprocessed_test_data = output[&quot;S3Output&quot;][&quot;S3Uri&quot;]\n<\/code><\/pre>\n<p>I would like to get the following data:<\/p>\n<pre><code>pre_processed_train_data_shape = script_processor.train_data_shape?\n<\/code><\/pre>\n<p>I am just not sure how to get the value out of the docker container.  I have reviewed the documentation here:<a href=\"https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html\" rel=\"nofollow noreferrer\">https:\/\/sagemaker.readthedocs.io\/en\/stable\/api\/training\/processing.html<\/a><\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1647581865580,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":334.0,
        "Answer_body":"<p>There are a few options:<\/p>\n<ol>\n<li><p>Write some data to a text file at <code>\/opt\/ml\/output\/message<\/code>, then call <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/sagemaker\/describe-processing-job.html\" rel=\"nofollow noreferrer\">DescribeProcessingJob<\/a> (using Boto3 or the AWS CLI or API) and retrieve the <code>ExitMessage<\/code> value<\/p>\n<pre><code>aws sagemaker describe-processing-job \\\n  --processing-job-name foo \\\n  --output text \\\n  --query ExitMessage\n<\/code><\/pre>\n<\/li>\n<li><p>Add a new output to your processing job and send data there<\/p>\n<\/li>\n<li><p>If your <code>train_data<\/code> is in CSV, JSON, or Parquet then use an <a href=\"https:\/\/docs.aws.amazon.com\/cli\/latest\/reference\/s3api\/select-object-content.html\" rel=\"nofollow noreferrer\">S3 Select query<\/a> on <code>train_data<\/code> for it's # of rows\/columns<\/p>\n<pre><code>aws s3api select-object-content \\\n  --bucket foo \\\n  --key 'path\/to\/train_data.csv' \\\n  --expression &quot;SELECT count(*) FROM s3object&quot; \\\n  --expression-type 'SQL' \\\n  --input-serialization '{&quot;CSV&quot;: {}}' \\\n  --output-serialization '{&quot;CSV&quot;: {}}' \/dev\/stdout\n<\/code><\/pre>\n<\/li>\n<\/ol>\n<p>Set <code>expression<\/code> to <code>select * from s3object limit 1<\/code> to get the columns<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1654189401480,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71522857",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1647621141128,
        "Question_original_content":"variabl script processor distribut tensorflow model train serv try shape pre process dataset scriptprocessor provid tensorflow environ script processor scriptprocessor command python imag uri preprocess img uri role role instanc count session session instanc type preprocess instanc type script processor run code preprocess script uri input processinginput sourc sourc dir usernam dataset destin opt process input output processingoutput output train data sourc opt process train processingoutput output test data sourc opt process test argument filepath dataset label label test size shuffl fals lookback preprocess job descript script processor job output config preprocess job descript processingoutputconfig output output config output output outputnam train data preprocess train data output soutput suri output outputnam test data preprocess test data output soutput suri like follow data pre process train data shape script processor train data shape sure valu docker contain review document http readthedoc stabl api train process html",
        "Question_preprocessed_content":"variabl script processor distribut tensorflow model train serv try shape dataset scriptprocessor provid tensorflow environ like follow data sure valu docker contain review document",
        "Question_gpt_summary_original":"The user is facing a challenge in getting the shape of pre-processed datasets from the ScriptProcessor in SageMaker for distributed TensorFlow model training and serving. They are unsure of how to get the value out of the docker container and have reviewed the documentation for guidance.",
        "Question_gpt_summary":"user face challeng get shape pre process dataset scriptprocessor distribut tensorflow model train serv unsur valu docker contain review document guidanc",
        "Answer_original_content":"option write data text file opt output messag describeprocessingjob boto aw cli api retriev exitmessag valu process job process job foo output text queri exitmessag add new output process job send data train data csv json parquet us select queri train data row column aw sapi select object content bucket foo kei path train data csv express select count sobject express type sql input serial csv output serial csv dev stdout set express select sobject limit column",
        "Answer_preprocessed_content":"option write data text file describeprocessingjob retriev valu add new output process job send data csv json parquet us select queri set column",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Write some data to a text file at `\/opt\/ml\/output\/message`, then call `DescribeProcessingJob` (using Boto3 or the AWS CLI or API) and retrieve the `ExitMessage` value.\n2. Add a new output to the processing job and send data there.\n3. If the `train_data` is in CSV, JSON, or Parquet, then use an `S3 Select query` on `train_data` for its number of rows\/columns.\n\nNo personal opinions or biases are included in the summary.",
        "Answer_gpt_summary":"possibl solut mention discuss write data text file opt output messag describeprocessingjob boto aw cli api retriev exitmessag valu add new output process job send data train data csv json parquet us select queri train data number row column person opinion bias includ summari"
    },
    {
        "Question_title":"[problem at MMS predict] At MMS(sagemaker), error code(500), type(InternalServerException)",
        "Question_body":"I make pytorch model with sagemaker, MMS. This is my mms code.\n\n%%time\ninstance_type = 'c5.large'\n# accelerator_type = 'eia2.medium'\npredictor = mme.deploy(\n    initial_instance_count=1,\n    instance_type=f\"ml.{instance_type}\"\n)\n\nmme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\")\nlist(mme.list_models())\n#> [ 'model.tar.gz']\n\nI try to predict with this code.\n\nstart_time = time.time()\npredicted_value = predictor.predict(requests, target_model=\"LV1\")\nduration = time.time() - start_time\nprint(\"${:,.2f}, took {:,d} ms\\n\".format(predicted_value[0], int(duration * 1000)))\n\nAnd, return error message.\n\nModelError: An error occurred (ModelError) when calling the InvokeEndpoint operation: Received server error (500) from model with message \"{\n  \"code\": 500,\n  \"type\": \"InternalServerException\",\n  \"message\": \"Failed to start workers\"\n}\n\n\nMMS with pytorch is 'little' difficult. X)\n\nhelp me, please.",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1660209790025,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":80.0,
        "Answer_body":"Hi , I think your target model on the prediction needs to have the name of the model you have deployed - for example , when you are adding the model with mme.add_model(model_data_source=model_path, model_data_path=\"model.tar.gz\") the model_data_path contains the name of the model . From the sagemaker-examples: (https:\/\/github.com\/aws\/amazon-sagemaker-examples\/blob\/main\/advanced_functionality\/multi_model_xgboost_home_value\/xgboost_multi_model_endpoint_home_value.ipynb) **model_data_path is the relative path to the S3 prefix we specified above (i.e. model_data_prefix) where our endpoint will source models for inference requests.Since this is a relative path, we can simply pass the name of what we wish to call the model artifact at inference time (i.e. Chicago_IL.tar.gz). In your case \"model.tar.gz\". However, when predicting you call the model ,target_model=\"LV1\"?",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QUBCxtcfyrTymZ7isHG3X5Qg\/problem-at-mms-predict-at-mms-sagemaker-error-code-500-type-internal-server-exception",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1660312923532,
        "Question_original_content":"problem mm predict mm error code type internalserverexcept pytorch model mm mm code time instanc type larg acceler type eia medium predictor mme deploi initi instanc count instanc type instanc type mme add model model data sourc model path model data path model tar list mme list model model tar try predict code start time time time predict valu predictor predict request target model durat time time start time print took format predict valu int durat return error messag modelerror error occur modelerror call invokeendpoint oper receiv server error model messag code type internalserverexcept messag fail start worker mm pytorch littl difficult help",
        "Question_preprocessed_content":"mm error code type pytorch model mm mm code time predictor try predict code durat took int return error messag modelerror error occur call invokeendpoint oper receiv server error model messag code type internalserverexcept messag fail start worker mm pytorch littl difficult help",
        "Question_gpt_summary_original":"The user encountered an error code 500 with type InternalServerException while trying to predict with a pytorch model using MMS on Sagemaker. The error message stated that the workers failed to start, and the user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error code type internalserverexcept try predict pytorch model mm error messag state worker fail start user seek help resolv issu",
        "Answer_original_content":"think target model predict need model deploi exampl ad model mme add model model data sourc model path model data path model tar model data path contain model exampl http github com aw amazon exampl blob main advanc function multi model xgboost home valu xgboost multi model endpoint home valu ipynb model data path rel path prefix specifi model data prefix endpoint sourc model infer request rel path simpli pass wish model artifact infer time chicago tar case model tar predict model target model",
        "Answer_preprocessed_content":"think target model predict need model deploi exampl ad model contain model exampl rel path prefix specifi endpoint sourc model infer rel path simpli pass wish model artifact infer time case predict model",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to ensure that the target model on the prediction has the name of the model that has been deployed. The model_data_path should contain the name of the model, and when predicting, the model should be called with the correct target_model name.",
        "Answer_gpt_summary":"solut possibl solut mention discuss ensur target model predict model deploi model data path contain model predict model call correct target model"
    },
    {
        "Question_title":"What is called within a sagemaker custom (training) container?",
        "Question_body":"<p>Somewhere this spring the behaviour of the sagemaker docker image changed and I cannot find the way I need to construct it now.<\/p>\n<p><strong>Directory structure<\/strong><\/p>\n<pre><code>\/src\/some\/package\n\/project1\n    \/some_entrypoint.py\n    \/some_notebook.ipynb\n\/project2\n    \/another_entrypoint.py\n    \/another_notebook.ipynb\nsetup.py\n<\/code><\/pre>\n<p><strong>Docker file<\/strong><\/p>\n<p>Note that I want to shift tensorflow version, so I changed the <code>FROM<\/code> to the latest version. This was the\nbreaking change.<\/p>\n<pre><code># Core\nFROM 763104351884.dkr.ecr.eu-west-1.amazonaws.com\/tensorflow-training:2.3.0-cpu-py37-ubuntu18.04\n\nCOPY . \/opt\/ml\/code\/all\/\nRUN pip install \/opt\/ml\/code\/all\/\n\nWORKDIR &quot;\/opt\/ml\/code&quot;\n<\/code><\/pre>\n<p><strong>Python code<\/strong><\/p>\n<p>This code should start the entrypoints, for example here we have the code of some_notebook.ipynb. I tried all possible combinations of working directory + source_dir (None, '.', or '..'), entry_point (with or without \/), dependencies ('src')...<\/p>\n<ul>\n<li>if setup is present it tries to call my project as a module (python -m some_entrypoint)<\/li>\n<li>if not, it often is not able to find my entrypoint. Which I don't understand because the TensorFlow is supposed to add it to the container, isn't it?<\/li>\n<\/ul>\n<pre><code>estimator = TensorFlow(\n   entry_point='some_entrypoint.py', \n   image_name='ECR.dkr.ecr.eu-west-1.amazonaws.com\/overall-project\/sagemaker-training:latest',\n   source_dir='.',\n#    dependencies=['..\/src\/'],\n   script_mode=True,\n\n   train_instance_type='ml.m5.4xlarge',\n   train_instance_count=1,\n   train_max_run=60*60,  # seconds * minutes\n   train_max_wait=60*60,  # seconds * minutes. Must be &gt;= train_max_run\n   hyperparameters=hyperparameters,\n   metric_definitions=metrics,\n   role=role,\n   framework_version='2.0.0',\n   py_version='py3',\n  )\nestimator.fit({\n    'training': f&quot;s3:\/\/some-data\/&quot;}\n#   , wait=False\n)\n<\/code><\/pre>\n<p>Ideally I would want to understand the logic within: what is called given what settings?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1597694559337,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1597697877660,
        "Question_score":0.0,
        "Question_view_count":61.0,
        "Answer_body":"<p>when the training container runs, your entry_point script will be executed.<\/p>\n<p>Since your notebook file and entry_point script are under the same directory, your <code>source_dir<\/code> should just be &quot;.&quot;<\/p>\n<p>Does your entry_point script import any modules that are not installed by the tensorflow training container by default? Also could you share your stacktrace of the error?<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63457857",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1600272249596,
        "Question_original_content":"call custom train contain spring behaviour docker imag chang wai need construct directori structur src packag project entrypoint notebook ipynb project entrypoint notebook ipynb setup docker file note want shift tensorflow version chang latest version break chang core dkr ecr west amazonaw com tensorflow train cpu ubuntu copi opt code run pip instal opt code workdir opt code python code code start entrypoint exampl code notebook ipynb tri possibl combin work directori sourc dir entri point depend src setup present tri project modul python entrypoint abl entrypoint understand tensorflow suppos add contain isn estim tensorflow entri point entrypoint imag ecr dkr ecr west amazonaw com overal project train latest sourc dir depend src script mode true train instanc type xlarg train instanc count train max run second minut train max wait second minut train max run hyperparamet hyperparamet metric definit metric role role framework version version estim fit train data wait fals ideal want understand logic call given set",
        "Question_preprocessed_content":"call custom contain spring behaviour docker imag chang wai need construct directori structur docker file note want shift tensorflow version chang latest version break chang python code code start entrypoint exampl code tri possibl combin work directori depend setup present tri project modul abl entrypoint understand tensorflow suppos add contain isn ideal want understand logic call given set",
        "Question_gpt_summary_original":"The user is facing challenges with the behavior of the sagemaker docker image, specifically with the directory structure, docker file, and python code. They are trying to shift the tensorflow version and start the entrypoints, but have tried all possible combinations of working directory, source_dir, entry_point, and dependencies, and are still unable to find the entrypoint. The user is seeking to understand the logic within and what is called given the settings.",
        "Question_gpt_summary":"user face challeng behavior docker imag specif directori structur docker file python code try shift tensorflow version start entrypoint tri possibl combin work directori sourc dir entri point depend unabl entrypoint user seek understand logic call given set",
        "Answer_original_content":"train contain run entri point script execut notebook file entri point script directori sourc dir entri point script import modul instal tensorflow train contain default share stacktrac error",
        "Answer_preprocessed_content":"train contain run script execut notebook file script directori script import modul instal tensorflow train contain default share stacktrac error",
        "Answer_gpt_summary_original":"Solutions provided:\n- Set the source_dir to \".\" since the notebook file and entry_point script are under the same directory.\n- Check if the entry_point script imports any modules that are not installed by the tensorflow training container by default.\n- Share the stacktrace of the error to get more specific help.\n\nNo personal opinions or biases were included in the response.",
        "Answer_gpt_summary":"solut provid set sourc dir notebook file entri point script directori check entri point script import modul instal tensorflow train contain default share stacktrac error specif help person opinion bias includ respons"
    },
    {
        "Question_title":"How do I retrieve a model in Vertex AI?",
        "Question_body":"<p>I defined a training job:<\/p>\n<pre><code>job = aiplatform.AutoMLTextTrainingJob(...\n<\/code><\/pre>\n<p>then I created a model by running the job:<\/p>\n<pre><code>model = job.run(...\n<\/code><\/pre>\n<p>It worked fine but it is now the next day and the variable <code>model<\/code> was in a Jupyter notebook and no longer exists. I have tried to get it back with:<\/p>\n<pre><code>from google.cloud import aiplatform_v1beta1\n\ndef sample_get_model():\n    client = aiplatform_v1beta1.ModelServiceClient()\n\n    model_id=id_of_training_pipeline\n    name= f'projects\/{PROJECT}\/locations\/{REGION}\/models\/{model_id}'\n    \n    request = aiplatform_v1beta1.GetModelRequest(name=name)\n    response = client.get_model(request=request)\n    print(response)\n\nsample_get_model()\n<\/code><\/pre>\n<p>I have also tried the id of v1 of the model created in place of <code>id_of_training_pipeline<\/code> and I have tried <code>\/pipelines\/pipeline_id<\/code><\/p>\n<p>but I get:\n<code>E0805 15:12:36.784008212   28406 hpack_parser.cc:1234]       Error parsing metadata: error=invalid value key=content-type value=text\/html; charset=UTF-8<\/code><\/p>\n<p>(<code>PROJECT<\/code> and <code>REGION<\/code> are set correctly).<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1659709186783,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":45.0,
        "Answer_body":"<p>Found <a href=\"https:\/\/cloud.google.com\/vertex-ai\/docs\/samples\/aiplatform-get-model-sample#aiplatform_get_model_sample-python\" rel=\"nofollow noreferrer\">this<\/a> Google code which works.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73251212",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1659712728223,
        "Question_original_content":"retriev model defin train job job aiplatform automltexttrainingjob creat model run job model job run work fine dai variabl model jupyt notebook longer exist tri googl cloud import aiplatform vbeta def sampl model client aiplatform vbeta modelservicecli model train pipelin project project locat region model model request aiplatform vbeta getmodelrequest respons client model request request print respons sampl model tri model creat place train pipelin tri pipelin pipelin hpack parser error pars metadata error invalid valu kei content type valu text html charset utf project region set correctli",
        "Question_preprocessed_content":"retriev model defin train job creat model run job work fine dai variabl jupyt notebook longer exist tri tri model creat place tri set correctli",
        "Question_gpt_summary_original":"The user is facing a challenge in retrieving a model in Vertex AI that was created by running a training job in a Jupyter notebook. The user has tried to retrieve the model using the id of the training pipeline and the v1 id of the model, but is encountering an error related to parsing metadata.",
        "Question_gpt_summary":"user face challeng retriev model creat run train job jupyt notebook user tri retriev model train pipelin model encount error relat pars metadata",
        "Answer_original_content":"googl code work",
        "Answer_preprocessed_content":"googl code work",
        "Answer_gpt_summary_original":"Solution: A Google code was suggested which can be used to retrieve the model in Vertex AI that was created by running a training job in a Jupyter notebook.",
        "Answer_gpt_summary":"solut googl code suggest retriev model creat run train job jupyt notebook"
    },
    {
        "Question_title":"Azure ML studio - Container Registry Error while trying to submit a pipeline",
        "Question_body":"<p>I'm having the following error while trying to submit an Azure ML Studio pipeline<\/p>\n<p><code>Get credentials or pull docker image failed with err: error response from daemon: get https:\/\/lgcrmldev.azurecr.io\/v2\/azureml\/azureml_977f5bda2f6f4f634482661c121c8959\/manifests\/latest: unauthorized: authentication required, visit https:\/\/aka.ms\/acr\/authorization for more information.<\/code><\/p>\n<p>The notebook python code I'm doing is something on these lines:<\/p>\n<pre><code># create a Python script to do the actual work and save it in the pipeline folder:\n\n%%writefile $experiment_folder\/batch_online_retail.py\nimport os\nimport numpy as np\nfrom azureml.core import Model\nimport joblib\n\n\n# Called when the service is loaded\ndef init():\n    global model\n    \n    # Load the model\n    model_path = Model.get_model_path('Random_Forest_model')\n    model = joblib.load(model_path)\n\ndef run(batch):\n    try:\n        result = []\n        \n    # Process each line\n    for in range (len(batch)):\n        # Read the comma-delimited data into an array\n        data = np.genfromtxt(f, delimiter=',')        \n        # Reshape into a 2-dimensional array for prediction (model expects multiple items)\n        prediction = model.predict(data.reshape(1, -1))        \n        # Append prediction to results\n        resultList.append(&quot;{}: {}&quot;.format(os.path.basename(f), prediction[0]))\n    return resultList      \n<\/code><\/pre>\n<pre><code># Creating the run context\nfrom azureml.core import Environment\nfrom azureml.core.runconfig import DEFAULT_CPU_IMAGE\nfrom azureml.core.runconfig import CondaDependencies\n\n# Add dependencies required by the model\n# For scikit-learn models, you need scikit-learn\n# For parallel pipeline steps, you need azureml-core and azureml-dataprep[fuse]\ncd = CondaDependencies.create(conda_packages=['scikit-learn','pip'],\n                              pip_packages=['azureml-defaults','azureml-core','azureml-dataprep[fuse,pandas]'])\n\nbatch_env = Environment(name='batch_environment')\nbatch_env.python.conda_dependencies = cd\nbatch_env.docker.enabled = True\nbatch_env.docker.base_image = DEFAULT_CPU_IMAGE\nprint('Configuration ready.')\n\n<\/code><\/pre>\n<pre><code># Creating the ParallelRunStep\nfrom azureml.pipeline.steps import ParallelRunConfig, ParallelRunStep\nfrom azureml.pipeline.core import PipelineData\n\ndefault_ds = ws.get_default_datastore()\n\noutput_dir = PipelineData(name='inferences', \n                          datastore=default_ds, \n                          output_path_on_compute='online-retail\/results')\n\nparallel_run_config = ParallelRunConfig(\n    source_directory=experiment_folder,\n    entry_script=&quot;batch_online_retail.py&quot;,\n    mini_batch_size=&quot;5&quot;,\n    error_threshold=10,\n    output_action=&quot;append_row&quot;,\n    environment=batch_env,\n    compute_target=inference_cluster,\n    node_count=2)\n\nparallelrun_step = ParallelRunStep(\n    name='batch-score-retail',\n    parallel_run_config=parallel_run_config,\n    inputs=[batch_data_set.as_named_input('online_retail_batch')],\n    output=output_dir,\n    arguments=[],\n    allow_reuse=True\n)\n\nprint('Steps defined')\n<\/code><\/pre>\n<p>and finally,<\/p>\n<pre><code># Create an Azure ML experiment in your workspace, put the step into a pipeline and run it\nfrom azureml.core import Experiment\nfrom azureml.pipeline.core import Pipeline\n\npipeline = Pipeline(workspace=ws, steps=[parallelrun_step])\npipeline_run = Experiment(ws, 'online-retail-deployment-cf').submit(pipeline)\npipeline_run.wait_for_completion(show_output=True)\n<\/code><\/pre>\n<p>It's in this final step that I keep getting the error above.<\/p>\n<p>My Container Registry has my user and Azure ML resource as a Contributor in the access control panel, so I don't think it's lack of permissions.<\/p>\n<p>I've found this Microsoft page that seems to have a fix for the error I'm having:\n<a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-faq#docker-push-succeeds-but-docker-pull-fails-with-error-unauthorized-authentication-required\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/container-registry\/container-registry-faq#docker-push-succeeds-but-docker-pull-fails-with-error-unauthorized-authentication-required<\/a><\/p>\n<p>But I don't understand how can I implement the suggested fix. This is because the Docker image the notebook uses is inside the Compute Instance created in Azure ML which we have limited access.<\/p>\n<p>Any ideas on what is the problem and how to fix it?<\/p>\n<p>Thank you in advance,\nCarla<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1613668319863,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":869.0,
        "Answer_body":"<p>According to the example <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-train-with-custom-image#define-your-environment\" rel=\"nofollow noreferrer\">here<\/a>, I think you need to configure the environment variables for the docker images stored in the Azure Container Registry:<\/p>\n<pre><code>batch_env = Environment(name='batch_environment')\nbatch_env.python.conda_dependencies = cd\nbatch_env.docker.enabled = True\n# Set the container registry information.\nbatch_env.docker.base_image_registry.address = &quot;myregistry.azurecr.io&quot;\nbatch_env.docker.base_image_registry.username = &quot;username&quot;\nbatch_env.docker.base_image_registry.password = &quot;password&quot;\nbatch_env.docker.base_image = &quot;myregistry.azurecr.io\/DEFAULT_CPU_IMAGE&quot;\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66264795",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1613725945092,
        "Question_original_content":"studio contain registri error try submit pipelin have follow error try submit studio pipelin credenti pull docker imag fail err error respons daemon http lgcrmldev azurecr fbdafffcc manifest latest unauthor authent requir visit http aka acr author inform notebook python code line creat python script actual work save pipelin folder writefil experi folder batch onlin retail import import numpi core import model import joblib call servic load def init global model load model model path model model path random forest model model joblib load model path def run batch try result process line rang len batch read comma delimit data arrai data genfromtxt delimit reshap dimension arrai predict model expect multipl item predict model predict data reshap append predict result resultlist append format path basenam predict return resultlist creat run context core import environ core runconfig import default cpu imag core runconfig import condadepend add depend requir model scikit learn model need scikit learn parallel pipelin step need core dataprep fuse condadepend creat conda packag scikit learn pip pip packag default core dataprep fuse panda batch env environ batch environ batch env python conda depend batch env docker enabl true batch env docker base imag default cpu imag print configur readi creat parallelrunstep pipelin step import parallelrunconfig parallelrunstep pipelin core import pipelinedata default default datastor output dir pipelinedata infer datastor default output path comput onlin retail result parallel run config parallelrunconfig sourc directori experi folder entri script batch onlin retail mini batch size error threshold output action append row environ batch env comput target infer cluster node count parallelrun step parallelrunstep batch score retail parallel run config parallel run config input batch data set name input onlin retail batch output output dir argument allow reus true print step defin final creat experi workspac step pipelin run core import experi pipelin core import pipelin pipelin pipelin workspac step parallelrun step pipelin run experi onlin retail deploy submit pipelin pipelin run wait complet output true final step get error contain registri user resourc contributor access control panel think lack permiss microsoft page fix error have http doc microsoft com azur contain registri contain registri faq docker push succe docker pull fail error unauthor authent requir understand implement suggest fix docker imag notebook us insid comput instanc creat limit access idea problem fix thank advanc carla",
        "Question_preprocessed_content":"studio contain registri error try submit pipelin have follow error try submit studio pipelin notebook python code line final final step get error contain registri user resourc contributor access control panel think lack permiss microsoft page fix error have understand implement suggest fix docker imag notebook us insid comput instanc creat limit access idea problem fix thank advanc carla",
        "Question_gpt_summary_original":"The user is encountering an error while trying to submit an Azure ML Studio pipeline, specifically a Container Registry Error. The error message indicates that authentication is required to access the registry. The user has checked their permissions and found that they have Contributor access to the registry. The user has found a Microsoft page with a suggested fix but is unsure how to implement it since the Docker image used by the notebook is inside the Compute Instance created in Azure ML, which has limited access. The user is seeking help to understand the problem and how to fix it.",
        "Question_gpt_summary":"user encount error try submit studio pipelin specif contain registri error error messag indic authent requir access registri user check permiss contributor access registri user microsoft page suggest fix unsur implement docker imag notebook insid comput instanc creat limit access user seek help understand problem fix",
        "Answer_original_content":"accord exampl think need configur environ variabl docker imag store azur contain registri batch env environ batch environ batch env python conda depend batch env docker enabl true set contain registri inform batch env docker base imag registri address myregistri azurecr batch env docker base imag registri usernam usernam batch env docker base imag registri password password batch env docker base imag myregistri azurecr default cpu imag",
        "Answer_preprocessed_content":"accord exampl think need configur environ variabl docker imag store azur contain registri",
        "Answer_gpt_summary_original":"Solution: One possible solution suggested in the discussion is to configure the environment variables for the Docker images stored in the Azure Container Registry. This can be done by setting the container registry information in the environment variables of the Docker image. The example provided in the discussion shows how to do this.",
        "Answer_gpt_summary":"solut possibl solut suggest discuss configur environ variabl docker imag store azur contain registri set contain registri inform environ variabl docker imag exampl provid discuss show"
    },
    {
        "Question_title":"Azure ML endpoint 404 error",
        "Question_body":"<p>An Azure Data Factory pipeline for updating a trained ML model returns this error:<\/p>\n\n<pre><code>HTTP 404. The resource you are looking for (or one of its dependencies) could have been removed, had its name changed, or is temporarily unavailable. Please review the following URL and make sure that it is spelled correctly.\nRequested URL: \/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update\n\nDiagnostic details: job ID xxxx. Endpoint https:\/\/services.azureml.net\/workspaces\/xxxx\/webservices\/xxxx\/endpoints\/update.\n<\/code><\/pre>\n\n<p>I don't even want to think about why it returned a HTML document...\nI am 100% sure that the endpoint exists and the key provided is correct.\nSo what is my mistake?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1503048195263,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1503233060163,
        "Question_score":0.0,
        "Question_view_count":494.0,
        "Answer_body":"<p>Deleting and creating the endpoint again fixed it.<\/p>\n\n<p>Microsoft...<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/45753090",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1508942544572,
        "Question_original_content":"endpoint error azur data factori pipelin updat train model return error http resourc look depend remov chang temporarili unavail review follow url sure spell correctli request url workspac webservic endpoint updat diagnost detail job endpoint http servic net workspac webservic endpoint updat want think return html document sure endpoint exist kei provid correct mistak",
        "Question_preprocessed_content":"endpoint error azur data factori pipelin updat train model return error want think return html sure endpoint exist kei provid correct mistak",
        "Question_gpt_summary_original":"The user encountered a 404 error when trying to update a trained ML model using an Azure Data Factory pipeline. The error message suggests that the resource may have been removed, had its name changed, or is temporarily unavailable. The user is certain that the endpoint exists and the key provided is correct, but is unsure of what mistake they may have made.",
        "Question_gpt_summary":"user encount error try updat train model azur data factori pipelin error messag suggest resourc remov chang temporarili unavail user certain endpoint exist kei provid correct unsur mistak",
        "Answer_original_content":"delet creat endpoint fix microsoft",
        "Answer_preprocessed_content":"delet creat endpoint fix",
        "Answer_gpt_summary_original":"Solution: The user resolved the issue by deleting and creating the endpoint again. No other solutions were mentioned in the discussion.",
        "Answer_gpt_summary":"solut user resolv issu delet creat endpoint solut mention discuss"
    },
    {
        "Question_title":"Sagemaker file size limit?",
        "Question_body":"<p>I was wondering whether there is some size limit for storing a folder in sagemaker studio?\nI have this dataset stored in s3 but I want to download that dataset in my sagemaker studio environment to train my model. Is there some kind of limit in the size of a file i can download?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1638620800060,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":398.0,
        "Answer_body":"<p>Your <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/studio-tasks-manage-storage.html\" rel=\"nofollow noreferrer\">home directory in Amazon SageMaker Studio is stored in Amazon EFS file system<\/a>. The file system grows and shrink as you add and remove files.<br \/>\nAccording to <a href=\"https:\/\/docs.aws.amazon.com\/efs\/latest\/ug\/limits.html#limits-fs-specific\" rel=\"nofollow noreferrer\">Amazon EFS limits<\/a> the <strong>maximum size for a single file is 47.9TiB.<\/strong><\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70225564",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1638800978928,
        "Question_original_content":"file size limit wonder size limit store folder studio dataset store want download dataset studio environ train model kind limit size file download",
        "Question_preprocessed_content":"file size limit wonder size limit store folder studio dataset store want download dataset studio environ train model kind limit size file download",
        "Question_gpt_summary_original":"The user is inquiring about the size limit for storing a folder in Sagemaker Studio and whether there is a limit on the size of a file that can be downloaded to the environment for training a model.",
        "Question_gpt_summary":"user inquir size limit store folder studio limit size file download environ train model",
        "Answer_original_content":"home directori studio store amazon ef file file grow shrink add remov file accord amazon ef limit maximum size singl file tib",
        "Answer_preprocessed_content":"home directori studio store amazon ef file file grow shrink add remov file accord amazon ef limit maximum size singl file",
        "Answer_gpt_summary_original":"Solution: The maximum size for a single file in Sagemaker Studio is 47.9TiB, according to Amazon EFS limits.",
        "Answer_gpt_summary":"solut maximum size singl file studio tib accord amazon ef limit"
    },
    {
        "Question_title":"How to get a graph with the best performing runs via Sweeps (Weights & Biases)?",
        "Question_body":"<p>For my NER model I use Weights &amp; Biases sweeps for hyperparameter search. I do a grid search with about 100 runs and there are some really meaningful graphs. However, I can't figure out how to create a graph that shows about the best 10 runs in terms of f-score. Does anyone know how to do this?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":3,
        "Question_created_time":1658389162900,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":58.0,
        "Answer_body":"<p>In the sweep view, you can filter runs by certain criteria by clicking this button:\n<a href=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/kVFUC.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>There, you can add a filter to only show runs with an f1 score, or an accuracy or whatever metric you have logged higher than a certain value:\n<a href=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/tAVJF.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Of course, this won't filter for the 10 best runs, but for all runs with an accuracy of 0.9 and higher (example in picture).<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73062370",
        "Tool":"Weights & Biases",
        "Platform":"Stack Overflow",
        "Question_closed_time":1658393400156,
        "Question_original_content":"graph best perform run sweep ner model us sweep hyperparamet search grid search run meaning graph figur creat graph show best run term score know",
        "Question_preprocessed_content":"graph best perform run sweep ner model us sweep hyperparamet search grid search run meaning graph figur creat graph show best run term know",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a graph that shows the top 10 performing runs in terms of f-score for their NER model using Weights & Biases sweeps for hyperparameter search. They have already done a grid search with about 100 runs and have some meaningful graphs, but are unsure how to create a graph specifically for the best 10 runs.",
        "Question_gpt_summary":"user face challeng creat graph show perform run term score ner model sweep hyperparamet search grid search run meaning graph unsur creat graph specif best run",
        "Answer_original_content":"sweep view filter run certain criteria click button add filter run score accuraci metric log higher certain valu cours won filter best run run accuraci higher exampl pictur",
        "Answer_preprocessed_content":"sweep view filter run certain criteria click button add filter run score accuraci metric log higher certain valu cours won filter best run run accuraci higher",
        "Answer_gpt_summary_original":"Solution: The user can filter runs by certain criteria in the sweep view by clicking on the filter button and adding a filter to only show runs with a certain metric logged higher than a certain value. However, this won't filter for the 10 best runs specifically.",
        "Answer_gpt_summary":"solut user filter run certain criteria sweep view click filter button ad filter run certain metric log higher certain valu won filter best run specif"
    },
    {
        "Question_title":"Converting rating column into boolean column with custom filter in Azure ML",
        "Question_body":"<p>I have a dataset in which the Rating column is an integer column with values ranging from 1 to 10.<\/p>\n\n<p>I would like to convert that column into a simple boolean positive\/negative categorical column, so that if the value is less than 6 it is a negative rating, and if it is greater or equal 6 it would become a positive rating.<\/p>\n\n<p>I'm not sure how to do that.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1496660018143,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":240.0,
        "Answer_body":"<p>Azure Machine Learning allows at least 3 options to do that:<\/p>\n\n<ul>\n<li>Apply SQL Transformation <code>select *,case when rating&lt;6 then 0 else 1 end RatingB from t1<\/code><\/li>\n<li>Execute Python Script <code>return dataframe1.rating[dataframe1.rating &lt; 6] = 0<\/code><\/li>\n<li>Execute R Script <code>dataset1$rating[dataset1$rating &lt; 6] &lt;- 0<\/code><\/li>\n<\/ul>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/44367367",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1498563773572,
        "Question_original_content":"convert rate column boolean column custom filter dataset rate column integ column valu rang like convert column simpl boolean posit neg categor column valu neg rate greater equal posit rate sure",
        "Question_preprocessed_content":"convert rate column boolean column custom filter dataset rate column integ column valu rang like convert column simpl boolean categor column valu neg rate greater equal posit rate sure",
        "Question_gpt_summary_original":"The user is facing a challenge of converting an integer column with values ranging from 1 to 10 into a boolean positive\/negative categorical column based on a custom filter in Azure ML. The user wants to set a filter where values less than 6 are considered negative and values greater or equal to 6 are considered positive.",
        "Question_gpt_summary":"user face challeng convert integ column valu rang boolean posit neg categor column base custom filter user want set filter valu consid neg valu greater equal consid posit",
        "Answer_original_content":"allow option appli sql transform select case rate end ratingb execut python script return datafram rate datafram rate execut script dataset rate dataset rate",
        "Answer_preprocessed_content":"allow option appli sql transform execut python script execut script",
        "Answer_gpt_summary_original":"The discussion provides three possible solutions to the challenge of converting an integer column into a boolean positive\/negative categorical column based on a custom filter in Azure ML. The solutions include applying SQL transformation, executing Python script, and executing R script.",
        "Answer_gpt_summary":"discuss provid possibl solut challeng convert integ column boolean posit neg categor column base custom filter solut includ appli sql transform execut python script execut script"
    },
    {
        "Question_title":"AWS Sagemaker - ClientError: Data download failed",
        "Question_body":"<p><strong>Problem:<\/strong>\nI am trying to setup a model in Sagemaker, however it fails when it comes to downloading the data.\nDoes anyone know what I am doing wrong?<\/p>\n\n<p><strong>What I did so far<\/strong>:\nIn order to avoid any mistakes on my side I decided to use the AWS tutorial:\ntensorflow_iris_dnn_classifier_using_estimators<\/p>\n\n<p>And I made only two changes:<\/p>\n\n<ol>\n<li>I copied the dataset to my own S3 instance. --> I tested if I could access \/ show the data and it worked.<\/li>\n<li>I edited the path to point to the new folder.<\/li>\n<\/ol>\n\n<p>This is the AWS source code:\n<a href=\"https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators\" rel=\"noreferrer\">https:\/\/github.com\/awslabs\/amazon-sagemaker-examples\/tree\/master\/sagemaker-python-sdk\/tensorflow_iris_dnn_classifier_using_estimators<\/a><\/p>\n\n<pre><code>%%time\nimport boto3\n\n# use the region-specific sample data bucket\nregion = boto3.Session().region_name\n#train_data_location = 's3:\/\/sagemaker-sample-data-{}\/tensorflow\/iris'.format(region)\ntrain_data_location = 's3:\/\/my-s3-bucket'\n\niris_estimator.fit(train_data_location)\n<\/code><\/pre>\n\n<p>And this is the error I get:<\/p>\n\n<pre><code>\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/interactiveshell.pyc in run_cell_magic(self, magic_name, line, cell)\n   2115             magic_arg_s = self.var_expand(line, stack_depth)\n   2116             with self.builtin_trap:\n-&gt; 2117                 result = fn(magic_arg_s, cell)\n   2118             return result\n   2119 \n\n&lt;decorator-gen-60&gt; in time(self, line, cell, local_ns)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magic.pyc in &lt;lambda&gt;(f, *a, **k)\n    186     # but it's overkill for just that one bit of state.\n    187     def magic_deco(arg):\n--&gt; 188         call = lambda f, *a, **k: f(*a, **k)\n    189 \n    190         if callable(arg):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/IPython\/core\/magics\/execution.pyc in time(self, line, cell, local_ns)\n   1191         else:\n   1192             st = clock2()\n-&gt; 1193             exec(code, glob, local_ns)\n   1194             end = clock2()\n   1195             out = None\n\n&lt;timed exec&gt; in &lt;module&gt;()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit(self, inputs, wait, logs, job_name, run_tensorboard_locally)\n    314                 tensorboard.join()\n    315         else:\n--&gt; 316             fit_super()\n    317 \n    318     @classmethod\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/tensorflow\/estimator.pyc in fit_super()\n    293 \n    294         def fit_super():\n--&gt; 295             super(TensorFlow, self).fit(inputs, wait, logs, job_name)\n    296 \n    297         if run_tensorboard_locally and wait is False:\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in fit(self, inputs, wait, logs, job_name)\n    232         self.latest_training_job = _TrainingJob.start_new(self, inputs)\n    233         if wait:\n--&gt; 234             self.latest_training_job.wait(logs=logs)\n    235 \n    236     def _compilation_job_name(self):\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/estimator.pyc in wait(self, logs)\n    571     def wait(self, logs=True):\n    572         if logs:\n--&gt; 573             self.sagemaker_session.logs_for_job(self.job_name, wait=True)\n    574         else:\n    575             self.sagemaker_session.wait_for_job(self.job_name)\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in logs_for_job(self, job_name, wait, poll)\n   1126 \n   1127         if wait:\n-&gt; 1128             self._check_job_status(job_name, description, 'TrainingJobStatus')\n   1129             if dot:\n   1130                 print()\n\n\/home\/ec2-user\/anaconda3\/envs\/tensorflow_p27\/lib\/python2.7\/site-packages\/sagemaker\/session.pyc in _check_job_status(self, job, desc, status_key_name)\n    826             reason = desc.get('FailureReason', '(No reason provided)')\n    827             job_type = status_key_name.replace('JobStatus', ' job')\n--&gt; 828             raise ValueError('Error for {} {}: {} Reason: {}'.format(job_type, job, status, reason))\n    829 \n    830     def wait_for_endpoint(self, endpoint, poll=5):\n\nValueError: Error for Training job sagemaker-tensorflow-2019-01-03-16-32-16-435: Failed Reason: ClientError: Data download failed:S3 key: s3:\/\/my-s3-bucket\/\/sagemaker-tensorflow-2019-01-03-14-02-39-959\/source\/sourcedir.tar.gz has an illegal char sub-sequence '\/\/' in it\n<\/code><\/pre>",
        "Question_answer_count":4,
        "Question_comment_count":2,
        "Question_created_time":1546534753933,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":9.0,
        "Question_view_count":5145.0,
        "Answer_body":"<p>The script is expecting 'bucket' to be bucket = Session().default_bucket() or your own. Have you tried setting bucket equal to your personal bucket?<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54026623",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1546594981603,
        "Question_original_content":"clienterror data download fail problem try setup model fail come download data know wrong far order avoid mistak decid us aw tutori tensorflow iri dnn classifi estim chang copi dataset instanc test access data work edit path point new folder aw sourc code http github com awslab amazon exampl tree master python sdk tensorflow iri dnn classifi estim time import boto us region specif sampl data bucket region boto session region train data locat sampl data tensorflow iri format region train data locat bucket iri estim fit train data locat error home user anaconda env tensorflow lib python site packag ipython core interactiveshel pyc run cell magic self magic line cell magic arg self var expand line stack depth self builtin trap result magic arg cell return result time self line cell local home user anaconda env tensorflow lib python site packag ipython core magic pyc overkil bit state def magic deco arg lambda callabl arg home user anaconda env tensorflow lib python site packag ipython core magic execut pyc time self line cell local clock exec code glob local end clock home user anaconda env tensorflow lib python site packag tensorflow estim pyc fit self input wait log job run tensorboard local tensorboard join fit super classmethod home user anaconda env tensorflow lib python site packag tensorflow estim pyc fit super def fit super super tensorflow self fit input wait log job run tensorboard local wait fals home user anaconda env tensorflow lib python site packag estim pyc fit self input wait log job self latest train job trainingjob start new self input wait self latest train job wait log log def compil job self home user anaconda env tensorflow lib python site packag estim pyc wait self log def wait self log true log self session log job self job wait true self session wait job self job home user anaconda env tensorflow lib python site packag session pyc log job self job wait poll wait self check job statu job descript trainingjobstatu dot print home user anaconda env tensorflow lib python site packag session pyc check job statu self job desc statu kei reason desc failurereason reason provid job type statu kei replac jobstatu job rais valueerror error reason format job type job statu reason def wait endpoint self endpoint poll valueerror error train job tensorflow fail reason clienterror data download fail kei bucket tensorflow sourc sourcedir tar illeg char sub sequenc",
        "Question_preprocessed_content":"clienterror data download fail problem try setup model fail come download data know wrong far order avoid mistak decid us aw tutori chang copi dataset instanc test access data work edit path point new folder aw sourc code error",
        "Question_gpt_summary_original":"The user encountered a challenge while setting up a model in AWS Sagemaker. The error occurred when downloading the data, and the user followed an AWS tutorial and made only two changes. The error message indicates that the data download failed due to an illegal character sub-sequence in the S3 key.",
        "Question_gpt_summary":"user encount challeng set model error occur download data user follow aw tutori chang error messag indic data download fail illeg charact sub sequenc kei",
        "Answer_original_content":"script expect bucket bucket session default bucket tri set bucket equal person bucket",
        "Answer_preprocessed_content":"script expect bucket bucket tri set bucket equal person bucket",
        "Answer_gpt_summary_original":"Solution: The user can try setting the bucket equal to their personal bucket to resolve the error while downloading data in AWS Sagemaker.",
        "Answer_gpt_summary":"solut user try set bucket equal person bucket resolv error download data"
    },
    {
        "Question_title":"Letter Recognition Error in Azure ML Studio",
        "Question_body":"<p>I'm having troubles with a Letter Recognition model I'm creating in Azure ML Studio.<\/p>\n<p>I'm running a few algorithms - Decision Jungle, Neural Network, Decision Forest, Logistic Regression, One vs. All Multiclass, and then I append them using the Add rows method (Neural Network and Desicion Jungle\/ Decision Forest and Logistic Regression), until I append them all.<\/p>\n<p>However, appending Decision Forest and Logistic Regression I get the following error:<\/p>\n<pre><code>requestId = 9292bc066f51404eb5e0d0d219d3a072 errorComponent=Module. taskStatusCode=400. {&quot;Exception&quot;:{&quot;ErrorId&quot;:&quot;NotInRangeValue&quot;,&quot;ErrorCode&quot;:&quot;0008&quot;,&quot;ExceptionType&quot;:&quot;ModuleException&quot;,&quot;Message&quot;:&quot;Error 0008: Parameter \\&quot;Dataset2(number of columns)\\&quot; value should be in the range of [3, 3].&quot;}}Error: Error 0008: Parameter &quot;Dataset2(number of columns)&quot; value should be in the range of [3, 3]. Process exited with error code -2\n<\/code><\/pre>\n<p>Any advice what should I do? Huge thanks in advance<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1599640980777,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1599642150896,
        "Question_score":0.0,
        "Question_view_count":37.0,
        "Answer_body":"<p>This error occurs when there is a mismatch of number of columns of the two dataset you are appending.<\/p>\n<p>Looking at the error :<\/p>\n<p>The output of one model is returning rows with 3 columns and other one is having either more or less than 3 columns.<\/p>\n<p>Before this step &quot;Add Rows&quot; step -&gt; Do quick Visualize<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/PsYQT.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/PsYQT.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>This will give a view of the dataset that you are planning to append.<\/p>\n<p><a href=\"https:\/\/i.stack.imgur.com\/x442d.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/x442d.png\" alt=\"![enter image description here\" \/><\/a><\/p>\n<p>Ensure for both, the columns numbers are same.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63807950",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1599653828448,
        "Question_original_content":"letter recognit error studio have troubl letter recognit model creat studio run algorithm decis jungl neural network decis forest logist regress multiclass append add row method neural network desicion jungl decis forest logist regress append append decis forest logist regress follow error requestid bcfebeddda errorcompon modul taskstatuscod except errorid notinrangevalu errorcod exceptiontyp moduleexcept messag error paramet dataset number column valu rang error error paramet dataset number column valu rang process exit error code advic huge thank advanc",
        "Question_preprocessed_content":"letter recognit error studio have troubl letter recognit model creat studio run algorithm decis jungl neural network decis forest logist regress multiclass append add row method append append decis forest logist regress follow error advic huge thank advanc",
        "Question_gpt_summary_original":"The user is encountering an error while creating a Letter Recognition model in Azure ML Studio. They are using several algorithms and appending them using the Add rows method, but when they try to append Decision Forest and Logistic Regression, they receive an error message stating that the parameter \"Dataset2(number of columns)\" value should be in the range of [3, 3]. The user is seeking advice on how to resolve this issue.",
        "Question_gpt_summary":"user encount error creat letter recognit model studio algorithm append add row method try append decis forest logist regress receiv error messag state paramet dataset number column valu rang user seek advic resolv issu",
        "Answer_original_content":"error occur mismatch number column dataset append look error output model return row column have column step add row step quick visual view dataset plan append ensur column number",
        "Answer_preprocessed_content":"error occur mismatch number column dataset append look error output model return row column have column step add row step quick visual view dataset plan append ensur column number",
        "Answer_gpt_summary_original":"Solution: The error occurs due to a mismatch of the number of columns of the two datasets being appended. To resolve this issue, the user can do a quick visualization of the datasets before appending them using the \"Add Rows\" method. This will give a view of the datasets and ensure that the number of columns is the same for both datasets.",
        "Answer_gpt_summary":"solut error occur mismatch number column dataset append resolv issu user quick visual dataset append add row method view dataset ensur number column dataset"
    },
    {
        "Question_title":"AWS Sagemaker Multi-Model Endpoint with Scikit Learn: UnexpectedStatusException whilst using a training script",
        "Question_body":"<p>I am trying to create a multi-model endpoint in AWS sagemaker using Scikit-learn and a custom training script. When I attempt to train my model using the following code:<\/p>\n<pre><code>estimator = SKLearn(\n    entry_point=TRAINING_FILE, # script to use for training job\n    role=role,\n    source_dir=SOURCE_DIR, # Location of scripts\n    train_instance_count=1,\n    train_instance_type=TRAIN_INSTANCE_TYPE,\n    framework_version='0.23-1',\n    output_path=s3_output_path,# Where to store model artifacts\n    base_job_name=_job,\n    code_location=code_location,# This is where the .tar.gz of the source_dir will be stored\n    hyperparameters = {'max-samples'    : 100,\n                       'model_name'     : key})\n\nDISTRIBUTION_MODE = 'FullyReplicated'\n\ntrain_input = sagemaker.s3_input(s3_data=inputs+'\/train', \n                                  distribution=DISTRIBUTION_MODE, content_type='csv')\n    \nestimator.fit({'train': train_input}, wait=True)\n<\/code><\/pre>\n<p>where 'TRAINING_FILE' contains:<\/p>\n<pre><code>\nimport argparse\nimport os\n\nimport numpy as np\nimport pandas as pd\nimport joblib\nimport sys\n\nfrom sklearn.ensemble import IsolationForest\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument('--max_samples', type=int, default=100)\n    \n    parser.add_argument('--model_dir', type=str, default=os.environ.get('SM_MODEL_DIR'))\n    parser.add_argument('--train', type=str, default=os.environ.get('SM_CHANNEL_TRAIN'))\n    parser.add_argument('--model_name', type=str)\n\n    args, _ = parser.parse_known_args()\n\n    print('reading data. . .')\n    print('model_name: '+args.model_name)    \n    \n    train_file = os.path.join(args.train, args.model_name + '_train.csv')    \n    train_df = pd.read_csv(train_file) # read in the training data\n    train_tgt = train_df.iloc[:, 1] # target column is the second column\n    \n    clf = IsolationForest(max_samples = args.max_samples)\n    clf = clf.fit([train_tgt])\n    \n    path = os.path.join(args.model_dir, 'model.joblib')\n    joblib.dump(clf, path)\n    print('model persisted at ' + path)\n<\/code><\/pre>\n<p>The training script succeeds but sagemaker throws an <code>UnexpectedStatusException<\/code>:\n<a href=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" rel=\"nofollow noreferrer\"><img src=\"https:\/\/i.stack.imgur.com\/mXgjS.png\" alt=\"enter image description here\" \/><\/a><\/p>\n<p>Has anybody ever experienced anything like this before? I've checked all the cloudwatch logs and found nothing of use, and I'm completely stumped on what to try next.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":1,
        "Question_created_time":1603208778703,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":263.0,
        "Answer_body":"<p>To anyone that comes across this issue in future, the problem has been solved.<\/p>\n<p>The issue was nothing to do with the training, but with invalid characters in directory names being sent to S3. So the script would produce the artifacts correctly, but sagemaker would throw an exception when trying to save them to S3<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/64448720",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1603707766790,
        "Question_original_content":"multi model endpoint scikit learn unexpectedstatusexcept whilst train script try creat multi model endpoint scikit learn custom train script attempt train model follow code estim sklearn entri point train file script us train job role role sourc dir sourc dir locat script train instanc count train instanc type train instanc type framework version output path output path store model artifact base job job code locat code locat tar sourc dir store hyperparamet max sampl model kei distribut mode fullyrepl train input input data input train distribut distribut mode content type csv estim fit train train input wait true train file contain import argpars import import numpi import panda import joblib import sy sklearn ensembl import isolationforest main parser argpars argumentpars parser add argument max sampl type int default parser add argument model dir type str default environ model dir parser add argument train type str default environ channel train parser add argument model type str arg parser pars known arg print read data print model arg model train file path join arg train arg model train csv train read csv train file read train data train tgt train iloc target column second column clf isolationforest max sampl arg max sampl clf clf fit train tgt path path join arg model dir model joblib joblib dump clf path print model persist path train script succe throw unexpectedstatusexcept anybodi experienc like check cloudwatch log us complet stump try",
        "Question_preprocessed_content":"endpoint scikit learn unexpectedstatusexcept whilst train script try creat endpoint custom train script attempt train model follow code contain train script succe throw anybodi experienc like check cloudwatch log us complet stump try",
        "Question_gpt_summary_original":"The user is encountering an UnexpectedStatusException while attempting to create a multi-model endpoint in AWS Sagemaker using Scikit-learn and a custom training script. The training script succeeds, but Sagemaker throws the exception, and the user is unable to find any useful information in the Cloudwatch logs. The user is seeking advice on how to proceed.",
        "Question_gpt_summary":"user encount unexpectedstatusexcept attempt creat multi model endpoint scikit learn custom train script train script succe throw except user unabl us inform cloudwatch log user seek advic proce",
        "Answer_original_content":"come issu futur problem solv issu train invalid charact directori name sent script produc artifact correctli throw except try save",
        "Answer_preprocessed_content":"come issu futur problem solv issu train invalid charact directori name sent script produc artifact correctli throw except try save",
        "Answer_gpt_summary_original":"Solution: The issue was caused by invalid characters in directory names being sent to S3. The solution is to ensure that the directory names do not contain any invalid characters.",
        "Answer_gpt_summary":"solut issu caus invalid charact directori name sent solut ensur directori name contain invalid charact"
    },
    {
        "Question_title":"AWS Sagemaker KeyError: 'SM_CHANNEL_TRAINING' when tuning hyperparameters",
        "Question_body":"<p>When I try to use hyperparameters tuning on Sagemaker I get this error:<\/p>\n<pre><code>UnexpectedStatusException: Error for HyperParameterTuning job imageclassif-job-10-21-47-43: Failed. Reason: No training job succeeded after 5 attempts. Please take a look at the training job failures to get more details.\n<\/code><\/pre>\n<p>When I look up the logs on CloudWatch all 5 failed training jobs have the same error at the end:<\/p>\n<pre><code>Traceback (most recent call last):\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 184, in _run_module_as_main\n    &quot;__main__&quot;, mod_spec)\n  File &quot;\/usr\/lib\/python3.5\/runpy.py&quot;, line 85, in _run_code\n    exec(code, run_globals)\n  File &quot;\/opt\/ml\/code\/train.py&quot;, line 117, in &lt;module&gt;\n    parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])\n  File &quot;\/usr\/lib\/python3.5\/os.py&quot;, line 725, in __getitem__\n    raise KeyError(key) from None\n<\/code><\/pre>\n<p>and<\/p>\n<pre><code>KeyError: 'SM_CHANNEL_TRAINING'\n<\/code><\/pre>\n<p>The problem is at the Step 4 of the project: <a href=\"https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb\" rel=\"nofollow noreferrer\">https:\/\/github.com\/petrooha\/Deploying-LSTM\/blob\/main\/SageMaker%20Project.ipynb<\/a><\/p>\n<p>Would hihgly appreciate any hints on where to look next<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1615420401190,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":1121.0,
        "Answer_body":"<p>In your <code>train.py<\/code> file, changing the environment variable from<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAINING'])<\/code><\/p>\n<p>to<\/p>\n<p><code>parser.add_argument('--data-dir', type=str, default=os.environ['SM_CHANNEL_TRAIN'])<\/code> should address the issue.<\/p>\n<p>This is the case with Torch's framework_version 1.3.1 but other versions might also be affected. Here is the <a href=\"https:\/\/github.com\/aws\/sagemaker-python-sdk\/issues\/1292\" rel=\"nofollow noreferrer\">link<\/a> for your reference.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66574569",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1625508026043,
        "Question_original_content":"keyerror channel train tune hyperparamet try us hyperparamet tune error unexpectedstatusexcept error hyperparametertun job imageclassif job fail reason train job succeed attempt look train job failur detail look log cloudwatch fail train job error end traceback recent file usr lib python runpi line run modul main main mod spec file usr lib python runpi line run code exec code run global file opt code train line parser add argument data dir type str default environ channel train file usr lib python line getitem rais keyerror kei keyerror channel train problem step project http github com petrooha deploi lstm blob main project ipynb hihgli appreci hint look",
        "Question_preprocessed_content":"keyerror tune hyperparamet try us hyperparamet tune error look log cloudwatch fail train job error end problem step project hihgli appreci hint look",
        "Question_gpt_summary_original":"The user is encountering an error while trying to use hyperparameter tuning on AWS Sagemaker. All 5 failed training jobs have the same error related to a KeyError: 'SM_CHANNEL_TRAINING'. The problem is occurring at Step 4 of the project and the user is seeking help to resolve the issue.",
        "Question_gpt_summary":"user encount error try us hyperparamet tune fail train job error relat keyerror channel train problem occur step project user seek help resolv issu",
        "Answer_original_content":"train file chang environ variabl parser add argument data dir type str default environ channel train parser add argument data dir type str default environ channel train address issu case torch framework version version affect link refer",
        "Answer_preprocessed_content":"file chang environ variabl address issu case torch version affect link refer",
        "Answer_gpt_summary_original":"Solution: The solution to the error related to KeyError: 'SM_CHANNEL_TRAINING' is to change the environment variable in the train.py file from \"SM_CHANNEL_TRAINING\" to \"SM_CHANNEL_TRAIN\". This solution is specific to Torch's framework_version 1.3.1, but other versions may also be affected. The user can refer to the provided link for more information.",
        "Answer_gpt_summary":"solut solut error relat keyerror channel train chang environ variabl train file channel train channel train solut specif torch framework version version affect user refer provid link inform"
    },
    {
        "Question_title":"Azure ML SDK DataReference - File Pattern - MANY files",
        "Question_body":"<p>I\u2019m building out a pipeline that should execute and train fairly frequently.  I\u2019m following this: <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-create-your-first-pipeline<\/a> <\/p>\n\n<p>Anyways, I\u2019ve got a stream analytics job dumping telemetry into .json files on blob storage (soon to be adls gen2).  Anyways, I want to find all .json files and use all of those files to train with.  I could possibly use just new .json files as well (interesting option honestly).<\/p>\n\n<p>Currently I just have the store mounted to a data lake and available; and it just iterates the mount for the data files and loads them up.<\/p>\n\n<ol>\n<li>How can I use data references for this instead?<\/li>\n<li>What does data references do for me that mounting time stamped data does not?\na.  From an audit perspective, I have version control, execution time and time stamped read only data.  Albeit, doing a replay on this would require additional coding, but is do-able.<\/li>\n<\/ol>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1566830274697,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":296.0,
        "Answer_body":"<p>You could pass pointer to folder as an input parameter for the pipeline, and then your step can mount the folder to iterate over the json files.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57660058",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1566854588780,
        "Question_original_content":"sdk datarefer file pattern file build pipelin execut train fairli frequent follow http doc microsoft com azur machin learn servic creat pipelin anywai iv got stream analyt job dump telemetri json file blob storag soon adl gen anywai want json file us file train possibl us new json file interest option honestli current store mount data lake avail iter mount data file load us data refer instead data refer mount time stamp data audit perspect version control execut time time stamp read data albeit replai requir addit code abl",
        "Question_preprocessed_content":"sdk datarefer file pattern file build pipelin execut train fairli frequent follow anywai iv got stream analyt job dump telemetri json file blob storag anywai want json file us file train possibl us new json file current store mount data lake avail iter mount data file load us data refer instead data refer mount time stamp data audit perspect version control execut time time stamp read data albeit replai requir addit code",
        "Question_gpt_summary_original":"The user is building a pipeline to execute and train frequently using Azure ML SDK DataReference. They have a stream analytics job that dumps telemetry into .json files on blob storage and want to find all .json files to train with. They are unsure how to use data references for this and are questioning the benefits of using data references over mounting time-stamped data for version control and audit purposes.",
        "Question_gpt_summary":"user build pipelin execut train frequent sdk datarefer stream analyt job dump telemetri json file blob storag want json file train unsur us data refer question benefit data refer mount time stamp data version control audit purpos",
        "Answer_original_content":"pass pointer folder input paramet pipelin step mount folder iter json file",
        "Answer_preprocessed_content":"pass pointer folder input paramet pipelin step mount folder iter json file",
        "Answer_gpt_summary_original":"Solution: One possible solution mentioned in the discussion is to pass a pointer to the folder containing the .json files as an input parameter for the pipeline. Then, the step can mount the folder to iterate over the .json files.",
        "Answer_gpt_summary":"solut possibl solut mention discuss pass pointer folder contain json file input paramet pipelin step mount folder iter json file"
    },
    {
        "Question_title":"Adding Confusion Matrix to report programatically",
        "Question_body":"<p>We are experimenting with programmatic report generation with WandB.<br>\nI would like to be able to add a Confusion Matrix to a report, but this is not one of the base types (as far as I can tell). Is there a good way to do this?<br>\nI could generate a PNG\/Image and insert it, but I can\u2019t figure out how to add an Image to a report yet (see recent question).  Are there other ways?<br>\nThanks.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1665987764533,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":97.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/kevinashaw\">@kevinashaw<\/a> I am also posting here <a href=\"https:\/\/colab.research.google.com\/drive\/1Fepp-JLFvK-wLL2BZ_BnkCG_fAC6HFbo#scrollTo=An_example_with_all_of_the_blocks_and_panels\" rel=\"noopener nofollow ugc\">this Colab<\/a> and the Python SDK commands of our <a href=\"https:\/\/docs.wandb.ai\/guides\/reports\/edit-a-report#add-plots\">Reports reference docs<\/a> which may be helpful.<\/p>\n<p>The confusion matrix isn\u2019t <a href=\"https:\/\/github.com\/wandb\/wandb\/blob\/main\/wandb\/apis\/reports\/panels.py\" rel=\"noopener nofollow ugc\">currently exposed<\/a> but I have increased this feature requests for our engineering team. We will reach out to you once this is implemented. I hope this helps, please let me know if you have any further questions or issues with the Reports API.<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/adding-confusion-matrix-to-report-programatically\/3267",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1666015569168,
        "Question_original_content":"ad confus matrix report programat experi programmat report gener like abl add confus matrix report base type far tell good wai gener png imag insert figur add imag report recent question wai thank",
        "Question_preprocessed_content":"ad confus matrix report programat experi programmat report gener like abl add confus matrix report base type good wai gener insert figur add imag report wai thank",
        "Question_gpt_summary_original":"The user is facing challenges in adding a Confusion Matrix to a programmatic report using WandB. They are unsure if this is possible as it is not one of the base types. The user has considered generating a PNG\/Image and inserting it, but they are also unsure how to add an image to the report.",
        "Question_gpt_summary":"user face challeng ad confus matrix programmat report unsur possibl base type user consid gener png imag insert unsur add imag report",
        "Answer_original_content":"kevinashaw post colab python sdk command report refer doc help confus matrix isnt current expos increas featur request engin team reach implement hope help let know question issu report api",
        "Answer_preprocessed_content":"post colab python sdk command report refer doc help confus matrix isnt current expos increas featur request engin team reach implement hope help let know question issu report api",
        "Answer_gpt_summary_original":"Solution: No solution is provided in the discussion. The user is informed that the confusion matrix is not currently exposed in WandB and that the feature request has been increased for the engineering team.",
        "Answer_gpt_summary":"solut solut provid discuss user inform confus matrix current expos featur request increas engin team"
    },
    {
        "Question_title":"AzureML Environment for Inference : can't add pip packages to dependencies",
        "Question_body":"<p>I can't find the proper way to add dependencies to my Azure Container Instance for ML Inference.<\/p>\n<p>I basically started by following this tutorial : <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-train-deploy-notebook\" rel=\"nofollow noreferrer\">Train and deploy an image classification model with an example Jupyter Notebook<\/a><\/p>\n<p>It works fine.<\/p>\n<p>Now I want to deploy my trained TensorFlow model for inference. I tried many ways, but I was never able to add python dependencies to the Environment.<\/p>\n<h1>From the TensorFlow curated environment<\/h1>\n<p>Using <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/resource-curated-environments#inference-curated-environments-and-prebuilt-docker-images\" rel=\"nofollow noreferrer\">AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference<\/a> :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code>from azureml.core import Workspace\n\n\n# connect to your workspace\nws = Workspace.from_config()\n\n# names\nexperiment_name = &quot;my-experiment&quot;\nmodel_name = &quot;my-model&quot;\nenv_version=&quot;1&quot;\nenv_name=&quot;my-env-&quot;+env_version\nservice_name = str.lower(model_name + &quot;-service-&quot; + env_version)\n\n\n# create environment for the deploy\nfrom azureml.core.environment import Environment, DEFAULT_CPU_IMAGE\nfrom azureml.core.conda_dependencies import CondaDependencies\nfrom azureml.core.webservice import AciWebservice\n\n# get a curated environment\nenv = Environment.get(\n    workspace=ws, \n    name=&quot;AzureML-tensorflow-2.4-ubuntu18.04-py37-cpu-inference&quot;,\n# )\ncustom_env = env.clone(env_name)\ncustom_env.inferencing_stack_version='latest'\n\n# add packages\nconda_dep = CondaDependencies()\npython_packages = ['joblib', 'numpy', 'os', 'json', 'tensorflow']\nfor package in python_packages:\n    conda_dep.add_pip_package(package)\n    conda_dep.add_conda_package(package)\n\n# Adds dependencies to PythonSection of env\ncustom_env.python.user_managed_dependencies=True\ncustom_env.python.conda_dependencies=conda_dep\n\ncustom_env.register(workspace=ws)\n\n# create deployment config i.e. compute resources\naciconfig = AciWebservice.deploy_configuration(\n    cpu_cores=1,\n    memory_gb=1,\n    tags={&quot;experiment&quot;: experiment_name, &quot;model&quot;: model_name},\n)\n\nfrom azureml.core.model import InferenceConfig\nfrom azureml.core.model import Model\n\n# get the registered model\nmodel = Model(ws, model_name)\n\n# create an inference config i.e. the scoring script and environment\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=custom_env)\n\n# deploy the service\nservice = Model.deploy(\n    workspace=ws,\n    name=service_name,\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aciconfig,\n)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<p>I get the following log :<\/p>\n<pre><code>\nAzureML image information: tensorflow-2.4-ubuntu18.04-py37-cpu-inference:20220110.v1\n\n\nPATH environment variable: \/opt\/miniconda\/envs\/amlenv\/bin:\/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\nPYTHONPATH environment variable: \n\nPip Dependencies\n---------------\nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-24T10:21:09,855130300+00:00 - iot-server\/finish 1 0\n2022-01-24T10:21:09,856870100+00:00 - Exit code 1 is normal. Not restarting iot-server.\nabsl-py==0.15.0\napplicationinsights==0.11.10\nastunparse==1.6.3\nazureml-inference-server-http==0.4.2\ncachetools==4.2.4\ncertifi==2021.10.8\ncharset-normalizer==2.0.10\nclick==8.0.3\nFlask==1.0.3\nflatbuffers==1.12\ngast==0.3.3\ngoogle-auth==2.3.3\ngoogle-auth-oauthlib==0.4.6\ngoogle-pasta==0.2.0\ngrpcio==1.32.0\ngunicorn==20.1.0\nh5py==2.10.0\nidna==3.3\nimportlib-metadata==4.10.0\ninference-schema==1.3.0\nitsdangerous==2.0.1\nJinja2==3.0.3\nKeras-Preprocessing==1.1.2\nMarkdown==3.3.6\nMarkupSafe==2.0.1\nnumpy==1.19.5\noauthlib==3.1.1\nopt-einsum==3.3.0\npandas==1.1.5\nprotobuf==3.19.1\npyasn1==0.4.8\npyasn1-modules==0.2.8\npython-dateutil==2.8.2\npytz==2021.3\nrequests==2.27.1\nrequests-oauthlib==1.3.0\nrsa==4.8\nsix==1.15.0\ntensorboard==2.7.0\ntensorboard-data-server==0.6.1\ntensorboard-plugin-wit==1.8.1\ntensorflow==2.4.0\ntensorflow-estimator==2.4.0\ntermcolor==1.1.0\ntyping-extensions==3.7.4.3\nurllib3==1.26.8\nWerkzeug==2.0.2\nwrapt==1.12.1\nzipp==3.7.0\n\n\nEntry script directory: \/var\/azureml-app\/.\n\nDynamic Python package installation is disabled.\nStarting AzureML Inference Server HTTP.\n\nAzure ML Inferencing HTTP server v0.4.2\n\n\nServer Settings\n---------------\nEntry Script Name: score.py\nModel Directory: \/var\/azureml-app\/azureml-models\/my-model\/1\nWorker Count: 1\nWorker Timeout (seconds): 300\nServer Port: 31311\nApplication Insights Enabled: false\nApplication Insights Key: None\n\n\nServer Routes\n---------------\nLiveness Probe: GET   127.0.0.1:31311\/\nScore:          POST  127.0.0.1:31311\/score\n\nStarting gunicorn 20.1.0\nListening at: http:\/\/0.0.0.0:31311 (69)\nUsing worker: sync\nBooting worker with pid: 100\nException in worker process\nTraceback (most recent call last):\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/arbiter.py&quot;, line 589, in spawn_worker\n    worker.init_process()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 134, in init_process\n    self.load_wsgi()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/workers\/base.py&quot;, line 146, in load_wsgi\n    self.wsgi = self.app.wsgi()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/base.py&quot;, line 67, in wsgi\n    self.callable = self.load()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 58, in load\n    return self.load_wsgiapp()\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/app\/wsgiapp.py&quot;, line 48, in load_wsgiapp\n    return util.import_app(self.app_uri)\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/gunicorn\/util.py&quot;, line 359, in import_app\n    mod = importlib.import_module(module)\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/importlib\/__init__.py&quot;, line 127, in import_module\n    return _bootstrap._gcd_import(name[level:], package, level)\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 1006, in _gcd_import\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 983, in _find_and_load\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 967, in _find_and_load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 677, in _load_unlocked\n  File &quot;&lt;frozen importlib._bootstrap_external&gt;&quot;, line 728, in exec_module\n  File &quot;&lt;frozen importlib._bootstrap&gt;&quot;, line 219, in _call_with_frames_removed\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/entry.py&quot;, line 1, in &lt;module&gt;\n    import create_app\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/create_app.py&quot;, line 4, in &lt;module&gt;\n    from routes_common import main\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/routes_common.py&quot;, line 32, in &lt;module&gt;\n    from aml_blueprint import AMLBlueprint\n  File &quot;\/opt\/miniconda\/envs\/amlenv\/lib\/python3.7\/site-packages\/azureml_inference_server_http\/server\/aml_blueprint.py&quot;, line 28, in &lt;module&gt;\n    main_module_spec.loader.exec_module(main)\n  File &quot;\/var\/azureml-app\/score.py&quot;, line 4, in &lt;module&gt;\n    import joblib\nModuleNotFoundError: No module named 'joblib'\nWorker exiting (pid: 100)\nShutting down: Master\nReason: Worker failed to boot.\n2022-01-24T10:21:13,851467800+00:00 - gunicorn\/finish 3 0\n2022-01-24T10:21:13,853259700+00:00 - Exit code 3 is not normal. Killing image.\n<\/code><\/pre>\n<h1>From a Conda specification<\/h1>\n<p>Same as before, but with a fresh environment from Conda specification and changing the <code>env_version<\/code> number :<\/p>\n<pre class=\"lang-py prettyprint-override\"><code># ...\n\n\nenv_version=&quot;2&quot;\n\n# ...\n\ncustom_env = Environment.from_conda_specification(name=env_name, file_path=&quot;my-env.yml&quot;)\ncustom_env.docker.base_image = DEFAULT_CPU_IMAGE\n\n# ...\n\n<\/code><\/pre>\n<p>with <code>my-env.yml<\/code> :<\/p>\n<pre class=\"lang-yaml prettyprint-override\"><code>name: my-env\ndependencies:\n- python\n\n- pip:\n  - azureml-defaults\n  - azureml-sdk\n  - sklearn\n  - numpy\n  - matplotlib\n  - joblib\n  - uuid\n  - requests\n  - tensorflow\n\n<\/code><\/pre>\n<p>I get this log :<\/p>\n<pre><code>2022-01-24T11:06:54,887886931+00:00 - iot-server\/run \n2022-01-24T11:06:54,891839877+00:00 - rsyslog\/run \n2022-01-24T11:06:54,893640998+00:00 - gunicorn\/run \n2022-01-24T11:06:54,912032812+00:00 - nginx\/run \nEdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n2022-01-24T11:06:55,398420960+00:00 - iot-server\/finish 1 0\n2022-01-24T11:06:55,414425146+00:00 - Exit code 1 is normal. Not restarting iot-server.\n\nPATH environment variable: \/opt\/miniconda\/bin:\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin\nPYTHONPATH environment variable: \n\nPip Dependencies\n---------------\nbrotlipy==0.7.0\ncertifi==2020.6.20\ncffi @ file:\/\/\/tmp\/build\/80754af9\/cffi_1605538037615\/work\nchardet @ file:\/\/\/tmp\/build\/80754af9\/chardet_1605303159953\/work\nconda==4.9.2\nconda-package-handling @ file:\/\/\/tmp\/build\/80754af9\/conda-package-handling_1603018138503\/work\ncryptography @ file:\/\/\/tmp\/build\/80754af9\/cryptography_1605544449973\/work\nidna @ file:\/\/\/tmp\/build\/80754af9\/idna_1593446292537\/work\npycosat==0.6.3\npycparser @ file:\/\/\/tmp\/build\/80754af9\/pycparser_1594388511720\/work\npyOpenSSL @ file:\/\/\/tmp\/build\/80754af9\/pyopenssl_1605545627475\/work\nPySocks @ file:\/\/\/tmp\/build\/80754af9\/pysocks_1594394576006\/work\nrequests @ file:\/\/\/tmp\/build\/80754af9\/requests_1592841827918\/work\nruamel-yaml==0.15.87\nsix @ file:\/\/\/tmp\/build\/80754af9\/six_1605205313296\/work\ntqdm @ file:\/\/\/tmp\/build\/80754af9\/tqdm_1605303662894\/work\nurllib3 @ file:\/\/\/tmp\/build\/80754af9\/urllib3_1603305693037\/work\n\nStarting HTTP server\n2022-01-24T11:06:59,701365128+00:00 - gunicorn\/finish 127 0\n.\/run: line 127: exec: gunicorn: not found\n2022-01-24T11:06:59,706177784+00:00 - Exit code 127 is not normal. Killing image.\n    \n<\/code><\/pre>\n<p>I really don't know what I'm missing, and I've been searching for too long already (Azure docs, SO, ...).<\/p>\n<p>Thanks for your help !<\/p>\n<p>Edit : Non-exhaustive list of solutions I tried :<\/p>\n<ul>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/65159778\/how-to-create-azureml-environement-and-add-required-packages\">How to create AzureML environement and add required packages<\/a><\/li>\n<li><a href=\"https:\/\/stackoverflow.com\/questions\/65159308\/how-to-use-existing-conda-environment-as-a-azureml-environment\">how to use existing conda environment as a AzureML environment<\/a><\/li>\n<li>...<\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#environment-building-caching-and-reuse\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/concept-environments#environment-building-caching-and-reuse<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#add-packages-to-an-environment\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-use-environments#add-packages-to-an-environment<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-inferencing-gpus<\/a><\/li>\n<li><a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-deployment-configuration\" rel=\"nofollow noreferrer\">https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/how-to-deploy-and-where?tabs=python#define-a-deployment-configuration<\/a><\/li>\n<li>...<\/li>\n<\/ul>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1643025448957,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1643163454172,
        "Question_score":0.0,
        "Question_view_count":902.0,
        "Answer_body":"<p>OK, I got it working : I started over from scratch and it worked.<\/p>\n<p>I have no idea what was wrong in all my preceding tries, and that is terrible.<\/p>\n<p>Multiple problems and how I (think I) solved them :<\/p>\n<ul>\n<li><code>joblib<\/code> : I actually didn't need it to load my Keras model. But the problem was not with this specific library, rather that I couldn't add dependencies to the inference environment.<\/li>\n<li><code>Environment<\/code> : finally, I was only able to make things work with a custom env : <code>Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)<\/code> . I haven't been able to add my libraries (or specify a specific package version) to a &quot;currated environment&quot;. I don't know why though...<\/li>\n<li><code>TensorFlow<\/code> : last problem I had was that I trained and registered my model in AzureML Notebook's <code>azureml_py38_PT_TF<\/code> kernel (<code>tensorflow==2.7.0<\/code>), and tried to load it in the inference Docker image (<code>tensorflow==2.4.0<\/code>). So I had to specify the version of TensorFlow I wanted to use in the inference image (which required the previous point to be solved).<\/li>\n<\/ul>\n<p>What finally worked :<\/p>\n<ul>\n<li>notebook.ipynb<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>import uuid\nfrom azureml.core import Workspace, Environment, Model\nfrom azureml.core.webservice import AciWebservice\nfrom azureml.core.model import InferenceConfig\n\n\nversion = &quot;test-&quot;+str(uuid.uuid4())[:8]\n\nenv = Environment.from_conda_specification(name=version, file_path=&quot;conda_dependencies.yml&quot;)\ninference_config = InferenceConfig(entry_script=&quot;score.py&quot;, environment=env)\n\nws = Workspace.from_config()\nmodel = Model(ws, model_name)\n\naci_config = AciWebservice.deploy_configuration(\n    cpu_cores=1,\n    memory_gb=1,\n)\n\nservice = Model.deploy(\n    workspace=ws,\n    name=version,\n    models=[model],\n    inference_config=inference_config,\n    deployment_config=aci_config,\n    overwrite=True,\n)\n\nservice.wait_for_deployment(show_output=True)\n<\/code><\/pre>\n<ul>\n<li>conda_dependencies.yml<\/li>\n<\/ul>\n<pre class=\"lang-yaml prettyprint-override\"><code>channels:\n- conda-forge\ndependencies:\n- python=3.8\n- pip:\n  - azureml-defaults\n  - azureml-sdk\n  - numpy\n  - tensorflow==2.7.0\n\n<\/code><\/pre>\n<ul>\n<li>score.py<\/li>\n<\/ul>\n<pre class=\"lang-py prettyprint-override\"><code>import os\nimport json\nimport numpy as np\nimport tensorflow as tf\n\n\ndef init():\n    global model\n\n    model_path = os.path.join(os.getenv(&quot;AZUREML_MODEL_DIR&quot;), &quot;model\/data\/model&quot;)\n    model = tf.keras.models.load_model(model_path)\n\n\n\ndef run(raw_data):\n    data = np.array(json.loads(raw_data)[&quot;data&quot;])\n    y_hat = model.predict(data)\n\n    return y_hat.tolist()\n\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70833499",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1643188448820,
        "Question_original_content":"environ infer add pip packag depend proper wai add depend azur contain instanc infer basic start follow tutori train deploi imag classif model exampl jupyt notebook work fine want deploi train tensorflow model infer tri wai abl add python depend environ tensorflow curat environ tensorflow ubuntu cpu infer core import workspac connect workspac workspac config name experi experi model model env version env env env version servic str lower model servic env version creat environ deploi core environ import environ default cpu imag core conda depend import condadepend core webservic import aciwebservic curat environ env environ workspac tensorflow ubuntu cpu infer custom env env clone env custom env inferenc stack version latest add packag conda dep condadepend python packag joblib numpi json tensorflow packag python packag conda dep add pip packag packag conda dep add conda packag packag add depend pythonsect env custom env python user manag depend true custom env python conda depend conda dep custom env regist workspac creat deploy config comput resourc aciconfig aciwebservic deploi configur cpu core memori tag experi experi model model core model import inferenceconfig core model import model regist model model model model creat infer config score script environ infer config inferenceconfig entri script score environ custom env deploi servic servic model deploi workspac servic model model infer config infer config deploy config aciconfig servic wait deploy output true follow log imag inform tensorflow ubuntu cpu infer path environ variabl opt miniconda env amlenv bin opt miniconda bin usr local sbin usr local bin usr sbin usr bin sbin bin pythonpath environ variabl pip depend edgehubconnectionstr iotedg iothubhostnam set exit iot server finish exit code normal restart iot server absl applicationinsight astunpars infer server http cachetool certifi charset normal click flask flatbuff gast googl auth googl auth oauthlib googl pasta grpcio gunicorn hpy idna importlib metadata infer schema itsdanger jinja kera preprocess markdown markupsaf numpi oauthlib opt einsum panda protobuf pyasn pyasn modul python dateutil pytz request request oauthlib rsa tensorboard tensorboard data server tensorboard plugin wit tensorflow tensorflow estim termcolor type extens urllib werkzeug wrapt zipp entri script directori var app dynam python packag instal disabl start infer server http inferenc http server server set entri script score model directori var app model model worker count worker timeout second server port applic insight enabl fals applic insight kei server rout live probe score post score start gunicorn listen http worker sync boot worker pid except worker process traceback recent file opt miniconda env amlenv lib python site packag gunicorn arbit line spawn worker worker init process file opt miniconda env amlenv lib python site packag gunicorn worker base line init process self load wsgi file opt miniconda env amlenv lib python site packag gunicorn worker base line load wsgi self wsgi self app wsgi file opt miniconda env amlenv lib python site packag gunicorn app base line wsgi self callabl self load file opt miniconda env amlenv lib python site packag gunicorn app wsgiapp line load return self load wsgiapp file opt miniconda env amlenv lib python site packag gunicorn app wsgiapp line load wsgiapp return util import app self app uri file opt miniconda env amlenv lib python site packag gunicorn util line import app mod importlib import modul modul file opt miniconda env amlenv lib python importlib init line import modul return bootstrap gcd import level packag level file line gcd import file line load file line load unlock file line load unlock file line exec modul file line frame remov file opt miniconda env amlenv lib python site packag infer server http server entri line import creat app file opt miniconda env amlenv lib python site packag infer server http server creat app line rout common import main file opt miniconda env amlenv lib python site packag infer server http server rout common line aml blueprint import amlblueprint file opt miniconda env amlenv lib python site packag infer server http server aml blueprint line main modul spec loader exec modul main file var app score line import joblib modulenotfounderror modul name joblib worker exit pid shut master reason worker fail boot gunicorn finish exit code normal kill imag conda specif fresh environ conda specif chang env version number env version custom env environ conda specif env file path env yml custom env docker base imag default cpu imag env yml env depend python pip default sdk sklearn numpi matplotlib joblib uuid request tensorflow log iot server run rsyslog run gunicorn run nginx run edgehubconnectionstr iotedg iothubhostnam set exit iot server finish exit code normal restart iot server path environ variabl opt miniconda bin usr local sbin usr local bin usr sbin usr bin sbin bin pythonpath environ variabl pip depend brotlipi certifi cffi file tmp build cffi work chardet file tmp build chardet work conda conda packag handl file tmp build conda packag handl work cryptographi file tmp build cryptographi work idna file tmp build idna work pycosat pycpars file tmp build pycpars work pyopenssl file tmp build pyopenssl work pysock file tmp build pysock work request file tmp build request work ruamel yaml file tmp build work tqdm file tmp build tqdm work urllib file tmp build urllib work start http server gunicorn finish run line exec gunicorn exit code normal kill imag know miss search long azur doc thank help edit non exhaust list solut tri creat environ add requir packag us exist conda environ environ http doc microsoft com azur machin learn concept environ environ build cach reus http doc microsoft com azur machin learn us environ add packag environ http doc microsoft com azur machin learn deploi inferenc gpu http doc microsoft com azur machin learn deploi tab python defin deploy configur",
        "Question_preprocessed_content":"environ infer add pip packag depend proper wai add depend azur contain instanc infer basic start follow tutori train deploi imag classif model exampl jupyt notebook work fine want deploi train tensorflow model infer tri wai abl add python depend environ tensorflow curat environ follow log conda specif fresh environ conda specif chang number log know miss search long thank help edit list solut tri creat environ add requir packag us exist conda environ environ",
        "Question_gpt_summary_original":"The user is facing challenges in adding python dependencies to their Azure Container Instance for ML Inference. They have tried multiple ways, including following a tutorial and using a Conda specification, but have not been successful. They have also tried various solutions from Azure documentation and Stack Overflow.",
        "Question_gpt_summary":"user face challeng ad python depend azur contain instanc infer tri multipl wai includ follow tutori conda specif success tri solut azur document stack overflow",
        "Answer_original_content":"got work start scratch work idea wrong preced tri terribl multipl problem think solv joblib actual need load kera model problem specif librari couldn add depend infer environ environ final abl thing work custom env environ conda specif version file path conda depend yml haven abl add librari specifi specif packag version currat environ know tensorflow problem train regist model notebook kernel tensorflow tri load infer docker imag tensorflow specifi version tensorflow want us infer imag requir previou point solv final work notebook ipynb import uuid core import workspac environ model core webservic import aciwebservic core model import inferenceconfig version test str uuid uuid env environ conda specif version file path conda depend yml infer config inferenceconfig entri script score environ env workspac config model model model aci config aciwebservic deploi configur cpu core memori servic model deploi workspac version model model infer config infer config deploy config aci config overwrit true servic wait deploy output true conda depend yml channel conda forg depend python pip default sdk numpi tensorflow score import import json import numpi import tensorflow def init global model model path path join getenv model dir model data model model kera model load model model path def run raw data data arrai json load raw data data hat model predict data return hat tolist",
        "Answer_preprocessed_content":"got work start scratch work idea wrong preced tri terribl multipl problem solv actual need load kera model problem specif librari couldn add depend infer environ final abl thing work custom env haven abl add librari currat environ know problem train regist model notebook kernel tri load infer docker imag specifi version tensorflow want us infer imag final work",
        "Answer_gpt_summary_original":"Solution:\n- The user was able to solve the problem by creating a custom environment using the Conda specification file and specifying the version of TensorFlow they wanted to use in the inference image. They were not able to add their libraries or specify a specific package version to a \"curated environment\".\n- They shared the code that finally worked for them, which includes importing the necessary libraries, creating a custom environment, defining the inference configuration, deploying the model, and specifying the scoring script.",
        "Answer_gpt_summary":"solut user abl solv problem creat custom environ conda specif file specifi version tensorflow want us infer imag abl add librari specifi specif packag version curat environ share code final work includ import necessari librari creat custom environ defin infer configur deploi model specifi score script"
    },
    {
        "Question_title":"Access denied for aws public sagemaker xgboost registry",
        "Question_body":"<p>I am trying to pull a prebuilt xgboost image from the public aws xgboost registry specified here: <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title\" rel=\"nofollow noreferrer\">https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/ecr-us-west-2.html#xgboost-us-west-2.title<\/a>, however whenever I run the sagemaker pipeline I get the error:<\/p>\n<pre><code>ClientError: Failed to invoke sagemaker:CreateModelPackage. \nError Details: Access denied for registry ID: 246618743249, repository name: sagemaker-xgboost. \nPlease check if your ECR image exists and has proper pull permissions for SageMaker.\n<\/code><\/pre>\n<p>Here is the attached role boundary I am using to run the pipeline:<\/p>\n<pre><code>{\n    &quot;Version&quot;: &quot;2012-10-17&quot;,\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: [\n                &quot;codebuild:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;codepipeline:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;events:*&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;logs:CreateLogGroup&quot;,\n                &quot;logs:CreateLogStream&quot;,\n                &quot;logs:DescribeLogStreams&quot;,\n                &quot;logs:DescribeLogGroups&quot;,\n                &quot;logs:PutLogEvents&quot;\n            ],\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;iam:PassRole&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:iam::xxxxxxxxxxxx:role\/ml-*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:*&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;arn:aws:ecr:us-west-2:246618743249:repository\/246618743249.dkr.ecr.us-west-2.amazonaws.com\/sagemaker-xgboost:1.0-1&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        },\n        {\n            &quot;Action&quot;: [\n                &quot;ecr:GetAuthorizationToken&quot;\n            ],\n            &quot;Resource&quot;: [\n                &quot;*&quot;\n            ],\n            &quot;Effect&quot;: &quot;Allow&quot;\n        }\n    ]\n}\n<\/code><\/pre>\n<p>and below is the attached policies for the role:<\/p>\n<pre><code>{\n    &quot;Statement&quot;: [\n        {\n            &quot;Action&quot;: &quot;ecr:*&quot;,\n            &quot;Effect&quot;: &quot;Allow&quot;,\n            &quot;Resource&quot;: &quot;*&quot;,\n            &quot;Sid&quot;: &quot;&quot;\n        }\n    ],\n    &quot;Version&quot;: &quot;2012-10-17&quot;\n}\n<\/code><\/pre>\n<p>plus the AWSCodePipelineFullAccess, AWSCodeBuildAdminAccess, and AmazonSageMakerFullAccess managed policies.<\/p>\n<p>Why can't I access the image\/why am I getting this error? As you can see I gave my role full permissions for the ecr registry in the boundary, and full permissions for ecr in the attached policy.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1658354562373,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1658354892732,
        "Question_score":1.0,
        "Question_view_count":61.0,
        "Answer_body":"<p>I had to change the boundary to be this: <code> arn:aws:ecr:us-west-2:246618743249:repository\/sagemaker-xgboost<\/code><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73058582",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1658508649572,
        "Question_original_content":"access deni aw public xgboost registri try pull prebuilt xgboost imag public aw xgboost registri specifi http doc aw amazon com latest ecr west html xgboost west titl run pipelin error clienterror fail invok createmodelpackag error detail access deni registri repositori xgboost check ecr imag exist proper pull permiss attach role boundari run pipelin version statement action codebuild resourc effect allow action codepipelin resourc effect allow action event resourc effect allow action log createloggroup log createlogstream log describelogstream log describeloggroup log putlogev resourc effect allow action iam passrol resourc arn aw iam role effect allow action ecr resourc arn aw ecr west repositori dkr ecr west amazonaw com xgboost effect allow action ecr getauthorizationtoken resourc effect allow attach polici role statement action ecr effect allow resourc sid version plu awscodepipelinefullaccess awscodebuildadminaccess amazonfullaccess manag polici access imag get error gave role permiss ecr registri boundari permiss ecr attach polici",
        "Question_preprocessed_content":"access deni aw public xgboost registri try pull prebuilt xgboost imag public aw xgboost registri specifi run pipelin error attach role boundari run pipelin attach polici role plu awscodepipelinefullaccess awscodebuildadminaccess amazonfullaccess manag polici access get error gave role permiss ecr registri boundari permiss ecr attach polici",
        "Question_gpt_summary_original":"The user is encountering an \"Access denied\" error when trying to pull a prebuilt xgboost image from the public AWS xgboost registry. The error message suggests that the user does not have proper pull permissions for SageMaker. The user has provided the role boundary and policies, which include full permissions for the ECR registry, but the issue persists.",
        "Question_gpt_summary":"user encount access deni error try pull prebuilt xgboost imag public aw xgboost registri error messag suggest user proper pull permiss user provid role boundari polici includ permiss ecr registri issu persist",
        "Answer_original_content":"chang boundari arn aw ecr west repositori xgboost",
        "Answer_preprocessed_content":"chang boundari",
        "Answer_gpt_summary_original":"Solution: The solution mentioned in the discussion is to change the role boundary to include the specific ARN for the SageMaker xgboost repository.",
        "Answer_gpt_summary":"solut solut mention discuss chang role boundari includ specif arn xgboost repositori"
    },
    {
        "Question_title":"If I'm logging metrics for MLOps from a test pipeline, how do I create a separate api key for that?",
        "Question_body":"<p>How do I create a separate API so that I can log metrics from test pipelines? It doesn\u2019t make sense to use a personal key for that.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1675375672133,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":47.0,
        "Answer_body":"<p>Hi <a class=\"mention\" href=\"\/u\/adgudime\">@adgudime<\/a>, thanks for your question! You can use a <a href=\"https:\/\/docs.wandb.ai\/guides\/technical-faq\/general#what-is-a-service-account-and-why-is-it-useful\">service account<\/a> for this purpose,  could you please check if this would work for you? Thanks!<\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/if-im-logging-metrics-for-mlops-from-a-test-pipeline-how-do-i-create-a-separate-api-key-for-that\/3803",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1675687046564,
        "Question_original_content":"log metric mlop test pipelin creat separ api kei creat separ api log metric test pipelin doesnt sens us person kei",
        "Question_preprocessed_content":"log metric mlop test pipelin creat separ api kei creat separ api log metric test pipelin doesnt sens us person kei",
        "Question_gpt_summary_original":"The user is facing a challenge in creating a separate API key for logging metrics from test pipelines, as using a personal key for this purpose is not feasible.",
        "Question_gpt_summary":"user face challeng creat separ api kei log metric test pipelin person kei purpos feasibl",
        "Answer_original_content":"adgudim thank question us servic account purpos check work thank",
        "Answer_preprocessed_content":"thank question us servic account purpos check work thank",
        "Answer_gpt_summary_original":"Solution: The solution suggested in the discussion is to use a service account for logging metrics from test pipelines.",
        "Answer_gpt_summary":"solut solut suggest discuss us servic account log metric test pipelin"
    },
    {
        "Question_title":"Azure ML - Train a model on segments of the data-set",
        "Question_body":"<p>I could really use some help!<\/p>\n\n<p>The company I work for is made up of 52 very different businesses so I can't predict at the company level but instead need to predict business by business then roll up the result to give company wide prediction.<\/p>\n\n<p>I have written an ML model in studio.azureml.net\nIt works great with a 0.947 Coefficient of Determination, but this is for 1 of the businesses.\nI now need to train the model for the other 51.<\/p>\n\n<p>Is there a way to do this in a single ML model rather than having to create 52 very similar models?<\/p>\n\n<p>Any help would be much appreciated !!!<\/p>\n\n<p>Kind Regards\nMartin<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1559146852583,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1560930522380,
        "Question_score":1.0,
        "Question_view_count":123.0,
        "Answer_body":"<p>You can use Ensembles, combining several models to improve predictions. The most direct is stacking when the outputs of all the models are trained on the entire dataset. \nThe method that, I think, corresponds the best to your problem is bagging (bootstrap aggregation). You need to divide the training set into different subsets (each corresponding to a certain business), then train a different model on each subset and combine the result of each classifier. \nAnother way is boosting but it is difficult to implement in Azure ML. \nYou can see an example in <a href=\"https:\/\/gallery.azure.ai\/Experiment\/b6b09fc0c26047e6b4c733ab78a86498\" rel=\"nofollow noreferrer\">Azure ML Gallery<\/a>. <\/p>\n\n<p>Quote from book:<\/p>\n\n<blockquote>\n  <p>Stacking and bagging can be easily implemented in Azure Machine\n  Learning, but other ensemble methods are more difficult. Also, it\n  turns out to be very tedious to implement in Azure Machine Learning an\n  ensemble of, say, more than five models. The experiment is filled with\n  modules and is quite difficult to maintain. Sometimes it is worthwhile\n  to use any ensemble method available in R or Python. Adding more\n  models to an ensemble written in a script can be as trivial as\n  changing a number in the code, instead of copying and pasting modules\n  into the experiment.<\/p>\n<\/blockquote>\n\n<p>You may also have a look at <a href=\"http:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\" rel=\"nofollow noreferrer\">sklearn (Python)<\/a> and caret (R) documentation for further details.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/56364828",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1559897214220,
        "Question_original_content":"train model segment data set us help compani work differ busi predict compani level instead need predict busi busi roll result compani wide predict written model studio net work great coeffici determin busi need train model wai singl model have creat similar model help appreci kind regard martin",
        "Question_preprocessed_content":"train model segment us help compani work differ busi predict compani level instead need predict busi busi roll result compani wide predict written model work great coeffici determin busi need train model wai singl model have creat similar model help appreci kind regard martin",
        "Question_gpt_summary_original":"The user needs to train an ML model for 51 different businesses in their company, but currently only has a model that works for one business. They are seeking help to determine if there is a way to train a single model for all 51 businesses instead of creating 52 separate models.",
        "Question_gpt_summary":"user need train model differ busi compani current model work busi seek help determin wai train singl model busi instead creat separ model",
        "Answer_original_content":"us ensembl combin model improv predict direct stack output model train entir dataset method think correspond best problem bag bootstrap aggreg need divid train set differ subset correspond certain busi train differ model subset combin result classifi wai boost difficult implement exampl galleri quot book stack bag easili implement azur machin learn ensembl method difficult turn tediou implement ensembl model experi fill modul difficult maintain worthwhil us ensembl method avail python ad model ensembl written script trivial chang number code instead copi past modul experi look sklearn python caret document detail",
        "Answer_preprocessed_content":"us ensembl combin model improv predict direct stack output model train entir dataset method think correspond best problem bag need divid train set differ subset train differ model subset combin result classifi wai boost difficult implement exampl galleri quot book stack bag easili implement azur machin learn ensembl method difficult turn tediou implement ensembl model experi fill modul difficult maintain worthwhil us ensembl method avail python ad model ensembl written script trivial chang number code instead copi past modul experi look sklearn caret document detail",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n- Using ensembles, such as stacking or bagging, to combine several models and improve predictions.\n- For the specific problem of training a single model for all 51 businesses, the bagging method can be used by dividing the training set into different subsets, training a different model on each subset, and combining the result of each classifier.\n- Boosting is another ensemble method, but it may be difficult to implement in Azure ML.\n- It is also suggested to check the documentation of sklearn (Python) and caret (R) for further details on ensemble methods.",
        "Answer_gpt_summary":"possibl solut mention discuss ensembl stack bag combin model improv predict specif problem train singl model busi bag method divid train set differ subset train differ model subset combin result classifi boost ensembl method difficult implement suggest check document sklearn python caret detail ensembl method"
    },
    {
        "Question_title":"How do you see ALL predictors by influence not just the top predictors of AutoML training reports?",
        "Question_body":"The \"top predictors by influence\" in the training reports of AutoML regression models is very useful (see reference image), but I'm looking for a way to display all of the predictors, not just the top 10. Any way I can visualise this either in the training report or using the data tables themselves would be very useful.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1635159990897,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, PowerBi is not currently supported here on Q&A. Please post your question on the PowerBI community forum for faster response. Thanks.\n\n\n\n\n--- Kindly Accept Answer if the information helps. Thanks.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/602784\/how-do-you-see-all-predictors-by-influence-not-jus.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1635215186563,
        "Question_original_content":"predictor influenc predictor automl train report predictor influenc train report automl regress model us refer imag look wai displai predictor wai visualis train report data tabl us",
        "Question_preprocessed_content":"predictor influenc predictor automl train report predictor influenc train report automl regress model us look wai displai predictor wai visualis train report data tabl us",
        "Question_gpt_summary_original":"The user is facing a challenge with the AutoML regression model's training reports, which only display the top 10 predictors by influence. The user is looking for a way to visualize all predictors, and is seeking a solution either within the training report or using the data tables themselves.",
        "Question_gpt_summary":"user face challeng automl regress model train report displai predictor influenc user look wai visual predictor seek solut train report data tabl",
        "Answer_original_content":"powerbi current support post question powerbi commun forum faster respons thank kindli accept answer inform help thank",
        "Answer_preprocessed_content":"powerbi current support post question powerbi commun forum faster respons thank kindli accept answer inform help thank",
        "Answer_gpt_summary_original":"No solutions were provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"ClearML multiple tasks in single script changes logged value names",
        "Question_body":"<p>I trained multiple models with different configuration for a custom hyperparameter search. I use pytorch_lightning and its logging (TensorboardLogger).\nWhen running my training script after Task.init() ClearML auto-creates a Task and connects the logger output to the server.<\/p>\n<p>I log for each straining stage <code>train<\/code>, <code>val<\/code> and <code>test<\/code> the following scalars at each epoch: <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code><\/p>\n<p>When I have multiple configuration, e.g. <code>networkA<\/code> and <code>networkB<\/code> the first training log its values to <code>loss<\/code>, <code>acc<\/code> and <code>iou<\/code>, but the second to <code>networkB:loss<\/code>, <code>networkB:acc<\/code> and <code>networkB:iou<\/code>. This makes values umcomparable.<\/p>\n<p>My training loop with Task initalization looks like this:<\/p>\n<pre><code>names = ['networkA', networkB']\nfor name in names:\n     task = Task.init(project_name=&quot;NetworkProject&quot;, task_name=name)\n     pl_train(name)\n     task.close()\n<\/code><\/pre>\n<p>method pl_train is a wrapper for whole training with Pytorch Ligtning. No ClearML code is inside this method.<\/p>\n<p>Do you have any hint, how to properly use the usage of a loop in a script using completly separated tasks?<\/p>\n<hr \/>\n<p>Edit: ClearML version was 0.17.4. Issue is fixed in main branch.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1613745436903,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1614004159640,
        "Question_score":1.0,
        "Question_view_count":279.0,
        "Answer_body":"<p>Disclaimer I'm part of the ClearML (formerly Trains) team.<\/p>\n<p><code>pytorch_lightning<\/code> is creating a new Tensorboard for each experiment. When ClearML logs the TB scalars, and it captures the same scalar being re-sent again, it adds a prefix so if you are reporting the same metric it will not overwrite the previous one. A good example would be reporting <code>loss<\/code> scalar in the training phase vs validation phase (producing &quot;loss&quot; and &quot;validation:loss&quot;). It might be the <code>task.close()<\/code> call does not clear the previous logs, so it &quot;thinks&quot; this is the same experiment, hence adding the prefix <code>networkB<\/code> to the <code>loss<\/code>. As long as you are closing the Task after training is completed you should have all experiments log with the same metric\/variant (title\/series). I suggest opening a GitHub issue, this should probably be considered a bug.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/66279581",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1613773903383,
        "Question_original_content":"multipl task singl script chang log valu name train multipl model differ configur custom hyperparamet search us pytorch lightn log tensorboardlogg run train script task init auto creat task connect logger output server log strain stage train val test follow scalar epoch loss acc iou multipl configur networka networkb train log valu loss acc iou second networkb loss networkb acc networkb iou make valu umcompar train loop task init look like name networka networkb name task task init project networkproject task train task close method train wrapper train pytorch ligtn code insid method hint properli us usag loop script completli separ task edit version issu fix main branch",
        "Question_preprocessed_content":"multipl task singl script chang log valu name train multipl model differ configur custom hyperparamet search us log run train script task connect logger output server log strain stage follow scalar epoch multipl configur train log valu second make valu umcompar train loop task init look like method wrapper train pytorch ligtn code insid method hint properli us usag loop script completli separ task edit version issu fix main branch",
        "Question_gpt_summary_original":"The user encountered a challenge when training multiple models with different configurations for a custom hyperparameter search using pytorch_lightning and ClearML. When logging for each training stage, the first model logs its values to \"loss\", \"acc\", and \"iou\", but the second model logs its values to \"networkB:loss\", \"networkB:acc\", and \"networkB:iou\", making the values incomparable. The user is seeking advice on how to properly use a loop in a script using completely separated tasks.",
        "Question_gpt_summary":"user encount challeng train multipl model differ configur custom hyperparamet search pytorch lightn log train stage model log valu loss acc iou second model log valu networkb loss networkb acc networkb iou make valu incompar user seek advic properli us loop script complet separ task",
        "Answer_original_content":"disclaim train team pytorch lightn creat new tensorboard experi log scalar captur scalar sent add prefix report metric overwrit previou good exampl report loss scalar train phase valid phase produc loss valid loss task close clear previou log think experi ad prefix networkb loss long close task train complet experi log metric variant titl seri suggest open github issu probabl consid bug",
        "Answer_preprocessed_content":"disclaim team creat new tensorboard experi log scalar captur scalar add prefix report metric overwrit previou good exampl report scalar train phase valid phase clear previou log think experi ad prefix long close task train complet experi log suggest open github issu probabl consid bug",
        "Answer_gpt_summary_original":"Solution: The issue might be caused by the `task.close()` call not clearing the previous logs, which makes ClearML add a prefix to the metric name. To solve this, the user should ensure that the `task.close()` call is clearing the previous logs. If the issue persists, the user can open a GitHub issue as it might be considered a bug.",
        "Answer_gpt_summary":"solut issu caus task close clear previou log make add prefix metric solv user ensur task close clear previou log issu persist user open github issu consid bug"
    },
    {
        "Question_title":"Sagemaker Data Capture does not write files",
        "Question_body":"<p>I want to enable data capture for a specific endpoint (so far, only via the console). The endpoint works fine and also logs &amp; returns the desired results. However, no files are written to the specified S3 location.<\/p>\n<h3>Endpoint Configuration<\/h3>\n<p>The endpoint is based on a training job with a scikit learn classifier. It has only one variant which is a <code>ml.m4.xlarge<\/code> instance type. Data Capture is enabled with a sampling percentage of 100%. As data capture storage locations I tried <code>s3:\/\/&lt;bucket-name&gt;<\/code> as well as <code>s3:\/\/&lt;bucket-name&gt;\/&lt;some-other-path&gt;<\/code>. With the &quot;Capture content type&quot; I tried leaving everything blank, setting <code>text\/csv<\/code> in &quot;CSV\/Text&quot; and <code>application\/json<\/code> in &quot;JSON&quot;.<\/p>\n<h3>Endpoint Invokation<\/h3>\n<p>The endpoint is invoked in a Lambda function with a client. Here's the call:<\/p>\n<pre><code>sagemaker_body_source = {\n            &quot;segments&quot;: segments,\n            &quot;language&quot;: language\n        }\npayload = json.dumps(sagemaker_body_source).encode()\nresponse = self.client.invoke_endpoint(EndpointName=endpoint_name,\n                                       Body=payload,\n                                       ContentType='application\/json',\n                                       Accept='application\/json')\nresult = json.loads(response['Body'].read().decode())\nreturn result[&quot;predictions&quot;]\n<\/code><\/pre>\n<p>Internally, the endpoint uses a Flask API with an <code>\/invocation<\/code> path that returns the result.<\/p>\n<h3>Logs<\/h3>\n<p>The endpoint itself works fine and the Flask API is logging input and output:<\/p>\n<pre><code>INFO:api:body: {'segments': [&lt;strings...&gt;], 'language': 'de'}\n<\/code><\/pre>\n<pre><code>INFO:api:output: {'predictions': [{'text': 'some text', 'label': 'some_label'}, ....]}\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1660052882547,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":65.0,
        "Answer_body":"<p>So the issue seemed to be related to the IAM role. The default role (<code>ModelEndpoint-Role<\/code>) does not have access to write S3 files. It worked via the SDK since it uses another role in the sagemaker studio. I did not receive any error message about this.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/73292975",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1660654017400,
        "Question_original_content":"data captur write file want enabl data captur specif endpoint far consol endpoint work fine log return desir result file written specifi locat endpoint configur endpoint base train job scikit learn classifi variant xlarg instanc type data captur enabl sampl percentag data captur storag locat tri captur content type tri leav blank set text csv csv text applic json json endpoint invok endpoint invok lambda function client bodi sourc segment segment languag languag payload json dump bodi sourc encod respons self client invok endpoint endpointnam endpoint bodi payload contenttyp applic json accept applic json result json load respons bodi read decod return result predict intern endpoint us flask api invoc path return result log endpoint work fine flask api log input output info api bodi segment languag info api output predict text text label label",
        "Question_preprocessed_content":"data captur write file want enabl data captur specif endpoint endpoint work fine log return desir result file written specifi locat endpoint configur endpoint base train job scikit learn classifi variant instanc type data captur enabl sampl percentag data captur storag locat tri captur content type tri leav blank set json endpoint invok endpoint invok lambda function client intern endpoint us flask api path return result log endpoint work fine flask api log input output",
        "Question_gpt_summary_original":"The user is facing a challenge with Sagemaker Data Capture as no files are being written to the specified S3 location despite enabling data capture for a specific endpoint. The endpoint is based on a training job with a scikit learn classifier and has only one variant which is a ml.m4.xlarge instance type. The endpoint is invoked in a Lambda function with a client and internally uses a Flask API with an \/invocation path that returns the result. The endpoint works fine and logs input and output, but the user is unable to capture data.",
        "Question_gpt_summary":"user face challeng data captur file written specifi locat despit enabl data captur specif endpoint endpoint base train job scikit learn classifi variant xlarg instanc type endpoint invok lambda function client intern us flask api invoc path return result endpoint work fine log input output user unabl captur data",
        "Answer_original_content":"issu relat iam role default role modelendpoint role access write file work sdk us role studio receiv error messag",
        "Answer_preprocessed_content":"issu relat iam role default role access write file work sdk us role studio receiv error messag",
        "Answer_gpt_summary_original":"Solution: The issue was related to the IAM role. The default role (<code>ModelEndpoint-Role<\/code>) does not have access to write S3 files. The solution was to use another role in the Sagemaker studio via the SDK.",
        "Answer_gpt_summary":"solut issu relat iam role default role modelendpoint role access write file solut us role studio sdk"
    },
    {
        "Question_title":"How to run functions from a Class in the nodes.py file?",
        "Question_body":"<p>I want to organize the node functions by Classes in the nodes.py file. For example, functions related to cleaning data are in the \"CleanData\" Class, with a @staticmethod decorator, while other functions will stay in the \"Other\" Class, without any decorator (the names of these classes are merely representative). In the pipeline file, I tried importing the names of the classes, the names of the nodes and the following way: CleanData.function1 (which gave an error) and none of them worked. How can I call the nodes from the classes, if possible, please?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1573208985150,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":289.0,
        "Answer_body":"<p>I'm not entirely certain what the error you're getting is. If you're literally trying to do <code>from .nodes import CleanData.function1<\/code> that won't work. Imports don't work like that in Python. If you do something like this:<\/p>\n\n<p><code>nodes.py<\/code> has:<\/p>\n\n<pre><code>class CleanData:\n    def clean(arg1):\n        pass\n<\/code><\/pre>\n\n<p>and <code>pipeline.py<\/code> has:<\/p>\n\n<pre><code>from kedro.pipeline import Pipeline, node\nfrom .nodes import CleanData\n\ndef create_pipeline(**kwargs):\n    return Pipeline(\n        [\n            node(\n                CleanData.clean,\n                \"example_iris_data\",\n                None,\n            )\n        ]\n    )\n<\/code><\/pre>\n\n<p>that should work.<\/p>",
        "Answer_comment_count":2.0,
        "Answer_last_edit_time":null,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58764792",
        "Tool":"Kedro",
        "Platform":"Stack Overflow",
        "Question_closed_time":1573209953536,
        "Question_original_content":"run function class node file want organ node function class node file exampl function relat clean data cleandata class staticmethod decor function stai class decor name class mere repres pipelin file tri import name class name node follow wai cleandata function gave error work node class possibl",
        "Question_preprocessed_content":"run function class file want organ node function class file exampl function relat clean data cleandata class decor function stai class decor pipelin file tri import name class name node follow wai work node class possibl",
        "Question_gpt_summary_original":"The user is facing challenges in organizing node functions by classes in the nodes.py file and calling them in the pipeline file. They have tried importing the names of the classes and nodes but have not been successful in calling the nodes from the classes.",
        "Question_gpt_summary":"user face challeng organ node function class node file call pipelin file tri import name class node success call node class",
        "Answer_original_content":"entir certain error get liter try node import cleandata function won work import work like python like node class cleandata def clean arg pass pipelin pipelin import pipelin node node import cleandata def creat pipelin kwarg return pipelin node cleandata clean exampl iri data work",
        "Answer_preprocessed_content":"entir certain error get liter try won work import work like python like work",
        "Answer_gpt_summary_original":"Solution: The solution provided in the discussion is to define the classes and functions in the nodes.py file and import them in the pipeline.py file. The functions can be called using the class name and function name separated by a dot.",
        "Answer_gpt_summary":"solut solut provid discuss defin class function node file import pipelin file function call class function separ dot"
    },
    {
        "Question_title":"how to train and deploy YOLOv5 on aws sagemaker",
        "Question_body":"<p>I want to train YOLOv5 on aws sagemaker also deploy the model on sagemaker itself,need to know about entrypoint python script as well. how can I build a pipeline for this?<\/p>",
        "Question_answer_count":2,
        "Question_comment_count":1,
        "Question_created_time":1632686467053,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1632754422356,
        "Question_score":0.0,
        "Question_view_count":2510.0,
        "Answer_body":"<p>This official AWS <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/speed-up-yolov4-inference-to-twice-as-fast-on-amazon-sagemaker\/\" rel=\"nofollow noreferrer\">blog post<\/a> has information on how to deploy YOLOv4. I wonder if you can use it as a guide and change the model to v5.<\/p>\n<p>If not, there is a 3rd party implementation of YOLOv5 <a href=\"https:\/\/github.com\/HKT-SSA\/yolov5-on-sagemaker\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/69338516",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1632853607047,
        "Question_original_content":"train deploi yolov want train yolov deploi model need know entrypoint python script build pipelin",
        "Question_preprocessed_content":"train deploi yolov want train yolov deploi model need know entrypoint python script build pipelin",
        "Question_gpt_summary_original":"The user is facing challenges in training and deploying YOLOv5 on AWS Sagemaker. They are specifically seeking information on the entrypoint python script and how to build a pipeline for this process.",
        "Question_gpt_summary":"user face challeng train deploi yolov specif seek inform entrypoint python script build pipelin process",
        "Answer_original_content":"offici aw blog post inform deploi yolov wonder us guid chang model parti implement yolov",
        "Answer_preprocessed_content":"offici aw blog post inform deploi yolov wonder us guid chang model parti implement yolov",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Using the official AWS blog post on deploying YOLOv4 as a guide and modifying it for YOLOv5.\n2. Utilizing a third-party implementation of YOLOv5 available on GitHub.",
        "Answer_gpt_summary":"possibl solut mention discuss offici aw blog post deploi yolov guid modifi yolov util parti implement yolov avail github"
    },
    {
        "Question_title":"Copy experiment within workspace",
        "Question_body":"How to duplicate experiments within workspace during debugging",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1653905030690,
        "Question_favorite_count":11.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hello @Alexandre-4252\n\nAre you mentioning studio classic? You can click the save as buttion and then you can save the experience and duplicate it, if you are mentioning studio, sorry, you can not do that.\n\n\n\n\nRegards,\nYutong",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/869578\/copy-experiment-within-workspace.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1653987284412,
        "Question_original_content":"copi experi workspac duplic experi workspac debug",
        "Question_preprocessed_content":"copi experi workspac duplic experi workspac debug",
        "Question_gpt_summary_original":"The user is facing a challenge in duplicating experiments within a workspace while debugging. They are seeking guidance on how to perform this task.",
        "Question_gpt_summary":"user face challeng duplic experi workspac debug seek guidanc perform task",
        "Answer_original_content":"hello alexandr mention studio classic click save buttion save experi duplic mention studio sorri regard yutong",
        "Answer_preprocessed_content":"hello mention studio classic click save buttion save experi duplic mention studio sorri regard yutong",
        "Answer_gpt_summary_original":"Solution: One solution mentioned in the discussion is to use the \"save as\" button in Studio Classic to save and duplicate the experiment. However, it was also mentioned that this solution may not work in Studio. No other solutions were provided.",
        "Answer_gpt_summary":"solut solut mention discuss us save button studio classic save duplic experi mention solut work studio solut provid"
    },
    {
        "Question_title":"Deploying Model to Kubernetes",
        "Question_body":"<p>I am trying to deploy a model to Kubernetes in Azure Machine Learning Studio, it was working for a while, but now, it fails during deployment, the error message is as follows:<\/p>\n<pre><code>Deploy: Failed on step WaitServiceCreating. Details: AzureML service API error. \nYour container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r.\nFrom the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs.\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally.\nPlease refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.\n<\/code><\/pre>\n<pre class=\"lang-js prettyprint-override\"><code>{\n    &quot;code&quot;: &quot;KubernetesDeploymentFailed&quot;,\n    &quot;statusCode&quot;: 400,\n    &quot;message&quot;: &quot;Kubernetes Deployment failed&quot;,\n    &quot;details&quot;: [\n        {\n            &quot;code&quot;: &quot;CrashLoopBackOff&quot;,\n            &quot;message&quot;: &quot;Your container application crashed. This may be caused by errors in your scoring file's init() function.\nPlease check the logs for your container instance: pipeline-created-on-07-28-2020-r. From the AML SDK, you can run print(service.get_logs()) if you have service object to fetch the logs. \\nYou can also try to run image viennaglobal.azurecr.io\/azureml\/azureml_6ae744633f749472feb283065055dc2c:latest locally. Please refer to http:\/\/aka.ms\/debugimage#service-launch-fails for more information.&quot;\n        }\n    ]\n}\n<\/code><\/pre>",
        "Question_answer_count":2,
        "Question_comment_count":2,
        "Question_created_time":1595913837627,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1595945366092,
        "Question_score":1.0,
        "Question_view_count":186.0,
        "Answer_body":"<p>It seems it was a bug, got corrected by itself today. Closing this question now<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/63127521",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1596104635683,
        "Question_original_content":"deploi model kubernet try deploi model kubernet studio work fail deploy error messag follow deploi fail step waitservicecr detail servic api error contain applic crash caus error score file init function check log contain instanc pipelin creat aml sdk run print servic log servic object fetch log try run imag viennaglob azurecr aeffebdcc latest local refer http aka debugimag servic launch fail inform code kubernetesdeploymentfail statuscod messag kubernet deploy fail detail code crashloopbackoff messag contain applic crash caus error score file init function check log contain instanc pipelin creat aml sdk run print servic log servic object fetch log nyou try run imag viennaglob azurecr aeffebdcc latest local refer http aka debugimag servic launch fail inform",
        "Question_preprocessed_content":"deploi model kubernet try deploi model kubernet studio work fail deploy error messag follow",
        "Question_gpt_summary_original":"The user is facing challenges while deploying a model to Kubernetes in Azure Machine Learning Studio. The deployment fails during the process and the error message suggests that the container application crashed, which could be due to errors in the scoring file's init() function. The user is advised to check the logs for the container instance and run print(service.get_logs()) to fetch the logs. The user can also try running the image locally and refer to the provided link for more information.",
        "Question_gpt_summary":"user face challeng deploi model kubernet studio deploy fail process error messag suggest contain applic crash error score file init function user advis check log contain instanc run print servic log fetch log user try run imag local refer provid link inform",
        "Answer_original_content":"bug got correct todai close question",
        "Answer_preprocessed_content":"bug got correct todai close question",
        "Answer_gpt_summary_original":"No solutions were mentioned in the discussion as the issue was resolved by itself.",
        "Answer_gpt_summary":"solut mention discuss issu resolv"
    },
    {
        "Question_title":"How to access to the featurized dataset in Automated ML",
        "Question_body":"I\u2019m performing a series of experiments with AutoML and I need to see the featurized data. I mean, not just the new features names retrieved by method get_engineered_feature_names() or the featurization details retrieved by get_featurization_summary(), I refer to the whole transformed dataset, the one obtained after scaling\/normalization\/featurization that is then used to train the models.\n\nIs it possible to access to this dataset or download it as a file?\n\nThanks.",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1618306428473,
        "Question_favorite_count":8.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":null,
        "Answer_body":"Hi, currently, we don't store the dataset from scaling\/normalization\/featurization after the run is complete. This feature isn't supported at this time. Sorry for the inconvenience.",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/learn.microsoft.com\/answers\/questions\/355323\/how-to-access-to-the-featurized-dataset-in-automat.html",
        "Tool":"Azure Machine Learning",
        "Platform":"Tool-specific",
        "Question_closed_time":1618340955136,
        "Question_original_content":"access featur dataset autom perform seri experi automl need featur data mean new featur name retriev method engin featur name featur detail retriev featur summari refer transform dataset obtain scale normal featur train model possibl access dataset download file thank",
        "Question_preprocessed_content":"access featur dataset autom perform seri experi automl need featur data mean new featur name retriev method featur detail retriev refer transform dataset obtain train model possibl access dataset download file thank",
        "Question_gpt_summary_original":"The user is facing a challenge in accessing the featurized dataset in Automated ML. They are able to retrieve the names of new features and featurization details, but they need to access the entire transformed dataset that is used to train the models. The user is seeking information on whether it is possible to access or download this dataset as a file.",
        "Question_gpt_summary":"user face challeng access featur dataset autom abl retriev name new featur featur detail need access entir transform dataset train model user seek inform possibl access download dataset file",
        "Answer_original_content":"current store dataset scale normal featur run complet featur isn support time sorri inconveni",
        "Answer_preprocessed_content":"current store dataset run complet featur isn support time sorri inconveni",
        "Answer_gpt_summary_original":"Solution: No solution is provided.",
        "Answer_gpt_summary":"solut solut provid"
    },
    {
        "Question_title":"Is there a solution for multi-user Notebook on SageMaker?",
        "Question_body":"Is there a solution for multi-user Notebook on Studio Notebook or Notebook Instances? Eg if we want several developers to interact on the same notebook at the same time",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1592989945000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":457.0,
        "Answer_body":"Notebook instances are not connected to the user. So if two users has the same access rights they will see and will be able to access the same instance (even in the same time).\n\nThe issue is - Jupyter Notebook is not ready for that, both users will have the same privileges, no tracking who did what, ... And working on the same notebook on the same time - basically they will overwrite each other saves.\n\nI had a need for similar thing (pair programming - data scientist and software engineer) - the only viable solution we were able to find was desktop sharing (like TeamViewer, ...)",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/repost.aws\/questions\/QU4mxRvXy2QYmkYCvdqNVa2g\/is-there-a-solution-for-multi-user-notebook-on-sage-maker",
        "Tool":"Amazon SageMaker",
        "Platform":"Tool-specific",
        "Question_closed_time":1593009773000,
        "Question_original_content":"solut multi user notebook solut multi user notebook studio notebook notebook instanc want develop interact notebook time",
        "Question_preprocessed_content":"solut notebook solut notebook studio notebook notebook instanc want develop interact notebook time",
        "Question_gpt_summary_original":"The user is facing a challenge of finding a solution for multi-user Notebook on SageMaker, specifically on Studio Notebook or Notebook Instances, to allow several developers to interact on the same notebook simultaneously.",
        "Question_gpt_summary":"user face challeng find solut multi user notebook specif studio notebook notebook instanc allow develop interact notebook simultan",
        "Answer_original_content":"notebook instanc connect user user access right abl access instanc time issu jupyt notebook readi user privileg track work notebook time basic overwrit save need similar thing pair program data scientist softwar engin viabl solut abl desktop share like teamview",
        "Answer_preprocessed_content":"notebook instanc connect user user access right abl access instanc issu jupyt notebook readi user privileg track work notebook time basic overwrit save need similar thing viabl solut abl desktop share",
        "Answer_gpt_summary_original":"Solution: The discussion suggests that there is no direct solution for multi-user Notebook on SageMaker, specifically on Studio Notebook or Notebook Instances, to allow several developers to interact on the same notebook simultaneously. However, the only viable solution suggested was desktop sharing (like TeamViewer, ...) for pair programming - data scientist and software engineer.",
        "Answer_gpt_summary":"solut discuss suggest direct solut multi user notebook specif studio notebook notebook instanc allow develop interact notebook simultan viabl solut suggest desktop share like teamview pair program data scientist softwar engin"
    },
    {
        "Question_title":"How to determin which GCP VM do I need for ML",
        "Question_body":"Hi to allIm trying to run a procedure looking to reduce the number of features for a model.The first try was with google Colab pro+ but it keep crashing and nver run the entire process, then I got a VM n1-highmem-8 that has: GPUs1 x NVIDIA Tesla V100  +  n1-highmem-8 (vCPUs: 8, RAM: 52GB)and still not getting the process done.The question is how to determin which type of machine should I use? Can I get any metric from the cell that is runing in colab and be able to determin the Type of VM that I need?",
        "Question_answer_count":2,
        "Question_comment_count":0,
        "Question_created_time":1658842140000,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":73.0,
        "Answer_body":"There are a few things to take in consideration:\n\nHave you installed all the necessary drivers for the GPU? Here is a complete guide that you can follow.\nI do not see any Python wrapper for CUDA in your code. The way you specify when to use the GPU for specific tasks is through this wrapper, it seems to me that you are using the CPU instead and that is why the task keeps crashing. Now, converting your code to a CUDA version is not a trivial task, and it involves a deeper knowledge on how a GPU works. If you are in a hurry, you could try the Py2CUDA github project, but I would strongly recommend taking a look at the Getting Started Blogs.\u00a0\u00a0\n\nView solution in original post",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/www.googlecloudcommunity.com\/gc\/AI-ML\/How-to-determin-which-GCP-VM-do-I-need-for-ML\/td-p\/447075\/jump-to\/first-unread-message",
        "Tool":"Vertex AI",
        "Platform":"Tool-specific",
        "Question_closed_time":1659349860000,
        "Question_original_content":"determin gcp need allim try run procedur look reduc number featur model try googl colab pro crash nver run entir process got highmem gpu nvidia tesla highmem vcpu ram get process question determin type machin us metric cell rune colab abl determin type need",
        "Question_preprocessed_content":"determin gcp need allim try run procedur look reduc number featur try googl colab pro crash nver run entir process got gpu nvidia tesla get process question determin type machin us metric cell rune colab abl determin type need",
        "Question_gpt_summary_original":"The user is facing challenges in determining the appropriate GCP VM for running a procedure to reduce the number of features for a model. They have tried using Google Colab Pro+ but it keeps crashing, and even after getting a VM with 1 NVIDIA Tesla V100 and n1-highmem-8 (8 vCPUs and 52GB RAM), the process is still not completing. The user is seeking advice on how to determine the type of machine they need and if they can get any metrics from the cell running in Colab to help with this decision.",
        "Question_gpt_summary":"user face challeng determin appropri gcp run procedur reduc number featur model tri googl colab pro keep crash get nvidia tesla highmem vcpu ram process complet user seek advic determin type machin need metric cell run colab help decis",
        "Answer_original_content":"thing consider instal necessari driver gpu complet guid follow python wrapper cuda code wai specifi us gpu specif task wrapper cpu instead task keep crash convert code cuda version trivial task involv deeper knowledg gpu work hurri try pycuda github project strongli recommend take look get start blog view solut origin post",
        "Answer_preprocessed_content":"thing consider instal necessari driver gpu complet guid follow python wrapper cuda code wai specifi us gpu specif task wrapper cpu instead task keep crash convert code cuda version trivial task involv deeper knowledg gpu work hurri try cuda github project strongli recommend take look get start blog view solut origin post",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n- Check if all necessary drivers for the GPU are installed.\n- Use a Python wrapper for CUDA to specify when to use the GPU for specific tasks.\n- Consider converting the code to a CUDA version, which involves a deeper knowledge of how a GPU works.\n- Try the Py2CUDA github project if in a hurry, but it is recommended to take a look at the Getting Started Blogs.\n\nNo solution is mentioned for determining the type of machine needed or getting metrics from the cell running in Colab to help with this decision.",
        "Answer_gpt_summary":"possibl solut mention discuss check necessari driver gpu instal us python wrapper cuda specifi us gpu specif task consid convert code cuda version involv deeper knowledg gpu work try pycuda github project hurri recommend look get start blog solut mention determin type machin need get metric cell run colab help decis"
    },
    {
        "Question_title":"Cannot deploy a trained model to an existing AKS compute target",
        "Question_body":"<p>I have a model that was trained on a Machine Learning Compute on Azure Machine Learning Service. The registered model already lives my workspace and I would like to deploy it to a pre-existing AKS instance that I previously provisioned in my workspace. I am able to successfully configure and register the container image: <\/p>\n\n<pre><code># retrieve cloud representations of the models\nrf = Model(workspace=ws, name='pumps_rf')\nle = Model(workspace=ws, name='pumps_le')\nohc = Model(workspace=ws, name='pumps_ohc')\nprint(rf); print(le); print(ohc)\n\n&lt;azureml.core.model.Model object at 0x7f66ab3b1f98&gt;\n&lt;azureml.core.model.Model object at 0x7f66ab7e49b0&gt;\n&lt;azureml.core.model.Model object at 0x7f66ab85e710&gt;\n\npackage_list = [\n  'category-encoders==1.3.0',\n  'numpy==1.15.0',\n  'pandas==0.24.1',\n  'scikit-learn==0.20.2']\n\n# Conda environment configuration\nmyenv = CondaDependencies.create(pip_packages=package_list)\nconda_yml = 'file:'+os.getcwd()+'\/myenv.yml'\n\nwith open(conda_yml,\"w\") as f:\n    f.write(myenv.serialize_to_string())\n<\/code><\/pre>\n\n<p>Configuring and registering the image works:<\/p>\n\n<pre><code># Image configuration\nimage_config = ContainerImage.image_configuration(execution_script='score.py', \n                                                  runtime='python', \n                                                  conda_file='myenv.yml',\n                                                  description='Pumps Random Forest model')\n\n\n# Register the image from the image configuration\n# to Azure Container Registry\nimage = ContainerImage.create(name = Config.IMAGE_NAME, \n                              models = [rf, le, ohc],\n                              image_config = image_config,\n                              workspace = ws)\n\nCreating image\nRunning....................\nSucceededImage creation operation finished for image pumpsrfimage:2, operation \"Succeeded\"\n<\/code><\/pre>\n\n<p>Attaching to an existing cluster also works:<\/p>\n\n<pre><code># Attach the cluster to your workgroup\nattach_config = AksCompute.attach_configuration(resource_group = Config.RESOURCE_GROUP,\n                                                cluster_name = Config.DEPLOY_COMPUTE)\naks_target = ComputeTarget.attach(workspace=ws, \n                                  name=Config.DEPLOY_COMPUTE, \n                                  attach_configuration=attach_config)\n\n# Wait for the operation to complete\naks_target.wait_for_completion(True)\nSucceededProvisioning operation finished, operation \"Succeeded\"\n<\/code><\/pre>\n\n<p>However, when I try to deploy the image to the existing cluster, it fails with a <code>WebserviceException<\/code>. <\/p>\n\n<pre><code># Set configuration and service name\naks_config = AksWebservice.deploy_configuration()\n\n# Deploy from image\nservice = Webservice.deploy_from_image(workspace = ws,\n                                       name = 'pumps-aks-service-1' ,\n                                       image = image,\n                                       deployment_config = aks_config,\n                                       deployment_target = aks_target)\n# Wait for the deployment to complete\nservice.wait_for_deployment(show_output = True)\nprint(service.state)\n\nWebserviceException: Unable to create service with image pumpsrfimage:1 in non \"Succeeded\" creation state.\n---------------------------------------------------------------------------\nWebserviceException                       Traceback (most recent call last)\n&lt;command-201219424688503&gt; in &lt;module&gt;()\n      7                                        image = image,\n      8                                        deployment_config = aks_config,\n----&gt; 9                                        deployment_target = aks_target)\n     10 # Wait for the deployment to complete\n     11 service.wait_for_deployment(show_output = True)\n\n\/databricks\/python\/lib\/python3.5\/site-packages\/azureml\/core\/webservice\/webservice.py in deploy_from_image(workspace, name, image, deployment_config, deployment_target)\n    284                         return child._deploy(workspace, name, image, deployment_config, deployment_target)\n    285 \n--&gt; 286         return deployment_config._webservice_type._deploy(workspace, name, image, deployment_config, deployment_target)\n    287 \n    288     @staticmethod\n\n\/databricks\/python\/lib\/python3.5\/site-packages\/azureml\/core\/webservice\/aks.py in _deploy(workspace, name, image, deployment_config, deployment_target)\n<\/code><\/pre>\n\n<p>Any ideas on how to solve this issue? I am writing the code in a Databricks notebook. Also, I am able to create and deploy the cluster using Azure Portal no problem so this appears to be an issue with my code\/Python SDK or the way Databricks works with AMLS.<\/p>\n\n<p>UPDATE:\nI was able to deploy my image to AKS using Azure Portal and the webservice works as expected. This means the issue lies somewhere between Databricks, the Azureml Python SDK and Machine Learning Service.<\/p>\n\n<p>UPDATE 2:\nI'm working with Microsoft to fix this issue. Will report back once we have a solution.<\/p>",
        "Question_answer_count":3,
        "Question_comment_count":2,
        "Question_created_time":1550704526117,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1554647342087,
        "Question_score":2.0,
        "Question_view_count":1051.0,
        "Answer_body":"<p>In my initial code, when creating the image, I was not using:<\/p>\n\n<pre><code>image.wait_for_creation(show_output=True)\n<\/code><\/pre>\n\n<p>As a consequence, I was calling <code>CreateImage<\/code> and <code>DeployImage<\/code> before the image was created which errored out. Can't believe it was that simple.. <\/p>\n\n<p>UPDATED IMAGE CREATION SNIPPET:<\/p>\n\n<pre><code># Register the image from the image configuration\n# to Azure Container Registry\nimage = ContainerImage.create(name = Config.IMAGE_NAME, \n                              models = [rf, le, ohc],\n                              image_config = image_config,\n                              workspace = ws)\n\nimage.wait_for_creation(show_output=True)\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/54796762",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1551990125420,
        "Question_original_content":"deploi train model exist ak comput target model train machin learn comput servic regist model live workspac like deploi pre exist ak instanc previous provis workspac abl successfulli configur regist contain imag retriev cloud represent model model workspac pump model workspac pump ohc model workspac pump ohc print print print ohc packag list categori encod numpi panda scikit learn conda environ configur myenv condadepend creat pip packag packag list conda yml file getcwd myenv yml open conda yml write myenv serial string configur regist imag work imag configur imag config containerimag imag configur execut script score runtim python conda file myenv yml descript pump random forest model regist imag imag configur azur contain registri imag containerimag creat config imag model ohc imag config imag config workspac creat imag run succeededimag creation oper finish imag pumpsrfimag oper succeed attach exist cluster work attach cluster workgroup attach config akscomput attach configur resourc group config resourc group cluster config deploi comput ak target computetarget attach workspac config deploi comput attach configur attach config wait oper complet ak target wait complet true succeededprovis oper finish oper succeed try deploi imag exist cluster fail webserviceexcept set configur servic ak config akswebservic deploi configur deploi imag servic webservic deploi imag workspac pump ak servic imag imag deploy config ak config deploy target ak target wait deploy complet servic wait deploy output true print servic state webserviceexcept unabl creat servic imag pumpsrfimag non succeed creation state webserviceexcept traceback recent imag imag deploy config ak config deploy target ak target wait deploy complet servic wait deploy output true databrick python lib python site packag core webservic webservic deploi imag workspac imag deploy config deploy target return child deploi workspac imag deploy config deploy target return deploy config webservic type deploi workspac imag deploy config deploy target staticmethod databrick python lib python site packag core webservic ak deploi workspac imag deploy config deploy target idea solv issu write code databrick notebook abl creat deploi cluster azur portal problem appear issu code python sdk wai databrick work aml updat abl deploi imag ak azur portal webservic work expect mean issu li databrick python sdk machin learn servic updat work microsoft fix issu report solut",
        "Question_preprocessed_content":"deploi train model exist ak comput target model train machin learn comput servic regist model live workspac like deploi ak instanc previous provis workspac abl successfulli configur regist contain imag configur regist imag work attach exist cluster work try deploi imag exist cluster fail idea solv issu write code databrick notebook abl creat deploi cluster azur portal problem appear issu sdk wai databrick work aml updat abl deploi imag ak azur portal webservic work expect mean issu li databrick python sdk machin learn servic updat work microsoft fix issu report solut",
        "Question_gpt_summary_original":"The user is facing challenges in deploying a trained model to an existing AKS compute target. Although the user is able to configure and register the container image, attaching to an existing cluster works, but when trying to deploy the image to the existing cluster, it fails with a WebserviceException. The user is seeking help to solve this issue and has reported the problem to Microsoft.",
        "Question_gpt_summary":"user face challeng deploi train model exist ak comput target user abl configur regist contain imag attach exist cluster work try deploi imag exist cluster fail webserviceexcept user seek help solv issu report problem microsoft",
        "Answer_original_content":"initi code creat imag imag wait creation output true consequ call createimag deployimag imag creat error believ simpl updat imag creation snippet regist imag imag configur azur contain registri imag containerimag creat config imag model ohc imag config imag config workspac imag wait creation output true",
        "Answer_preprocessed_content":"initi code creat imag consequ call imag creat error believ updat imag creation snippet",
        "Answer_gpt_summary_original":"Solution: The user was able to solve the issue by adding the line \"image.wait_for_creation(show_output=True)\" in the code before calling CreateImage and DeployImage. This ensured that the image was created before it was deployed, which resolved the WebserviceException error.",
        "Answer_gpt_summary":"solut user abl solv issu ad line imag wait creation output true code call createimag deployimag ensur imag creat deploi resolv webserviceexcept error"
    },
    {
        "Question_title":"Registering and downloading a fastText .bin model fails with Azure Machine Learning Service",
        "Question_body":"<p>I have a simple RegisterModel.py script that uses the Azure ML Service SDK to register a fastText .bin model. This completes successfully and I can see the model in the Azure Portal UI (I cannot see what model files are in it). I then want to download the model (DownloadModel.py) and use it (for testing purposes), however it throws an error on the <strong>model.download<\/strong> method (<em>tarfile.ReadError: file could not be opened successfully<\/em>) and makes a 0 byte rjtestmodel8.tar.gz file.<\/p>\n\n<p>I then use the Azure Portal and Add Model and select the same bin model file and it uploads fine. Downloading it with the download.py script below works fine, so I am assuming something is not correct with the Register script.<\/p>\n\n<p>Here are the 2 scripts and the stacktrace - let me know if you can see anything wrong:<\/p>\n\n<p><strong>RegisterModel.py<\/strong><\/p>\n\n<pre><code>import azureml.core\nfrom azureml.core import Workspace, Model\nws = Workspace.from_config()\nmodel = Model.register(workspace=ws,\n                       model_name='rjSDKmodel10',\n                       model_path='riskModel.bin')\n<\/code><\/pre>\n\n<p><strong>DownloadModel.py<\/strong><\/p>\n\n<pre><code># Works when downloading the UI Uploaded .bin file, but not the SDK registered .bin file\nimport os\nimport azureml.core\nfrom azureml.core import Workspace, Model\n\nws = Workspace.from_config()\nmodel = Model(workspace=ws, name='rjSDKmodel10')\nmodel.download(target_dir=os.getcwd(), exist_ok=True)\n<\/code><\/pre>\n\n<p><strong>Stacktrace<\/strong><\/p>\n\n<pre><code>Traceback (most recent call last):\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\ptvsd_launcher.py\", line 43, in &lt;module&gt;\n    main(ptvsdArgs)\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 432, in main\n    run()\n  File \"...\\.vscode\\extensions\\ms-python.python-2019.9.34474\\pythonFiles\\lib\\python\\ptvsd\\__main__.py\", line 316, in run_file\n    runpy.run_path(target, run_name='__main__')\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 263, in run_path\n    pkg_name=pkg_name, script_name=fname)\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 96, in _run_module_code\n    mod_name, mod_spec, pkg_name, script_name)\n  File \"...\\.conda\\envs\\DoC\\lib\\runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"...\\\\DownloadModel.py\", line 21, in &lt;module&gt;\n    model.download(target_dir=os.getcwd(), exist_ok=True)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 712, in download\n    file_paths = self._download_model_files(sas_to_relative_download_path, target_dir, exist_ok)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 658, in _download_model_files\n    file_paths = self._handle_packed_model_file(tar_path, target_dir, exist_ok)\n  File \"...\\.conda\\envs\\DoC\\lib\\site-packages\\azureml\\core\\model.py\", line 670, in _handle_packed_model_file\n    with tarfile.open(tar_path) as tar:\n  File \"...\\.conda\\envs\\DoC\\lib\\tarfile.py\", line 1578, in open\n    raise ReadError(\"file could not be opened successfully\")\ntarfile.ReadError: file could not be opened successfully\n<\/code><\/pre>\n\n<p><strong>Environment<\/strong><\/p>\n\n<ul>\n<li>riskModel.bin is 6 megs<\/li>\n<li>AMLS 1.0.60<\/li>\n<li>Python 3.7<\/li>\n<li>Working locally with Visual Code<\/li>\n<\/ul>",
        "Question_answer_count":1,
        "Question_comment_count":2,
        "Question_created_time":1568032306497,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1568060910630,
        "Question_score":0.0,
        "Question_view_count":281.0,
        "Answer_body":"<p>The Azure Machine Learning service SDK has a bug with how it interacts with Azure Storage, which causes it to upload corrupted files if it has to retry uploading. <\/p>\n\n<p>A couple workarounds:<\/p>\n\n<ol>\n<li>The bug was introduced in 1.0.60 release. If you downgrade to AzureML-SDK 1.0.55, the code should fail when there are issue uploading instead of silently corrupting data.<\/li>\n<li>It's possible that the retry is being triggered by the low timeout values that the AzureML-SDK defaults to. You could investigate changing the timeout in <code>site-packages\/azureml\/_restclient\/artifacts_client.py<\/code><\/li>\n<\/ol>\n\n<p>This bug should be fixed in the next release of the AzureML-SDK.<\/p>",
        "Answer_comment_count":4.0,
        "Answer_last_edit_time":1568054711300,
        "Answer_score":2.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57854136",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1568054240467,
        "Question_original_content":"regist download fasttext bin model fail servic simpl registermodel script us servic sdk regist fasttext bin model complet successfulli model azur portal model file want download model downloadmodel us test purpos throw error model download method tarfil readerror file open successfulli make byte rjtestmodel tar file us azur portal add model select bin model file upload fine download download script work fine assum correct regist script script stacktrac let know wrong registermodel import core core import workspac model workspac config model model regist workspac model rjsdkmodel model path riskmodel bin downloadmodel work download upload bin file sdk regist bin file import import core core import workspac model workspac config model model workspac rjsdkmodel model download target dir getcwd exist true stacktrac traceback recent file vscode extens python python pythonfil ptvsd launcher line main ptvsdarg file vscode extens python python pythonfil lib python ptvsd main line main run file vscode extens python python pythonfil lib python ptvsd main line run file runpi run path target run main file conda env doc lib runpi line run path pkg pkg script fname file conda env doc lib runpi line run modul code mod mod spec pkg script file conda env doc lib runpi line run code exec code run global file downloadmodel line model download target dir getcwd exist true file conda env doc lib site packag core model line download file path self download model file sa rel download path target dir exist file conda env doc lib site packag core model line download model file file path self handl pack model file tar path target dir exist file conda env doc lib site packag core model line handl pack model file tarfil open tar path tar file conda env doc lib tarfil line open rais readerror file open successfulli tarfil readerror file open successfulli environ riskmodel bin meg aml python work local visual code",
        "Question_preprocessed_content":"regist download fasttext bin model fail servic simpl script us servic sdk regist fasttext bin model complet successfulli model azur portal want download model us throw error method make byte file us azur portal add model select bin model file upload fine download script work fine assum correct regist script script stacktrac let know wrong stacktrac environ meg aml python work local visual code",
        "Question_gpt_summary_original":"The user is facing challenges in registering and downloading a fastText .bin model using Azure Machine Learning Service SDK. The model is successfully registered, but the download fails with an error message. The user has provided two scripts and a stack trace, and the error seems to be related to the tarfile.ReadError: file could not be opened successfully. The user has also provided information about the environment, including the model size, AMLS version, Python version, and the use of Visual Code.",
        "Question_gpt_summary":"user face challeng regist download fasttext bin model servic sdk model successfulli regist download fail error messag user provid script stack trace error relat tarfil readerror file open successfulli user provid inform environ includ model size aml version python version us visual code",
        "Answer_original_content":"servic sdk bug interact azur storag caus upload corrupt file retri upload coupl workaround bug introduc releas downgrad sdk code fail issu upload instead silent corrupt data possibl retri trigger low timeout valu sdk default investig chang timeout site packag restclient artifact client bug fix releas sdk",
        "Answer_preprocessed_content":"servic sdk bug interact azur storag caus upload corrupt file retri upload coupl workaround bug introduc releas downgrad sdk code fail issu upload instead silent corrupt data possibl retri trigger low timeout valu sdk default investig chang timeout bug fix releas sdk",
        "Answer_gpt_summary_original":"Solutions provided:\n- Downgrade to AzureML-SDK 1.0.55 to avoid silently corrupting data.\n- Investigate changing the timeout in `site-packages\/azureml\/_restclient\/artifacts_client.py` to avoid triggering the retry that causes the issue.\n- The bug should be fixed in the next release of the AzureML-SDK.",
        "Answer_gpt_summary":"solut provid downgrad sdk avoid silent corrupt data investig chang timeout site packag restclient artifact client avoid trigger retri caus issu bug fix releas sdk"
    },
    {
        "Question_title":"Use dataset registed in on pipelines in AML",
        "Question_body":"<p>I was following the <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/tutorial-pipeline-python-sdk\" rel=\"nofollow noreferrer\">SDK v2 Python tutorial<\/a> in order to create a pipeline job with my own assets. I notice that in this tutorial they let you use a csv file that can be downloaded but Im trying to use a registered dataset that I already registered by my own. The problem that I facing is that I dont know where I need to specify the dataset.<\/p>\n<p>The funny part is that at the beginning they create this dataset like this:<\/p>\n<pre><code>credit_data = ml_client.data.create_or_update(credit_data)\nprint(\n    f&quot;Dataset with name {credit_data.name} was registered to workspace, the dataset version is {credit_data.version}&quot;\n)\n<\/code><\/pre>\n<p>But the only part where they refer to this dataset is on the last part where they # the line:<\/p>\n<pre><code>registered_model_name = &quot;credit_defaults_model&quot;\n\n# Let's instantiate the pipeline with the parameters of our choice\npipeline = credit_defaults_pipeline(\n    # pipeline_job_data_input=credit_data,\n    pipeline_job_data_input=Input(type=&quot;uri_file&quot;, path=web_path),\n    pipeline_job_test_train_ratio=0.2,\n    pipeline_job_learning_rate=0.25,\n    pipeline_job_registered_model_name=registered_model_name,\n)\n<\/code><\/pre>\n<p>For me this means that I can use this data like this (a already registered dataset), the problem is that I don't know where I need to do the changes (I know that in the data_prep.py and in the code below but I don\u00b4t know where else) and I don't know how to set this:<\/p>\n<pre><code>%%writefile {data_prep_src_dir}\/data_prep.py\n...\n\ndef main():\n    &quot;&quot;&quot;Main function of the script.&quot;&quot;&quot;\n\n    # input and output arguments\n    parser = argparse.ArgumentParser()\n    parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don\u00b4t know how\n    parser.add_argument(&quot;--test_train_ratio&quot;, type=float, required=False, default=0.25)\n    parser.add_argument(&quot;--train_data&quot;, type=str, help=&quot;path to train data&quot;)\n    parser.add_argument(&quot;--test_data&quot;, type=str, help=&quot;path to test data&quot;)\n    args = parser.parse_args()\n\n...\n<\/code><\/pre>\n<p>Does anyone have experience working as registered datasets?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1656688618547,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1657482591390,
        "Question_score":0.0,
        "Question_view_count":85.0,
        "Answer_body":"<blockquote>\n<p>parser.add_argument(&quot;--data&quot;, type=str, help=&quot;path to input data&quot;) # &lt;=== Here, but I don\u00b4t know how<\/p>\n<\/blockquote>\n<p>To get the path to input data, according to <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/how-to-train-with-datasets.md\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<ul>\n<li><p>You can get <code>--input-data<\/code> by ID which you can access in your training script.<\/p>\n<\/li>\n<li><p>Use it as <code>argument<\/code> on <code>mounted_input_path<\/code><\/p>\n<\/li>\n<\/ul>\n<p>For example, try the following three code snippets taken from the <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/how-to-train-with-datasets.md\" rel=\"nofollow noreferrer\">documentation<\/a>:<\/p>\n<p><strong>Access dataset in training script:<\/strong><\/p>\n<pre><code>parser = argparse.ArgumentParser()\nparser.add_argument(&quot;--input-data&quot;, type=str)\nargs = parser.parse_args()\n\nrun = Run.get_context()\nws = run.experiment.workspace\n\n# get the input dataset by ID\ndataset = Dataset.get_by_id(ws, id=args.input_data)\n<\/code><\/pre>\n<p><strong>Configure the training run:<\/strong><\/p>\n<pre><code>src = ScriptRunConfig(source_directory=script_folder,\n                      script='train_titanic.py',\n                      # pass dataset as an input with friendly name 'titanic'\n                      arguments=['--input-data', titanic_ds.as_named_input('titanic')],\n                      compute_target=compute_target,\n                      environment=myenv)\n<\/code><\/pre>\n<p><strong>Pass <code>mounted_input_path<\/code> as argument:<\/strong><\/p>\n<pre><code>mounted_input_path = sys.argv[1]\nmounted_output_path = sys.argv[2]\n\nprint(&quot;Argument 1: %s&quot; % mounted_input_path)\nprint(&quot;Argument 2: %s&quot; % mounted_output_path)\n<\/code><\/pre>\n<p>References: <a href=\"https:\/\/github.com\/MicrosoftDocs\/azure-docs\/blob\/main\/articles\/machine-learning\/v1\/how-to-create-register-datasets.md\" rel=\"nofollow noreferrer\">How to create register dataset<\/a> and <a href=\"https:\/\/github.com\/Azure\/MachineLearningNotebooks\/blob\/master\/how-to-use-azureml\/work-with-data\/datasets-tutorial\/scriptrun-with-data-input-output\/how-to-use-scriptrun.ipynb\" rel=\"nofollow noreferrer\">How to use configure a training run with data input and output<\/a><\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/72831360",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1657082550892,
        "Question_original_content":"us dataset regist pipelin aml follow sdk python tutori order creat pipelin job asset notic tutori let us csv file download try us regist dataset regist problem face dont know need specifi dataset funni begin creat dataset like credit data client data creat updat credit data print dataset credit data regist workspac dataset version credit data version refer dataset line regist model credit default model let instanti pipelin paramet choic pipelin credit default pipelin pipelin job data input credit data pipelin job data input input type uri file path web path pipelin job test train ratio pipelin job learn rate pipelin job regist model regist model mean us data like regist dataset problem know need chang know data prep code dont know know set writefil data prep src dir data prep def main main function script input output argument parser argpars argumentpars parser add argument data type str help path input data dont know parser add argument test train ratio type float requir fals default parser add argument train data type str help path train data parser add argument test data type str help path test data arg parser pars arg experi work regist dataset",
        "Question_preprocessed_content":"us dataset regist pipelin aml follow sdk python tutori order creat pipelin job asset notic tutori let us csv file download try us regist dataset regist problem face dont know need specifi dataset funni begin creat dataset like refer dataset line mean us data like problem know need chang know set experi work regist dataset",
        "Question_gpt_summary_original":"The user is facing challenges in using a registered dataset in a pipeline job while following the SDK v2 Python tutorial. The user is unsure where to specify the dataset and how to make changes in the data_prep.py file to use the registered dataset. The user is seeking help from someone with experience in working with registered datasets.",
        "Question_gpt_summary":"user face challeng regist dataset pipelin job follow sdk python tutori user unsur specifi dataset chang data prep file us regist dataset user seek help experi work regist dataset",
        "Answer_original_content":"parser add argument data type str help path input data dont know path input data accord document input data access train script us argument mount input path exampl try follow code snippet taken document access dataset train script parser argpars argumentpars parser add argument input data type str arg parser pars arg run run context run experi workspac input dataset dataset dataset arg input data configur train run src scriptrunconfig sourc directori script folder script train titan pass dataset input friendli titan argument input data titan name input titan comput target comput target environ myenv pass mount input path argument mount input path sy argv mount output path sy argv print argument mount input path print argument mount output path refer creat regist dataset us configur train run data input output",
        "Answer_preprocessed_content":"type str help path input data dont know path input data accord document access train script us exampl try follow code snippet taken document access dataset train script configur train run pass argument refer creat regist dataset us configur train run data input output",
        "Answer_gpt_summary_original":"The discussion provides solutions to the challenge of using a registered dataset in a pipeline job while following the SDK v2 Python tutorial. The solutions include accessing the dataset in the training script using the dataset ID, configuring the training run by passing the dataset as an input with a friendly name, and passing the mounted_input_path as an argument. The references to the documentation on creating and registering datasets and configuring a training run with data input and output are also provided.",
        "Answer_gpt_summary":"discuss provid solut challeng regist dataset pipelin job follow sdk python tutori solut includ access dataset train script dataset configur train run pass dataset input friendli pass mount input path argument refer document creat regist dataset configur train run data input output provid"
    },
    {
        "Question_title":"Wandb for Huggingface Trainer saves only first model",
        "Question_body":"<p>I am finetuning multiple models using for loop as follows.<\/p>\n<pre><code class=\"lang-auto\">for file in os.listdir(args.data_dir):\n    finetune(args, file)\n<\/code><\/pre>\n<p>BUT <code>wandb<\/code> shows logs only for the first file in <code>data_dir<\/code> although it is training and saving models for other files. It feels very strange behavior.<\/p>\n<pre><code class=\"lang-auto\">wandb: Synced bertweet-base-finetuned-file1: https:\/\/wandb.ai\/***\/huggingface\/runs\/***\n<\/code><\/pre>\n<p>This is a small snippet of <strong>finetuning<\/strong> code with Huggingface:<\/p>\n<pre><code class=\"lang-auto\">def finetune(args, file):\n    training_args = TrainingArguments(\n        output_dir=f'{model_name}-finetuned-{file}',\n        overwrite_output_dir=True,\n        evaluation_strategy='no',\n        num_train_epochs=args.epochs,\n        learning_rate=args.lr,\n        weight_decay=args.decay,\n        per_device_train_batch_size=args.batch_size,\n        per_device_eval_batch_size=args.batch_size,\n        fp16=True, # mixed-precision training to boost speed\n        save_strategy='no',\n        seed=args.seed,\n        dataloader_num_workers=4,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=tokenized_dataset['train'],\n        eval_dataset=None,\n        data_collator=data_collator,\n    )\n    trainer.train()\n    trainer.save_model()\n<\/code><\/pre>",
        "Question_answer_count":3,
        "Question_comment_count":0,
        "Question_created_time":1650439111790,
        "Question_favorite_count":null,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":166.0,
        "Answer_body":"<p><code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> helped me to log the models <strong>separately<\/strong> on wandb website.<\/p>\n<p>The working code looks like below:<\/p>\n<pre><code class=\"lang-auto\">\nfor file in os.listdir(args.data_dir):\n    finetune(args, file)\n\nimport wandb\ndef finetune(args, file):\n    run = wandb.init(reinit=True)\n    ...\n    run.finish()\n<\/code><\/pre>\n<p>Reference: <a href=\"https:\/\/docs.wandb.ai\/guides\/track\/launch#how-do-i-launch-multiple-runs-from-one-script\" class=\"inline-onebox\">Launch Experiments with wandb.init - Documentation<\/a><\/p>",
        "Answer_comment_count":null,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/community.wandb.ai\/t\/wandb-for-huggingface-trainer-saves-only-first-model\/2270",
        "Tool":"Weights & Biases",
        "Platform":"Tool-specific",
        "Question_closed_time":1650552651672,
        "Question_original_content":"huggingfac trainer save model finetun multipl model loop follow file listdir arg data dir finetun arg file show log file data dir train save model file feel strang behavior sync bertweet base finetun file http huggingfac run small snippet finetun code huggingfac def finetun arg file train arg trainingargu output dir model finetun file overwrit output dir true evalu strategi num train epoch arg epoch learn rate arg weight decai arg decai devic train batch size arg batch size devic eval batch size arg batch size true mix precis train boost speed save strategi seed arg seed dataload num worker trainer trainer model model arg train arg train dataset token dataset train eval dataset data collat data collat trainer train trainer save model",
        "Question_preprocessed_content":"huggingfac trainer save model finetun multipl model loop follow show log file train save model file feel strang behavior small snippet finetun code huggingfac",
        "Question_gpt_summary_original":"The user is facing a challenge where the logs from wandb for Huggingface Trainer are only showing for the first file in the data directory, even though the code is training and saving models for other files. The user finds this behavior strange and is seeking a solution.",
        "Question_gpt_summary":"user face challeng log huggingfac trainer show file data directori code train save model file user find behavior strang seek solut",
        "Answer_original_content":"init reinit true run finish help log model separ websit work code look like file listdir arg data dir finetun arg file import def finetun arg file run init reinit true run finish refer launch experi init document",
        "Answer_preprocessed_content":"help log model separ websit work code look like refer launch experi init document",
        "Answer_gpt_summary_original":"Solution: The user found a solution to log the models separately on the wandb website by using the <code>wandb.init(reinit=True)<\/code> and <code>run.finish()<\/code> functions. The user modified the code to include these functions within the <code>finetune()<\/code> function and looped through the files in the data directory to train and save models for each file separately.",
        "Answer_gpt_summary":"solut user solut log model separ websit init reinit true run finish function user modifi code includ function finetun function loop file data directori train save model file separ"
    },
    {
        "Question_title":"Cannot construct an Explanation object",
        "Question_body":"<p>Trying to construct an <code>Explanation<\/code> object for a unit test, but can't seem to get it to work. Here's what I'm trying:<\/p>\n<pre><code>from google.cloud import aiplatform\n\naiplatform.compat.types.explanation_v1.Explanation(\n    attributions=aiplatform.compat.types.explanation_v1.Attribution(\n        {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n    )\n)\n<\/code><\/pre>\n<p>which gives:<\/p>\n<pre><code>&quot;.venv\/lib\/python3.7\/site-packages\/proto\/message.py&quot;, line 521, in __init__\n    super().__setattr__(&quot;_pb&quot;, self._meta.pb(**params))\nTypeError: Value must be iterable\n<\/code><\/pre>\n<p>I found <a href=\"https:\/\/github.com\/googleapis\/gapic-generator-python\/issues\/413#issuecomment-872094378\" rel=\"nofollow noreferrer\">this<\/a> on github, but I'm not sure how to apply that workaround here.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1644344024883,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":74.0,
        "Answer_body":"<p>As the error mentioned value to be passed at <code>attributions<\/code> should be <strong>iterable<\/strong>. See <a href=\"https:\/\/cloud.google.com\/python\/docs\/reference\/aiplatform\/latest\/google.cloud.aiplatform_v1.types.Explanation\" rel=\"nofollow noreferrer\">Explanation attributes documentation<\/a>.<\/p>\n<p>I tried your code and placed the <code>Attribution<\/code> object in a list and the error is gone. I assigned your objects in variables just so the code is readable.<\/p>\n<p>See code and testing below:<\/p>\n<pre><code>from google.cloud import aiplatform\n\ntest = {\n            &quot;approximation_error&quot;: 0.010399332817679649,\n            &quot;baseline_output_value&quot;: 0.9280818700790405,\n            &quot;feature_attributions&quot;: {\n                &quot;feature_1&quot;: -0.0410824716091156,\n                &quot;feature_2&quot;: 0.01155053575833639,\n            },\n            &quot;instance_output_value&quot;: 0.6717480421066284,\n            &quot;output_display_name&quot;: &quot;true&quot;,\n            &quot;output_index&quot;: [0],\n            &quot;output_name&quot;: &quot;scores&quot;,\n        }\n\nattributions=aiplatform.compat.types.explanation_v1.Attribution(test)\nx  = aiplatform.compat.types.explanation_v1.Explanation(\n    attributions=[attributions]\n)\nprint(x)\n<\/code><\/pre>\n<p>Output:<\/p>\n<pre><code>attributions {\n  baseline_output_value: 0.9280818700790405\n  instance_output_value: 0.6717480421066284\n  feature_attributions {\n    struct_value {\n      fields {\n        key: &quot;feature_1&quot;\n        value {\n          number_value: -0.0410824716091156\n        }\n      }\n      fields {\n        key: &quot;feature_2&quot;\n        value {\n          number_value: 0.01155053575833639\n        }\n      }\n    }\n  }\n  output_index: 0\n  output_display_name: &quot;true&quot;\n  approximation_error: 0.010399332817679649\n  output_name: &quot;scores&quot;\n}\n<\/code><\/pre>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1644387344880,
        "Answer_score":1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/71038823",
        "Tool":"Vertex AI",
        "Platform":"Stack Overflow",
        "Question_closed_time":1644385932888,
        "Question_original_content":"construct explan object try construct explan object unit test work try googl cloud import aiplatform aiplatform compat type explan explan attribut aiplatform compat type explan attribut approxim error baselin output valu featur attribut featur featur instanc output valu output displai true output index output score give venv lib python site packag proto messag line init super setattr self meta param typeerror valu iter github sure appli workaround",
        "Question_preprocessed_content":"construct explan object try construct object unit test work try give github sure appli workaround",
        "Question_gpt_summary_original":"The user is facing a challenge in constructing an Explanation object for a unit test using the Google Cloud AI Platform. The code they are trying to use is not working and is resulting in a TypeError. They have found a workaround on GitHub but are unsure how to apply it to their situation.",
        "Question_gpt_summary":"user face challeng construct explan object unit test googl cloud platform code try us work result typeerror workaround github unsur appli situat",
        "Answer_original_content":"error mention valu pass attribut iter explan attribut document tri code place attribut object list error gone assign object variabl code readabl code test googl cloud import aiplatform test approxim error baselin output valu featur attribut featur featur instanc output valu output displai true output index output score attribut aiplatform compat type explan attribut test aiplatform compat type explan explan attribut attribut print output attribut baselin output valu instanc output valu featur attribut struct valu field kei featur valu number valu field kei featur valu number valu output index output displai true approxim error output score",
        "Answer_preprocessed_content":"error mention valu pass iter explan attribut document tri code place object list error gone assign object variabl code readabl code test output",
        "Answer_gpt_summary_original":"Solution: The error is caused by passing a non-iterable value to the `attributions` parameter. The solution is to pass an iterable value, such as a list, to the `attributions` parameter. A code example is provided in the discussion that demonstrates how to create an iterable `Attribution` object and pass it to the `Explanation` object.",
        "Answer_gpt_summary":"solut error caus pass non iter valu attribut paramet solut pass iter valu list attribut paramet code exampl provid discuss demonstr creat iter attribut object pass explan object"
    },
    {
        "Question_title":"Import error when using remote Azure Automated Machine Learning model in Azure notebook",
        "Question_body":"<p>I have trained an automated machine learning model on an Azure ML compute cluster.<\/p>\n\n<p>I am trying to use that remote model in my Azure hosted Jupyter notebook. <\/p>\n\n<p>This is the code in the workbook that tries to load the remote model:<\/p>\n\n<pre><code>remote_run = AutoMLRun(experiment = experiment, run_id = '... Experiment id ...')\nremote_best_run, remote_fitted_model = remote_run.get_output()\n<\/code><\/pre>\n\n<p>This code fails with the following error:<\/p>\n\n<blockquote>\n  <p>ModuleNotFoundError                       Traceback (most recent call\n  last)  in \n        2 # remote_run.wait_for_completion(show_output = True)\n        3 import pandas as pd\n  ----> 4 remote_best_run, remote_fitted_model = remote_run.get_output()\n        5 #!pip list<\/p>\n  \n  <p>~\/anaconda3_501\/lib\/python3.6\/site-packages\/azureml\/train\/automl\/run.py\n  in get_output(self, iteration, metric)\n      406 \n      407         with open(model_local, \"rb\") as model_file:\n  --> 408             fitted_model = pickle.load(model_file)\n      409         return curr_run, fitted_model\n      410 <\/p>\n  \n  <p>ModuleNotFoundError: No module named 'pandas._libs.tslibs.timestamps'<\/p>\n<\/blockquote>\n\n<p>Presumably there is a version difference between what is installed on the Azure ML compute cluster vs what is installed in the kernel of the Jupyter notebook, or I have a package missing. <\/p>\n\n<p>How can I make this remote model work?<\/p>\n\n<p>For additional reference, I am following this tutorial: <a href=\"https:\/\/notebooks.azure.com\/NileshA\/projects\/GlobalAI\" rel=\"nofollow noreferrer\">https:\/\/notebooks.azure.com\/NileshA\/projects\/GlobalAI<\/a><\/p>\n\n<p><strong>Note 1<\/strong> I can also reproduce this error by running the following code in my jupyter notebook: <\/p>\n\n<pre><code>import pickle\n\nwith open('model.pkl', 'rb') as p_f:\n    data = pickle.load(p_f)\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1544869470737,
        "Question_favorite_count":1.0,
        "Question_last_edit_time":1544870483248,
        "Question_score":0.0,
        "Question_view_count":353.0,
        "Answer_body":"<p>I emailed the Auto ML helpdesk and they solved the problem.<\/p>\n\n<p>Quote from them: <\/p>\n\n<blockquote>\n  <p>We have a bug where the AutoML inferencing fails because the pandas\n  version is 0.22.0 which doesn\u2019t have some API support.<\/p>\n<\/blockquote>\n\n<p>I upgraded pandas on my hosted notebook to version 0.23.4, and after this the model unpickles and works successfully<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/53791461",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1545041889310,
        "Question_original_content":"import error remot azur autom machin learn model azur notebook train autom machin learn model comput cluster try us remot model azur host jupyt notebook code workbook tri load remot model remot run automlrun experi experi run experi remot best run remot fit model remot run output code fail follow error modulenotfounderror traceback recent remot run wait complet output true import panda remot best run remot fit model remot run output pip list anaconda lib python site packag train automl run output self iter metric open model local model file fit model pickl load model file return curr run fit model modulenotfounderror modul name panda lib tslib timestamp presum version differ instal comput cluster instal kernel jupyt notebook packag miss remot model work addit refer follow tutori http notebook azur com nilesha project globalai note reproduc error run follow code jupyt notebook import pickl open model pkl data pickl load",
        "Question_preprocessed_content":"import error remot azur autom machin learn model azur notebook train autom machin learn model comput cluster try us remot model azur host jupyt notebook code workbook tri load remot model code fail follow error modulenotfounderror traceback true import panda pip list iter metric return modulenotfounderror modul name presum version differ instal comput cluster instal kernel jupyt notebook packag miss remot model work addit refer follow tutori note reproduc error run follow code jupyt notebook",
        "Question_gpt_summary_original":"The user is encountering an import error when trying to use a remote Azure Automated Machine Learning model in their Azure hosted Jupyter notebook. The error message suggests a version difference or missing package between the Azure ML compute cluster and the Jupyter notebook kernel. The user is seeking a solution to make the remote model work.",
        "Question_gpt_summary":"user encount import error try us remot azur autom machin learn model azur host jupyt notebook error messag suggest version differ miss packag comput cluster jupyt notebook kernel user seek solut remot model work",
        "Answer_original_content":"email auto helpdesk solv problem quot bug automl inferenc fail panda version doesnt api support upgrad panda host notebook version model unpickl work successfulli",
        "Answer_preprocessed_content":"email auto helpdesk solv problem quot bug automl inferenc fail panda version doesnt api support upgrad panda host notebook version model unpickl work successfulli",
        "Answer_gpt_summary_original":"Solution: The user emailed the Auto ML helpdesk and they suggested upgrading pandas on the hosted notebook to version 0.23.4, which solved the problem.",
        "Answer_gpt_summary":"solut user email auto helpdesk suggest upgrad panda host notebook version solv problem"
    },
    {
        "Question_title":"Set Spark version for Sagemaker on Glue Dev Endpoint",
        "Question_body":"<p>To create my Glue scripts, I use development endpoints with Sagemaker notebooks that run the Pyspark (Sparkmagic) kernel.\nThe latest version of Glue (version 1.0) supports Spark 2.4. However, my Sagemaker notebook uses Spark version 2.2.1. \nThe function I want to test only exists as of Spark 2.3. \nIs there a way to solve this mismatch between the dev endpoint and the Glue job? Can I somehow set the Spark version of the notebook?<br>\nI couldn't find anything in the documentation.<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1566812956087,
        "Question_favorite_count":2.0,
        "Question_last_edit_time":null,
        "Question_score":2.0,
        "Question_view_count":642.0,
        "Answer_body":"<p>When you create a SageMaker notebook for the Glue dev endpoint, it launches a SageMaker notebook instance with a specific lifecycle configuration. This LC provides the configurations to create a connection between the SageMaker notebook and the development endpoint. Upon running cells from the PySpark kernel, the code is sent to the Livy server running in the development endpoint via REST APIs. <\/p>\n\n<p>Thus, the PySpark version that you see and on which the SageMaker notebook runs depends on the development endpoint and is not configurable from the SageMaker point of view.<\/p>\n\n<p>Since Glue is a managed service, root access is restricted for the development endpoint. Thus, you cannot update the spark version to a more later version. The feature of using Spark version 2.4 has been newly introduced in Glue and it seems that it has not yet been released for dev endpoint.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":1567004276928,
        "Answer_score":5.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/57655516",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1566968757932,
        "Question_original_content":"set spark version glue dev endpoint creat glue script us develop endpoint notebook run pyspark sparkmag kernel latest version glue version support spark notebook us spark version function want test exist spark wai solv mismatch dev endpoint glue job set spark version notebook couldn document",
        "Question_preprocessed_content":"set spark version glue dev endpoint creat glue script us develop endpoint notebook run pyspark kernel latest version glue support spark notebook us spark version function want test exist spark wai solv mismatch dev endpoint glue job set spark version notebook couldn document",
        "Question_gpt_summary_original":"The user is facing a challenge with the mismatch between the Spark version used in their Sagemaker notebook (2.2.1) and the latest version of Glue (1.0) which supports Spark 2.4. The function they want to test only exists in Spark 2.3. The user is looking for a way to set the Spark version of the notebook to solve this issue but couldn't find any information in the documentation.",
        "Question_gpt_summary":"user face challeng mismatch spark version notebook latest version glue support spark function want test exist spark user look wai set spark version notebook solv issu couldn inform document",
        "Answer_original_content":"creat notebook glue dev endpoint launch notebook instanc specif lifecycl configur provid configur creat connect notebook develop endpoint run cell pyspark kernel code sent livi server run develop endpoint rest api pyspark version notebook run depend develop endpoint configur point view glue manag servic root access restrict develop endpoint updat spark version later version featur spark version newli introduc glue releas dev endpoint",
        "Answer_preprocessed_content":"creat notebook glue dev endpoint launch notebook instanc specif lifecycl configur provid configur creat connect notebook develop endpoint run cell pyspark kernel code sent livi server run develop endpoint rest api pyspark version notebook run depend develop endpoint configur point view glue manag servic root access restrict develop endpoint updat spark version later version featur spark version newli introduc glue releas dev endpoint",
        "Answer_gpt_summary_original":"No solutions are provided in the discussion.",
        "Answer_gpt_summary":"solut provid discuss"
    },
    {
        "Question_title":"How to pass in the model name during init in Azure Machine Learning Service?",
        "Question_body":"<p>I am deploying 50 NLP models on Azure Container Instances via the Azure Machine Learning service. All 50 models are quite similar and have the same input\/output format with just the model implementation changing slightly. <\/p>\n\n<p>I want to write a generic score.py entry file and pass in the model name as a parameter. The interface method signature does not allow a parameter in the init() method of score.py, so I moved the model loading into the run method. I am assuming the init() method gets run once whereas Run(data) will get executed on every invocation, so this is possibly not ideal (the models are 1 gig in size)<\/p>\n\n<p>So how can I pass in some value to the init() method of my container to tell it what model to load? <\/p>\n\n<p>Here is my current, working code:<\/p>\n\n<pre><code>def init():\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    # extract model_name from raw_data omitted...\n    model = loadModel(model_name)\n\n    ...\n<\/code><\/pre>\n\n<p>but this is what I would like to do (which breaks the interface)<\/p>\n\n<pre><code>def init(model_name):\n    model = loadModel(model_name)\n\ndef loadModel(model_name):\n    model_path = Model.get_model_path(model_name)  \n    return fasttext.load_model(model_path)\n\ndef run(raw_data):\n    ...\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1572325608637,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":851.0,
        "Answer_body":"<p>If you're looking to use the same deployed container and switch models between requests; it's not the preferred design choice for Azure machine learning service, we need to specify the model name to load during build\/deploy.<\/p>\n\n<p>Ideally, each deployed web-service endpoint should allow inference of one model only; with the model name defined before the container the image starts building\/deploying. <\/p>\n\n<p>It is mandatory that the entry script has both <code>init()<\/code> and <code>run(raw_data)<\/code> with those <strong>exact<\/strong> signatures. <\/p>\n\n<p>At the moment, we can't change the signature of <code>init()<\/code> method to take a parameter like in <code>init(model_name)<\/code>.  <\/p>\n\n<p>The only dynamic user input you'd ever get to pass into this web-service is via <code>run(raw_data)<\/code> method. As you have tried, given the size of your model passing it via run is not feasible. <\/p>\n\n<p><code>init()<\/code> is run first and only <strong>once<\/strong> after your web-service deploy. Even if <code>init()<\/code> took the <code>model_name<\/code> parameter, there isn't a straight forward way to call this method directly and pass your desired model name.<\/p>\n\n<hr>\n\n<p>But, one possible solution is: <\/p>\n\n<p>You can create params file like below and store the file in azure blob storage.<\/p>\n\n<p>Example runtime parameters generation script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>import pickle\n\nparams = {'model_name': 'YOUR_MODEL_NAME_TO_USE'}\n\nwith open('runtime_params.pkl', 'wb') as file:\n    pickle.dump(params, file)\n\n<\/code><\/pre>\n\n<p>You'll need to use <a href=\"https:\/\/github.com\/Azure\/azure-storage-python\" rel=\"nofollow noreferrer\">Azure Storage Python SDK<\/a> to write code that can read from your blob storage account. This also mentioned in the official docs <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/service\/how-to-deploy-and-where#prepare-to-deploy\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>Then you can access this from <code>init()<\/code> function in your score script. <\/p>\n\n<p>Example <code>score.py<\/code> script:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>from azure.storage.blob import BlockBlobService\nimport pickle\n\ndef init():\n\n  global model\n\n  block_blob_service = BlockBlobService(connection_string='your_connection_string')\n\n  blob_item = block_blob_service.get_blob_to_bytes('your-container-name','runtime_params.pkl')\n\n  params = pickle.load(blob_item.content)\n\n  model = loadModel(params['model_name'])\n<\/code><\/pre>\n\n<p>You can store connection strings in Azure KeyVault for secure access. Azure ML Workspaces comes with built-in KeyVault integration. More info <a href=\"https:\/\/docs.microsoft.com\/en-us\/python\/api\/azureml-core\/azureml.core.keyvault.keyvault?view=azure-ml-py#get-secret-name-\" rel=\"nofollow noreferrer\">here<\/a>.<\/p>\n\n<p>With this approach, you're abstracting runtime params config to another cloud location rather than the container itself. So you wouldn't need to re-build the image or deploy the web-service again. Simply restarting the container will work.<\/p>\n\n<hr>\n\n<p>If you're looking to simply re-use <code>score.py<\/code> (not changing code) for <strong>multiple model deployments in multiple containers<\/strong> then here's another possible solution.<\/p>\n\n<p>You can define your model name to use in web-service in a text file and read it in score.py. You'll need to pass this text file as a dependency when setting up the image config.<\/p>\n\n<p>This would, however, need multiple params files for each container deployment.<\/p>\n\n<p>Passing 'runtime_params.pkl' in <code>dependencies<\/code> to your image config (More detail example <a href=\"https:\/\/github.com\/rithinch\/heartfulness-similar-content-service\/blob\/master\/experiments\/notebooks\/Deploy%20Model%20-%20Azure.ipynb\" rel=\"nofollow noreferrer\">here<\/a>):<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>image_config = ContainerImage.image_configuration(execution_script=\"score.py\", \n                                                  runtime=\"python\", \n                                                  conda_file=\"myenv.yml\",\n                                                  dependencies=[\"runtime_params.pkl\"],\n                                                  docker_file=\"Dockerfile\")\n<\/code><\/pre>\n\n<p>Reading this in your score.py <code>init()<\/code> function:<\/p>\n\n<pre class=\"lang-py prettyprint-override\"><code>def init():\n\n  global model\n\n  with open('runtime_params.pkl', 'rb') as file:\n    params = pickle.load(file)\n\n  model = loadModel(params['model_name'])\n\n<\/code><\/pre>\n\n<p>Since your creating a new image config with this approach, you'll need to build the image and re-deploy the service.<\/p>",
        "Answer_comment_count":1.0,
        "Answer_last_edit_time":1572389991296,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/58601697",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1572381894847,
        "Question_original_content":"pass model init servic deploi nlp model azur contain instanc servic model similar input output format model implement chang slightli want write gener score entri file pass model paramet interfac method signatur allow paramet init method score move model load run method assum init method get run run data execut invoc possibl ideal model gig size pass valu init method contain tell model load current work code def init def loadmodel model model path model model path model return fasttext load model model path def run raw data extract model raw data omit model loadmodel model like break interfac def init model model loadmodel model def loadmodel model model path model model path model return fasttext load model model path def run raw data",
        "Question_preprocessed_content":"pass model init servic deploi nlp model azur contain instanc servic model similar format model implement chang slightli want write gener entri file pass model paramet interfac method signatur allow paramet init method move model load run method assum init method get run run execut invoc possibl ideal pass valu init method contain tell model load current work code like",
        "Question_gpt_summary_original":"The user is facing a challenge in passing the model name as a parameter to the init() method in Azure Machine Learning service. The interface method signature does not allow a parameter in the init() method of score.py, so the user moved the model loading into the run method. However, this is not ideal as the models are 1 gig in size and the run method gets executed on every invocation. The user is seeking a solution to pass in some value to the init() method of the container to tell it what model to load.",
        "Question_gpt_summary":"user face challeng pass model paramet init method servic interfac method signatur allow paramet init method score user move model load run method ideal model gig size run method get execut invoc user seek solut pass valu init method contain tell model load",
        "Answer_original_content":"look us deploi contain switch model request prefer design choic servic need specifi model load build deploi ideal deploi web servic endpoint allow infer model model defin contain imag start build deploi mandatori entri script init run raw data exact signatur moment chang signatur init method paramet like init model dynam user input pass web servic run raw data method tri given size model pass run feasibl init run web servic deploi init took model paramet isn straight forward wai method directli pass desir model possibl solut creat param file like store file azur blob storag exampl runtim paramet gener script import pickl param model model us open runtim param pkl file pickl dump param file need us azur storag python sdk write code read blob storag account mention offici doc access init function score script exampl score script azur storag blob import blockblobservic import pickl def init global model block blob servic blockblobservic connect string connect string blob item block blob servic blob byte contain runtim param pkl param pickl load blob item content model loadmodel param model store connect string azur keyvault secur access workspac come built keyvault integr info approach abstract runtim param config cloud locat contain wouldn need build imag deploi web servic simpli restart contain work look simpli us score chang code multipl model deploy multipl contain possibl solut defin model us web servic text file read score need pass text file depend set imag config need multipl param file contain deploy pass runtim param pkl depend imag config exampl imag config containerimag imag configur execut script score runtim python conda file myenv yml depend runtim param pkl docker file dockerfil read score init function def init global model open runtim param pkl file param pickl load file model loadmodel param model creat new imag config approach need build imag deploi servic",
        "Answer_preprocessed_content":"look us deploi contain switch model request prefer design choic servic need specifi model load ideal deploi endpoint allow infer model model defin contain imag start mandatori entri script exact signatur moment chang signatur method paramet like dynam user input pass method tri given size model pass run feasibl run deploi took paramet isn straight forward wai method directli pass desir model possibl solut creat param file like store file azur blob storag exampl runtim paramet gener script need us azur storag python sdk write code read blob storag account mention offici doc access function score script exampl script store connect string azur keyvault secur access workspac come keyvault integr info approach abstract runtim param config cloud locat contain wouldn need imag deploi simpli restart contain work look simpli multipl model deploy multipl contain possibl solut defin model us text file read need pass text file depend set imag config need multipl param file contain deploy pass imag config read function creat new imag config approach need build imag servic",
        "Answer_gpt_summary_original":"One possible solution to the challenge is to create a params file and store it in Azure blob storage. The runtime parameters generation script can be used to create the params file. The user can access this from the init() function in the score script. Another possible solution is to define the model name to use in a text file and read it in score.py. The user will need to pass this text file as a dependency when setting up the image config. This would need multiple params files for each container deployment.",
        "Answer_gpt_summary":"possibl solut challeng creat param file store azur blob storag runtim paramet gener script creat param file user access init function score script possibl solut defin model us text file read score user need pass text file depend set imag config need multipl param file contain deploy"
    },
    {
        "Question_title":"Azure: plot without labels",
        "Question_body":"<p>Suppose I have a dataframe (myDataframe) with two column of values (third and fourth). I want to plot them in a bi-dimensional graph. If I do it in R it works, but it returns me a graph without labels when I run the script from Azure Machine Learning. Someone with ideas?<\/p>\n\n<pre><code>...\nplot(myDataframe[,3],myDataframe[,4], \n       main=\"my title\",\n       xlab= \"x\"\n       ylab= \"y\",\n       col= \"blue\", pch = 19, cex = 0.1, lty = \"solid\", lwd = 2)\n\n# lines(x,y=x, col=\"yellow\")\n\n# add LABELS\ntext(DF_relativo[,A], DF_relativo[,B], \n       labels=DF_relativo$names, cex= 0.7, pos=2)\n...\n<\/code><\/pre>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1453470518060,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":0.0,
        "Question_view_count":91.0,
        "Answer_body":"<p>using ggplot2() in AzureML is bit different. It can use to have plots with labels. Here's the <a href=\"https:\/\/gallery.cortanaintelligence.com\/Experiment\/b1c26728eb6c4e4d80dddceae992d653\" rel=\"nofollow\">Cortana Intelligence gallery example<\/a> for the particular task.  <\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":-1.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/34948242",
        "Tool":"Azure Machine Learning",
        "Platform":"Stack Overflow",
        "Question_closed_time":1466402236047,
        "Question_original_content":"azur plot label suppos datafram mydatafram column valu fourth want plot dimension graph work return graph label run script idea plot mydatafram mydatafram main titl xlab ylab col blue pch cex lty solid lwd line col yellow add label text relativo relativo label relativo name cex po",
        "Question_preprocessed_content":"azur plot label suppos datafram column valu want plot graph work return graph label run script idea",
        "Question_gpt_summary_original":"The user is facing a challenge in plotting a bi-dimensional graph in Azure Machine Learning without labels, even though it works in R. They are seeking ideas on how to add labels to the graph.",
        "Question_gpt_summary":"user face challeng plot dimension graph label work seek idea add label graph",
        "Answer_original_content":"ggplot bit differ us plot label cortana intellig galleri exampl particular task",
        "Answer_preprocessed_content":"ggplot bit differ us plot label cortana intellig galleri exampl particular task",
        "Answer_gpt_summary_original":"Solution: The discussion suggests using ggplot2() in AzureML to plot the bi-dimensional graph with labels. A Cortana Intelligence gallery example is also provided for reference.",
        "Answer_gpt_summary":"solut discuss suggest ggplot plot dimension graph label cortana intellig galleri exampl provid refer"
    },
    {
        "Question_title":"How does one make AWS SageMaker available to people outside their organization?",
        "Question_body":"<p>I am setting up SageMaker for a group outside my organization, and wondering how I provide access to SageMaker Studio (and any dependent AWS resources SageMaker would use) to these users.\nThe idea is to have then use SageMaker studio to do train and test models.<\/p>\n<p>Ideally, it would be great if I simply send them a link where they sign in, and are then granted access to SageMaker hosted from my AWS account.<\/p>\n<p>I believe to grant temporary access I need to attach a <strong>role<\/strong> to a group of <strong>users<\/strong>, and also attach a <strong>policy<\/strong> to this group, them finally associate this role with SageMaker. But how do I then create a <strong>link<\/strong> to SageMaker so these users can sign in?<\/p>\n<p>The other option would be to set this up using cross-account access, since those I want to give access to also have their own AWS account. But again, how does one generate a link to direct these users to the SageMaker on my AWS account?<\/p>\n<p>There appears to be 2 options, as per <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/dg\/gs-studio-onboard.html\" rel=\"nofollow noreferrer\">SageMaker Onboarding<\/a>:<\/p>\n<ol>\n<li><strong>AWS SSO authentication<\/strong><\/li>\n<\/ol>\n<ul>\n<li>access to Studio via unique sign-in URL that directly opens Studio<\/li>\n<li>sign in with their SSO credentials<\/li>\n<li>organizations manages members in AWS SSO instead of Studio<\/li>\n<li>can assign multiple members access to Studio at the same time<\/li>\n<\/ul>\n<ol start=\"2\">\n<li><strong>IAM authentication<\/strong><\/li>\n<\/ol>\n<ul>\n<li>sign in through the SageMaker console<\/li>\n<li>must add and manage members manually one at time using the Studio Control Panel<\/li>\n<\/ul>\n<p>I don't understand the 2nd approach, since wouldn't this mean users would have to sign-in as root to the console anyway and then have full access. I could edit the policy attached to IAM users, but this begs another question:<\/p>\n<p>...does SageMaker come pre-baked with policies that include its dependent AWS services? For example, SageMaker will use S3 for storage and EC2 for processing; do I need to set these individually in the policy, or can I simply use a SageMaker policy, which will include all those dependencies by default?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1625496004123,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":1625502032280,
        "Question_score":1.0,
        "Question_view_count":1359.0,
        "Answer_body":"<p>First of all, you can generate &quot;disposable&quot; (called pre-signed) URLs which can be used for accessing SageMaker Studio User Profiles without any AWS credentials. These URls can be valid for max 5 minutes and can be generated with a single AWS API call.<\/p>\n<p>One approach to provide Studio access to your users is to set up a service which can authenticate your Studio users and then calls the <a href=\"https:\/\/docs.aws.amazon.com\/sagemaker\/latest\/APIReference\/API_CreatePresignedDomainUrl.html\" rel=\"nofollow noreferrer\">CreatePresignedDomainUrl SageMaker API<\/a> method to sends back the generated pre-signed URL to the user.<\/p>\n<p>Alternatively, you can use AWS SSO as well, which can do most of the heavy lifting for you, especially if you'd like to integrate with a single sign-on service. AWS SSO integrates with SageMaker Studio and you can assign Studio user profiles to your onboarded users. Your users then can go through your single sign-on service and can launch the Studio without logging into the AWS Console.<\/p>\n<p>An another approach is to use IAM Federation where you basically provide access to the AWS API and\/or to the Console to your users which authenticated by an (external) identity provider. Federated users can assume specific roles to operate with the AWS API or the Management Console. For accessing SageMaker Studio, users just need to have the CreatePresignedDomainUrl access policy which allows them to create the pre-signed URL by themselves. If you want to isolate your SageMaker user profiles and ensure each federated user can access just those user profiles which are assigned to them, please see the following <a href=\"https:\/\/aws.amazon.com\/blogs\/machine-learning\/configuring-amazon-sagemaker-studio-for-teams-and-groups-with-complete-resource-isolation\/\" rel=\"nofollow noreferrer\">blog post<\/a> for more information.<\/p>\n<p>And finally, please note that, once the user has logged in to Studio, the Execution Role configured for the specific user profile will determine what the Studio user can access and is able to do (e.g. spinning up SageMaker training jobs, deploying models, accessing S3, etc). Thus, you don't need to set up these policies for your IAM users or roles used by the federated users.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":3.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/68258003",
        "Tool":"Amazon SageMaker",
        "Platform":"Stack Overflow",
        "Question_closed_time":1626467994683,
        "Question_original_content":"avail peopl outsid organ set group outsid organ wonder provid access studio depend aw resourc us user idea us studio train test model ideal great simpli send link sign grant access host aw account believ grant temporari access need attach role group user attach polici group final associ role creat link user sign option set cross account access want access aw account gener link direct user aw account appear option onboard aw sso authent access studio uniqu sign url directli open studio sign sso credenti organ manag member aw sso instead studio assign multipl member access studio time iam authent sign consol add manag member manual time studio control panel understand approach wouldn mean user sign root consol access edit polici attach iam user beg question come pre bake polici includ depend aw servic exampl us storag process need set individu polici simpli us polici includ depend default",
        "Question_preprocessed_content":"avail peopl outsid organ set group outsid organ wonder provid access studio user idea us studio train test model ideal great simpli send link sign grant access host aw account believ grant temporari access need attach role group user attach polici group final associ role creat link user sign option set access want access aw account gener link direct user aw account appear option onboard aw sso authent access studio uniqu url directli open studio sign sso credenti organ manag member aw sso instead studio assign multipl member access studio time iam authent sign consol add manag member manual time studio control panel understand approach wouldn mean user root consol access edit polici attach iam user beg question come polici includ depend aw servic exampl us storag process need set individu polici simpli us polici includ depend default",
        "Question_gpt_summary_original":"The user is facing challenges in providing access to SageMaker Studio and its dependent AWS resources to a group outside their organization. They are considering granting temporary access by attaching a role and policy to a group of users and associating it with SageMaker. The user is also exploring two options for generating a link to direct users to SageMaker, namely AWS SSO authentication and IAM authentication. The user is unsure about the second approach and whether SageMaker comes with pre-baked policies that include its dependent AWS services.",
        "Question_gpt_summary":"user face challeng provid access studio depend aw resourc group outsid organ consid grant temporari access attach role polici group user associ user explor option gener link direct user aw sso authent iam authent user unsur second approach come pre bake polici includ depend aw servic",
        "Answer_original_content":"gener dispos call pre sign url access studio user profil aw credenti url valid max minut gener singl aw api approach provid studio access user set servic authent studio user call createpresigneddomainurl api method send gener pre sign url user altern us aw sso heavi lift especi like integr singl sign servic aw sso integr studio assign studio user profil onboard user user singl sign servic launch studio log aw consol approach us iam feder basic provid access aw api consol user authent extern ident provid feder user assum specif role oper aw api manag consol access studio user need createpresigneddomainurl access polici allow creat pre sign url want isol user profil ensur feder user access user profil assign follow blog post inform final note user log studio execut role configur specif user profil determin studio user access abl spin train job deploi model access need set polici iam user role feder user",
        "Answer_preprocessed_content":"gener dispos url access studio user profil aw credenti url valid max minut gener singl aw api approach provid studio access user set servic authent studio user call createpresigneddomainurl api method send gener url user altern us aw sso heavi lift especi like integr singl servic aw sso integr studio assign studio user profil onboard user user singl servic launch studio log aw consol approach us iam feder basic provid access aw api consol user authent ident provid feder user assum specif role oper aw api manag consol access studio user need createpresigneddomainurl access polici allow creat url want isol user profil ensur feder user access user profil assign follow blog post inform final note user log studio execut role configur specif user profil determin studio user access abl need set polici iam user role feder user",
        "Answer_gpt_summary_original":"Possible solutions mentioned in the discussion are:\n\n1. Generate \"disposable\" pre-signed URLs that can be used for accessing SageMaker Studio User Profiles without any AWS credentials. These URLs can be valid for a maximum of 5 minutes and can be generated with a single AWS API call.\n\n2. Set up a service that can authenticate Studio users and then call the CreatePresignedDomainUrl SageMaker API method to send back the generated pre-signed URL to the user.\n\n3. Use AWS SSO, which integrates with SageMaker Studio and allows assigning Studio user profiles to onboarded users. Users can launch the Studio without logging into the AWS Console.\n\n4. Use IAM Federation to provide access to the AWS API and\/or Console to users authenticated by an external",
        "Answer_gpt_summary":"possibl solut mention discuss gener dispos pre sign url access studio user profil aw credenti url valid maximum minut gener singl aw api set servic authent studio user createpresigneddomainurl api method send gener pre sign url user us aw sso integr studio allow assign studio user profil onboard user user launch studio log aw consol us iam feder provid access aw api consol user authent extern"
    },
    {
        "Question_title":"What would stop credentials from validation on a ClearML server?",
        "Question_body":"<p>I've set up a ClearML server in GCP using the sub-domain approach. I can access all three domains (<code>https:\/\/app.clearml.mydomain.com<\/code>, <code>https:\/\/api.clearml.mydomain.com<\/code> and <code>https:\/\/files.clearml.mydomain.com<\/code>) in a browser and see what I think is the correct response, but when connecting with the python SDK via <code>clearml-init<\/code> I get the following error:<\/p>\n<pre><code>clearml.backend_api.session.session.LoginError: Failed getting token (error 400 from https:\/\/api.clearml.mydomain.com): Bad Request\n<\/code><\/pre>\n<p>Are there any likely causes of this error?<\/p>",
        "Question_answer_count":1,
        "Question_comment_count":0,
        "Question_created_time":1639763350487,
        "Question_favorite_count":0.0,
        "Question_last_edit_time":null,
        "Question_score":1.0,
        "Question_view_count":194.0,
        "Answer_body":"<p>Following the discussion <a href=\"https:\/\/github.com\/allegroai\/clearml\/issues\/517\" rel=\"nofollow noreferrer\">here<\/a>, it seemed that the load balancer being used was blocking <code>GET<\/code> requests with a payload which are used by ClearML. A <a href=\"https:\/\/github.com\/allegroai\/clearml\/pull\/521\" rel=\"nofollow noreferrer\">fix<\/a> is being worked on to allow the method to be changed to a <code>POST<\/code> request via an environment variable.<\/p>",
        "Answer_comment_count":0.0,
        "Answer_last_edit_time":null,
        "Answer_score":0.0,
        "Question_link":"https:\/\/stackoverflow.com\/questions\/70397010",
        "Tool":"ClearML",
        "Platform":"Stack Overflow",
        "Question_closed_time":1640162037740,
        "Question_original_content":"stop credenti valid server set server gcp sub domain approach access domain http app mydomain com http api mydomain com http file mydomain com browser think correct respons connect python sdk init follow error backend api session session loginerror fail get token error http api mydomain com bad request like caus error",
        "Question_preprocessed_content":"stop credenti valid server set server gcp approach access domain browser think correct respons connect python sdk follow error like caus error",
        "Question_gpt_summary_original":"The user is encountering an error while trying to connect to a ClearML server using the python SDK via clearml-init. The error message indicates that the credentials are not being validated due to a Bad Request error. The user is seeking assistance in identifying the likely causes of this error.",
        "Question_gpt_summary":"user encount error try connect server python sdk init error messag indic credenti valid bad request error user seek assist identifi like caus error",
        "Answer_original_content":"follow discuss load balanc block request payload fix work allow method chang post request environ variabl",
        "Answer_preprocessed_content":"follow discuss load balanc block request payload fix work allow method chang request environ variabl",
        "Answer_gpt_summary_original":"Solution: A fix is being worked on to allow the method to be changed to a POST request via an environment variable.",
        "Answer_gpt_summary":"solut fix work allow method chang post request environ variabl"
    }
]