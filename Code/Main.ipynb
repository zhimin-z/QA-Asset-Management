{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_stack_overflow = os.path.join(path_dataset, 'Stack Overflow')\n",
    "path_tool_specific = os.path.join(path_dataset, 'Tool-specific')\n",
    "path_labeling = os.path.join(path_dataset, 'Labeling')  \n",
    "\n",
    "if not os.path.exists(path_dataset):\n",
    "    os.makedirs(path_dataset)\n",
    "\n",
    "if not os.path.isdir(path_stack_overflow):\n",
    "    os.mkdir(path_stack_overflow)\n",
    "\n",
    "if not os.path.isdir(path_tool_specific):\n",
    "    os.mkdir(path_tool_specific)\n",
    "\n",
    "if not os.path.isdir(path_labeling):\n",
    "    os.mkdir(path_labeling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_tags = {\n",
    "    'Amazon SageMaker': {'amazon-sagemaker', 'amazon-sagemaker-experiments', 'amazon-sagemaker-studio'},\n",
    "    'Azure Machine Learning': {'azure-machine-learning-service', 'azure-machine-learning-studio', 'azure-machine-learning-workbench'},\n",
    "    'ClearML': {'clearml'},\n",
    "    'Comet': {'comet-ml'},\n",
    "    'DVC': {'dvc'},\n",
    "    'Kedro': {'kedro'},\n",
    "    'MLflow': {'mlflow'},\n",
    "    'MLRun': {'mlrun'},\n",
    "    'Neptune': {'neptune'},\n",
    "    'Optuna': {'optuna'},\n",
    "    'Sacred': {'python-sacred'},\n",
    "    'Vertex AI': {'google-cloud-vertex-ai'},\n",
    "    'Weights & Biases': {'wandb'}\n",
    "}\n",
    "\n",
    "tools_keywords = {\n",
    "    'Aim': ['aim'],\n",
    "    'Amazon SageMaker': ['sage maker', 'sagemaker'],\n",
    "    'Azure Machine Learning': ['azure machine learning', 'azure ml', 'azureml'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'cnvrg.io': ['cnvrg'],\n",
    "    'Codalab': ['codalab'],\n",
    "    'Comet': ['comet'],\n",
    "    'Determined': ['determined'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild ai', 'guildai'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow'],\n",
    "    'MLRun': ['mlrun'],\n",
    "    'ModelDB': ['modeldb'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Valohai': ['valohai'],\n",
    "    'Vertex AI': ['vertex ai', 'vertexai'],\n",
    "    'Weights & Biases': ['weights and biases', 'wandb', 'weights & biases', 'weights&biases', 'w & b', 'w&b']\n",
    "}\n",
    "\n",
    "tool_no_accepted_answer = {\n",
    "    'Domino', \n",
    "    'DVC', \n",
    "    'Guild AI\"', \n",
    "    'MLflow', \n",
    "    'Polyaxon', \n",
    "    'SigOpt'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6999"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import google.auth\n",
    "import pandas_gbq\n",
    "\n",
    "credentials, _ = google.auth.default()\n",
    "\n",
    "pandas_gbq.context.credentials = credentials\n",
    "pandas_gbq.context.project = 'stack-overflow-dataset-330612'\n",
    "\n",
    "with open(os.path.join(path_stack_overflow, 'bigquery.sql'), 'r') as sql_file:\n",
    "    sql = sql_file.read()\n",
    "    df = pandas_gbq.read_gbq(sql)\n",
    "    df['Question_tags'] = df['Question_tags'].str.split('|')\n",
    "    df['Question_favorite_count'] = df['Question_favorite_count'].fillna(0)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tag collection\n",
    "tags = set()\n",
    "for key, value in tool_tags.items():\n",
    "    tags = tags.union(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tags\n",
    "df['Question_valid_tags'] = [[] for _ in range(len(df))]\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'Question_valid_tags'] = list(tags.intersection(set(row['Question_tags'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts with at least 1 tags has 5308 in total.\n",
      "Posts with at least 2 tags has 220 in total.\n",
      "Posts with at least 3 tags has 18 in total.\n"
     ]
    }
   ],
   "source": [
    "# count post number with different tags\n",
    "arity = 0\n",
    "while True:\n",
    "    post_number = df[df['Question_valid_tags'].map(len) > arity].shape[0]\n",
    "    if post_number < 1:\n",
    "        break\n",
    "    arity = arity + 1\n",
    "    print(f'Posts with at least {arity} tags has {post_number} in total.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhimi\\AppData\\Local\\Temp\\ipykernel_19224\\1002186168.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid['Question_link'] = df_valid['Question_id'].apply(lambda x: f'https://stackoverflow.com/questions/{x}')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5308"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclude Stack Overflow posts with unrelated tags\n",
    "df_valid = df[df['Question_valid_tags'].map(len) > 0]\n",
    "df_valid['Question_link'] = df_valid['Question_id'].apply(lambda x: f'https://stackoverflow.com/questions/{x}')\n",
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5175"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclude Stack Overflow posts with negative upvote count\n",
    "df_qualified = df_valid[df_valid['Question_score_count'] > -1]\n",
    "len(df_qualified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map from tag to tool\n",
    "tag2tool = dict()\n",
    "for key, value in tool_tags.items():\n",
    "    for elem in value:\n",
    "        tag2tool.setdefault(elem, key)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhimi\\AppData\\Local\\Temp\\ipykernel_19224\\971631455.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_qualified.at[index, 'Tools'] = sorted(list(tags))\n"
     ]
    }
   ],
   "source": [
    "# extract Stack Overflow post collection with multiple tags based on the tool map\n",
    "for index, row in df_qualified.iterrows():\n",
    "    tags = set()\n",
    "    for tag in row['Question_valid_tags']:\n",
    "        tags.add(tag2tool[tag])\n",
    "    df_qualified.at[index, 'Tools'] = list(tags)\n",
    "\n",
    "del df_qualified['Question_valid_tags']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhimi\\AppData\\Local\\Temp\\ipykernel_19224\\3788368826.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_qualified['Challenge_self_resolution'] = df_qualified['Poster_id'] == df_qualified['Answerer_id']\n"
     ]
    }
   ],
   "source": [
    "df_qualified['Challenge_self_resolution'] = df_qualified['Poster_id'] == df_qualified['Answerer_id']\n",
    "\n",
    "del df_qualified['Poster_id']\n",
    "del df_qualified['Answerer_id']\n",
    "del df_qualified['Question_id']\n",
    "del df_qualified['Question_tags']\n",
    "\n",
    "df_qualified.to_json(os.path.join(\n",
    "    path_stack_overflow, 'questions.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape the posts from the tool-specific discussion fora\n",
    "\n",
    "import requests\n",
    "\n",
    "def scrape_post(base_url, page_suffix, file_name):\n",
    "    page = -1\n",
    "    posts = pd.DataFrame()\n",
    "    \n",
    "    post_url_lst = set()\n",
    "\n",
    "    while True:\n",
    "        page = page + 1\n",
    "        page_url = base_url + page_suffix + str(page)\n",
    "        topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "        for topic in topic_list['topics']:\n",
    "            post_url = base_url + 't/' + \\\n",
    "                topic['slug'] + '/' + str(topic['id'])\n",
    "                \n",
    "            if post_url in post_url_lst:\n",
    "                continue\n",
    "            \n",
    "            post_url_lst.add(post_url)\n",
    "\n",
    "            post = {}\n",
    "            post['Question_title'] = topic['title']\n",
    "            post['Question_link'] = post_url\n",
    "            post['Question_created_time'] = topic['created_at']\n",
    "            post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "            post['Question_score_count'] = topic['like_count']\n",
    "            post['Question_view_count'] = topic['views']\n",
    "            \n",
    "            comments = requests.get(\n",
    "                post_url + '.json').json()['post_stream']['posts']\n",
    "            post['Question_body'] = comments[0]['cooked']\n",
    "            post['Question_closed_time'] = np.nan\n",
    "            post['Answer_body'] = np.nan\n",
    "            post[\"Question_self_closed\"] = np.nan\n",
    "            \n",
    "            if topic['has_accepted_answer']:\n",
    "                for comment in comments[1:]:\n",
    "                    if comment['accepted_answer']:\n",
    "                        post['Question_closed_time'] = comment['created_at']\n",
    "                        post['Answer_body'] = comment['cooked']\n",
    "                        post['Question_self_closed'] = comment['username'] == comments[0]['username']\n",
    "                        break\n",
    "            \n",
    "            post = pd.DataFrame([post])\n",
    "            posts = pd.concat([posts, post], ignore_index=True)\n",
    "            time.sleep(5)\n",
    "\n",
    "        if 'more_topics_url' not in topic_list.keys():\n",
    "            break\n",
    "    \n",
    "    posts.to_json(os.path.join(path_tool_specific, file_name + '.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from DVC\n",
    "base_url = 'https://discuss.dvc.org/'\n",
    "page_suffix = 'c/questions/9.json?page='\n",
    "file_name = 'DVC'\n",
    "scrape_post(base_url, page_suffix, file_name)\n",
    "\n",
    "# scrape posts from Guild AI\n",
    "base_url = 'https://my.guild.ai/'\n",
    "page_suffix = 'c/troubleshooting/6.json?page='\n",
    "file_name = 'Guild AI'\n",
    "scrape_post(base_url, page_suffix, file_name)\n",
    "\n",
    "# scrape posts from SigOpt\n",
    "base_url = 'https://community.sigopt.com/'\n",
    "page_suffix = 'c/general-discussion/9.json?page='\n",
    "file_name = 'SigOpt'\n",
    "scrape_post(base_url, page_suffix, file_name)\n",
    "\n",
    "# scrape posts from Weights & Biases\n",
    "base_url = 'https://community.wandb.ai/'\n",
    "page_suffix = 'c/w-b-support/36.json?page='\n",
    "file_name = 'Weights & Biases'\n",
    "scrape_post(base_url, page_suffix, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "path_code = 'Scrape'\n",
    "\n",
    "subprocess.run(['python', os.path.join(path_code, 'Amazon SageMaker.py')])\n",
    "subprocess.run(['python', os.path.join(path_code, 'Azure Machine Learning.py')])\n",
    "subprocess.run(['python', os.path.join(path_code, 'Domino.py')])\n",
    "subprocess.run(['python', os.path.join(path_code, 'MLflow.py')])\n",
    "subprocess.run(['python', os.path.join(path_code, 'Polyaxon.py')])\n",
    "subprocess.run(['python', os.path.join(path_code, 'Vertex AI.py')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4982 4981\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# exclude tool-specific posts with negative upvote count\n",
    "df_questions_ts = pd.DataFrame()\n",
    "total_post = 0\n",
    "\n",
    "for file_name in glob.glob(os.path.join(path_tool_specific, '*.json')):\n",
    "    posts = pd.read_json(file_name)\n",
    "    total_post += len(posts)    \n",
    "    if 'Question_score_count' in posts.columns:\n",
    "        posts = posts[posts['Question_score_count'] > -1]\n",
    "    posts['Tools'] = [list(os.path.split(file_name)[1].split('.')[0]) for _ in range(len(posts))]\n",
    "    df_questions_ts = pd.concat([df_questions_ts, posts], ignore_index=True)\n",
    "    \n",
    "print(total_post, df_questions_ts.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create question dataset\n",
    "\n",
    "df_question_so = pd.read_json(os.path.join(path_stack_overflow, 'questions.json'))\n",
    "\n",
    "df_question_so['Platform'] = 'Stack Overflow'\n",
    "df_questions_ts['Platform'] = 'Tool-specific'\n",
    "\n",
    "df_questions = pd.concat([df_question_so, df_questions_ts], ignore_index=True)\n",
    "df_questions['Question_body'] = df_questions['Question_body'].fillna('')\n",
    "\n",
    "df_questions = df_questions.reindex(sorted(df_questions.columns), axis=1)\n",
    "df_questions.to_json(os.path.join(path_labeling, 'questions.json'), indent=4, orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
