{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_so = os.path.join(path_dataset, 'Stack Overflow')\n",
    "path_ts = os.path.join(path_dataset, 'Tool-specific Others')\n",
    "path_labeling = os.path.join(path_dataset, 'Labeling')  \n",
    "\n",
    "path_so_raw = os.path.join(path_so, 'Raw')\n",
    "path_ts_raw = os.path.join(path_ts, 'Raw')\n",
    "path_so_filtered = os.path.join(path_so, 'Filtered')\n",
    "path_ts_filtered = os.path.join(path_ts, 'Filtered')\n",
    "\n",
    "if not os.path.exists(path_dataset):\n",
    "    os.makedirs(path_dataset)\n",
    "\n",
    "if not os.path.isdir(path_so):\n",
    "    os.mkdir(path_so)\n",
    "\n",
    "if not os.path.isdir(path_ts):\n",
    "    os.mkdir(path_ts)\n",
    "\n",
    "if not os.path.isdir(path_labeling):\n",
    "    os.mkdir(path_labeling)\n",
    "\n",
    "if not os.path.isdir(path_so_raw):\n",
    "    os.mkdir(path_so_raw)\n",
    "\n",
    "if not os.path.isdir(path_ts_raw):\n",
    "    os.mkdir(path_ts_raw)\n",
    "\n",
    "if not os.path.isdir(path_so_filtered):\n",
    "    os.mkdir(path_so_filtered)\n",
    "\n",
    "if not os.path.isdir(path_ts_filtered):\n",
    "    os.mkdir(path_ts_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool2tag = {\n",
    "    'Amazon SageMaker': {'amazon-sagemaker', 'amazon-sagemaker-experiments', 'amazon-sagemaker-studio'},\n",
    "    'Azure Machine Learning': {'azure-machine-learning-service', 'azure-machine-learning-studio', 'azure-machine-learning-workbench'},\n",
    "    'ClearML': {'clearml'},\n",
    "    'Comet': {'comet-ml'},\n",
    "    'DVC': {'dvc'},\n",
    "    'Kedro': {'kedro'},\n",
    "    'MLflow': {'mlflow'},\n",
    "    'MLRun': {'mlrun'},\n",
    "    'Neptune': {'neptune'},\n",
    "    'Sacred': {'python-sacred'},\n",
    "    'Vertex AI': {'google-cloud-vertex-ai'},\n",
    "    'Weights & Biases': {'wandb'}\n",
    "}\n",
    "\n",
    "tools_keywords = {\n",
    "    'Amazon SageMaker': ['amazon sagemaker', 'aws sagemaker', 'sagemaker'],\n",
    "    'Azure Machine Learning': ['microsoft azure machine learning', 'azure machine learning', 'microsoft azure ml', 'microsoft azureml', 'azure ml', 'azureml'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'Comet': ['comet'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild ai'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Vertex AI': ['google vertex ai', 'vertex ai'],\n",
    "    'Weights & Biases': ['weights & biases', 'weights and biases', 'wandb']\n",
    "}\n",
    "\n",
    "ignore_tools = {\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# function to scrape the posts from the tool-specific discussion fora\n",
    "\n",
    "\n",
    "def scrape_post(base_url, page_suffix, file_name):\n",
    "    page = -1\n",
    "    post_list = []\n",
    "\n",
    "    while True:\n",
    "        page = page + 1\n",
    "        page_url = base_url + page_suffix + str(page)\n",
    "        topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "        for topic in topic_list['topics']:\n",
    "            post_url = base_url + 't/' + \\\n",
    "                topic['slug'] + '/' + str(topic['id'])\n",
    "\n",
    "            post = {}\n",
    "            post['Question_title'] = topic['title']\n",
    "            post['Question_link'] = post_url\n",
    "            post['Question_creation_time'] = topic['created_at']\n",
    "            post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "            post['Question_score'] = topic['like_count']\n",
    "            post['Question_view_count'] = topic['views']\n",
    "            post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "            comments = requests.get(\n",
    "                post_url + '.json').json()['post_stream']['posts']\n",
    "            post['Question_body'] = comments[0]['cooked']\n",
    "            \n",
    "            answer_list = []\n",
    "            for comment in comments[1:]:\n",
    "                answer = {}\n",
    "                answer['Answer_creation_time'] = comment['created_at']\n",
    "                answer['Answer_body'] = comment['cooked']\n",
    "                answer['Answer_score'] = comment['score']\n",
    "                answer['Answer_has_accepted'] = comment['accepted_answer']\n",
    "                answer_list.append(answer)                \n",
    "            post['Answer_list'] = answer_list\n",
    "            \n",
    "            post_list.append(post)\n",
    "            time.sleep(5)\n",
    "\n",
    "        if 'more_topics_url' not in topic_list.keys():\n",
    "            break\n",
    "\n",
    "    with open(os.path.join(path_ts_raw, file_name), 'w') as outfile:\n",
    "        json_post_list = json.dumps(post_list, indent='\\t')\n",
    "        outfile.write(json_post_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from Guild AI\n",
    "base_url = 'https://my.guild.ai/'\n",
    "page_suffix = 'c/troubleshooting/6.json?page='\n",
    "file_name = 'Guild AI.json'\n",
    "scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from Weights & Biases\n",
    "base_url = 'https://community.wandb.ai/'\n",
    "page_suffix = 'c/w-b-support/36.json?page='\n",
    "file_name = 'Weights & Biases.json'\n",
    "scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from SigOpt\n",
    "base_url = 'https://community.sigopt.com/'\n",
    "page_suffix = 'c/general-discussion/9.json?page='\n",
    "file_name = 'SigOpt.json'\n",
    "scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from DVC\n",
    "base_url = 'https://discuss.dvc.org/'\n",
    "page_suffix = 'c/questions/9.json?page='\n",
    "file_name = 'DVC.json'\n",
    "scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# exclude tool-specific posts with negative upvote count\n",
    "df_ts_questions = pd.DataFrame()\n",
    "\n",
    "for file_name in glob.glob(os.path.join(path_ts_raw, '*.json')):\n",
    "    repos = pd.read_json(file_name)\n",
    "    if 'Question_score' in repos.columns:\n",
    "        repos = repos[repos['Question_score'] > -1]\n",
    "    repos['Tool'] = os.path.split(file_name)[1].split('.')[0]\n",
    "    df_ts_questions = pd.concat([df_ts_questions, repos], ignore_index=True)\n",
    "\n",
    "df_ts_questions.to_json(os.path.join(path_ts_filtered,\n",
    "                                     'questions.json'), orient='records', indent=4)\n",
    "\n",
    "# keep only posts with at least one answer\n",
    "df_ts_answers = df_ts_questions[df_ts_questions['Question_answer_count'] > 0]\n",
    "\n",
    "for tool in df_ts_answers['Tool'].unique().tolist():\n",
    "    number_accepted_answer = df_ts_answers[df_ts_answers['Tool']\n",
    "                                           == tool]['Question_has_accepted_answer'].sum()\n",
    "    if number_accepted_answer > 0:\n",
    "        df_ts_answers = df_ts_answers.drop(df_ts_answers[(df_ts_answers['Tool'] == tool) & (\n",
    "            df_ts_answers['Question_has_accepted_answer'] == False)].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#Question</th>\n",
       "      <th>#Answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>528</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>1435</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DVC</td>\n",
       "      <td>348</td>\n",
       "      <td>330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Domino</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guild AI</td>\n",
       "      <td>118</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLFlow</td>\n",
       "      <td>280</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>43</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>297</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>735</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Tool  #Question  #Answered\n",
       "0        Amazon SageMaker        528        167\n",
       "1  Azure Machine Learning       1435        343\n",
       "2                     DVC        348        330\n",
       "3                  Domino         13          4\n",
       "4                Guild AI        118        109\n",
       "5                  MLFlow        280        143\n",
       "6                Polyaxon         43         34\n",
       "7                  SigOpt         15          7\n",
       "8               Vertex AI        297         32\n",
       "9        Weights & Biases        735        117"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ts_question_summary = df_ts_questions.groupby(\n",
    "    'Tool').count()['Question_title'].reset_index()\n",
    "df_ts_answer_summary = df_ts_answers.groupby(\n",
    "    'Tool').count()['Question_title'].reset_index()\n",
    "\n",
    "df_ts_question_summary.columns = ['Tool', '#Question']\n",
    "df_ts_answer_summary.columns = ['Tool', '#Answered']\n",
    "\n",
    "df_summary = pd.merge(df_ts_question_summary, df_ts_answer_summary, on='Tool')\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question_id</th>\n",
       "      <th>Question_title</th>\n",
       "      <th>Question_body</th>\n",
       "      <th>Question_answer_count</th>\n",
       "      <th>Question_comment_count</th>\n",
       "      <th>Question_creation_time</th>\n",
       "      <th>Question_favorite_count</th>\n",
       "      <th>Question_score</th>\n",
       "      <th>Question_tags</th>\n",
       "      <th>Question_view_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Owner_up_votes</th>\n",
       "      <th>Owner_down_votes</th>\n",
       "      <th>Owner_views</th>\n",
       "      <th>Answer_body</th>\n",
       "      <th>Answer_comment_count</th>\n",
       "      <th>Answer_creation_time</th>\n",
       "      <th>Answer_score</th>\n",
       "      <th>Owner_location</th>\n",
       "      <th>Question_last_edit_time</th>\n",
       "      <th>Answer_last_edit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70098779</td>\n",
       "      <td>How to connect to MLFlow tracking server that ...</td>\n",
       "      <td>&lt;p&gt;I want to connect to remote tracking server...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-11-24 15:30:11.310000+00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[authorization, tracking, mlflow]</td>\n",
       "      <td>2102</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://mlflow.org/docs/latest/tra...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-11-24 17:01:13.483000+00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38927230</td>\n",
       "      <td>Panda AssertionError columns passed, passed da...</td>\n",
       "      <td>&lt;p&gt;I am working on Azure ML implementation on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-12 22:23:17.197000+00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[python, pandas, dataframe, nltk, azure-machin...</td>\n",
       "      <td>48200</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>55</td>\n",
       "      <td>339</td>\n",
       "      <td>&lt;p&gt;Try this:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;dataframe_outpu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-08-12 22:26:09.603000+00:00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Toronto, ON, Canada</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68773463</td>\n",
       "      <td>AccessDeniedException on sagemaker:CreateDomai...</td>\n",
       "      <td>&lt;p&gt;I am trying to use the AWS SageMaker Studio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-08-13 13:49:08.683000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazon-web-services, amazon-iam, amazon-sagem...</td>\n",
       "      <td>366</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67701971</td>\n",
       "      <td>How to label a text with multiple paragraphs i...</td>\n",
       "      <td>&lt;p&gt;I was trying setup a single label labeling ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-05-26 09:16:33.420000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[amazon-web-services, text, amazon-sagemaker, ...</td>\n",
       "      <td>161</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zürich, Suïssa</td>\n",
       "      <td>2021-05-26 11:54:00.030000+00:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48398509</td>\n",
       "      <td>How to Invoke AWS Sagemaker API with c# .NET?</td>\n",
       "      <td>&lt;p&gt;I have trained and deployed a model in AWS ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-23 09:42:48.607000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[c#, asp.net, amazon-web-services, aws-sdk, am...</td>\n",
       "      <td>743</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pune India</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Question_id                                     Question_title  \\\n",
       "0     70098779  How to connect to MLFlow tracking server that ...   \n",
       "1     38927230  Panda AssertionError columns passed, passed da...   \n",
       "2     68773463  AccessDeniedException on sagemaker:CreateDomai...   \n",
       "3     67701971  How to label a text with multiple paragraphs i...   \n",
       "4     48398509      How to Invoke AWS Sagemaker API with c# .NET?   \n",
       "\n",
       "                                       Question_body  Question_answer_count  \\\n",
       "0  <p>I want to connect to remote tracking server...                      1   \n",
       "1  <p>I am working on Azure ML implementation on ...                      1   \n",
       "2  <p>I am trying to use the AWS SageMaker Studio...                      1   \n",
       "3  <p>I was trying setup a single label labeling ...                      0   \n",
       "4  <p>I have trained and deployed a model in AWS ...                      1   \n",
       "\n",
       "   Question_comment_count           Question_creation_time  \\\n",
       "0                       0 2021-11-24 15:30:11.310000+00:00   \n",
       "1                       0 2016-08-12 22:23:17.197000+00:00   \n",
       "2                       0 2021-08-13 13:49:08.683000+00:00   \n",
       "3                       2 2021-05-26 09:16:33.420000+00:00   \n",
       "4                       0 2018-01-23 09:42:48.607000+00:00   \n",
       "\n",
       "   Question_favorite_count  Question_score  \\\n",
       "0                      1.0               1   \n",
       "1                      3.0               7   \n",
       "2                      NaN               0   \n",
       "3                      NaN               1   \n",
       "4                      NaN               0   \n",
       "\n",
       "                                       Question_tags  Question_view_count  \\\n",
       "0                  [authorization, tracking, mlflow]                 2102   \n",
       "1  [python, pandas, dataframe, nltk, azure-machin...                48200   \n",
       "2  [amazon-web-services, amazon-iam, amazon-sagem...                  366   \n",
       "3  [amazon-web-services, text, amazon-sagemaker, ...                  161   \n",
       "4  [c#, asp.net, amazon-web-services, aws-sdk, am...                  743   \n",
       "\n",
       "   ... Owner_up_votes Owner_down_votes  Owner_views  \\\n",
       "0  ...              0                0           11   \n",
       "1  ...            136               55          339   \n",
       "2  ...              0                0           11   \n",
       "3  ...             75               10          147   \n",
       "4  ...             34                1          124   \n",
       "\n",
       "                                         Answer_body  Answer_comment_count  \\\n",
       "0  <p><a href=\"https://mlflow.org/docs/latest/tra...                   2.0   \n",
       "1  <p>Try this:</p>\\n\\n<pre><code>dataframe_outpu...                   0.0   \n",
       "2                                                NaN                   NaN   \n",
       "3                                                NaN                   NaN   \n",
       "4                                                NaN                   NaN   \n",
       "\n",
       "              Answer_creation_time Answer_score       Owner_location  \\\n",
       "0 2021-11-24 17:01:13.483000+00:00          2.0                  NaN   \n",
       "1 2016-08-12 22:26:09.603000+00:00         13.0  Toronto, ON, Canada   \n",
       "2                              NaT          NaN                  NaN   \n",
       "3                              NaT          NaN       Zürich, Suïssa   \n",
       "4                              NaT          NaN           Pune India   \n",
       "\n",
       "           Question_last_edit_time  Answer_last_edit_time  \n",
       "0                              NaT                    NaT  \n",
       "1                              NaT                    NaT  \n",
       "2                              NaT                    NaT  \n",
       "3 2021-05-26 11:54:00.030000+00:00                    NaT  \n",
       "4                              NaT                    NaT  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(\n",
    "    path_so_raw, 'bq-results-20230201-032754-1675222092237.json'), lines=True)\n",
    "df['Question_tags'] = df['Question_tags'].str.split('|')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tag collection\n",
    "tags = set()\n",
    "for key, value in tool2tag.items():\n",
    "    tags = tags.union(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tags\n",
    "df['Question_valid_tags'] = [[] for _ in range(len(df))]\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'Question_valid_tags'] = list(\n",
    "        tags.intersection(set(row['Question_tags'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts with at least 1 tags has 5130 in total.\n",
      "Posts with at least 2 tags has 220 in total.\n",
      "Posts with at least 3 tags has 18 in total.\n"
     ]
    }
   ],
   "source": [
    "# count post number with different tags\n",
    "arity = 0\n",
    "while True:\n",
    "    post_number = df[df['Question_valid_tags'].map(len) > arity].shape[0]\n",
    "    if post_number < 1:\n",
    "        break\n",
    "    arity = arity + 1\n",
    "    print(f'Posts with at least {arity} tags has {post_number} in total.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-8534f91f27f9>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid['Question_link'] = df_valid['Question_id'].apply(\n"
     ]
    }
   ],
   "source": [
    "# exclude Stack Overflow posts with unrelated tags\n",
    "df_valid = df[df['Question_valid_tags'].map(len) > 0]\n",
    "df_valid['Question_link'] = df_valid['Question_id'].apply(\n",
    "    lambda x: f'https://stackoverflow.com/questions/{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude Stack Overflow posts with negative upvote count\n",
    "df_qualified = df_valid[df_valid['Question_score'] > -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map from tag to tool\n",
    "tag2tool = dict()\n",
    "for key, value in tool2tag.items():\n",
    "    for elem in value:\n",
    "        tag2tool.setdefault(elem, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract Stack Overflow post collection with multiple tags based on the tool map\n",
    "for index, row in df_qualified.iterrows():\n",
    "    tags = set()\n",
    "    for tag in row['Question_valid_tags']:\n",
    "        tags.add(tag2tool[tag])\n",
    "    df_qualified.at[index, 'Question_valid_tags'] = sorted(list(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Amazon SageMaker, MLFlow]                 16\n",
       "[Azure Machine Learning, MLFlow]           11\n",
       "[Kedro, MLFlow]                             4\n",
       "[Azure Machine Learning, Kedro, MLFlow]     2\n",
       "[DVC, MLFlow]                               1\n",
       "[MLFlow, Sacred]                            1\n",
       "[Kedro, Neptune]                            1\n",
       "Name: Question_valid_tags, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how the posts with more than one tags look like\n",
    "df_multiply_tagged = df_qualified[df_qualified['Question_valid_tags'].map(\n",
    "    len) > 1]\n",
    "df_multiply_tagged['Question_valid_tags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-9867e953fe3c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_qualified.at[index, 'Tool'] = tags[0]\n"
     ]
    }
   ],
   "source": [
    "# create Stack Overflow post collection with exclusive tags\n",
    "multiply_tagged_posts_split = []\n",
    "df_qualified.assign(Tool='')\n",
    "\n",
    "for index, row in df_qualified.iterrows():\n",
    "    tags = row['Question_valid_tags']\n",
    "    df_qualified.at[index, 'Tool'] = tags[0]\n",
    "    if len(tags) > 1:\n",
    "        for tag in tags[1:]:\n",
    "            series = row.copy()\n",
    "            series['Tool'] = tag\n",
    "            multiply_tagged_posts_split.append(series)\n",
    "\n",
    "df_multiply_tagged_posts_split = pd.DataFrame(multiply_tagged_posts_split)\n",
    "df_qualified_exclusive_tagged = pd.concat(\n",
    "    [df_qualified, df_multiply_tagged_posts_split], ignore_index=True)\n",
    "del df_qualified_exclusive_tagged['Question_valid_tags']\n",
    "\n",
    "# keep Stack Overflow posts with accepted answers\n",
    "df_qualified_exclusive_tagged_completed = df_qualified_exclusive_tagged.dropna(\n",
    "    subset=['Answer_body'])\n",
    "\n",
    "df_qualified_exclusive_tagged.to_json(os.path.join(\n",
    "    path_so_filtered, 'questions.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#Question</th>\n",
       "      <th>#Answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>2233</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>1530</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClearML</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comet</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DVC</td>\n",
       "      <td>91</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kedro</td>\n",
       "      <td>149</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLFlow</td>\n",
       "      <td>551</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neptune</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sacred</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>341</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool  #Question  #Answered\n",
       "0         Amazon SageMaker       2233        737\n",
       "1   Azure Machine Learning       1530        586\n",
       "2                  ClearML         40         20\n",
       "3                    Comet         10          4\n",
       "4                      DVC         91         49\n",
       "5                    Kedro        149         60\n",
       "6                   MLFlow        551        129\n",
       "7                  Neptune          8          3\n",
       "8                   Sacred         10          7\n",
       "9                Vertex AI        341        112\n",
       "10        Weights & Biases         77         22"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_so_question_summary = df_qualified_exclusive_tagged.groupby(\n",
    "    'Tool').count()['Question_id'].reset_index()\n",
    "df_so_answer_summary = df_qualified_exclusive_tagged_completed.groupby(\n",
    "    'Tool').count()['Question_id'].reset_index()\n",
    "\n",
    "df_so_question_summary.columns = ['Tool', '#Question']\n",
    "df_so_answer_summary.columns = ['Tool', '#Answered']\n",
    "\n",
    "df_summary = pd.merge(df_so_question_summary, df_so_answer_summary, on='Tool')\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create question dataset\n",
    "\n",
    "df_question_so = pd.read_json(os.path.join(path_so_filtered, 'questions.json'))\n",
    "df_question_ts = pd.read_json(os.path.join(path_ts_filtered, 'questions.json'))\n",
    "\n",
    "df_question_so['Platform'] = 'Stack Overflow'\n",
    "df_question_ts['Platform'] = 'Tool-specific'\n",
    "\n",
    "df_questions = pd.concat([df_question_so, df_question_ts], ignore_index=True)\n",
    "del df_questions['Question_tags']\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling, 'original.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add potential field to questions for later filling\n",
    "df_questions = pd.read_json(os.path.join(path_labeling, 'original.json'))\n",
    "\n",
    "# remove questions with uninformed content\n",
    "df_questions = df_questions[df_questions['Question_title'].apply(len) > 5]\n",
    "\n",
    "# Experiment 1: feed the original content to BerTopic\n",
    "df_questions['Question_original_content'] = ''\n",
    "\n",
    "# Experiment 2: feed the original content to text-davinci-003 model and get the generated summary, then feed the summary to BerTopic\n",
    "df_questions['Question_original_content_gpt_summary'] = ''\n",
    "\n",
    "# Experiment 3: feed the preprocessed content to BerTopic\n",
    "df_questions['Question_preprocessed_content'] = ''\n",
    "\n",
    "# Experiment 4: feed the original content to BerTopic\n",
    "df_questions['Answer_original_content'] = ''\n",
    "\n",
    "# Experiment 5: feed the original content to text-davinci-003 model and get the generated summary, then feed the summary to BerTopic\n",
    "df_questions['Answer_original_content_gpt_summary'] = ''\n",
    "\n",
    "# Experiment 6: feed the preprocessed content to BerTopic\n",
    "df_questions['Answer_preprocessed_content'] = ''\n",
    "\n",
    "df_questions.to_json(os.path.join(path_labeling, 'topic_modeling.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# content filtering patterns\n",
    "regex = r\"(<.*?>)|({.*?})|((!)?\\[.*?\\])|(\\(.*?\\))|(\\`{3}.+?\\`{3})|(\\`{2}.+?\\`{2})|(\\`{1}.+?\\`{1})|([^\\s]*[<=>]=[^\\s]+)|(@[^\\s]+)|((https?:\\/)?\\/[^\\s]+)|([^\\s]*\\\\[^\\s]+)|([^\\s]+\\/[^\\s]+)|([^\\s]+\\.[^\\s]+)|([^\\s]+-[^\\s]+)|([^\\s]+_[^\\s]+)|(_+[^\\s]+_*)|(_*[^\\s]+_+)|([0-9\\|\\-\\r\\n\\t\\\"\\-#*=~:{}\\(\\)\\[\\]<>]+)\"\n",
    "\n",
    "\n",
    "def preprocess_text(text, remove_code=False):\n",
    "    text = text.lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    remove_tags = ['script', 'style']\n",
    "    remove_tags.append('code') if remove_code else None\n",
    "    for tag in soup(remove_tags):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(regex, ' ', text, 0, re.DOTALL) if remove_code else text\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt for gpt model\n",
    "prompt_question = 'Please write a one-sentence summary of the user\\'s encountered challenges. For instance, you could begin with a sentence such as: \"The user XXXXXX.\"\\n###'\n",
    "prompt_answer = 'Below is a question-answer pair. Given the context of the question, extract any possible solutions (if any) from the answer and make a high-level, human-readable, and concise summary of them.\\n###'\n",
    "\n",
    "import random\n",
    "\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    content = 'Title: ' + \\\n",
    "        preprocess_text(row['Question_title']) + '; Content: ' + \\\n",
    "        preprocess_text(str(row['Question_body']))\n",
    "\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in content:\n",
    "                content = content.replace(tool_keyword, '')\n",
    "            \n",
    "    df_questions.at[index, 'Question_original_content'] = ' '.join(\n",
    "        content.split())\n",
    "    \n",
    "# remove questions with uninformed content\n",
    "df_questions[df_questions['Question_original_content'].apply(len) > 15]\n",
    "df_questions.to_json(os.path.join(path_labeling,\n",
    "                     'topic_modeling.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    if row['Question_original_content_gpt_summary']:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful summarizer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_question + row['Question_original_content'] + '###\\n'},\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=200,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=10,\n",
    "            stream=False\n",
    "        )\n",
    "        df_questions.at[index,\n",
    "                      'Question_original_content_gpt_summary'] = response['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        # output unsuccesful requests\n",
    "        print(f'{e} on question {row[\"Question_link\"]}')\n",
    "\n",
    "    if index % 50 == 0:\n",
    "        print(f'persisting on question {index}')\n",
    "        df_questions.to_json(os.path.join(\n",
    "            path_labeling, 'topic_modeling.json'), indent=4, orient='records')\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling, 'topic_modeling.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8852"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'topic_modeling.json'))\n",
    "assert (df_questions.shape[0] == df_questions.dropna(\n",
    "    subset=['Question_original_content_gpt_summary']).shape[0])\n",
    "\n",
    "# output the number of asset-management-related discussion posts\n",
    "df_questions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    content = 'Title: ' + preprocess_text(row['Question_title'], remove_code=True) + \\\n",
    "        '; Content: ' + \\\n",
    "        preprocess_text(str(row['Question_body']), remove_code=True)\n",
    "\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in content:\n",
    "                content = content.replace(tool_keyword, '')\n",
    "\n",
    "    df_questions.at[index, 'Question_preprocessed_content'] = ' '.join(\n",
    "        content.split())\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling, 'topic_modeling.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4\n",
    "\n",
    "df_answers = pd.read_json(os.path.join(path_labeling, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_answers.iterrows():\n",
    "    answer = ''\n",
    "    if row['Answer_body']:\n",
    "        answer = row['Answer_body']\n",
    "    elif row['Answer_list']:\n",
    "        if row['Question_has_accepted_answer']:\n",
    "            if 'Answer_has_accepted' in row['Answer_list'][0]:\n",
    "                for comment in row['Answer_list']:\n",
    "                    if comment['Answer_has_accepted']:\n",
    "                        answer = comment['Answer_body']\n",
    "                        break\n",
    "        elif 'Answer_body' in row['Answer_list'][0]:\n",
    "            for comment in row['Answer_list']:\n",
    "                answer += comment['Answer_body'] + '\\n'\n",
    "\n",
    "    answer = preprocess_text(answer)\n",
    "\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in content:\n",
    "                content = content.replace(tool_keyword, '')\n",
    "\n",
    "    df_answers.at[index, 'Answer_original_content'] = ' '.join(answer.split())\n",
    "\n",
    "df_answers.to_json(os.path.join(path_labeling, 'topic_modeling.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persisting on answer 0\n",
      "persisting on answer 50\n",
      "persisting on answer 100\n",
      "persisting on answer 150\n",
      "persisting on answer 200\n",
      "persisting on answer 250\n",
      "persisting on answer 300\n",
      "persisting on answer 350\n",
      "persisting on answer 400\n",
      "persisting on answer 450\n",
      "persisting on answer 500\n",
      "persisting on answer 550\n",
      "persisting on answer 600\n",
      "persisting on answer 650\n",
      "persisting on answer 700\n",
      "persisting on answer 750\n",
      "persisting on answer 800\n",
      "persisting on answer 850\n",
      "persisting on answer 900\n",
      "persisting on answer 950\n",
      "persisting on answer 1000\n",
      "persisting on answer 1050\n",
      "persisting on answer 1100\n",
      "persisting on answer 1150\n",
      "persisting on answer 1200\n",
      "persisting on answer 1250\n",
      "persisting on answer 1300\n",
      "persisting on answer 1350\n",
      "persisting on answer 1400\n",
      "persisting on answer 1450\n",
      "persisting on answer 1500\n",
      "persisting on answer 1550\n",
      "persisting on answer 1600\n",
      "persisting on answer 1650\n",
      "persisting on answer 1700\n",
      "persisting on answer 1750\n",
      "persisting on answer 1800\n",
      "persisting on answer 1850\n",
      "persisting on answer 1900\n",
      "persisting on answer 1950\n",
      "persisting on answer 2000\n",
      "persisting on answer 2050\n",
      "persisting on answer 2100\n",
      "persisting on answer 2150\n",
      "persisting on answer 2200\n",
      "persisting on answer 2250\n",
      "persisting on answer 2300\n",
      "persisting on answer 2350\n",
      "persisting on answer 2400\n",
      "persisting on answer 2450\n",
      "persisting on answer 2500\n",
      "persisting on answer 2550\n",
      "persisting on answer 2600\n",
      "persisting on answer 2650\n",
      "persisting on answer 2700\n",
      "persisting on answer 2750\n",
      "persisting on answer 2800\n",
      "persisting on answer 2850\n",
      "persisting on answer 2900\n",
      "persisting on answer 2950\n",
      "persisting on answer 3000\n",
      "persisting on answer 3050\n",
      "persisting on answer 3100\n",
      "persisting on answer 3150\n",
      "persisting on answer 3200\n",
      "persisting on answer 3250\n",
      "persisting on answer 3300\n",
      "persisting on answer 3350\n",
      "persisting on answer 3400\n",
      "persisting on answer 3450\n",
      "persisting on answer 3500\n",
      "persisting on answer 3550\n",
      "persisting on answer 3600\n",
      "persisting on answer 3650\n",
      "persisting on answer 3700\n",
      "persisting on answer 3750\n",
      "persisting on answer 3800\n",
      "persisting on answer 3850\n",
      "persisting on answer 3900\n",
      "persisting on answer 3950\n",
      "persisting on answer 4000\n",
      "persisting on answer 4050\n",
      "persisting on answer 4100\n",
      "persisting on answer 4150\n",
      "persisting on answer 4200\n",
      "persisting on answer 4250\n",
      "persisting on answer 4300\n",
      "persisting on answer 4350\n",
      "persisting on answer 4400\n",
      "persisting on answer 4450\n",
      "persisting on answer 4500\n",
      "persisting on answer 4550\n",
      "persisting on answer 4600\n",
      "persisting on answer 4650\n",
      "persisting on answer 4700\n",
      "persisting on answer 4750\n",
      "persisting on answer 4800\n",
      "persisting on answer 4850\n",
      "persisting on answer 4900\n",
      "persisting on answer 4950\n",
      "persisting on answer 5000\n",
      "persisting on answer 5050\n",
      "persisting on answer 5100\n",
      "persisting on answer 5150\n",
      "persisting on answer 5200\n",
      "persisting on answer 5250\n",
      "persisting on answer 5300\n",
      "persisting on answer 5350\n",
      "persisting on answer 5400\n",
      "persisting on answer 5450\n",
      "persisting on answer 5500\n",
      "persisting on answer 5550\n",
      "persisting on answer 5600\n",
      "persisting on answer 5650\n",
      "persisting on answer 5700\n",
      "persisting on answer 5750\n",
      "persisting on answer 5800\n",
      "persisting on answer 5850\n",
      "persisting on answer 5900\n",
      "persisting on answer 5950\n",
      "persisting on answer 6000\n",
      "persisting on answer 6050\n",
      "persisting on answer 6100\n",
      "persisting on answer 6150\n",
      "persisting on answer 6200\n",
      "persisting on answer 6250\n",
      "persisting on answer 6300\n",
      "persisting on answer 6350\n",
      "persisting on answer 6400\n",
      "persisting on answer 6450\n",
      "persisting on answer 6500\n",
      "persisting on answer 6550\n",
      "persisting on answer 6600\n",
      "persisting on answer 6650\n",
      "persisting on answer 6700\n",
      "persisting on answer 6750\n",
      "persisting on answer 6800\n",
      "persisting on answer 6850\n",
      "persisting on answer 6900\n",
      "persisting on answer 6950\n",
      "persisting on answer 7000\n",
      "persisting on answer 7050\n",
      "persisting on answer 7100\n",
      "persisting on answer 7150\n",
      "persisting on answer 7200\n",
      "persisting on answer 7250\n",
      "persisting on answer 7300\n",
      "persisting on answer 7350\n",
      "persisting on answer 7400\n",
      "persisting on answer 7450\n",
      "persisting on answer 7500\n",
      "persisting on answer 7550\n",
      "persisting on answer 7600\n",
      "persisting on answer 7650\n",
      "persisting on answer 7700\n",
      "persisting on answer 7750\n",
      "persisting on answer 7800\n",
      "persisting on answer 7850\n",
      "persisting on answer 7900\n",
      "persisting on answer 7950\n",
      "persisting on answer 8000\n",
      "persisting on answer 8050\n",
      "persisting on answer 8100\n",
      "persisting on answer 8150\n",
      "persisting on answer 8200\n",
      "persisting on answer 8250\n",
      "persisting on answer 8300\n",
      "persisting on answer 8350\n",
      "persisting on answer 8400\n",
      "persisting on answer 8450\n",
      "persisting on answer 8500\n",
      "persisting on answer 8550\n",
      "persisting on answer 8600\n",
      "persisting on answer 8650\n",
      "persisting on answer 8700\n",
      "persisting on answer 8750\n",
      "persisting on answer 8800\n"
     ]
    }
   ],
   "source": [
    "# Experiment 5\n",
    "\n",
    "df_answers = pd.read_json(os.path.join(\n",
    "    path_labeling, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_answers.iterrows():\n",
    "    if index % 50 == 0:\n",
    "        print(f'persisting on answer {index}')\n",
    "        df_answers.to_json(os.path.join(\n",
    "            path_labeling, 'topic_modeling.json'), indent=4, orient='records')\n",
    "\n",
    "    if not row['Answer_original_content'] or row['Answer_original_content_gpt_summary']:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-3.5-turbo',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful summarizer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt_answer + 'Question: ' + row['Question_original_content_gpt_summary'] + '; Answer: ' + row['Answer_original_content'] + '###\\n'},\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=200,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=10,\n",
    "            stream=False\n",
    "        )\n",
    "        df_answers.at[index,\n",
    "                      'Answer_original_content_gpt_summary'] = response['choices'][0]['message']['content'].strip()\n",
    "    except Exception as e:\n",
    "        # output unsuccesful requests\n",
    "        print(f'{e} on answer {row[\"Question_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df_answers.to_json(os.path.join(\n",
    "    path_labeling, 'topic_modeling.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4874"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_answers = pd.read_json(os.path.join(\n",
    "    path_labeling, 'topic_modeling.json'))\n",
    "assert len(df_answers[df_answers['Answer_original_content'] != '']) == len(df_answers[df_answers['Answer_original_content_gpt_summary'] != ''])\n",
    "\n",
    "# output the number of asset-management-related QA answers\n",
    "len(df_answers[df_answers['Answer_original_content'] != ''])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size is based on the recommendation from https://www.calculator.net/sample-size-calculator.html\n",
    "\n",
    "sample_size = 369\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'topic_modeling.json'))\n",
    "\n",
    "df_sample = df_issues[df_answers['Answer_original_content'] != ''].sample(n=sample_size, random_state=42)\n",
    "\n",
    "df_sample.to_json(os.path.join(\n",
    "    path_labeling, 'sample.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6\n",
    "\n",
    "df_answers = pd.read_json(os.path.join(path_labeling, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_answers.iterrows():\n",
    "    answer = ''\n",
    "    if row['Answer_body']:\n",
    "        answer = row['Answer_body']\n",
    "    elif row['Answer_list']:\n",
    "        if row['Question_has_accepted_answer']:\n",
    "            if 'Answer_has_accepted' in row['Answer_list'][0]:\n",
    "                for comment in row['Answer_list']:\n",
    "                    if comment['Answer_has_accepted']:\n",
    "                        answer = comment['Answer_body']\n",
    "                        break\n",
    "        elif 'Answer_body' in row['Answer_list'][0]:\n",
    "            for comment in row['Answer_list']:\n",
    "                answer += comment['Answer_body'] + '\\n'\n",
    "\n",
    "    answer = preprocess_text(answer, remove_code=True)\n",
    "\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in answer:\n",
    "                answer = answer.replace(tool_keyword, '')\n",
    "\n",
    "    df_answers.at[index, 'Answer_preprocessed_content'] = ' '.join(answer.split())\n",
    "\n",
    "df_answers.to_json(os.path.join(path_labeling, 'topic_modeling.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4\n",
    "\n",
    "df_answers = pd.read_json(os.path.join(path_labeling, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_answers.iterrows():\n",
    "    if not row['Answer_original_content_gpt_summary']:\n",
    "        continue\n",
    "    content = row['Answer_original_content_gpt_summary'].lower()\n",
    "\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in content:\n",
    "                content = content.replace(tool_keyword, '')\n",
    "\n",
    "    df_answers.at[index, 'Answer_original_content_gpt_summary'] = content\n",
    "\n",
    "df_answers.to_json(os.path.join(path_labeling, 'topic_modeling.json'), indent=4, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
