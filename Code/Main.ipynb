{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_so = os.path.join(path_dataset, 'Stack Overflow')\n",
    "path_ts = os.path.join(path_dataset, 'Tool-specific Others')\n",
    "path_labeling = os.path.join(path_dataset, 'Labeling')  \n",
    "\n",
    "path_so_raw = os.path.join(path_so, 'Raw')\n",
    "path_ts_raw = os.path.join(path_ts, 'Raw')\n",
    "path_so_filtered = os.path.join(path_so, 'Filtered')\n",
    "path_ts_filtered = os.path.join(path_ts, 'Filtered')\n",
    "\n",
    "if not os.path.exists(path_dataset):\n",
    "    os.makedirs(path_dataset)\n",
    "\n",
    "if not os.path.isdir(path_so):\n",
    "    os.mkdir(path_so)\n",
    "\n",
    "if not os.path.isdir(path_ts):\n",
    "    os.mkdir(path_ts)\n",
    "\n",
    "if not os.path.isdir(path_labeling):\n",
    "    os.mkdir(path_labeling)\n",
    "\n",
    "if not os.path.isdir(path_so_raw):\n",
    "    os.mkdir(path_so_raw)\n",
    "\n",
    "if not os.path.isdir(path_ts_raw):\n",
    "    os.mkdir(path_ts_raw)\n",
    "\n",
    "if not os.path.isdir(path_so_filtered):\n",
    "    os.mkdir(path_so_filtered)\n",
    "\n",
    "if not os.path.isdir(path_ts_filtered):\n",
    "    os.mkdir(path_ts_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool2tag = {\n",
    "    'Amazon SageMaker': {'amazon-sagemaker', 'amazon-sagemaker-experiments', 'amazon-sagemaker-studio'},\n",
    "    'Azure Machine Learning': {'azure-machine-learning-service', 'azure-machine-learning-studio', 'azure-machine-learning-workbench'},\n",
    "    'ClearML': {'clearml'},\n",
    "    'Comet': {'comet-ml'},\n",
    "    'DVC': {'dvc'},\n",
    "    'Kedro': {'kedro'},\n",
    "    'MLflow': {'mlflow'},\n",
    "    'MLRun': {'mlrun'},\n",
    "    'Neptune': {'neptune'},\n",
    "    'Optuna': {'optuna'},\n",
    "    'Sacred': {'python-sacred'},\n",
    "    'Vertex AI': {'google-cloud-vertex-ai'},\n",
    "    'Weights & Biases': {'wandb'}\n",
    "}\n",
    "\n",
    "tools_keywords = {\n",
    "    'Amazon SageMaker': ['amazon sagemaker', 'aws sagemaker', 'sagemaker'],\n",
    "    'Azure Machine Learning': ['microsoft azure machine learning', 'azure machine learning', 'microsoft azure ml', 'microsoft azureml', 'azure ml', 'azureml'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'Comet': ['comet'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Optuna': ['optuna'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Vertex AI': ['google vertex ai', 'vertex ai'],\n",
    "    'Weights & Biases': ['weights & biases', 'weights and biases', 'wandb']\n",
    "}\n",
    "\n",
    "tool_no_accepted_answer = {\n",
    "    'Domino', \n",
    "    'DVC', \n",
    "    'Guild AI\"', \n",
    "    'MLflow', \n",
    "    'Polyaxon', \n",
    "    'SigOpt'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = 'all'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# function to scrape the posts from the tool-specific discussion fora\n",
    "\n",
    "\n",
    "def scrape_post(base_url, page_suffix, file_name):\n",
    "    page = -1\n",
    "    post_list = []\n",
    "\n",
    "    while True:\n",
    "        page = page + 1\n",
    "        page_url = base_url + page_suffix + str(page)\n",
    "        topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "        for topic in topic_list['topics']:\n",
    "            post_url = base_url + 't/' + \\\n",
    "                topic['slug'] + '/' + str(topic['id'])\n",
    "\n",
    "            post = {}\n",
    "            post['Question_title'] = topic['title']\n",
    "            post['Question_link'] = post_url\n",
    "            post['Question_created_time'] = topic['created_at']\n",
    "            post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "            post['Question_score_count'] = topic['like_count']\n",
    "            post['Question_view_count'] = topic['views']\n",
    "            post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "            comments = requests.get(\n",
    "                post_url + '.json').json()['post_stream']['posts']\n",
    "            post['Question_body'] = comments[0]['cooked']\n",
    "            \n",
    "            answer_list = []\n",
    "            for comment in comments[1:]:\n",
    "                answer = {}\n",
    "                answer['Answer_created_time'] = comment['created_at']\n",
    "                answer['Answer_body'] = comment['cooked']\n",
    "                answer['Answer_has_accepted'] = comment['accepted_answer']\n",
    "                answer_list.append(answer)                \n",
    "            post['Answer_list'] = answer_list\n",
    "            \n",
    "            post_list.append(post)\n",
    "            time.sleep(5)\n",
    "\n",
    "        if 'more_topics_url' not in topic_list.keys():\n",
    "            break\n",
    "\n",
    "    with open(os.path.join(path_ts_raw, file_name), 'w') as outfile:\n",
    "        json_post_list = json.dumps(post_list, indent='\\t')\n",
    "        outfile.write(json_post_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from Guild AI\n",
    "base_url = 'https://my.guild.ai/'\n",
    "page_suffix = 'c/troubleshooting/6.json?page='\n",
    "file_name = 'Guild AI.json'\n",
    "scrape_post(base_url, page_suffix, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from Weights & Biases\n",
    "base_url = 'https://community.wandb.ai/'\n",
    "page_suffix = 'c/w-b-support/36.json?page='\n",
    "file_name = 'Weights & Biases.json'\n",
    "scrape_post(base_url, page_suffix, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from SigOpt\n",
    "base_url = 'https://community.sigopt.com/'\n",
    "page_suffix = 'c/general-discussion/9.json?page='\n",
    "file_name = 'SigOpt.json'\n",
    "scrape_post(base_url, page_suffix, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from DVC\n",
    "base_url = 'https://discuss.dvc.org/'\n",
    "page_suffix = 'c/questions/9.json?page='\n",
    "file_name = 'DVC.json'\n",
    "scrape_post(base_url, page_suffix, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3812\n",
      "3812\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "# exclude tool-specific posts with negative upvote count\n",
    "df_ts_questions = pd.DataFrame()\n",
    "total_post = 0\n",
    "\n",
    "for file_name in glob.glob(os.path.join(path_ts_raw, '*.json')):\n",
    "    repos = pd.read_json(file_name)\n",
    "    total_post += len(repos)    \n",
    "    if 'Question_score_count' in repos.columns:\n",
    "        repos = repos[repos['Question_score_count'] > -1]\n",
    "    repos['Tool'] = os.path.split(file_name)[1].split('.')[0]\n",
    "    df_ts_questions = pd.concat([df_ts_questions, repos], ignore_index=True)\n",
    "\n",
    "df_ts_questions['Question_comment_count'] = df_ts_questions['Question_comment_count'].fillna(0)\n",
    "df_ts_questions.to_json(os.path.join(path_ts_filtered,\n",
    "                                     'questions.json'), orient='records', indent=4)\n",
    "print(total_post, df_ts_questions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.auth\n",
    "import pandas_gbq\n",
    "\n",
    "credentials, _ = google.auth.default()\n",
    "\n",
    "pandas_gbq.context.credentials = credentials\n",
    "pandas_gbq.context.project = 'stack-overflow-dataset-330612'\n",
    "\n",
    "with open(os.path.join(path_so_raw, 'bigquery.sql'), 'r') as sql_file:\n",
    "    sql = sql_file.read()\n",
    "    df = pandas_gbq.read_gbq(sql)\n",
    "    df['Question_tags'] = df['Question_tags'].str.split('|')\n",
    "    df['Question_favorite_count'] = df['Question_favorite_count'].fillna(0)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tag collection\n",
    "tags = set()\n",
    "for key, value in tool2tag.items():\n",
    "    tags = tags.union(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tags\n",
    "df['Question_valid_tags'] = [[] for _ in range(len(df))]\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'Question_valid_tags'] = list(\n",
    "        tags.intersection(set(row['Question_tags'])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts with at least 1 tags has 5308 in total.\n",
      "Posts with at least 2 tags has 220 in total.\n",
      "Posts with at least 3 tags has 18 in total.\n"
     ]
    }
   ],
   "source": [
    "# count post number with different tags\n",
    "arity = 0\n",
    "while True:\n",
    "    post_number = df[df['Question_valid_tags'].map(len) > arity].shape[0]\n",
    "    if post_number < 1:\n",
    "        break\n",
    "    arity = arity + 1\n",
    "    print(f'Posts with at least {arity} tags has {post_number} in total.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhimi\\AppData\\Local\\Temp\\ipykernel_8032\\3717773066.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid['Question_link'] = df_valid['Question_id'].apply(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5308"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclude Stack Overflow posts with unrelated tags\n",
    "df_valid = df[df['Question_valid_tags'].map(len) > 0]\n",
    "df_valid['Question_link'] = df_valid['Question_id'].apply(\n",
    "    lambda x: f'https://stackoverflow.com/questions/{x}')\n",
    "len(df_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5175"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# exclude Stack Overflow posts with negative upvote count\n",
    "df_qualified = df_valid[df_valid['Question_score_count'] > -1]\n",
    "len(df_qualified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map from tag to tool\n",
    "tag2tool = dict()\n",
    "for key, value in tool2tag.items():\n",
    "    for elem in value:\n",
    "        tag2tool.setdefault(elem, key)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract Stack Overflow post collection with multiple tags based on the tool map\n",
    "for index, row in df_qualified.iterrows():\n",
    "    tags = set()\n",
    "    for tag in row['Question_valid_tags']:\n",
    "        tags.add(tag2tool[tag])\n",
    "    df_qualified.at[index, 'Question_valid_tags'] = sorted(list(tags))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Amazon SageMaker, MLflow]                 16\n",
       "[Azure Machine Learning, MLflow]           11\n",
       "[Kedro, MLflow]                             4\n",
       "[Azure Machine Learning, Kedro, MLflow]     2\n",
       "[DVC, MLflow]                               1\n",
       "[Kedro, Neptune]                            1\n",
       "[MLflow, Sacred]                            1\n",
       "Name: Question_valid_tags, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how the posts with more than one tags look like\n",
    "df_multiply_tagged = df_qualified[df_qualified['Question_valid_tags'].map(\n",
    "    len) > 1]\n",
    "df_multiply_tagged['Question_valid_tags'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Stack Overflow post collection with exclusive tags\n",
    "multiply_tagged_posts_split = []\n",
    "df_qualified.assign(Tool='')\n",
    "\n",
    "for index, row in df_qualified.iterrows():\n",
    "    tags = row['Question_valid_tags']\n",
    "    df_qualified.at[index, 'Tool'] = tags[0]\n",
    "    if len(tags) > 1:\n",
    "        for tag in tags[1:]:\n",
    "            series = row.copy()\n",
    "            series['Tool'] = tag\n",
    "            multiply_tagged_posts_split.append(series)\n",
    "\n",
    "df_multiply_tagged_posts_split = pd.DataFrame(multiply_tagged_posts_split)\n",
    "df_qualified_exclusive_tagged = pd.concat(\n",
    "    [df_qualified, df_multiply_tagged_posts_split], ignore_index=True)\n",
    "\n",
    "df_qualified_exclusive_tagged['Challenge_self_resolution'] = df_qualified_exclusive_tagged['Poster_id'] == df_qualified_exclusive_tagged['Answerer_id']\n",
    "\n",
    "del df_qualified_exclusive_tagged['Poster_id']\n",
    "del df_qualified_exclusive_tagged['Answerer_id']\n",
    "del df_qualified_exclusive_tagged['Question_id']\n",
    "# remove null age due to anonymity\n",
    "del df_qualified_exclusive_tagged['Poster_age']\n",
    "del df_qualified_exclusive_tagged['Answerer_age']\n",
    "del df_qualified_exclusive_tagged['Question_valid_tags']\n",
    "\n",
    "df_qualified_exclusive_tagged.to_json(os.path.join(\n",
    "    path_so_filtered, 'questions.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create question dataset\n",
    "\n",
    "df_question_so = pd.read_json(os.path.join(path_so_filtered, 'questions.json'))\n",
    "df_question_ts = pd.read_json(os.path.join(path_ts_filtered, 'questions.json'))\n",
    "\n",
    "df_question_so['Platform'] = 'Stack Overflow'\n",
    "df_question_so['Question_closed_time'] = df_question_so['Answer_created_time']\n",
    "\n",
    "df_question_ts['Platform'] = 'Tool-specific'\n",
    "df_question_ts['Question_closed_time'] = np.nan\n",
    "\n",
    "# Retrieve the creation time of the acctepted answer \n",
    "for index, row in df_question_ts.iterrows():\n",
    "    if row['Tool'] in tool_no_accepted_answer or not row['Question_has_accepted_answer']:\n",
    "        continue\n",
    "    for comment in row['Answer_list']:\n",
    "        if comment['Answer_has_accepted']:\n",
    "            df_question_ts.at[index, 'Answer_body'] = comment['Answer_body']\n",
    "            df_question_ts.at[index, 'Question_closed_time'] = pd.to_datetime(comment['Answer_created_time'])\n",
    "            if 'Answer_comment_count' in comment:\n",
    "                df_question_ts.at[index, 'Answer_comment_count'] = pd.to_datetime(comment['Answer_comment_count'])\n",
    "            if 'Answer_score_count' in comment:\n",
    "                df_question_ts.at[index, 'Answer_score_count'] = pd.to_datetime(comment['Answer_score_count'])\n",
    "            break\n",
    "\n",
    "df_questions = pd.concat([df_question_so, df_question_ts], ignore_index=True)\n",
    "\n",
    "del df_questions['Question_tags']\n",
    "del df_questions['Question_has_accepted_answer']\n",
    "del df_questions['Answer_list']\n",
    "del df_questions['Answer_created_time']\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling, 'original.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add potential field to questions for later filling\n",
    "df_questions = pd.read_json(os.path.join(path_labeling, 'original.json'))\n",
    "\n",
    "df_questions['Question_original_content'] = np.nan\n",
    "df_questions['Question_preprocessed_content'] = np.nan\n",
    "df_questions['Question_gpt_summary_original'] = np.nan\n",
    "df_questions['Question_gpt_summary'] = np.nan\n",
    "df_questions['Answer_original_content'] = np.nan\n",
    "df_questions['Answer_preprocessed_content'] = np.nan\n",
    "df_questions['Answer_gpt_summary_original'] = np.nan\n",
    "df_questions['Answer_gpt_summary'] = np.nan\n",
    "\n",
    "df_questions.to_json(os.path.join(path_labeling, 'questions.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# content filtering patterns\n",
    "\n",
    "from gensim.parsing.preprocessing import preprocess_string\n",
    "from string import ascii_lowercase\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "regex = r\"(ftp|https?://[^\\s]+)|(<.*?>)|({.*?})|((!)?\\[.*?\\])|(\\(.*?\\))|(\\`{3}.+?\\`{3})|(\\`{2}.+?\\`{2})|(\\`{1}.+?\\`{1})|([^\\s]*[<=>]=[^\\s]+)|(@[^\\s]+)|([^\\s]*\\\\[^\\s]+)|([^\\s]+\\/[^\\s]+)|([^\\s]+\\.[^\\s]+)|([^\\s]+-[^\\s]+)|([^\\s]+_[^\\s]+)|(_+[^\\s]+_*)|(_*[^\\s]+_+)|([0-9\\|\\-\\r\\n\\t\\\"\\-#*=~:{}\\(\\)\\[\\]<>]+)\"\n",
    "\n",
    "len_max = len('aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa')\n",
    "\n",
    "def preprocess_text(text, remove_code=False):          \n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    remove_tags = ['script', 'style']\n",
    "    remove_tags.append('code') if remove_code else None\n",
    "    for tag in soup(remove_tags):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    text = text.lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "    for tool_keywords in tools_keywords.values():\n",
    "        for tool_keyword in tool_keywords:\n",
    "            if tool_keyword in text:\n",
    "                text = text.replace(tool_keyword, '')\n",
    "    \n",
    "    text = re.sub(regex, ' ', text, 0, re.DOTALL) if remove_code else text\n",
    "    \n",
    "    # remove repeated letters\n",
    "    for time in range(3, len_max + 1):\n",
    "        for letter in ascii_lowercase:\n",
    "            text = text.replace(letter * time, '')\n",
    "    \n",
    "    text = preprocess_string(text)\n",
    "    text = ' '.join(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create prompt for gpt model\n",
    "\n",
    "import random\n",
    "\n",
    "prompt_question = 'Your task is to provide a brief and accurate summary of the challenges that the user has encountered based on the given text. Your summary should be concise, highlighting only the most important details related to the challenges faced by the user. Please note that your response should focus on providing an objective and factual summary of the challenges without including any personal opinions or biases.\\n###'\n",
    "prompt_answer = 'Given a challenge-discussion pair, please extract any possible solutions mentioned in the discussion and provide a brief and accurate summary of them. If no solution is mentioned, please indicate that there are no solutions provided. Please note that your response should focus on providing an objective and factual summary without including any personal opinions or biases.\\n###'\n",
    "\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmy/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/jimmy/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Experiment 1\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    question = preprocess_text(row['Question_title']) + ' ' + preprocess_text(str(row['Question_body']))\n",
    "\n",
    "    if len(question.split()) < 6:\n",
    "        df_questions.drop(index, inplace=True)\n",
    "        print(question)\n",
    "    else:\n",
    "        df_questions.at[index, 'Question_original_content'] = question\n",
    "\n",
    "df_questions.to_json(os.path.join(path_labeling,\n",
    "                     'questions.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Platform</th>\n",
       "      <th>Question_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Stack Overflow</td>\n",
       "      <td>5213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tool-specific</td>\n",
       "      <td>3788</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Platform  Question_title\n",
       "0  Stack Overflow            5213\n",
       "1   Tool-specific            3788"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions = pd.read_json(os.path.join(path_labeling, 'questions.json'))\n",
    "df_questions.groupby('Platform').count()['Question_title'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>Question_title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>2776</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>2974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClearML</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comet</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DVC</td>\n",
       "      <td>439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Domino</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Guild AI</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Kedro</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>MLflow</td>\n",
       "      <td>831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Neptune</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Optuna</td>\n",
       "      <td>141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sacred</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>811</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool  Question_title\n",
       "0         Amazon SageMaker            2776\n",
       "1   Azure Machine Learning            2974\n",
       "2                  ClearML              40\n",
       "3                    Comet              10\n",
       "4                      DVC             439\n",
       "5                   Domino              13\n",
       "6                 Guild AI             118\n",
       "7                    Kedro             149\n",
       "8                   MLflow             831\n",
       "9                  Neptune               8\n",
       "10                  Optuna             141\n",
       "11                Polyaxon              41\n",
       "12                  Sacred              10\n",
       "13                  SigOpt              14\n",
       "14               Vertex AI             626\n",
       "15        Weights & Biases             811"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions = pd.read_json(os.path.join(path_labeling, 'questions.json'))\n",
    "df_questions.groupby('Tool').count()['Question_title'].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persisting on question 49\n",
      "persisting on question 99\n",
      "persisting on question 149\n",
      "persisting on question 199\n",
      "persisting on question 249\n",
      "persisting on question 299\n",
      "persisting on question 349\n",
      "persisting on question 399\n",
      "persisting on question 449\n",
      "persisting on question 499\n",
      "persisting on question 549\n",
      "persisting on question 599\n",
      "persisting on question 649\n",
      "persisting on question 699\n",
      "persisting on question 749\n",
      "persisting on question 799\n",
      "persisting on question 849\n",
      "persisting on question 899\n",
      "persisting on question 949\n",
      "persisting on question 999\n",
      "persisting on question 1049\n",
      "persisting on question 1099\n",
      "persisting on question 1149\n",
      "persisting on question 1199\n",
      "persisting on question 1249\n",
      "persisting on question 1299\n",
      "persisting on question 1349\n",
      "persisting on question 1399\n",
      "persisting on question 1449\n",
      "persisting on question 1499\n",
      "persisting on question 1549\n",
      "persisting on question 1599\n",
      "persisting on question 1649\n",
      "persisting on question 1699\n",
      "persisting on question 1749\n",
      "persisting on question 1799\n",
      "persisting on question 1849\n",
      "persisting on question 1899\n",
      "persisting on question 1949\n",
      "persisting on question 1999\n",
      "persisting on question 2049\n",
      "persisting on question 2099\n",
      "persisting on question 2149\n",
      "persisting on question 2199\n",
      "persisting on question 2249\n",
      "persisting on question 2299\n",
      "persisting on question 2349\n",
      "persisting on question 2399\n",
      "persisting on question 2449\n",
      "persisting on question 2499\n",
      "persisting on question 2549\n",
      "persisting on question 2599\n",
      "persisting on question 2649\n",
      "persisting on question 2699\n",
      "persisting on question 2749\n",
      "persisting on question 2799\n",
      "persisting on question 2849\n",
      "persisting on question 2899\n",
      "persisting on question 2949\n",
      "persisting on question 2999\n",
      "persisting on question 3049\n",
      "persisting on question 3099\n",
      "persisting on question 3149\n",
      "persisting on question 3199\n",
      "persisting on question 3249\n",
      "persisting on question 3299\n",
      "persisting on question 3349\n",
      "persisting on question 3399\n",
      "persisting on question 3449\n",
      "persisting on question 3499\n",
      "persisting on question 3549\n",
      "persisting on question 3599\n",
      "persisting on question 3649\n",
      "persisting on question 3699\n",
      "persisting on question 3749\n",
      "persisting on question 3799\n",
      "persisting on question 3849\n",
      "persisting on question 3899\n",
      "persisting on question 3949\n",
      "persisting on question 3999\n",
      "persisting on question 4049\n",
      "persisting on question 4099\n",
      "persisting on question 4149\n",
      "persisting on question 4199\n",
      "persisting on question 4249\n",
      "persisting on question 4299\n",
      "persisting on question 4349\n",
      "persisting on question 4399\n",
      "persisting on question 4449\n",
      "persisting on question 4499\n",
      "persisting on question 4549\n",
      "persisting on question 4599\n",
      "persisting on question 4649\n",
      "persisting on question 4699\n",
      "persisting on question 4749\n",
      "persisting on question 4799\n",
      "persisting on question 4849\n",
      "persisting on question 4899\n",
      "persisting on question 4949\n",
      "persisting on question 4999\n",
      "persisting on question 5049\n",
      "persisting on question 5099\n",
      "persisting on question 5149\n",
      "persisting on question 5199\n",
      "persisting on question 5249\n",
      "persisting on question 5299\n",
      "persisting on question 5349\n",
      "persisting on question 5399\n",
      "persisting on question 5449\n",
      "persisting on question 5499\n",
      "persisting on question 5549\n",
      "persisting on question 5599\n",
      "persisting on question 5649\n",
      "persisting on question 5699\n",
      "persisting on question 5749\n",
      "persisting on question 5799\n",
      "persisting on question 5849\n",
      "persisting on question 5899\n",
      "persisting on question 5949\n",
      "persisting on question 5999\n",
      "persisting on question 6049\n",
      "persisting on question 6099\n",
      "persisting on question 6149\n",
      "persisting on question 6199\n",
      "persisting on question 6249\n",
      "persisting on question 6299\n",
      "persisting on question 6349\n",
      "persisting on question 6399\n",
      "persisting on question 6449\n",
      "persisting on question 6499\n",
      "persisting on question 6549\n",
      "persisting on question 6599\n",
      "persisting on question 6649\n",
      "persisting on question 6699\n",
      "persisting on question 6749\n",
      "persisting on question 6799\n",
      "persisting on question 6849\n",
      "persisting on question 6899\n",
      "persisting on question 6949\n",
      "persisting on question 6999\n",
      "persisting on question 7049\n",
      "persisting on question 7099\n",
      "persisting on question 7149\n",
      "persisting on question 7199\n",
      "persisting on question 7249\n",
      "persisting on question 7299\n",
      "persisting on question 7349\n",
      "persisting on question 7399\n",
      "persisting on question 7449\n",
      "persisting on question 7499\n",
      "persisting on question 7549\n",
      "persisting on question 7599\n",
      "persisting on question 7649\n",
      "persisting on question 7699\n",
      "persisting on question 7749\n",
      "persisting on question 7799\n",
      "persisting on question 7849\n",
      "persisting on question 7899\n",
      "persisting on question 7949\n",
      "persisting on question 7999\n",
      "persisting on question 8049\n",
      "persisting on question 8099\n",
      "persisting on question 8149\n",
      "persisting on question 8199\n",
      "persisting on question 8249\n",
      "persisting on question 8299\n",
      "persisting on question 8349\n",
      "persisting on question 8399\n",
      "persisting on question 8449\n",
      "persisting on question 8499\n",
      "persisting on question 8549\n",
      "persisting on question 8599\n",
      "persisting on question 8649\n",
      "persisting on question 8699\n",
      "persisting on question 8749\n",
      "persisting on question 8799\n",
      "persisting on question 8849\n",
      "persisting on question 8899\n",
      "persisting on question 8949\n",
      "persisting on question 8999\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    if index % 50 == 49:\n",
    "        print(f'persisting on question {index}')\n",
    "        df_questions.to_json(os.path.join(\n",
    "            path_labeling, 'questions.json'), indent=4, orient='records')\n",
    "        \n",
    "    if row['Question_gpt_summary_original']:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prompt = prompt_question + 'Title: ' + row['Question_title'] + ' Body: ' + row['Question_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-4-32k',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an accurate summarizer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=150,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=10,\n",
    "            stream=False\n",
    "        )\n",
    "        content = response['choices'][0]['message']['content'].strip()\n",
    "        df_questions.at[index, 'Question_gpt_summary_original'] = content\n",
    "        df_questions.at[index, 'Question_gpt_summary'] = preprocess_text(content)\n",
    "    except Exception as e:\n",
    "        # output unsuccesful requests\n",
    "        print(f'{e} on question {row[\"Question_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling, 'questions.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert df_questions.shape[0] == df_questions[df_questions['Question_gpt_summary_original'].str.len() > 0].shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmy/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/Users/jimmy/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:404: MarkupResemblesLocatorWarning: The input looks more like a URL than markup. You may want to use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    content = preprocess_text(row['Question_title'], remove_code=True) + ' ' + preprocess_text(str(row['Question_body']), remove_code=True)\n",
    "    df_questions.at[index, 'Question_preprocessed_content'] = content\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling, 'questions.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmy/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Experiment 4\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_labeling, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    if pd.notna(row['Answer_body']):\n",
    "        df_questions.at[index, 'Answer_original_content'] = preprocess_text(row['Answer_body'])\n",
    "\n",
    "df_questions.to_json(os.path.join(path_labeling, 'questions.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "persisting on question 49\n",
      "persisting on question 99\n",
      "persisting on question 149\n",
      "persisting on question 199\n",
      "persisting on question 249\n",
      "persisting on question 299\n",
      "persisting on question 349\n",
      "persisting on question 399\n",
      "persisting on question 449\n",
      "persisting on question 499\n",
      "persisting on question 549\n",
      "persisting on question 599\n",
      "persisting on question 649\n",
      "persisting on question 699\n",
      "persisting on question 749\n",
      "persisting on question 799\n",
      "persisting on question 849\n",
      "persisting on question 899\n",
      "persisting on question 949\n",
      "persisting on question 999\n",
      "persisting on question 1049\n",
      "persisting on question 1099\n",
      "persisting on question 1149\n",
      "persisting on question 1199\n",
      "persisting on question 1249\n",
      "persisting on question 1299\n",
      "persisting on question 1349\n",
      "persisting on question 1399\n",
      "persisting on question 1449\n",
      "persisting on question 1499\n",
      "persisting on question 1549\n",
      "persisting on question 1599\n",
      "persisting on question 1649\n",
      "persisting on question 1699\n",
      "persisting on question 1749\n",
      "persisting on question 1799\n",
      "persisting on question 1849\n",
      "persisting on question 1899\n",
      "persisting on question 1949\n",
      "persisting on question 1999\n",
      "persisting on question 2049\n",
      "persisting on question 2099\n",
      "persisting on question 2149\n",
      "persisting on question 2199\n",
      "persisting on question 2249\n",
      "persisting on question 2299\n",
      "persisting on question 2349\n",
      "persisting on question 2399\n",
      "persisting on question 2449\n",
      "persisting on question 2499\n",
      "persisting on question 2549\n",
      "persisting on question 2599\n",
      "persisting on question 2649\n",
      "persisting on question 2699\n",
      "persisting on question 2749\n",
      "persisting on question 2799\n",
      "persisting on question 2849\n",
      "persisting on question 2899\n",
      "persisting on question 2949\n",
      "persisting on question 2999\n",
      "persisting on question 3049\n",
      "persisting on question 3099\n",
      "persisting on question 3149\n",
      "persisting on question 3199\n",
      "persisting on question 3249\n",
      "persisting on question 3299\n",
      "persisting on question 3349\n",
      "persisting on question 3399\n",
      "persisting on question 3449\n",
      "persisting on question 3499\n",
      "persisting on question 3549\n",
      "persisting on question 3599\n",
      "persisting on question 3649\n",
      "persisting on question 3699\n",
      "persisting on question 3749\n",
      "persisting on question 3799\n",
      "persisting on question 3849\n",
      "persisting on question 3899\n",
      "persisting on question 3949\n",
      "persisting on question 3999\n",
      "persisting on question 4049\n",
      "persisting on question 4099\n",
      "persisting on question 4149\n",
      "persisting on question 4199\n",
      "persisting on question 4249\n",
      "persisting on question 4299\n",
      "persisting on question 4349\n",
      "persisting on question 4399\n",
      "persisting on question 4449\n",
      "persisting on question 4499\n",
      "persisting on question 4549\n",
      "persisting on question 4599\n",
      "persisting on question 4649\n",
      "persisting on question 4699\n",
      "persisting on question 4749\n",
      "persisting on question 4799\n",
      "persisting on question 4849\n",
      "persisting on question 4899\n",
      "persisting on question 4949\n",
      "persisting on question 4999\n",
      "persisting on question 5049\n",
      "persisting on question 5099\n",
      "persisting on question 5149\n",
      "persisting on question 5199\n",
      "persisting on question 5249\n",
      "persisting on question 5299\n",
      "persisting on question 5349\n",
      "persisting on question 5399\n",
      "persisting on question 5449\n",
      "persisting on question 5499\n",
      "persisting on question 5549\n",
      "persisting on question 5599\n",
      "persisting on question 5649\n",
      "persisting on question 5699\n",
      "persisting on question 5749\n",
      "persisting on question 5799\n",
      "persisting on question 5849\n",
      "persisting on question 5899\n",
      "persisting on question 5949\n",
      "persisting on question 5999\n",
      "persisting on question 6049\n",
      "persisting on question 6099\n",
      "persisting on question 6149\n",
      "persisting on question 6199\n",
      "persisting on question 6249\n",
      "persisting on question 6299\n",
      "persisting on question 6349\n",
      "persisting on question 6399\n",
      "persisting on question 6449\n",
      "persisting on question 6499\n",
      "persisting on question 6549\n",
      "persisting on question 6599\n",
      "persisting on question 6649\n",
      "persisting on question 6699\n",
      "persisting on question 6749\n",
      "persisting on question 6799\n",
      "persisting on question 6849\n",
      "persisting on question 6899\n",
      "persisting on question 6949\n",
      "persisting on question 6999\n",
      "persisting on question 7049\n",
      "persisting on question 7099\n",
      "persisting on question 7149\n",
      "persisting on question 7199\n",
      "persisting on question 7249\n",
      "persisting on question 7299\n",
      "persisting on question 7349\n",
      "persisting on question 7399\n",
      "persisting on question 7449\n",
      "persisting on question 7499\n",
      "persisting on question 7549\n",
      "persisting on question 7599\n",
      "persisting on question 7649\n",
      "persisting on question 7699\n",
      "persisting on question 7749\n",
      "persisting on question 7799\n",
      "persisting on question 7849\n",
      "persisting on question 7899\n",
      "persisting on question 7949\n",
      "persisting on question 7999\n",
      "persisting on question 8049\n",
      "persisting on question 8099\n",
      "persisting on question 8149\n",
      "persisting on question 8199\n",
      "persisting on question 8249\n",
      "persisting on question 8299\n",
      "persisting on question 8349\n",
      "persisting on question 8399\n",
      "persisting on question 8449\n",
      "persisting on question 8499\n",
      "persisting on question 8549\n",
      "persisting on question 8599\n",
      "persisting on question 8649\n",
      "persisting on question 8699\n",
      "persisting on question 8749\n",
      "persisting on question 8799\n",
      "persisting on question 8849\n",
      "persisting on question 8899\n",
      "persisting on question 8949\n",
      "persisting on question 8999\n"
     ]
    }
   ],
   "source": [
    "# Experiment 5\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    if index % 50 == 49:\n",
    "        print(f'persisting on question {index}')\n",
    "        df_questions.to_json(os.path.join(\n",
    "            path_labeling, 'questions.json'), indent=4, orient='records')\n",
    "    \n",
    "    if pd.isna(row['Answer_body']) or row['Answer_gpt_summary_original']:\n",
    "        continue\n",
    "\n",
    "    try:\n",
    "        prompt = prompt_answer + 'Challenge: ' + row['Question_gpt_summary_original'] + ' Discussion: ' + row['Answer_body'] + '###\\n'\n",
    "        response = retry_with_backoff(\n",
    "            openai.ChatCompletion.create,\n",
    "            model='gpt-4-32k',\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an accurate summarizer.\"},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            temperature=0,\n",
    "            max_tokens=150,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=10,\n",
    "            stream=False\n",
    "        )\n",
    "        content = response['choices'][0]['message']['content'].strip()\n",
    "        df_questions.at[index, 'Answer_gpt_summary_original'] = content\n",
    "        df_questions.at[index, 'Answer_gpt_summary'] = preprocess_text(content)\n",
    "    except Exception as e:\n",
    "        # output unsuccesful requests\n",
    "        print(f'{e} on question {row[\"Question_link\"]}')\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling, 'questions.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jimmy/anaconda3/lib/python3.10/site-packages/bs4/__init__.py:435: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Experiment 6\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_labeling, 'questions.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    if pd.notna(row['Answer_body']):\n",
    "        df_questions.at[index, 'Answer_preprocessed_content'] = preprocess_text(row['Answer_body'], remove_code=True)\n",
    "\n",
    "df_questions.to_json(os.path.join(path_labeling, 'questions.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9001"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_questions = pd.read_json(os.path.join(path_labeling, 'questions.json'))\n",
    "\n",
    "# output the number of asset-management-related Q&A questions\n",
    "len(df_questions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size is based on the recommendation from https://www.calculator.net/sample-size-calculator.html\n",
    "\n",
    "sample_size = 369\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'questions.json'))\n",
    "\n",
    "df_sample = df_questions[~df_questions['Answer_body'].isna()].sample(n=sample_size, random_state=42)\n",
    "\n",
    "df_sample.to_json(os.path.join(\n",
    "    path_labeling, 'sample.json'), indent=4, orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
