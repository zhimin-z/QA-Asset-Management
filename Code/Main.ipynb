{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_stack_overflow = os.path.join(path_dataset, 'Stack Overflow')\n",
    "path_tool_specific = os.path.join(path_dataset, 'Tool-specific')\n",
    "path_labeling = os.path.join(path_dataset, 'Labeling')  \n",
    "\n",
    "if not os.path.exists(path_dataset):\n",
    "    os.makedirs(path_dataset)\n",
    "\n",
    "if not os.path.isdir(path_stack_overflow):\n",
    "    os.mkdir(path_stack_overflow)\n",
    "\n",
    "if not os.path.isdir(path_tool_specific):\n",
    "    os.mkdir(path_tool_specific)\n",
    "\n",
    "if not os.path.isdir(path_labeling):\n",
    "    os.mkdir(path_labeling)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_tag_mapping = {\n",
    "    'Amazon SageMaker': {'amazon-sagemaker', 'amazon-sagemaker-experiments', 'amazon-sagemaker-studio', 'amz-sagemaker-distributed-training', 'amazon-sagemaker-debugger', 'amazon-sagemaker-clarify', 'amazon-sagemaker-compilers', 'amazon-sagemaker-experiments', 'amazon-sagemaker-neo'},\n",
    "    'Azure Machine Learning': {'azureml-python-sdk', 'azuremlsdk', 'azure-machine-learning-service', 'azure-machine-learning-studio', 'azure-machine-learning-workbench', 'azure-ml-pipelines', 'azure-ml-component'},\n",
    "    'ClearML': {'clearml'},\n",
    "    'Comet': {'comet-ml'},\n",
    "    'DVC': {'dvc'},\n",
    "    'Kedro': {'kedro'},\n",
    "    'MLflow': {'mlflow'},\n",
    "    'MLRun': {'mlrun'},\n",
    "    'Neptune': {'neptune', 'neptune-python-utils'},\n",
    "    'Optuna': {'optuna'},\n",
    "    'Sacred': {'python-sacred'},\n",
    "    'Vertex AI': {'google-cloud-vertex-ai', 'vertex-ai-pipeline'},\n",
    "    'Weights & Biases': {'wandb'}\n",
    "}\n",
    "\n",
    "tool_no_accepted_answer = {\n",
    "    'Domino', \n",
    "    'DVC', \n",
    "    'Guild AI\"', \n",
    "    'MLflow', \n",
    "    'Polyaxon', \n",
    "    'SigOpt'\n",
    "}\n",
    "\n",
    "regex_tag = r'<(.*?)>'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6768"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create tag collection\n",
    "tool_tags = set()\n",
    "for key, value in tool_tag_mapping.items():\n",
    "    tool_tags = tool_tags.union(value)\n",
    "\n",
    "# create a map from tag to tool\n",
    "tag2tool = dict()\n",
    "for key, value in tool_tag_mapping.items():\n",
    "    for elem in value:\n",
    "        tag2tool.setdefault(elem, key)\n",
    "\n",
    "df = pd.read_csv(os.path.join(path_stack_overflow, 'original.csv'))\n",
    "\n",
    "# split tags\n",
    "df['Question_valid_tags'] = [[] for _ in df.index]\n",
    "for index, row in df.iterrows():\n",
    "    tags = re.findall(regex_tag, row['Question_tags'])\n",
    "    df.at[index, 'Question_valid_tags'] = list(tool_tags.intersection(tags))\n",
    "    df.at[index, 'Question_tag_count'] = len(tags)\n",
    "    if pd.isna(row['Question_body']):\n",
    "        df.drop(index, inplace=True)\n",
    "\n",
    "# exclude Stack Overflow posts with unrelated tags\n",
    "df = df[df['Question_valid_tags'].map(len) > 0]\n",
    "\n",
    "df['Tools'] = [[] for _ in df.index]\n",
    "# extract Stack Overflow post collection with multiple tags based on the tool map\n",
    "for index, row in df.iterrows():\n",
    "    tags = set()\n",
    "    for tag in row['Question_valid_tags']:\n",
    "        tags.add(tag2tool[tag])\n",
    "    df.at[index, 'Tools'] = list(tags)\n",
    "\n",
    "df['Question_self_closed'] = df['Poster_id'] == df['Answerer_id']\n",
    "df['Question_link'] = df['Question_id'].apply(lambda x: f'https://stackoverflow.com/questions/{x}')\n",
    "\n",
    "del df['Poster_id']\n",
    "del df['Answerer_id']\n",
    "del df['Question_id']\n",
    "del df['Question_tags']\n",
    "del df['Question_valid_tags']\n",
    "\n",
    "df.to_json(os.path.join(path_stack_overflow, 'filtered.json'), indent=4, orient='records')\n",
    "len(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to scrape the posts from the tool-specific discussion fora\n",
    "\n",
    "import requests\n",
    "\n",
    "def scrape_post(base_url, page_suffix, file_name):\n",
    "    page = -1\n",
    "    posts = pd.DataFrame()\n",
    "    \n",
    "    post_url_lst = set()\n",
    "\n",
    "    while True:\n",
    "        page = page + 1\n",
    "        page_url = base_url + page_suffix + str(page)\n",
    "        topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "        for topic in topic_list['topics']:\n",
    "            post_url = base_url + 't/' + topic['slug'] + '/' + str(topic['id'])\n",
    "                \n",
    "            if post_url in post_url_lst:\n",
    "                continue\n",
    "            \n",
    "            post_url_lst.add(post_url)\n",
    "\n",
    "            post = {}\n",
    "            post['Question_title'] = topic['title']\n",
    "            post['Question_link'] = post_url\n",
    "            post['Question_created_time'] = topic['created_at']\n",
    "            post['Question_comment_count'] = topic['posts_count'] - 1\n",
    "            post['Question_score_count'] = topic['like_count']\n",
    "            post['Question_view_count'] = topic['views']\n",
    "            \n",
    "            comments = requests.get(post_url + '.json').json()['post_stream']['posts']\n",
    "            post['Question_body'] = comments[0]['cooked']\n",
    "            post['Question_closed_time'] = np.nan\n",
    "            post['Comment_body'] = np.nan\n",
    "            post[\"Question_self_closed\"] = np.nan\n",
    "            \n",
    "            if topic['has_accepted_answer']:\n",
    "                for comment in comments[1:]:\n",
    "                    if comment['accepted_answer']:\n",
    "                        post['Question_closed_time'] = comment['created_at']\n",
    "                        post['Comment_body'] = comment['cooked']\n",
    "                        post['Question_self_closed'] = comment['username'] == comments[0]['username']\n",
    "                        break\n",
    "            \n",
    "            post = pd.DataFrame([post])\n",
    "            posts = pd.concat([posts, post], ignore_index=True)\n",
    "            time.sleep(5)\n",
    "\n",
    "        if 'more_topics_url' not in topic_list.keys():\n",
    "            break\n",
    "    \n",
    "    posts.to_json(os.path.join(path_tool_specific, file_name + '.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from DVC\n",
    "base_url = 'https://discuss.dvc.org/'\n",
    "page_suffix = 'c/questions/9.json?page='\n",
    "file_name = 'DVC'\n",
    "scrape_post(base_url, page_suffix, file_name)\n",
    "\n",
    "# scrape posts from Guild AI\n",
    "base_url = 'https://my.guild.ai/'\n",
    "page_suffix = 'c/troubleshooting/6.json?page='\n",
    "file_name = 'Guild AI'\n",
    "scrape_post(base_url, page_suffix, file_name)\n",
    "\n",
    "# scrape posts from SigOpt\n",
    "base_url = 'https://community.sigopt.com/'\n",
    "page_suffix = 'c/general-discussion/9.json?page='\n",
    "file_name = 'SigOpt'\n",
    "scrape_post(base_url, page_suffix, file_name)\n",
    "\n",
    "# scrape posts from Weights & Biases\n",
    "base_url = 'https://community.wandb.ai/'\n",
    "page_suffix = 'c/w-b-support/36.json?page='\n",
    "file_name = 'Weights & Biases'\n",
    "scrape_post(base_url, page_suffix, file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "path_code = 'Scrape'\n",
    "\n",
    "subprocess.run(['python', os.path.join(path_code, 'Amazon SageMaker.py')])\n",
    "subprocess.run(['python', os.path.join(path_code, 'Azure Machine Learning.py')])\n",
    "subprocess.run(['python', os.path.join(path_code, 'Domino.py')])\n",
    "subprocess.run(['python', os.path.join(path_code, 'MLflow.py')])\n",
    "subprocess.run(['python', os.path.join(path_code, 'Polyaxon.py')])\n",
    "subprocess.run(['python', os.path.join(path_code, 'Vertex AI.py')])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5452\n"
     ]
    }
   ],
   "source": [
    "# create question dataset\n",
    "\n",
    "import glob\n",
    "\n",
    "df_questions_ts = pd.DataFrame()\n",
    "\n",
    "for file_name in glob.glob(os.path.join(path_tool_specific, '*.json')):\n",
    "    posts = pd.read_json(file_name)\n",
    "    tool_name = os.path.split(file_name)[1].split('.')[0]\n",
    "    posts['Tools'] = [[tool_name] for _ in posts.index]\n",
    "    df_questions_ts = pd.concat([df_questions_ts, posts], ignore_index=True)\n",
    "\n",
    "print(df_questions_ts.shape[0])\n",
    "\n",
    "df_question_so = pd.read_json(os.path.join(path_stack_overflow, 'filtered.json'))\n",
    "\n",
    "df_question_so['Platform'] = 'Stack Overflow'\n",
    "df_questions_ts['Platform'] = 'Tool-specific'\n",
    "\n",
    "df_questions = pd.concat([df_question_so, df_questions_ts], ignore_index=True)\n",
    "df_questions.to_json(os.path.join(path_labeling, 'questions.json'), indent=4, orient='records')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
