{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from contextualized_topic_models.evaluation.measures import InvertedRBO, TopicDiversity, CoherenceCV, CoherenceNPMI, CoherenceUMASS, CoherenceUCI\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_so = os.path.join(path_dataset, 'Stack Overflow')\n",
    "path_ts = os.path.join(path_dataset, 'Tool-specific Others')\n",
    "path_labeling = os.path.join(path_dataset, 'Labeling')\n",
    "\n",
    "path_so_raw = os.path.join(path_so, 'Raw')\n",
    "path_ts_raw = os.path.join(path_ts, 'Raw')\n",
    "path_so_filtered = os.path.join(path_so, 'Filtered')\n",
    "path_ts_filtered = os.path.join(path_ts, 'Filtered')\n",
    "    \n",
    "if not os.path.exists(path_dataset):\n",
    "    os.makedirs(path_dataset)\n",
    "\n",
    "if not os.path.isdir(path_so):\n",
    "    os.mkdir(path_so)\n",
    "\n",
    "if not os.path.isdir(path_ts):\n",
    "    os.mkdir(path_ts)\n",
    "\n",
    "if not os.path.isdir(path_labeling):\n",
    "    os.mkdir(path_labeling)\n",
    "\n",
    "if not os.path.isdir(path_so_raw):\n",
    "    os.mkdir(path_so_raw)\n",
    "\n",
    "if not os.path.isdir(path_ts_raw):\n",
    "    os.mkdir(path_ts_raw)\n",
    "\n",
    "if not os.path.isdir(path_so_filtered):\n",
    "    os.mkdir(path_so_filtered)\n",
    "\n",
    "if not os.path.isdir(path_ts_filtered):\n",
    "    os.mkdir(path_ts_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool2tag = {\n",
    "    'Amazon SageMaker': {'amazon-sagemaker', 'amazon-sagemaker-experiments', 'amazon-sagemaker-studio'},\n",
    "    'Azure Machine Learning': {'azure-machine-learning-service', 'azure-machine-learning-studio', 'azure-machine-learning-workbench'},\n",
    "    'ClearML': {'clearml'},\n",
    "    'Comet': {'comet-ml'},\n",
    "    'DVC': {'dvc'},\n",
    "    'Kedro': {'kedro'},\n",
    "    'MLflow': {'mlflow'},\n",
    "    'MLRun': {'mlrun'},\n",
    "    'Neptune': {'neptune'},\n",
    "    'Sacred': {'python-sacred'},\n",
    "    'Vertex AI': {'google-cloud-vertex-ai'},\n",
    "    'Weights & Biases': {'wandb'}\n",
    "}\n",
    "\n",
    "tools_keywords = {\n",
    "    'Amazon SageMaker': ['amazon sagemaker', 'aws sagemaker', 'sagemaker'],\n",
    "    'Azure Machine Learning': ['azureml', 'azure ml', 'azure machine learning'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'Comet': ['comet'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild ai'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Vertex AI': ['vertex ai'],\n",
    "    'Weights & Biases': ['weights & biases', 'weights and biases', 'wandb']\n",
    "}\n",
    "\n",
    "ignore_tools = {\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mzhiminy\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login(key=os.getenv('WANDB_API_KEY'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# function to scrape the posts from the tool-specific discussion fora\n",
    "\n",
    "def scrape_post(base_url, page_suffix, file_name):\n",
    "    page = -1\n",
    "    post_list = []\n",
    "    \n",
    "    while True:\n",
    "        page = page + 1\n",
    "        page_url = base_url + page_suffix + str(page)\n",
    "        topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "        for topic in topic_list['topics']:\n",
    "            post_url = base_url + 't/' + \\\n",
    "            topic['slug'] + '/' + str(topic['id'])\n",
    "\n",
    "            post = {}\n",
    "            post['Question_title'] = topic['title']\n",
    "            post['Question_link'] = post_url\n",
    "            post['Question_creation_time'] = topic['created_at']\n",
    "            post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "            post['Question_score'] = topic['like_count']\n",
    "            post['Question_view_count'] = topic['views']\n",
    "            post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "            comments = requests.get(post_url + '.json').json()['post_stream']['posts']\n",
    "            post['Question_body'] = comments[0]['cooked']\n",
    "            post['Answer_list'] = comments[1:]\n",
    "            post_list.append(post)\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "        if 'more_topics_url' not in topic_list.keys():\n",
    "            break\n",
    "        \n",
    "    with open(os.path.join(path_ts_raw, file_name), 'w') as outfile:\n",
    "        json_post_list = json.dumps(post_list, indent='\\t')\n",
    "        outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from Guild AI\n",
    "base_url = 'https://my.guild.ai/'\n",
    "page_suffix = 'c/troubleshooting/6.json?page='\n",
    "file_name = 'Guild AI.json'\n",
    "post_list = scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from Weights & Biases\n",
    "base_url = 'https://community.wandb.ai/'\n",
    "page_suffix = 'c/w-b-support/36.json?page='\n",
    "file_name = 'Weights & Biases.json'\n",
    "post_list = scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from SigOpt\n",
    "base_url = 'https://community.sigopt.com/'\n",
    "page_suffix = 'c/general-discussion/9.json?page='\n",
    "file_name = 'SigOpt.json'\n",
    "post_list = scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from DVC\n",
    "base_url = 'https://discuss.dvc.org/'\n",
    "page_suffix = 'c/questions/9.json?page='\n",
    "file_name = 'DVC.json'\n",
    "post_list = scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts_questions = pd.DataFrame()\n",
    "\n",
    "# exclude tool-specific posts with negative upvote count\n",
    "for file_name in glob.glob(os.path.join(path_ts_raw, '*.json')):\n",
    "    repos = pd.read_json(file_name)\n",
    "    if 'Question_score' in repos.columns:\n",
    "        repos = repos[repos['Question_score'] > -1]\n",
    "    repos['Tool'] = os.path.split(file_name)[1].split('.')[0]\n",
    "    df_ts_questions = pd.concat([df_ts_questions, repos], ignore_index=True)\n",
    "    \n",
    "df_ts_answers = df_ts_questions[df_ts_questions['Question_answer_count'] > 0]\n",
    "for tool in df_ts_answers['Tool'].unique().tolist():\n",
    "    number_accepted_answer = df_ts_answers[df_ts_answers['Tool']\n",
    "                                            == tool]['Question_has_accepted_answer'].sum()\n",
    "    if number_accepted_answer > 0:\n",
    "        df_ts_answers = df_ts_answers.drop(df_ts_answers[(df_ts_answers['Tool'] == tool) & (\n",
    "            df_ts_answers['Question_has_accepted_answer'] == False)].index)\n",
    "\n",
    "df_ts_questions.to_json(os.path.join(path_ts_filtered,\n",
    "              'questions.json'), orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#Question</th>\n",
       "      <th>#Answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>528</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>1435</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DVC</td>\n",
       "      <td>315</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Domino</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guild AI</td>\n",
       "      <td>115</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLFlow</td>\n",
       "      <td>280</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>43</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>297</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>583</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Tool  #Question  #Answered\n",
       "0        Amazon SageMaker        528        167\n",
       "1  Azure Machine Learning       1435        343\n",
       "2                     DVC        315        300\n",
       "3                  Domino         13          4\n",
       "4                Guild AI        115        108\n",
       "5                  MLFlow        280        143\n",
       "6                Polyaxon         43         34\n",
       "7                  SigOpt         15          7\n",
       "8               Vertex AI        297         32\n",
       "9        Weights & Biases        583         92"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only posts with at least one answer\n",
    "df_ts_question_summary = df_ts_questions.groupby(\n",
    "    'Tool').count()['Question_title'].reset_index()\n",
    "df_ts_answer_summary = df_ts_answers.groupby(\n",
    "    'Tool').count()['Question_title'].reset_index()\n",
    "\n",
    "df_ts_question_summary.columns = ['Tool', '#Question']\n",
    "df_ts_answer_summary.columns = ['Tool', '#Answered']\n",
    "\n",
    "df_summary = pd.merge(df_ts_question_summary, df_ts_answer_summary, on='Tool')\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#Question</th>\n",
       "      <th>#Answered</th>\n",
       "      <th>#Sample Question</th>\n",
       "      <th>#Sample Answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>528</td>\n",
       "      <td>167</td>\n",
       "      <td>223</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>1435</td>\n",
       "      <td>343</td>\n",
       "      <td>304</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DVC</td>\n",
       "      <td>315</td>\n",
       "      <td>300</td>\n",
       "      <td>174</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Domino</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guild AI</td>\n",
       "      <td>115</td>\n",
       "      <td>108</td>\n",
       "      <td>89</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLFlow</td>\n",
       "      <td>280</td>\n",
       "      <td>143</td>\n",
       "      <td>163</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>43</td>\n",
       "      <td>34</td>\n",
       "      <td>39</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>297</td>\n",
       "      <td>32</td>\n",
       "      <td>168</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>583</td>\n",
       "      <td>92</td>\n",
       "      <td>232</td>\n",
       "      <td>75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Tool  #Question  #Answered  #Sample Question  \\\n",
       "0        Amazon SageMaker        528        167               223   \n",
       "1  Azure Machine Learning       1435        343               304   \n",
       "2                     DVC        315        300               174   \n",
       "3                  Domino         13          4                13   \n",
       "4                Guild AI        115        108                89   \n",
       "5                  MLFlow        280        143               163   \n",
       "6                Polyaxon         43         34                39   \n",
       "7                  SigOpt         15          7                15   \n",
       "8               Vertex AI        297         32               168   \n",
       "9        Weights & Biases        583         92               232   \n",
       "\n",
       "   #Sample Answered  \n",
       "0               117  \n",
       "1               182  \n",
       "2               169  \n",
       "3                 4  \n",
       "4                85  \n",
       "5               105  \n",
       "6                32  \n",
       "7                 7  \n",
       "8                30  \n",
       "9                75  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # After having the population for each tool and discussion channel, we then find out the minimum number of necessary samples with the [calculator](https://www.calculator.net/sample-size-calculator.html).\n",
    "# df_summary = pd.read_csv(os.path.join(path_ts, 'summary.csv'))\n",
    "# df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ts = pd.read_json(os.path.join(\n",
    "#     path_ts_filtered, 'non_negative_scored.json'))\n",
    "# df_ts_answered = pd.read_json(os.path.join(\n",
    "#     path_ts_filtered, 'completed_non_negative_scored.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample tool-specific posts accordingly\n",
    "# df_question_samples = pd.DataFrame()\n",
    "# df_answer_samples = pd.DataFrame()\n",
    "\n",
    "# for index, row in df_summary.iterrows():\n",
    "#     df_question_sample = df_ts[df_ts['Tool'] == row['Tool']].sample(\n",
    "#         n=row['#Sample Question'], random_state=0)\n",
    "#     df_answer_sample = df_ts_answered[df_ts_answered['Tool'] == row['Tool']].sample(\n",
    "#         n=row['#Sample Answered'], random_state=0)\n",
    "#     df_question_samples = pd.concat(\n",
    "#         [df_question_samples, df_question_sample], ignore_index=True)\n",
    "#     df_answer_samples = pd.concat(\n",
    "#         [df_answer_samples, df_answer_sample], ignore_index=True)\n",
    "\n",
    "# df_question_samples.to_json(os.path.join(\n",
    "#     path_ts_sampled, 'questions.json'), indent=4, orient='records')\n",
    "# df_answer_samples.to_json(os.path.join(\n",
    "#     path_ts_sampled, 'answers.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concatenate tool-specific answered and unanswered posts\n",
    "# df_question_samples = pd.read_json(os.path.join(path_ts_sampled, 'questions.json'))\n",
    "# df_answer_samples = pd.read_json(os.path.join(path_ts_sampled, 'answers.json'))\n",
    "\n",
    "# df_question_samples.drop(['Question_topic', 'Question_tag', 'Answers'], axis=1, inplace=True)\n",
    "# df_answer_samples.drop(['Question_topic', 'Question_tag', 'Answers'], axis=1, inplace=True)\n",
    "\n",
    "# df_question_samples_ts = pd.merge(df_question_samples, df_answer_samples, on=df_question_samples.columns.tolist(), how='outer')\n",
    "# df_question_samples_ts.to_json(os.path.join(path_labeling, 'questions_ts.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question_id</th>\n",
       "      <th>Question_title</th>\n",
       "      <th>Question_body</th>\n",
       "      <th>Question_answer_count</th>\n",
       "      <th>Question_comment_count</th>\n",
       "      <th>Question_creation_time</th>\n",
       "      <th>Question_favorite_count</th>\n",
       "      <th>Question_score</th>\n",
       "      <th>Question_tags</th>\n",
       "      <th>Question_view_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Owner_up_votes</th>\n",
       "      <th>Owner_down_votes</th>\n",
       "      <th>Owner_views</th>\n",
       "      <th>Answer_body</th>\n",
       "      <th>Answer_comment_count</th>\n",
       "      <th>Answer_creation_time</th>\n",
       "      <th>Answer_score</th>\n",
       "      <th>Owner_location</th>\n",
       "      <th>Question_last_edit_time</th>\n",
       "      <th>Answer_last_edit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70098779</td>\n",
       "      <td>How to connect to MLFlow tracking server that ...</td>\n",
       "      <td>&lt;p&gt;I want to connect to remote tracking server...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-11-24 15:30:11.310000+00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[authorization, tracking, mlflow]</td>\n",
       "      <td>2102</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://mlflow.org/docs/latest/tra...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-11-24 17:01:13.483000+00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38927230</td>\n",
       "      <td>Panda AssertionError columns passed, passed da...</td>\n",
       "      <td>&lt;p&gt;I am working on Azure ML implementation on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-12 22:23:17.197000+00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[python, pandas, dataframe, nltk, azure-machin...</td>\n",
       "      <td>48200</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>55</td>\n",
       "      <td>339</td>\n",
       "      <td>&lt;p&gt;Try this:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;dataframe_outpu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-08-12 22:26:09.603000+00:00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Toronto, ON, Canada</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68773463</td>\n",
       "      <td>AccessDeniedException on sagemaker:CreateDomai...</td>\n",
       "      <td>&lt;p&gt;I am trying to use the AWS SageMaker Studio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-08-13 13:49:08.683000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazon-web-services, amazon-iam, amazon-sagem...</td>\n",
       "      <td>366</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67701971</td>\n",
       "      <td>How to label a text with multiple paragraphs i...</td>\n",
       "      <td>&lt;p&gt;I was trying setup a single label labeling ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-05-26 09:16:33.420000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[amazon-web-services, text, amazon-sagemaker, ...</td>\n",
       "      <td>161</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zürich, Suïssa</td>\n",
       "      <td>2021-05-26 11:54:00.030000+00:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48398509</td>\n",
       "      <td>How to Invoke AWS Sagemaker API with c# .NET?</td>\n",
       "      <td>&lt;p&gt;I have trained and deployed a model in AWS ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-23 09:42:48.607000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[c#, asp.net, amazon-web-services, aws-sdk, am...</td>\n",
       "      <td>743</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pune India</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Question_id                                     Question_title  \\\n",
       "0     70098779  How to connect to MLFlow tracking server that ...   \n",
       "1     38927230  Panda AssertionError columns passed, passed da...   \n",
       "2     68773463  AccessDeniedException on sagemaker:CreateDomai...   \n",
       "3     67701971  How to label a text with multiple paragraphs i...   \n",
       "4     48398509      How to Invoke AWS Sagemaker API with c# .NET?   \n",
       "\n",
       "                                       Question_body  Question_answer_count  \\\n",
       "0  <p>I want to connect to remote tracking server...                      1   \n",
       "1  <p>I am working on Azure ML implementation on ...                      1   \n",
       "2  <p>I am trying to use the AWS SageMaker Studio...                      1   \n",
       "3  <p>I was trying setup a single label labeling ...                      0   \n",
       "4  <p>I have trained and deployed a model in AWS ...                      1   \n",
       "\n",
       "   Question_comment_count           Question_creation_time  \\\n",
       "0                       0 2021-11-24 15:30:11.310000+00:00   \n",
       "1                       0 2016-08-12 22:23:17.197000+00:00   \n",
       "2                       0 2021-08-13 13:49:08.683000+00:00   \n",
       "3                       2 2021-05-26 09:16:33.420000+00:00   \n",
       "4                       0 2018-01-23 09:42:48.607000+00:00   \n",
       "\n",
       "   Question_favorite_count  Question_score  \\\n",
       "0                      1.0               1   \n",
       "1                      3.0               7   \n",
       "2                      NaN               0   \n",
       "3                      NaN               1   \n",
       "4                      NaN               0   \n",
       "\n",
       "                                       Question_tags  Question_view_count  \\\n",
       "0                  [authorization, tracking, mlflow]                 2102   \n",
       "1  [python, pandas, dataframe, nltk, azure-machin...                48200   \n",
       "2  [amazon-web-services, amazon-iam, amazon-sagem...                  366   \n",
       "3  [amazon-web-services, text, amazon-sagemaker, ...                  161   \n",
       "4  [c#, asp.net, amazon-web-services, aws-sdk, am...                  743   \n",
       "\n",
       "   ... Owner_up_votes Owner_down_votes  Owner_views  \\\n",
       "0  ...              0                0           11   \n",
       "1  ...            136               55          339   \n",
       "2  ...              0                0           11   \n",
       "3  ...             75               10          147   \n",
       "4  ...             34                1          124   \n",
       "\n",
       "                                         Answer_body  Answer_comment_count  \\\n",
       "0  <p><a href=\"https://mlflow.org/docs/latest/tra...                   2.0   \n",
       "1  <p>Try this:</p>\\n\\n<pre><code>dataframe_outpu...                   0.0   \n",
       "2                                                NaN                   NaN   \n",
       "3                                                NaN                   NaN   \n",
       "4                                                NaN                   NaN   \n",
       "\n",
       "              Answer_creation_time Answer_score       Owner_location  \\\n",
       "0 2021-11-24 17:01:13.483000+00:00          2.0                  NaN   \n",
       "1 2016-08-12 22:26:09.603000+00:00         13.0  Toronto, ON, Canada   \n",
       "2                              NaT          NaN                  NaN   \n",
       "3                              NaT          NaN       Zürich, Suïssa   \n",
       "4                              NaT          NaN           Pune India   \n",
       "\n",
       "           Question_last_edit_time  Answer_last_edit_time  \n",
       "0                              NaT                    NaT  \n",
       "1                              NaT                    NaT  \n",
       "2                              NaT                    NaT  \n",
       "3 2021-05-26 11:54:00.030000+00:00                    NaT  \n",
       "4                              NaT                    NaT  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(\n",
    "    path_so_raw, 'bq-results-20230201-032754-1675222092237.json'), lines=True)\n",
    "df['Question_tags'] = df['Question_tags'].str.split('|')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tag collection\n",
    "tags = set()\n",
    "for key, value in tool2tag.items():\n",
    "    tags = tags.union(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tags\n",
    "df['Question_valid_tags'] = [[] for _ in range(len(df))]\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'Question_valid_tags'] = list(tags.intersection(set(row['Question_tags'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts with at least 1 tags has 5130 in total.\n",
      "Posts with at least 2 tags has 220 in total.\n",
      "Posts with at least 3 tags has 18 in total.\n"
     ]
    }
   ],
   "source": [
    "# count post number with different tags\n",
    "arity = 0\n",
    "while True:\n",
    "    post_number = df[df['Question_valid_tags'].map(len) > arity].shape[0]\n",
    "    if post_number < 1:\n",
    "        break\n",
    "    arity = arity + 1\n",
    "    print(f'Posts with at least {arity} tags has {post_number} in total.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-8534f91f27f9>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid['Question_link'] = df_valid['Question_id'].apply(\n"
     ]
    }
   ],
   "source": [
    "# exclude Stack Overflow posts with unrelated tags\n",
    "df_valid = df[df['Question_valid_tags'].map(len) > 0]\n",
    "df_valid['Question_link'] = df_valid['Question_id'].apply(\n",
    "    lambda x: f'https://stackoverflow.com/questions/{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude Stack Overflow posts with negative upvote count\n",
    "df_qualified = df_valid[df_valid['Question_score'] > -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map from tag to tool\n",
    "tag2tool = dict()\n",
    "for key, value in tool2tag.items():\n",
    "    for elem in value:\n",
    "        tag2tool.setdefault(elem, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract Stack Overflow post collection with multiple tags based on the tool map\n",
    "for index, row in df_qualified.iterrows():\n",
    "    tags = set()\n",
    "    for tag in row['Question_valid_tags']:\n",
    "        tags.add(tag2tool[tag])\n",
    "    df_qualified.at[index, 'Question_valid_tags'] = sorted(list(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Amazon SageMaker, MLFlow]                 16\n",
       "[Azure Machine Learning, MLFlow]           11\n",
       "[Kedro, MLFlow]                             4\n",
       "[Azure Machine Learning, Kedro, MLFlow]     2\n",
       "[DVC, MLFlow]                               1\n",
       "[MLFlow, Sacred]                            1\n",
       "[Kedro, Neptune]                            1\n",
       "Name: Question_valid_tags, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how the posts with more than one tags look like\n",
    "df_multiply_tagged = df_qualified[df_qualified['Question_valid_tags'].map(\n",
    "    len) > 1]\n",
    "df_multiply_tagged['Question_valid_tags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-9867e953fe3c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_qualified.at[index, 'Tool'] = tags[0]\n"
     ]
    }
   ],
   "source": [
    "# create Stack Overflow post collection with exclusive tags\n",
    "multiply_tagged_posts_split = []\n",
    "df_qualified.assign(Tool='')\n",
    "\n",
    "for index, row in df_qualified.iterrows():\n",
    "    tags = row['Question_valid_tags']\n",
    "    df_qualified.at[index, 'Tool'] = tags[0]\n",
    "    if len(tags) > 1:\n",
    "        for tag in tags[1:]:\n",
    "            series = row.copy()\n",
    "            series['Tool'] = tag\n",
    "            multiply_tagged_posts_split.append(series)\n",
    "\n",
    "df_multiply_tagged_posts_split = pd.DataFrame(multiply_tagged_posts_split)\n",
    "df_qualified_exclusive_tagged = pd.concat(\n",
    "    [df_qualified, df_multiply_tagged_posts_split], ignore_index=True)\n",
    "del df_qualified_exclusive_tagged['Question_valid_tags']\n",
    "\n",
    "# keep Stack Overflow posts with accepted answers\n",
    "df_qualified_exclusive_tagged_completed = df_qualified_exclusive_tagged.dropna(\n",
    "    subset=['Answer_body'])\n",
    "\n",
    "df_qualified_exclusive_tagged.to_json(os.path.join(\n",
    "    path_so_filtered, 'questions.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#Question</th>\n",
       "      <th>#Answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>2233</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>1530</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClearML</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comet</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DVC</td>\n",
       "      <td>91</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kedro</td>\n",
       "      <td>149</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLFlow</td>\n",
       "      <td>551</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neptune</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sacred</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>341</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool  #Question  #Answered\n",
       "0         Amazon SageMaker       2233        737\n",
       "1   Azure Machine Learning       1530        586\n",
       "2                  ClearML         40         20\n",
       "3                    Comet         10          4\n",
       "4                      DVC         91         49\n",
       "5                    Kedro        149         60\n",
       "6                   MLFlow        551        129\n",
       "7                  Neptune          8          3\n",
       "8                   Sacred         10          7\n",
       "9                Vertex AI        341        112\n",
       "10        Weights & Biases         77         22"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_so_question_summary = df_qualified_exclusive_tagged.groupby(\n",
    "    'Tool').count()['Question_id'].reset_index()\n",
    "df_so_answer_summary = df_qualified_exclusive_tagged_completed.groupby(\n",
    "    'Tool').count()['Question_id'].reset_index()\n",
    "\n",
    "df_so_question_summary.columns = ['Tool', '#Question']\n",
    "df_so_answer_summary.columns = ['Tool', '#Answered']\n",
    "\n",
    "df_summary = pd.merge(df_so_question_summary, df_so_answer_summary, on='Tool')\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#Question</th>\n",
       "      <th>#Answered</th>\n",
       "      <th>#Sample Question</th>\n",
       "      <th>#Sample Answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>2233</td>\n",
       "      <td>737</td>\n",
       "      <td>328</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>1530</td>\n",
       "      <td>586</td>\n",
       "      <td>308</td>\n",
       "      <td>233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClearML</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "      <td>37</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comet</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DVC</td>\n",
       "      <td>91</td>\n",
       "      <td>49</td>\n",
       "      <td>74</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kedro</td>\n",
       "      <td>149</td>\n",
       "      <td>60</td>\n",
       "      <td>108</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLFlow</td>\n",
       "      <td>551</td>\n",
       "      <td>129</td>\n",
       "      <td>227</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neptune</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sacred</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>341</td>\n",
       "      <td>112</td>\n",
       "      <td>181</td>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "      <td>65</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool  #Question  #Answered  #Sample Question  \\\n",
       "0         Amazon SageMaker       2233        737               328   \n",
       "1   Azure Machine Learning       1530        586               308   \n",
       "2                  ClearML         40         20                37   \n",
       "3                    Comet         10          4                10   \n",
       "4                      DVC         91         49                74   \n",
       "5                    Kedro        149         60               108   \n",
       "6                   MLFlow        551        129               227   \n",
       "7                  Neptune          8          3                 8   \n",
       "8                   Sacred         10          7                10   \n",
       "9                Vertex AI        341        112               181   \n",
       "10        Weights & Biases         77         22                65   \n",
       "\n",
       "    #Sample Answered  \n",
       "0                253  \n",
       "1                233  \n",
       "2                 20  \n",
       "3                  4  \n",
       "4                 44  \n",
       "5                 53  \n",
       "6                 97  \n",
       "7                  3  \n",
       "8                  7  \n",
       "9                 87  \n",
       "10                21  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # After having the population for each tool and discussion channel, we then find out the minimum number of necessary samples with the [calculator](https://www.calculator.net/sample-size-calculator.html).\n",
    "# df_summary = pd.read_csv(os.path.join(path_so, 'summary.csv'))\n",
    "# df_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_qualified_exclusive_tagged = pd.read_json(os.path.join(\n",
    "#     path_so_filtered, 'questions.json'))\n",
    "# df_qualified_exclusive_tagged_completed = pd.read_json(os.path.join(\n",
    "#     path_so_filtered, 'answers.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sample Stack Overflow posts accordingly\n",
    "# df_question_samples = pd.DataFrame()\n",
    "# df_answer_samples = pd.DataFrame()\n",
    "\n",
    "# for index, row in df_summary.iterrows():\n",
    "#     df_question_sample = df_qualified_exclusive_tagged[df_qualified_exclusive_tagged['Tool'] == row['Tool']].sample(\n",
    "#         n=row['#Sample Question'], random_state=0)\n",
    "#     df_answer_sample = df_qualified_exclusive_tagged_completed[df_qualified_exclusive_tagged_completed['Tool'] == row['Tool']].sample(\n",
    "#         n=row['#Sample Answered'], random_state=0)\n",
    "#     df_question_samples = pd.concat(\n",
    "#         [df_question_samples, df_question_sample], ignore_index=True)\n",
    "#     df_answer_samples = pd.concat(\n",
    "#         [df_answer_samples, df_answer_sample], ignore_index=True)\n",
    "\n",
    "# df_question_samples.to_json(os.path.join(\n",
    "#     path_so_sampled, 'questions.json'), indent=4, orient='records')\n",
    "# df_answer_samples.to_json(os.path.join(\n",
    "#     path_so_sampled, 'answers.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concatenate Stack Overflow answered and unanswered posts\n",
    "# df_question_samples = pd.read_json(os.path.join(path_so_sampled, 'questions.json'))\n",
    "# df_answer_samples = pd.read_json(os.path.join(path_so_sampled, 'answers.json'))\n",
    "# df_question_samples_so = pd.merge(df_question_samples, df_answer_samples, on=df_question_samples.columns.tolist(), how='outer')\n",
    "# df_question_samples_so.to_json(os.path.join(path_labeling, 'questions_so.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concatenate Stack Overflow and tool-specific posts\n",
    "# df_question_samples_ts = pd.read_json(os.path.join(path_labeling, 'questions_ts.json'))\n",
    "# df_question_samples_so = pd.read_json(os.path.join(path_labeling, 'questions_so.json'))\n",
    "\n",
    "# df_answer_samples_ts = pd.read_json(os.path.join(path_ts_sampled, 'answers.json'))\n",
    "# df_answer_samples_so = pd.read_json(os.path.join(path_so_sampled, 'answers.json'))\n",
    "\n",
    "# df_question_samples_all = pd.concat([df_question_samples_ts, df_question_samples_so], ignore_index=True)\n",
    "# df_answer_samples_all = pd.concat([df_answer_samples_ts, df_answer_samples_so], ignore_index=True)\n",
    "\n",
    "# df_question_samples_all.to_json(os.path.join(path_labeling, 'questions_all.json'), indent=4, orient='records')\n",
    "# df_answer_samples_all.to_json(os.path.join(path_labeling, 'answers_all.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine Stack Overflow and tool-specific fora posts\n",
    "df_question_so = pd.read_json(os.path.join(path_so_filtered, 'questions.json'))\n",
    "df_question_ts = pd.read_json(os.path.join(path_ts_filtered, 'questions.json'))\n",
    "\n",
    "df_question_so['Forum'] = 'Stack Overflow'\n",
    "df_question_ts['Forum'] = 'Tool-specific'\n",
    "\n",
    "df_questions = pd.concat([df_question_so, df_question_ts], ignore_index=True)\n",
    "df_questions.to_json(os.path.join(path_labeling, 'questions_original.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add potential field to questions for later filling\n",
    "df_questions = pd.read_json(os.path.join(path_labeling, 'questions_original.json'))\n",
    "\n",
    "# Experiment 1: feed the original content to BerTopic\n",
    "df_questions['Question_original_content_preprocessed_text'] = ''\n",
    "\n",
    "# Experiment 2: feed the original content to text-davinci-003 model and get the generated summary, then feed the summary to BerTopic\n",
    "df_questions['Question_original_content_gpt_summary'] = ''\n",
    "\n",
    "# Experiment 3: feed the preprocessed content to BerTopic\n",
    "df_questions['Question_preprocessed_content'] = ''\n",
    "\n",
    "# Experiment 4: feed the preprocessed content to text-davinci-003 model and get the generated summary, then feed the summary to BerTopic\n",
    "df_questions['Question_preprocessed_content_gpt_summary'] = ''\n",
    "\n",
    "df_questions.to_json(os.path.join(path_labeling, 'questions_topic_modeling.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create summary field for all issues\n",
    "# question_prefill_so = pd.read_json(os.path.join(path_so_filtered, 'questions.json'))\n",
    "# question_prefill_ts = pd.read_json(os.path.join(path_ts_filtered, 'questions.json'))\n",
    "# question_prefill_so['Question_summary'] = ''\n",
    "# question_prefill_ts['Question_summary'] = ''\n",
    "# question_prefill_so.to_json(os.path.join(path_labeling, 'question_prefill_so.json'), indent=4, orient='records')\n",
    "# question_prefill_ts.to_json(os.path.join(path_labeling, 'question_prefill_ts.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stackoverflow.com/questions/61285248\n",
      "https://stackoverflow.com/questions/62993397\n",
      "https://stackoverflow.com/questions/68955197\n",
      "https://stackoverflow.com/questions/40907303\n",
      "https://stackoverflow.com/questions/73064569\n",
      "https://stackoverflow.com/questions/56046428\n",
      "https://stackoverflow.com/questions/63339703\n",
      "https://stackoverflow.com/questions/65577286\n",
      "https://stackoverflow.com/questions/51064366\n",
      "https://stackoverflow.com/questions/60957084\n",
      "https://stackoverflow.com/questions/68150444\n",
      "https://stackoverflow.com/questions/59762829\n",
      "https://stackoverflow.com/questions/62813017\n",
      "https://stackoverflow.com/questions/62836278\n",
      "https://stackoverflow.com/questions/69466354\n",
      "https://stackoverflow.com/questions/68489311\n",
      "https://stackoverflow.com/questions/67599026\n",
      "https://stackoverflow.com/questions/69721067\n",
      "https://stackoverflow.com/questions/68192602\n",
      "https://stackoverflow.com/questions/73085199\n",
      "https://stackoverflow.com/questions/62569747\n",
      "https://stackoverflow.com/questions/51088145\n",
      "https://stackoverflow.com/questions/71876157\n",
      "https://stackoverflow.com/questions/73462205\n",
      "https://stackoverflow.com/questions/73812159\n",
      "https://stackoverflow.com/questions/68397384\n",
      "https://stackoverflow.com/questions/69944447\n",
      "https://stackoverflow.com/questions/70428593\n",
      "https://stackoverflow.com/questions/70968412\n",
      "https://stackoverflow.com/questions/63204081\n",
      "https://stackoverflow.com/questions/63518174\n",
      "https://stackoverflow.com/questions/73650387\n",
      "https://stackoverflow.com/questions/62525490\n",
      "https://stackoverflow.com/questions/60418931\n",
      "https://stackoverflow.com/questions/67160576\n",
      "https://stackoverflow.com/questions/69147788\n",
      "https://stackoverflow.com/questions/57724414\n",
      "https://stackoverflow.com/questions/71395237\n",
      "https://stackoverflow.com/questions/73499320\n",
      "https://stackoverflow.com/questions/73624005\n",
      "https://stackoverflow.com/questions/72633246\n",
      "https://stackoverflow.com/questions/53695161\n",
      "https://stackoverflow.com/questions/73334915\n",
      "https://stackoverflow.com/questions/73836459\n",
      "https://stackoverflow.com/questions/67093041\n",
      "https://stackoverflow.com/questions/69534189\n",
      "https://stackoverflow.com/questions/63770171\n",
      "https://stackoverflow.com/questions/73838064\n",
      "https://stackoverflow.com/questions/63116338\n",
      "https://stackoverflow.com/questions/66561959\n",
      "https://stackoverflow.com/questions/68090119\n",
      "https://stackoverflow.com/questions/72734803\n",
      "https://stackoverflow.com/questions/68517516\n",
      "https://stackoverflow.com/questions/73823926\n",
      "https://stackoverflow.com/questions/58592206\n",
      "https://stackoverflow.com/questions/72653823\n",
      "https://stackoverflow.com/questions/73642527\n",
      "https://stackoverflow.com/questions/62261222\n",
      "https://stackoverflow.com/questions/70833499\n",
      "https://stackoverflow.com/questions/63492891\n",
      "https://stackoverflow.com/questions/70291455\n",
      "https://stackoverflow.com/questions/62962319\n",
      "https://stackoverflow.com/questions/62422682\n",
      "https://stackoverflow.com/questions/73761872\n",
      "https://stackoverflow.com/questions/59177264\n",
      "https://stackoverflow.com/questions/73064066\n",
      "https://stackoverflow.com/questions/73734047\n",
      "https://stackoverflow.com/questions/71652798\n",
      "https://stackoverflow.com/questions/73098371\n",
      "https://stackoverflow.com/questions/61239274\n",
      "https://stackoverflow.com/questions/60385064\n",
      "https://stackoverflow.com/questions/67650010\n",
      "https://stackoverflow.com/questions/63116338\n"
     ]
    }
   ],
   "source": [
    "# # manually preprocess the content of the issues and feed them into \"text-davinci-003\" for summary generation\n",
    "# for index, row in question_prefill_so.iterrows():\n",
    "#     if (len(str(row['Question_body'])) > 10000):\n",
    "#         print(row['Question_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://repost.aws/questions/QU6Ahf5zWZRZq63k1TOQ_48w/using-hugging-face-in-sagemaker-studio-as-part-of-a-project\n",
      "https://repost.aws/questions/QUVfbc_AsXRzaxAl69MMFlsQ/error-for-training-job-catboost-classification-model-error-message-type-error-cannot-convert-xxx-to-float\n",
      "https://repost.aws/questions/QU8UWsqxW8RbejgzLHYIFduA/inconsistent-keras-model-summary-output-shapes-on-aws-sage-maker-and-ec-2\n",
      "https://repost.aws/questions/QULAis68RtShua1Wg8A5EFXg/deploy-yol-ov-5-in-sagemaker-model-error-invoke-endpoint-operation-received-server-error-0\n",
      "https://repost.aws/questions/QUT86tGF5tRB2hC168_n5MAQ/failed-ping-healthcheck-after-deploying-tf-2-1-model-with-tf-serving-contain\n",
      "https://learn.microsoft.com/answers/questions/893526/acideploymentfailed.html\n",
      "https://learn.microsoft.com/answers/questions/981325/azureml-core-1440-fails-to-deploy-model-to-webserv.html\n",
      "https://learn.microsoft.com/answers/questions/925083/az-ml-designer-swagger-file-missing-on-deployment.html\n",
      "https://learn.microsoft.com/answers/questions/893075/specifying-input-type-as-number-in-componentcomman.html\n",
      "https://learn.microsoft.com/answers/questions/873192/web-service-rest-type-post-in-azure-machine-learni.html\n",
      "https://learn.microsoft.com/answers/questions/690970/azureml-no-module-found-error-on-deployment-of-inf.html\n",
      "https://learn.microsoft.com/answers/questions/889212/how-should-i-create-a-scoring-script-for-object-de.html\n",
      "https://learn.microsoft.com/answers/questions/771372/multiple-new-errors-when-deploying-to-aci-webservi.html\n",
      "https://learn.microsoft.com/answers/questions/742817/deploying-from-azure-ml-studio-designer-is-giving.html\n",
      "https://learn.microsoft.com/answers/questions/530817/azure-ml-upload-file-to-step-run39s-output-authent.html\n",
      "https://learn.microsoft.com/answers/questions/557185/import-data-error-2.html\n",
      "https://learn.microsoft.com/answers/questions/514710/cuda-not-compatible-with-pytorch-installation-erro.html\n",
      "https://learn.microsoft.com/answers/questions/441498/featurizationconfig-not-working.html\n",
      "https://learn.microsoft.com/answers/questions/403018/pipeline-can-not-be-built-using-a-hyperdrivestep-i.html\n",
      "https://learn.microsoft.com/answers/questions/375577/submitted-script-failed-with-a-non-zero-exit-code.html\n",
      "https://learn.microsoft.com/answers/questions/373687/ml-model-deployement-issue.html\n",
      "https://learn.microsoft.com/answers/questions/255760/azure-error-httpsconnectionpoolhost39westus2apiazu.html\n",
      "https://discuss.dvc.org/t/error-unexpected-error-errno-2-no-such-file-or-directory/1381\n",
      "https://discuss.dvc.org/t/ssh-remote-unexpected-error-permission-denied/1300\n",
      "https://discuss.dvc.org/t/unexpected-error-when-push-to-ssh-remote/1105\n",
      "https://groups.google.com/g/mlflow-users/c/GrCd-t0gx8U\n",
      "https://community.wandb.ai/t/windows-11-wandb-sweep-gives-connectionreseterror-winerror-10054/3217\n",
      "https://community.wandb.ai/t/problem-with-sweep-how-to-use-run-finish-and-log-without-error-question-about-defined-metric/3260\n",
      "https://community.wandb.ai/t/how-to-install-wandb-on-a-docker-image-for-arm/3080\n"
     ]
    }
   ],
   "source": [
    "# # manually preprocess the content of the issues and feed them into \"text-davinci-003\" for summary generation\n",
    "# for index, row in question_prefill_ts.iterrows():\n",
    "#     if (len(str(row['Question_body'])) > 10000):\n",
    "#         print(row['Question_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4967, 3595)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# question_so = pd.read_json(os.path.join(path_labeling, 'question_prefill_so.json'))\n",
    "# question_ts = pd.read_json(os.path.join(path_labeling, 'question_prefill_ts.json'))\n",
    "# question_todo_so = question_so[question_so['Question_summary'] == '']\n",
    "# question_todo_ts = question_ts[question_ts['Question_summary'] == '']\n",
    "# question_done_so = question_so[question_so['Question_summary'] != '']\n",
    "# question_done_ts = question_ts[question_ts['Question_summary'] != '']\n",
    "# question_todo_so.shape[0], question_todo_ts.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_labeling, 'questions_topic_modeling.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    title = row['Question_title']\n",
    "    body = str(row['Question_body'])    \n",
    "    content = ('Title: ' + title + '; Content:' + body if len(body) else title).lower()\n",
    "    \n",
    "    for tool_keyword in tools_keywords[row['Tool']]:\n",
    "        if tool_keyword in content:\n",
    "            content = content.replace(tool_keyword, '')\n",
    "    \n",
    "    df_questions.at[index, 'Question_original_content_preprocessed_text'] = content\n",
    "    \n",
    "df_questions.to_json(os.path.join(path_labeling, 'questions_topic_modeling.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"name\": \"question-experiment-1\",\n",
    "    \"metric\": {\n",
    "        'name': 'CoherenceCV',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        'n_neighbors': {\n",
    "            'values': list(range(10, 22, 2))\n",
    "        },\n",
    "        'n_components': {\n",
    "            'values': list(range(4, 12, 2))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# set default values for hyperparameters\n",
    "config_defaults = {\n",
    "    'model_name': 'all-mpnet-base-v2',\n",
    "    'metric_distane': 'manhattan',\n",
    "    'low_memory': False,\n",
    "    'stop_words': 'english',\n",
    "    'ngram_range': (1, 5),\n",
    "    'reduce_frequent_words': True\n",
    "}\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'questions_topic_modeling.json'))\n",
    "docs = df_issues['Question_original_content_preprocessed_text'].tolist()\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config) as run:\n",
    "        # update any values not set by sweep\n",
    "        run.config.setdefaults(config_defaults)\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Step 1 - Extract embeddings\n",
    "        embedding_model = SentenceTransformer(run.config.model_name)\n",
    "\n",
    "        # Step 2 - Reduce dimensionality\n",
    "        umap_model = UMAP(n_neighbors=config.n_neighbors, n_components=config.n_components,\n",
    "                          metric=run.config.metric_distane, low_memory=run.config.low_memory)\n",
    "\n",
    "        # Step 3 - Cluster reduced embeddings\n",
    "        hdbscan_model = HDBSCAN()\n",
    "\n",
    "        # Step 4 - Tokenize topics\n",
    "        vectorizer_model = TfidfVectorizer(\n",
    "            stop_words=run.config.stop_words, ngram_range=run.config.ngram_range)\n",
    "\n",
    "        # Step 5 - Create topic representation\n",
    "        ctfidf_model = ClassTfidfTransformer(\n",
    "            reduce_frequent_words=run.config.reduce_frequent_words)\n",
    "\n",
    "        # Step 6 - (Optional) Fine-tune topic representations with a `bertopic.representation` model\n",
    "        representation_model = KeyBERTInspired()\n",
    "\n",
    "        # All steps together\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,            # Step 1 - Extract embeddings\n",
    "            umap_model=umap_model,                      # Step 2 - Reduce dimensionality\n",
    "            hdbscan_model=hdbscan_model,                # Step 3 - Cluster reduced embeddings\n",
    "            vectorizer_model=vectorizer_model,          # Step 4 - Tokenize topics\n",
    "            ctfidf_model=ctfidf_model,                  # Step 5 - Extract topic words\n",
    "            representation_model=representation_model,  # Step 6 - (Optional) Fine-tune topic represenations\n",
    "            # verbose=True                              # Step 7 - Track model stages\n",
    "        )\n",
    "\n",
    "        topic_model = topic_model.fit(docs)\n",
    "\n",
    "        tokenized_docs = [doc.tolist() for doc in vectorizer_model.inverse_transform(\n",
    "            vectorizer_model.transform(docs))]\n",
    "        topic_words = [[words for words, _ in topic_model.get_topic(\n",
    "            topic)] for topic in range(len(topic_model.get_topics())-1)]\n",
    "\n",
    "        wandb.log({'CoherenceCV': CoherenceCV(\n",
    "            topics=topic_words, texts=tokenized_docs).score()})\n",
    "        wandb.log({'CoherenceNPMI': CoherenceNPMI(\n",
    "            topics=topic_words, texts=tokenized_docs).score()})\n",
    "        wandb.log({'CoherenceUMASS': CoherenceUMASS(\n",
    "            topics=topic_words, texts=tokenized_docs).score()})\n",
    "        wandb.log({'CoherenceUCI': CoherenceUCI(\n",
    "            topics=topic_words, texts=tokenized_docs).score()})\n",
    "        wandb.log({'InvertedRBO': InvertedRBO(topics=topic_words).score()})\n",
    "        wandb.log({'TopicDiversity': TopicDiversity(\n",
    "            topics=topic_words).score(10)})\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_configuration, project='asset-management-project')\n",
    "# Create sweep with ID: 232p6rp5\n",
    "wandb.agent(sweep_id=sweep_id, function=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 & 4\n",
    "\n",
    "import random\n",
    "\n",
    "prompt = 'Please use 1 to 2 sentences to summarize the following issues beginning with \"The user\".\\n\"\"\"'\n",
    "\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'questions_topic_modeling.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    if row['Question_original_content_gpt_summary']:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        response = retry_with_backoff(\n",
    "            openai.Completion.create,\n",
    "            model='text-davinci-003',\n",
    "            prompt=prompt +\n",
    "            row['Question_original_content_preprocessed_text'] + '\"\"\"\\n',\n",
    "            temperature=0,\n",
    "            max_tokens=200,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=10,\n",
    "            stream=False\n",
    "        )\n",
    "        df_questions.at[index, 'Question_original_content_gpt_summary'] = response['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        print(f'{e} on question {index}')\n",
    "        \n",
    "    if index % 50 == 0:\n",
    "        print(f'persisting on question {index}')\n",
    "        df_questions.to_json(os.path.join(\n",
    "            path_labeling, 'questions_topic_modeling.json'), indent=4, orient='records')\n",
    "        \n",
    "    time.sleep(5)\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling, 'questions_topic_modeling.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://stackoverflow.com/questions/40907303\n",
      "https://stackoverflow.com/questions/63339703\n",
      "https://stackoverflow.com/questions/65577286\n",
      "https://stackoverflow.com/questions/51064366\n",
      "https://stackoverflow.com/questions/68150444\n",
      "https://stackoverflow.com/questions/59762829\n",
      "https://stackoverflow.com/questions/62813017\n",
      "https://stackoverflow.com/questions/62836278\n",
      "https://stackoverflow.com/questions/69466354\n",
      "https://stackoverflow.com/questions/68489311\n",
      "https://stackoverflow.com/questions/67599026\n",
      "https://stackoverflow.com/questions/69721067\n",
      "https://stackoverflow.com/questions/73085199\n",
      "https://stackoverflow.com/questions/62569747\n",
      "https://stackoverflow.com/questions/70567307\n",
      "https://stackoverflow.com/questions/73462205\n",
      "https://stackoverflow.com/questions/73812159\n",
      "https://stackoverflow.com/questions/68397384\n",
      "https://stackoverflow.com/questions/70968412\n",
      "https://stackoverflow.com/questions/63204081\n",
      "https://stackoverflow.com/questions/63518174\n",
      "https://stackoverflow.com/questions/73650387\n",
      "https://stackoverflow.com/questions/60418931\n",
      "https://stackoverflow.com/questions/69147788\n",
      "https://stackoverflow.com/questions/57724414\n",
      "https://stackoverflow.com/questions/73499320\n",
      "https://stackoverflow.com/questions/73624005\n",
      "https://stackoverflow.com/questions/72633246\n",
      "https://stackoverflow.com/questions/73334915\n",
      "https://stackoverflow.com/questions/73836459\n",
      "https://stackoverflow.com/questions/67093041\n",
      "https://stackoverflow.com/questions/69534189\n",
      "https://stackoverflow.com/questions/63770171\n",
      "https://stackoverflow.com/questions/73838064\n",
      "https://stackoverflow.com/questions/63116338\n",
      "https://stackoverflow.com/questions/66561959\n",
      "https://stackoverflow.com/questions/68090119\n",
      "https://stackoverflow.com/questions/72734803\n",
      "https://stackoverflow.com/questions/58592206\n",
      "https://stackoverflow.com/questions/73642527\n",
      "https://stackoverflow.com/questions/62261222\n",
      "https://stackoverflow.com/questions/70833499\n",
      "https://stackoverflow.com/questions/63492891\n",
      "https://stackoverflow.com/questions/70291455\n",
      "https://stackoverflow.com/questions/62962319\n",
      "https://stackoverflow.com/questions/62422682\n",
      "https://stackoverflow.com/questions/73761872\n",
      "https://stackoverflow.com/questions/59177264\n",
      "https://stackoverflow.com/questions/65102653\n",
      "https://stackoverflow.com/questions/73064066\n",
      "https://stackoverflow.com/questions/73734047\n",
      "https://stackoverflow.com/questions/71652798\n",
      "https://stackoverflow.com/questions/73098371\n",
      "https://stackoverflow.com/questions/61239274\n",
      "https://stackoverflow.com/questions/67650010\n",
      "https://stackoverflow.com/questions/63116338\n",
      "https://repost.aws/questions/QUVfbc_AsXRzaxAl69MMFlsQ/error-for-training-job-catboost-classification-model-error-message-type-error-cannot-convert-xxx-to-float\n",
      "https://repost.aws/questions/QUT86tGF5tRB2hC168_n5MAQ/failed-ping-healthcheck-after-deploying-tf-2-1-model-with-tf-serving-contain\n",
      "https://learn.microsoft.com/answers/questions/893526/acideploymentfailed.html\n",
      "https://learn.microsoft.com/answers/questions/925083/az-ml-designer-swagger-file-missing-on-deployment.html\n",
      "https://learn.microsoft.com/answers/questions/873192/web-service-rest-type-post-in-azure-machine-learni.html\n",
      "https://learn.microsoft.com/answers/questions/690970/azureml-no-module-found-error-on-deployment-of-inf.html\n",
      "https://learn.microsoft.com/answers/questions/889212/how-should-i-create-a-scoring-script-for-object-de.html\n",
      "https://learn.microsoft.com/answers/questions/771372/multiple-new-errors-when-deploying-to-aci-webservi.html\n",
      "https://learn.microsoft.com/answers/questions/742817/deploying-from-azure-ml-studio-designer-is-giving.html\n",
      "https://learn.microsoft.com/answers/questions/530817/azure-ml-upload-file-to-step-run39s-output-authent.html\n",
      "https://learn.microsoft.com/answers/questions/557185/import-data-error-2.html\n",
      "https://learn.microsoft.com/answers/questions/514710/cuda-not-compatible-with-pytorch-installation-erro.html\n",
      "https://learn.microsoft.com/answers/questions/375577/submitted-script-failed-with-a-non-zero-exit-code.html\n",
      "https://learn.microsoft.com/answers/questions/390003/azure-ml-pipeline-fails-giving-no-error.html\n",
      "https://learn.microsoft.com/answers/questions/373687/ml-model-deployement-issue.html\n",
      "https://learn.microsoft.com/answers/questions/255760/azure-error-httpsconnectionpoolhost39westus2apiazu.html\n",
      "https://discuss.dvc.org/t/error-unexpected-error-errno-2-no-such-file-or-directory/1381\n",
      "https://discuss.dvc.org/t/ssh-remote-unexpected-error-permission-denied/1300\n",
      "https://discuss.dvc.org/t/error-unexpected-error-nonetype-object-has-no-attribute-flush/1031\n",
      "https://discuss.dvc.org/t/unexpected-error-when-push-to-ssh-remote/1105\n",
      "https://groups.google.com/g/mlflow-users/c/GrCd-t0gx8U\n",
      "https://community.wandb.ai/t/windows-11-wandb-sweep-gives-connectionreseterror-winerror-10054/3217\n",
      "https://community.wandb.ai/t/problem-with-sweep-how-to-use-run-finish-and-log-without-error-question-about-defined-metric/3260\n",
      "https://community.wandb.ai/t/how-to-install-wandb-on-a-docker-image-for-arm/3080\n"
     ]
    }
   ],
   "source": [
    "# Experiment 2\n",
    "\n",
    "# output unsuccesful summary requests\n",
    "for index, row in df_questions.iterrows():\n",
    "    if not row['Question_original_content_gpt_summary']:\n",
    "        print(row['Question_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "\n",
    "sweep_configuration = {\n",
    "    \"name\": \"question-experiment-2\",\n",
    "    \"metric\": {\n",
    "        'name': 'CoherenceCV',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        'n_neighbors': {\n",
    "            'values': list(range(10, 22, 2))\n",
    "        },\n",
    "        'n_components': {\n",
    "            'values': list(range(4, 12, 2))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# set default values for hyperparameters\n",
    "config_defaults = {\n",
    "    'model_name': 'all-mpnet-base-v2',\n",
    "    'metric_distane': 'manhattan',\n",
    "    'low_memory': False,\n",
    "    'stop_words': 'english',\n",
    "    'ngram_range': (1, 5),\n",
    "    'reduce_frequent_words': True\n",
    "}\n",
    "\n",
    "df_issues = pd.read_json(os.path.join(\n",
    "    path_labeling, 'questions_topic_modeling.json'))\n",
    "docs = df_issues['Question_original_content_gpt_summary'].tolist()\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config) as run:\n",
    "        # update any values not set by sweep\n",
    "        run.config.setdefaults(config_defaults)\n",
    "\n",
    "        # If called by wandb.agent, as below,\n",
    "        # this config will be set by Sweep Controller\n",
    "        config = wandb.config\n",
    "\n",
    "        # Step 1 - Extract embeddings\n",
    "        embedding_model = SentenceTransformer(run.config.model_name)\n",
    "\n",
    "        # Step 2 - Reduce dimensionality\n",
    "        umap_model = UMAP(n_neighbors=config.n_neighbors, n_components=config.n_components,\n",
    "                          metric=run.config.metric_distane, low_memory=run.config.low_memory)\n",
    "\n",
    "        # Step 3 - Cluster reduced embeddings\n",
    "        hdbscan_model = HDBSCAN()\n",
    "\n",
    "        # Step 4 - Tokenize topics\n",
    "        vectorizer_model = TfidfVectorizer(\n",
    "            stop_words=run.config.stop_words, ngram_range=run.config.ngram_range)\n",
    "\n",
    "        # Step 5 - Create topic representation\n",
    "        ctfidf_model = ClassTfidfTransformer(\n",
    "            reduce_frequent_words=run.config.reduce_frequent_words)\n",
    "\n",
    "        # Step 6 - (Optional) Fine-tune topic representations with a `bertopic.representation` model\n",
    "        representation_model = KeyBERTInspired()\n",
    "\n",
    "        # All steps together\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,            # Step 1 - Extract embeddings\n",
    "            umap_model=umap_model,                      # Step 2 - Reduce dimensionality\n",
    "            hdbscan_model=hdbscan_model,                # Step 3 - Cluster reduced embeddings\n",
    "            vectorizer_model=vectorizer_model,          # Step 4 - Tokenize topics\n",
    "            ctfidf_model=ctfidf_model,                  # Step 5 - Extract topic words\n",
    "            representation_model=representation_model,  # Step 6 - (Optional) Fine-tune topic represenations\n",
    "            # verbose=True                              # Step 7 - Track model stages\n",
    "        )\n",
    "        \n",
    "        topic_model = topic_model.fit(docs)\n",
    "\n",
    "        tokenized_docs = [doc.tolist() for doc in vectorizer_model.inverse_transform(\n",
    "            vectorizer_model.transform(docs))]\n",
    "        topic_words = [[words for words, _ in topic_model.get_topic(\n",
    "            topic)] for topic in range(len(topic_model.get_topics())-1)]\n",
    "\n",
    "        wandb.log({'CoherenceCV': CoherenceCV(\n",
    "            topics=topic_words, texts=tokenized_docs).score()})\n",
    "        wandb.log({'CoherenceNPMI': CoherenceNPMI(\n",
    "            topics=topic_words, texts=tokenized_docs).score()})\n",
    "        wandb.log({'CoherenceUMASS': CoherenceUMASS(\n",
    "            topics=topic_words, texts=tokenized_docs).score()})\n",
    "        wandb.log({'CoherenceUCI': CoherenceUCI(\n",
    "            topics=topic_words, texts=tokenized_docs).score()})\n",
    "        wandb.log({'InvertedRBO': InvertedRBO(topics=topic_words).score()})\n",
    "        wandb.log({'TopicDiversity': TopicDiversity(\n",
    "            topics=topic_words).score(10)})\n",
    "\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_configuration, project='asset-management-project')\n",
    "# Create sweep with ID: 232p6rp5\n",
    "wandb.agent(sweep_id=sweep_id, function=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from revChatGPT.V1 import Chatbot\n",
    "\n",
    "# prompt = 'Please use 1 to 2 sentences to summarize the following issues beginning with \"The user\".\\n\"\"\"'\n",
    "\n",
    "# chatbot = Chatbot(\n",
    "#     config={\n",
    "#         'email': os.getenv('CHATGPT_EMAIL'),\n",
    "#         'password': os.getenv('CHATGPT_PASSWORD'),\n",
    "#         'paid': True\n",
    "#     }\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Experiment 2\n",
    "\n",
    "# df_questions = pd.read_json(os.path.join(\n",
    "#     path_labeling, 'questions_topic_modeling.json'))\n",
    "# last_summary = summary = ''\n",
    "\n",
    "# for index, row in df_questions.iterrows():\n",
    "#     if row['Question_original_content_gpt_summary']:\n",
    "#         continue\n",
    "        \n",
    "#     question = prompt + \\\n",
    "#         row['Question_original_content_preprocessed_text'] + '\"\"\"\\n'\n",
    "    \n",
    "#     try:\n",
    "#         for data in chatbot.ask(question):\n",
    "#             summary = data['message']\n",
    "#         if last_summary == summary:\n",
    "#             raise Exception('no response')        \n",
    "#         last_summary = summary\n",
    "#         df_questions.at[index, 'Question_original_content_gpt_summary'] = summary        \n",
    "#     except Exception as e:\n",
    "#         print(f'{e} on question {index}')\n",
    "        \n",
    "#     if index % 50 == 0:\n",
    "#         print(f'persistence on question {index}')\n",
    "#         df_questions.to_json(os.path.join(\n",
    "#             path_labeling, 'questions_topic_modeling.json'), indent=4, orient='records')\n",
    "        \n",
    "#     time.sleep(100)\n",
    "\n",
    "# df_questions.to_json(os.path.join(\n",
    "#     path_labeling, 'questions_topic_modeling.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8664"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# output the number of asset-management-related discussion posts\n",
    "df_questions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size is based on the recommendation from https://www.calculator.net/sample-size-calculator.html\n",
    "\n",
    "sample_size = 368\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling, 'questions_topic_modeling.json'))\n",
    "\n",
    "df_sample = df_questions.sample(n=sample_size, random_state=42)\n",
    "\n",
    "df_sample.to_json(os.path.join(\n",
    "    path_labeling, 'questions_sample.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# content filtering patterns\n",
    "regex_filter = r\"(<.+?>)|(\\`{3}.+?\\`{3})|(\\`{2}.+?\\`{2})|(\\`{1}.+?\\`{1})|({.*?})|(\\\\u[^\\s]+)|((!)?\\[.*?\\])|(\\(.+?\\))|([^\\s]*[<=>]=[^\\s]+)|(@[^\\s]+)|((https?:\\/)?\\/[^\\s]+)|([^\\s]*\\\\[^\\s]+)|([^\\s]+\\/[^\\s]+)|([^\\s]+\\.[^\\s]+)|([^\\s]+_[^\\s]+)|(_+[^\\s]+_*)|(_*[^\\s]+_+)|([0-9\\|\\-\\r\\n\\t\\\"\\-#*=~:{}\\(\\)\\[\\]]+)|(info(rmation)?)\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
