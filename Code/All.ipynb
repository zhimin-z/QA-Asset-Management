{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.representation import KeyBERTInspired\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "\n",
    "import gensim.corpora as corpora\n",
    "import pandas as pd\n",
    "import time\n",
    "import glob\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_so = os.path.join(path_dataset, 'Stack Overflow')\n",
    "path_ts = os.path.join(path_dataset, 'Tool-specific Others')\n",
    "path_labeling = os.path.join(path_dataset, 'Labeling')\n",
    "\n",
    "path_so_raw = os.path.join(path_so, 'Raw')\n",
    "path_ts_raw = os.path.join(path_ts, 'Raw')\n",
    "path_so_filtered = os.path.join(path_so, 'Filtered')\n",
    "path_ts_filtered = os.path.join(path_ts, 'Filtered')\n",
    "\n",
    "path_labeling_question = os.path.join(path_labeling, 'Question')\n",
    "path_labeling_answer = os.path.join(path_labeling, 'Answer')\n",
    "    \n",
    "if not os.path.exists(path_dataset):\n",
    "    os.makedirs(path_dataset)\n",
    "\n",
    "if not os.path.isdir(path_so):\n",
    "    os.mkdir(path_so)\n",
    "\n",
    "if not os.path.isdir(path_ts):\n",
    "    os.mkdir(path_ts)\n",
    "\n",
    "if not os.path.isdir(path_labeling):\n",
    "    os.mkdir(path_labeling)\n",
    "\n",
    "if not os.path.isdir(path_so_raw):\n",
    "    os.mkdir(path_so_raw)\n",
    "\n",
    "if not os.path.isdir(path_ts_raw):\n",
    "    os.mkdir(path_ts_raw)\n",
    "\n",
    "if not os.path.isdir(path_so_filtered):\n",
    "    os.mkdir(path_so_filtered)\n",
    "\n",
    "if not os.path.isdir(path_ts_filtered):\n",
    "    os.mkdir(path_ts_filtered)\n",
    "\n",
    "if not os.path.exists(path_labeling_question):\n",
    "    os.makedirs(path_labeling_question)\n",
    "\n",
    "if not os.path.exists(path_labeling_answer):\n",
    "    os.makedirs(path_labeling_answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool2tag = {\n",
    "    'Amazon SageMaker': {'amazon-sagemaker', 'amazon-sagemaker-experiments', 'amazon-sagemaker-studio'},\n",
    "    'Azure Machine Learning': {'azure-machine-learning-service', 'azure-machine-learning-studio', 'azure-machine-learning-workbench'},\n",
    "    'ClearML': {'clearml'},\n",
    "    'Comet': {'comet-ml'},\n",
    "    'DVC': {'dvc'},\n",
    "    'Kedro': {'kedro'},\n",
    "    'MLflow': {'mlflow'},\n",
    "    'MLRun': {'mlrun'},\n",
    "    'Neptune': {'neptune'},\n",
    "    'Sacred': {'python-sacred'},\n",
    "    'Vertex AI': {'google-cloud-vertex-ai'},\n",
    "    'Weights & Biases': {'wandb'}\n",
    "}\n",
    "\n",
    "tools_keywords = {\n",
    "    'Amazon SageMaker': ['amazon sagemaker', 'aws sagemaker', 'sagemaker'],\n",
    "    'Azure Machine Learning': ['microsoft azure machine learning', 'azure machine learning', 'microsoft azure ml', 'microsoft azureml', 'azure ml', 'azureml'],\n",
    "    'ClearML': ['clearml'],\n",
    "    'Comet': ['comet'],\n",
    "    'Domino': ['domino'],\n",
    "    'DVC': ['dvc'],\n",
    "    'Guild AI': ['guild ai'],\n",
    "    'Kedro': ['kedro'],\n",
    "    'MLflow': ['mlflow'],\n",
    "    'Neptune': ['neptune'],\n",
    "    'Polyaxon': ['polyaxon'],\n",
    "    'Sacred': ['sacred'],\n",
    "    'SigOpt': ['sigopt'],\n",
    "    'Vertex AI': ['google vertex ai', 'vertex ai'],\n",
    "    'Weights & Biases': ['weights & biases', 'weights and biases', 'wandb']\n",
    "}\n",
    "\n",
    "ignore_tools = {\n",
    "    \n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb_project = 'asset-management-project'\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# function to scrape the posts from the tool-specific discussion fora\n",
    "\n",
    "def scrape_post(base_url, page_suffix, file_name):\n",
    "    page = -1\n",
    "    post_list = []\n",
    "    \n",
    "    while True:\n",
    "        page = page + 1\n",
    "        page_url = base_url + page_suffix + str(page)\n",
    "        topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "        for topic in topic_list['topics']:\n",
    "            post_url = base_url + 't/' + \\\n",
    "            topic['slug'] + '/' + str(topic['id'])\n",
    "\n",
    "            post = {}\n",
    "            post['Question_title'] = topic['title']\n",
    "            post['Question_link'] = post_url\n",
    "            post['Question_creation_time'] = topic['created_at']\n",
    "            post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "            post['Question_score'] = topic['like_count']\n",
    "            post['Question_view_count'] = topic['views']\n",
    "            post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "            comments = requests.get(post_url + '.json').json()['post_stream']['posts']\n",
    "            post['Question_body'] = comments[0]['cooked']\n",
    "            post['Answer_list'] = comments[1:]\n",
    "            post_list.append(post)\n",
    "\n",
    "            time.sleep(2)\n",
    "\n",
    "        if 'more_topics_url' not in topic_list.keys():\n",
    "            break\n",
    "        \n",
    "    with open(os.path.join(path_ts_raw, file_name), 'w') as outfile:\n",
    "        json_post_list = json.dumps(post_list, indent='\\t')\n",
    "        outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from Guild AI\n",
    "base_url = 'https://my.guild.ai/'\n",
    "page_suffix = 'c/troubleshooting/6.json?page='\n",
    "file_name = 'Guild AI.json'\n",
    "post_list = scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from Weights & Biases\n",
    "base_url = 'https://community.wandb.ai/'\n",
    "page_suffix = 'c/w-b-support/36.json?page='\n",
    "file_name = 'Weights & Biases.json'\n",
    "post_list = scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from SigOpt\n",
    "base_url = 'https://community.sigopt.com/'\n",
    "page_suffix = 'c/general-discussion/9.json?page='\n",
    "file_name = 'SigOpt.json'\n",
    "post_list = scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape posts from DVC\n",
    "base_url = 'https://discuss.dvc.org/'\n",
    "page_suffix = 'c/questions/9.json?page='\n",
    "file_name = 'DVC.json'\n",
    "post_list = scrape_post(base_url, page_suffix, file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ts_questions = pd.DataFrame()\n",
    "\n",
    "# exclude tool-specific posts with negative upvote count\n",
    "for file_name in glob.glob(os.path.join(path_ts_raw, '*.json')):\n",
    "    repos = pd.read_json(file_name)\n",
    "    if 'Question_score' in repos.columns:\n",
    "        repos = repos[repos['Question_score'] > -1]\n",
    "    repos['Tool'] = os.path.split(file_name)[1].split('.')[0]\n",
    "    df_ts_questions = pd.concat([df_ts_questions, repos], ignore_index=True)\n",
    "    \n",
    "df_ts_answers = df_ts_questions[df_ts_questions['Question_answer_count'] > 0]\n",
    "for tool in df_ts_answers['Tool'].unique().tolist():\n",
    "    number_accepted_answer = df_ts_answers[df_ts_answers['Tool']\n",
    "                                            == tool]['Question_has_accepted_answer'].sum()\n",
    "    if number_accepted_answer > 0:\n",
    "        df_ts_answers = df_ts_answers.drop(df_ts_answers[(df_ts_answers['Tool'] == tool) & (\n",
    "            df_ts_answers['Question_has_accepted_answer'] == False)].index)\n",
    "\n",
    "df_ts_questions.to_json(os.path.join(path_ts_filtered,\n",
    "              'questions.json'), orient='records', indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#Question</th>\n",
       "      <th>#Answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>528</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>1435</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DVC</td>\n",
       "      <td>315</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Domino</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Guild AI</td>\n",
       "      <td>115</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLFlow</td>\n",
       "      <td>280</td>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>43</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>15</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>297</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>583</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Tool  #Question  #Answered\n",
       "0        Amazon SageMaker        528        167\n",
       "1  Azure Machine Learning       1435        343\n",
       "2                     DVC        315        300\n",
       "3                  Domino         13          4\n",
       "4                Guild AI        115        108\n",
       "5                  MLFlow        280        143\n",
       "6                Polyaxon         43         34\n",
       "7                  SigOpt         15          7\n",
       "8               Vertex AI        297         32\n",
       "9        Weights & Biases        583         92"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep only posts with at least one answer\n",
    "df_ts_question_summary = df_ts_questions.groupby(\n",
    "    'Tool').count()['Question_title'].reset_index()\n",
    "df_ts_answer_summary = df_ts_answers.groupby(\n",
    "    'Tool').count()['Question_title'].reset_index()\n",
    "\n",
    "df_ts_question_summary.columns = ['Tool', '#Question']\n",
    "df_ts_answer_summary.columns = ['Tool', '#Answered']\n",
    "\n",
    "df_summary = pd.merge(df_ts_question_summary, df_ts_answer_summary, on='Tool')\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question_id</th>\n",
       "      <th>Question_title</th>\n",
       "      <th>Question_body</th>\n",
       "      <th>Question_answer_count</th>\n",
       "      <th>Question_comment_count</th>\n",
       "      <th>Question_creation_time</th>\n",
       "      <th>Question_favorite_count</th>\n",
       "      <th>Question_score</th>\n",
       "      <th>Question_tags</th>\n",
       "      <th>Question_view_count</th>\n",
       "      <th>...</th>\n",
       "      <th>Owner_up_votes</th>\n",
       "      <th>Owner_down_votes</th>\n",
       "      <th>Owner_views</th>\n",
       "      <th>Answer_body</th>\n",
       "      <th>Answer_comment_count</th>\n",
       "      <th>Answer_creation_time</th>\n",
       "      <th>Answer_score</th>\n",
       "      <th>Owner_location</th>\n",
       "      <th>Question_last_edit_time</th>\n",
       "      <th>Answer_last_edit_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>70098779</td>\n",
       "      <td>How to connect to MLFlow tracking server that ...</td>\n",
       "      <td>&lt;p&gt;I want to connect to remote tracking server...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-11-24 15:30:11.310000+00:00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>[authorization, tracking, mlflow]</td>\n",
       "      <td>2102</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>&lt;p&gt;&lt;a href=\"https://mlflow.org/docs/latest/tra...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-11-24 17:01:13.483000+00:00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38927230</td>\n",
       "      <td>Panda AssertionError columns passed, passed da...</td>\n",
       "      <td>&lt;p&gt;I am working on Azure ML implementation on ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-08-12 22:23:17.197000+00:00</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>[python, pandas, dataframe, nltk, azure-machin...</td>\n",
       "      <td>48200</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>55</td>\n",
       "      <td>339</td>\n",
       "      <td>&lt;p&gt;Try this:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;dataframe_outpu...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-08-12 22:26:09.603000+00:00</td>\n",
       "      <td>13.0</td>\n",
       "      <td>Toronto, ON, Canada</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68773463</td>\n",
       "      <td>AccessDeniedException on sagemaker:CreateDomai...</td>\n",
       "      <td>&lt;p&gt;I am trying to use the AWS SageMaker Studio...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-08-13 13:49:08.683000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[amazon-web-services, amazon-iam, amazon-sagem...</td>\n",
       "      <td>366</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67701971</td>\n",
       "      <td>How to label a text with multiple paragraphs i...</td>\n",
       "      <td>&lt;p&gt;I was trying setup a single label labeling ...</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-05-26 09:16:33.420000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>[amazon-web-services, text, amazon-sagemaker, ...</td>\n",
       "      <td>161</td>\n",
       "      <td>...</td>\n",
       "      <td>75</td>\n",
       "      <td>10</td>\n",
       "      <td>147</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Zürich, Suïssa</td>\n",
       "      <td>2021-05-26 11:54:00.030000+00:00</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48398509</td>\n",
       "      <td>How to Invoke AWS Sagemaker API with c# .NET?</td>\n",
       "      <td>&lt;p&gt;I have trained and deployed a model in AWS ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2018-01-23 09:42:48.607000+00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>[c#, asp.net, amazon-web-services, aws-sdk, am...</td>\n",
       "      <td>743</td>\n",
       "      <td>...</td>\n",
       "      <td>34</td>\n",
       "      <td>1</td>\n",
       "      <td>124</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Pune India</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Question_id                                     Question_title  \\\n",
       "0     70098779  How to connect to MLFlow tracking server that ...   \n",
       "1     38927230  Panda AssertionError columns passed, passed da...   \n",
       "2     68773463  AccessDeniedException on sagemaker:CreateDomai...   \n",
       "3     67701971  How to label a text with multiple paragraphs i...   \n",
       "4     48398509      How to Invoke AWS Sagemaker API with c# .NET?   \n",
       "\n",
       "                                       Question_body  Question_answer_count  \\\n",
       "0  <p>I want to connect to remote tracking server...                      1   \n",
       "1  <p>I am working on Azure ML implementation on ...                      1   \n",
       "2  <p>I am trying to use the AWS SageMaker Studio...                      1   \n",
       "3  <p>I was trying setup a single label labeling ...                      0   \n",
       "4  <p>I have trained and deployed a model in AWS ...                      1   \n",
       "\n",
       "   Question_comment_count           Question_creation_time  \\\n",
       "0                       0 2021-11-24 15:30:11.310000+00:00   \n",
       "1                       0 2016-08-12 22:23:17.197000+00:00   \n",
       "2                       0 2021-08-13 13:49:08.683000+00:00   \n",
       "3                       2 2021-05-26 09:16:33.420000+00:00   \n",
       "4                       0 2018-01-23 09:42:48.607000+00:00   \n",
       "\n",
       "   Question_favorite_count  Question_score  \\\n",
       "0                      1.0               1   \n",
       "1                      3.0               7   \n",
       "2                      NaN               0   \n",
       "3                      NaN               1   \n",
       "4                      NaN               0   \n",
       "\n",
       "                                       Question_tags  Question_view_count  \\\n",
       "0                  [authorization, tracking, mlflow]                 2102   \n",
       "1  [python, pandas, dataframe, nltk, azure-machin...                48200   \n",
       "2  [amazon-web-services, amazon-iam, amazon-sagem...                  366   \n",
       "3  [amazon-web-services, text, amazon-sagemaker, ...                  161   \n",
       "4  [c#, asp.net, amazon-web-services, aws-sdk, am...                  743   \n",
       "\n",
       "   ... Owner_up_votes Owner_down_votes  Owner_views  \\\n",
       "0  ...              0                0           11   \n",
       "1  ...            136               55          339   \n",
       "2  ...              0                0           11   \n",
       "3  ...             75               10          147   \n",
       "4  ...             34                1          124   \n",
       "\n",
       "                                         Answer_body  Answer_comment_count  \\\n",
       "0  <p><a href=\"https://mlflow.org/docs/latest/tra...                   2.0   \n",
       "1  <p>Try this:</p>\\n\\n<pre><code>dataframe_outpu...                   0.0   \n",
       "2                                                NaN                   NaN   \n",
       "3                                                NaN                   NaN   \n",
       "4                                                NaN                   NaN   \n",
       "\n",
       "              Answer_creation_time Answer_score       Owner_location  \\\n",
       "0 2021-11-24 17:01:13.483000+00:00          2.0                  NaN   \n",
       "1 2016-08-12 22:26:09.603000+00:00         13.0  Toronto, ON, Canada   \n",
       "2                              NaT          NaN                  NaN   \n",
       "3                              NaT          NaN       Zürich, Suïssa   \n",
       "4                              NaT          NaN           Pune India   \n",
       "\n",
       "           Question_last_edit_time  Answer_last_edit_time  \n",
       "0                              NaT                    NaT  \n",
       "1                              NaT                    NaT  \n",
       "2                              NaT                    NaT  \n",
       "3 2021-05-26 11:54:00.030000+00:00                    NaT  \n",
       "4                              NaT                    NaT  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(\n",
    "    path_so_raw, 'bq-results-20230201-032754-1675222092237.json'), lines=True)\n",
    "df['Question_tags'] = df['Question_tags'].str.split('|')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tag collection\n",
    "tags = set()\n",
    "for key, value in tool2tag.items():\n",
    "    tags = tags.union(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tags\n",
    "df['Question_valid_tags'] = [[] for _ in range(len(df))]\n",
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'Question_valid_tags'] = list(tags.intersection(set(row['Question_tags'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Posts with at least 1 tags has 5130 in total.\n",
      "Posts with at least 2 tags has 220 in total.\n",
      "Posts with at least 3 tags has 18 in total.\n"
     ]
    }
   ],
   "source": [
    "# count post number with different tags\n",
    "arity = 0\n",
    "while True:\n",
    "    post_number = df[df['Question_valid_tags'].map(len) > arity].shape[0]\n",
    "    if post_number < 1:\n",
    "        break\n",
    "    arity = arity + 1\n",
    "    print(f'Posts with at least {arity} tags has {post_number} in total.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-8534f91f27f9>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_valid['Question_link'] = df_valid['Question_id'].apply(\n"
     ]
    }
   ],
   "source": [
    "# exclude Stack Overflow posts with unrelated tags\n",
    "df_valid = df[df['Question_valid_tags'].map(len) > 0]\n",
    "df_valid['Question_link'] = df_valid['Question_id'].apply(\n",
    "    lambda x: f'https://stackoverflow.com/questions/{x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclude Stack Overflow posts with negative upvote count\n",
    "df_qualified = df_valid[df_valid['Question_score'] > -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map from tag to tool\n",
    "tag2tool = dict()\n",
    "for key, value in tool2tag.items():\n",
    "    for elem in value:\n",
    "        tag2tool.setdefault(elem, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract Stack Overflow post collection with multiple tags based on the tool map\n",
    "for index, row in df_qualified.iterrows():\n",
    "    tags = set()\n",
    "    for tag in row['Question_valid_tags']:\n",
    "        tags.add(tag2tool[tag])\n",
    "    df_qualified.at[index, 'Question_valid_tags'] = sorted(list(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Amazon SageMaker, MLFlow]                 16\n",
       "[Azure Machine Learning, MLFlow]           11\n",
       "[Kedro, MLFlow]                             4\n",
       "[Azure Machine Learning, Kedro, MLFlow]     2\n",
       "[DVC, MLFlow]                               1\n",
       "[MLFlow, Sacred]                            1\n",
       "[Kedro, Neptune]                            1\n",
       "Name: Question_valid_tags, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check how the posts with more than one tags look like\n",
    "df_multiply_tagged = df_qualified[df_qualified['Question_valid_tags'].map(\n",
    "    len) > 1]\n",
    "df_multiply_tagged['Question_valid_tags'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-33-9867e953fe3c>:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_qualified.at[index, 'Tool'] = tags[0]\n"
     ]
    }
   ],
   "source": [
    "# create Stack Overflow post collection with exclusive tags\n",
    "multiply_tagged_posts_split = []\n",
    "df_qualified.assign(Tool='')\n",
    "\n",
    "for index, row in df_qualified.iterrows():\n",
    "    tags = row['Question_valid_tags']\n",
    "    df_qualified.at[index, 'Tool'] = tags[0]\n",
    "    if len(tags) > 1:\n",
    "        for tag in tags[1:]:\n",
    "            series = row.copy()\n",
    "            series['Tool'] = tag\n",
    "            multiply_tagged_posts_split.append(series)\n",
    "\n",
    "df_multiply_tagged_posts_split = pd.DataFrame(multiply_tagged_posts_split)\n",
    "df_qualified_exclusive_tagged = pd.concat(\n",
    "    [df_qualified, df_multiply_tagged_posts_split], ignore_index=True)\n",
    "del df_qualified_exclusive_tagged['Question_valid_tags']\n",
    "\n",
    "# keep Stack Overflow posts with accepted answers\n",
    "df_qualified_exclusive_tagged_completed = df_qualified_exclusive_tagged.dropna(\n",
    "    subset=['Answer_body'])\n",
    "\n",
    "df_qualified_exclusive_tagged.to_json(os.path.join(\n",
    "    path_so_filtered, 'questions.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#Question</th>\n",
       "      <th>#Answered</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>2233</td>\n",
       "      <td>737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>1530</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ClearML</td>\n",
       "      <td>40</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Comet</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DVC</td>\n",
       "      <td>91</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Kedro</td>\n",
       "      <td>149</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLFlow</td>\n",
       "      <td>551</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Neptune</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Sacred</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>341</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>77</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool  #Question  #Answered\n",
       "0         Amazon SageMaker       2233        737\n",
       "1   Azure Machine Learning       1530        586\n",
       "2                  ClearML         40         20\n",
       "3                    Comet         10          4\n",
       "4                      DVC         91         49\n",
       "5                    Kedro        149         60\n",
       "6                   MLFlow        551        129\n",
       "7                  Neptune          8          3\n",
       "8                   Sacred         10          7\n",
       "9                Vertex AI        341        112\n",
       "10        Weights & Biases         77         22"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_so_question_summary = df_qualified_exclusive_tagged.groupby(\n",
    "    'Tool').count()['Question_id'].reset_index()\n",
    "df_so_answer_summary = df_qualified_exclusive_tagged_completed.groupby(\n",
    "    'Tool').count()['Question_id'].reset_index()\n",
    "\n",
    "df_so_question_summary.columns = ['Tool', '#Question']\n",
    "df_so_answer_summary.columns = ['Tool', '#Answered']\n",
    "\n",
    "df_summary = pd.merge(df_so_question_summary, df_so_answer_summary, on='Tool')\n",
    "df_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create question dataset\n",
    "\n",
    "df_question_so = pd.read_json(os.path.join(path_so_filtered, 'questions.json'))\n",
    "df_question_ts = pd.read_json(os.path.join(path_ts_filtered, 'questions.json'))\n",
    "\n",
    "df_question_so['Platform'] = 'Stack Overflow'\n",
    "df_question_ts['Platform'] = 'Tool-specific'\n",
    "\n",
    "df_questions = pd.concat([df_question_so, df_question_ts], ignore_index=True)\n",
    "df_questions.to_json(os.path.join(path_labeling, 'original.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add potential field to questions for later filling\n",
    "df_questions = pd.read_json(os.path.join(path_labeling_question, 'original.json'))\n",
    "\n",
    "# Experiment 1: feed the original content to BerTopic\n",
    "df_questions['Question_original_content_preprocessed_text'] = ''\n",
    "\n",
    "# Experiment 2: feed the original content to text-davinci-003 model and get the generated summary, then feed the summary to BerTopic\n",
    "df_questions['Question_original_content_gpt_summary'] = ''\n",
    "\n",
    "# Experiment 3: feed the preprocessed content to BerTopic\n",
    "df_questions['Question_preprocessed_content'] = ''\n",
    "\n",
    "# Experiment 4: feed the preprocessed content to text-davinci-003 model and get the generated summary, then feed the summary to BerTopic\n",
    "df_questions['Question_preprocessed_content_gpt_summary'] = ''\n",
    "\n",
    "df_questions.to_json(os.path.join(path_labeling_question, 'topic_modeling.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_labeling_question, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    title = row['Question_title'].lower()\n",
    "    body = str(row['Question_body']).lower()\n",
    "    content = 'Title: ' + title + '; Content: ' + body\n",
    "    \n",
    "    for tool_keyword in tools_keywords[row['Tool']]:\n",
    "        if tool_keyword in content:\n",
    "            content = content.replace(tool_keyword, '')\n",
    "    \n",
    "    df_questions.at[index, 'Question_original_content_preprocessed_text'] = ' '.join(content.split())\n",
    "    \n",
    "df_questions.to_json(os.path.join(path_labeling_question, 'topic_modeling.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2 & 4\n",
    "\n",
    "import random\n",
    "\n",
    "question_prompt = 'Please write a one-sentence summary of the encountered challenges. For instance, you could begin with a sentence such as: \"The user XXXX\".\\n\"\"\"'\n",
    "\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    if row['Question_original_content_gpt_summary']:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        response = retry_with_backoff(\n",
    "            openai.Completion.create,\n",
    "            model='text-davinci-003',\n",
    "            prompt=question_prompt +\n",
    "            row['Question_original_content_preprocessed_text'] + '\"\"\"\\n',\n",
    "            temperature=0,\n",
    "            max_tokens=200,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=10,\n",
    "            stream=False\n",
    "        )\n",
    "        df_questions.at[index, 'Question_original_content_gpt_summary'] = response['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        print(f'{e} on question {index}')\n",
    "        \n",
    "    if index % 50 == 0:\n",
    "        print(f'persisting on question {index}')\n",
    "        df_questions.to_json(os.path.join(\n",
    "            path_labeling_question, 'topic_modeling.json'), indent=4, orient='records')\n",
    "        \n",
    "    time.sleep(5)\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "\n",
    "# output unsuccesful summary requests\n",
    "for index, row in df_questions.iterrows():\n",
    "    if not row['Question_original_content_gpt_summary']:\n",
    "        print(row['Issue_link'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "assert (df_questions.shape[0] == df_questions.dropna(\n",
    "    subset=['Question_preprocessed_content_gpt_summary']).shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_questions.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample size is based on the recommendation from https://www.calculator.net/sample-size-calculator.html\n",
    "\n",
    "sample_size = 368\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "\n",
    "df_sample = df_questions.sample(n=sample_size, random_state=42)\n",
    "\n",
    "df_sample.to_json(os.path.join(\n",
    "    path_labeling_question, 'sample.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# content filtering patterns\n",
    "regex_filter = r\"(<.*?>)|({.*?})|((!)?\\[.*?\\])|(\\(.*?\\))|(\\`{3}.+?\\`{3})|(\\`{2}.+?\\`{2})|(\\`{1}.+?\\`{1})|([^\\s]*[<=>]=[^\\s]+)|(@[^\\s]+)|((https?:\\/)?\\/[^\\s]+)|([^\\s]*\\\\[^\\s]+)|([^\\s]+\\/[^\\s]+)|([^\\s]+\\.[^\\s]+)|([^\\s]+_[^\\s]+)|(_+[^\\s]+_*)|(_*[^\\s]+_+)|([0-9\\|\\-\\r\\n\\t\\\"\\-#*=~:{}\\(\\)\\[\\]]+)|(info(rmation)?)\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    for tag in soup(['code']):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(regex_filter, ' ', text, flags=re.S)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    title = row['Question_title'].lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "    body = row['Question_body'].lower().encode('ascii', errors='ignore').decode('ascii')    \n",
    "    content = 'Title: ' + preprocess_text(title) + '; Content: ' + preprocess_text(body)\n",
    "\n",
    "    for tool_keyword in tools_keywords[row['Tool']]:\n",
    "        if tool_keyword in content:\n",
    "            content = content.replace(tool_keyword, '')\n",
    "\n",
    "    df_questions.at[index, 'Question_preprocessed_content'] = ' '.join(content.split())\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_questions.iterrows():        \n",
    "    if row['Question_preprocessed_content_gpt_summary']:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        response = retry_with_backoff(\n",
    "            openai.Completion.create,\n",
    "            model='text-davinci-003',\n",
    "            prompt=question_prompt + row['Question_preprocessed_content'] + '\"\"\"\\n',\n",
    "            temperature=0,\n",
    "            max_tokens=200,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=10,\n",
    "            stream=False\n",
    "        )\n",
    "        df_questions.at[index,\n",
    "                     'Question_preprocessed_content_gpt_summary'] = response['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        print(f'{e} on question {index}')\n",
    "        \n",
    "    if index % 50 == 0:\n",
    "        print(f'persisting on question {index}')\n",
    "        df_questions.to_json(os.path.join(\n",
    "            path_labeling_question, 'topic_modeling.json'), indent=4, orient='records')\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove \"title\" and \"content\" from the content\n",
    "# remove \"The user\" from the beginning of the summary\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "\n",
    "df_questions['Question_preprocessed_content'] = df_questions['Question_preprocessed_content'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "df_questions['Question_original_content_preprocessed_text'] = df_questions['Question_original_content_preprocessed_text'].apply(\n",
    "    lambda x: x.replace('Title: ', '').replace('Content: ', ''))\n",
    "\n",
    "df_questions['Question_original_content_gpt_summary'] = df_questions['Question_original_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "df_questions['Question_preprocessed_content_gpt_summary'] = df_questions['Question_preprocessed_content_gpt_summary'].apply(\n",
    "    lambda x: x.removeprefix('The user '))\n",
    "\n",
    "df_questions.to_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create answer dataset\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(path_labeling_question, 'original.json'))\n",
    "df_answers = []\n",
    "\n",
    "for index, row in df_questions.iterrows():\n",
    "    if row['Answer_body']:\n",
    "        df_answers.append(row)\n",
    "    elif row['Answer_list']:\n",
    "        df_answers.append(row)\n",
    "\n",
    "df_answers = pd.concat(df_answers, axis=1, ignore_index=True).T\n",
    "df_answers.to_json(os.path.join(path_labeling_answer, 'original.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add potential field to questions for later filling\n",
    "df_answers = pd.read_json(os.path.join(path_labeling_answer, 'original.json'))\n",
    "\n",
    "# Experiment 1: feed the original content to BerTopic\n",
    "df_answers['Answer_original_content_preprocessed_text'] = ''\n",
    "\n",
    "# Experiment 2: feed the original content to text-davinci-003 model and get the generated summary, then feed the summary to BerTopic\n",
    "df_answers['Answer_original_content_gpt_summary'] = ''\n",
    "\n",
    "# Experiment 3: feed the preprocessed content to BerTopic\n",
    "df_answers['Answer_preprocessed_content'] = ''\n",
    "\n",
    "# Experiment 4: feed the preprocessed content to text-davinci-003 model and get the generated summary, then feed the summary to BerTopic\n",
    "df_answers['Answer_preprocessed_content_gpt_summary'] = ''\n",
    "\n",
    "df_answers.to_json(os.path.join(path_labeling_answer, 'topic_modeling.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 5\n",
    "\n",
    "with open(os.path.join(path_labeling_answer, 'topic_modeling.json'), encoding='utf8') as answer_file:\n",
    "    df_answers = json.load(answer_file)\n",
    "    \n",
    "    for row in df_answers:\n",
    "        question = row['Question_title']\n",
    "        answer = ''\n",
    "        if row['Answer_body']:\n",
    "            answer = row['Answer_body']\n",
    "        else:\n",
    "            if row['Question_has_accepted_answer']:\n",
    "                if 'Answer_has_accepted' in row['Answer_list'][0]:\n",
    "                    for comment in row['Answer_list']:\n",
    "                        if comment['Answer_has_accepted']:\n",
    "                            answer = comment['Answer_body']\n",
    "                            break\n",
    "                else:\n",
    "                    for comment in row['Answer_list']:\n",
    "                        if comment['accepted_answer']:\n",
    "                            answer = comment['cooked']\n",
    "                            break\n",
    "            elif 'Answer_body' in row['Answer_list'][0]:\n",
    "                for comment in row['Answer_list']:\n",
    "                    answer += comment['Answer_body'] + '\\n'\n",
    "            elif 'cooked' in row['Answer_list'][0]:\n",
    "                for comment in row['Answer_list']:\n",
    "                    answer += comment['cooked'] + '\\n'\n",
    "        \n",
    "        question = question.lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "        answer = answer.lower().encode('ascii', errors='ignore').decode('ascii')        \n",
    "        content = 'Question: ' + question + '; Answer: ' + answer\n",
    "    \n",
    "        for tool_keyword in tools_keywords[row['Tool']]:\n",
    "            if tool_keyword in content:\n",
    "                content = content.replace(tool_keyword, '')\n",
    "        \n",
    "        row['Answer_original_content_preprocessed_text'] = ' '.join(content.split())\n",
    "\n",
    "with open(os.path.join(path_labeling_answer, 'topic_modeling.json'), 'w') as outfile:\n",
    "    json_post_list = json.dumps(df_answers, indent='\\t')\n",
    "    outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6 & 8\n",
    "\n",
    "import random\n",
    "\n",
    "answer_prompt = 'Please list the solutions (if any) from the following answer. For instance, you could begin with a sentence such as: \"There are three solutions together: [1]. XXX; [2]. YYY; [3]. ZZZ.\".\\n\"\"\"'\n",
    "\n",
    "def retry_with_backoff(fn, retries=2, backoff_in_seconds=1, *args, **kwargs):\n",
    "    x = 0\n",
    "\n",
    "    if args is None:\n",
    "        args = []\n",
    "    if kwargs is None:\n",
    "        kwargs = {}\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            return fn(*args, **kwargs)\n",
    "        except:\n",
    "            if x == retries:\n",
    "                raise\n",
    "\n",
    "            sleep = backoff_in_seconds * 2 ** x + random.uniform(0, 1)\n",
    "            time.sleep(sleep)\n",
    "            x += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 6\n",
    "\n",
    "df_answers = pd.read_json(os.path.join(\n",
    "    path_labeling_answer, 'topic_modeling.json'))\n",
    "\n",
    "for index, row in df_answers.iterrows():\n",
    "    if row['Question_original_content_gpt_summary']:\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        response = retry_with_backoff(\n",
    "            openai.Completion.create,\n",
    "            model='text-davinci-003',\n",
    "            prompt=question_prompt +\n",
    "            row['Answer_original_content_gpt_summary'] + '\"\"\"\\n',\n",
    "            temperature=0,\n",
    "            max_tokens=200,\n",
    "            top_p=1,\n",
    "            frequency_penalty=0,\n",
    "            presence_penalty=0,\n",
    "            timeout=10,\n",
    "            stream=False\n",
    "        )\n",
    "        df_answers.at[index, 'Question_original_content_gpt_summary'] = response['choices'][0]['text'].strip()\n",
    "    except Exception as e:\n",
    "        print(f'{e} on answer {index}')\n",
    "        \n",
    "    if index % 50 == 0:\n",
    "        print(f'persisting on answer {index}')\n",
    "        df_answers.to_json(os.path.join(\n",
    "            path_labeling_answer, 'topic_modeling.json'), indent=4, orient='records')\n",
    "        \n",
    "    time.sleep(5)\n",
    "\n",
    "df_answers.to_json(os.path.join(\n",
    "    path_labeling_answer, 'topic_modeling.json'), indent=4, orient='records')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "# content filtering patterns\n",
    "regex_filter = r\"(<.*?>)|({.*?})|((!)?\\[.*?\\])|(\\(.*?\\))|(\\`{3}.+?\\`{3})|(\\`{2}.+?\\`{2})|(\\`{1}.+?\\`{1})|([^\\s]*[<=>]=[^\\s]+)|(@[^\\s]+)|((https?:\\/)?\\/[^\\s]+)|([^\\s]*\\\\[^\\s]+)|([^\\s]+\\/[^\\s]+)|([^\\s]+\\.[^\\s]+)|([^\\s]+_[^\\s]+)|(_+[^\\s]+_*)|(_*[^\\s]+_+)|([0-9\\|\\-\\r\\n\\t\\\"\\-#*=~:{}\\(\\)\\[\\]]+)|(info(rmation)?)\"\n",
    "\n",
    "def preprocess_text(text):\n",
    "    soup = BeautifulSoup(text, 'html.parser')\n",
    "    for tag in soup(['code']):\n",
    "        tag.decompose()\n",
    "    text = soup.get_text()\n",
    "    text = re.sub(regex_filter, ' ', text, flags=re.S)\n",
    "    text = ' '.join(text.split())\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 7\n",
    "\n",
    "with open(os.path.join(path_labeling_answer, 'topic_modeling.json'), encoding='utf8') as answer_file:\n",
    "    df_answers = json.load(answer_file)\n",
    "    \n",
    "    for row in df_answers:\n",
    "        question = row['Question_title']\n",
    "        answer = ''\n",
    "        if row['Answer_body']:\n",
    "            answer = row['Answer_body']\n",
    "        else:\n",
    "            if row['Question_has_accepted_answer']:\n",
    "                if 'Answer_has_accepted' in row['Answer_list'][0]:\n",
    "                    for comment in row['Answer_list']:\n",
    "                        if comment['Answer_has_accepted']:\n",
    "                            answer = comment['Answer_body']\n",
    "                            break\n",
    "                else:\n",
    "                    for comment in row['Answer_list']:\n",
    "                        if comment['accepted_answer']:\n",
    "                            answer = comment['cooked']\n",
    "                            break\n",
    "            elif 'Answer_body' in row['Answer_list'][0]:\n",
    "                for comment in row['Answer_list']:\n",
    "                    answer += comment['Answer_body'] + '\\n'\n",
    "            elif 'cooked' in row['Answer_list'][0]:\n",
    "                for comment in row['Answer_list']:\n",
    "                    answer += comment['cooked'] + '\\n'\n",
    "        \n",
    "        question = question.lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "        answer = answer.lower().encode('ascii', errors='ignore').decode('ascii')\n",
    "        content = 'Question: ' + preprocess_text(question) + '; Answer: ' + preprocess_text(answer)\n",
    "    \n",
    "        for tool_keyword in tools_keywords[row['Tool']]:\n",
    "            if tool_keyword in content:\n",
    "                content = content.replace(tool_keyword, '')\n",
    "        \n",
    "        row['Answer_preprocessed_content'] = ' '.join(content.split())\n",
    "\n",
    "with open(os.path.join(path_labeling_answer, 'topic_modeling.json'), 'w') as outfile:\n",
    "    json_post_list = json.dumps(df_answers, indent='\\t')\n",
    "    outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set general sweep configuration\n",
    "sweep_configuration = {\n",
    "    \"metric\": {\n",
    "        'name': 'CoherenceCV',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        'n_neighbors': {\n",
    "            'values': list(range(10, 22, 2))\n",
    "        },\n",
    "        'n_components': {\n",
    "            'values': list(range(4, 12, 2))\n",
    "        },\n",
    "        'min_cluster_size': {\n",
    "            'values': list(range(20, 110, 10))\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# set default sweep configuration\n",
    "config_defaults = {\n",
    "    'model_name': 'all-mpnet-base-v2',\n",
    "    'metric_distane': 'manhattan',\n",
    "    'low_memory': True,\n",
    "    'stop_words': 'english',\n",
    "    'ngram_range': (1, 3),\n",
    "    'reduce_frequent_words': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 1\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "docs = df_questions['Question_original_content_preprocessed_text'].tolist()\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        # update any values not set by sweep\n",
    "        run.config.setdefaults(config_defaults)\n",
    "\n",
    "        # Step 1 - Extract embeddings\n",
    "        embedding_model = SentenceTransformer(run.config.model_name)\n",
    "\n",
    "        # Step 2 - Reduce dimensionality\n",
    "        umap_model = UMAP(n_neighbors=wandb.config.n_neighbors, n_components=wandb.config.n_components,\n",
    "                          metric=run.config.metric_distane, low_memory=run.config.low_memory)\n",
    "\n",
    "        # Step 3 - Cluster reduced embeddings\n",
    "        hdbscan_model = HDBSCAN()\n",
    "\n",
    "        # Step 4 - Tokenize topics\n",
    "        vectorizer_model = TfidfVectorizer(\n",
    "            stop_words=run.config.stop_words, ngram_range=run.config.ngram_range)\n",
    "\n",
    "        # Step 5 - Create topic representation\n",
    "        ctfidf_model = ClassTfidfTransformer(\n",
    "            reduce_frequent_words=run.config.reduce_frequent_words)\n",
    "\n",
    "        # Step 6 - Fine-tune topic representation\n",
    "        representation_model = KeyBERTInspired()\n",
    "\n",
    "        # All steps together\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            representation_model=representation_model,\n",
    "            # Step 7 - Track model stages\n",
    "            # verbose=True\n",
    "        )\n",
    "\n",
    "        topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "        # Preprocess documents\n",
    "        documents = pd.DataFrame(\n",
    "            {\"Document\": docs,\n",
    "             \"ID\": range(len(docs)),\n",
    "             \"Topic\": topics}\n",
    "        )\n",
    "        documents_per_topic = documents.groupby(\n",
    "            ['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "        cleaned_docs = topic_model._preprocess_text(\n",
    "            documents_per_topic.Document.values)\n",
    "\n",
    "        # Extract vectorizer and analyzer from fit model\n",
    "        analyzer = vectorizer_model.build_analyzer()\n",
    "        # Extract features for topic coherence evaluation\n",
    "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "        dictionary = corpora.Dictionary(tokens)\n",
    "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "        topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
    "                       for topic in range(len(set(topics))-1)]\n",
    "\n",
    "        coherence_cv = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "\n",
    "        coherence_umass = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='u_mass'\n",
    "        )\n",
    "\n",
    "        coherence_cuci = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_uci'\n",
    "        )\n",
    "\n",
    "        coherence_cnpmi = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_npmi'\n",
    "        )\n",
    "\n",
    "        wandb.log({'CoherenceCV': coherence_cv.get_coherence()})\n",
    "        wandb.log({'CoherenceUMASS': coherence_umass.get_coherence()})\n",
    "        wandb.log({'CoherenceUCI': coherence_cuci.get_coherence()})\n",
    "        wandb.log({'CoherenceNPMI': coherence_cnpmi.get_coherence()})\n",
    "\n",
    "\n",
    "sweep_configuration['name'] = 'question-experiment-1'\n",
    "sweep_id = wandb.sweep(sweep_configuration, project=wandb_project)\n",
    "# Create sweep with ID: j7pnz7gn\n",
    "wandb.agent(sweep_id=sweep_id, function=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 2\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "docs = df_questions['Question_original_content_gpt_summary'].tolist()\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        # update any values not set by sweep\n",
    "        run.config.setdefaults(config_defaults)\n",
    "\n",
    "        # Step 1 - Extract embeddings\n",
    "        embedding_model = SentenceTransformer(run.config.model_name)\n",
    "\n",
    "        # Step 2 - Reduce dimensionality\n",
    "        umap_model = UMAP(n_neighbors=wandb.config.n_neighbors, n_components=wandb.config.n_components,\n",
    "                          metric=run.config.metric_distane, low_memory=run.config.low_memory)\n",
    "\n",
    "        # Step 3 - Cluster reduced embeddings\n",
    "        hdbscan_model = HDBSCAN()\n",
    "\n",
    "        # Step 4 - Tokenize topics\n",
    "        vectorizer_model = TfidfVectorizer(\n",
    "            stop_words=run.config.stop_words, ngram_range=run.config.ngram_range)\n",
    "\n",
    "        # Step 5 - Create topic representation\n",
    "        ctfidf_model = ClassTfidfTransformer(\n",
    "            reduce_frequent_words=run.config.reduce_frequent_words)\n",
    "\n",
    "        # Step 6 - Fine-tune topic representation\n",
    "        representation_model = KeyBERTInspired()\n",
    "\n",
    "        # All steps together\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            representation_model=representation_model,\n",
    "            # Step 7 - Track model stages\n",
    "            # verbose=True\n",
    "        )\n",
    "\n",
    "        topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "        # Preprocess documents\n",
    "        documents = pd.DataFrame(\n",
    "            {\"Document\": docs,\n",
    "             \"ID\": range(len(docs)),\n",
    "             \"Topic\": topics}\n",
    "        )\n",
    "        documents_per_topic = documents.groupby(\n",
    "            ['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "        cleaned_docs = topic_model._preprocess_text(\n",
    "            documents_per_topic.Document.values)\n",
    "\n",
    "        # Extract vectorizer and analyzer from fit model\n",
    "        analyzer = vectorizer_model.build_analyzer()\n",
    "        # Extract features for topic coherence evaluation\n",
    "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "        dictionary = corpora.Dictionary(tokens)\n",
    "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "        topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
    "                       for topic in range(len(set(topics))-1)]\n",
    "\n",
    "        coherence_cv = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "\n",
    "        coherence_umass = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='u_mass'\n",
    "        )\n",
    "\n",
    "        coherence_cuci = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_uci'\n",
    "        )\n",
    "\n",
    "        coherence_cnpmi = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_npmi'\n",
    "        )\n",
    "\n",
    "        wandb.log({'CoherenceCV': coherence_cv.get_coherence()})\n",
    "        wandb.log({'CoherenceUMASS': coherence_umass.get_coherence()})\n",
    "        wandb.log({'CoherenceUCI': coherence_cuci.get_coherence()})\n",
    "        wandb.log({'CoherenceNPMI': coherence_cnpmi.get_coherence()})\n",
    "\n",
    "\n",
    "sweep_configuration['name'] = 'question-experiment-2'\n",
    "sweep_id = wandb.sweep(sweep_configuration, project=wandb_project)\n",
    "# Create sweep with ID: j7pnz7gn\n",
    "wandb.agent(sweep_id=sweep_id, function=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 3\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "docs = df_questions['Question_preprocessed_content'].tolist()\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        # update any values not set by sweep\n",
    "        run.config.setdefaults(config_defaults)\n",
    "\n",
    "        # Step 1 - Extract embeddings\n",
    "        embedding_model = SentenceTransformer(run.config.model_name)\n",
    "\n",
    "        # Step 2 - Reduce dimensionality\n",
    "        umap_model = UMAP(n_neighbors=wandb.config.n_neighbors, n_components=wandb.config.n_components,\n",
    "                          metric=run.config.metric_distane, low_memory=run.config.low_memory)\n",
    "\n",
    "        # Step 3 - Cluster reduced embeddings\n",
    "        hdbscan_model = HDBSCAN()\n",
    "\n",
    "        # Step 4 - Tokenize topics\n",
    "        vectorizer_model = TfidfVectorizer(\n",
    "            stop_words=run.config.stop_words, ngram_range=run.config.ngram_range)\n",
    "\n",
    "        # Step 5 - Create topic representation\n",
    "        ctfidf_model = ClassTfidfTransformer(\n",
    "            reduce_frequent_words=run.config.reduce_frequent_words)\n",
    "\n",
    "        # Step 6 - Fine-tune topic representation\n",
    "        representation_model = KeyBERTInspired()\n",
    "\n",
    "        # All steps together\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            representation_model=representation_model,\n",
    "            # Step 7 - Track model stages\n",
    "            # verbose=True\n",
    "        )\n",
    "\n",
    "        topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "        # Preprocess documents\n",
    "        documents = pd.DataFrame(\n",
    "            {\"Document\": docs,\n",
    "             \"ID\": range(len(docs)),\n",
    "             \"Topic\": topics}\n",
    "        )\n",
    "        documents_per_topic = documents.groupby(\n",
    "            ['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "        cleaned_docs = topic_model._preprocess_text(\n",
    "            documents_per_topic.Document.values)\n",
    "\n",
    "        # Extract vectorizer and analyzer from fit model\n",
    "        analyzer = vectorizer_model.build_analyzer()\n",
    "        # Extract features for topic coherence evaluation\n",
    "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "        dictionary = corpora.Dictionary(tokens)\n",
    "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "        topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
    "                       for topic in range(len(set(topics))-1)]\n",
    "\n",
    "        coherence_cv = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "\n",
    "        coherence_umass = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='u_mass'\n",
    "        )\n",
    "\n",
    "        coherence_cuci = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_uci'\n",
    "        )\n",
    "\n",
    "        coherence_cnpmi = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_npmi'\n",
    "        )\n",
    "\n",
    "        wandb.log({'CoherenceCV': coherence_cv.get_coherence()})\n",
    "        wandb.log({'CoherenceUMASS': coherence_umass.get_coherence()})\n",
    "        wandb.log({'CoherenceUCI': coherence_cuci.get_coherence()})\n",
    "        wandb.log({'CoherenceNPMI': coherence_cnpmi.get_coherence()})\n",
    "\n",
    "\n",
    "sweep_configuration['name'] = 'question-experiment-3'\n",
    "sweep_id = wandb.sweep(sweep_configuration, project=wandb_project)\n",
    "# Create sweep with ID: j7pnz7gn\n",
    "wandb.agent(sweep_id=sweep_id, function=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment 4\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "docs = df_questions['Question_preprocessed_content_gpt_summary'].tolist()\n",
    "\n",
    "\n",
    "def train():\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init() as run:\n",
    "        # update any values not set by sweep\n",
    "        run.config.setdefaults(config_defaults)\n",
    "\n",
    "        # Step 1 - Extract embeddings\n",
    "        embedding_model = SentenceTransformer(run.config.model_name)\n",
    "\n",
    "        # Step 2 - Reduce dimensionality\n",
    "        umap_model = UMAP(n_neighbors=wandb.config.n_neighbors, n_components=wandb.config.n_components,\n",
    "                          metric=run.config.metric_distane, low_memory=run.config.low_memory)\n",
    "\n",
    "        # Step 3 - Cluster reduced embeddings\n",
    "        hdbscan_model = HDBSCAN()\n",
    "\n",
    "        # Step 4 - Tokenize topics\n",
    "        vectorizer_model = TfidfVectorizer(\n",
    "            stop_words=run.config.stop_words, ngram_range=run.config.ngram_range)\n",
    "\n",
    "        # Step 5 - Create topic representation\n",
    "        ctfidf_model = ClassTfidfTransformer(\n",
    "            reduce_frequent_words=run.config.reduce_frequent_words)\n",
    "\n",
    "        # Step 6 - Fine-tune topic representation\n",
    "        representation_model = KeyBERTInspired()\n",
    "\n",
    "        # All steps together\n",
    "        topic_model = BERTopic(\n",
    "            embedding_model=embedding_model,\n",
    "            umap_model=umap_model,\n",
    "            hdbscan_model=hdbscan_model,\n",
    "            vectorizer_model=vectorizer_model,\n",
    "            ctfidf_model=ctfidf_model,\n",
    "            representation_model=representation_model,\n",
    "            # Step 7 - Track model stages\n",
    "            # verbose=True\n",
    "        )\n",
    "\n",
    "        topics, _ = topic_model.fit_transform(docs)\n",
    "\n",
    "        # Preprocess documents\n",
    "        documents = pd.DataFrame(\n",
    "            {\"Document\": docs,\n",
    "             \"ID\": range(len(docs)),\n",
    "             \"Topic\": topics}\n",
    "        )\n",
    "        documents_per_topic = documents.groupby(\n",
    "            ['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "        cleaned_docs = topic_model._preprocess_text(\n",
    "            documents_per_topic.Document.values)\n",
    "\n",
    "        # Extract vectorizer and analyzer from fit model\n",
    "        analyzer = vectorizer_model.build_analyzer()\n",
    "        # Extract features for topic coherence evaluation\n",
    "        tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "        dictionary = corpora.Dictionary(tokens)\n",
    "        corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "        topic_words = [[words for words, _ in topic_model.get_topic(topic)]\n",
    "                       for topic in range(len(set(topics))-1)]\n",
    "\n",
    "        coherence_cv = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_v'\n",
    "        )\n",
    "\n",
    "        coherence_umass = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='u_mass'\n",
    "        )\n",
    "\n",
    "        coherence_cuci = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_uci'\n",
    "        )\n",
    "\n",
    "        coherence_cnpmi = CoherenceModel(\n",
    "            topics=topic_words,\n",
    "            texts=tokens,\n",
    "            corpus=corpus,\n",
    "            dictionary=dictionary,\n",
    "            coherence='c_npmi'\n",
    "        )\n",
    "\n",
    "        wandb.log({'CoherenceCV': coherence_cv.get_coherence()})\n",
    "        wandb.log({'CoherenceUMASS': coherence_umass.get_coherence()})\n",
    "        wandb.log({'CoherenceUCI': coherence_cuci.get_coherence()})\n",
    "        wandb.log({'CoherenceNPMI': coherence_cnpmi.get_coherence()})\n",
    "\n",
    "\n",
    "sweep_configuration['name'] = 'question-experiment-4'\n",
    "sweep_id = wandb.sweep(sweep_configuration, project=wandb_project)\n",
    "# Create sweep with ID: l21k4zhd\n",
    "wandb.agent(sweep_id=sweep_id, function=train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1</td>\n",
       "      <td>4603</td>\n",
       "      <td>-1_tensorflow serving_mlflow_tensorflow_deploy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>268</td>\n",
       "      <td>0_batch prediction_forecasting model_timeserie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>251</td>\n",
       "      <td>1_importerror module named_pip installed_pip i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>228</td>\n",
       "      <td>2_git lfs_files remote_version control_shared ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>185</td>\n",
       "      <td>3_custom charts_logged metrics_metrics cloudwa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>181</td>\n",
       "      <td>4_passing data pipeline_data pipeline_paramete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>170</td>\n",
       "      <td>5_deployment failed_trying deploy_deploy model...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>158</td>\n",
       "      <td>6_rstudio application_custom rstudio_rstudio_e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>151</td>\n",
       "      <td>7_created docker image_docker run_environment ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>135</td>\n",
       "      <td>8_azure data lake_datastore azure_dataset init...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>127</td>\n",
       "      <td>9_labelling job_custom labeling job_labeling j...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>10</td>\n",
       "      <td>126</td>\n",
       "      <td>10_endpoint error_endpoint aws_endpoint input_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>11</td>\n",
       "      <td>116</td>\n",
       "      <td>11_dask dataframe_pandas data_dataframe conten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>12</td>\n",
       "      <td>112</td>\n",
       "      <td>12_memoryerror unable_memoryerror_memoryerror ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>13</td>\n",
       "      <td>108</td>\n",
       "      <td>13_gpuutilization metric computed_gpuutilizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>14</td>\n",
       "      <td>102</td>\n",
       "      <td>14_mount azure blob_file mount azure_job faile...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>15</td>\n",
       "      <td>96</td>\n",
       "      <td>15_hyperparameter sweep_parameters sweep_param...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>16</td>\n",
       "      <td>92</td>\n",
       "      <td>16_pyspark using_pyspark notebook_model spark ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>17</td>\n",
       "      <td>86</td>\n",
       "      <td>17_bucket notebook instance_aws bucket_aws buc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>18</td>\n",
       "      <td>72</td>\n",
       "      <td>18_running jupyter notebook_run jupyter notebo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>19</td>\n",
       "      <td>72</td>\n",
       "      <td>19_xgboost hyperparameter tuning_hyperparamete...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>20</td>\n",
       "      <td>66</td>\n",
       "      <td>20_create compute instance_start compute insta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>21</td>\n",
       "      <td>66</td>\n",
       "      <td>21_run distributed training_support distribute...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>22</td>\n",
       "      <td>66</td>\n",
       "      <td>22_major features improvements_models add supp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>23</td>\n",
       "      <td>65</td>\n",
       "      <td>23_deploy multiple models_deploy model trained...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>24</td>\n",
       "      <td>62</td>\n",
       "      <td>24_vs azure databricks_title difference_usage ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>25</td>\n",
       "      <td>60</td>\n",
       "      <td>25_artifacts stored locally_artifact storage_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>26</td>\n",
       "      <td>59</td>\n",
       "      <td>26_error submitting pipeline_pipeline run_subm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>27</td>\n",
       "      <td>59</td>\n",
       "      <td>27_azure pricing_azure_microsoft community_aws...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>28</td>\n",
       "      <td>56</td>\n",
       "      <td>28_google cloud vision_cloud vision api_azure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>29</td>\n",
       "      <td>56</td>\n",
       "      <td>29_data passed numpy_xgboost model_using batch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>30</td>\n",
       "      <td>52</td>\n",
       "      <td>30_organizations using_company list_organizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>31</td>\n",
       "      <td>47</td>\n",
       "      <td>31_model versioning_model version content_regi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>32</td>\n",
       "      <td>45</td>\n",
       "      <td>32_issues guild file_guild run notebook_given ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>33</td>\n",
       "      <td>43</td>\n",
       "      <td>33_speech text work_speech text using_speech t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>34</td>\n",
       "      <td>42</td>\n",
       "      <td>34_role amazon executionrole_amazon executionr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>35</td>\n",
       "      <td>42</td>\n",
       "      <td>35_container gives filenotfounderror_docker im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>36</td>\n",
       "      <td>41</td>\n",
       "      <td>36_web service azure_web service output_azure ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>37</td>\n",
       "      <td>37</td>\n",
       "      <td>37_lifecycle configuration script_lifecycle co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>38</td>\n",
       "      <td>37</td>\n",
       "      <td>38_permanently delete experiment_delete experi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>39</td>\n",
       "      <td>34</td>\n",
       "      <td>39_tensorboard logging_tensorboard logs cloud_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>40</td>\n",
       "      <td>34</td>\n",
       "      <td>40_model file saved_model file_provided model ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>41</td>\n",
       "      <td>33</td>\n",
       "      <td>41_tutorial failed load_errored notfounderror_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>42</td>\n",
       "      <td>30</td>\n",
       "      <td>42_log outputs file_logging file_logs notebook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>43</td>\n",
       "      <td>28</td>\n",
       "      <td>43_google cloud translate_google cloud transla...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>44</td>\n",
       "      <td>24</td>\n",
       "      <td>44_endpoint autoscaling_autoscaling endpoint_a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>45</td>\n",
       "      <td>21</td>\n",
       "      <td>45_cancel job_stop job_job stuck_job status ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>46</td>\n",
       "      <td>20</td>\n",
       "      <td>46_notebook instance ip_ip notebook instance_n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                               Name\n",
       "0      -1   4603     -1_tensorflow serving_mlflow_tensorflow_deploy\n",
       "1       0    268  0_batch prediction_forecasting model_timeserie...\n",
       "2       1    251  1_importerror module named_pip installed_pip i...\n",
       "3       2    228  2_git lfs_files remote_version control_shared ...\n",
       "4       3    185  3_custom charts_logged metrics_metrics cloudwa...\n",
       "5       4    181  4_passing data pipeline_data pipeline_paramete...\n",
       "6       5    170  5_deployment failed_trying deploy_deploy model...\n",
       "7       6    158  6_rstudio application_custom rstudio_rstudio_e...\n",
       "8       7    151  7_created docker image_docker run_environment ...\n",
       "9       8    135  8_azure data lake_datastore azure_dataset init...\n",
       "10      9    127  9_labelling job_custom labeling job_labeling j...\n",
       "11     10    126  10_endpoint error_endpoint aws_endpoint input_...\n",
       "12     11    116  11_dask dataframe_pandas data_dataframe conten...\n",
       "13     12    112  12_memoryerror unable_memoryerror_memoryerror ...\n",
       "14     13    108  13_gpuutilization metric computed_gpuutilizati...\n",
       "15     14    102  14_mount azure blob_file mount azure_job faile...\n",
       "16     15     96  15_hyperparameter sweep_parameters sweep_param...\n",
       "17     16     92  16_pyspark using_pyspark notebook_model spark ...\n",
       "18     17     86  17_bucket notebook instance_aws bucket_aws buc...\n",
       "19     18     72  18_running jupyter notebook_run jupyter notebo...\n",
       "20     19     72  19_xgboost hyperparameter tuning_hyperparamete...\n",
       "21     20     66  20_create compute instance_start compute insta...\n",
       "22     21     66  21_run distributed training_support distribute...\n",
       "23     22     66  22_major features improvements_models add supp...\n",
       "24     23     65  23_deploy multiple models_deploy model trained...\n",
       "25     24     62  24_vs azure databricks_title difference_usage ...\n",
       "26     25     60  25_artifacts stored locally_artifact storage_a...\n",
       "27     26     59  26_error submitting pipeline_pipeline run_subm...\n",
       "28     27     59  27_azure pricing_azure_microsoft community_aws...\n",
       "29     28     56  28_google cloud vision_cloud vision api_azure ...\n",
       "30     29     56  29_data passed numpy_xgboost model_using batch...\n",
       "31     30     52  30_organizations using_company list_organizati...\n",
       "32     31     47  31_model versioning_model version content_regi...\n",
       "33     32     45  32_issues guild file_guild run notebook_given ...\n",
       "34     33     43  33_speech text work_speech text using_speech t...\n",
       "35     34     42  34_role amazon executionrole_amazon executionr...\n",
       "36     35     42  35_container gives filenotfounderror_docker im...\n",
       "37     36     41  36_web service azure_web service output_azure ...\n",
       "38     37     37  37_lifecycle configuration script_lifecycle co...\n",
       "39     38     37  38_permanently delete experiment_delete experi...\n",
       "40     39     34  39_tensorboard logging_tensorboard logs cloud_...\n",
       "41     40     34  40_model file saved_model file_provided model ...\n",
       "42     41     33  41_tutorial failed load_errored notfounderror_...\n",
       "43     42     30  42_log outputs file_logging file_logs notebook...\n",
       "44     43     28  43_google cloud translate_google cloud transla...\n",
       "45     44     24  44_endpoint autoscaling_autoscaling endpoint_a...\n",
       "46     45     21  45_cancel job_stop job_job stuck_job status ca...\n",
       "47     46     20  46_notebook instance ip_ip notebook instance_n..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# visualize the best topic model\n",
    "\n",
    "# Step 1 - Extract embeddings\n",
    "embedding_model = SentenceTransformer(\"all-mpnet-base-v2\")\n",
    "\n",
    "# Step 2 - Reduce dimensionality\n",
    "umap_model = UMAP(n_neighbors=18, n_components=8, metric='manhattan')\n",
    "\n",
    "# Step 3 - Cluster reduced embeddings\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=20, max_cluster_size=1000)\n",
    "\n",
    "# Step 4 - Tokenize topics\n",
    "vectorizer_model = TfidfVectorizer(stop_words=\"english\", ngram_range=(1, 3))\n",
    "\n",
    "# Step 5 - Create topic representation\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "# Step 6 - (Optional) Fine-tune topic representation\n",
    "representation_model = KeyBERTInspired()\n",
    "\n",
    "# All steps together\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=embedding_model,            # Step 1 - Extract embeddings\n",
    "    umap_model=umap_model,                      # Step 2 - Reduce dimensionality\n",
    "    hdbscan_model=hdbscan_model,                # Step 3 - Cluster reduced embeddings\n",
    "    vectorizer_model=vectorizer_model,          # Step 4 - Tokenize topics\n",
    "    ctfidf_model=ctfidf_model,                  # Step 5 - Extract topic words\n",
    "    # Step 6 - (Optional) Fine-tune topic represenations\n",
    "    representation_model=representation_model,\n",
    "    # verbose=True                              # Step 7 - Track model stages\n",
    ")\n",
    "\n",
    "df_questions = pd.read_json(os.path.join(\n",
    "    path_labeling_question, 'topic_modeling.json'))\n",
    "docs = df_questions['Question_preprocessed_content'].tolist()\n",
    "\n",
    "topics, _ = topic_model.fit_transform(docs)\n",
    "# topic_model.save(os.path.join(path_labeling_question, 'Topic model'))\n",
    "\n",
    "# fig = topic_model.visualize_topics()\n",
    "# fig.write_html(os.path.join(path_labeling_question, 'Topic visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_barchart()\n",
    "# fig.write_html(os.path.join(path_labeling_question, 'Term visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_heatmap()\n",
    "# fig.write_html(os.path.join(path_labeling_question,\n",
    "#                'Topic similarity visualization.html'))\n",
    "\n",
    "# fig = topic_model.visualize_term_rank()\n",
    "# fig.write_html(os.path.join(path_labeling_question,\n",
    "#                'Term score decline visualization.html'))\n",
    "\n",
    "# hierarchical_topics = topic_model.hierarchical_topics(docs)\n",
    "# fig = topic_model.visualize_hierarchy(hierarchical_topics=hierarchical_topics)\n",
    "# fig.write_html(os.path.join(path_labeling_question,\n",
    "#                'Hierarchical clustering visualization.html'))\n",
    "\n",
    "# embeddings = embedding_model.encode(docs, show_progress_bar=False)\n",
    "# fig = topic_model.visualize_documents(docs, embeddings=embeddings)\n",
    "# fig.write_html(os.path.join(path_labeling_question,\n",
    "#                'Document visualization.html'))\n",
    "\n",
    "info_df = topic_model.get_topic_info()\n",
    "info_df\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
