{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a fraction of elements from a list\n",
    "def random_sample(lst, frac=0.1, lowerbound=20, upperbound=50):\n",
    "    sample_size = int(len(lst) * frac)\n",
    "    if sample_size < lowerbound:\n",
    "        sample_size = lowerbound\n",
    "    if sample_size > min(len(lst), upperbound):\n",
    "        sample_size = min(len(lst), upperbound)\n",
    "    return random.sample(lst, sample_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = '../Dataset'\n",
    "\n",
    "raw_path = os.path.join(root_path, 'Raw')\n",
    "\n",
    "if not os.path.exists(raw_path):\n",
    "    os.makedirs(raw_path)\n",
    "    \n",
    "filter_path = os.path.join(root_path, 'Filter')\n",
    "\n",
    "if not os.path.isdir(filter_path):\n",
    "    os.mkdir(filter_path)\n",
    "    \n",
    "round1_path = os.path.join(filter_path, 'Round#1')\n",
    "    \n",
    "if not os.path.isdir(round1_path):\n",
    "    os.mkdir(round1_path)\n",
    "    \n",
    "others_round1_path = os.path.join(round1_path, 'Others')\n",
    "    \n",
    "if not os.path.isdir(others_round1_path):\n",
    "    os.mkdir(others_round1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = -1\n",
    "base_url = 'https://my.guild.ai/'\n",
    "page_suffix = 'c/troubleshooting/6.json?page='\n",
    "post_list = []\n",
    "\n",
    "while True:\n",
    "    page = page + 1\n",
    "    page_url = base_url + page_suffix + str(page)\n",
    "    topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "    for topic in topic_list['topics']:\n",
    "        post_url = base_url + 't/' + \\\n",
    "            topic['slug'] + '/' + str(topic['id']) + '.json'\n",
    "\n",
    "        post = {}\n",
    "        post['Question_title'] = topic['title']\n",
    "        post['Question_link'] = post_url\n",
    "        post['Question_creation_date'] = topic['created_at']\n",
    "        post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "        post['Question_upvote_count'] = topic['like_count']\n",
    "        post['Question_view_count'] = topic['views']\n",
    "        post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "        comments = requests.get(post_url).json()['post_stream']['posts']\n",
    "        post['Question_body'] = comments[0]['cooked']\n",
    "        post['Answers'] = comments[1:]\n",
    "        post_list.append(post)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    if 'more_topics_url' not in topic_list.keys():\n",
    "        break\n",
    "\n",
    "json_post_list = json.dumps(post_list, indent='\\t')\n",
    "with open(os.path.join(raw_path, 'Guild AI.json'), 'w') as outfile:\n",
    "    outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = -1\n",
    "base_url = 'https://community.wandb.ai/'\n",
    "page_suffix = 'c/w-b-support/36.json?page='\n",
    "post_list = []\n",
    "\n",
    "while True:\n",
    "    page = page + 1\n",
    "    page_url = base_url + page_suffix + str(page)\n",
    "    topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "    for topic in topic_list['topics']:\n",
    "        post_url = base_url + 't/' + \\\n",
    "            topic['slug'] + '/' + str(topic['id']) + '.json'\n",
    "\n",
    "        post = {}\n",
    "        post['Question_title'] = topic['title']\n",
    "        post['Question_link'] = post_url\n",
    "        post['Question_creation_date'] = topic['created_at']\n",
    "        post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "        post['Question_upvote_count'] = topic['like_count']\n",
    "        post['Question_view_count'] = topic['views']\n",
    "        post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "        comments = requests.get(post_url).json()['post_stream']['posts']\n",
    "        post['Question_body'] = comments[0]['cooked']\n",
    "        post['Answers'] = comments[1:]\n",
    "        post_list.append(post)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    if 'more_topics_url' not in topic_list.keys():\n",
    "        break\n",
    "\n",
    "json_post_list = json.dumps(post_list, indent='\\t')\n",
    "with open(os.path.join(raw_path, 'Weights & Biases.json'), 'w') as outfile:\n",
    "    outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = -1\n",
    "base_url = 'https://community.sigopt.com/'\n",
    "page_suffix = 'c/general-discussion/9.json?page='\n",
    "post_list = []\n",
    "\n",
    "while True:\n",
    "    page = page + 1\n",
    "    page_url = base_url + page_suffix + str(page)\n",
    "    topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "    for topic in topic_list['topics']:\n",
    "        post_url = base_url + 't/' + \\\n",
    "            topic['slug'] + '/' + str(topic['id']) + '.json'\n",
    "\n",
    "        post = {}\n",
    "        post['Question_title'] = topic['title']\n",
    "        post['Question_link'] = post_url\n",
    "        post['Question_creation_date'] = topic['created_at']\n",
    "        post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "        post['Question_upvote_count'] = topic['like_count']\n",
    "        post['Question_view_count'] = topic['views']\n",
    "        post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "        comments = requests.get(post_url).json()['post_stream']['posts']\n",
    "        post['Question_body'] = comments[0]['cooked']\n",
    "        post['Answers'] = comments[1:]\n",
    "        post_list.append(post)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    if 'more_topics_url' not in topic_list.keys():\n",
    "        break\n",
    "\n",
    "json_post_list = json.dumps(post_list, indent='\\t')\n",
    "with open(os.path.join(raw_path, 'SigOpt.json'), 'w') as outfile:\n",
    "    outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = -1\n",
    "base_url = 'https://discuss.dvc.org/'\n",
    "page_suffix = 'c/questions/9.json?page='\n",
    "post_list = []\n",
    "\n",
    "while True:\n",
    "    page = page + 1\n",
    "    page_url = base_url + page_suffix + str(page)\n",
    "    topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "    for topic in topic_list['topics']:\n",
    "        post_url = base_url + 't/' + \\\n",
    "            topic['slug'] + '/' + str(topic['id']) + '.json'\n",
    "\n",
    "        post = {}\n",
    "        post['Question_title'] = topic['title']\n",
    "        post['Question_link'] = post_url\n",
    "        post['Question_creation_date'] = topic['created_at']\n",
    "        post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "        post['Question_upvote_count'] = topic['like_count']\n",
    "        post['Question_view_count'] = topic['views']\n",
    "        post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "        comments = requests.get(post_url).json()['post_stream']['posts']\n",
    "        post['Question_body'] = comments[0]['cooked']\n",
    "        post['Answers'] = comments[1:]\n",
    "        post_list.append(post)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    if 'more_topics_url' not in topic_list.keys():\n",
    "        break\n",
    "\n",
    "json_post_list = json.dumps(post_list, indent='\\t')\n",
    "with open(os.path.join(raw_path, 'DVC.json'), 'w') as outfile:\n",
    "    outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Amazon SageMaker': 528,\n",
       " 'Azure Machine Learning': 1435,\n",
       " 'Databricks': 154,\n",
       " 'Domino': 13,\n",
       " 'DVC': 315,\n",
       " 'Guild AI': 115,\n",
       " 'MLFlow': 280,\n",
       " 'Polyaxon': 43,\n",
       " 'SigOpt': 15,\n",
       " 'Vertex AI': 297,\n",
       " 'Weights & Biases': 583}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "external_post_number = {}\n",
    "for file_name in glob.glob(os.path.join(raw_path, '*.json')):\n",
    "    with open(file_name) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        tool_name = file_name.split('\\\\')[-1].split('.')[0]\n",
    "        external_post_number[tool_name] = len(json_data)\n",
    "external_post_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_others_round1_path = os.path.join(others_round1_path, 'Sample')\n",
    "    \n",
    "if not os.path.isdir(sample_others_round1_path):\n",
    "    os.mkdir(sample_others_round1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in glob.glob(os.path.join(raw_path, '*.json')):\n",
    "    if 'SO.json' in file_name:\n",
    "        continue\n",
    "    \n",
    "    with open(file_name) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        tool_name = file_name.split('\\\\')[-1].split('.')[0]\n",
    "        sample_posts = random_sample(json_data)\n",
    "    \n",
    "    json_sample_posts = json.dumps(sample_posts, indent='\\t')\n",
    "    with open(os.path.join(sample_others_round1_path, f'{tool_name}.json'), 'w') as outfile:\n",
    "        outfile.write(json_sample_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question_id</th>\n",
       "      <th>Question_title</th>\n",
       "      <th>Question_body</th>\n",
       "      <th>Question_answer_count</th>\n",
       "      <th>Question_comment_count</th>\n",
       "      <th>Question_creation_date</th>\n",
       "      <th>Question_favorite_count</th>\n",
       "      <th>Question_last_edit_date</th>\n",
       "      <th>Question_score</th>\n",
       "      <th>Question_tags</th>\n",
       "      <th>...</th>\n",
       "      <th>Owner_location</th>\n",
       "      <th>Owner_reputation</th>\n",
       "      <th>Owner_up_votes</th>\n",
       "      <th>Owner_down_votes</th>\n",
       "      <th>Owner_views</th>\n",
       "      <th>Answer_body</th>\n",
       "      <th>Answer_comment_count</th>\n",
       "      <th>Answer_creation_date</th>\n",
       "      <th>Answer_last_edit_date</th>\n",
       "      <th>Answer_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>62841756</td>\n",
       "      <td>Can't use HDFS path to set_tracking_uri in mlf...</td>\n",
       "      <td>&lt;p&gt;I'm new to mlflow so I may misunderstand ho...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-07-10 20:21:30.347000 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-07-10 21:00:46.223000 UTC</td>\n",
       "      <td>0</td>\n",
       "      <td>python|mlflow</td>\n",
       "      <td>...</td>\n",
       "      <td>Atlanta, GA</td>\n",
       "      <td>458</td>\n",
       "      <td>76</td>\n",
       "      <td>5</td>\n",
       "      <td>119</td>\n",
       "      <td>&lt;p&gt;Ok. So it looks like while the ARTIFACTS ST...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-07-13 17:24:30.470000 UTC</td>\n",
       "      <td>None</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>70333546</td>\n",
       "      <td>How to create Azure Databricks Notebook via Te...</td>\n",
       "      <td>&lt;p&gt;So I am completely new to the terraform and...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2021-12-13 10:58:55.800000 UTC</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2021-12-13 13:06:41.373000 UTC</td>\n",
       "      <td>1</td>\n",
       "      <td>terraform|azure-databricks|terraform-provider-...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>395</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>&lt;p&gt;In general you can put these objects in any...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-12-13 13:16:28.957000 UTC</td>\n",
       "      <td>None</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62841754</td>\n",
       "      <td>Databricks- Move data from Databricks temp to ...</td>\n",
       "      <td>&lt;p&gt;How do we move data from databricks temp vi...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2020-07-10 20:21:27.120000 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020-08-18 08:49:38.443000 UTC</td>\n",
       "      <td>1</td>\n",
       "      <td>data-warehouse|databricks|azure-databricks|azu...</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70535547</td>\n",
       "      <td>How do I import custom libraries in Databricks...</td>\n",
       "      <td>&lt;p&gt;I uploaded a jar library on my cluster in D...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2021-12-30 18:46:06.973000 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>databricks|azure-databricks</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>70883687</td>\n",
       "      <td>In Azure, is there a way to maintain a current...</td>\n",
       "      <td>&lt;p&gt;I have a Delta Table in Azure Databricks th...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-01-27 18:13:31.857000 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>2</td>\n",
       "      <td>databricks|azure-databricks|delta-lake|azure-s...</td>\n",
       "      <td>...</td>\n",
       "      <td>Vienna, VA</td>\n",
       "      <td>565</td>\n",
       "      <td>2594</td>\n",
       "      <td>32</td>\n",
       "      <td>200</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Question_id                                     Question_title  \\\n",
       "0     62841756  Can't use HDFS path to set_tracking_uri in mlf...   \n",
       "1     70333546  How to create Azure Databricks Notebook via Te...   \n",
       "2     62841754  Databricks- Move data from Databricks temp to ...   \n",
       "3     70535547  How do I import custom libraries in Databricks...   \n",
       "4     70883687  In Azure, is there a way to maintain a current...   \n",
       "\n",
       "                                       Question_body  Question_answer_count  \\\n",
       "0  <p>I'm new to mlflow so I may misunderstand ho...                      2   \n",
       "1  <p>So I am completely new to the terraform and...                      1   \n",
       "2  <p>How do we move data from databricks temp vi...                      1   \n",
       "3  <p>I uploaded a jar library on my cluster in D...                      1   \n",
       "4  <p>I have a Delta Table in Azure Databricks th...                      0   \n",
       "\n",
       "   Question_comment_count          Question_creation_date  \\\n",
       "0                       0  2020-07-10 20:21:30.347000 UTC   \n",
       "1                       0  2021-12-13 10:58:55.800000 UTC   \n",
       "2                       0  2020-07-10 20:21:27.120000 UTC   \n",
       "3                       2  2021-12-30 18:46:06.973000 UTC   \n",
       "4                       3  2022-01-27 18:13:31.857000 UTC   \n",
       "\n",
       "   Question_favorite_count         Question_last_edit_date  Question_score  \\\n",
       "0                      NaN  2020-07-10 21:00:46.223000 UTC               0   \n",
       "1                      1.0  2021-12-13 13:06:41.373000 UTC               1   \n",
       "2                      NaN  2020-08-18 08:49:38.443000 UTC               1   \n",
       "3                      NaN                            None               1   \n",
       "4                      NaN                            None               2   \n",
       "\n",
       "                                       Question_tags  ...  Owner_location  \\\n",
       "0                                      python|mlflow  ...     Atlanta, GA   \n",
       "1  terraform|azure-databricks|terraform-provider-...  ...            None   \n",
       "2  data-warehouse|databricks|azure-databricks|azu...  ...            None   \n",
       "3                        databricks|azure-databricks  ...            None   \n",
       "4  databricks|azure-databricks|delta-lake|azure-s...  ...      Vienna, VA   \n",
       "\n",
       "  Owner_reputation Owner_up_votes Owner_down_votes  Owner_views  \\\n",
       "0              458             76                5          119   \n",
       "1              395             49                1           24   \n",
       "2               49              0                0           31   \n",
       "3               31             10                0            5   \n",
       "4              565           2594               32          200   \n",
       "\n",
       "                                         Answer_body  Answer_comment_count  \\\n",
       "0  <p>Ok. So it looks like while the ARTIFACTS ST...                   1.0   \n",
       "1  <p>In general you can put these objects in any...                   0.0   \n",
       "2                                               None                   NaN   \n",
       "3                                               None                   NaN   \n",
       "4                                               None                   NaN   \n",
       "\n",
       "             Answer_creation_date Answer_last_edit_date  Answer_score  \n",
       "0  2020-07-13 17:24:30.470000 UTC                  None           0.0  \n",
       "1  2021-12-13 13:16:28.957000 UTC                  None           2.0  \n",
       "2                            None                  None           NaN  \n",
       "3                            None                  None           NaN  \n",
       "4                            None                  None           NaN  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(raw_path, 'SO.json'))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = {'azure-machine-learning-service', 'azure-machine-learning-studio', 'azure-machine-learning-workbench', 'google-cloud-vertex-ai', 'databricks-unity-catalog',\n",
    "        'databricks-ml', 'amazon-sagemaker', 'amazon-sagemaker-experiments', 'mlflow', 'dvc', 'wandb', 'clearml', 'comet-ml', 'neptune', 'pachyderm', 'lakefs'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Question_valid_tags'] = [[] for _ in range(len(df))]\n",
    "for index, row in df.iterrows():\n",
    "    row_tags = set(str(row['Question_tags']).strip().split('|'))\n",
    "    df.at[index, 'Question_valid_tags'] = list(row_tags.intersection(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+: 1579\n",
      "1+: 69\n",
      "2+: 8\n"
     ]
    }
   ],
   "source": [
    "arity = 0\n",
    "while True:\n",
    "    post_number = df[df['Question_valid_tags'].map(len) > arity].shape[0]\n",
    "    if post_number < 1:\n",
    "        break\n",
    "    print(f'{arity}+: {post_number}')\n",
    "    arity = arity + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "SO_round1_path = os.path.join(round1_path, 'SO')\n",
    "    \n",
    "if not os.path.isdir(SO_round1_path):\n",
    "    os.mkdir(SO_round1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df[df['Question_valid_tags'].map(len) > 0]\n",
    "df_valid.to_json(os.path.join(SO_round1_path, 'Valid.json'),\n",
    "                 indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qualified = df_valid[df_valid['Question_score'] > -1]\n",
    "df_qualified.to_json(os.path.join(\n",
    "    SO_round1_path, 'Qualified.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qualified_completed = df_qualified.dropna(subset=['Answer_body'])\n",
    "df_qualified_completed.to_json(os.path.join(\n",
    "    SO_round1_path, 'Qualified_completed.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"['azure-machine-learning-studio', 'azure-machine-learning-service']\",\n",
       "       \"['azure-machine-learning-studio', 'mlflow']\",\n",
       "       \"['azure-machine-learning-service', 'mlflow']\",\n",
       "       \"['azure-machine-learning-studio', 'azure-machine-learning-workbench', 'azure-machine-learning-service']\",\n",
       "       \"['azure-machine-learning-studio', 'azure-machine-learning-workbench']\",\n",
       "       \"['azure-machine-learning-workbench', 'azure-machine-learning-service']\",\n",
       "       \"['amazon-sagemaker', 'mlflow']\"], dtype=object)"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one_plus_tags = df[df['Question_valid_tags'].map(len) > 1]\n",
    "df_one_plus_tags['Question_valid_tags'].map(str).unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    multiplicity = 0\n",
    "    for tag in row['Question_valid_tags']:\n",
    "        if 'azure' in tag:\n",
    "            multiplicity += 1\n",
    "    if multiplicity == len(row['Question_valid_tags']) or multiplicity > 1:\n",
    "        df.at[index, 'Question_valid_tags'] = ['azure']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_one_plus_tags = df[df['Question_valid_tags'].map(len) > 1]\n",
    "df_one_plus_tags.to_json(os.path.join(SO_round1_path, 'Qualified_one_plus_tags.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['mlflow', 'azure', 'amazon-sagemaker', 'google-cloud-vertex-ai',\n",
       "       'databricks-unity-catalog', 'dvc', 'clearml', 'pachyderm', 'wandb',\n",
       "       'comet-ml', 'neptune', 'databricks-ml', 'lakefs', 'sagemaker'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one_labelled_tag = pd.read_json(os.path.join(SO_round1_path, 'Qualified_one_annotated_tag.json'))\n",
    "df_one_labelled_tag['Question_valid_tags'] = df_one_labelled_tag['taxonomy'].map(lambda x: x[0]['taxonomy'][0])\n",
    "df_exclusive_tag = df[df['Question_valid_tags'].map(len) == 1]\n",
    "df_exclusive_tag = pd.concat([df_exclusive_tag, df_one_labelled_tag])\n",
    "df_exclusive_tag['Question_valid_tags'] = df_exclusive_tag['Question_valid_tags'].map(lambda x: x[0])\n",
    "df_exclusive_tag['Question_valid_tags'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df_exclusive_tag.iterrows():\n",
    "    if 'sagemaker' in row['Question_valid_tags']:\n",
    "        df_exclusive_tag.at[index, 'Question_valid_tags'] = 'sagemaker'\n",
    "    elif 'databricks' in row['Question_valid_tags']:\n",
    "        df_exclusive_tag.at[index, 'Question_valid_tags'] = 'databricks'\n",
    "    elif 'vertex-ai' in row['Question_valid_tags']:\n",
    "        df_exclusive_tag.at[index, 'Question_valid_tags'] = 'vertex-ai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_SO_round1_path = os.path.join(SO_round1_path, 'Sample')\n",
    "    \n",
    "if not os.path.isdir(sample_SO_round1_path):\n",
    "    os.mkdir(sample_SO_round1_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df_exclusive_tag.groupby(['Question_valid_tags']):\n",
    "    json_group = group.to_json(orient='records')\n",
    "    parsed_group = json.loads(json_group)\n",
    "    sample_group = random_sample(parsed_group)\n",
    "    json_sample_group = json.dumps(sample_group, indent='\\t')\n",
    "    with open(os.path.join(sample_SO_round1_path, f'{name}.json'), 'w') as outfile:\n",
    "        outfile.write(json_sample_group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "5885b8dfb9cbbda4406b6affad9fec9c9d8707d239c56787746651bbddd9354d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
