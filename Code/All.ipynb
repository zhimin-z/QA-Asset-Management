{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import random\n",
    "import json\n",
    "import time\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool2tag = {\n",
    "    'Azure Machine Learning': {'azure-machine-learning-service', 'azure-machine-learning-studio', 'azure-machine-learning-workbench'},\n",
    "    'Vertex AI': {'google-cloud-vertex-ai'},\n",
    "    'Databricks': {'databricks-unity-catalog', 'databricks-ml'},\n",
    "    'Amazon SageMaker': {'amazon-sagemaker', 'amazon-sagemaker-experiments'},\n",
    "    'MLFlow': {'mlflow'},\n",
    "    'DVC': {'dvc'},\n",
    "    'Weights & Biases': {'wandb'},\n",
    "    'ClearML': {'clearml'},\n",
    "    'Comet': {'comet-ml'},\n",
    "    'Neptune': {'neptune'},\n",
    "    'Pachyderm': {'pachyderm'},\n",
    "    'LakeFS': {'lakefs'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dataset = '../Dataset'\n",
    "\n",
    "path_so = os.path.join(path_dataset, 'Stack Overflow')\n",
    "path_to = os.path.join(path_dataset, 'Tool-specific Others')\n",
    "    \n",
    "path_so_round1 = os.path.join(path_so, 'Round#1')\n",
    "path_to_round1 = os.path.join(path_to, 'Round#1')\n",
    "    \n",
    "path_so_round1_raw = os.path.join(path_so_round1, 'Raw')\n",
    "path_to_round1_raw = os.path.join(path_to_round1, 'Raw')\n",
    "path_so_round1_filtered = os.path.join(path_so_round1, 'Filtered')\n",
    "\n",
    "if not os.path.exists(path_dataset):\n",
    "    os.makedirs(path_dataset)\n",
    "\n",
    "if not os.path.isdir(path_so):\n",
    "    os.mkdir(path_so)\n",
    "    \n",
    "if not os.path.isdir(path_to):\n",
    "    os.mkdir(path_to)\n",
    "    \n",
    "if not os.path.isdir(path_so_round1):\n",
    "    os.mkdir(path_so_round1)\n",
    "    \n",
    "if not os.path.isdir(path_to_round1):\n",
    "    os.mkdir(path_to_round1)\n",
    "    \n",
    "if not os.path.isdir(path_so_round1_raw):\n",
    "    os.mkdir(path_so_round1_raw)\n",
    "    \n",
    "if not os.path.isdir(path_to_round1_raw):\n",
    "    os.mkdir(path_to_round1_raw)\n",
    "    \n",
    "if not os.path.isdir(path_so_round1_filtered):\n",
    "    os.mkdir(path_so_round1_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = -1\n",
    "base_url = 'https://my.guild.ai/'\n",
    "page_suffix = 'c/troubleshooting/6.json?page='\n",
    "post_list = []\n",
    "\n",
    "while True:\n",
    "    page = page + 1\n",
    "    page_url = base_url + page_suffix + str(page)\n",
    "    topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "    for topic in topic_list['topics']:\n",
    "        post_url = base_url + 't/' + \\\n",
    "            topic['slug'] + '/' + str(topic['id']) + '.json'\n",
    "\n",
    "        post = {}\n",
    "        post['Question_title'] = topic['title']\n",
    "        post['Question_link'] = post_url\n",
    "        post['Question_creation_date'] = topic['created_at']\n",
    "        post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "        post['Question_upvote_count'] = topic['like_count']\n",
    "        post['Question_view_count'] = topic['views']\n",
    "        post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "        comments = requests.get(post_url).json()['post_stream']['posts']\n",
    "        post['Question_body'] = comments[0]['cooked']\n",
    "        post['Answers'] = comments[1:]\n",
    "        post_list.append(post)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    if 'more_topics_url' not in topic_list.keys():\n",
    "        break\n",
    "\n",
    "json_post_list = json.dumps(post_list, indent='\\t')\n",
    "with open(os.path.join(path_to_round1_raw, 'Guild AI.json'), 'w') as outfile:\n",
    "    outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = -1\n",
    "base_url = 'https://community.wandb.ai/'\n",
    "page_suffix = 'c/w-b-support/36.json?page='\n",
    "post_list = []\n",
    "\n",
    "while True:\n",
    "    page = page + 1\n",
    "    page_url = base_url + page_suffix + str(page)\n",
    "    topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "    for topic in topic_list['topics']:\n",
    "        post_url = base_url + 't/' + \\\n",
    "            topic['slug'] + '/' + str(topic['id']) + '.json'\n",
    "\n",
    "        post = {}\n",
    "        post['Question_title'] = topic['title']\n",
    "        post['Question_link'] = post_url\n",
    "        post['Question_creation_date'] = topic['created_at']\n",
    "        post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "        post['Question_upvote_count'] = topic['like_count']\n",
    "        post['Question_view_count'] = topic['views']\n",
    "        post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "        comments = requests.get(post_url).json()['post_stream']['posts']\n",
    "        post['Question_body'] = comments[0]['cooked']\n",
    "        post['Answers'] = comments[1:]\n",
    "        post_list.append(post)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    if 'more_topics_url' not in topic_list.keys():\n",
    "        break\n",
    "\n",
    "json_post_list = json.dumps(post_list, indent='\\t')\n",
    "with open(os.path.join(path_to_round1_raw, 'Weights & Biases.json'), 'w') as outfile:\n",
    "    outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = -1\n",
    "base_url = 'https://community.sigopt.com/'\n",
    "page_suffix = 'c/general-discussion/9.json?page='\n",
    "post_list = []\n",
    "\n",
    "while True:\n",
    "    page = page + 1\n",
    "    page_url = base_url + page_suffix + str(page)\n",
    "    topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "    for topic in topic_list['topics']:\n",
    "        post_url = base_url + 't/' + \\\n",
    "            topic['slug'] + '/' + str(topic['id']) + '.json'\n",
    "\n",
    "        post = {}\n",
    "        post['Question_title'] = topic['title']\n",
    "        post['Question_link'] = post_url\n",
    "        post['Question_creation_date'] = topic['created_at']\n",
    "        post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "        post['Question_upvote_count'] = topic['like_count']\n",
    "        post['Question_view_count'] = topic['views']\n",
    "        post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "        comments = requests.get(post_url).json()['post_stream']['posts']\n",
    "        post['Question_body'] = comments[0]['cooked']\n",
    "        post['Answers'] = comments[1:]\n",
    "        post_list.append(post)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    if 'more_topics_url' not in topic_list.keys():\n",
    "        break\n",
    "\n",
    "json_post_list = json.dumps(post_list, indent='\\t')\n",
    "with open(os.path.join(path_to_round1_raw, 'SigOpt.json'), 'w') as outfile:\n",
    "    outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = -1\n",
    "base_url = 'https://discuss.dvc.org/'\n",
    "page_suffix = 'c/questions/9.json?page='\n",
    "post_list = []\n",
    "\n",
    "while True:\n",
    "    page = page + 1\n",
    "    page_url = base_url + page_suffix + str(page)\n",
    "    topic_list = requests.get(page_url).json()['topic_list']\n",
    "\n",
    "    for topic in topic_list['topics']:\n",
    "        post_url = base_url + 't/' + \\\n",
    "            topic['slug'] + '/' + str(topic['id']) + '.json'\n",
    "\n",
    "        post = {}\n",
    "        post['Question_title'] = topic['title']\n",
    "        post['Question_link'] = post_url\n",
    "        post['Question_creation_date'] = topic['created_at']\n",
    "        post['Question_answer_count'] = topic['posts_count'] - 1\n",
    "        post['Question_upvote_count'] = topic['like_count']\n",
    "        post['Question_view_count'] = topic['views']\n",
    "        post['Question_has_accepted_answer'] = topic['has_accepted_answer']\n",
    "        comments = requests.get(post_url).json()['post_stream']['posts']\n",
    "        post['Question_body'] = comments[0]['cooked']\n",
    "        post['Answers'] = comments[1:]\n",
    "        post_list.append(post)\n",
    "\n",
    "        time.sleep(2)\n",
    "\n",
    "    if 'more_topics_url' not in topic_list.keys():\n",
    "        break\n",
    "\n",
    "json_post_list = json.dumps(post_list, indent='\\t')\n",
    "with open(os.path.join(path_to_round1_raw, 'DVC.json'), 'w') as outfile:\n",
    "    outfile.write(json_post_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tool</th>\n",
       "      <th>#Posts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Amazon SageMaker</td>\n",
       "      <td>528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Azure Machine Learning</td>\n",
       "      <td>1435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Databricks</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Domino</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DVC</td>\n",
       "      <td>315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Guild AI</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLFlow</td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Polyaxon</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SigOpt</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Vertex AI</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Weights &amp; Biases</td>\n",
       "      <td>583</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Tool  #Posts\n",
       "0         Amazon SageMaker     528\n",
       "1   Azure Machine Learning    1435\n",
       "2               Databricks     154\n",
       "3                   Domino      13\n",
       "4                      DVC     315\n",
       "5                 Guild AI     115\n",
       "6                   MLFlow     280\n",
       "7                 Polyaxon      43\n",
       "8                   SigOpt      15\n",
       "9                Vertex AI     297\n",
       "10        Weights & Biases     583"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_specific_post_summary = {}\n",
    "for file_name in glob.glob(os.path.join(path_to_round1_raw, '*.json')):\n",
    "    with open(file_name) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        tool_name = os.path.split(file_name)[1].split('.')[0]\n",
    "        tool_specific_post_summary[tool_name] = len(json_data)\n",
    "tool_specific_post_summary = pd.DataFrame(tool_specific_post_summary.items(), columns=['Tool', '#Posts'])\n",
    "tool_specific_post_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question_title</th>\n",
       "      <th>Question_body</th>\n",
       "      <th>Question_answer_count</th>\n",
       "      <th>Question_comment_count</th>\n",
       "      <th>Question_creation_date</th>\n",
       "      <th>Question_favorite_count</th>\n",
       "      <th>Question_score</th>\n",
       "      <th>Question_tags</th>\n",
       "      <th>Question_view_count</th>\n",
       "      <th>Owner_creation_date</th>\n",
       "      <th>...</th>\n",
       "      <th>Owner_reputation</th>\n",
       "      <th>Owner_up_votes</th>\n",
       "      <th>Owner_down_votes</th>\n",
       "      <th>Owner_views</th>\n",
       "      <th>Answer_body</th>\n",
       "      <th>Answer_comment_count</th>\n",
       "      <th>Answer_creation_date</th>\n",
       "      <th>Answer_last_edit_date</th>\n",
       "      <th>Answer_score</th>\n",
       "      <th>Question_last_edit_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Databricks display() function equivalent or al...</td>\n",
       "      <td>&lt;p&gt;I'm in the process of migrating current Dat...</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-09-08 23:11:57.953 UTC</td>\n",
       "      <td>3.0</td>\n",
       "      <td>11</td>\n",
       "      <td>apache-spark|jupyter-notebook|databricks</td>\n",
       "      <td>23405</td>\n",
       "      <td>2016-06-15 18:26:00.217 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>3287</td>\n",
       "      <td>99</td>\n",
       "      <td>24</td>\n",
       "      <td>435</td>\n",
       "      <td>&lt;p&gt;When you use Jupyter, instead of using df.s...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2020-03-18 15:19:00.54 UTC</td>\n",
       "      <td>2022-05-31 11:43:02.247 UTC</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spark: Read an inputStream instead of File</td>\n",
       "      <td>&lt;p&gt;I'm using SparkSQL in a Java application to...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2016-07-20 21:13:20.757 UTC</td>\n",
       "      <td>3.0</td>\n",
       "      <td>14</td>\n",
       "      <td>java|apache-spark|apache-spark-sql|spark-dataf...</td>\n",
       "      <td>10691</td>\n",
       "      <td>2014-11-22 00:35:18.107 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>3120</td>\n",
       "      <td>173</td>\n",
       "      <td>17</td>\n",
       "      <td>266</td>\n",
       "      <td>&lt;p&gt;You can use at least four different approac...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2016-07-25 20:08:43.747 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>databricks/spark/python/pyspark/serializers.py...</td>\n",
       "      <td>&lt;p&gt;When executing the following code provide b...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-06-03 21:18:43.163 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>python-3.x|pyspark|databricks|azure-databricks</td>\n",
       "      <td>255</td>\n",
       "      <td>2018-03-26 15:02:34.45 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>96</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>&lt;p&gt;The &lt;code&gt;SparkTrials&lt;/code&gt; automatically ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2022-06-09 15:32:38.88 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Run Azure Databricks without Spark cluster</td>\n",
       "      <td>&lt;p&gt;I have used Domino Data Lab for a while and...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2018-10-11 17:46:49.543 UTC</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6</td>\n",
       "      <td>python|azure|databricks</td>\n",
       "      <td>1829</td>\n",
       "      <td>2016-08-29 20:06:38.91 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>67</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>&lt;p&gt;You always have to have a &amp;quot;cluster&amp;quo...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2018-10-12 01:38:58.893 UTC</td>\n",
       "      <td>2020-12-17 09:14:27.683 UTC</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the difference between sagemaker-pytor...</td>\n",
       "      <td>&lt;p&gt;When porting PyTorch code / models to SageM...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-09-12 20:03:07.273 UTC</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>amazon-web-services|pytorch|amazon-sagemaker</td>\n",
       "      <td>23</td>\n",
       "      <td>2022-09-08 15:07:33.073 UTC</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      Question_title  \\\n",
       "0  Databricks display() function equivalent or al...   \n",
       "1         Spark: Read an inputStream instead of File   \n",
       "2  databricks/spark/python/pyspark/serializers.py...   \n",
       "3         Run Azure Databricks without Spark cluster   \n",
       "4  What is the difference between sagemaker-pytor...   \n",
       "\n",
       "                                       Question_body  Question_answer_count  \\\n",
       "0  <p>I'm in the process of migrating current Dat...                      5   \n",
       "1  <p>I'm using SparkSQL in a Java application to...                      1   \n",
       "2  <p>When executing the following code provide b...                      1   \n",
       "3  <p>I have used Domino Data Lab for a while and...                      2   \n",
       "4  <p>When porting PyTorch code / models to SageM...                      2   \n",
       "\n",
       "   Question_comment_count       Question_creation_date  \\\n",
       "0                       3  2017-09-08 23:11:57.953 UTC   \n",
       "1                       0  2016-07-20 21:13:20.757 UTC   \n",
       "2                       0  2022-06-03 21:18:43.163 UTC   \n",
       "3                       2  2018-10-11 17:46:49.543 UTC   \n",
       "4                       0  2022-09-12 20:03:07.273 UTC   \n",
       "\n",
       "   Question_favorite_count  Question_score  \\\n",
       "0                      3.0              11   \n",
       "1                      3.0              14   \n",
       "2                      NaN               1   \n",
       "3                      0.0               6   \n",
       "4                      NaN               0   \n",
       "\n",
       "                                       Question_tags  Question_view_count  \\\n",
       "0           apache-spark|jupyter-notebook|databricks                23405   \n",
       "1  java|apache-spark|apache-spark-sql|spark-dataf...                10691   \n",
       "2     python-3.x|pyspark|databricks|azure-databricks                  255   \n",
       "3                            python|azure|databricks                 1829   \n",
       "4       amazon-web-services|pytorch|amazon-sagemaker                   23   \n",
       "\n",
       "           Owner_creation_date  ... Owner_reputation Owner_up_votes  \\\n",
       "0  2016-06-15 18:26:00.217 UTC  ...             3287             99   \n",
       "1  2014-11-22 00:35:18.107 UTC  ...             3120            173   \n",
       "2   2018-03-26 15:02:34.45 UTC  ...               96             10   \n",
       "3   2016-08-29 20:06:38.91 UTC  ...               67              8   \n",
       "4  2022-09-08 15:07:33.073 UTC  ...                3              0   \n",
       "\n",
       "   Owner_down_votes  Owner_views  \\\n",
       "0                24          435   \n",
       "1                17          266   \n",
       "2                 0           31   \n",
       "3                 0            6   \n",
       "4                 0            1   \n",
       "\n",
       "                                         Answer_body  Answer_comment_count  \\\n",
       "0  <p>When you use Jupyter, instead of using df.s...                   1.0   \n",
       "1  <p>You can use at least four different approac...                   2.0   \n",
       "2  <p>The <code>SparkTrials</code> automatically ...                   0.0   \n",
       "3  <p>You always have to have a &quot;cluster&quo...                   0.0   \n",
       "4                                                NaN                   NaN   \n",
       "\n",
       "          Answer_creation_date        Answer_last_edit_date Answer_score  \\\n",
       "0   2020-03-18 15:19:00.54 UTC  2022-05-31 11:43:02.247 UTC         10.0   \n",
       "1  2016-07-25 20:08:43.747 UTC                          NaN          4.0   \n",
       "2   2022-06-09 15:32:38.88 UTC                          NaN          1.0   \n",
       "3  2018-10-12 01:38:58.893 UTC  2020-12-17 09:14:27.683 UTC          8.0   \n",
       "4                          NaN                          NaN          NaN   \n",
       "\n",
       "  Question_last_edit_date  \n",
       "0                     NaN  \n",
       "1                     NaN  \n",
       "2                     NaN  \n",
       "3                     NaN  \n",
       "4                     NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(os.path.join(path_so_round1_raw, 'BigQuery.json'), lines=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tag collection\n",
    "tags = []\n",
    "for key, value in tool2tag.items():\n",
    "    tags.extend(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split tags \n",
    "df['Question_valid_tags'] = [[] for _ in range(len(df))]\n",
    "for index, row in df.iterrows():\n",
    "    row_tags = set(str(row['Question_tags']).strip().split('|'))\n",
    "    df.at[index, 'Question_valid_tags'] = list(row_tags.intersection(tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0+: 4997\n",
      "1+: 213\n",
      "2+: 16\n"
     ]
    }
   ],
   "source": [
    "# count post number with different tags\n",
    "arity = 0\n",
    "while True:\n",
    "    post_number = df[df['Question_valid_tags'].map(len) > arity].shape[0]\n",
    "    if post_number < 1:\n",
    "        break\n",
    "    print(f'{arity}+: {post_number}')\n",
    "    arity = arity + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid = df[df['Question_valid_tags'].map(len) > 0]\n",
    "df_valid.to_json(os.path.join(path_so_round1_filtered, 'valid_posts.json'),\n",
    "                 indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qualified = df_valid[df_valid['Question_score'] > -1]\n",
    "df_qualified.to_json(os.path.join(\n",
    "    path_so_round1_filtered, 'valid_posts_with_at_least_zero_score.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_qualified_accepted = df_qualified.dropna(subset=['Answer_body'])\n",
    "df_qualified_accepted.to_json(os.path.join(\n",
    "    path_so_round1_filtered, 'valid_posts_with_at_least_zero_score_and_answered_accepted.json'), indent=4, orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"['azure-machine-learning-workbench', 'azure-machine-learning-studio']\",\n",
       " \"['azure-machine-learning-service', 'azure-machine-learning-studio']\",\n",
       " \"['azure-machine-learning-service', 'azure-machine-learning-workbench', 'azure-machine-learning-studio']\",\n",
       " \"['azure-machine-learning-service', 'mlflow']\",\n",
       " \"['mlflow', 'amazon-sagemaker']\",\n",
       " \"['azure-machine-learning-service', 'azure-machine-learning-workbench']\",\n",
       " \"['mlflow', 'azure-machine-learning-studio']\",\n",
       " \"['mlflow', 'dvc']\",\n",
       " \"['pachyderm', 'dvc']\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_one_plus_tags = df[df['Question_valid_tags'].map(len) > 1]\n",
    "df_one_plus_tags['Question_valid_tags'].map(str).unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a map from tag to tool\n",
    "tag2tool = {}\n",
    "for key, value in tool2tag.items():\n",
    "    for elem in value:\n",
    "        tag2tool.setdefault(elem, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\zhimi\\AppData\\Local\\Temp\\ipykernel_13032\\3894066259.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_qualified.loc[index, 'Question_exclusive_tag'] = tool_list[0]\n"
     ]
    }
   ],
   "source": [
    "df_qualified.assign(Question_exclusive_tag='')\n",
    "posts_split = []\n",
    "\n",
    "for index, row in df_qualified.iterrows():\n",
    "    tool_list = []\n",
    "    for tag in row['Question_valid_tags']:\n",
    "        tool_list.append(tag2tool[tag])\n",
    "    df_qualified.loc[index, 'Question_exclusive_tag'] = tool_list[0]\n",
    "    if len(set(tool_list)) > 1:\n",
    "        for tool in set(tool_list[1:]):\n",
    "            series = row.copy()\n",
    "            series['Question_exclusive_tag'] = tool\n",
    "            posts_split.append(series)\n",
    "df_qualified = df_qualified.drop(columns=['Question_valid_tags'])\n",
    "df_qualified = pd.concat([df_qualified, pd.DataFrame(posts_split)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample a fraction of elements from a list\n",
    "def random_sample(lst, frac=0.1, lowerbound=50, upperbound=100):\n",
    "    sample_size = int(len(lst) * frac)\n",
    "    if sample_size < lowerbound:\n",
    "        sample_size = lowerbound\n",
    "    if sample_size > min(len(lst), upperbound):\n",
    "        sample_size = min(len(lst), upperbound)\n",
    "    return random.sample(lst, sample_size)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file_name in glob.glob(os.path.join(raw_path, '*.json')):\n",
    "    with open(file_name) as json_file:\n",
    "        json_data = json.load(json_file)\n",
    "        sample_posts = random_sample(json_data)\n",
    "\n",
    "    json_sample_posts = json.dumps(sample_posts, indent='\\t')\n",
    "    tool_name = os.path.split(file_name)[1].split('.')[0]\n",
    "    with open(os.path.join(sample_others_round1_path, f'{tool_name}.json'), 'w') as outfile:\n",
    "        outfile.write(json_sample_posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, group in df_exclusive_tag.groupby(['Question_valid_tags']):\n",
    "    json_group = group.to_json(orient='records')\n",
    "    parsed_group = json.loads(json_group)\n",
    "    sample_group = random_sample(parsed_group)\n",
    "    json_sample_group = json.dumps(sample_group, indent='\\t')\n",
    "    with open(os.path.join(sample_SO_round1_path, f'{name}.json'), 'w') as outfile:\n",
    "        outfile.write(json_sample_group)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb9e7b88a259684df50811b5249344f7cc06d54cdb1cf11111ce301ae44eac9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
